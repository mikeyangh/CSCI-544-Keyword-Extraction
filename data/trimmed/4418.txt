Page1基于在线消息传递的主题追踪方法龚声蓉叶芸刘纯平季怡(苏州大学计算机科学与技术学院江苏苏州215006)摘要主题追踪因可以有效地汇集和组织分散在不同时间、地点的信息,并从主题层次的角度对某个主题相关事件的时效性、动态演化关系等得到比较全面的把握,成为当前数据挖掘领域的重要研究方向.现有基于概率主题模型的主题追踪方法主要以潜在狄利克雷分布(LatentDirichletallocation,LDA)模型为基础,采用在线吉布斯采样(OnlineGibbsSampling,OGS)和在线变分贝叶斯(OnlineVariationalBayesian,OVB)算法进行参数估计.OGS和OVB算法尽管解决了LDA模型中使用传统离线近似推理方法所需内存空间的大小随数据集的增长而不断增加,无法训练海量数据集以及数据流数据的问题,但训练的精度和速度均有待提高.该文基于LDA模型的改进因子图提出了一种在线消息传递(OnlineBeliefPropagation,OBP)的主题追踪算法.该算法借助因子图中消息传递(BeliefPropagation,BP)算法的推理,通过切分海量数据集为段,并用前一段数据集训练后的参数计算当前段的梯度下降,使得主题追踪更加快速和准确.四组大规模文本数据集的实验对比表明,LDA模型中OBP算法在速度和精度上均优越于OGS和OVB算法,文中也从理论上进一步验证了OBP算法的收敛性,并给出了主题追踪的具体应用.关键词潜在狄利克雷分布;吉布斯采样;变分贝叶斯;消息传递算法;主题追踪;社交网络;社会计算1引言随着信息技术的高速发展,来源于门户网站、电子商务网站、社交网站、论坛、博客和微博等信息正以指数级的方式增长.搜索引擎虽然可以方便的提供很多信息资源,但却不能有效地发现和管理与某一主题相关的信息.如何从上述海量的文本集中寻找热点话题成为当前信息检索(InformationRetrieval,IR)领域的研究关键,而基于概率主题模型的主题追踪方法则可以有效地发现与该主题相关的信息[1].现有基于概率主题模型的主题追踪方法,利用快速的学习算法从高维稀疏的单词数据中提取低维的主题表示,从而追踪主题的不断变化趋势.常见的主题模型有空间向量模型[2]、潜在语义分析(LatentSemanticAnalysis,LSA)[3]模型、概率潜在语义分析(ProbabilisticLatentSemanticAnalysis,PLSA)[4]模型和潜在狄利克雷分布(LDA)模型[5]等.PLSA模型和LDA模型通过联合概率描述文本单词和主题之间的关系,并且每个概率均有合理的物理解释,从而能够很好地解决文档聚类的问题,是目前最常用的主题模型.应用这两种模型的核心在于学习模型中的参数,而参数的个数对模型的复杂程度有很大的影响.PLSA模型中的参数个数会随训练文本的不断增加而增加,LDA模型对外始终只有两个超参,因此LDA模型更有利于训练海量数据集.LDA模型学习的关键在于从主题和单词的联合概率中推断出在可观测变量下主题的后验概率分布,但是无法直接通过后验概率分布求解模型中的参数,一般需要采用近似后验推理方法.目前广泛采用的近似推理方法有吉布斯采样(GibbsSampling,GS)[6]、变分贝叶斯(VariationalBayesian,VB)[5]和离线消息传递(BeliefPropagation,BP)算法[7].离线GS、VB和BP近似推理算法已经在小规模数据集上取得了应用.文献[5]提出了VB近似推理算法对文本数据分类,在平均包含16000篇文本的文本集上的实验表明,LDA模型比PLSA模型在分类速度和精度方面有实质性的提高.文献[6]首次提出了GS近似推理算法,并对图像进行分类.在2000幅,每幅大小为5×5像素的图像数据集上的实验验证了GS比VB收敛的速度快且精度高.文献[7]基于LDA模型,在四组文本数据集上对比分析了BP、GS和VB三种近似推理算法.实验表明BP近似推理算法的精度和训练速度均优于GS和VB算法.这些离线算法尽管简单稳定,但收敛速度通常很慢,且需要将整个训练集加载到内存.由于实际中往往处理的是大规模的实时流数据,如博客等流型数据,离线算法因数据集本身不完全以及内存不足而无法处理.为了克服离线算法在处理这类流数据时的缺陷,将海量数据集切分成若干小段,然后顺序处理每一段数据的在线学习算法成为一个首选.对每个时间段,在线算法只加载一小段数据到内存,并对当前段用梯度下降法[8]估计模型的参数,在当前段训练结束后,将该段数据集移出内存,再加载下一段数据集进行学习.目前在线学习算法已经在主题模型的近似推理[9-14]、目标检测[15-16]、大规模矩阵分解[17-18]、高维数据分析[19]和SVM中的核函数在线学习[20]等众多方面取得了广泛的应用.Canini等人[21]提出了基于LDA模型的增长式吉布斯采样(IncrementalGibbsSampling,IGS)算法,即在线吉布斯采样(OGS)算法;Hoffman等人[12]提出了在线变分贝叶斯(OVB)算法.这两类在线近似推理方法在分类时比离线算法需要较少的迭代次数就能达到收敛,此外需要的内存空间是固定的,仅与每一小段数据集的大小成比例.但OGS和OVB方法以离线GS和VB算法为基础,而GS算法在近似推理时需要对所有文本中的每个单词训练,VB算法中引入了时间复杂度较高的digamma函数,这就导致OGS和OVB算法的精度和速度都有待提高.在OGS在线学习算法的基础上,文献[22]和文献[10]提出了基于OGS的在线主题追踪算法.为了提高主题追踪的精度和速度,本文提出基于LDA模型的在线消息传递(OBP)算法,该算法可以将隐藏变量的联合概率分布分解成因子间的乘积,计算其后验边界概率而非后验联合概率,即变量与因子Page3之间传递的消息,而消息可以通过本地计算并归一化得到.文献[23]已提出了基于PLSA模型的OBP算法,由于PLSA模型中包含的参数会随训练文本的不断增加而增加,使得模型的复杂度也不断增加.不同于文献[23],LDA模型是将参数看作变量,引入了两个语料库级超级参数,使得模型对外始终只有两个参数.实验表明LDA模型下的OBP算法比PLSA模型下的OBP算法更优越,且LDA模型下的OBP算法比OGS和OVB算法更准确和快速.更具体的说,OBP算法是把海量数据集切分成若干独立小段.训练时先随机初始化第1段数据集的参数,训练结束后保存训练结果,而从第2段到最后一段,OBP算法将前一段数据集训练的结果作为当前段参数的初始化,然后依次训练每一段数据集.对于每段数据集,OBP算法使用随机优化方法稳定学习后的参数,确保OBP算法收敛到目标函数的局部最优,最后本文从理论方面进一步验证了OBP算法的有效性.本文第2节简单介绍LDA模型及现有在线学习算法分析;第3节介绍LDA模型的因子图表示,给出应用于大规模数据集的LDA模型的OBP算法,并从理论上证明OBP算法的收敛性;第4节给出OBP算法在4个大规模数据集下实验对比及主题追踪的具体应用;最后一节为总结.2LDA及其在线学习算法分析2.1LDA模型概述LDA模型是将实际可观测的“文档-词”的高维稀疏空间,通过快速的学习算法降低到低维空间,图1给出了LDA模型的3层概率图表示.LDA模型由单词、文本和语料库3层构成.其中单词层包括可观测的单词wn(1wnW)和隐藏主题zn(zn=k,1kK);文本层包括指定文本所对应的主题分布θd和指定主题对应单词表的概率分布k和;语料层包括控制文本层θd和k变量的α和β超参,D为语料库中总文档数,N为平均每篇文本的单词数,W为单词表大小,K为总主题数.模型中用到的符号标记说明如表1所示.在每篇文档只有单一主题的假设前提下,展开下面的分析和讨论.1dD1wW1kKw={w,d}z={zw,d}LDA是一个生成模型,即文本可以由多个隐藏主题混合而构成.基于LDA模型,文本生成的过程如下:(1)根据先验分布θd~Dirichlet(α),随机选择一个多项式分布θd,其中1dD,确定文本主题分布;(2)根据先验分布k~Dirichlet(β),随机选择一个多项式分布k,其中1kK,确定该主题下词表中的单词分布;(3)对文本d中的每个单词w,1wW:首先根据zj~Discrete(θd)选择一个主题zj,然后根据wi~Discrete(zj)从被选中主题所对应的单词分布中选择一个单词wi,其中1iN.2.2OGS和OVB算法分析LDA模型的目标是在给定文本数据集w=(w1,…,wN)的条件下,推断出文本对应的主题分布θ,主题对应单词表的概率分布和单词所属隐藏主题变量z=(z1,…,zN)分布.但后验概率分布p(θ,,z|w)的复杂性使得我们不能直接求解,而是通常采用离线GS、VB、BP和在线OGS、OVB近似推理算法.在线算法OGS和OVB分别以离线GS和VB算法为基础,而GS算法从后验边际概率p(z)中,对每个单词w采样一个主题标签z.理论上而言,多次扫描迭代后p(z)会收敛到真实后验概率分布.由于GS算法需对每个单词扫描,当每篇文本中单词数量较大,扫描时间必然增加.此外,GS算法收敛速度很慢,实际中需要对文档-词汇矩阵扫描500~1000次才会收敛.因此,GS算法无法满足对海量数据的处理.VB算法利用一个可以分解且方便优化的近似下界函数逼近后验概率函数.由于VB算法中下界函数与真实目标函数间存在误差,收敛时精度不如GS算法.因此为了克服这一不足,引入了较复杂的digamma函数,但这大大降低了VB算法的计算效率,甚至使其收敛速度低于GS算法.鉴于Page4此,文献[6]提出了离线BP算法,并验证了BP算法优于离线GS和VB算法.针对海量数据训练,直接利用在线OGS和OVB算法,其精度和速度都有待改善.本文借助基于离线GS和VB算法的在线OGS和OVB算法构建思想,提出了基于离线BP的在线OBP算法.3LDA模型在线消息传递算法3.1LDA模型的因子图表示离线消息传递BP算法[7]是一种从马尔可夫框架推导出的新颖近似推理方法,为了推理消息传递,Zeng等人[7]将传统LDA模型的概率图表示(图1)转变为因子图表示(图2).离线BP算法将主题模型视为贴标签问题,即为单词表中所有单词索引w={w,d}赋予语义标签z={zw,d}.在无向概率图模型中,马尔可夫模型可以借助邻居系统和团势函数,基于最大化后验估计获得的最大化后验概率指派最佳主题标签.因此因子图表示的LDA模型的离线BP算法,首先定义主题标签zw,d的邻居系统z-w,d和zw,-d,其中z-w,d表示除w外,文本d中所有单词的主题标签,zw,-d表示除文本d外,单词w在所有文本中的主题标签;其次设置合适的团势函数,以惩罚或奖励邻居系统中不同的局部标签,从而实现主题模型的3个本质假设:共现、平滑和聚集[7].图2中方框表示因子θd和w,圆圈表示的zw,d是两个因子间的连接变量.由于图1和图2具有相同的邻居系统、相同的连接隐藏变量以及团势函数,因此从主题模型角度而言,图2与图1等价.3.2LDA模型的BP算法在LDA模型中,BP算法不直接求后验分布p(z|w),而是求边缘概率p(zw,d),即消息μ(zw,d).消息μ(zw,d)等于邻居系统的消息,即μ(zw,d)∝μθd→zw,d(zw,d)×μw→zw,d(zw,d)(1)其中箭头方向为消息传递方向.为了描述简单,下面均用μ(zw-,d)=∑v≠wμ(zv,d),μ(z·,d)=∑wμ(zw,d),μ(zw,珔d)=∑s≠dμ(zw,s),μ(zw,·)=∑dμ(zw,d)来代替.因子传递给变量的消息是所有邻居变量传入消息的叠加,即基于马尔可夫主题平滑先验,本文设因子函数为为了便于文本间的可比性,等式(4)用文本d所有主题的消息归一化了传入消息.同理,为了单词间的可比性,等式(5)用单词表中所有单词归一化了传入消息上.因此消息更新等式可写为μ(zw,d=k)∝μ(zw-,d=k)+α其中∑kμ(zw,d=k)实际为∑zw,d=kμ(zw,d=k),为了简洁性,用∑kμ(zw,d=k)来表示.对更新的消息归一化,即∑kμ(zw,d=k)=1.然后更新参数θd和w,直到最大循环次数:3.3LDA模型在线消息传递算法鉴于LDA模型中离线算法内存空间随数据集大小的增长而不断增加,不能用于处理海量数据集.仿照OGS和OVB在线算法构建思路,我们提出了在线OBP算法来估计LDA模型中的参数.图3给出了在线学习的主要思想.OBP算法将整个数据集切分成一系列小段,对于第1段数据集,OBP算法和离线BP算法相同,从第2段到最后一段,OBP算法先固定前一段的参数w(k),然后计算当前段的消息.当OBP算法收敛或达到最大迭代次数时更新参数w(k).根据在线随机优化理论,文中权重函数选用指数函数形式:并且当前段及已训练段结果分别设置权重为ρt和Page51-ρt,其中参数κ∈(0.5,1]控制已经处理过的数据集,参数τ00用于减小每段开始迭代时的影响.图3中当S=D且κ=0时,在线消息传递算法即转化为离线消息传递算法.在训练中,首先在0与1之间随机初始化第1段参数w(k)、μw,d(k)和θd(k),为了简洁,记μ(zw,d=k)=μw,d(k)训练结束后保存参数w(k).从第2段到最后一段,只需随机初始化θd(k)参数,固定参数w(k),更新消息直到收敛.在该算法中,根据θd(k)的差值来判断当前段是否收敛,也可以采用等价的μw,d的差值来判断,因为在更新参数μw,d时是固定参数w(k).基于收敛后的消息,估计参数w(k)new∝(μw,·(k)+β)∑w数w(k),取当前段和已训练段的权重和:w(k)=(1-ρt)w(k)+ρtw(k)new(10)从式(10)可以看出,参数w(k)是对当前段及所有已经训练段的结果进行权重加和,因此,距离当前段越远的数据段被乘了多重权重因子,对当前数据段的影响也越小;相反距离当前段越近的数据段对当前段的影响也就越大.所以当处理数据内容信息分布不一致的实时数据流时,也能根据相邻数据段的内容比较快速的给出正确的主题信息.OBP算法的时间和空间复杂性相比OGS和OVB算法而言都是最小的.OBP算法在每次迭代过程中只计算各个单词间的消息传递,OGS算法却要对所有文本中的每个单词计算,因此OBP算法、OGS算法和OVB算法的时间复杂度分别为O(KDWDT)、O(KDNDT)和O(KDWDT),其中K是主题数,D是当前段的文本数,WD是单词表大小,而ND是每篇文本的单词数,T是迭代收敛次数.尽管OVB算法和OBP算法的时间复杂度相同,但OVB算法因引入了非常耗时的digamma函数,基于文本的稀疏性WD通常远小于ND,所以每次迭代时间,OBP算法少于OGS和OVB算法.此外,OGS、OVB和OBP算法的空间复杂度分别是O(KND+KS)、O(KND+KS)和O(KWD+KS),其中S是每段数据集中的文本数.因此OBP算法相对OGS和OVB的空间复杂度也是最小的.OBP算法见算法1.算法1.在线消息传递算法.输入:w,θd和μw,d输出:w和θd定义ρt=(τ0+t)-κ随机初始化并归一化第1段参数w和μw,dFort=0todo初始化当前段θdrepeatμt+1until1计算当前段数据集w(k)new∝(μw,·(k)+β)∑wEndfor3.4LDA模型的收敛性分析给定文本数据集w=(w1,…,wN),LDA模型推断文本对应主题分布θ、主题对应单词表概率分布和单词所属隐藏主题变量z=(z1,…,zN)分布.算法1则可以收敛到一个稳定值,下面给出证明.L(w,,θ,μ)=∏dLDA模型的目标函数为∝∑d=∑(d=∑d因此,用μ(nd,)代表μd和θ(nd,)代表θd计算μ和Page6θ.最大化L(n,)=∑d可以通过估计参数来实现.在线OBP算法收敛性可用随机自然梯度下降的方法来分析.在随机最优算法中最优化目标函数一般用梯度估计来完成.首先定义不断采样文本函数s(n)=1否则I(n=nd)=0.因此似然目标函数可重写为L(s,)=DEs[(n,μ(n,),θ(n,),)](12)其中定义见等式(11).给定,等式(11)的最大化,可通过nt~s不断采样观测样本,μt=μ(nt,),θt=θ(nt,)来实现,因此更新参数为其中权重ρt=(τ0+t)-κ.对每篇文本nt,固定参数,将μt和θt参数均看作随机变量,则有Εs[D(nt,μt,θt,)|]=∑d∑t=0ρ2θd,)会收敛到0[7],因此将会收敛到某个稳定值.等式(13)中只用了一阶梯度.若对梯度乘以一个合适的正定矩阵犎的逆,可加速随机梯度算法,常用正定矩阵犎是目标函数的哈森矩阵[7].t<,参数收敛,并且梯度∑d(nt,μt,θt,)k,w对等式(13)乘以ρtD再加上,便得到算法1中参数的更新等式.4实验结果与分析实验采用4组海量数据集:美国政坛领域关于政治博客blog[24]、邮件enron[25]、新闻摘要nytimes[25]和摘要pubmed[25].4个数据集大小如表2所示,其中D为数据集总文本数,W为数据集对应单词表的总单词数.在训练前先打乱重排数据集,Train为训练文档数,Test为测试文档数.所有实验迭代次数为500,实际迭代次数以模型收敛为止,主题数均为10~50,步长为10.在CPU为两个6核、频率为3.46GHz和内存128GB的SunFireX4270M2服务器下用MATLAB[26]和MEXC++获得实验.D5177398613000008200000W3357428102102660141043Train4500360002500008000000Test677386150000200000为验证LDA模型下OBP算法的高效性和准确性,在4个数据集上比较了OBP和OGS及OVB算法的混淆度[5]和训练消耗时间,OBP算法在PLSA模型和LDA模型上的实验对比,且给出了nytimes数据集上主题随训练数据集变化的演变图.4.1评估学习参数在线学习算法的权重函数[12]中引入了3个学习参数、控制已训练数据段被遗忘的缓慢程度κ∈(0.5,1]参数、用于降低每段数据集起始迭代结果影响的常数τ0(τ00),和限制切分后每段数据集文本数参数S.LDA模型在线算法训练结果与3个学习参数的有效性密切相关,通常对于S值的选取是在内存容量范围内越大越好;若S=D,则在线算法等价于传统的离线算法.表3给出了enron数据集上最佳参数值的选取和测试集预测混淆度值.从表3中不同参数值组的实验分析可以看出,当κ=0.6和τ0=1时,测试混淆度值最小.为了更好的对比LDA模型的不同近似推理算法,必须在相同参数条件下进行,为此固定κ=0.6对不同参数值组实验,对参数的选取进行了对比(表4).从表4中可以看出,当τ0越小,S越大,对应的预测混淆度值也越小.因此,本文所有实验均选用κ=0.6和τ0=1,而S值的选取则根据具体数据集的大小确定.参数κτ0S混淆度组10.9102445554.1组20.81024165490.6组30.71024643817.9组40.62562562936.2组50.66410242353.6组60.6组70.6Page7参数κτ0S混淆度组10.610242563045.2组20.62562562936.2组30.625610242472.7组40.66410242353.6组50.66440002151.0组60.6组70.64.2算法自身性能分析在假设海量数据分段后,后段权重训练依赖前段训练结果的前提下,采用提出的OBP算法对海量图4海量数据集前段训练结果对后段的影响4.3LDA模型不同算法的对比分析下面给出LDA模型下,OBP与OGS及OVB算法在4个数据集blog,enron,nytimes和pubmed上混淆度和训练耗时的对比分析.图5给出了OBP、OGS及OVB算法的混淆度对比分析结果.在4个大规模数据集上,OBP算法的混淆度均低于OGS和OVB算法,这说明相对于OGS和OVB算法,用OBP算法训练LDA模型,具有更好的预测性能.图6也给出了3种在线算法训练时间的对比分析.由于digamma函数的引入,OVB算法训练非常耗时,图6中所给OVB算法的时间是其真实时间的0.3倍.OBP算法相比OGS图5主题上混淆度的对比数据进行训练.为验证这一假设的准确性,采用混淆度和训练时间两个评价指标,在nytimes数据集上分别进行了权重依赖rely和完全独立independent的实验,实验结果如图4所示.其中混淆度是评价用训练数据集训练所得到的结果来预测测试集的一个客观指标,值越小表明对未知测试集的预测能力越好.从图4中可以看出,完全独立实验不仅模型预测集的混淆度值大,而且训练也更加耗时,若后段依赖前段训练结果,模型收敛所需迭代次数减少,相对耗时少.实验表明,OBP算法对海量数据集的分段假设在训练LDA模型上是可行的.和OVB算法都要快速.因此,在LDA模型中,OBP算法相对于OGS和OVB算法更加的高效和准确.4.4OBP算法在LDA模型与PLSA模型上的对比分析文献[23]中已经提出了基于PLSA模型因子图的OBP算法,并给出了算法的具体实现过程以及算法收敛条件.从主题模型的角度,PLSA模型中参数的个数会随训练文本数的增加而不断增加,从而导致在训练海量数据时模型中参数的个数较为庞大.LDA模型是在PLSA模型的基础上提出来的,是一个层次贝叶斯模型,模型中将参数看作随机变量,并且引入了控制参数的参数,即语料库级超参,Page8图6主题上训练耗时的对比因此LDA模型对外表现出的参数始终只有两个超参,有效地减少了模型中参数个数.图7和图8分别给出了enron和nytimes数据集在PLSA和LDA模型上训练混淆度和训练耗时对比.从图7和图8可以明显的看出,在相同训练数据集上,LDA模型不仅训练的混淆度值低于PLSA模型,而且训练所消耗的时间也远少于PLSA模型.这就从实验上进一步验证了LDA模型下的OBP算法比PLSA模型的OBP算法预测的更加精确.4.5主题追踪主题追踪的目标是针对不断增长的数据流,追踪某个给定主题随时间的不断变化.基本思路是,根据给定的训练文本,采用主题模型的近似推理算法对训练文本进行学习,得到每篇文本属于各个主题的概率以及各个主题对应单词表的概率分布.当新的流数据到来时,按照主题模型已经训练的结果对新数据进行预测,一方面预测新数据中每篇文本属于各主题的概率值;另一方面同时更新各主题所对应单词表的概率分布.为了更准确给出主题追踪信息,首先验证OBP算法在LDA模型下预测测试集的准确性.图9给出了在nytimes数据集上,OBP算法和OGS及OVB算法对测试数据集预测的混淆度对比分析图,实验选取nytimes数据集的前25000篇文本作为训练样本集,后50000篇文本作为测试样本集.训练前先将训练集切分成10段视为数据流.由于目前给定标签的语料库都是小规模的数据集,而且仅有很少的语料库会给定标签,本文采用的4组数据集均未给定标签,主题个数的最佳取值未知,所以文中在进行主题追踪时,主题个数选定为K=50.图9给出了nytimes数据集在OGS,OVB和OBP三种数据集上训练数据流数据对应的预测混淆度值对比,其中横坐标表示当前已经训练过的总文本数,纵坐标是当前训练样本对应的测试混淆度.从图9中可看出,在整个训练过程中,OBP算法的混淆度值均小于OGS和OVB算法,而且在最终收敛时,OBP算法的混淆度值也远小于OGS和OVB算法,这说明在同一模型和相同训练样本集下,OBP算法对未知测试集的预测能力最好.此外,OBP算法随训练样本数的不断增加,混淆度值降低越迅速,表明收敛速度图9nytimes数据集在OBP与OGS及OVB算法的混淆度对比Page9也更快.实验表明,基于LDA模型,OBP算法预测准确性随处理数据流数据的增加而不断上升,即在追踪某个主题时,能更准确的给出与该主题相关的信息.由于数据获取的限制,图9给出的实验是将海量数据切分后视为数据流处理,而并不是真的数据流,对同一个数据集,其内部可能是服从一致分布,但对流数据,可能其服从的分布会随时间不断变化.本文的在线算法是对当前段和已经得到的结果取权重叠加,而权重均是(0,1)之间的小数,所以对任意段数据,对其影响最大的是其相邻段,而距当前段很远的数据段由于经过若干段权重的相乘,影响就很小.所以,若处理的数据存在不一致分布,则不一致的前几段训练结果可能不准确,但是若干段之后又能准确预测.为了验证在LDA模型下OBP算法能应用于主题追踪,表5给出了模型在enron语料库前9个数据段训练后对应主题的变化,由于空间的限制,表5中仅给出了第10,20,30和40个主题在第1,第5和第9段数据集训练之后所包含的单词.实验是选取enron语料库前36000篇文本,并将其均分成9段数据集视做数据流处理,且模型训练时主题的个数选定为50.根据表5中列出的主题在训练若干段数据流之后的不断变化,可以看出每个主题所包含的单词会随时间不断的变化,但始终是围绕当前主题的主旨,如第10个主题,在训练完第1个数据段后包含office,interview等与工作相关的词,训练第5段和第9段后的主题包含的单词也主要是围绕图10nytimes数据集在OBP算法上前25个主题演变过程(横坐标为当前训练集所处理的训练段数;纵坐标为各主题在当前数据段所对应的概率值;右侧为当前主题中概率值较大的部分单词)business浮动.因此LDA模型下的OBP算法可以用于处理数据流数据,并追踪主题的不断变化.表5OBP算法在enron数据流上的主题变化1:enaofficegrouplondonprocessinterviewsallyanalystrole5:teamgroupprocessofficeenasallybusinesssupportforward9:groupteamofficeprocessbusinesssupportenamanagementbusiness_unitprogramforward1:bassericdadrespondweekendlumthinkdprFridaydinner5:bassemployeesdprrespondericlarryweekendFridaythink9:respondemployeesFridaybassericdprweekendthinkopendadfloor1:accountaccesspassworduseronlinepageoasisstatementport-5:pageaccountaccessserviceusercustomeronlinepasswordsite9:accessaccountpagecustomerserviceuseronlinesitestatementfoliovisitstatementschwab1:agreementattachedcostsectionplanloanprogramprovidecopynumber5:agreementplanprovideemployeesattachedissuessectioncostprogramrequired9:agreementplanprovideissuesproposalcostprogramdirectis-suesection在处理流数据时,LDA模型对每个主题会随流数据训练样本的变化而变化.实验中的训练数据是将nytimes数据集切分成10段视为数据流,每段25000篇文本.图10和图11给出了OBP算法的LDA模型训练流数据时,50个主题的演变图.图10给出了前25个主题演变图,图11给出了后25个主题演变图.Page10图11nytimes数据集在OBP算法上后25个主题的演变过程(横坐标为当前训练集所处理的训练段数;纵坐标为各主题在当前数据段所对应的概率值;右侧为当前主题中概率值较大的部分单词)从图10和图11可以看出,主题随训练数据集的增加而不断变化,如图10中的第1个主题演变图,其概率值随训练文本的增多而不断变大,表明该主题得到的关注度正在持续上升;图10的第13个主题对应的概率值随训练文本的增加而先变大后变小,表明该主题被研究的热度不断下降;图10的第9个主题随训练文本的增加,概率值变化比较缓慢,表明该主题被关注的程度几乎保持不变.每个主题演变图反映了主题随训练样本不断增加的变化,结合所有主题图还可以挖掘出哪些主题是当前的热门话题.根据主题演变图能够直接反映出各个主题随时间的变化趋势,追踪与各主题相关的数据信息,所以搜索引擎等可以借助该模型通过对历史数据的不断训练,从而给用户提供更加准确的搜索结果.鉴于本文主题模型上的主题定义为单词表上的概率分布,即每个主题是由单词表中所有单词的不同排列构成,而图10和图11的每个主题右侧列出了该主题对应概率值较大的部分单词,所以对每个主题会存在相同的单词.5结论本文基于LDA模型的因子图提出了在线消息传递(OBP)算法,通过实验验证了OBP算法比OGS和OVB算法有显著提高,且LDA模型下的OBP算法优越于PLSA模型下的OBP算法,并将其应用到主题追踪上,获取了更准确的信息.
