Page1基于粒子滤波的驾驶员视线自动校准算法1)(大连海事大学信息科学技术学院辽宁大连116026)2)(大连理工大学物理与光电工程学院辽宁大连116024)3)(杜兰大学数学系新奥尔良美国70118)4)(哈佛大学医学院波士顿美国02114)摘要虽然驾驶员的注视点定位对研究驾驶行为和提高驾驶安全有重要作用,但是光线不均匀和驾驶员头部的大角度转动造成了传统视线跟踪算法必需进行的校准过程在真实驾驶环境下无法实现.文中根据车辆环境和驾驶员注视行为特点,构建驾驶员注视视线区域和头部姿态的映射关系,以实现视线自动校准.文中利用稳健的面部特征定位及三维几何结构估计头部姿态,利用粒子滤波对头部姿态进行学习和跟踪.文中使用辅助采样的方法解决粒子滤波在给定的状态空间上的粒子贫化和权值分配问题.文中的自动校准算法误差较小,可以满足驾驶安全和驾驶行为研究的需要.关键词视线跟踪;头部姿态;校准;粒子滤波;驾驶行为1引言视线跟踪技术在人机交互、医疗和驾驶行为研究方面有重要的作用.目前成熟的精度较高的商业视线跟踪仪一般会利用腮托或头戴等方法限制头部运动,通过基于红外光线的瞳孔-角膜反射视线跟踪方法获取视线参数.在头部运动受限制的情况下,头部和眼睛位置估计可以得到较好的准确度[1-2].如果不固定头部运动,就需要采用运动传感器或多相机对头部运动进行补偿,但这种视线追踪系统精度不高.真实驾驶环境下的基于视频的视线追踪方法是智能交通领域的研究热点[3-4].但受驾驶环境和自然环境因素影响,传统视线追踪方法几乎不可用.首先,在自然环境下,光线比较复杂,红外光源在户外受阳光影响后不稳定,传统的瞳孔-角膜反射方法可用性差.再次,驾驶员的头部转动幅度很大,转动角度有时大于180°,并且在自然环境下也不利于采用固定或限制头部运动的方法,因此自然场景下的注视点定位问题一直难以实现.其中,视线校准是视线追踪或注视点估计的必要环节,虽然在简单环境下有专门的校准过程,校准的算法也比较多[5],但在复杂场景下由于条件限制,一般很难进行校准.而传统视线跟踪方法必须进行校准,这也说明了如果要实现注视点定位的自动校准就必须采用新的注视点定位方法.对于较大的头部旋转角度,许多研究者通过头部朝向来估计视线方向[6],将视线估计问题转换为头部姿态估计问题.因为人眼的有效视场是有限的,所以正常情况下头部朝向可以代表70%的视线方向,而驾驶员在注视某个对象时总是转动头部面向该对象.本文中也将驾驶员的视线方向用头部的朝向来表示.驾驶员的头部姿态估计方法利用脸部特征点的位置、形状、外观或左右瞳孔的距离等参数估计头部的朝向,头部位置估计通常需要多摄像机和复杂的脸部模型来进行初始化.如Murphy-Chutorian等人[7]提取驾驶员脸部的外观特征,构建梯度方向直方图,通过支持向量回归估计视线方向和头部朝向.Tawari等人[8]则在驾驶视频图像中定位驾驶员的眼角、鼻角等多个特征点位置,使用三维模型分析其几何结构来估计头部姿态.头部姿态的3个自由度如图1所示.同时,交通领域的研究人员利用眼动仪研究汽车驾驶员注视点分布规律等问题,得到了部分驾驶员视觉搜索行为的统计特性.如袁伟等人[9]基于此提出基于马尔可夫链的驾驶员视觉转移特征,运用动态聚类方法,对驾驶员注视点的视野平面解析坐标进行聚类,通过对注视行为平稳分布特征分析得出:长时间驾驶后各区域的注视点分布的概率会趋于稳定,并主要集中在若干个区域.因此可以在驾驶环境中利用头部方向估计代替视线方向估计.根据驾驶习惯和驾驶员头部方向统计结果[10],驾驶员大多数时间都是朝前看,并且以相对固定的头部转动角度观察左右后视镜、车内后视镜和风挡玻璃的不同区域,因此头部方向可以采用追踪和学习的方法被实时估计出来,以实现自动的校准[11].本文算法基于以下两个条件,一是当驾驶员调整好座椅时,驾驶员的头部朝向和后视镜等驾驶环境部件的相对位置是固定的,车内的这些部件可以用来进行视线的校准;二是驾驶员通常具有一定的驾驶习惯,例如如何观看后视镜,打开转向灯前后如何观察后视镜等.本文算法是在文献[11]的方法上进行了改进,文献[11]的主要贡献:一是针对不同的头部姿势和视线方向的自动分类方法提出了一种新的算法;二是将自动人脸检测、样本集学习算法和粒子滤波追踪估计合并为一个循环结构,从而使跟踪系统能够自动进行.本文的改进之处在于:利用较少形变的几何特征定位方法提高头部姿态的精确度,同时使用稳健的稀疏人脸追踪方法加快特征点搜索速度;在状态空间模型中,对头部姿态进行粒子滤波追踪和学习,使用辅助采样的方法解决粒子贫化和权值分配问题.本文不仅侧重于头部姿态的粒子滤波学习,包括了水平偏移度、垂直倾斜度和左右旋转度(即Yaw、Pitch和Roll)共3个自由度;还基于注视区域的聚类分析优化了注视区域的划分,使其更符合自然驾驶状态的注视特性.本文第2节介绍驾驶员视线自动校准算法;第3节主要介绍驾驶员状态模型及粒子滤波算法在头部姿态的学习和估计方法中的应用;第4节给出本Page3文算法的实验结果,并将本文算法与真实数据进行了多组对比;第5节对本文工作进行总结,并指出进一步的工作.2驾驶员视线自动校准算法我们提出的驾驶员视线自动校准算法由3部分构成,包括头部区域确定、基本帧的学习和视线区域的估计,如图2所示.确定头部区域简化了人脸特征提取,再使用基于粒子滤波框架的驾驶员状态模型进行基本帧学习,通过度量不同区域的图像距离,确定头部姿态与视线区域的映射关系,从而实现驾驶员视线的自动校准.其中,视线区域是根据驾驶员注视特性和视觉搜索行为特征预先划定的.首先,在头部区域确定阶段,脸部区域在我们接收的整幅图像中被确定,其他区域即被抑制.受光照及驾驶环境影响,人脸检测效率不高,必要的人脸追踪算法被采用以提高人脸区域确定的速度和准确度.本文采用文献[12]的人脸稀疏追踪结合改进的Viola方法[13]检测驾驶员的脸部,增加了对侧脸等不同姿态人脸的检测,使用追踪结合检测的方法主要是为提高特征点选取的速度和准确度.当系统识别脸部失败,基本帧的学习可能导致最后的相应视线区域的样本集有误,因此在初始化时在固定的时间内接收的图像均进行脸部检测.脸部检测算法在一定时间周期内会自动重复.同时,我们增加遮挡因子及面积因子以降低人脸追踪误差和人脸深度信息缺失带来的影响,两因子均根据统计特性学习得到.对于遮挡因子较大的人脸,即人脸区域被遮挡较多的人脸在基本帧学习时降低其置信水平.对于面积因子较大的人脸,即尺度发生变化的人脸在基本帧学习时暂时不接受.在图像序列中利用改进的文献[14]中的方法提取人脸特征,并自动对感兴趣的特征进行特殊标注.通常情况下,遮挡、光线条件、头部大幅度转动和其他影响因素限制了许多已有算法的应用.混合树模型把人脸每个特征点作为一部分,通过全局的混合捕捉人脸不同姿态下的特征点变化.而AAM(ActiveAppearanceModel)模型易错估人脸整体轮廓,CLM(ConstrainedLocalModel)模型则会牺牲鼻子特征点的精确度.由于本系统可以自动进行鼻子中心点的标注,因此系统不需要对一个指定的人进行特别的训练以获得相应的参数,因此不需要驾驶者的特别合作;另外,算法对于驾驶者戴眼镜和大幅度的移动头部具有很好的鲁棒性.不同驾驶员头部大幅度转动条件下的特征提取过程如图3所示.其中“×”为自动标注的鼻子顶点位置.其次,在基本帧学习阶段,关键帧表示为与我们划分的区域相对应的头部水平偏离度(Yaw)、垂直倾斜度(Pitch)和左右旋转度(Roll).我们通过对驾驶员状态模型进行粒子滤波学习,基于图像上定义的距离把所有的样本分为与事先划定的视线区域相同的集合类,并最终用每个集合的中心来代表相对应的视线区域.驾驶员状态模型及粒子滤波学习算法会在本文第3节详细阐述.学习算法需要在每一时间段结束后进行重复,使得样本集中的元素能够以更高的置信概率定位视线区域.本文采用与文献[15]相似的学习算法的框架.基于对驾驶者大部分时间会正视前方这个假设,我们选取定位鼻子顶点作为兴趣点并进行加权平均,最后得到的值被认为驾驶者直视前方的中心坐标,取为原点,同时设定相应的Page4视线区域编号.中心的选取需要在特定的时间周期内进行重复使得其可信度增高.视线区域的头部姿态角度关系在系统初始化之前需根据头部传感器给出的数据进行校准对应.最后,在视线区域估计阶段,驾驶员头部的中心由事先确定的校准点进行加权平均算出,转化为相对应的头部姿态,最后在集合类中选取距离最近的集合指标作为其估计的视线区域.头部姿态由兴趣点在三维空间中根据几何关系计算角度得到.并用此基于粒子滤波算法的估计结果在图像的估计状态和样本学习结果的距离空间度量视线区域结果.其中,我们加入了估计的置信映射来帮助我们了解估计的准确性.关键帧的集合为犕={犕j|1jk},其中k是关键帧的数量,这一数量由事先确定的视线区域决定.为了建立最优的基本帧的集合,在每一帧中的图像元素都会被分析,进而找到其相对应的置信概率最高的视线区域.当我们确定样本集后,新的图像帧就可以与样本集的中心进行比较,这里我们用条件分布p(犕|犕j)来表示在新的图像帧的状态已知的情况下推测其属于样本j的概率,在求出相应的离散分布后我们选择概率最大的样本的指数作为当下驾驶者的视线区域号.为了计算简便,样本集的基数会尽可能地小.此外,头部姿态的估计模型还保留了随机变量的相关矩阵Λx=E(犕-μ)(犕-μ),此为头部姿态的协方差矩阵,并以此为依据估计残差.3粒子滤波学习过程粒子滤波方法或序贯蒙特卡罗方法是一组通过直接实现贝叶斯递归方程来估计状态空间后验密度的在线后验密度估计算法.它提供了一种产生样本的行之有效的方法,且无需假设状态空间模型或状态分布.状态空间模型可以是非线性的,初始状态和噪声分布可以采取任何形式要求.概率分布用一组粒子的集合来表示,通过寻找一组在状态空间中传播的随机样本来近似地表示概率密度函数,获得系统状态的最小方差估计.我们假定驾驶员头部为刚性物体,可以通过帧与帧之前的特征点跟踪计算运动参数,即头部姿态参数.将人脸单一特征点映射至三维图像空间,利用粒子滤波的追踪方法估计和学习驾驶员头部姿态,头部姿态角度由三维空间几何关系计算得到.与通常在三维空间内进行所有可能的全局校准不同,我们在目标平面自动锁定事先选择的已知点,然后对驾驶员移动头部位置的情况进行已知点的匹配和插值.目标的识别通过基于动态的重要性采样和重采样过程的算法来实现.在最初校准的过程中,我们可以通过标准化的模式平移向量来帮助在新的图像中定位已知的校准点.因此,当前帧的头部姿态可以基于这些校准点集合加上系统刚开始进行时所施加的学习算法-粒子滤波算法更加准确的估算出来.3.1状态空间模型在状态空间模型中考虑非线性非高斯的动力学模型,其中犡1:t={犡1,犡2,…,犡t}作为感兴趣的不可观测状态随时间演变,而在连续的时间点上也有部分可观测状态犢1:t={犢1,犢2,…,犢t}.同时,假定状态序列为马尔科夫链.在此情况下,与文献[16]类似,状态空间模型可以表示时间序列的过程,其主要构成为:状态转换模型:犡t=Ft(犡t-1,犝t)ft(犡t|犡t-1观测模型:犢t=Gt(犡t,犞t)gt(犢t|犡t我们定义驾驶员头部姿态状态犡t=[犃t狏t]T,其中犃t=(Tx,Ty,Tz,αt,βt,γt)是一个六维向量,由平面坐标的x,y,z轴的位移和头部的水平偏离度αt、垂直倾斜度βt、左右旋转度γt组成.这里狏t为线速度和角速度组成的向量.在对头部运动进行追踪时,我们假定驾驶者的视线在大多数时间专注于一个方向,这与真实驾驶行为一致,故我们在初始化时锁定这一视线区域的参数.当驾驶者的头部有很大偏移时,我们的模型能计算驾驶者当前的头部中心位置和实现锁定的视线区域,进而基于事先设定的视线区域标号对当前的视线区域进行最近距离的估计.我们用文献[7]中提到的混合状态犡(1-τt)犡其中τt为二值随机变量,取值范围0和1,且犡10()00犡态,犡(2)t=里我们忽略了对加速度的考虑,这是因为头部在转动的时候更趋向于常速度连续的转动.其中,狌t是微小的头部瞬间转动.因此其实现为犢t=犜∈犡t+犞t,犜∈是两个空间的转换,犞t是t时刻的噪声.τt的分布基于物体的转动速度,当τt=1时满足d(犢t,犢t-1)>ε1,d(犃t,犃t-1)>ε2.其中ε1,ε2为转动速度阈值,即两个Page5时间点脸部特征的移动超过这个值我们认为头部在转动,否则τt=0.3.2粒子滤波算法接下来我们介绍适用于时间序列模型即状态空间模型的粒子滤波算法.在贝叶斯框架中,依赖后验概率密度进行推理,状态犡t和观测犢t的联合概率密度为p0:T,0:T(犡0:T,犢0:T)=p0(犡0)g(犢0|犡0)×∏其中p0是犡0的初始概率密度.因此,依据状态转换模型和观测模型,状态和数据从状态空间模型中随机采样的过程中,样本路径(狓~0~p0(犡0),其余狓~狓~(狔~0,…,狔~gt(狔t|犡t=狓~在粒子滤波算法中,对转换密度和观测密度函数的分布估计是至关重要的.事实上对于当前状态分布p(犡)我们不知道其准确的趋势,故用任一经标准化的重要性分布q(犡)替代,然后根据前一时刻的状态和观测值对当前的样本数据赋予权重.对第i个样本权重wp(犡t=狓t|犢1:t=狔1:t)=σt|0:t-1=∫则有由于我们基本不可能依据密度函数p0:t|0:t(犡0:t│犢0:t)进行取样,故我们根据概率密度q(0:t)(犡0:t│犢0:t)选取N个样本狓~(i)这些权重经过标准化被映射到[0,1]区间,其中(i)t=wj=1∑q0:t(犡0:t=狓0:t|犢0:t=狔0:t)=q0:t-1(狓0:t-1|狔0:t-1我们找到了ww~(i)t==w~(i)=cw其中c为常数,i=1,2,…,N.值σt|0:t-1(狔t|狔0:t-1)由于会最终进行标准化所以在计算中不起作用.由于时间过长后权重的作用会渐渐失效,故我们要在每一步计算完权重后进行重采样.粒子滤波方法用于原则上提升时间点t的新状态的概率分布,我们在标准重采样过程上进行了改进,使用辅助采样的方法使得在下一时刻有利于粒子的存活.首先,假设在时间点t-1联合后验概率函数可以很好的被^p0:t-1|0:t-1(d狓0:t)=∑致的近似,其中δ狓间点t的联合后验概率可以近似为p0:t|0:t(d狓0:t|狔0:t)≈Nw1Z∑其中归一化因子Z定义为i=1因此新粒子狓qt(d狓0:t)=q0:t-1(d狓0:t-1|狔0:t)qt(d狓t|狓t-1,狔t)(=∑Ni=1Ni=1其中∑过程狓0:t-1过去路径的边缘分布q0:t-1和新状态狓t的条件分布qt.第一部分在老粒子路径{狓散分布.权重函数v可以预先选择适应性好的粒子.取状态某一点的值μ均,计算权值函数作为该点的似然函数,vg(狔t|μ(i)vt-1=w点t的滤波,从边缘条件分布p0:t-1|0:t可以得到过去的路径狓0:t-1,使用时间点t-1的粒子近似可以得到p0:t-1|0:t(d狓0:t-1|狔0:t)∝∫p0:t-1|0:t-1(d狓0:t-1|狔0:t-1)f(狓t|狓t-1)g(狔t|狓t)d狓tNw≈∑i=1Page6在蒙特卡罗方法中,使用粗略的近似函数来处理,令f(d狓t|狓p0:t-1|0:t(d狓0:t-1|狔0:t)≈∑我们使用上述机制定义粒子的广义重要性比率,即与标准的序贯重要性采样相比,我们的采样方法在计算偏置时修正重要权值等于1/v一层权值为比率w采样过程中,可以利用原来时刻预测的似然度大的粒子扩展到该时刻,增加了粒子的多样性的同时减少了重要性权值的方差,可以产生更精确的估计.我们能够基于这个状态模型和观测密度函数gt(犢t|犡t)∝(1/B)exp(-λd(犢t,犜∈犡t+犞t))来估计转移密度函数ft(犡t|犡t-1),其中B为标准化常数.利用粒子滤波方法估计当前的状态犡t,其由样本点的加权平均值犡^粒子滤波方法作为一种基于贝叶斯估计的非线性滤波算法,在处理非高斯非线性时变系统的参数估计和状态滤波问题方面有独到的优势.由上述方法可知,在系统运行之前,设定所有要求的参数,则系统可以通过粒子滤波对头部姿态的追踪和估计,实现视线区域的自动校准,完全不需要驾驶人特殊的合作.4实验结果4.1实验环境及数据来源在系统初始化前,我们用驾驶员头部运动传感器实时记录头部姿态的水平偏离度、垂直倾斜度和左右旋转度,并与设定好的视线区域编号进行对应,确定相应的头部姿态角度和区域编号的映射.具体的注视区域划分详见4.2节.然后使用摄像头实时记录驾驶员的图像,对每帧图像进行人脸检测和追踪,提取感兴趣的特征点,并估计驾驶员的头部姿态角度.为保证头部姿态角度的正确性,会使用部分数据初始化头部姿态零点.最后,施加样本集学习和粒子滤波算法对头部姿态更新,根据置信概率确定状态可信性,并输出当前的注视视线区域,完成对注视视线的校准.为了验证本文的自动校准算法效果,我们采集了真实环境的图像数据进行测试实验.其中,实验环境为IntelCorei52.67GHzCPU和4GBRAM的电脑.图像数据来源于同一个USB摄像头,图像分辨率为640×480像素.摄像头固定在靠近方向盘的仪表盘中心,距驾驶员有一定角度.我们在捕捉驾驶者脸部特征的摄像头里加装了能够同时适应白天和黑夜的红外光源及光照转换装置,增强实验装置抗弱光线条件的鲁棒性.驾驶者测试时头戴TrivisioColibri惯性位置追踪器,以接收头部姿态的实际数据.我们随机选取8个个体,共收集16万帧的连续图像数据,来测试我们提出的算法适应不同个体的驾驶习惯的能力.其中,部分被试驾驶员情况信息如表1所示.由于不同的驾驶员的视觉搜索模式(注视持续时间、注视点分布、注视视角等)具有差异性,因此表中列出驾驶员的性别、年龄、身高、驾龄等情况,并在之后的部分对驾驶员的误差进行了对比.我们将驾驶员划分为熟练驾驶员和非熟练驾驶员:其中熟练驾驶员为驾龄5年及以上的驾驶员,即表1中前4位驾驶员;而非熟练驾驶员则为驾龄5年以下的驾驶员,即表1中后4位驾驶员.编号0010020030040050060070084.2视线区域的表示在一般情况下,驾驶员以头部中心为轴,转向其注视的物体.因此,驾驶者头部姿态决定了视线区域.我们根据驾驶环境及注视习惯划分了不同的视线区域.本文所划分的9个视线区域参照了文献[11]中所描述的方法.文献[11]中的部分视线区域在正常行驶过程中注视概率几乎为零,并且不同驾驶员朝向后视镜的注视概率并不稳定,因此我们使用K-mean聚类的方法将原有12个视线区域优化为9个视线区域.如图4所示,视线区域的划分是基于真实的驾驶环境.由于朝向后视窗头部特征几乎无法提取,我们采用带有倒车影像的车辆规避倒车时头部转向.因此中央控制台区域的注视频率实际也包含了朝向后视窗的注视.在朝向风挡玻璃即区域3、区域5及Page7区域6时,驾驶员通常会结合头部转动和眼球运动,但只对同一视野的近距离、中距离和远距离的车前状况,有较多的眼球运动观察.4.3视线区域的估计视线区域的估计步骤需在平面上水平偏离度、垂直倾斜度和左右旋转度零点位置已知的情况下进行初始化.通常我们认为直视视线区域6的中心位置为零点.在此实验中,我们用500帧的图像初始化零点,进而认为其欧拉角为零(水平偏离度、垂直倾斜度和左右旋转度均为零).犚(θ)=(α,β,γ)表示在XYZ坐标系下的欧拉角,其中α为水平偏离度、β为垂直倾斜度,γ为左右旋转度,如图5所示.这里使用单目摄像头的深度估计建立鼻子顶点P到坐标轴中心点O的位移和偏移欧拉角.其中坐标轴中心点即头部中心点.之后,我们对图像序列进行训练以寻找基本帧,进而在形成样本集之后我们能用样本集的元素来估计当前驾驶者所处的状态,进而求出其视线区域.当学习算法对50000帧的图像经过两轮学习过后,我们得到表2,即每个视线区域中心的水平偏离度、垂直倾斜度与左右旋转度.当追踪器得到每一帧图像后,算法会针对当前状态在样本空间生成概率分布,最后概率值最大的样本集指标为驾驶者当前视线区域的标号.对于同一个图像提取装置和同一段图像序列,由于我们采取了随机结构用于驾驶者脸部的识别,视线区域123456789故我们重复实验所得到的数据会有区别于表2,但不影响对视线区域的估计结果.我们对全部图像帧进行水平偏离度、垂直倾斜度和左右旋转度的计算后发现,其平均值最后稳定到0.024、-0.005和0.013.这表示驾驶者大部分时间都注视前方,若与此结果相悖,则很有可能在这段时间驾驶者处于比较疲劳的阶段,则能够通过此结果对驾驶员进行警示或者采取刹车处理.在真实驾驶环境中,部分驾驶员的置信概率平均值曲线如图6所示.基于当前状态的置信评估来判断头部姿态估计的准确性,初期置信概率较不稳定,随着帧数的增加,置信概率的平均值逐步上升,这说明对头部姿态的估计数值逐渐更为准确.以001驾驶员为例,在真实驾驶环境中,我们得到至9000帧时,置信概率的平均值为0.6743,标准差为0.0963,基本趋于稳定.而其他驾驶员的置信概率也分别稳定至0.6682和0.7052之前.除此之外,式(1)中的τt需进行准确取值,在我们的实验中其当前值的确定依赖于之前一个时刻头部摆动速度的值,τt=1必有vt超出了我们事先设定Page8的阈值.经过对初始化前传感器的样本进行统计我们得到平均角速度大约为1.5°,故我们取1.4°为阈值,这是因为在统计过程中计算的方差较大,取这个值可以大致符合实际情况.此阈值会在对头部姿态判断的过程中进行不断修改使得其相对准确.4.4视线区域注视的频率统计我们对所有数据帧进行视线区域注视概率的统计得到了表3,其中对于熟练驾驶员和非熟练驾驶员分别进行了统计.视线区域由表3可知,区域6即正前方区域最频繁被使用.除了区域6以外,区域1、区域2及区域4也经常是驾驶员注视的目标,即左视镜,右视镜和后视镜,这符合大多数人的开车习惯.非熟练驾驶员比熟练驾驶员更注意区域1、区域2及区域4,即左视镜,右视镜和后视镜.而熟练驾驶员对前方的区域注视频率比非熟练驾驶员更为平均一些.4.5与标准数据进行对比我们对每个被试驾驶员所收集的数据帧中选取帧500到帧7500进行测试,并对比我们的方法估计的头部姿态和头部运动传感器的真实头部姿态,这时估计值是乘以扩张系数的最终值.我们最终得到的水平偏离度、垂直倾斜度和左右旋转度的误差曲线如图7和图8所示.由于有预先的视线区域的划分,在本文中我们设定水平偏移角和垂直倾斜角的容忍误差可接收率由划分的视线区域块的最小块决定,其中水平偏离度2°,垂直倾斜度和左右旋转度表4绝对误差的统计特性编号001002003004005006007008均为1.5°.由图7和图8所示,水平偏移角和垂直倾斜角的平均绝对误差曲线是收敛的,并且极限值均在1.5°左右,数值能够保证大部分时间内视线区域的准确性.当然对于倾斜度而言,估计误差稍大.这些数据保证了在长时间驾驶中此自动校准算法能够对驾驶人的视线区域有相当准确的判断.综合图7、图8和表4结果可知,熟练驾驶员的绝对平均误差曲线收敛速度略快于非熟练驾驶员.绝对误差方差1.2621.1341.2121.2361.3981.3231.4011.365Page9而非熟练驾驶员的视线区域的可接收率略小于熟练驾驶员.但总体而言,所有被试驾驶员的平均绝对误差相差并不大,并且误差曲线均收敛,同时绝对误差方差较小,而且具有较高的可接收率.因此,该结果表明,本文算法受不同个体驾驶习惯影响较小.4.6自动校准算法的应用我们的方法无需驾驶员主动配合,便可自动校准驾驶员的注视视线区域,并稳定的输出驾驶员的头部姿态角度和当前注视.由结果可知,基于头部姿态的注视视线估计的误差低,可用性高.同时,关联头部姿态和注视视线区域的自动校准方法受不同驾驶员及驾驶习惯的影响小.通过追踪和估计驾驶员的注视视线及视线区域,可以分析驾驶员的注视持续时间、注视点分布、注视视角等特性,实时掌握驾驶员的状况,感知驾驶员对路况及交通信息的注意程度,以便搭建新型的车联网智能交通安全驾驶系统.5结论本文提出了一个在真实驾驶环境中对驾驶员视线方向进行自动校准和估计的算法.系统的运行基于嵌入的样本集学习算法和粒子滤波算法.两个算法融合在一起可以估计驾驶员头部运动,也可以在其他方面有广泛应用.这种算法的优势在于其能够使系统进行自动估计而不需要驾驶员的合作,系统运行之前会有初始化的过程,进行标准数据预设参数和视线区域的划分.在算法执行之前我们加入脸部识别算法以确定头部姿态零点,利用稳定的人脸特征提取方法提取特征.之后,学习算法在图像中对不同的视线区域选取基本帧构成不同的集合类,即视线区域的商集.进而对粒子滤波算法提供基本的样本集,经过粒子滤波算法对当前驾驶员的状态进行取样,计算加权平均值,我们得到估计的水平偏离度、垂直倾斜度和左右旋转度,进而构造其在样本集空间的条件概率分布,选取概率最大的样本指标作为视线区域的编号.本文算法能够在连续时间内自动地对驾驶员的视线区域进行估计,因此,此算法具有自适应性,可以嵌入到硬件设备中,自动对驾驶员视线校准并对注视区域进行准确估计.
