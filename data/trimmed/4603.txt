Page1基于Chord的多租户索引机制研究邹立达李庆忠孔兰菊王振坤(山东大学计算机科学与技术学院济南250101)(山东省软件工程重点实验室济南250101)摘要多租户数据管理是支持SaaS应用高效运行的重要组成部分.随着租户规模的不断扩大,多租户数据库需要云计算环境下的每个节点都存储并处理租户数据,因此适合利用对等结构(P2P)组织管理多租户数据.组织良好、易扩展的云中多租户索引机制是高效查询的关键.文中基于P2P结构对多租户索引机制展开研究,针对不同租户的索引易相互干扰、租户数据分布无序的问题,通过对Chord的映射方法进行改进将所有租户索引统一映射到一个标识符空间,给出的映射函数使单个租户索引可以隔离、保序地在空间分布.同时设计了各节点所负责的标识符空间范围的分配算法,使租户索引及数据可均衡、聚集地放置在各节点,从而使查询时既能通过对等结构避免性能瓶颈,也降低了数据传输成本.文中给出了多租户索引机制的动态维护策略,提出的标识符空间增倍方法使该索引机制能够适应租户数量与索引不断增加的应用场景.实验结果表明,该机制在租户规模较大时,与集中式索引相比查询时间至少可以节省50%,吞吐量提高1.5倍.关键词多租户索引;P2P;Chord;多租户数据库;标识符空间;云计算1引言随着SaaS(SoftwareasaService)[1-2]应用规模的不断扩大,多租户数据管理成为SaaS应用快速开发和高效运行的重要基础.数据处理资源被多个租户共享使用是多租户数据库的重要特征,对于共享方案,Chong等人[3]提出了3种解决思路:独立数据库,即为每个租户建立一个数据库实例;共享数据库独立模式,即所有租户共享一个数据库实例,但每个租户拥有个性化定制的模式;共享数据库共享模式(又称共享模式),即所有租户共享一个数据库实例及同一个数据模式,同时在表中通过租户标识区分数据所属的租户,此种方案的共享程度最高.对于共享模式,Weissman等人[1]提出了通用表共享方案,即将所有租户的业务数据存储到一个通用表中;Aulbach等人[4]提出了ChunkFolding方案,即把租户的数据表垂直划分为多个块,将它们折叠存储至不同的Chunk表中;Ni等人[5]则将租户共享的、关系依赖的属性存储在基表,其他的属性则存储在辅助表.Aulbach等人[4,6]证明共享模式的系统性能最优,因此主流的多租户数据库多采用共享模式.本文主要对共享模式多租户数据库的索引机制展开研究.多租户数据库有租户数量多、单个租户数据量小但数据总量大的特点,集中式的数据处理结构易造成性能瓶颈[7],因此需要云计算环境下的每个节点都能存储并处理数据,适合利用对等结构(P2P)组织管理多租户数据.在云计算环境下为多租户数据建立良好的索引机制是多租户数据高效查询的关键.目前,云中多租户索引主要存在以下两个问题:首先,在创建与维护索引时租户间存在相互干扰.即在共享模式下,一个租户在某个属性上建立索引,会导致其他租户也在该属性上被动建立索引,即使其他租户没有索引需求.其次,现有的多租户索引多以集中式索引为主,没有考虑云计算环境中多租户数据总量大的场景,易导致过多的查询集中在入口节点,使得查询性能下降.P2P协议[8]能较好地提供性能平衡与扩展的功能,适合利用P2P结构建立多租户数据索引.在P2P协议中,Chord[9]协议能以较低的路由表维护代价提供高效的查询跳转效率,且索引映射结构简单易于改造,适用于多租户场景.然而,基于Chord协议建立的多租户索引还存在以下挑战:(1)索引分布的无序性增加了租户查询处理的数据传输成本.多租户场景中每个租户的数据量小,且租户的查询具有隔离性,即只查询属于自己的索引与数据.若一个租户的索引与数据只聚集地放置于一个或者少量节点,则在查询处理时,可避免或减少跨节点的数据传输.然而,Chord协议采用随机方法对索引进行标识符空间映射,属于一个租户的索引及数据散乱地分布在较多节点中,因此需要在节点间传输大量数据才能完成租户的查询处理功能;(2)无法满足多租户数据索引对扩展性的需求.随着多租户数据库投入使用,租户数量与数据量会以较快的速度增加[10].Chord协议无法保证新增的租户与原有租户隔离的分布,并且无法保证一个租户新增的索引放置到其租户数据所在的节点,因此会降低数据查询效率.针对以上问题,需通过对Chord协议进行改进建立多租户索引机制,使其适用于多租户数据的特点,提高数据查询效率.本文通过改进Chord映射方法对多租户索引进行组织管理,使建立的索引机制满足多租户数据库对隔离性、扩展性的需求.本文通过将Chord标识符空间划分为若干个大小相同的区域,使每个区域对应一个租户,实现租户索引的隔离管理;设计标示符空间的分配算法,确定各节点所负责的空间范围,使单个租户的数据与索引在节点中聚集地分布与存储,以减少查询时数据传输的成本;给出多租户索引机制动态维护策略,使其适应租户与索引数量不断增加的应用场景.由此本文建立了一个支持P2P结构的多租户索引机制,避免出现性能瓶颈的同时提供动态扩展性能,并通过实验验证了该机制在较大租户规模时查询效率、查询吞吐率有较好的表现.本文第2节讨论多租户索引的相关研究工作及存在问题;第3节给出多租户索引机制的框架及相关定义;第4节改进Chord映射函数,并讨论索引条目在各节点的分布与组织;第5节给出索引机制的动态扩展方法;第6节对多租户索引机制进行实验验证;第7节给出本文的结论.2相关研究对于多租户数据共享模式存储,索引管理是实现其高效查询的重要环节.直接在共享模式存储的多租户数据上统一建立索引,将使得所有租户数据一起被索引,因此对于特定租户来说,在索引维护与查询时会产生严重的性能问题.文献[1,6,11-12]针对多租户索引隔离问题展开了研究.Weissman等Page3人[1]在其平台Force.com中使用一系列的数据透视表(PivotTable)来存储租户的索引,有效地支撑了租户数据的高速访问,但随着租户数据量的增加,透视表占用的存储空间会暴涨,从而导致系统效率降低.M-Store[12]针对多租户共享索引查询效率不高的问题,提出了多分离索引技术.其为不同租户单独建立索引,提供了索引可定制功能,但随着租户数量与索引数的增加,数据引擎则需管理大量的索引实例.Aulbach等人[6,11]提出多种多租户数据共享存储模式,并介绍了ChunkTable与宽表租户索引建立方法,其根据租户是否有建立索引需求将租户数据分为两类,分别存储在两个Table中,在其中一个Table建立索引,另外一个则不建立索引,以较低的资源成本为租户索引提供了隔离性.以上文献针对索引隔离问题给出了解决方案,但都没有考虑云计算环境的多租户索引结构,而大量的多租户数据需要在云中管理,并建立可扩展、支持大量并发查询的索引机制.目前,根据组织方式云中的索引可分成两类:集中式结构和P2P结构.集中式的索引方式[13-14]以一个节点作为查询入口,极易造成性能瓶颈[7].P2P[15-17]结构的索引采用节点对等工作模式平衡负载,为解决性能瓶颈问题提供了一种途径,而且自组织的P2P结构允许节点自由加入与离开,为云中多租户数据索引管理提供了一种理论上可无限扩展的组织模型.文献[18-19]基于P2P结构建立了可扩展和高吞吐量的索引机制,为云中的多租户索引提供了借鉴.Wu等人[18]提出了CG-index(CloudGlobalindex),即所有节点组成一个覆盖网络,每个节点上建立局部索引并选择索引信息发布到覆盖网络上,其根据查询模式自适应地选择可发布的B树节点,提高了查询效率.Chen等人[19]提出了一种针对DaaS的云中类关系(DBMS-like)模型的索引机制,将索引保序地发布到P2P网络中,以支持范围查询与比较操作,为云中关系查询处理的索引建立提供了基础.但是,在多租户环境中直接应用这些云索引技术既不能提供索引隔离的功能,也不能满足租户数量与数据量扩展的需求.常见的P2P索引协议有CAN[20]、BATON[21]、[22]、Chord[9]等.相比其他P2P索引协议,PastryChord能以较低路由表存储代价提供高效的查询跳转效率(O(logN)),且映射结构简单、易于改造与维护,适用于多租户场景.因此,本文基于Chord协议建立多租户索引机制.然而,直接使用Chord协议[9]建立索引机制无法满足多租户数据库对索引的特殊需求:首先,由于Chord协议采用随机方法对关键字位置及节点所负责的标识符空间范围进行映射,无法满足多租户数据库对索引在节点中隔离、聚集分布的需求.隔离分布是指一个租户使用索引查询时,只能检索到自己的数据,而随机映射的索引条目使得租户在查询时,特别是在范围查询时,租户获得自己数据的同时也获得了其他租户的数据.聚集分布是指一个租户的索引与数据尽可能分布于尽可能少的节点中,从而减少跨节点查询次数,而随机映射方法使得一个租户的索引分布于较多节点.其次,多租户索引机制需要能动态地扩展.多租户数量是动态增加的,由于Chord协议标识符空间的大小初始化后不能再更改,新增租户映射的位置会与原有租户产生冲突,破坏了租户间的隔离性.除以上特殊需求之外,与单租户场景相同,多租户数据库也需要索引在标识符空间保序地映射及在节点间均衡地分布,而Chord协议[9]并不能较好地支持该需求.因此需对Chord协议进行改进,使得建立的多租户索引在支持保序、均衡分布的前提下,实现各租户索引隔离、聚集地分布,并具有可扩展性.3云计算环境下的多租户索引框架Chord协议[9]可提供一种能在P2P中快速定位资源的方法,基本思想是将代表资源信息的关键字和服务器节点统一映射到一个标识符空间,并通过标识符空间定位资源位置.Chord采用SHA-1[23]将资源信息及节点随机映射到标识符空间.然而,随机映射的方法可能将不同租户的索引条目映射到标识符空间的相同位置且不保证原有条目的顺序,所以不能适用于多租户数据索引隔离、保序的场景.因此本文提出一种通过改进Chord映射方法的多租户索引机制(Multi-tenantIndexingMechanismBasedonImprovedChordMappingApproach,MIMC)为多租户提供索引管理功能.MIMC的组织结构如图1所示:多租户数据库的索引分布在各个局部节点中,且索引与其指向的数据放置在同一个局部节点;每个局部节点维护一个路由表,并利用B+树建立本节点的局部索引;所有节点基于Chord协议形成多租户数据的云中索引机制;在查询时,先通过路由表找到索引条目所在节点,再通过局部索引得到租户数据.Page4图1云中的多租户索引机制为方便描述,相关定义如下.定义1.租户.租用多租户应用的组织或用户称为租户.用tni表示,i为租户编号,0<iI,I为租户数量.定义2.数据对象.用于描述tni某一业务对象的一条数据记录称为数据对象,表示为rdj.用于描述相同业务对象的数据记录集合称为数据对象集合,表示为Dk=∪{rdj},k为一个租户数据对象集合的编号.定义3.多租户数据库.存储与处理租户数据的系统称为多租户数据库.多租户数据库以共享的方式被租户使用,数据对象集合以共享模式的方式存储,通过定制可满足租户对模式的不同需求,租户通过应用访问多租户数据库.定义4.索引条目.一个租户的索引可索引基本存储元素称为索引条目.索引条目是以键值对形式进行管理的,用er:〈k,v〉表示一个索引条目,r是索引条目的标识,k表示索引列的属性值,v是索引条目指向一个或多个数据对象的物理地址.定义5.数据对象索引.针对一个租户tni数据对象集合的某一属性,为加快数据对象集合查询速度而建立的索引条目集合称为数据对象索引,表示为i,j.一个数据对象集合可有多个数据对象索引,即对于一个数据对象集合,可根据租户的需求在其不同属性上建立若干个数据对象索引.数据对象索引表示为为tni建立的第j个数据对象索引,j以自然数编号.i,j是若干索引条目组成的集合,即定义6.租户索引集.针对一个租户建立的数据对象索引集合称为租户索引集.tni的租户索引集表示为数量.多租户数据库中所有的租户索引集合称为索引集,用II表示,那么II={∪定义7.标识符空间(IdentifierSpace).标识符空间是一致性哈希值[23]的范围,表示为S,S∈{[0,2m-1]|m标识符空间的位数}.S的大小表示为S,则S=2m.对于任意一个er,利用映射函数F可将k映射到S中.设k映射到S的值表示为ks,即ks=F(k).通过定义映射函数可使所有的索引条目与局部节点通过哈希函数都映射到S,从而得到局部节点与所负责管理的索引条目的对应关系.所有的局部节点形成一个覆盖网络,因此S的每个值都被分配到某个局部节点中,那么索引条目则根据ks的值分配到对应的局部节点.每个局部节点负责一部分标识符空间,且所负责的标识符空间不重叠.用Rh=[la,lb)表示局部节点nh负责标识符空间范围,那么S={∪Rh|nh∈Rg.la>Rh.lb或Rh.la>Rg.lb.局部节点只对落入自己负责的标识符空间的索引条目建立局部索引.定义8.路由表.每个节点存储一个路由表,路由表由局部节点与其所负责的标识符空间范围的对应关系的记录组成.对于nh,路由表有m(与标识符空间的位数相同)个路由条目,其第i个路由条目为〈xs,nx〉,xs是S的一个位置且xs=(la+2i-1)mod2m,nx是负责xs的局部节点.Page5本文对索引集II的索引条目统一管理.在初始化时,首先在云计算层基于Chord协议组织将索引条目及相关数据对象集合分布到各局部节点中;其次局部节点用B+树组织所负责的索引条目,并管理路由表.在数据查询时,给定查询k值,通过F计算ks,查找路由表,可找到负责管理任意er的nh,并在局部节点得到v,从而定位所需数据.本文建立的支持P2P多租户索引机制以索引条目为粒度进行管理.首先,研究索引条目在局部节点中的分布及局部的组织方式.主要通过设计租户索引条目在S的映射函数确定索引条目在节点中的分布,使租户的索引条目隔离、聚集、均衡的分布与存储.其次,针对新增租户与索引的标识符空间耗尽问题,本文研究了索引条目动态映射方法.采用倍增S的方法,将新租户的索引条目映射到新增的空间中去,使其适应租户数量与索引不断增加的应用场景.4索引条目的组织本文利用一个统一的映射方案对多租户数据库中所有的索引条目进行组织.对于云中的多租户索引,需先将索引条目按照一定规则分布到各节点中,图2标识符空间的划分现给出映射函数F:F(k)=STN×(i-1)+SIN×(j-1)+SIN×(k-mini,j)/(maxi,j+1-mini,j)(1)式中STN是STN的空间大小,STN=S/MaxTN,MaxTN是在S中可提供索引管理服务的租户数量.再在局部节点中组织存储所负责的索引条目.本小节首先给出索引条目在标识符空间S中具体的映射函数,此函数可保证索引条目的顺序不变且各租户索引相互隔离;其次给出各节点负责S范围的分配算法,此算法可保证一个租户的索引条目分布相对聚集且各节点分配索引条目的数量分布均衡;最后给出索引条目在局部节点的组织方法,该方法将索引条目及其所索引的数据存储在同节点,提高了索引效率.4.1索引条目在标识符空间映射在基于Chord多租户索引机制中,所有索引条目都映射到一个S中,每个索引条目在S的位置是通过映射函数给出的.Chord使用的SHA-1哈希函数不能满足多租户索引保序、隔离的映射,所以重新设计索引条目的映射函数.本文设计映射函数F的思想如图2所示.首先,将标识符空间S划分为若干小区域,每个区域分配给一个租户索引集使用,称为租户区域,表示为STN.其次,将每个租户区域进一步划分为若干更小的区域,称为数据对象索引区域,表示为SIN.一个SIN分配给一个数据对象索引使用.再次,将属于一个数据对象索引的索引条目按照顺序依次映射到SIN的位置中.SIN是SIN空间大小,SIN=STN/MaxIN,MaxIN是系统为每个租户可建立数据对象索引的最大数.MaxTN、MaxIN与S值的详细设置见第6节.由于k是某一数据对象的属性值,那么可知该数据对象所定义的数据类型的取值范围,函数(1)中mini,j与maxi,j是k所属数据类型可取值范围的最小值与最大值.比Page6如,若k的数据类型为MYSQL的BigInt,mini,j与maxi,j则分别为0与264-1.函数(1)给出k是数值型时的映射函数,k也可以是字符串型、日期型、多键值索引等多种数据类型.若为字符串型可利用可变长度文本串哈希[24]进行映射,若需支持范围查询则使用位置敏感哈希[25]映射,k如果是多键值索引,可以利用Z-Curve[19,26]或者Hilbert-Curve[27]的方法将多维键值映射到一维空间.由于所有数据对象索引都在相同大小的SIN中进行映射,对于取值范围较大的数据类型,即当maxi,j-mini,j>SIN时,不同k值的索引条目可能映射到S中的相同位置,从而产生哈希冲突.可采用链地址法解决冲突,链地址的数据结构可以是线性列表或者时间复杂度为O(logn)的自动平衡树[28].函数(1)应能保证索引条目的位置性与隔离性.以下给出相关定义.定义9.位置性.位置性是指属于同一数据对象索引保持原有顺序,即ea:〈ka,va〉∈i,j,eb:〈kb,vb〉∈i,j,ifka<kbF(ka)<F(kb).定义10.隔离性.隔离性是指属于不同er不会映射到S中的同一个位置.也可表述为一个数据对象索引在空间S中的分布有着自己独立的空间,与其他数据对象索引的映射空间相互隔离.形式化表示为ea:〈ka,va〉∈i,j,eb:〈kb,vb〉∈k,l,ifi,j≠k,lF(ka)≠F(kb).定理1.映射函数(1)具有位置性与隔离性.证明.根据函数(1),在同一个SIN,映射函数是单调递增的,所以保证了er的位置性.对于隔离性,当i=k时,不失一般性,设l>j,推出F(kb)-F(ka)=(l-j)×SIN+SIN×SIN×SIN-SIN×当i≠k时,不失一般性,设k>i,推出F(kb)-F(ka)=(k-i)STN+(l-j)SIN+SIN×kb-maxb(maxb+1-minb)-SIN×STN-(MaxTN-1)SIN-SIN×SIN-SIN×所以F(kb)≠F(ka).每个户可以独立地建立与定制数据对象索引,不影响其他的租户使用,从而保证了数据对象索引的隔离.同时,函数(1)的位置性可实现多租户数据的连接查询、范围查询等带有比较操作的查询处理.新增索引条目可利用函数(1)动态插入,并保持位置性与隔离性.4.2索引条目在局部节点的分布多租户数据库的索引条目及数据对象需分布到各局部节点,并在局部节点存储租户数据且建立局部索引.首先,根据一定规则确定每个局部节点所负责的索引条目,该内容将在本小节论述,其次,每个局部节点先存储所负责的索引条目对应的数据对象集合,然后建立B+树组织管理所负责的索引条目,此内容在4.3节论述.根据Chord协议,一个局部节点通过哈希函数随机的分配到一部分标识符空间,通过哈希索引函数落入此空间的索引条目都被该节点存储与管理[9].然而随机分配局部节点所负责的标识符空间范围不能适应多租户索引的应用场景.首先,随机分配会导致索引条目在节点中分布不均衡.索引条目在节点中均衡分布是P2P结构高效查询的关键.然而,映射到标识符空间的数据库索引并不是均匀分布的[29-30].特别是在多租户数据库中,由于不同数据对象索引所描述的数据对象集合的数据量大小及属性值分布是不同的,所以其包含的索引条目数量也不相同.采用Chord的方式确定各节点负责的标识符空间范围,将导致数据分布严重不均.其次,随机化分配导致一个租户的索引分布在多个节点中,增加查询时节点间数据传输的成本.属于一个租户的索引条目应聚集地分布,即分配到尽可能少且相邻的节点上.因此,需要设计标识符空间在节点中的分配方案,使索引条目在节点中均衡、聚集地分布.不同于Chord通过哈希IP地址得到局部节点所负责的标识符空间的起始值与结束值,本文通过计算索引条目数量的方法得到局部节点nh应负责的标识符空间Rh=[la,lb).为保证索引条目在各节Page7点分布数量的均衡化,在索引条目初始分配时,计算每个节点应分配的索引条目数量的平均值p照各索引条目的ks大小依次分配到各个节点,使每个局部节点负责的索引条目数量为pS在节点中的分配方法.算法1.标识符空间在每个节点中的分配.输入:II,II中索引条目的数量II,局部节点数量N输出:每个局部节点负责的标识符空间的范围R1.Data:acount←0,h←0,LL[0]←0,p2.FOReachi,j∈IIdo3.SORTerini,jorderbyer.kASC//按照键值升序4.FOReacher∈i,jdo5.acount++6.IFacountmodp7.LL[h++]←F(er.k)//获得每个节点的起始值8.ENDFOR9.ENDFOR10.FORjfrom0toN-111.R[j]←[LL[j],LL[j+1])//节点nj负责的空间范围12.ENDFOR13.RETURNR算法1输入索引集II,返回每个局部节点负责的标识符空间的范围R.对于局部节点ni,其所负责的标识符空间为R[i]=[LL[i],LL[i+1]),所有映射到R[i]的索引条目都由ni负责建立局部索引.例如,共有索引条目100条及10个节点,则每个节点分配的索引条目数量为10,那么节点n1所负责的标识符空间R[1]=[F(e10.k),F(e19.k)).算法1具有索引条目聚集分布的特点,即可将一个租户的索引条目分布到尽可能少的局部节点中,从而可减少查询时数据的传输成本.在一般情况下,一个租户的索引条目数量要小于局部节点容量-,所以一个租户的索引条目会分配到一个或两个p局部节点.当数据对象的索引条目数量大于局部节点容量时,分布到局部节点的数量仍能接近理想最佳节点数量,用OPT(optimalsolution)表示该数量,若每个局部节点管理的索引条目数量为pOPT=|i|p点数目的限制.定理2.算法1保证任意租户tni的索引条目分布到局部节点的数量NA1小于最优值加1,即NA1OPT+1.证明.对于租户tni的为|i|.因函数(1)保证属于一个租户的索引条目在标识符的空间分布是连续的,所以算法1在分配同一租户的索引条目时,会连续分配到若干个局部节点.设na、nb分别为部节点与最后一个局部节点,设Nei索引条目的数量,那么Nei索引条目分配到的一个中间局部节点,即f∈(a,b).那么nf只会分配到为中间局部节点nf的数量,则Nf=NA1-2.可得Nea+Ne|i-|imodp2OPT+1.在应用时,分配算法还可根据实际需要做一些调整.首先,算法1假设各个局部节点的存储能力是同构的.但在云计算环境中,一般局部节点能力是异构的.算法1中可对变量p引条目匹配每个局部节点的存储能力.其次,算法1假设索引条目使用频率是相同的,因此以索引条目的数量作为参考值.当索引条目的使用频率不同时,可给每个索引条目加上访问频率的权重.算法1第6步使分配到每个局部节点的索引条目都是均值p索引机制初始化使用.当索引实例运行后,可利用分裂与合并局部节点的方法[31-32]对各局部节点的索引条目的数目进行动态调整.对于各局部节点路由表的维护方法,MIMC仍然使用Chord协议[9]维护.至此各局部节点得到了自己所负责管理的索引条目,并组成了一个Chord网络.下一小节将介绍各局部节点的本地索引管理.4.3局部节点的索引组织本小节给出局部节点的索引组织策略,策略保证索引与相关数据存储在相同节点并利用B+树组织索引条目.在上一小节中,通过算法1可得到每个局部节点所负责的标识符空间范围,因此局部节点需组织管理映射到其所负责范围的索引条目.局部节点的索引组织策略分为以下两步完成:首先,将局部节点nh负责索引条目所涉及的数据对象集合存储到该nh.对于nh所负责的每一个索引条目er,获取er所属的数据对象集合Dk,并在nh存储Dk.不同租户的数据对象集合采用共享模式的方式存储到宽表中[1]或者文档数据库[33].Page8其次,令每个索引条目的v指向涉及数据对象的物理存储地址,并将索引条目加入到B+树进行存储.在局部节点上,本文利用一个B+树管理所负责的所有索引条目.局部节点先将每个索引条目变换为〈ks,v〉,再对索引条目建立关于ks的B+树索引.局部节点建立索引组织的具体步骤在算法2给出.设EEi为局部节点ni所应负责管理的索引条目集合,即EEi={er|er.ks∈Ri}.算法2.局部节点建立索引.输入:ni所应负责管理的索引条目集合EEi输出:B+树索引1.Foreacher∈EEi2.Di←getDataObject(er)3.IFDi尚未存储在本局部节点4.在本局部节点存储Di5.ENDIF6.ks←Function(1)(er.k)7.er.v指向数据对象的物理节点8.以ks为键值将er插入B+树9.ENDFOR不同于为每个租户分别建立索引实例[12]的情况,算法2只维护一个B+树即可实现对多个数据对象索引的管理,从而节省了缓存资源,降低了管理的复杂度.并且算法2将索引条目与其指向的数据存储到相同的局部节点,得到索引条目即可获得所需数据,提高了查询速度.算法2可减少多租户数据存储的冗余信息.文献[1,4,6]提出的多租户数据存储模式中每个元组都需要存储租户ID与数据对象集合ID以标识该数据所属租户及关系,以对元组的所属租户加以区分.而算法2可在租户数据对象集合的主关键字上建立索引,即主键索引,利用主键索引即可提供租户数据定位及全表扫描功能,使得租户数据存储中不再需要存储租户ID及数据对象集合ID,从而减少了多租户数据存储的冗余信息.本小节通过对索引条目在对等节点上的组织,建立起一个多租户索引机制.该机制允许在任意局部节点发起租户查询请求Q,Q的查询步骤如下:(1)局部节点解析Q得到所需查询数据的k,并利用函数(1)得到ks;(2)通过路由表得到ks对应的目标节点,并将Q发送到目标节点;(3)通过Chord路由协议,Q最终会路由到租户数据所在节点,并在数据所在节点进行租户数据的查询处理.5多租户索引机制的动态管理在多租户索引机制动态运行过程中,租户数量与租户索引是不断增长的.平台中随时都有新的租户加入或者新的索引建立,因此需要为新加入的租户或索引动态地分配标识符空间.如4.1节所述,在索引标识符空间映射初始化时,会预留若干租户区域(最大为MaxTN),每个租户也预留了若干数据对象的索引区域(最大为MaxIN),以备索引的新增.然而,当多租户数据库新增租户tni的i>MaxTN时,则没有新的租户区域可供租户使用;当租户tni新建数据对象索引对象索引区域可供数据对象索引使用.针对此问题,可以通过在系统初始化时预留足够数量的租户区域或者数据对象索引区域的方法来解决,然而预留的空间过大会增加B+树的存储空间.本文采用对标识符空间进行逐步扩展的方法解决此问题,扩大后的标识符空间可成倍增加多租户数据库可管理的租户数量及数据对象的索引数量.对标识符空间S进行扩展,应尽可能地减少对原有索引机制使用的影响.首先,不能影响原有索引映射的位置性与隔离性.其次,索引条目已经分配到局部节点,新形成的标识符空间在局部节点中的分配,不应使局部节点所负责的索引条目有过大的变动,导致大量索引及相关数据迁移.再次,在动态的标识符空间扩展中不应影响运行中的索引功能,通过维护局部节点路由表保证Chord路由寻址的正常运行.根据以上要求,5.1节与5.2节分别给出新增租户及新增数据对象索引S的扩展方法.5.1新增租户的标识符空间管理采用动态扩展标识符空间的方法来处理租户区域分配完毕的问题.当前标识符空间可管理租户数量为MaxTN,当新分配租户tni的序号i>MaxTN时,触发索引机制的标识符空间扩展指令.扩展标识符空间的方法是将原有标识符空间的空间增加一倍,即S=2S,如图3.因此,索引机制可管理的租户数量为MaxTN=2MaxTN.新增数据对象索引的索引条目仍然可用函数(1)在标识符空间S中映射.算法3给出租户区域分配完毕时新增租户的具体算法.Page9图3新增租户的标识符空间管理算法3.租户区域分配完毕时新增租户对S扩展.输入:当前租户数量currentNum,MaxTN,S输出:MaxTN,S,租户标识i,tni的租户区域1.i←currentNum+1//获得租户标识2.IFi>MaxTN3.寻找节点nh,该nh负责标识符S-1,即S-4.nh的Rh←Rh+[S,2S)5.S←2S6.MaxTN←2MaxTN7.ENDIF8.分配[STN(i-1),STN×i)给租户tni使用9.currentNum++;10.FOReachni∈N//N为节点集合11.更新ni的路由路由条目,使其第j个路由条目为12.ENDFOR算法3对MIMC扩展后,对索引机制的使用影图4更新局部节点Chord路由表5.2新增数据对象索引的标识符空间管理本小节给出对新增数据对象索引标识符空间管理的方法.一个租户区域在初始时固定大小为STN,多租户数据库根据租户需要后续地新建数据对象索引,当租户tni新建数据对象索引为j>MaxIN时,需扩展STN.新建立的数据对象索引应与原有该租户数据响较小.使用函数(1)保持了索引条目映射的位置性与隔离性.新增的标识符空间将分配到对等网络中第3步求得的局部节点nh中,其他局部节点负责的标识符空间范围没有变化.若后续增加的租户及数据对象索引引起此局部节点的索引条目数目过多,可将此节点分裂从而动态地平衡节点的数目.标识符空间扩展后,为保证各局部节点仍能正确寻址,需要对各局部节点的路由表进行修正.根据Chord路由协议[9]更新每个局部节点的Chord路由表.例如,若当前S=23,MaxTN=2,STN=22,已有租户2个,则当新增租户为tn3时,需要扩展S.那么扩展后S=23×2=24,MaxTN=4,tn3分配的空间为[22×2,22×3).扩展的空间由最后一个节点负责,如图4,N6原负责标识符空间[6,7],扩展后负责标识符空间[6,15].路由表的更新如N2,其路由表增加了一项.对象索引相邻近.所以与新增租户标识符空间管理的直接扩展S方法不同,此处采用扩展租户区域STN的方法,如图5,将每个租户区域扩大一倍.以下给出扩展租户区域的主要步骤:(1)每个租户区域扩大1倍,即STN=2STN.(2)标识符空间也相应扩大一倍,即S=2S.(3)由于租户区域扩大,对应索引条目的映射Page10图5新增数据对象索引的标识符空间管理函数也需相应更改.变更为F(k)=STN×(i-1)+SIN×(j-1)+SIN×(k-mini,j)/(maxi,j+1-mini,j)(2)根据函数(2)可看出,标识符空间的改变及映射函数的改变并没有影响到索引条目映射的位置性与隔离性.(4)局部节点采用多B+树的方法对所负责的索引条目进行管理.由于现有B+树中每个节点的k值是由函数(1)计算的,而映射函数变更后,多租户数据库则利用函数(2)查找索引条目,导致无法得到正确的目标索引条目.直观的解决方法是利用函数(2)重建B+树,但对于实时查询来说重建B+树的时间成本是不能忍受的.因此,本文采用多B+树的方法管理局部节点所负责的索引条目,映射到扩展前租户区域的索引条目仍然利用函数(1)组织与查找B+树,映射到扩展后租户区域的索引条目则利用函数(2)组织成一棵新的B+树,由此避免了现有B+树的重建.详细方法如下:每扩展租户区域一次新建一棵B+树.令初始B+树标识号b为0,后续新建的B+树依次顺序编号;同时对映射函数也进行初始为0的顺序编号.对于新添加的属于的B+树标识号b,其中Max0据对象索引数量的最大值.那么er应存储到标识为b的B+树,er的ks值也由映射函数b计算.(5)更改局部节点负责的标识符空间.对于任意局部节点所负责的标识符空间范围[la,lb),更新后的所负责的标识符空间范围为[la,lb).la的计算用函数(3),lb同理.la=la/STN×STN+lamod|STN|(4)(6)更新每个节点的路由表.每个路由条目的键值xs,可代入函数(4)进行更新;更新后,有部分的路由条目不满足Chord路由表规则,需添加与删除对应的路由条目.路由表更新的时间复杂度为O(logN),N为局部节点数量.路由表采用异步更新的方式,方法见5.3节,从而使得更新路由表的同时不影响查询的跳转与执行.例如,若当前S=23,MaxTN=2,STN=22,MaxIN=Max0据对象索引,则当tn1新建数据对象索引为需要扩展租户区域.那么扩展后S=23×2=24,STN=22×2=23,MaxIN=4.根据函数(3),的索引条目应插入到局部节点标识为1的B+树中.在第(4)步中,采用多B+树索引的方法管理,避免了因映射函数的改变而需重建B+树的操作,具有较高的扩展效率.属于一个同一个B+树中,因此租户查询中的比较操作不受影响.在处理租户查询时,若需利用数据对象索引i,j处理数据,则利用函数(3)选择标识为b的B+树进行查询;因b号B+树是利用标识为b的映射函数得到的ks建立的,在查询数据值为k的数据对象时,需先使用b号映射函数计算ks,再在该B+树中进行查找.因B+树的选择与ks的计算都是一次计算,所以查询性能不受影响.标识符空间的改变及映射函数的改变需保证每个局部节点所负责的索引条目应相对稳定,不会导致大量的数据迁移.在第(5)步中,局部节点所负责的标识符空间范围的维护方法可保证每个局部节点负责的索引条目不变,定理3形式化描述了该问题.定理3.er:〈k,v〉,令le=F(k),le=F(k),若le∈[la,lb),则le∈[la,lb).证明.先证:lalelale.(1)当lamodSTNlemodSTN时,lalela/STN×STNle/STN×STN,所以lale.(2)当lamodSTN>lemodSTN时,若la/STN+1>le/STN,由于左右式均为整数,可得la/STNSTN>盾,所以la/STN+1Page11STNSTN+STN>同理,可证le<lble<lb.定理3得证.证毕.5.3标识符空间扩展的动态路由寻址方法租户索引机制是动态运行的,在标识符空间扩展时不可能同时更新所有的局部节点.设ne是已经进行标识符空间扩展的局部节点,no是还未进行标识符空间扩展的局部节点,当租户查询请求Q由ne节点路由到no节点或者由no节点路由到ne节点时,会出现使用路由表版本的错误.例如,ne节点利用标识符空间扩展后的映射函数Fd,计算出ks,将索引查询路由到no节点后,其路由表还未按照函数(3)进行更新,将会路由到错误的节点.给出的解决方案如下:Q路由到一个局部节点后,检查标识符空间版本,如果不同,利用该局部节点当前使用的映射函数重新计算一遍ks,按照该局部节点的路由表路由即可.因为根据定理3可知一个局部节点负责的索引条目与数据项在标识符空间扩展后是不会发生变化的,所以只要使用相同版本的映射函数与路由表总可以正确指向索引条目所在的局部节点.6实验结果与分析本小节实现了MIMC的原型系统,并介绍了MIMC与比较对象的实验设置,其比较对象为基于MongoDB[33]集群的集中式多租户索引.考察了查询时间及吞吐量的表现,考察了索引扩展性、隔离性对查询的影响,并测试5.3节租户区域扩展的时间成本.6.1实验准备(1)实验环境利用OpenStack平台建立8到64个虚拟节点,每个节点安装64位Ubuntu操作系统,分配4个处理器、8GB内存、200GB磁盘.实验将MIMC与基于MongoDB集群的多租户数据库进行比较,以下分别介绍两者的实验设置:(2)MIMC实验设置①每个节点安装MongoDB作为局部节点的数据存储模块,每个节点将所有该节点负责的租户数据都存储到一个Collection上;②利用MongoDB索引功能实现本文的局部索引,方法是通过在每条记录上加一项ks的值属性,并建立B+树索引;③利用Java语言开发对MIMC编程实现,主要由Chord路由协议模块、Query网络传递协议模块、MongoDB本地查询模块、查询测试模块组成;④查询具体流程是先给定一个租户的查询,通过F(k)值的计算与路由表的查询,得到要处理该Query的节点,如果是本地负责查询则交由MongoDB处理,如果是其他节点负责,则通过Chord协议发送出去.(3)基于MongoDB集群的集中式多租户索引的实验设置①建立MongoDB集群;②在MongoDB集群存储多租户数据.所有租户的数据以共享模式的方式存储在一个Collection,每个数据对象加上租户标识与数据对象集合标识,并利用数据分片技术[33]将所有数据对象分布存储到节点中;③在MongoDB集群中建立索引:首先在集群路由节点建立全局索引,全局索引条目为〈tni,Dk,nx〉,即全局索引记录租户的数据对象集合放置的节点,其次在各分片节点建立B+树结构的局部索引,为提高获得租户数据的效率,建立由租户标识、数据对象集合标识、查询目标属性值组成的联合索引;④在查询时,首先对租户查询进行查询转换[34],其次将租户查询提交到集群入口节点,集群入口节点负责从各分片节点获取租户数据并执行查询处理.多租户数据处理具有在线性、交易性的特点,如Salesforce的客户关系管理产品,由于TPC-C是模拟一个批发商的货物管理环境的在线事务处理的基准项目,可以部署在多租户数据平台,贴近多租户数据的应用场景,因此本文采用TPC-C[35]的业务模型模拟生成租户数据.实验模拟租户对基准业务模型的数据模式定制,对于每个租户,实验中以原始数据表为基准,随机增加1到5个属性列;同时加上租户标识列,用以区分每条记录所属的租户.在实验中,主要为租户生成定单表与定单分录表数据,并为定单表的O_ID、定单分录表的OL_O_ID建立索引.为衡量MIMC在线事务处理系统的性能及其可伸缩性,本实验考察3种查询:点查询、范围查询和连接查询.点查询是租户对一个数据对象集合发起的等值查询,在实验中针对定单表(ORDER)查询的定单号属性进行条件查询,查询谓语为POINTQUERY:SELECTFROMORDERWHEREO_ID=γ范围查询是指租户对一个数据对象集合的属性给出最大值、最小值从而获得一个结果集,在实验中针对定单表(ORDER)的定单号进行范围查询,从而得到运单号结果集,查询谓语为Page12RANGEQUERY:SELECTO_CARRIER_IDFROMORDERWHEREO_ID>αANDO_ID<β对于连接查询,本实验只考察两个数据对象集合的等值连接查询,对定单表与定单分录表(ORDER_LINE)连接取相关信息.由于MongoDB不支持连接操作,实验在平台利用哈希连接的方法完成连接查询任务.连接查询谓语为JOINQUERY:SELECTORDER.O_CARRIER_ID,ORDER_LINE.OL_SUPPLY_W_ID,ORDER_LINE.OL_QUANTITYFROMORDER,ORDER_LINEWHEREO_ID>αANDO_ID<βANDO_ID=OL_O_ID为满足大多数数据类型的索引对哈希空间位数的需求,可设置SIN为2128,该SIN最大可保证128位长整型及16位字符型在映射中无冲突.在初始时,设置可管理租户数量为MaxTN=216,每个租户可建立MaxIN=28个索引,那么|S|=2128×216×28=2152,即标识符空间的位数为152.在实验中可利用java.math.BigInteger工具进行大整型运算.MaxTN与MaxIN的初始设置是一个较小的值,若系统管理租户与索引数量超过两值,可通过第5节空间扩展方法解决.空间扩展会导致S的位数的增加,本文通过实验测试S的位数在64到256位变化时,对平均查询时间的影响小于1ms.6.2MIMC的查询时间本小节对MIMC的查询时间进行测试.首先以租户数量作为变量,数量从160~6400个变化,考察使用MIMC的查询时间.建立32个局部节点管理租户数据,每个租户的数据量大小为100000条,每个租户同时发出3个查询请求.基于MongoDB集群的集中式多租户索引作为比较对象,其3种查询分别称为集中索引点查询、集中索引范围查询、集中索引连接查询.由于集中索引连接效率过低,实验中不再考察.实验结果如图6,MIMC的点查询、范围查询、连接查询有着较好的表现.随着租户数量的增多,MIMC的查询时间优势更加明显,因为集中索引使得入口节点的并发查询过多,从而使得查询等待执行的时间较长.在租户数量等于160时,MIMC范围查询仍然比集中索引范围查询消耗的时间少,因为范围查询结果集较大,MIMC直接将结果传输给查询发起节点从而减少了网络传输时间.其次考察每个租户数据量作为变量,数据量从1000~1000000条记录变化,32个节点,设定256个租户分布在这些节点上,每个租户同时发起5个查询请求.实验结果如图7,MIMC点查询的时间消耗不受数据量大小的影响,而MIMC范围查询与连接查询随着数据量的增加对节点内存要求增多,查询时间平缓增长.集中索引点查询与集中索引范围查询因为入口节点的并发查询队列等待时间过长,所以查询时间长于MIMC.6.3MIMC的扩展性本小节考察MIMC的扩展性能.首先测试节点数量的变化对查询吞吐量的影响.节点数量范围为8~64,同时每增加一个节点对应增加10个租户,每个租户的数据对象数量为100000条.实现结果如图8所示,MIMC的3种查询的吞吐量与节点数量成线性关系,特别是点查询与范围查询,而连接查询Page13消耗的CPU、内存资源较多,所以吞吐量增长较慢.由于入口节点的瓶颈,集中索引点查询与范围查询的吞吐量并没有随着节点数量的增加而增长.其次测试节点数量的扩展对查询时间的影响.分别测试MIMC点查询时间与集中索引点查询时间,5是指每增一个节点对应增加5个租户,50是指每增一个节点对应增加50个租户,且设定每个租户同时发起5个查询,每个租户的数据量为100000条.实验结果如图9,可观察到MIMC的点查询时间并不随着节点规模的扩大而显著延长,这是由于租户查询在节点规模增大时,只是指数级地增加路由的跳数,而每一跳消耗的时间数量级为10ms.对于集中索引点查询,查询时间随着租户与节点规模的扩大而增长较多,特别是在50时过多的时间消耗在队列等待中.以上实验表明MIMC可以通过增加节点数量适应大规模多租户数据查询处理的需求.由图6、图7可看出,当节点数量一定时,影响查询效率的主要因素是租户数量,而每个租户的数据量影响较小.这是因为同时发出的查询请求数量与租户数量成比例,当查询请求数量较多时,查询任务会在队列等待中消耗过多的时间.根据P2P结构的特性,MIMC会将查询请求分散到各节点中,从而避免各节点出现性能瓶颈.图6结果表明,在当前实验环境下,当租户与节点数量比小于100时,点查询与范围查询的时间成本较低.图9表明,同时增加租户数量与节点数量,查询时间增长缓慢,这是因为增加的跳数是节点数量的对数级.因此,使用MIMC时,可根据性能需要设置好租户与节点的比例关系,当租户数量增加时,按比例增加节点数量即可.6.4租户区域扩展的时间成本及查询影响本小节考察5.2节扩展租户区域的时间成本及扩展后对查询的影响.租户区域扩展时,主要耗时在步骤(4)B+树的维护及步骤(6)路由表的维护上.在步骤(4)中,针对B+树失效的问题,比较直观的解决方法是重建B+树,本文还提出了一种多B+树的方法.实验将两种方法进行比较,测试租户区域扩展的时间.实验还利用点查询测试两种方法建立索引结构对查询的影响.实验在一个局部节点上进行,管理8个租户,每个租户的数据对象数量从1000~1000000条变化.对于租户区域扩展的时间成本,如图10的柱状图比较,多B+树方法扩展时只需新建一个空B+树及维护路由表,与重建B+树方法相比有着明显优势,特别是在数据对象数量较大时.对于查询性能,如图10的折线比较,以上两种方法平均查询时间相似,这是由于多B+树方法只比重建B+树方法多一步B+树的选择,通过一次计算即可完成.6.5隔离性对查询的影响本小节考察隔离性对租户查询的影响.4.1节证明了MIMC的映射函数具有隔离性,租户使用索引查询时只检索到自己的数据,由此提高了查询效率.为验证隔离性对租户查询性能的提升,本节将MIMC与DBMS-like[19]进行比较.DBMS-like实验设置:DBMS-like是Chen等人[19]提出的一种针对DaaS的云中关系运算索引机制,其将索引保序地发布到P2P网络.在实验中,所有租户的数据对象都使用其保序函数在云中映射,并采用Chord协议组织P2P网络.由于相同k值但不同租户的索引条目会映射到S中的相同位置,所以此方法不能保证租户间的隔离性.基于MongoDB集群的集中式多租户索引也不具备租户隔离性,但其因资源争用会影响查询时间,所以不再考察.实验中,多租户数据放置于8个节点,租户数量由32到256变化,每个租户数据量为100000.为避免资源争用对查询时间的影响,实验测试只有一个租户发起查询的时间成本,测试的查询类型为点查询与范围查询.实验结果如图11,MIMC范围查询比Page14DBMS-like范围查询耗时少,这是由于DBMS-like检索到的数据含有大量其他租户的数据,需在这些数据中做进一步的筛选.而且当租户数量增多时,DBMS-like范围查询的耗时急剧增加,而MIMC范围查询的耗时基本不变,这是因为MIMC的租户索引是隔离的,不受租户数量的影响.而对于MIMC点查询与DBMS-like点查询,租户的隔离性对其查询耗时没有影响,这是由于DBMS-like方法只等值的检索一条或几条数据,并不需处理其他大量的无关数据.因此,租户隔离性对范围查询有较大影响.6.6MIMC的复杂查询性能本小节测试MIMC在较大数据量下复杂查询的性能.实验采用TPC-H的数据模式生成租户数据,并在TPC-H的数据模式上加入租户标识.节点数量为8个,租户数量由16到64变化,每个租户数据量为10000000.实验以Q6与Q17作为测试对象,Q6是针对LINEITEM的单表统计,Q17是涉及LINEITEM与PART的双表数据分析查询.在实验中,对P_PARTKEY、L_PARTKEY、L_SHIPDATE属性建立索引,设置每个租户同时发起1个查询.如图12,实验表明,利用MIMC比基于MongoDB集群的集中式多租户索引查询耗时少.特别是对于占用资源较多的Q17,因后者在查询时产生资源争用情况,租户数量越多耗时越长.因此,对于大数据量且较为复杂的租户查询,MIMC仍有较好的查询性能.7结论本文提出了一种通过改进映射方法的Chord多租户索引机制,其提供的映射函数及标识符空间分配算法可以隔离、保序、均衡地将租户的索引条目分配到云中节点上.提出的动态索引管理方法,适应了租户数量及数据无限增长的需求.MIMC的局部节点是对等网络节点,相对于集中式索引其不存在性能瓶颈.最后实验表明,当MIMC在租户规模较大时,查询时间与吞吐量有较好的性能表现.
