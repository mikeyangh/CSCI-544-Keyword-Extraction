Page1面向人机接口的多种输入驱动的三维虚拟人头1)(语音及语言信息处理国家工程实验室合肥230027)於俊1),2)汪增福1),2),3)2)(中国科学技术大学自动化系合肥230027)3)(中国科学院合肥智能机械研究所合肥230031)摘要面向人机接口领域,文中提出了一个可由多种输入驱动的三维虚拟人头系统.该系统具有如下特性:(1)由视频、文本和语音多种输入驱动,增加了与人交互的多样性;(2)在粒子滤波框架下根据在线外观模型跟踪视频中的人脸三维运动,并且融合多种观测信息来降低在线外观模型的光照敏感性和个体相关性;(3)参数模型和肌肉模型相结合的三维人脸动画,在保持生理结构的基础上描述人脸运动,进而达到高真实感,同时大大降低了计算量;(4)在保持正确率的前提下,采用三音子模型降低了可视化协同发音模型的计算复杂度.对该虚拟人头的客观和主观测试验证了其在人机交互上的有效性.关键词人机接口;虚拟人头;人脸运动跟踪;人脸动画1引言面对面的交流是人类之间的一种可靠和有效的沟通方式[1],因此在人机交互中,采用拟人化的接口会使得交流更加自然,促进人们设计出个性化、富有情感的、符合人类习惯的机器.其中,视频、文本和语音多驱动的三维虚拟人头(3DVirtualHead,Page23DVH)相比于机器人头,具有如下特点:(1)易于调整结构;(2)更改外貌和行为更加直接;(3)设计自由度和可扩展性更大.另外,国内外语音产业化企业近年在语音合成、识别和评测等领域取得了令人瞩目的成绩.但也存在着人机交互界面过于单调(只有语音输出)的问题,这是因为人类在面对面交流的时候,不仅听到声音而且还可以观察到对方的面部运动来辅助理解.鉴于上述原因,构建人机实时自然交互的3DVH系统是极具意义的研究课题.3DVH的设计分为从人脸运动行为出发的方法和从人脸生理结构出发的方法.Cole等人[2]对这两方面的工作以及它们的结合进行了详细的阐述,并指出将两者的优点相结合是未来发展的方向.Simmons等人[3]介绍了3DVH在机器人头辅助设计方面的工作,并指出3DVH的设计方法和机器人头的设计方法是相互促进、最终可以统一的.进一步,在机器人的真实感与人对它的接受度之间的关系上,心理学中的恐怖谷理论[4-7](图1)指出:随着机器人与人类在外表、动作上逼真度的增加,人类也随之增加对它的好感度,但至某个程度(称为恐怖谷)人类会突然变得极其反感(比如,虽然死尸/僵尸比毛绒娃娃更像人类,但人类明显反感前者而喜爱后者).可是当逼真度继续上升的时候,人类对它的反应亦会变回好感,且比进入恐怖谷前的程度更强.同样地,构建高真实感的3DVH使之能够跨越恐怖谷是3DVH研究追求的目标之一.(1)三维人脸动画因为人脸生理结构的复杂性、表情的丰富性和明显的个性化特点,要想快速、逼真地生成三维人脸动画仍然是非常困难的.第1步是人脸模型特定化.它可通过扫描仪[8]、多视角图像[9]、视频[10]或单幅图像[11]来完成.这些方法都有独特的优势,其中基于单幅图像的方法因其便捷性是目前研究的热点,但其得到的人脸模型只有正面纹理,而侧面纹理对后续人脸动画是至关重要的.基于单幅图像的方法进一步可分为基于几何变形的和基于学习的.前者通过变形人脸通用模型得到描述输入正面人脸图像的特定模型,简单有效,但需要精确定位图像上的人脸特征点以及足够灵活的插值算法;后者运用统计分析方法从人脸库重建出期望人脸,真实感较强,但其重建结果受限于人脸库的泛化性能和质量,且所需代价昂贵.第2步是人脸运动控制建模[12-13].参数模型[14]快速有效,但难以达到高真实感;通过模拟人脸动力学特性的物理模型[15]真实感较强,但计算和设置复杂;通过统计分析学习人脸运动规律的数据驱动模型[11]受限于训练库的泛化性能;基于人脸生理知识的肌肉模型[16-17]是一种从本质上表示人脸运动的方法,但建模复杂度和真实感有待改进.(2)基于视频的人脸三维运动跟踪基于视频的人脸三维运动跟踪是从已经获得的前一帧的人脸运动参数出发,根据当前帧的图像信息、人脸三维模型、人脸先验知识、滤波算法以及跟踪中获得的知识,得到当前帧的人脸运动参数.基于特征的方法[18]会累积估计误差进而发散,故需增加多种措施[19-22]来提高鲁棒性.基于外观模型的方法没有发散问题,它分为确定性的和统计性的.前者[23-24]通过对确定性的参考纹理进行几何变形来匹配当前帧,从而难以应付光照和表情变化下的情况.后者也分为离线的和在线的.离线方法[25-27]鲁棒性强,但无法适应训练库中没有的新情况.在线方法[28-30]不需要训练库,故更有优势,但如何在线更新模型以应对光照、个体相关性的影响是难点[31-34].进一步,对于非线性、非高斯分布的人脸运动,粒子滤波[32]得到了广泛应用,但也面临着计算盲目、计算量大和粒子退化等问题的挑战.(3)连续人脸语音动画连续人脸语音动画区别一般人脸动画的最主要一点是,当前口型既受当前发音的影响又受前后发音的影响.该现象称为可视化协同发音.对该现象的建模是制约连续人脸语音动画达到高真实感的重要瓶颈[35-36].针对于此,Cohen模型[37]基于发音器官状态模型[38]提出了权值融合的思想,但参数设置复杂;Bregler等人[39]利用上下文相关的音素模型寻找图像数据库中当前音素最匹配的发音器官运动,但代价较大;Pelachaud等人[40]建立规则集来描述协同发音,但适用范围有限.Page32系统框架本文面向人机接口研究领域,提出了一个视频、文本和语音多种输入驱动的3DVH系统(图2):(1)建立参数模型与肌肉模型相结合的三维人脸动画算法;(2)当输入视频时,首先提取多种观测量(Multi-Measurements,MM),接着在粒子滤波(ParticleFilter,PF)框架下根据在线外观模型(OnlineAppearanceModel,OAM)从视频中提取出人脸运动参数,然后根据三维人脸动画算法,由人脸运动参数驱动生成人脸动画,并同步播放伴随视频中的语音信息;(3)当输入文本或语音时,首先根据文语转换引擎(MicrosoftTexttoSpeech,MS-TTS)或语音识别引擎(MicrosoftSpeechRecognition,MS-SR)输出的发音信息,建立可视化协同发音模型(VisualCo-articulationModel,VCM),接着根据三维人脸动画算法合成与发音信息对应的视素,然后结合VCM对视素的影响,由非均匀有理B样条(Non-UniformRationalB-Spline,NURBS)在视素间插值生成人脸动画.3从视频中提取人脸三维运动信息因为在线外观模型在不需要额外离线信息等方犫=[犺T,βT,αT]T=[θx,θy,θz,s,tx,ty,βT,αT]T(1)其中,犺=[θx,θy,θz,s,tx,ty]T是全局刚体运动参数,β和α是形状、局部运动参数.面有独特优势,在此,我们深入挖掘它的潜力.3.1用于人脸运动跟踪的三维几何模型采用CANDIDE3[41]三维模型(图3(a)).该模型由以下人脸运动参数来控制:(其中(a)为CANDIDE3模型;(b)~(e)为GNFI的获取过程)3.2提取多种观测量观测量从对应于输入人脸图像的几何归一化人脸图像(GeometricalNormalizedFacialImage,GNFI)[25]中提取(图3(b)~(e)).我们首先从GNFI中提取颜色值作为观测量,但它易受光照影响且与个体高度相关,因此需要寻找更加鲁棒的观测量作为补充.考虑到光照比图像[33]和Gabor小波[42]的优良特性,将当前帧的GNFI和首帧的GNFI之间的光照比图像的Gabor小波变换系数的幅值作为第2部分的观测量.3.3构建在线外观模型与融合多种观测量为3个分量的混合高斯模型(观测模型):p(狔t/犫t)=∏dmi,t(j)=(1-c)mi,t-1(j)+cmi,t-1(j)N(yt-1(j);μs,t(j)=(1-c)μs,t-1(j)/ms,t(j)+σ2s,t(j)=(1-c)σ2s,t-1(j)/ms,t(j)+如文献[32]中所述,将第1部分观测量狔t建模相应外观模型的在线更新方式为对应第2部分观测量犌t的处理过程同上.最后采用乘法方式来融合多种观测量:p(狔t/犫t)·p(犌t/犫t),该融合结果将在粒子滤波中被设为粒子的权重.3.4应对遮挡挡的措施分为如下两个阶段:借助于人脸三维模型和在线外观模型,应对遮第1阶段是判断遮挡.首先计算视线与人脸三维模型中每个3D面片法向量的夹角的余弦值,如果为负则该3D面片被遮挡,否则没有被遮挡.第2阶段是处理遮挡.当第k个3D面片被遮挡时,它对应的GNFI中的2D面片(图3(d))中的观测量由前一时刻的值和在线外观模型来估计:y(j)烅烄y(j)烆Page4当观测量为Gabor小波系数时,与上述类似.3.5基于改进粒子滤波的运动滤波策略如前所述,粒子滤波在处理非线性非高斯分布的人脸运动上具有独特的优势,但也面临着一些问题.鉴于此,本文采取一些有针对性的措施.3.5.1结合局部优化来降低计算复杂度因为局部优化可以结合当前时刻的最新观测值来生成与真实状态分布产生的粒子偏差较小的粒子,所以在更新粒子权重之前增加局部优化步骤来降低计算盲目性:然后设置粒子数目正比于局部优化后得到的误为了应对粒子退化问题,在标准重采样前加入差e,从而自适应地降低计算量.3.5.2基于改进重采样来减少粒子退化PERM采样[43]:如果粒子权值π(j)a保留粒子,且π(j)留K次,且π(j)进行自适应的调整:如果6.1.5节(1)中定义的量化指标小则表示当前保留的粒子是好的,就增大a、π+和减小K、π-来弱化PERM采样的作用;否则强化PERM采样的作用.综上所述,上述改进粒子滤波方法被称为LOPSAPF(LocalizedOptimizationPERMSam-plingAdaptiveParticleFilter).3.6人脸和头发纹理的更新因为侧面纹理对后续人脸动画是至关重要的,所以我们在人脸运动跟踪过程中适时地更新人脸和头发的纹理.当跟踪结果中的旋转角大于设定阈值时进行纹理更新:首先由人脸三维模型在像平面上的投影来得到人脸外轮廓;其次获得头发外轮廓[44];然后将两个外轮廓组成的闭合区域中的图像作为更新纹理.算法流程.1.根据人脸模型特定化得到的人脸运动参数初始值犫0、先验分布p(犫0)和初始粒子数N0来得到初始粒子集合S0={犫(j)0,π(j)2.预测.基于前一帧的估计值犫t-1,由局部优化得到Δ犫t;然后移动粒子犫(j)确定粒子的数量Nt;3.重采样.首先PERM采样,接着标准重采样;4.由犫(j)5.更新.π(j)6.由犫t=∑Nt7.更新梯度矩阵、外观模型和π-、π+、a、K;8.t=t+1,返回步2.4三维人脸动画4.1用于人脸动画的三维几何模型器官,为高真实感人脸动画提供了坚实的基础.采用Alice[45]三维模型(图4(a)),包含完整的图4三维人脸几何模型(其中(a)是Alice模型;4.2基于动态纹理更新的人脸模型特定化综合考虑精度、代价和实时性,采用单幅图像方法中简单有效的几何变形方法,并且针对该方法重建出来的人脸模型只有正面纹理的问题,在视频驱动的人脸动画过程中动态地更新映射到人脸三维模型上的纹理贴图.具体过程如下.在第1阶段,对于首帧人脸图像:(1)由动态外观模型(ActiveAppearanceModel,AAM)[26]来确定图像中的特征点;(2)由数个特征点和文献[46]来得到全局运动参数;(3)特征网格点的位移由对特征点进行最小二乘拟合来得到;(4)其它网格点的位移由径向基插值来得到.在第2阶段,当后续帧产生如3.6节所述的更新纹理的时候,相应地更新映射到人脸三维模型上的纹理贴图:(1)根据当前帧与首帧的人脸全局运动参数的差值将更新纹理变换到首帧的坐标系下;(2)将变换后的更新纹理和首帧中的纹理的拼接结果作为纹理贴图.4.3人脸运动控制建模我们结合参数模型和肌肉模型合成动画.前者采用径向基插值,这里不作赘述.后者采用Waters肌肉模型[16-17],该模型根据人脸肌肉(图5)的运动和方向性特性,建立了相应的向量肌肉模型,是人脸生理动画模型的主流方法之一.Page5首先,人脸模型的网格点根据脸部器官的运动特征进行分区.在各区中,网格点分为3类:主特征点、次特征点和非特征点.主特征点是与MPEG-4标准中FDP定义的人脸特征点相对应的网格点;次特征点是主特征点所在肌肉模型作用范围内的网格点;非特征点是其它网格点.下巴、上嘴唇的功能区及其中的3类网格点如图4(b)~(c)所示.在某一分区,首先,主特征点的位移等于相应的人脸运动参数值;然后,次特征点的位移由Waters模型根据主特征点的位移来得到;最后,非特征点的位移由径向基插值根据主特征点的位移来得到.5三维连续人脸语音动画5.1视频驱动下的人脸语音动画我们根据从视频中提取的人脸运动参数犫以及4.3节的三维人脸动画算法来合成人脸动画,并同步播放伴随视频的音频信息.5.2文本或语音驱动下的人脸语音动画在第1阶段,我们合成未采用可视化协同发音模型(VCM)的视素.因为合成的视素要真实地反映音素对应的唇部运动,且舌头和牙齿的运动也要与唇型保持一致.所以我们根据英语音素和视素的分类①及对应的发音器官运动规律、人脸动作编码系统(FacialActionCodingSystem,FACS)[41,47]定义的视素对面部肌肉的影响和4.3节的人脸运动控制算法来完成.在第2阶段,我们首先基于MS-TTS引擎或MS-SR引擎,从文本或语音中得到发音信息(音素序列及时长),然后根据发音信息建立VCM.在各种VCM中,Cohen模型[37]取得了较好的效果.该模型的音素具有相关的人脸动画参数及对应的权值函数,当前视素是发音中所有视素对它的加权融合结果.但在实际中,相隔较远的音素对当前音素的影响是可以忽略的.鉴于此,采用三音子模型来设计权值函数,即只考虑前一和后一音素为对当前音素的影响,从而降低了计算复杂度.设权值函数W(t)为三阶多项式函数.令t-1、t0、t1分别为前一、当前和后一音素的开始时间,t2为后一音素的结束时间;λ为当前音素在(t0+t1)/2处的幅度,表示它的协同发音影响程度;k为当前音素的扩散程度,表示它的协同发音影响范围.据此得到W(t)的受限条件为(1)W(t-1)=0,W(t2)=0,(2)W(t-1)=0,W(t2)=0,(3)W((t0+t1)/2)=λ,(4)W(t0)=kλ,W(t1)=kλ.在解得权值W(t)后,利用它融合前一、当前和后一音素即得到我们所采用的VCM:其中Fs为未采用VCM的视素对应的人脸动画参数.对于占支配地位的元音音素,λ、k为1、0.8;对于占被支配地位的辅音音素,λ、k为0.1、0.5.在得到VCM后,我们根据它和第1阶段得到的未采用VCM的视素来合成采用VCM的视素.在第3阶段,考虑到细微的抖动会轻易地被人眼所察觉并产生不自然的感觉,进而抗拒与3DVH的交互.我们采用光滑的NURBS来插值生成视素之间的过渡动画.在第4阶段,考虑到人们说话时的内容应与表情相互协调,否则会在交流时产生误会,我们进行添加表情的人脸语音动画.首先根据人脸表情的生理知识[47]和4.3节的三维人脸动画算法来合成在给定幅度下的带有6种基本表情的人脸动画;接着在需要以某种表情说出的文本内容的首尾加上相应的标签;最后当在人脸动画中遇到这些标签的时候,进行表情和视素的混合,即对于同时受到表情和视素影响的模型顶点,将视素与表情在该点的运动值的平均作为该点的运动值.在第5阶段,将第4阶段的结果和发音信息同步播放即得到最终结果.6实验结果与分析实验配置:CPU3.01GHz,内存2GB,显卡GT200.①http://msdn.microsoft.com/downloads/sdks/platform/Page66.1基于视频的人脸三维运动跟踪6.1.1人脸运动跟踪结果从图6可见,针对受光照影响和人脸姿态变化较大的视频以及具有细微眼部动作的视频,人脸运动跟踪的效果较好.6.1.2单个观测量Vs多个观测量受光照影响较大的视频被用来验证融合多个观测量的有效性.由图7可见,采用多个观测量的结果更好.6.1.5节将给出进一步的比较.6.1.3不同粒子滤波算法的比较采用文献[30]中的非线性状态空间模型:xk=1+sin[0.06π(k-1)]+0.8xk-1+vk-1(6)3.5节所述的LOPSAPF的初始粒子数为200,被比较的其它算法的粒子数目为200.从表1可见LOPSAPF在估计精度上的优越性.算法EKF0.368460.01427160.054UKF0.259120.0113SIR0.181890.0411270.085EKPF0.281280.0148190.076UPF0.0435930.00432390.064LOPSAPF0.0167540.00041590.0696.1.4应对遮挡更加合理,进而可以得到更准确的跟踪结果.6.1.5人脸运动跟踪算法的评测与比较(1)定义一个量化指标:QT=∑N从图8的遮挡处理过程可见,处理后的右眉毛org)/(N·Mi),N为输入视频的帧数,Mi为视频y(i,j)第i帧中人脸区域的像素的个数,y(i,j)第j个像素的颜色值,y(i,j)映射到人脸三维模型上得到的合成图像中人脸区域内第j个像素的颜色值.由表2可见,对于MPEG-4中的13个人脸视频,CMUCohn-KanadeDataBase[48]中的110个视频和捕捉的78个人脸视频,本系统在跟踪精度上优于在线外观模型的代表性方法[31].本系统文献[31](2)首先在给定的人脸运动(真实值)和光照下用计算机图形绘制技术来合成人脸图像[49-50](图9),然后定义量化指标为在合成人脸图像上估计的运动值与真实值间误差的平均值.从表3可见,本系统在跟踪精度上优于文献[31],且采用多个观测量要优于采用单个观测量.采用单个观测量的本系统2.381.751.69采用多个观测量的本系统2.731.941.866.2三维人脸动画从图10可见,在单幅图像驱动下,得到的特定人脸模型只有正面纹理;而在视频驱动下,得到的特定人脸模型既有正面纹理也有侧面纹理,为后续的人脸动画提供了坚实的基础.Page7与6.1.5节(1)中定义的量化指标类似,这里也定义一个量化指标:QY=∑N(N·Mi).所不同的是,y(i,j)中人脸区域内第j个像素的颜色值.对于同样的输入人脸运动参数,从表4可见,与主流的人脸生理动画模型相比,本文算法以牺牲较少的真实感为代价,大大减少了耗时.表4本文算法和Waters模型的动画量化指标算法本文算法Waters模型6.3视频驱动下的人脸语音动画根据6.1节提取的人脸运动参数来驱动生成人脸动画.由图11可见,人脸动画较真实地复现视频中人脸的运动情况.6.4文本或语音驱动下的人脸语音动画6.4.1未采用VCM的视素合成由图12可见,在未采用VCM情况下,本系统合成的视素真实地反映了音素对应的唇部运动,且舌头和牙齿的运动与唇型较好地保持一致.6.4.2采用VCM的视素合成实验对象为文本“Istewtwocupoftea”,它可分割为以下音素序列:“ay,s,t,uw,t,uw,k,ae,p,ah,f,t,iy”.当人们在读“stew”的时候,对应于音素“s,t”的嘴型会被后面的对应于音素“uw”的嘴型影响,同样的现象也发生在“two”的音素“t”上面.由图13可见,在采用VCM情况下,本系统合成的视素较好地描述了上述现象:“stew”中音素“s,t”的嘴型和“two”中音素“t”的嘴型,出现了与后面音素“uw”的嘴型相似的圆唇动作,且离“uw”越近受到的影响越大.需要指明的是,在图中,受协同发音影响较强的采用纹理方式显示,否则采用网格方式显示.6.4.3过渡动画的合成在对文本“Istewtwocupoftea”合成了采用VCM的视素后,我们采用3阶NURBS来插值生成视素之间的过渡动画.由图14可见,所得到的连续人脸动画能够提供无缝而稳定的视觉体验.6.4.4添加表情的人脸语音动画当在文本“sobadweather”的首尾加上愤怒和眨眼等表情标签后,所产生的人脸动画如图15所示.由此可见,当本系统遇到表情标签时,它能够产生良好的视素和表情同步的人脸动画,该同步性在3DVH带表情说话的时候是至关重要的.6.4.5可视化协同发音模型的比较我们首先构建了具有协同发音现象的数十条文本语句,接着分别结合本文的VCM与Cohen模型,利用这些文本语句来驱动生成人脸动画.Page8图13对“Istewtwocupoftea”采用VCM的合成视素表5是本文VCM与Cohen模型在合成人脸动画的正确率和耗时方面的比较结果.由此可见:相较于Cohen模型,本文VCM在保持正确率的前提下,大大节约了耗时.本文VCMCohen模型6.5对虚拟人头的客观评测综合上述多种输入下驱动3DVH的过程,下面将该3DVH作为一个整体来进行评测.该3DVH在视频输入下驱动生成三维人脸动画的全过程详见如下录像http://staff.ustc.edu.cn/~harryjun/links/facial_motion_tracking_results_图14对“Istewtwocupoftea”中视素NURBSCarphone.wmv;在文本或语音输入下驱动生成三维人脸动画的过程详见如下录像http://staff.ustc.edu.cn/~harryjun/links/text-driven.wmv.从上述录像可见,本文构建的多输入驱动的3DVH较好地满足了作为人机接口的要求.Page9图15添加愤怒表情和眨眼动作的“sobadweather”动画表6是本文3DVH在各种输入驱动下生成动画的时间效率的评测结果.由此可见,它基本满足了实时性的要求.进一步,将其与文献[1]进行比较.从中可见,尽管文献[1]比本文3DVH的时间效率更高,但这是在不同系统配置和不同任务复杂度下比较的结果,故需要进行更详细的综合评测.本文3DVH的每帧动画耗时/s视频输入文本输入语音输入0.080.0450.043与文献[1-3,5,7]中的3DVH相比,本文3DVH具有以下特点和优势:首先,它可由视频、文本和语音多种输入来驱动,从而增加了与人交互的多样性,尽管现在也有3DVH可被多种输入来驱动,但现在还没有将视频、文本和语音这些最自然和有效的人机交互媒介综合起来研究的系统;其次,对于人脸三维运动跟踪,它不仅可以跟踪人脸姿态、缩放和位置,还可以跟踪人脸表情运动,尤其是眨眼幅度检测,从而具有理解人类情感的能力.而现有的许多3DVH只能跟踪人脸的位置或者只能跟踪人脸的全局刚体运动,基本没有理解人类情感的能力.本文3DVH使用在线更新的外观模型来匹配获取人脸三维运动参数,从而不需要大量的离线训练库.而现有的许多3DVH的跟踪性能依赖于这些离线训练库.进一步,本文3DVH深入挖掘了在线外观模型的潜力,即融合多种外观观测量来减少光照和个体相关性对其的影响,根据人脸三维模型和在线外观模型来处理遮挡.为了应对运动滤波中面临的计算盲目和计算量大的问题,本文3DVH基于局部优化和改进重采样来改进粒子滤波,进而获取了高精度的人脸运动估计;再次,对于人脸模型特定化,本文3DVH结合几何变形和动态纹理更新,在只有单个摄像机的情况下,获得了高真实感的人脸三维模型.现有的许多3DVH使用三维扫描仪直接获取人脸三维模型,尽管精度较高,但代价昂贵、耗时且需要复杂的设置;然后,对于普通PC机上的实时三维人脸动画,本文3DVH将参数模型和肌肉模型结合起来,在保持利用肌肉模型从生理结构上描述人脸运动来获得高真实感的同时,利用参数模型大大降低了计算量.而现有的许多3DVH需要在高性能计算机上才能达到实时;最后,对于视素合成,本文3DVH在保持正确率的前提下,采用三音子模型大大降低了可视化协同发音模型的计算复杂度.而现有的许多3DVH忽略了可视化协同发音的影响.6.6对虚拟人头的主观评测对虚拟人头的主观评测分为3步来实施.第1步是确定调查对象.我们的调查对象共34个人,他们的情况如表7所示.从中可见,调查对象的分布具有广泛的代表性.组年龄1.小于20;2.20和30之间;3.大于308/18/8性别1.男性;2.女性籍贯1.东部地区;2.中部地区;3.西部地区7/14/13Page10第2步是建立问卷.表8展示了其中的问题.对于调查对象的回答从“绝对不同意”到“完全同意”分为10级,并且采用Cronbach’salpha测试来验证问卷的内在可信度(测试结果在0.7以上表示问卷是可信的).分组表现力人脸运动跟踪积极期望外貌1.我喜欢虚拟人头的外貌.焦虑度1.与虚拟人头交互时感到舒适.语音合成自然度第3步是基于以上问卷获取调查对象对本文3DVH的可接受度.首先本文3DVH进行文本驱动下的添加表情的人脸动画,文本由带有6种基本表情标签的6段文字组成,然后在调查对象观察完该段动画后,让他们自由地通过摄像机、键盘和麦克风与本文3DVH进行交互,最后让他们填写问卷,回答的最高分是10分,最低分是0分.由表9可见,本系统的得分在7.5以上,这表明本系统能够增强人机交互的友好自然度和有效性.分组表现力人脸运动跟踪积极期望外貌1.我喜欢虚拟人头的外貌.焦虑度1.与虚拟人头交互时感到舒适.语音合成自然度7结论与展望本文面向人机接口研究领域,提出了一个视频、文本和语音多种输入驱动的3DVH方案.对该方案的客观性能测试和主观互动测试,验证了其在人机交互方面的有效性.针对本文的实验结果提出进一步研究方向:(1)将本文3DVH从个人计算机移植到嵌入式系统中;(2)将单种语言(英语)驱动扩展到可以被多种语言驱动,比如中文、日文等;(3)利用跟踪到的人脸运动参数进行表情识别.
