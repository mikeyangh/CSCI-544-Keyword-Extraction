Page1QoS保证的数据中心动态资源供应方法李青1),2),3)李勇1),2),3)涂碧波2)孟丹1),2)1)(中国科学院计算技术研究所北京100190)2)(中国科学院信息工程研究所北京100093)3)(中国科学院大学北京100049)摘要在满足应用QoS的前提下,提高系统的资源利用率,是数据中心资源管理的一个难点,原因在于应用的资源消耗是动态变化的.为了保证应用的QoS,需要实时预测应用的资源需求,并以此为基础动态按需供应资源.已有的资源预测算法可以分为两类:一类需要与应用进行实时交互以监测负载和性能,预测精度高,但扩展性差;另一类只需要在外部监测应用的资源消耗,扩展性好,但是现有算法的预测精度较低.该文针对第二类方法的不足,结合全局及局部的资源消耗变化趋势来改进已有的预测算法.实验证明改进后的预测精确度可以达到95%以上.资源需求的动态变化会导致节点资源无法满足所有应用的QoS,常用的解决方法是采用虚拟机迁移进行负载平衡,但是迁移虚拟机会带来相当大的额外资源消耗.该文提出了一种作业资源匹配算法,对数据中心所有作业进行布局,使不同优先级、不同资源需求及变化规律的作业在节点上混合部署,不仅极大地降低了发生节点资源紧缺的频度,而且实验证明,该算法在相同的资源需求下,还能提高应用的性能.关键词数据中心;混合部署;虚拟化;作业资源匹配算法;资源预测算法;云计算;大数据1引言近年来互联网业务蓬勃发展,业务类型越来越多,应用的资源需求呈现出多样化特征:CPU密集、I/O密集、网络密集等.并且,应用的资源需求还随着时间动态变化,尤其是在线网络应用.如图1所示为某互联网公司一台运行某在线游戏的4核节点连续24小时CPU负载的变化情况,波峰时的资源消耗为波谷时的数倍.为了节省软硬件成本,提高系统资源利用率,数据中心需要合理地调度各类作业并为其分配资源,使得系统资源有序复用,达到负载平衡.然而,应用对资源需求的多样性和动态性给资源管理带来了挑战:既要满足各个应用的服务质量(QoS),又要提高系统的资源利用率.应对这个挑战的难点之一是如何准确实时地预测应用的资源需求,从而动态地为应用供应资源.目前已有的预测技术根据输入可以分为两类:一类是根据应用当前的负载、性能和资源消耗来预测下一个时间周期的资源需求,由于应用的性能指标和负载的类型具有多样性,这类方法需要为每种应用设计不同的监控模块,而且要与应用进行交互以获得应用的当前性能及负载.因而,这类方法的一个显著不足就是通用性和扩展性差.另一类仅仅依据外围监测的资源消耗来预测,已有预测机制有:线性增加、指数下降,大幅增加、小幅下降,服务分级等,这类方法的优点是简单实用,不足是预测机制太简单导致预测精度不高.本文针对第二类方法的不足,结合全局及局部的资源消耗变化趋势进行改进.应对这个挑战的难点之二是如何有效地动态供应资源.数据中心资源管理的目的是整合服务、平衡负载、充分利用资源,不仅需要提高单台节点的资源利用率,还需要平衡整个数据中心的负载.资源需求的动态变化会导致节点资源无法满足所有应用的QoS,造成节点负载不均衡.常用的解决方法是通过虚拟机迁移,将应用迁移到其它负载较低的节点上.但是,迁移虚拟机会带来相当大的额外资源消耗,尤其是网络资源,而且在迁移的过程中,应用的性能也会受到影响.虚拟化是数据中心服务整合的使能技术,依据实现机制可以分为两类:一类是以XEN[1]为代表的Hypervisor类虚拟化技术,另一类是以容器(LinuxContainer①)为代表的操作系统级虚拟化技术.Hypervisor类虚拟化技术由于较强的安全隔离性被广泛应用在公有云等很多场景.由于更多的互联网数据中心主要支撑公司内部业务,是一个私有云环境,不仅要求一定的安全隔离性,还追求优异的性能[2].所以本文所述的资源管理技术使用容器作为资源管理的基本单位,采用两层资源管理框架:全局资源管理器和局部资源管理器.全局资源管理器在作业下发的时候,对数据中心所有作业进行布局,使不同优先级、不同资源需求及变化规律的作业在节点上混合部署,确保数据中心运行时的各类负载平衡,减少节点负载过重发生的频度;局部资源管理器是各个节点的本地资源管理器,根据资源需求预测结果,动态调整本节点上应用的资源配置,优先保证高优先级应用的服务质量.本文的贡献有:(1)实现了一种以容器作为资①LinuxContainer.http://lxc.sourceforge.net/Page3源管理单位的数据中心弹性资源管理框架,能满足互联网应用的多种资源需求;(2)提出了一种作业资源匹配算法,混合部署具有不同优先级、不同类型资源需求的作业,平衡数据中心节点间的负载;(3)提出了一种改进型的资源预测算法,通用性强,准确度高;(4)基于上述预测算法的动态资源供应方法保证了高优先级在线网络应用的服务质量.2相关研究资源管理的单位有3类:以Hadoop①为代表的槽(Slot)管理方式,以EC2②为代表的Hypervisor虚拟机管理方式,以Yarn③为代表的操作系统级虚拟化管理方式(容器),表1描述了这3种资源管理单位的各种特性.槽管理方式的优点是简单,缺点是槽一旦创建,资源配置就不再更改,会导致某些节点上存在资源浪费或某些应用的资源需求得不到满足.Hypervisor类虚拟机管理方式的优点是隔离性好,不足是存在额外的性能开销,由于在虚拟环境中运行,应用程序的性能会受到影响,虚拟机的部署、启动、销毁也需要开销,运行时间短的任务相对付出的额外开销就更大.操作系统级虚拟机的安全隔离性比Hypervisor类虚拟机要差一些,但是对应用的性能影响要小得多,适用于对安全隔离要求不是很高但是对性能要求比较高的场景.隔离性好性能开销较高管理难度较高动态配置可以在线迁移可以适合场景公有云资源需求预测的精确度是影响应用QoS的一个关键因素.已有的预测算法可以分为4个小类[3]:基于启发式的预测算法[4-5],基于机器学习的预测算法[6-7],基于控制理论的预测算法[8-9],基于效用函数的预测算法[10-12].基于启发式的预测算法:在线监测虚拟机的资源使用情况,以此为基础调整虚拟机的资源配置.资源消耗达到一个上限阈值时,增加资源,达到一个下限阈值时,回收部分资源.这类方法的优点是完全不用了解应用的特性,通用性强;缺点是预测方法不够灵活,精确度不高.基于机器学习的预测算法:离线或在线自学习应用的资源消耗、负载变化和性能之间的关系,找到在每个不同的负载点下,使应用的性能达到最优所需要的资源,并记录下来.实际在线运行时根据实时监测到的负载情况和性能指标与记录中的值进行比较,调整资源的配置.这类方法的优点是不需要建立复杂的输入输出模型,能使应用的性能达到最优.缺点是需要一段时间的学习,只能做到大概匹配.基于控制理论的预测算法:建立输入输出模型,以当前虚拟机的资源利用率以及应用的当前性能为输入,得出下一个时刻应该给虚拟机分配的资源.这类方法适用于单一输入单一输出的情况,在有多个输入或者多个输出的情况下,很难描述它们之间的关系.基于效用函数的预测方法:建立资源配置和应用性能指标之间的映射关系.文献[12]中的效用函数描述了网页服务的响应时间与服务器数量之间的映射关系.表2总结了这四种算法的特性.本文采用启发式的资源预测算法,根据资源的使用情况,结合历史数据和用户给出的波峰波谷的变化周期及幅度,预测下一个时间周期的资源需求,比已有的此类预测方法更灵活、更精确.需要监控应用负载否是是是需要监控应用性能否是是是需要获取资源状况是否是否可扩展性好差差差预测精确度低高高高关于数据中心资源管理,已有大量的相关研究.文献[13-14]专注于如何为虚拟机动态分配资源以满足应用的服务需求.文献[15-17]专注于如何迁移虚拟机以提高整个数据中心的资源利用率或节省功耗.文献[18-20]研究动态资源分配以节省功耗.文献[21]研究虚拟机和物理机的映射关系以达到效用最大化的目的.文献[22]主要针对Web应用研究虚拟机与物理节点之间的映射策略,以减少虚拟机迁移的次数.文献[23]关注数据中心虚拟机资源的各种静态配置方案以保证资源分配的公平性或提高资源利用率.文献[7,23]提出了两层的资源管理框架,本地资源管理器检测资源需求,汇报给全局资源管①②③Page4理器,全局资源管理器从整个数据中心的角度来计算每个虚拟机可以获得的资源.其中文献[7]的全局资源管理器的计算原则是系统获得最大的收益(出租相同的资源,获得最大的利润).文献[23]的全局资源管理器的计算原则是取得最优的应用性能.本文也采用两层的资源管理框架:全局资源管理器在下发作业时,结合作业的优先级、资源需求以及资源消耗变化规律等属性特征和数据中心资源负载的分布状况,为作业选择合适的节点,混合部署作业,以减少同类资源竞争,避免节点负载过重,实现无需进行虚拟机迁移就能达到数据中心负载平衡的目的;局部资源管理器动态预测各个应用的资源需求,调整节点上各个容器的资源配置,保证高优先级应用的服务质量.3基于LinuxContainer的资源管理框架基于容器的资源管理系统架构如图2所示.用户通过客户端提交作业.中央管理器是系统的核心,包含作业调度器和全局资源管理器两个组件,主要职责是为作业寻找合适的执行节点.为了增强可靠性,一个数据中心至少配置两个中央管理器,通过ZooKeeper①的领导者(Leader)选举保证只有一个处于活跃状态.系统中除了中央管理器以及运行Zookeeper的节点以外,其余所有节点都是执行节点.每台执行节点上都运行一个局部资源管理器,接收中央管理器下发的任务(一个作业包含若干个任务),并为每个任务创建一台轻量级的虚拟机(LinuxContainer).数据中心中的所有节点共享一个分布式文件系统.用户通过浏览器可以查询作业调度器中的作业所处的状态(等待下发、已下发、正在执行、执行出错)、各个节点的资源使用状况及节点上运行的作业信息.作业调度器接收用户提交的作业,按照优先级排序,每次选择优先级最高的作业,向全局资源管理器申请资源.全局资源管理器返回符合需求的最优N(N小于或等于作业的任务数)台节点.如果N等于任务数,则将作业下发到这N台节点上.但是如果N小于作业的任务数,若作业支持批量下发,则先下发一部分任务到符合要求的节点上,若不支持,选择下一个作业.全局资源管理器有3个功能:监控收集所有节点的剩余资源以及节点上运行的作业信息;接收作业调度器的资源请求,通过作业资源匹配算法为作业寻找符合要求的最优N台节点;协助局部资源管理器处理执行节点资源紧缺的情况.本系统采用数组存储节点上的剩余资源和作业(在等待和正在执行的所有作业)信息,数组中的每一项代表了一台物理节点.采用数组存储信息,不需要使用锁,多个线程可以同时接收多台节点的上报信息.有了每台节点的可用剩余资源和其上运行的作业信息,全局资源管理器就可以使用作业资源匹配算法,寻找满足作业资源需求的节点.然后对所有符合需求的节点按照一定的规则进行排序,选择最优的N台节点.全局资源管理器的另一个职责是协助局部资源管理器处理执行节点资源紧缺的问题.接收到局部资源管理器上报的节点资源紧缺的请求后,寻找拥有足够剩余资源的节点为应用增加实例或迁移虚拟机(寻找方式与作业首次下发时一致).当系统负载达到一个警戒值时,中央管理器将拒绝接收新的作业.运行在执行节点上的局部资源管理器包含两个功能部件:一个负责创建虚拟机,并在这个虚拟机环境中启动执行任务;另一个负责监控各个虚拟机的资源使用状况以及本节点的剩余可用资源,将信息周期性上报给全局资源管理器,并预测各个任务在下一个时间周期所需的资源,以此为基础更改虚拟机的资源配置.若结点的剩余资源无法满足预测值,则剥夺低优先级应用的资源或者向全局资源管理器求助.本系统还提供命令行查询接口、前台页面展示以及告警信息,以便管理员或相关负责人可以实时①http://zookeeper.apache.org/Page5获取数据中心的资源使用状况、作业分布和执行情况,在必要时干预调整资源分配和作业分布.4作业资源匹配算法作业资源匹配算法的作用是为作业寻找合适的执行节点,各个作业具有不同的资源需求特征,所以匹配算法将直接影响作业的分布、应用的QoS和系统的资源利用率.数据中心或云计算中,当节点的资源无法满足所有应用的需求时,一般都通过迁移虚拟机来调整资源和平衡负载.但是虚拟机迁移不但占用网络资源,而且因为需要停机从而也会影响应用的性能.作业资源匹配算法可以使作业的分布更合理,大幅降低虚拟机迁移的次数.为了合理地部署作业,系统需要知道作业的一些特性.用户在向中央管理器提交作业时需告知作业的资源需求(可根据测试或经验分析得出),包括CPU核的个数、内存的大小、网络带宽、磁盘带宽(网络与磁盘带宽用高、低以及无3个等级来表示)以及作业的优先级,是在线作业还是离线作业等特Resource_Requirements(CPU==1核)&&(Memory==1GB)表3作业的资源需求及特征属性Input_Files(可选)Dynamic_RealloctionCandidate_IP(可选)作业资源匹配算法由两部分组成:(1)寻找符合作业资源需求的所有节点;(2)从所有符合要求的节点中,按照排序规则选出最佳的N台节点(N为作业中任务的数量).全局资源管理器根据用户指定的资源需求(CPU和内存的数量,网络带宽和磁盘带宽在排序规则中用)去寻找符合需求的所有节点.如表3所示作业的资源需求为(Memory==1GB)&&(CPU==1核),所有在将来一段时间内(作业的Last_Time),空闲内存大于或等于1GB并且空闲核的个数大于征属性,如表3所示.资源管理器依据这些属性来为作业寻找合适的节点.有些属性是必填的,如果不填,客户端会报错,例如可执行文件的名字、可执行文件所在的位置、一个作业有多少个任务.还有一些属性用户如果不填,系统会给一个默认值,如作业的资源需求(默认给一个最低的资源值:CPU为一个处理器核的20%、内存为50MB、网络I/O和磁盘I/O为无)、作业所属的部门(一个被命名为“default”的部门)、作业的优先级(默认为低)、是否允许系统动态调整资源分配(默认为是)、作业大概需要运行多长时间(默认为常驻型的作业)、作业的资源变化特征(波峰波谷变化的周期,默认不会变化)、是在线服务作业还是离线作业(默认为离线).还有一些是属性可选的(有就填,没有就不用填),例如执行程序时的参数、文件在分布式文件系统中的位置(如果用户需要有额外输入的文件)、节点的网络IP地址(如果用户要求作业下发到特点的节点上).有了这些必填的和可选的属性信息,全局资源管理器下发作业时就可以采取一些策略来控制作业的布局.或等于1的节点均符合需求.由于应用的资源需求动态变化,特别是在线网络应用,因此难以估算一台节点在将来一段时间内的最低剩余资源.局部资源管理器依据用户指定的资源需求变化情况(如表3中的CPU_Period)以及作业资源消耗的历史数据来估算本节点在将来24h内消耗的资源量,并以表4所示的格式周期性上报给全局资源管理器,全局资源管理器根据这些信息就可以找出所有符合资源需求的节点(通过排序算法后返回的节点).如果符合要求的节点数小于任务的数量(Task_NumPage6的值),并且所有任务必须作为一个整体一次性下发,那么向作业管理器报告本次匹配失败,若支持部分下发,则先下发一部分任务(等于符合要求的节点数).表4节点上报的资源资源属性属性值的表达方式CPU{S1,E1,V1,…,Memory{S1,E1,V1,…,作业资源匹配算法好坏的关键在于排序规则,好的排序规则能够满足所有作业的QoS,同时平衡数据中心的负载、提高资源利用率.本文提出的排序算法依据以下5项原则对所有符合需求的节点进行排序:(1)高优先级作业和低优先级作业部署在同一台节点上,在资源紧张时可以剥夺低优先级作业的资源,以满足高优先级作业的需求.(2)不同密集型(CPU密集、网络密集、IO密集)的作业部署在同一台节点上,避免竞争同一类型的资源,影响性能.(3)优先选取存放了作业所需数据文件的节点(数据本地性).(4)负载动态变化且变化特征相似的作业不要部署在同一台节点上,避免无法满足作业处于波峰时的资源需求.上,减少资源抢占时的复杂度(同部门优先).Rank=w1×P_C+w2×T_C+w3×D_L+其中P_C、T_C、D_L、W_C、S_D的值都只有0、1两个取值,分别对应上述5项排序原则,w1至w5表示这5项原则在排序时所占的比重:P_C(priority_compatible,优先级兼容):1表示在一台节点上高优先级作业的数量没有超出阈值,超出则为0,阈值为节点数与高优先级作业数的比值.例如,如果待下发作业为高优先级,若该节点上已有的高优先级作业的数量加上1没有超过阈值则P_C值为1,超过了则为0;如果待下发的作业为低优先级,P_C值为1.T_C(type_compatible,密集型兼容):1表示一台节点上的作业具有不同的密集型,0则相反.例如,如果作业对网络带宽的需求为高,而节点上已经有作业的网络带宽需求也为高,那么T_C的值就为(5)同一个部门的作业尽量部署在一台节点由此得出以下计算优先级的表达式:0,如果没有冲突,则为1.D_L(data_locality,数据本地性):主要针对数据密集型作业,如果作业需要的数据文件存储在该节点上,则为1,否则为0.对于不需要拉取大量数据的计算密集型作业和网络密集型作业,这一项均为1.W_C(workload_compatible,负载波动性兼容):如果波动周期不冲突则为1,否则为0.例如游戏的高峰在晚上,而新闻的高峰在白天,那么如果这两个应用运行在一台节点上,W_C的值为1,而如果两个游戏运行在一台节点上,则W_C的值为0.S_D(same_department,同一个部门):表示如果节点上存在与待下发作业属于同一个部门的作业,则为1,否则为0.将同一个部门的作业放在一个节点上,是为了剥夺资源时尽量不影响其余部门的作业.对于在线网络应用,数据本地性永远成立,作业密集类型兼容比波动性兼容及同一个部门更重要,而波动性兼容又比同一个部门更重要,所以w5的值最小,赋为1,w2的值为2,w4要大于3(1+2),为4.对于离线作业,因为资源消耗平稳,所以波动兼容永远成立,与在线作业一样,可以得出w3的值为2,因为需要保证高优先级在线服务作业的QoS,所以w1要大于9(4+2+2+1),为10.若Rank值小于8,则表示优先级不兼容,同时波动性与密集类型也不兼容,必然会出现在高峰期竞争同一类型的资源,所以作业不能下发到这台节点上.作业资源匹配算法的伪代码表示如下.P_C=0;T_C=1;D_L=0;W_C=1;S_D=0;IF(task->priority==“high”)high_priority=1;node=c_node;//c_node为CPU和内存符合要求的节WHILE(node!=NULL){/目前一台节点上高优先级作业的数量不超过2/IF(high_priority+node->num_high_task<=2)Page7/只要有一项属性有冲突,T_C值就为0/IF((task->network_io==“high”node->/作业依赖的数据文件在该节点上,则D_L为1/IF(task->file_ip==node->ip)D_L=1;/如果作业的所有波动周期都与节点上已有作业的IF(task->num_cpu_period!=0node->FOR(INTi=0;i<task->num_cpu_period;IF(load_uncompatible(task->cpu_period[i],W_C=0;}}/部门用位来表示/IF(task->departmentnode->departments==1)S_D=1;node->score=P_C10+T_C4+D_L2+W_CIF(node->score>=8)v_node.push_back(node);//最终符合要求的节点node=node->next;}//WHILE/将得分大于或等于8的节点按得分高低排序,得分Qsort(v_node,v_node.size(),compare());作业资源匹配算法在为作业寻找符合资源需求的节点时,计算的是节点在将来一段时间的最低可用资源,在理论上满足了节点上所有应用的资源需求.而排序算法又使同一台节点上的作业具有不同的优先级和不同的密集类型,所以即使用户提供的资源需求有误差而使节点的资源无法同时保证所有应用的QoS,也可以通过为支持动态扩容的应用在别的节点增加实例来分担压力,或剥夺低优先级应用的资源,保证高优先级应用的QoS,减少虚拟机迁移的次数.5基于预测算法的资源动态供应如图3所示,一台物理节点上同时运行若干台虚拟机及本地资源管理器,每台虚拟机中运行的应用程序拥有不同的优先级、不同的资源需求、不同的负载变化特征.对每个应用,本地资源管理器周期性地读取各个虚拟机当前的CPU使用状况和内存使用状况,然后根据各个应用资源消耗的周期波动性来估算它们在下一个时间周期的资源需求,并更改资源配置.图4是WorldCup98①网络服务中第55天和第57天在连续24h内,接收到的访问请求量,为便于描述和测试,以10min的访问量压缩到1s的比例,将一天24h压缩到144s.若静态配置资源,以波峰值,一天中有一半以上的时间浪费了大半的资源.若不以波峰值分配,不能满足波峰时的资源需求.所以实时预测应用的资源需求,并以此为基础动态调整资源配置是必要的.除启发式预测算法以外的3种预测算法由于扩展性差,不适合我们的场景.启发式预测算法中的AIMD[6]无法应对访问量的突发性增长,MISD[7]能解决这个问题,但是存在3点不足:(1)预测规则太简单.MISD根据经验设置一个资源值(Wiggle-Room),若资源消耗接近配置,将Wiggle-Room翻一倍,下一个时间周期的资源配置更改为当前配置加上新的Wiggle-Room.若资源配置与消耗的差值大于一个阈值,Wiggle-Room等于(1-α)×Wiggle-Room(α为一个小于1的值,如0.2),下一个时间周期的资源配置为当前的资源配置减去α×Wiggle-Room.如果某一次资源配置与消耗的差值不符合更改配置的要求,而资源消耗又处在上升阶段,那么下一个时间周期的资源配置就无法满足需求;(2)Wiggle-Room增长的幅度太大(指数级),使得资源配置的波动幅度太大;(3)没有将资源的预测①http://ita.ee.lbl.gov/html/contrib/WorldCup.htmlPage8值和分配值分开,各种资源的特性是不一样的,例如CPU,如果需要占用一个处理器核80%的资源,实际上至少分配85%的资源应用的性能才不会受影响.本文针对MISD的这3点不足,分别做了改进:预测时结合资源消耗的变化趋势,而不是只依据当前的资源配置与消耗的差值;Wiggle-Room改为连续两次资源消耗的差值;分离预测值与分配值.称改进后的MISD算法为AMISD(Advanced-MISD)预测算法,由以下几个表达式组成:R_P表示预测值,R_U表示当前的消耗值,R_U_L表示上一个时间周期的资源消耗值,R_A表示下一个时间周期要分配的值,R_X表示某个不确定的值(以保证R_A与R_P的差距在5%到10%之间).系数β、γ的值是当前资源消耗变化对下一个时间段资源分配值的影响因子.当资源消耗上升时,用式(2)计算R_P值;当资源消耗下降时,用式(3)计算R_P值.影响因子β、γ的取值要与资源配置大幅增加小幅减小的思想吻合.本文经过反复试验总结得出:应用负载处于波谷时,β的取值为1;处于波峰时,β的取值为2;γ的取值始终为0.2;此时AMISD算法的预测精度最为理想.资源消耗上升时,若应用负载处于波谷阶段,如图4中的第1秒到第90秒,资源消耗的波动幅度不大,资源增加幅度也不需很大,则用β=1来预测下一个阶段的消耗值.相反,若处于波峰阶段,如图4中的第90秒到第130秒,存在访问量突发大幅增加的情况,资源消耗的波动幅度大,则用β=2来预测,以应对资源消耗的突发性增加.资源消耗下降时,如果分配值与消耗值的差距达到了一个阈值,如30%,则使用式(3)预测下一个时间周期的资源消耗值.资源配置的下调幅度很小,因为下一个时间周期的访问量可能会回升.最后均使用式(4)计算对应的资源配置值,使R_A与R_P的差值在5%到10%之间,如果现在的配置值与预测值的差在5%到10%之间,则不用更改配置值,以减少更改配置的次数.基于AMISD预测算法的本地资源动态供应流程如图5所示.预测出一个任务在下一个时间周期的资源配置值后,若节点上的剩余资源无法满足需求,则查看本节点上正在运行的作业是否支持扩容.若有,则请求全局资源管理器寻找合适的节点(遵循作业初次下发时的寻找规则)启动一个支持扩容的作业的实例映像,以分担负载压力.若无,则剥夺低优先级应用的部分资源(剥夺剩余运行时间最长的作业的资源,最多可以剥夺到该作业只剩最低资源配额,如果不够则继续剥夺下一个应用,直到没有资源可以剥夺为止).若没有优先级比其更低的应用或剥夺低优先级的资源仍无法满足需求,连续出现3次后(次数可以配置,高优先级作业和低优先级作业可以配置不同的值),再一次向全局资源管理器上报,以迁移出部分虚拟机.如果无法找到合适的节点用来扩容或迁移虚拟机,则向管理员等相关负责人发出告警信息.全局资源管理器在为作业选择执行节点时遵循的5项原则,保证了一台节点上最多只有两个高优先级的在线网络应用.而除在线网络应用以外,大多数应用的资源消耗平稳,所以需要调整资源配置的应用不多,减轻了局部资源管理器的负担.作业资源匹配算法为作业选择的是在正常情况下资源足够的节点,发生本地资源紧缺的概率很低,减轻了全局资源管理器的负担.本地资源紧缺大多由网络应用引起,而网络应用可以通过LVS①启动实例进行扩容,即使找不到合适的节点,也可以通过剥夺低优先级应用的资源来满足高优先级应用的需求,所以需要迁移虚拟机的概率很低.6实验评价本节通过两个实验验证结合作业资源匹配算法①http://www.linuxvirtualserver.org/Page9和资源预测算法,能在保证应用QoS(尤其是高优先级应用)的前提下提高数据中心的资源利用率.6.1实验平台和应用负载本实验的硬件平台,是由6台相同服务器组成的机群.其中,1台节点作为中央管理器,其它的节点作为任务执行节点.每台服务器配置英特尔4核处理器,主频为2.53GHz,4GB物理内存.本文使用的应用负载如下:(1)WorldCup98,一个真实的trace,记录每个客户请求的访问时间,客户端使用Httperf①向服务器发送Http请求,每秒钟发送请求的数量遵循WorldCup98的记录,服务端使用ApacheWeb②,ApacheWeb为在线网页服务,高优先级.(2)TPC-W③,模拟在线购书系统,实验时客户端启动500个模拟用户,服务端用Tomcat④.该应用也为在线网络应用,高优先级.(3)Memcache⑤,内存数据库,内存密集型应用,无磁盘需求.(4)计算Pi的值,计算密集型应用.(5)Mencode⑥,视频编码软件,计算密集型应用.(6)Gzip⑦,文件压缩,磁盘IO与CPU密集型的应用,突出的特点是磁盘IO的需求量特别大.(7)Log_Analysis,日志分析,分析WorldCup98的trace,统计服务器在1s内处理的请求数.6.2作业资源匹配算法效果评价首先评价作业资源匹配算法对资源利用率及应表5各个应用的资源需求ApacheWeb高TPC-WMemcache低GzipMencode高PiLog_A图7展示了各个任务独占一台节点以及与别的任务共享一台节点时的执行性能.从图中可以清晰的看出,TPC-W不宜与ApacheWeb或Gzip运行在一台节点上,Memcache不宜跟ApacheWeb或TPC-W在一起运行等等.依据图6与图7展示的结果,表6中每一大列左边的小列描述了每两个作业是否适合运行在一台节点上.根据式(1)来评价两个作业是否适合运行在一台节点上.评价的结果在表6中每一大列右边的小列中,除了Mencode、Gzip作为行,Memcache作为用性能的影响.为作业选择节点的方法有3种:(1)随机选择若干个节点,优势是简单;(2)尽量将作业集中在部分节点上,目的是提高资源利用率、节省功耗;(3)尽量将作业分散开来,目的是为了充分利用系统的资源,保证作业的QoS.表5给出了各个应用的资源需求以及优先级,对在线网络应用还给出了负载波动周期.图6展示了在各种情况下Http请求的响应时间.从图中可以看出当ApacheWeb独占一台节点,或者分别与Memcahe、TPC-W、Mencode&Log&Pi运行在一台节点上,在负载较轻时,客户端请求的响应时间均在1ms左右.但是当ApacheWeb与Gzip运行在同一台节点上时,Http请求的响应时间频繁的超过100ms,这是因为Gzip对磁盘I/O的需求量特别大,影响ApacheWeb读取文件的速度,所以ApacheWeb服务不宜与Gzip应用运行在一台节点上.内存列的时候,Mencode的实验测试性能与式(1)评价的结果不相符以外(受cache命中率的影响.对用户来说,cache对应用性能的影响很难估计,所以暂时忽略这一影响),其余都是相符的,分值越高表明越适合运行在一台节点上.①②③④⑤⑥⑦Page10表6作业两两组合ApacheWebTPC-WMemcacheGzipMencodePiApacheWebTomcat×3Memcache×15√15Gzip√15√15×19Mencode√15√19×19×15Pi√19√19√19√19√19Log_A√15√19√19×15√15√19作业下发时若遵循作业资源匹配算法,则不论作业到达的先后顺序,均可以保证不适宜运行在一起的作业不会被下发到一台节点上.若采用分散或随机策略来下发作业,作业的分布状况与作业的提交顺序及数据中心的节点数量有关.如果采用集中策略,作业的分布状况只与作业的提交顺序有关.在这3种策略下,任何两个作业都有可能被部署在同一台节点上.将作业集中起来需要√19√19√19√19的节点数量能达到最少,但不能保证性能,将作业尽量分散开来,虽然能保证性能,但是使用了大量的节点,资源利用率低.在本实验中,我们的算法在最坏情况下会比集中下发策略多占用一台节点,但能保证各个应用的性能,尤其是高优先级的应用.6.3资源预测算法精确度评价资源消耗具有明显波动性的典型应用是在线网络应用.本实验选择WorldCup98中具有代表性的两天作为客户负载:WC-55与WC-57.WC-55是访问量较大的代表、WC-57是访问量较小的代表,虽然它们的访问量不同,但是波动周期相同.通过预测CPU消耗来比较MISD与AMISD的预测精度.为了更好的说明问题,将ApacheWeb应用的所有线程绑定在一个处理器核上.图8和图9分别表示在以最大静态值配置资源(独占一个核)、AMISD及MISD预测算法动态配置资源,WC-55和WC-57作为客户端负载时的响应时间.在负载较轻时,MISD(其中第2个参数为0.4)预测算法和AMISD预测算法,都能达到与静态最大配置时几乎一样的客户响应时间,但是在负载较重时,AMISD的响应时间明显低于MISD.Page11以MISD算法动态配置资源,当WC-55作为客户端负载时,在117s的时候,响应时间出现了一个毛刺,当WC-57作为客户端负载时,在99s和138s的时候出现了两个毛刺,如图9所示.这是由于MISD预测算法的第1个不足导致的:MISD根据消耗与配置的差值大小决定是否更改下一个阶段的配置.可能出现在上升阶段时,由于某一次的上升幅度不是很大,而没有达到更改配置的条件,导致资源配置无法满足下一个时间周期的资源需求,从而出现毛刺.AMISD改进了这个不足点,避免了出现毛刺的情况.将CPU分成1000份,图10与图11分别描述了当WC-55与WC-57作为负载时,AMISD的资源预测值和实际消耗值,图12描述了预测值与实际消耗的差.图13与图14分别描述了当WC-55与WC-57作为负载时,MISD的资源预测值和消耗值,图15描述了预测值与实际消耗的差.在图12中,除了资源消耗突然从上升阶段转为下降及从下降阶段转为上升,且变化幅度较大时,预测值与消耗的差大于50、小于-50以外,其余均在-50到50以内,所以AMISD的预测精度可以达到95%以上.在图15中,除了资源消耗趋势突然发生变化以外,资源的预测值与消耗的差几乎都在50到100之间,比AMISD的预测精度低5%.对比图10与图12及图11与图13,可以看出AMISD修改资源配置的次数也远小于MISD.7结束语互联网应用呈现资源需求多样化和动态化的特征,为了保证应用的服务质量和数据中心负载平衡,本文提出并实现了QoS保证的数据中心动态资源供应方法:资源匹配算法将不同优先级、不同资源需求、不同负载变化规律的作业进行混合部署;基于改Page12进型预测算法的本地资源供应方法为应用按需动态供应资源.实验证明,作业资源匹配算法使应用的服务质量接近作业分散下发时的水平,资源利用率接近作业集中下发时的效果.资源预测算法的精确度能达到95%,基于该算法的动态资源供应方法既保证了高优先级在线网络应用的服务质量,也提高了单台节点的资源利用率,使一台节点上可以运行更多的作业.在应用多样化的数据中心,分布式文件系统中的数据分布影响应用的资源分配效果,特别是数据密集型应用.下一步,我们将研究动态资源供应方法与数据分布策略之间的关系.致谢本文的工作是在腾讯公司研究院的资助下与腾讯公司基础架构部共同开发完成的,在此感谢腾讯公司资源管理系统研发小组的陈军、罗韩梅、林恬、窦芊、李博、赵森、严俊明、彭亮等!
