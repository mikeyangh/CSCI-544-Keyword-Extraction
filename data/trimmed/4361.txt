Page1基于矩阵指数变换的边界Fisher分析何进荣1)丁立新1)崔梦天2),3)胡庆辉1),4)1)(武汉大学计算机学院软件工程国家重点实验室武汉430072)2)(西南民族大学计算机科学与技术学院成都610041)3)(电子科技大学计算机科学与工程学院成都610000)4)(桂林航天工业学院信息工程系广西桂林541004)摘要边界Fisher分析是一种经典的有监督线性降维方法,被广泛用于高维数据的模式分类.由于边界Fisher分析算法中涉及到矩阵求逆的运算,在数值计算中会产生矩阵的奇异性问题,尤其当样本的个数小于样本的维数时,导致所谓的“小样本问题”.采用主成分分析方法对样本数据进行预处理可以克服奇异性问题,然而可能会损失样本的某些判别信息.针对此不足之处,根据矩阵指数的非奇异性,对边界Fisher分析中的散度矩阵进行矩阵指数变换,从而克服了矩阵求逆中的奇异性问题.理论分析表明,该方法等价于零空间上的边界Fisher分析,有效利用了类内散度矩阵的零空间上的信息,因此其判别能力得到了增强.数据可视化和人脸识别实验表明,该方法可以有效挖掘样本中潜在的判别特性,提高分类性能.关键词边界Fisher分析;小样本问题;矩阵指数;人脸识别;数据挖掘1引言降维通常作为数据挖掘和模式识别任务的预处理阶段,可以有效克服“维数灾难[1]”,生成判别性更好的特征,在降低数据分析后续处理的计算复杂度的同时,提高了分类或者聚类算法的性能.随着流形学习的发展,许多学者提出了一系列非线性降维方法,包括局部线性嵌入(LocallyLinearEmbedding,LLE)[2]、等度规特征映射(IsometricMapping,ISO-MAP)[3]、Laplacian特征映射(LaplacianEigenmap)等[4],这些算法均假设高维样本数据近似地分布在一个低维的流形上,被用于挖掘样本分布中潜在的局部结构信息.然而流形学习方法属于非线性降维方法,在应用中会遇到“外样本”(Out-of-sample)问题[5],即高维数据与低维表示之间的映射关系是隐式的,对于新的测试样本,无法直接通过映射获得其低维表示.线性降维方法可以克服“外样本”问题,因此在实际应用中得到了广泛的关注.经典的线性降维方法有主成分分析(PrincipalComponentAnalysis,PCA)[6]和线性判别分析(LinearDiscriminantAnalysis,LDA)[7].PCA通过寻找使样本方差达到最大的投影方向,从而获得样本的低维表示;LDA利用样本的类别信息,以类间散度最大和类内散度最小为优化准则,寻找异类样本可分性最好的投影方向.LDA假设同一类的数据是高斯分布的,然而大多数来源于实际生活中的数据并不服从高斯分布,此时类间散度就不能够很好地反映异类数据的可分性.借鉴流形学习的思想,局部保持投影(LocalityPreservingProjection,LPP)[8]考虑了训练样本之间的局部特性,即高维空间中相邻近的样本,其低维表示也应该是相邻近的,由此得到的投影矩阵可以看作是流形上的Laplace-Beltrami算子的特征函数的最优线性近似.2005年,Yan等人[9]提出了流形学习的图嵌入框架,并同时考虑了训练样本的局部信息和类别信息,在图嵌入框架下,建立了一种非参数的判别学习算法———边界Fisher分析(MarginalFisherAnalysis,MFA).MFA不需要对样本的分布做任何假设,通过本征图和惩罚图来分别表示训练样本之间的局部同类关系和局部异类关系,然后最大化局部类间散度,同时最小化局部类内散度,以此增强投影之后低维数据的可分性.类似于基于张量的降维方法,Xu等人[10]提出了基于矩阵的MFA算法,可直接处理灰度图像等二维数据,并成功应用于人体姿态识别问题中.根据LDA中散度矩阵的定义可知,每一类的中心反映了数据的全局信息,Liu等人[11]提出了球面MFA算法,通过引入类中心点之间的邻接关系,构造了中心点间的散度矩阵,于是将数据的全局信息和局部信息融入到MFA算法中.然而,在数值计算中,LDA、LPP和MFA等算法都会遇到矩阵的奇异性问题,这种情形通常发生在训练样本的个数小于其维数时,因此又被称为“小样本”(SmallSampleSize)问题[7].解决“小样本”问题通常有两种思路:一是在优化建模时,采用相加或相减的形式[12-13],避免出现矩阵求逆的运算;二是采用PCA方法对原始数据进行预处理,去除那些特征值为0或者特征值相对贡献率较小的投影方向,此时得到的投影样本维数较低,以此来保证后续降维算法中的散度矩阵是非奇异的.这种方法虽然计算简单,但是可能会损失原始样本中的一些对后续分析有用的信息[14].2010年,Zhang等人[15]提出了基于矩阵指数的广义判别分析(ExponentialDiscriminantAnalysis,EDA),用于解决LDA算法中的奇异性问题,该方法通过距离扩散映射将散度矩阵变换到一个新的空间中,从而提高了分类的准确性.由于矩阵指数总是非奇异性的,将原始数据的散度矩阵经过矩阵指数运算之后,在克服数值计算中矩阵奇异性问题的同时,不会导致原始样本中的信息损失.根据矩阵指数的思想,Wang等人[16]提出了基于矩阵指数的LPP算法(ExponentialLPP,ELPP).尽管EDA和ELPP都可以克服奇异性问题,然而它们仍然继承了LDA和LPP算法的本质特Page3一般的判别分析方法.直接应用于小样本情形.性,即EDA忽略了数据的局部结构,ELPP没有利用训练样本数据的类别标号.为了克服以上算法的不足,本文提出了基于矩阵指数变换的MFA算法(ExponentialMFA,EMFA),且EMFA算法具有以下优点:(1)可以有效克服计算中的奇异性问题,从而(2)无需对数据分布做任何假设,EMFA是更(3)不同类别样本之间的距离被放大,因此经过EMFA算法进行降维之后的样本分类性能更好.本文所用到的基本概念和相关方法介绍在第2节中给出;第3节详细介绍文章的主要工作,即指数边界Fisher分析,并对该算法做了相应的理论分析;第4节中的数据可视化和人脸识别实验证明EMFA算法的有效性;最后总结全文.2相关概念和方法简介本节对矩阵指数和边界Fisher分析做简单的介绍.2.1矩阵指数的定义及性质义为定义1[17].若犃是n×n矩阵,其矩阵指数定exp(犃)=I+A+1矩阵指数可以看作是如下的一个非线性映射,即矩阵指数变换:矩阵指数具有以下性质:性质1.若λi和狏i分别表示矩阵犃的特征值和特征向量,则其矩阵指数exp(犃)的特征值为eλi,特征向量为狏i,其中i=1,2,…,n.性质2.若犃是对称矩阵,则exp(犃)是正定性质3.exp(犃)是非奇异的,且矩阵.性质4.若|犃|表示矩阵犃的行列式,tr(犃)表示矩阵犃的迹,则性质5.若犅是非奇异矩阵,则关于矩阵指数计算的数值方法可参见文献[18].本文采用缩放平方法(ScalingandSquaring假设给定数据矩阵犡=x1,x2,…,x{Method)[19]来进行计算.2.2边界Fisher分析其每一列表示一个训练样本,每一行表示一个特征,训练样本对应的类别标号为l(xi)∈犆,这里犆={c1,c2,…,cm}表示样本标号的集合.训练样本数据之间的内在关系可以通过无向加权图G={X,犠}来表示,其中图的结点集合X表示训练样本数据,权重矩阵犠表示数据结点之间的某种相似性度量.图G的Laplacian矩阵犔定义为这里犇是一个对角矩阵,且犇ii=∑n于是训练样本之间的类内紧密性和类间可分性可以采用本征图G+={X,犠+}和惩罚图G-={X,犠-}来分别表示.其中,权重矩阵的定义分别为此处N+k1(i)表示与训练样本xi同类的k1个最近邻结点的下标集合,N-k2(i)表示与训练样本xi异类的k2个最近邻结点的下标集合.为了获得训练样本数据的低维表示yi∈Rr×n(r<d),我们引入投影矩阵犞∈Rd×r,即yi=犞Txi.在投影之后的低维空间中,训练样本之间的类内紧密性和类间可分性可以分别表示为为了使样本的低维表示更有利于分类,我们希望其类间可分性越大越好,同时类内紧密性越小越好,于是类似于线性判别分析,边界Fisher分析最大化下面的目标函数为其中,犔-=犇--犠-和犔+=犇+-犠+分别是惩罚Page4图和本征图所对应的Laplacian矩阵.由于上述的迹比优化问题是非凸的,没有解析形式的全局最优解,通常将其近似转化为如下的比迹优化问题:J1(犞)=tr((犞TX犔+犡T犞)-1犞TX犔-犡T犞)(6)J2(犞)=犞TX犔-犡T犞由于J1(犞),J2(犞)是凸的,可以通过求解广义特征分解问题X犔-犡Tvi=λX犔+犡Tvi来得到最优解[20].边界Fisher分析考虑了样本的类别标号,可以看作是局部保持投影的有监督版本,保持了原始样本间的判别信息和局部结构.另外边界Fisher分析也可以看作是线性判别分析的局部化,即通过构造本征图和惩罚图定义了局部类内散度矩阵犛w和局部类间散度矩阵犛b:当训练样本的个数少于特征维数,即n<d时,局部类内散度矩阵犛w是降秩矩阵,即不可逆,此时便无法直接对比迹优化问题(6)进行求解.在许多实际应用问题中,这种情形很普遍,例如在图像识别中,我们获取到的训练图像的个数总是远小于每张图像的像素的个数.解决奇异性问题的一般方法是对原始样本首先通过主成分分析方法进行预处理,以此来保持边界Fisher分析中类内散度矩阵的可逆性.然而,原始训练样本的局部类内散度矩阵的零空间中也可能包含有判别信息,因此主成分分析方法预处理可能会损失样本中潜在的一些判别信息,降低分类性能.3指数边界Fisher分析根据矩阵指数的性质3,本文采用矩阵指数的方法来克服边界Fisher分析中的局部类内散度矩阵的奇异性问题.下面首先介绍基于矩阵指数变换的边界Fisher分析的算法步骤,其次对其做理论分析.与MFA的优化目标(5)类似,EMFA算法的目标函数为由于矩阵的迹是矩阵特征值的和,而行列式的值等价于矩阵特征值的乘积,类似于式(7),为了计算的方便,EMFA准则又可以近似转换为下面的行列式比值形式:E(犞)=犞Texp(犛b)犞J⌒其中Λb,Λw分别是由犛b,犛w的特征值构成的对角矩阵,犝b,犝w分别由Λb,Λw对应的特征向量组成.问题(11)的解可通过下面的广义特征分解得到[20]:令犞=v1,v2,…,v[根据矩阵指数的性质2,对exp(犛w)进行Cholesky分解,令exp(犛w)=犚T犚,其中犚是上三角矩阵.广义特征值问题(12)可以变换为标准的对称特征值问题3.1算法步骤EMFA算法的步骤总结如下.算法1.EMFA算法.输入:数据矩阵犡,类别标号以及目标维数r输出:低维表示Y1.数据规范化.将所有样本的每一维特征归一化为均值为0,方差为1.2.根据式(1)和式(2)分别计算权重矩阵犠+和犠-.3.根据式(8)和式(9)分别计算局部类内散度矩阵犛w和局部类间散度矩阵犛b以及它们的矩阵指数exp(犛w),exp(犛b),为了防止数值计算溢出错误,利用F范数将其规范化.4.计算投影矩阵犞=[v1,v2,…,vr].其中vi是矩阵(exp(犛w))-1exp(犛b)的第i个最大特征值(此处需保证所选取的特征值大于1,具体理由见3.2节定理1)对应的特征向量.5.将投影矩阵犞进行Gram-Schmidt正交化.6.计算yi=犞Txi,获得低维表示Y.由于EMFA算法没有采用PCA预处理,因此散度矩阵维数较高,且需要计算矩阵指数,其时间复杂度为O(d3),这里d为样本数据的维数.因此计算的时间代价高于MFA算法.3.2理论分析准则(5),投影矩阵犞的判别能力定义为定义2.给定训练样本数据X,根据迹比优化散度矩阵的迹可看作是某些样本点之间的距离度量的总和.假设投影之后的低维数据的类间散度矩阵为犛⌒们的特征值分别为λ-i和λ+i,则tr(犛⌒注意到,当类间散度矩阵犛⌒影之后的不同类数据之间的距离就越大,反之亦然.因此,当ρ(犞)越大,意味着犛⌒大,犛⌒w的特征值要尽可能的小,即经过投影之后,同Page5类样本数据之间的距离越小,异类样本数据之间的距离越大,此时数据之间的分类性能就越好,即投影矩阵犞的判别能力越强.准则具有更强的判别能力.定理1.EMFA准则下的投影矩阵犞比MFA证明.根据定义2及式(14)、式(15),MFA准则下的投影矩阵犞的判别能力为ρMFA(犞)=tr(犞T犛b犞)又由矩阵指数的性质1,EMFA准则下的投影矩阵犞的判别能力为ρEMFA(犞)=tr(犞Texp(犛b)犞)由于低维空间中异类样本之间的距离大于同类样本之间的距离,则当λbi>λwi>1时所以即λb1+λb2+…+λbnλw1+λw2+…+λwn<eλb根据上述证明过程可知,EMFA算法相当于将原始样本变换到一个新的空间中,此时样本之间的距离均被放大,然而与同类样本相比,异类样本之间的距离被放大的尺度更大,这种现象称为距离扩散效应.经过矩阵指数变换之后,异类样本之间的边界被放大,从而提高了样本的可分性.定理2.EMFA准则(11)等价于最大边界准则(MMC)[21].证明.根据行列式的性质,式(11)可以被改写为E(犞)=犞Texp(犛b)犞J⌒由式(14)和式(15)可得J⌒于是EMFA准则(11)等价于最大化下面的MMC准则:定理3.EMFA准则(11)等价于零空间MFA.证明.根据矩阵指数的性质2,exp(犛w),exp(犛b)均为正定矩阵,假设广义特征值问题的特征向量为犞=[v1,v2,…,vn],且关于exp(犛b)满足正交性[22],即犞Texp(犛w)犞=I,此时式(11)可以被改写为该问题等价于类似于零空间LDA[23],模型(16)称为零空间MFA.定理3表明,EMFA可以有效利用犛w的零空间中的信息.4实验结果为了验证EMFA算法的有效性,我们将其应用于数据可视化和人脸识别等领域,并与PCA、LDA、EDA、ELPP、MFA等算法进行比较.实验中的参数均由经验给出,在LPP、MFA和EMFA中,权重矩阵均由Gauss热核函数生成,其中参数t设定为训练样本之间欧氏距离的最大值,近邻参数k设定为L-1,同类近邻参数k1设定为L-1,异类近邻参数k2设定为2×k1.其中,为了克服计算中的奇异性问题,LDA和MFA均采用PCA进行预处理,并设置主成分贡献率为0.95.4.1数据可视化为了比较EMFA算法的性能,我们将其用于手写数字图像的可视化,即将所有图像投影至2维平面上,然后观察属于异类样本点的分布.实验中选用MNIST手写数字数据集①,每个数字选取了200个样本,共2000张黑白图像,每张图像的大小被裁剪为28×28个像素,图1显示了MNIST数据集中的部分样本.不同算法的可视化效果比较如图2所示.从图2可以看出,PCA得到的是最大方差方向的投①http://yann.lecun.com/exdb/mnist/Page6影,但是没有利用类别标记信息,因此不同数字之间的区分性并不明显;LDA和MFA只能区分6、7等具有明显差异的数字;相比之下,基于矩阵指数思想的EDA、ELPP和EMFA区分性更好,但是EDA不能区分3与6、4与5、6与8、5与8、7与9等数字,ELPP可以正确区分6、8、7、1等数字,但其他数字完全无法区分,EMFA可以区分0、3、4、6、9等数图2MNIST数据集可视化效果比较4.2人脸识别4.2.1数据集及参数设置不同光照、姿态和表情等因素变化下的人脸图像可以近似看作是嵌入在高维空间中的低维流形,因此可以首先采用降维算法将高维数据投影至低维的子空间中,然后进行分类,实验中采用最近邻分类器,以保证实验结果比较的公平性.为了便于数值处理,实验中所选用的人脸图像均采用灰度图像,且以眼睛坐标对齐,经过裁剪和缩放至合适的大小,人脸数据集说明如表1所示.ORL数据集①由剑桥大学AT&T实验室创建,每个人脸有10张图像,包括了姿态,表情和面部饰物的变化,部分样本图像如图3字,但是无法区分7和5、8和9等数字.与LDA、MFA相比,EDA和EMFA投影后的样本更加分散,这表明矩阵指数变换对样本之间的距离具有扩散效应,由于异类样本间的距离大于同类样本间的距离,因此经矩阵指数变换之后,类间距离的扩散尺度大于类内距离的扩散尺度,于是异类样本之间的边界被放大了.(a)所示;Yale数据集②由耶鲁大学计算视觉与控制中心创建,每个人脸有15张图像,包括了光照(如正面、左侧和右侧),是否戴眼镜,人脸表情(如正常、高兴、悲伤、困乏、惊讶和眨眼)等因素的变化,部分样本图像如图3(b)所示;UMIST数据集③包含了每个人从侧面到正面的多张图像,具有典型的流形结构,部分样本图像如图3(c)所示.由表1可以看出,实验数据集中样本的维数均高于样本的个数,属于典型的小样本问题.名称ORL32×32400Yale32×32165UMIST56×46575实验中针对每个人随机选取L(L=3,4,5)张图像作为训练集,其余的图像作为测试集,此时训练集中样本个数为L×c.当L给定时,重复进行20次随机划分,并报告各个降维算法在不同的嵌入维数(2维至50维)上的平均识别率及对应的标准差等信息.①②③Page74.2.2识别效果对比为了验证EMFA的有效性,本文分别在表1所示的3个人脸数据集上进行人脸识别实验.当训练集中的各类别样本数L=3,4,5时,各个降维算法图4ORL数据集上不同约简维数下的最优识别率比较图5Yale数据集上不同约简维数下的最优识别率比较图6UMIST数据集上不同约简维数下的最优识别率比较表2ORL数据集降维后最优识别率比较/%方法3个训练样本4个训练样本5个训练样本PCA77.70±2.23(44)83.75±2.81(45)88.78±1.56(47)LDA86.52±1.89(39)92.06±2.15(39)93.95±1.57(39)EDA88.39±2.40(46)92.94±1.92(45)94.80±1.26(46)ELPP72.32±4.49(50)77.31±4.94(50)77.55±2.42(50)MFA89.02±2.22(46)93.02±2.36(50)94.55±1.72(50)EMFA89.98±2.00(42)94.29±1.99(32)95.90±1.47(36)表3Yale数据集降维后最优识别率比较/%方法3个训练样本4个训练样本5个训练样本PCA53.83±4.45(44)55.86±2.68(39)59.39±3.25(29)LDA62.17±4.82(14)71.38±4.53(14)76.78±3.60(14)EDA58.33±4.76(50)61.19±3.30(50)64.50±3.74(50)ELPP34.00±6.53(37)34.52±6.59(43)41.83±7.34(19)MFA65.46±3.96(16)72.52±3.53(16)78.00±3.67(21)EMFA67.42±4.50(26)74.14±4.36(17)77.78±3.90(18)在不同数据集上的平均识别率随着嵌入维数的变化而变化,结果如图4、图5和图6所示.各个数据集上的最优识别结果(包括平均识别率及其对应的标准差,最优嵌入维数)如表2、表3和表4所示.表4UMIST数据集降维后最优识别率比较/%方法3个训练样本4个训练样本5个训练样本PCA71.53±4.01(50)78.00±3.15(48)82.83±2.02(49)LDA80.32±4.34(15)85.15±3.42(18)89.63±2.17(18)EDA85.40±3.16(40)89.80±3.14(34)93.14±2.04(41)ELPP67.33±3.82(50)76.69±4.18(50)83.42±2.14(45)MFA83.03±4.25(22)86.74±3.15(26)90.98±1.98(27)EMFA85.18±3.60(35)90.36±2.68(30)93.84±1.76(28)在ORL数据集上,LDA、EDA、MFA和EMFA都取得了大致相当的识别结果;Yale数据集比ORL数据集人脸的差异更为复杂,当L=3时,EMFA明显优于其他算法,当L增加时,EMFA与MFA、LDA最优识别率差别不大.当训练集样本个数较少Page8时,样本中蕴含的差异信息不足以反映出不同类样本之间的判别信息,为了充分利用训练样本中的判别信息,需要同时考虑样本之间的相似信息和差异信息,由于EMFA算法保留了类内散度矩阵的零空间中的信息,即同类样本之间的相似信息,因此其分类性能更好.由于训练集中每类样本个数较少,不能充分反映出流形结构,因此ELPP在ORL和Yale数据集上效果都不理想.UMIST数据集具有典型的流形结构,EMFA的最优识别率仍然高于其他算法.从表2、表3和表4中可以看出,EMFA在多数情形下最优识别率均高于其他算法,且方差较小,取得最优识别率的维数也相对较低,从而说明EMFA算法的判别性能高于其他算法.其中,当L=3时,PCA、LDA、EDA、ELPP、MFA和EMFA算法在ORL人脸数据集上的前10个最佳判别特征脸(取投影矩阵犞中前10列特征向量)如图7所示.与其他算法的特征脸相比,EMFA特征脸中蕴含了更多的细节信息.图7ORL人脸数据集上前10个特征脸(L=3)另外,本文对各个算法的时间消耗进行了比较,所选用的数据集为Yale,对每张人脸随机选取5个样本作为训练集,嵌入维数设置为50,计算时间如表5所示.实验采用个人计算机,处理器为英特尔双核3.16GHz,内存为4GB,操作系统为64位Windows7,所有算法采用MATLAB2010b编程实现.从表5中可以看出,基于矩阵指数变换的算法时间消耗远高于不采用矩阵指数变换的算法.实验中也发现,在EMFA计算中,大约69%的时间消耗在矩阵指数的计算上,30%的时间消耗在矩阵的特征分解上.这也是该类算法在应用中亟待解决的问题之一.表5Yale数集上(犔=5)各算法时间消耗比较/ms方法PCALDAEDAELPPMFAEMFA时间消耗36.2642.013063.203584.30192.522580.505结论为了克服MFA算法在实际应用中遇到的小样本问题,EMFA算法对MFA算法中的散度矩阵进行矩阵指数变换,然后再通过广义特征分解进行求解,由此得到的低维表示既克服了矩阵求逆中的奇异性问题,又继承并发扬了MFA算法的优点.经过矩阵指数变换之后的散度矩阵具有更好的判别性能,它等价于零空间上的MFA算法,由于类间距离的放大尺度大于类内距离的放大尺度,从而异类样本之间的边界更加明显.理论分析和对比实验表明,EMFA算法可以充分利用原始训练样本中的内在信息,与MFA相比,具有更好的判别性能.然而,由于EMFA算法涉及到矩阵指数计算,而矩阵指数计算是计算数学中的一个挑战性课题,如何提高计算效率仍然是值得研究的重要问题.致谢在此,向对本文工作予以帮助和建议的老师和同学表示感谢,并向对本文工作中的不足之处提出宝贵评审意见的审稿专家表示衷心的感谢!
