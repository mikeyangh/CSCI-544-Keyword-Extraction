Page1基于GPU的稀疏矩阵Cholesky分解邹丹窦勇郭松(国防科学与技术大学计算机学院长沙410073)摘要稀疏矩阵Cholesky分解是求解大规模稀疏线性方程组的核心算法,也是求解过程中最耗时的部分.近年来,一系列并行算法通过图形处理器(GPU)获得了显著的加速比,然而,由于访存的不规则性以及任务间的大量数据依赖关系,稀疏矩阵Cholesky分解算法在GPU上的计算效率很低.文中实现了一种新的基于GPU的稀疏矩阵Cholesky分解算法.在数据组织方面,改进了稀疏矩阵超节点数据结构,通过超节点合并和分块控制计算粒度;在计算调度方面,将稀疏矩阵Cholesky分解过程映射为一系列的数据块任务,并设计了相应的任务生成与调度算法,在满足数据依赖性的前提下提高任务的并行性.实验结果表明,该算法能够显著提高稀疏矩阵Cholesky分解算法在GPU上的实现效率,在单个GPU上获得了相对4核CPU平台2.69~3.88倍的加速比.关键词稀疏矩阵;Cholesky分解;GPU1引言稀疏矩阵Cholesky分解是科学工程计算领域Page2入了大量不规则间接寻址操作,造成了访存和计算的不规则.相应的,稀疏矩阵Cholesky分解并行算法需要解决数据组织与计算调度两个基本问题,即如何提高访存和计算的效率.近年来,以高性能和高带宽为设计目标的GPU在高性能计算领域发挥了日益重要的作用.与通用多核处理器相比,GPU简化了计算核心的分支处理能力并减小了缓存容量,将大量的晶体管资源用于计算单元设计,从而获得了高于通用多核处理器的计算性能,CPU+GPU异构并行体系结构已经成为高性能计算系统的主流体系结构之一.以稠密矩阵运算为代表的具有规则的访存和计算特性的并行算法易于在GPU上获得高于通用处理器的计算性能,然而,以稀疏矩阵Cholesky分解为代表的具有不规则的访存和计算特性的并行算法在GPU上的计算效率很低.大量的离散数据访问和分支计算使得GPU的线程串行化执行,无法发挥GPU的并行化优势,这种情况下GPU的实际性能往往低于多核CPU.为了在GPU上高效实现稀疏矩阵Cholesky分解,需要从数据组织和计算调度两方面对现有算法重新设计.我们发现将相邻的具有相似稀疏结构的矩阵列合并为多列存储的超节点数据结构,能够有效地减少间接访存的开销并提高计算粒度,但会增加额外的存储和计算开销,并且由于计算粒度分布不均匀而使得计算负载不均衡.为了降低额外开销以及控制计算粒度,我们对大规模超节点进行分块,得到了适合GPU存储和计算的规则数据结构.另外我们还发现,通过将稀疏矩阵压缩存储数据结构中的矩阵元素数据和索引数据分离,在计算过程中将矩阵元素数据存储在GPU端,而索引数据存储在CPU端,能够显著降低CPU与GPU的通信开销.在超节点分块存储数据结构的基础上,我们将稀疏矩阵Cholesky分解过程映射为一系列面向数据块对象的计算任务.为了在满足任务数据依赖性的前提下实现任务并行的最大化,我们设计了基于队列的任务生成与调度方法,解决了任务间的并行与互斥问题.在此基础上,我们实现了基于GPU的稀疏矩阵Cholesky分解算法.实验表明,该算法能够充分发挥GPU的计算能力,获得了相对于4核处理器2.69~3.88倍的加速比.本文的贡献如下:(1)数据组织我们改进了面向GPU的稀疏矩阵数据组织方法.通过将稀疏矩阵稀疏结构相似的列合并成超节点后再进行数据分块,使得计算任务处理的对象由不规则的稀疏列转换为了规则的稠密数据块.通过将矩阵数据存储在GPU端,索引和控制数据存储在CPU端,降低了CPU与GPU的通信开销.(2)任务生成与调度方法我们设计了基于队列的任务生成与调度方法.计算任务在集成树的指导下生成并加入任务队列.在同一个时间内,可以有多个GPU任务同时执行,这些任务间的并行是通过流机制实现的,而任务间的数据依赖性则是通过事件同步机制进行控制.通过流并行以及事件同步机制,在保证计算正确性的前提下提高了计算效率.(3)高性能在双精度条件下,我们获得了相对于4核处理器2.69~3.88倍的加速比.据我们所知,这是当前稀疏矩阵Cholesky分解算法在GPU平台上获得的相对多核通用CPU平台的最高加速比.2算法及平台介绍2.1稀疏矩阵Cholesky分解算法稀疏矩阵Cholesky分解算法将对称正定稀疏矩阵犃分解为犔犔T,其中犔是对角元素为正的下三角矩阵.由于犃为对称矩阵,只需要存储和处理矩阵犃的下三角部分的非零元素,犔的数值直接覆盖矩阵犃的下三角部分.稀疏矩阵Cholesky分解算法按照从左向右的顺序逐列更新犔.为了便于描述计算过程,我们定义两类计算任务:列分解任务cdiv和列更新任务cmod.其中,cdiv(k)对第k列进行分解,cmod(k,j)使用第j列更新第k列.稀疏矩阵Cholesky分解过程可以表示为一系列计算任务:按照由左至右的顺序,首先生成cdiv(j)任务,对当前列j进行分解;然后生成cmod(j+1:n,j)任务,使用当前列j更新右侧所有相关列.在分解过程中,矩阵犃中的一部分零元素会转变为非零元素,这种元素称为填充元.稀疏矩阵Cholesky算法描述如算法1所示.算法1.稀疏矩阵Cholesky分解算法.输入:分解前的稀疏下三角矩阵犔(li,j),矩阵规模n输出:分解后的稀疏下三角矩阵犔(li,j)FORj=0ton-1DOFORi=jton-1DOFORk=j+1ton-1DOPage3计算任务间的数据相关性通常采用消去树T表示[1].消去树中的节点编号与矩阵犃中的列编号一一对应.消去树节点间关系由稀疏矩阵犃的分解因子犔的稀疏结构决定,定义如下:其中,parent(j)表示节点j的父节点编号.对于图1(a)中的稀疏矩阵,实心点表示矩阵分解前的非零元素,空心点表示矩阵分解过程中产生的填充元,其对应的消去树如图1(b)所示.为简便起见,在本文接下来的部分将不再区分节点与对应的矩阵列.消去树的父子节点间存在数据依赖关系,只有当子节点计算完成后,父节点才能开始计算.因此,稀疏矩阵Cholesky分解过程的任务执行顺序可以表示为消去树自底向上的节点遍历过程.稀疏矩阵通常采用压缩存储数据结构,仅为矩阵的非零元素分配存储空间,从而降低存储量和计算量.基本的稀疏矩阵存储数据结构是列压缩(CompressedSparseColumn,CSC)数据结构[2-3],由3个数组构成,其中val按列存放非零元素,rowSub记录对应于val中每个非零元素的行号,colPtr记录每列第1个非零元素在val和rowSub中的位置.CSC数据结构节省了存储空间,但增加了访存的开销,每个矩阵元素都需要通过间接索引才能访问.为了降低间接寻址的开销,稀疏矩阵分解算法的一种常用策略是将具有相同稀疏结构的列合并,按照稠密矩阵方式存储合并的列内部的非零元素.这样的数据结构称为超节点,由相邻的列集合{i,i+1,…,i+w}组成.超节点中的列满足以下条件:Adj(i)={i+1,i+2,…,i+w}∪Adj(i+w)(2)其中,Adj(i)表示第i列对角线下方非零元素的行坐标集合.以图1(a)中的稀疏矩阵为例,由于Adj(0)={1}∪Adj(1)={1,5},第0列与第1列合并;由于Adj(3)={4}∪Adj(4)={4,5},并且Adj(4)={5}∪Adj(5)={5},第3列、第4列与第5列合并.经过列合并后形成的超节点数据结构如图2所示,形成了s0,s1,s2共3个超节点.超节点数据结构由6个数组构成,colSub记录首列的列号,colNum记录相邻列数量,rowSub记录了对角线下方的行号,val按行记录元素数值,rowPtr和snPtr分别指向各超节点对应于rowSub和val数组中的开始位置.以引进了额外的零元素并增加了存储和计算开销为代价,超节点数据结构使得稀疏矩阵的非零数据存储相对集中.一方面,在超节点内部可以直接寻址,减少了间接寻址次数;另一方面,超节点内部数据访问相对集中,提高了数据的复用率.由于超节点采用稠密矩阵数据块存储,可以调用BLAS(BasicLinearAlgebraSubprograms)库函数以提高计算效率[4].因此,现有的稀疏矩阵分解算法大多采用超节点数据结构.2.2GPU架构当前用于通用计算的主流GPU架构是CUDA(ComputeUnifiedDeviceArchitecture)架构.硬件结构方面,GPU由多个流多处理器(StreamingMultiprocessor,SM)组成,每个SM包含多个标量处理器(ScalarProcessor,SP).每个SM中的指令发射部件将相同的指令发射到各个SP上,各SP在不同的数据集上执行相同的指令.软件结构方面,GPU的基本编程单元是核程序(kernel),基本运算单元是线程.每个线程执行同一个核程序的代码,根据线程编号处理不同的数据.多个线程构成线程块,线程块内的线程以单指令多线程(SingleInstructionMultipleThreads,SIMT)方式执行.CUDA的流机制能够支持多个核程序在GPU上同时执行.流是一系列顺序执行的操作,流之间并发执行各自的命令.每个流拥有唯一的编号.发射到同一个流的核程序按照发射顺序执行,发射到不同流的核程序在计算资源满足的前提下能够同时运行.CUDA的事件机制提供了检测流内的任务执行进度的方式.事件插入流的位置称为事件记载点,只有当流中在时间记载点之前的所有任务全部完成后,事件才会被记载.在CPU-GPU异构系统中,GPU作为协处理器,由CPU控制将数据从主机的DRAM(DynamicRandomAccessMemory)传输到GPU的DRAM,然后将计算任务对应的核程序发射到GPU上,由GPU完成对数据的处理.当GPU运算结束后,再由Page4CPU控制将数据从GPU的DRAM传回主机的DRAM.3相关工作稀疏矩阵Cholesky分解算法包括预处理和数值分解两个阶段.预处理阶段包括矩阵行列重排序和符号分解,所有的数值计算都在数值分解部分完成.在预处理阶段,首先,通过启发式矩阵排序算法交换稀疏矩阵犃的行和列,试图降低矩阵分解过程中产生的填充元的数量,从而减小矩阵分解的存储量和计算量;然后,通过符号分解算法确定犃分解后的分解因子犔的稀疏结构,即确定在分解过程中增加的非零元素位置,预先分配存储空间并生成超节点数据结构和消去树.在数值分解阶段,计算分解因子犔中的所有非零元数值.对于大规模稀疏矩阵,数值分解时间是整个矩阵分解过程中最耗时的部分,通过加速数值分解能够有效提高矩阵的分解效率[5-6].根据数值分解算法的不同,稀疏矩阵Cholesky分解算法主要包括多波前法(MultifrontalMethod)[7-8]和超节点法(SupernodalMethod)[5-6].这两种方法都是基于超节点数据结构,通过减少间接地址访问,提高Cache的命中率,利用优化的BLAS库函数进行面向稠密数据块的浮点计算以提高计算性能.不同之处在于超节点间更新的方法:多波前法通过形成波前阵(FrontalMatrix)将超节点间的更新累积起来,延迟对后续超节点的更新[7];超节点法直接进行超节点间的更新[8].近年来,一些研究致力于将稀疏矩阵Cholesky分解算法移植到GPU,主要思想是根据计算任务的特性和粒度,将数值分解过程中一部分稠密矩阵的BLAS计算任务映射到GPU上[9-12].多波前法的稠密矩阵操作粒度通常大于超节点法,更易于在GPU上实现,因此这些研究都选择了多波前法作为研究对象.Vuduc等人[9]将计算任务直接映射到GPU,而George等人[10]在模型的指导下选择计算任务的映射方式,只将GPU上预期运行效率高于CPU的任务调度到GPU上.Lucas等人[11]在消去树的底部使用多个CPU线程加速小规模计算任务,而在消去树的顶端使用GPU加速大规模计算任务.Faverge使用通用的任务调度框架替换现有稀疏矩阵分解算法中的任务调度模块,试图为稀疏矩阵分解应用提供CPU-GPU异构并行平台上统一的编程接口[12].当前研究的共同方法是在现有稀疏矩阵分解算法的基础上,不改变符号分解部分,只修改数值分解部分计算任务的调度方法,将一部分计算任务分配到GPU.这种方法的优势是易于将现有算法移植到GPU,但没有产生适合GPU处理的数据结构,因此实际的算法性能较低.在双精度条件下,Vuduc等人[9]的实现达到串行程序性能的3倍,Faverge的实现最高能够达到串行程序性能的5倍[12].George等人[10]的实现最高能够达到串行程序性能的9倍,Lucas等人[11]的实现最高能够达到串行程序性能的6倍,但两者都是使用单精度的GPU程序与双精度的CPU程序进行比较.GPU的单精度计算能力是双精度计算能力的2倍~8倍,并且双精度访存数据量是单精度访存数据量的2倍,因此在单精度条件下的GPU性能是双精度情况下的2倍以上.可见,在CPU-GPU异构平台上,稀疏矩阵Cholesky分解目前所能达到的性能最高仅为单核处理器性能的4倍左右,与多核处理器的性能相当.现有的稀疏矩阵Cholesky分解算法无法充分发挥GPU的性能优势,其原因主要包括:(1)没有针对GPU进行专门的数据结构优化,生成的数据结构通常更适合CPU而不是GPU.(2)频繁的数据传输开销,每次调用GPU计算任务的时候都需要产生多次数据传输.(3)没有实现GPU端多个任务间的并行,降低了GPU的利用率.针对以上问题,我们设计了基于GPU的稀疏矩阵Cholesky分解算法,提出以下的解决方案.(1)修改符号分解部分,形成适合GPU处理的数据结构.一方面,采用超节点合并算法将规模小的相邻超节点合并,增大计算任务的规模;另一方面,采用超节点分块算法降低超节点合并过程中额外引入的零元素存储和计算开销.(2)将稀疏矩阵压缩存储数据结构中的矩阵元素数据和索引数据分开存储,在计算过程中将矩阵元素数据存储在GPU端,而索引数据存储在CPU端,降低CPU与GPU的通信开销.(3)设计了基于队列的任务生成与调度方法,使用流重叠和事件同步机制,使得满足数据依赖性的多个GPU任务能够并行执行,提高GPU的利用率.多波前法虽然BLAS操作效率更高,但存储需求量较大;GPU上的存储空间有限,因此我们选择了存储需求量较小的超节点法作为研究对象,直接进行超节点间的更新.Page54基于GPU的稀疏Cholesky分解在本节中,我们介绍了基于GPU的稀疏矩阵Cholesky分解算法.为了形成适合GPU处理的数据结构,首先,介绍超节点的生成与合并过程,将稀疏矩阵组织成一系列规则的数据块.接下来,介绍了对应于扩展超节点分块存储数据结构的计算任务定义.最后,设计了基于队列的任务生成与调度算法,实现了多个GPU任务间的并行化.4.1超节点的生成、合并与分块稀疏矩阵犃如图3(a)所示,其中,实心点表示犃中原有的非零元,空心点表示填充元.超节点生成算法合并相邻的稀疏结构相同的矩阵列,形成超节点数据结构,合并规则如下:merge(ci,ci+1)iffi+1=parent(i)其中ci表示第i列,parent(i)表示消去树中节点i的父节点编号,nnz(i)表示第i列中对角线下方的非零元数量.列合并后,稀疏矩阵被划分为一系列超节点,如图3(a)与图3(b)所示.相应的,消去树中各节点合并后形成新的树结构,称为集成树,如图3(c)所示.集成树的节点对应于各超节点,节点间父子关系对应于超节点间的数据相关性.大多数超节点的宽度(合并的列数量)较小,相应的计算任务处理的矩阵规模也较小,直接将这些计算任务映射到GPU上无法充分发挥GPU的高带宽和高并行计算性能的优势,因此需要将一些稀疏结构相近的相邻超节点进一步合并,通过增加超节点的宽度以增大计算任务的粒度.为了区分合并前后的超节点,我们将合并前的超节点称为基本超节点,合并后的超节点称为扩展超节点.超节点合并规则采用与Hogg相同的方法[13],具体描述为其中,si表示第i个基本超节点,parent(i)表示集成树中节点i的父节点编号,width(s)表示基本超节点s的宽度,t为合并阈值.宽度小于t的相邻父子基本超节点将被合并为扩展超节点.设t为2,通过合并满足合并条件的相邻基本超节点得到图4所示的扩展超节点结构.其中,s0与s1合并生成了s0,s6与s7合并生成了s5.超节点的生成与合并增加了矩阵分解过程中计算任务处理的矩阵规模,提高了任务的计算效率.但是,超节点也增加了额外的存储和计算量.s5的生成过程如图5(a)与图5(b)所示,其中三角形和正方形分别表示基本超节点生成过程与合并过程所引进的额外零元素.额外的零元素增加了存储和计算的开销.为了减少额外的零元素,我们对规模超过阈值的扩展超节点进行分块处理.当阈值为2时,我们对s5分块结果如图5(c)所示.分块后的扩展超节点包含一系列Page6由数据块组成的列,我们将这样的结构称为块列.扩展超节点分块数据结构中,矩阵数据按照块列顺序逐块存储,块内数据逐行存储.在基本超节点数据结构的基础上,增加了指向块列元素起始地址的blkColPtr数组,而snPtr数组指向扩展超节点对应的blkColPtr数组的起始位置.4.2任务定义通过将稀疏矩阵由CSC数据结构转换为扩展超节点分块数据结构,计算任务处理的对象由矩阵列转换为扩展超节点中的数据块.按照处理数据结构的层次关系,计算任务分为3级,自上而下包括扩展超节点任务、块列任务和数据块任务.扩展超节点任务包括扩展超节点内更新任务snScale和扩展超节点间更新任务snUpdate.其中,snUpdate(sk,si)表示si更新sk.块列任务包括块列内更新任务bcScale、扩展超节点内块列间更新任务bcUpdateInternal与扩展超节点间块列间更新任务bcUpdateBetween.数据块任务完成对矩阵元素的更新,包括(1)potrf(犅)Cholesky分解犅=犔犔T.(2)trsm(犅,犆)计算犅=犅\犆.(3)syrk(犅,犆)计算犅=犅-犆犆T.(4)gemm(犅,犆,犇)计算犅=犅-犆犇T.(5)modify(犅,Buffer)使用Buffer更新B.其中,犅、犆、犇、犈为数据块,Buffer为数据缓冲区.前4种任务是标准的稠密矩阵操作,能够直接调用优化的高性能算法库实现.扩展超节点间的数据块往往没有对齐,在块列更新的时候,需要将更新的数据先存入Buffer,再根据索引关系更新目标块中相应位置的元素,因此在以上4种任务外,还需要实现modify任务,以完成不同扩展超节点间的数据块更新.由扩展超节点任务到数据块任务的映射如图6和图7所示.其中,pv,k表示sv中的第k个块列,bv,k表示块列pv,k的数据块个数,Bv,k,i表示块列pv,k中第i个数据块.findBlk(pv,k,m,pv,i)查找块列pv,i中与块列pv,k中第m个数据块同行的数据块起始位置,如果不存在同行数据块则返回-1.clear(Buffer)将Buffer中的所有元素值设为0.图6扩展超节点内更新任务到数据块任务的映射图7扩展超节点间更新任务到数据块任务的映射至此,我们已经将稀疏矩阵Cholesky分解的计算任务由扩展超节点任务映射到数据块任务.接下来,介绍面向GPU的任务生成与调度方法.4.3任务的生成与调度我们设计了面向GPU的任务生成与调度方Page7法.按照数据相关性,CPU生成计算任务并插入任务队列.GPU任务通过流和事件进行组织,不同流间的任务在满足数据依赖性的前提下能够并行执行.接下来,我们首先介绍集成树指导的任务生成算法,然后介绍基于流和事件的任务调度算法.在集成树的指导下,稀疏矩阵Cholesky分解的计算任务生成算法描述如算法2所示.算法2.任务生成算法.输入:集成树T,稀疏矩阵输出:任务队列WHILE!isEmpty(T)DO生成消去树T的叶节点编号集合GFORi∈GDOFORi∈EDO任务按照生成的先后顺序插入任务队列,所有与任务I存在数据依赖性的任务都在任务I之后插入队列.对于图4(a)中的稀疏矩阵,任务生成过程如图8所示.任务调度算法按照FIFO的顺序,由任务队列中取出任务并发射到GPU.为了实现多任务并行,将数据块任务插入不同的流.每个任务插入流后都随之插入一个事件,作为该任务完成的标志.当后续任务与该任务存在数据依赖性的时候,在将后续任务插入流之前需要先插入一个事件等待命令,使得该任务只有在对应事件完成之后才能启动,从而保证了任务的执行顺序.任务调度算法描述如算法3所示.算法3.任务调度算法.输入:任务队列taskQueue,稀疏矩阵,流及事件数组输出:无streamID=1;WHILE!isEmpty(taskQueue)task=getTask(taskQueue)//取任务StreamSynchronize(stream[streamID])//等待任务i=(streamID-1+streamNum)modstreamNumDOWHILEi!=streamIDInsertTask(stream[streamID],task)//将任务插入流InsertEvent(stream[streamID],event[streamID])//记录当前流的目标数据块target[streamID]=task.destID算法实现中,为了防止任务发送速度超过任务执行速度所带来的GPU缓冲区溢出,我们规定每个流内只能有一个未完成任务.5实验与讨论5.1环境配置及测试集合硬件方面,主机配置为主频为2.67GHz的IntelQuadQ9400四核处理器,8GBDDR2800内存.GPU为NVIDIAGTX480,工作频率为1.4GHz,共有15个SM,每个SM包含32个SP,显存位宽为384位,显存容量为1.5GB.软件方面,采用64位RedHatRHEL5.5操作系统.C编译器为gcc4.1.2,Fortran编译器为ifort12.1.0.GPU运行环境为CUDASDK4.0.17,CUDAToolkit4.0.17,CUDADriver4.0.CPU端BLAS库函数采用IntelMKL10.3.7数学库.GPU端,trsm、syrk和gemm任务使用CuBLAS数学库实现,potrf与modify任务使用自定义核程序实现.预处理过程中矩阵排序采用Metis4.0算法包的Metis_NodeND算法①.测试矩阵集如表1所示,所有数据来自UniversityofFloridaSparseMatrixCollection②.①②Page8编号矩阵名称规模(103)非零元数量(106)123Trefethen_2000020.004Trefethen_20000b20.05.2实验与分析在本部分,我们从基本的超节点稀疏矩阵Cholesky分解算法出发,采用在第4节中介绍的数据组织与任务组织方法,以矩阵1为例,逐步将其由CPU平台移植到GPU平台,通过实验和数据分析验证我们的稀疏矩阵Cholesky分解算法中的关键技术的有效性.实验数据中的计算时间为符号分解时间与数值分解时间之和.5.2.1任务映射与数据存储方案配置1是作为基准程序的基本超节点稀疏矩阵分解算法.配置2将除modify外的数据块任务映射到GPU(modify任务根据索引直接对矩阵数据进行修改,因此当矩阵数据存储在CPU端时,modify任务需要在CPU端实现).配置3将所有数据块任务映射到GPU,并将稀疏矩阵数据在分解开始前传递到GPU端,分解完成后再传递回CPU端.3种配置下,稀疏矩阵分解时间如表2所示.与配置1相比,由于GPU的计算能力高于CPU,配置2的矩阵分解时间低于配置1.然而,由于每次执行GPU任务都需要产生CPU与GPU数据传输,由此带来的频繁的数据传输开销使得配置2的总时间多于配置1.可见,简单的将计算任务映射到GPU上并不能提高算法的性能.通过在分解开始前将矩阵数据迁移到GPU端,配置3减少了分解过程中CPU与GPU间的通信开销,并且由于GPU的访存带宽高于CPU,计算时间随着modify任务性能的提高而有所降低.配置1配置2配置35.2.2超节点合并与分块虽然配置3与配置2相比性能有所提高,然而加速比仅为配置1的3倍左右.这是由于形成的基本超节点大多宽度较小,相应的GPU任务计算粒度小,无法有效发挥GPU的计算性能.为了提高GPU任务的计算粒度,需要增大超节点的宽度.我们使用超节点合并方法,按照4.1节中的规则(4),将相邻的满足条件的父子基本超节点合并.与基本超节点相比,扩展超节点的规模更大,有利于提高GPU计算效率.但是,超节点合并会增加超节点中零元素的数量,从而增加了存储量和计算量.测试矩阵包括2115个基本超节点,对于不同的合并阈值,其分解因子的扩展超节点数量、存储量和计算时间如表3所示.合并阈值扩展超节点数量存储量/GB计算时间/s0163264128256实验结果表明,随着扩展超节点规模的增大,起初,由于计算任务粒度的增大,GPU计算效率提高,分解时间减少;随后,额外的零元素带来的开销超过了GPU计算效率的影响,计算时间开始增加.在合并阈值为64时,矩阵分解时间最小,因此在接下来的部分,选用64作为默认合并阈值.为了减少扩展超节点引入的非零元数量,我们对扩展超节点进行分块处理.对于不同的分块大小,稀疏矩阵分解因子的存储量和计算时间如表4所示.在分块规模为1024时,矩阵分解时间达到最小,因此在接下来的部分,选用1024作为默认分块规模.分块规模1024204840965.2.3基于流和事件的GPU任务并行在前面的实现中,GPU任务按照生成的顺序串行执行.为了提高算法性能,需要使得满足数据依赖性的多个GPU任务能够同时执行.通过采用4.3节中介绍的基于流并行以及事件同步的任务调度方法,在不同流数量下的计算时间如表5所示.随着流数量的增加,起初,由于任务间的计算重叠,算法分解时间降低;随后,由于流间同步控制的开销随流数量增加而增加,算法分解时间又逐渐回升.当流数量为4时,算法分解时间达到最小,因此采用4作为默Page9认流数量.5.3性能对比在本节中,我们将本文中面向GPU的稀疏矩阵Cholesky分解算法与面向多核CPU平台的并行算法进行了性能对比.多核CPU并行算法我们选择了HSL_MA87[13].HSL_MA87是由Hogg开发的稀疏线性方程组求解程序,包含了目前为止多核处理器平台上性能最高的稀疏矩阵Cholesky分解算法实现.HSL_MA87所有参数使用了默认设置,两种算法的性能对比数据如表6所示.矩阵编号CPU分解时间/sGPU分解时间/s加速比1234测试数据表明,与4核CPU平台下的并行稀疏矩阵Cholesky分解算法相比,面向GPU的稀疏矩阵Cholesky分解能够达到2.69~3.88倍加速比.6结语本文中,我们提出了面向GPU的稀疏矩阵数据组织方法.通过将稀疏矩阵稀疏结构相似的列合并成超节点后再进行数据分块,使得计算任务处理的对象由列转换为了规则的稠密矩阵数据块.通过将矩阵数据存储在GPU端,控制数据存储在CPU端,减低了CPU与GPU间的通信开销.在扩展超节点分块存储数据结构的基础上,我们将稀疏矩阵Cholesky分解过程映射为一系列面向数据块对象的计算任务.为了在满足任务数据依赖性的前提下实现任务并行的最大化,我们设计了面向GPU的任务生成与调度方法,解决了任务间的并行与互斥问题.在此基础上,我们实现了面向GPU的稀疏矩阵Cholesky分解算法.实验表明,该算法能够充分发挥GPU的计算能力,获得了相对于单个多核处理器2.69~3.88倍的加速比.在接下来的工作中,我们将研究多GPU下的数据组织与任务调度算法.一方面,研究矩阵稀疏结构和排序算法对任务粒度与并行度的影响;另一方面,研究多个CPU与多个GPU间的任务并行策略.最终我们的目标是实现能够高效运行在由多个GPU节点组成的计算阵列上的稀疏矩阵分解算法.
