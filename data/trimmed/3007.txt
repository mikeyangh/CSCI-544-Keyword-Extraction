Page1多元多项式函数的三层前向神经网络逼近方法王建军1),2)徐宗本2)1)(西南大学数学与统计学院重庆400715)2)(西安交通大学信息与系统科学研究所西安710049)摘要该文首先用构造性方法证明:对任意r阶多元多项式,存在确定权值和确定隐元个数的三层前向神经网络,它能以任意精度逼近该多项式,其中权值由所给多元多项式的系数和激活函数确定,而隐元个数由r与输入变量维数确定.作者给出算法和算例,说明基于文中所构造的神经网络可非常高效地逼近多元多项式函数.具体化到一元多项式的情形,文中结果比曹飞龙等所提出的网络和算法更为简单、高效;所获结果对前向神经网络逼近多元多项式函数类的网络构造以及逼近等具有重要的理论与应用意义,为神经网络逼近任意函数的网络构造的理论与方法提供了一条途径.关键词前向神经网络;多元多项式;逼近;算法1引言近年来,许多学者对神经网络逼近问题进行了研究,取得了一系列重要成果.神经网络已经在工程、计算机、物理、生物等学科中得到了广泛的应用,大多数应用都被转化为利用神经网络逼近多元函数的问题(如文献[1-5]等).神经网络之所以能得到广泛应用,其主要原因之一是它具有一定意义上的万有逼近性(如文献[6-7]等).所有这些研究的一个典型结论是:任何一个定义在Rd上的连续函数可以通过一个具有单一隐层的三层前向神经网络任意逼近.一个具有单一隐层,含d个输入、1个输出的三层前向神经网络数学上可表示为N(狓)=∑m其中1im,θi∈R是阈值,wi=(wi1,wi2,…,wid)T∈Rd是输入层与隐层第i个神经元的连接权值,ci是隐层与输出层之间的连接权值,σ是隐层节点的激活函数(传递函数).通常情况下,网络激活函数σ取为sigmoid型函数,即满足σ(t)→1(t→),σ(t)→0(t→-)的函数.用向量形式表示,式(1)可进一步表达为众所周知,人工神经网络的结构设计(使之有能力学习给定的函数)是其应用中的重要而基本的问题.最近,有较多的工作(如文献[8-10])研究前向神经网络的逼近精度与隐元个数的关系,以从理论上反映网络逼近速度与网络拓扑之间的关系.但是,这些理论结果并没有给出实现函数逼近的具体算法,所构造的网络也过于复杂,不易实现,所以很难在实际中得到应用.一元多项式是最简单和最基本的被逼近函数形式.在文献[11]中,作者对一元多项式构造了一种前向神经网络,给出了逼近的理论结果和算法实现;对于多元情况,由于多元区域中点的方向的无穷性、多项式的展开分解以及差分的介入等问题的复杂性,它并不能表示为一元多项式的简单叠加,在逼近意义下,也不是一元多项式的简单推广,因而对于多元多项式,神经网络实现起来并不容易.然而,我们知道,多元多项式能够任意逼近任何一个连续多元函数,因而如何高效实现多元多项式的神经网络逼近对于发展对一般函数的神经网络逼近(特别是网络设计理论)有重要意义.有鉴于此,本文研究目标函数为多元多项式的三层前向神经网络的逼近问题.我们将给出一个具有确定隐元个数和确定权值向量的三层前向神经网络来实现对多元多项式的任意逼近.所给出的确定网络,其隐层节点数由所逼近多项式的阶数r和输入空间的维数d确定,而权值由所逼近多项式的系数和激活函数确定.我们将给出一个具体的算法实现网络设计.算例表明:所提出的算法十分高效,在一元情况,同文献[11]的结果相比,本文算法所构造网络的逼近精度比文献[11]提高了10倍.本文所获结果对前向神经网络逼近多元多项式函数类的网络具体构造以及实现逼近的方法等问题具有重要的指导意义.2记号及主要结果在本文中我们将采用如下记号Z0,R分别表示非负整数、实数,Rd表示d维实Euclid空间;Zd0表示Z0×Z0×…×Z烉烇(y1,y2,…,yd)∈Rd,犼=(j1,j2,…,jd),狇=(q1,q2,…,qd)∈Zd0.向量狓与狔的内积表示为同时记用犼狇表示jiqi(i=1,2,…,d),|犼||狇|表示ji∑d∑d|犼|-阶偏导数表示如下:f(狉)(狓)··=|狉|fi=1其中|狉|=r1+r2+…+rd.我们用P狉(d)表示定义在有界区域S上的所有d元实的、次数不超过|狉|的代数多项式.我们利用这些记号给出如下的逼近定理.定理.设是定义在R上的具有|狉|+1-阶连续有界导数的函数,且对任意的k∈Z0,0k|狉|+1,存在某一θ∈R,使得(k)(θ)≠0.pr(狓)∈P狉(d),则可以构造一个输入、一个输出及隐元个数为n=∑0犼狉∏dNn(狓)=∑0犼狉∑0犻犼使得Page3证明.设记Mi={|xi|max,狓=(x1,x2,…,xd)∈S},i=1,2,…,d.由于[9](犼)(ω·狓+θ)=|犼|于是因而犼,狓(θ)=(犼)(ω·狓+θ)|ω=0=狓犼(|犼|)(θ)(4)将式(5)代入式(2),我们得到对任意固定的b∈R,我们考虑以下有限j-阶差分h,狓(θ)=∑0犻犼Δ犼其中()犼犻=∏d表示,我们得到pr(狓)-∑0犼狉=∑0犼狉∑0犼狉∑0犼狉=∑0犼狉狓犼(2h)-|犼|∫h(τ1+τ2+…+τj1)x1+…+(τ|犼|-jd+1+…+τ|犼|)xd)dτ=∑0犼狉∫h-h∫h(τ1+τ2+…+τj1)x1+…+(τ|犼|-jd+1+…+τ|犼|)xd)dτ…∫h-h∑0犼狉2M1h∑0犼狉其中Ω(,δ)=sup|t-x|<δ|(狓)-(t)|(见文献[12])是函数的连续模,且当有连续导数时,Ω(,δ)δ(=supx|(狓)|).令M0=max{|(i)(θ)|,i=0,1,…,|狉|+1},于是p狉(狓)-∑0犼狉2M1M0h∑0犼狉其中常数由式(8)、(9),我们可以构造出如下神经网络其中且由式(9)知Nn(狓)满足令h<ε至于隐层单元个数,我们由式(10)很容易看出,网络具有∑0犼狉∏d注1.从上述证明中我们看到,对于给定的多元多项式p狉(狓),其神经网络的权值可具体由式(11)和(12)确定.从式(11)和(12)我们看到,它由所逼近多项式的系数和所选定的网络激活函数在θ点的各阶导数值唯一确定.3算法和算例总结上节讨论,我们能给出如下构造逼近多元多项式神经网络的算法.要求的激活函数的具体表达式;搜索步长Δ.算法1.给定参数:多元多项式的系数a犼,阶数r;误差要求ε,输入最大值Mi=ximax,i=1,2,…,d;满足1.求出隐元个数∑0犼狉∏d2.选择阈值θ并计算(|犼|)(θ);Page43.求出4.计算5.选取h满足h<ε6.利用方程(12)计算权c犻,犼,即7.结束.下面给出算例.首先,我们采用文献[11]中的例子作为我们第例1.选取激活函数φ(x)=1一个算例,且和文献[11]的结果进行比较.近的多项式函数为P1(x)=1-3x,输入的最大值为M1=xmax=10,误差要求ε=0.001,则节点数是3,选取阈值θ使得e-θ=3,通过计算,得到φ(θ)=4,φ(1)(θ)=31于是,我们可以选择h=10-7<ε达式,我们计算得到c0,0=4,c0,1=-8×107,c1,1=8×107.表1No.xP1(x)|N(x)-P1(x)|N(x)|N(x)-P1(x)|10.01.00000000001.00000000000.00000000001.00000000000.000000000020.10.70000000000.700000004043690.000000004043690.70000000290.000000002930.20.40000000000.399999999205590.000000000794410.39999999100.000000009040.30.10000000000.100000003249280.000000003249280.09999999400.000000006050.4-0.2000000000-0.200000001588820.00000000158882-0.20000001040.000000010460.5-0.5000000000-0.499999997545130.00000000245487-0.50000001490.000000014970.6-0.8000000000-0.800000002383230.00000000238323-0.80000002680.000000026880.7-1.1000000000-1.099999998339540.00000000166046-1.10000003120.000000031290.8-1.4000000000-1.400000003177640.00000000317764-1.40000005060.0000000506100.9-1.7000000000-1.700000003574840.00000000357484-1.70000006250.0000000625111.0-2.0000000000-1.999999997310710.00000000268929-1.70000006700.0000000670121.1-2.3000000000-2.300000004369250.00000000436925-1.70000009380.0000000938131.2-2.6000000000-2.599999998105120.00000000189488-2.60000010570.0000001057141.3-2.9000000000-2.899999998502320.00000000149768-2.90000012510.0000001251151.4-3.2000000000-3.199999998899530.00000000110047-3.20000014450.0000001445161.5-3.5000000000-3.499999999296730.00000000070327-3.50000017130.0000001713171.6-3.8000000000-3.799999997473490.00000000252651-3.80000019070.0000001907181.7-4.1000000000-4.100000004532030.00000000453203-4.10000021750.0000002175191.8-4.4000000000-4.399999998267900.00000000173210-4.40000024430.0000002443201.9-4.7000000000-4.699999998665110.00000000133489-4.70000026730.0000002673212.0-5.0000000000-5.000000001282760.00000000128276-5.00000029800.0000002980959.4-27.2000000000-27.199999995148770.00000000485123-27.20000662650.0000066265969.5-27.5000000000-27.499999997766420.00000000223358-27.50000677250.0000067725979.6-27.8000000000-27.800000000384070.00000000038407-27.80000691110.0000069111989.7-28.1000000000-28.100000003001730.00000000300172-28.10000706460.0000070646从而,对于多项式函数P1(x)=1-3x,我们可以构造前向神经网络N(x)=c0,0(θ)-8×107(-10-7x+θ)+计算结果见表1,误差曲线见图1,文献[11]中关于此例的误差曲线见图2.Page5No.xP1(x)|N(x)-P1(x)|N(x)|N(x)-P1(x)|999.8-28.4000000000-28.399999998958040.00000000104196-28.40000720320.00000720321009.9-28.7000000000-28.699999997134800.00000000286521-28.70000734920.0000073492193-9.228.600000000028.600000001015700.0000000010157028.59999365350.0000063465194-9.328.900000000028.900000003633350.0000000036333528.89999351650.0000064835195-9.429.200000000029.199999995148770.0000000048512329.19999336450.0000066355196-9.529.500000000029.499999997766420.0000000022335829.49999321990.0000067801197-9.629.800000000029.800000000384070.0000000003840829.79999309030.0000069097198-9.730.100000000030.100000003001730.0000000030017330.09999294570.0000070543199-9.830.400000000030.399999998958040.0000000010419630.39999279370.0000072063200-9.930.700000000030.699999997134800.0000000028652130.69999264180.0000073582201-1031.000000000030.999999997532000.0000000024680030.99999249720.0000075038注:表1中N(x)是例1用本文方法所构造的网络,N(x)是文献[11]所构造的网络.10-9<εN(x)=16h-1(φ(hx1+θ)-φ(-hx1+θ))-8h-2(φ(h(x2+x1)+θ)-φ(-h(x1-x2)+θ)-φ(-h(x2-x1)+θ)+φ(-h(x2+x1)+θ))-16/3h-3(φ(3hx2+θ)-3φ(hx2+θ)+3φ(-hx2+θ)-φ(-3hx2+θ))注2.从这个例子可以看出,我们所构造的网络不但简单、容易计算网络权值,而且逼近精度非常理想.从网络构造上来说,本文所构造的网络比文献[11]的网络更容易实现,其计算复杂度明显下降,这只需注意到,文献[11]中需要通过计算以下矩阵的逆来实现网络构造,而显然,当多项式的阶数很高时,上式计算复杂度甚高;而本文的算法不涉及这样的矩阵求逆运算.从计算结果来看,本文的结果明显好于文献[11]的结果,其误差精度提高了10倍.例2.选取激活函数φ(x)=1近函数为二元三次多项式P2(x)=6x1-3x1x2+x32,输入的最大值为M1=M2=10,误差要求ε=0.001,则节点数n=30,并且φ(θ)=1316,φ(2)(θ)=3以M0=1表2No.x1x210.00.020.01.030.02.040.03.027.00000000000000039550.04.064.00000000000000166460.05.0125.0000000000000050870.06.0216.0000000000000126480.07.0343.0000000000000273190.08.0512.00000000000005325100.09.0729.00000000000009595110.010.01000.0000000000001625121.00.0131.01.0使得表2(以搜索步长Δ=1.0)和图3(误差曲线图(h=10-4))对例2进一步加以说明.27.00000000000000000064.000000000000000000125.00000000000000000216.00000000000000000343.00000000000000000512.00000000000000000729.000000000000000001000.0000000000000000Page6No.x1x2141.02.0151.03.0161.04.0171.05.01109.010.011110.00.011210.01.011310.02.011410.03.011510.04.011610.05.0343-1.02.0344-1.03.0345-1.04.0346-1.05.0437-10.06.0438-10.07.0439-10.08.0440-10.09.0441-10.010.0注:表2中N(x)是例2所构造的网络.注3.从例2可以看出,我们构造的网络实际用到的隐元个数是10(由式(13),N(x)右端共有10项是非零的,故此时隐元个数为10),而不是像定理所预测的30(∑1由于我们的定理是对于一般的多元多项式来给出隐元个数,而本例仅是一般多项式的一个特例.同时我们知道,对多元多项式,由于其方向的多样性,并不是简单的一元多项式的叠加,比一维情况复杂得多;从这个算例我们可以看到,我们所构造的神经网络简单,非常容易计算,实现了对多元多项式的逼近,误差结果十分理想,它为我们实现对任意多元函数的逼近提供了一个很好的范例.例3.选取激活函数φ(x)=1P3(x,y)=x3-3x2y+3xy2-y3,用我们的方法得到的网络逼近P3(x,y)的仿真曲面图和误差曲面分别见图4、图5.4结论本文所构造的三层前向人工神经网络的方法和逼近的具体算法实用简单,容易计算.通过算例看出实现这一逼近比较容易,而且十分高效,计算复杂度较低.所获结果表明:对给定阶数r的多元多项式,存在确定权值和确定隐元个数的三层前向神经-3.00000000000000000002.3136×10-15网络,它能以任意精度逼近该多项式,其中权值由所给多元多项式的系数和激活函数确定,而隐元个数由r与输入变量维数确定,而且这一逼近完全可以Page7通过一个具体的算法实现,这为我们对任意函数被神经网络逼近提供了一个很好的理论和实践方法,具有重要的指导意义.致谢作者衷心感谢审稿人提出的宝贵意见和建议!
