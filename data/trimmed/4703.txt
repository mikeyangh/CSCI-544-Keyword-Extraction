Page1神经网络七十年:回顾与展望1)(西安电子科技大学智能感知与图像理解教育部重点实验室、智能感知与计算国际联合研究中心、智能感知与计算国际合作联合实验室西安710071)2)(西安电子科技大学计算机学院西安710071)摘要作为联接主义智能实现的典范,神经网络采用广泛互联的结构与有效的学习机制来模拟人脑信息处理的过程,是人工智能发展中的重要方法,也是当前类脑智能研究中的有效工具.在七十年的发展历程中,神经网络曾历经质疑、批判与冷落,同时也几度繁荣并取得了许多瞩目的成就.从20世纪40年代的M-P神经元和Hebb学习规则,到50年代的Hodykin-Huxley方程、感知器模型与自适应滤波器,再到60年代的自组织映射网络、神经认知机、自适应共振网络,许多神经计算模型都发展成为信号处理、计算机视觉、自然语言处理与优化计算等领域的经典方法,为该领域带来了里程碑式的影响.目前,模拟人脑复杂的层次化认知特点的深度学习已经成为类脑智能中的一个重要研究方向.通过增加网络层数所构造的“深层神经网络”使机器能够获得“抽象概念”能力,在诸多领域都取得了巨大的成功,又掀起了神经网络研究的一个新高潮.文中回顾了神经网络的发展历程,综述了其当前研究进展以及存在的问题,展望了未来神经网络的发展方向.关键词类脑智能;神经网络;深度学习;大数据;并行计算;机器学习1引言实现人工智能是人类长期以来一直追求的梦想.虽然计算机技术在过去几十年里取得了长足的发展,但是实现真正意义上的机器智能至今仍然困难重重.伴随着神经解剖学的发展,观测大脑微观结构的技术手段日益丰富,人类对大脑组织的形态、结构与活动的认识越来越深入,人脑信息处理的奥秘也正在被逐步揭示.如何借助神经科学、脑科学与认知科学的研究成果,研究大脑信息表征、转换机理和学习规则,建立模拟大脑信息处理过程的智能计算模型,最终使机器掌握人类的认知规律,是“类脑智能”的研究目标.近年来,类脑智能已成为世界各国研究和角逐的热点.继美国及欧盟各国之后,我国经过两三年筹备的“中国脑科学计划”在2015年浮出水面,科技部正在规划“脑科学与类脑研究”的重大专项,北京大学、清华大学、复旦大学等高校和中国科学院等研究机构也发力推动神经与类脑计算的相关研究,大规模“类脑智能”的研究正蓄势待发.类脑智能是涉及计算科学、认知科学、神经科学与脑科学的交叉前沿方向.类脑智能的实现离不开大脑神经系统的研究.众所周知,人脑是由几十多亿个高度互联的神经元组成的复杂生物网络,也是人类分析、联想、记忆和逻辑推理等能力的来源.神经元之间通过突触连接以相互传递信息,连接的方式和强度随着学习发生改变,从而将学习到的知识进行存储.模拟人脑中信息存储和处理的基本单元-神经元而组成的人工神经网络模型具有自学习与自组织等智能行为,能够使机器具有一定程度上的智能水平.神经网络的计算结构和学习规则遵照生物神经网络设计,在数字计算机中,神经细胞接收周围细胞的刺激并产生相应输出信号的过程可以用“线性加权和”及“函数映射”的方式来模拟,而网络结构和权值调整的过程用优化学习算法实现.按照该方式建立的这种仿生智能计算模型虽然不能和生物神经网络完全等价和媲美,但已经在某些方面取得了优越的性能.从20世纪40年代的M-P神经元和Hebb学习规则,到50年代的Hodykin-Huxley方程、感知器模型与自适应滤波器,再到60年代的自组织映射网络、神经认知机、自适应共振网络,许多神经计算模型都发展成为信号处理、计算机视觉、自然语言处理与优化计算等领域的经典方法,为该领域带来了里程碑式的影响.目前神经网络已经发展了上百种模型,在诸如手写体识别[1-2]、图像标注[3]、语义理解[4-6]和语音识别[7-9]等技术领域取得了非常成功的应用.从数据容量和处理速度来看,目前大多数神经网络是生物网络的简化形式,在应对海量数据和处理复杂任务时显得力不从心.例如,人脑被证明可以在没有导师监督的情况下主动地完成学习任务,仅凭借传统的浅层神经网络是无法实现这一点的.最近发展起来的深层神经网络就是一种类脑智能软件系统,它使得人工智能的研究进入了一个新阶段.深层神经网络通过增加网络的层数来模拟人脑复杂的层次化认知规律,以使机器获得“抽象概念”的能力,在无监督特征学习方面具有更强的能力.然而,受到计算平台和学习算法的限制,对深层神经网络的研究曾一度消弭.2006年,Hinton在《科学》上提出了一种面向复杂通用学习任务的深层神经网络,指出具有大量隐层的网络具有优异的特征学习能力,而网络的训练可以采用“逐层初始化”与“反向微调”技术解决.人类借助神经网络找到了处理“抽象概念”的方法,神经网络的研究又进入了一个崭新的时代[10-12],深度学习的概念开始被提出.深度学习兴起的背景是计算能力的提高与大数据时代的来临,其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习.深层神经网络能够获得巨大成功与其对应在训练算法上所取得的突破性进展是密不可分的.传统的反向传播算法(BackPropagation)随着传递层数的增加,残差会越Page3来越小,出现所谓的“梯度扩散”(GradientDiffusion)现象,故而不适于深层网络的训练.深度学习模型中的受限玻尔兹曼机(RestrictedBoltzmannMachines)和自编码器(Auto-Encoder)采用了“自下而上的无监督学习”和“自顶向下的监督学习”策略来实现对网络的“预训练”和“微调”,可使学习算法收敛到较为理想的解上,而当前使用更为广泛的卷积神经网络(ConvolutionalNeuralNetworks)则采用局部感受野、权值共享和时空亚采样的思想,显著地减少了网络中自由参数的个数,并且使得采用反向传播来进行网络的并行学习成为可能.除了以上优势外,深度学习最具吸引力的地方还在于能凭借无标签的数据来进行学习,而不需要依赖于监督信息的支撑[13].现实世界的很多问题中,对数据的标记通常是耗时耗力甚至是不可行的,无监督学习可以自动抽取出抽象的高层属性和特征,是解决样本标记难问题的一个重大突破.深度学习的成功引起了包括产业界和学术界在内的诸多人士的关注,其影响力甚至上升到了国家战略层面.2012年6月,《纽约时报》披露了GoogleBrain项目,该项目拟计划在包含16000个中央处理单元的分布式并行计算平台上构建一种被称之为“深度神经网络”的类脑学习模型,其主要负责人为机器学习界的泰斗、来自斯坦福大学的Ng教授和Google软件架构天才、大型并发编程框架MapReduce的作者JeffDean;2012年10月,在天津举行的“21世纪的计算大会”上,微软首席研究官RickRashid展示了一套全自动同声传译系统,演讲者的英文能够被实时、流畅地转换成与之对应的、音色相近的中文,其背后的关键技术深度神经网络也逐渐被人们所知.2013年1月,作为百度公司创始人兼CEO的李彦宏在其年会上宣布了成立百度研究院的计划,并且强调首当其冲的就是组建“深度学习研究所”.在2015年3月9日的两会期间,李彦宏又提议设立“中国大脑”计划的提案,与2013年1月和2013年4月的“欧盟大脑计划”和“美国大脑计划”相呼应.2015年3月,阿里巴巴公司的创始人马云通过支付宝的“刷脸支付”功能,在德国举行的IT博览会上成功购得了一款汉诺威纪念邮票.这一人脸识别技术在商业领域的应用雏形所采用的是基于神经网络的技术,其网络训练所使用的正是“深度学习算法”.在学术界,以Hinton、LeCun、Bengio和Ng等为代表的神经网络大师们不断将深度学习的研究推向新的高峰,对包括计算机视觉、自然语言处理和机器学习在内的诸多领域带来了深远的影响[14].自2006年深度学习出现以来,关于深度学习理论和应用方面的研究文献在国际知名期刊和会议上不断涌现,如《自然》、《科学》、PAMI、NIPS、CVPR、ICML等.同时,由Bengio等人编写的第一本关于深度学习的专著“DeepLearning”也即将由MIT出版社出版.包括斯坦福大学、卡内基梅隆大学、纽约大学、多伦多大学等在内的机构都提供了深度学习的公开课程,并公开了实验数据和源代码,为深度学习的进一步发展做出了贡献.在国内,深度学习也受到了学术界的广泛关注,但目前主要是以深度学习的应用研究为主,在理论方面的工作相对较少.以北京大学、浙江大学、上海交通大学、哈尔滨工业大学和西安电子科技大学等为代表的研究人员将深度学习算法应用到遥感图像分类[15]、多媒体检索[16]、交通流预测[17]和盲图像质量评价[18]等领域,取得了较传统方法更优的效果.本文将以神经网络的理论和应用为主线,回顾神经网络在过去七十多年的发展历程及主要成就,重点对新近发展起来的深度学习进行阐述和讨论,并对未来的研究方向做出展望.2神经网络发展回顾自从西班牙解剖学家Cajal于19世纪末创立了神经元学说以来,关于神经元的生物学特征和相关的电学性质在之后被相继发现.1943年,神经元的M-P模型(如图1所示)在论文《神经活动中所蕴含思想的逻辑活动》中被首次提出[19],创建该模型的是来自美国的心理学家McCulloch以及另一位数学家Pitts.图中,xi(i=1,2,…,n)表示来自与当前神经元相连的其他神经元传递的输入信号,wij代表从神经元j到神经元i的连接强度或权值,θi为神经元的激活阈值或偏置,f称作激活函数或转移函数.神经元Page4的输出yi可以表示为如下形式:该模型从逻辑功能器件的角度来描述神经元,为神经网络的理论研究开辟了道路.M-P模型是对生物神经元信息处理模式的数学简化,后续的神经网络研究工作都是以它为基础的.1949年,在《行为的组织》一书中心理学家Hebb对神经元之间连接强度的变化规则进行了分析,并基于此提出了著名的Hebb学习规则[20].受启发于巴浦洛夫的条件反射实验,Hebb认为如果两个神经元在同一时刻被激发,则它们之间的联系应该被强化,基于此所定义的Hebb学习规则如下所示:其中,wij(t+1)和wij(t)分别表示在t+1和t时刻时,神经元j到神经元i之间的连接强度,而yi和yj则为神经元i和j的输出.Hebb规则隶属于无监督学习算法的范畴,其主要思想是根据两个神经元的激发状态来调整其连接关系,以此实现对简单神经活动的模拟.继Hebb学习规则之后,神经元的有监督Delta学习规则被提出,用以解决在输入输出已知的情况下神经元权值的学习问题.该算法通过对连接权值进行不断调整以使神经元的实际输出和期望输出到达一致,其学习修正公式如下[21]:其中,α为算法的学习速率,di和yi为神经元i的期望输出和实际输出,xj(t)表示神经元j在t时刻的状态(激活或抑制).从直观上来说,当神经元i的实际输出比期望输出大,则减小与已激活神经元的连接权重,同时增加与已抑制神经元的连接权重;当神经元i的实际输出比期望输出小,则增加与已激活神经元的连接权重,同时减小与已抑制神经元的连接权重.通过这样的调节过程,神经元会将输入和输出之间的正确映射关系存储在权值中,从而具备了对数据的表示能力.Hebb学习规则和Delta学习规则都是针对单个神经元而提出的,在神经元组成的网络中参数的学习规则将会在后续述及.以上先驱者所做的研究工作为后来神经计算的出现铺平了道路,激发了许多学者对这一领域的继续探索和研究.1958年,Rosenblatt等人成功研制出了代号为MarkI的感知机(Perceptron),这是历史上首个将神经网络的学习功能用于模式识别的装置,标志着神经网路进入了新的发展阶段[22].感知机是二分类的线性判别模型,旨在通过最小化误分类损失函数来优化分类超平面,从而对新的实例实现准确预测.假设输入特征向量空间为x∈y={-1,+1},则感知机模型如下:其中,w和b为神经元的权值向量和偏置;w·x表示w和x的内积;sign为符号函数:感知机的假设空间是定义在特征空间中的所有线性分类器,所得的超平面把特征空间划分为两部分,位于两侧的点分别为正负两类.感知机参数的学习是基于经验损失函数最小化的,旨在最小化误分类点到决策平面的距离.给定一组数据集T={(x1,y1),(x2,y2),…,(xn,yn)},假设超平面S下误分类点的集合为M,则感知机学习的损失函数定义为感知机学习算法通过最小化经验风险来优化参数w和b:优化过程采用随机梯度下降法,每次随机选取一个误分类点使其梯度下降.首先分别求出损失函数对w和b偏导数:然后,随机选取一个误分类点(xi,yi)对w和b进行更新其中,0<η1是学习步长.以上为感知机学习的原始形式,与之相对应的另一种结构是感知机学习的对偶形式.其基本思想是将w和b表示为所有实例点的线性组合形式,通过求解系数来得到w和b.不失一般性,首先将w和b的初始值设为0,对于误分类点按照式(10)和(11)的规则来对w和b的值进行更新.假设总共进行了n次更新,则最终学习到的w和b可表示为Page5其中,ai=niη(ni为第i次时的累积更新次数).继感知机之后,许多新的学习型神经网络模型被提出,其中包括Widrow等人设计的自适应线性元件Adaline[23]和由Steinbuch等人设计的被称为学习矩阵的二进制联想网络及其硬件实现[24].随着对感知机研究的逐渐深入,1969年Minsky和Papert从数学的角度证明了单层神经网络具有有限的功能,甚至在面对简单的“异或”逻辑问题时也显得无能为力[25].同时,他们发现许多复杂的函数关系是无法通过对单层网络训练得到的,至于多层网络是否可行还值得怀疑.他们所著的《感知机》一书出版后给当时神经网络感知机方向的研究泼了一盆冷水,美国和前苏联在此后很长一段时间内也未资助过神经网络方面的研究工作.虽然感知机具备了基本的神经计算单元和网络结构,也拥有一套有效的参数学习算法,但是其特定的结构使得其在很多问题上都不能奏效.此后很长一段时间内神经网络的研究处在低迷期,直到1982年美国加州理工学院的Hopfield提出了连续和离散的Hopfield神经网络模型,并采用全互联型神经网络尝试对非多项式复杂度的旅行商问题(TravellingSalesmanProblem,TSP)进行了求解,促进神经网络的研究再次进入了蓬勃发展的时期[26].Hopfield强调工程实践的重要性,他利用电阻、电容和运算放大器等元件组成的模拟电路实现了对网络神经元的描述,把最优化问题的目标函数转换成Hopfield神经网络的能量函数,通过网络能量函数最小化来寻找对应问题的最优解.Hopfield网络是一种循环神经网络,从输出到输入有反馈连接,典型的Hopfield神经网络模型如图2所示.图2中,每组运算放大器及其相关的电阻、电容组成的网络代表一个神经元.每个神经元有两组输入,一组是恒定的外部电流,另一组是来自其他运算放大器输出的正向或反向的反馈连接.假设第i个神经元的内部膜电位为Ui(i=1,2,…,n),细胞膜的输入电容和传递电阻分别为Ci和Ri,神经元的输出电位为Vi,外部输入电流为Ii,并用电阻Rij(i,j=1,2,…,n)来模拟第i个和第j个神经元之间的突触特性.由基尔霍夫电流定律(Kirchhoff’sCurrentLaw,KCL)可知,放大器输入节点处的流入电流和流出电流保持平衡,亦即有下式成立:nVj(t)∑j=1Rij同时,每一个运算放大器模拟了神经元输入和输出之间的非线性特性,即有其中,fi代表了第i个神经元的传递函数,并定义ij(i,j=1,2,…,n)为网络的权系数矩阵.为W=R-1证明连续型网络的稳定性,Hopfield定义了如下的能量函数:E(t)=-其中,f-1为神经元传递函数的反函数.经过推导后得出以下两点结论:一是对于具有单调递增传递函数且对称权系数矩阵的网络来说,其能量会随着时间的变化而趋于稳定;二是当且仅当网络中所有神经元的输出不再随时间变化时,则可以认为网络的能量保持不变.在将网络用于求解诸如旅行商的组合优化问题时,Hopfield将优化的目标函数转化为网络的能量函数,对应地将待求解问题的变量用网络中神经元的状态来表示.由这样的表示方式可知当网络的能量衰减到稳定值时,问题的最优解也随之求出.Hopfield网络一个重要的特点是它可以实现联想记忆功能,亦即作为联想存储器.当网络的权系数通过学习训练确定之后,即便输入不完整或者部分不正确的数据,网络仍旧可以通过联想记忆来给出完整的数据输出结果.Hopfield提出该模型后,许多人试图对其进行进一步的扩展,以希望能够设计出更接近人脑功能特性的网络模型.1983年,“隐单元”的概念首次被Sejnowski和Hinton提出,并且他们基于此设计出了波尔兹曼机(BoltzmannMachine,BM),其结构如图3所示[27-28].波尔兹曼机是一种由随机神经元全连接组成的反馈神经网络,其包含一个可见层和一个隐层.网络中神Page6经元的输出只有两种状态(未激活和激活,用二进制0和1表示),其取值根据概率统计规则决定.波尔兹曼机具有较强的无监督学习能力,可以从数据中学习到复杂的知识规则,然而也存在着训练和学习时间过长的问题.此外,不仅难以准确计算BM所表示的分布,得到服从BM所表示分布的随机样本也很困难.基于以上原因,对波尔兹曼机进行了改进,引入了限制波尔兹曼机(RestrictedBoltzmannMachine,RBM)[29].相比于波尔兹曼机,RBM的网络结构中层内神经元之间没有连接,尽管RBM所表示的分布仍然无法有效计算,但可以通过Gibbs采样得到服从RBM所表示分布的随机样本.Hinton于2002年提出了一个RBM学习的快速算法(对比散度),只要隐层单元的数目足够多时,RBM就能拟合任意离散分布[30].RBM已被用于解决不同的机器学习问题,比如分类、回归、降维、高维时间序列建模、语音图像特征提取和协同过滤等方面[31-33].同时,作为目前深度学习主要框架之一的深度信念网也是以RBM为基本组成单元的.这一阶段的神经网络已经从起初的单层结构扩展到了双层,隐含层的出现使得网络具有更强的数据表示能力.虽然层数的增加可以为网络提供更大的灵活性,但是参数的训练算法一直是制约多层神经网络发展的一个重要瓶颈.直到1974年,Werbos在他的博士论文里提出了用于神经网络学习的BP(BackPropagation)算法,才为多层神经网络的学习训练与实现提供了一种切实可行的解决途径,同时在1986年由Rumelhart和McCelland为首的科学家小组对多层网络的误差反向传播算法进行了详尽的分析,进一步推动了BP算法的发展[34-37].BP网络的拓扑结构包括输入层、隐层和输出层,它能够在事先不知道输入输出具体数学表达式的情况下,通过学习来存储这种复杂的映射关系.其网络中参数的学习通常采用反向传播的策略,借助最速梯度信息来寻找使网络误差最小化的参数组合.常见的3层BP网络模型如图4所示.其中,各节点的传递函数f必须满足处处可导的条件,最常用的为Sigmoid函数,第i个神经元的净输入为neti,输出为Oi.如果网络输出层第k个神经元的期望输出为y由于BP算法按照误差函数E的负梯度修改权值,故权值的更新公式可表示为其中,t表示迭代次数,gt=层神经元权值的更新公式为wt+1kj=wt其中,δk称作输出层第k个神经元的学习误差.对隐含层神经元权值的更新公式为wt+1ji=wt=wt=wt=wt=wt其中,δj称作隐含层第j个神经元的学习误差.BP的误差反向传播思想可以概括为:利用输出层的误差来估计出其直接前导层的误差,再借助于这个新的误差来计算更前一层的误差,按照这样的方式逐层反传下去便可以得到所有各层的误差估计.BP算法的提出在一定程度上解决了多层网络参数训练Page7难的问题,但是其自身也存在如下一些问题.首先,误差在反向传播过程中会逐渐衰减,经过多层的传递后将会变得很小,这使得BP在深层网络中并不可行.其次,BP采用最速梯度下降的优化思想,而实际问题的误差函数通常不是凸的,存在众多局部极小值点,算法很难得到最优解.再次,由于训练过程中依靠于导数信息来进行权值的调整,当权值调节过大时会使大部分神经元的加权和过大,致使传递函数工作于S型函数的饱和区,所以权值的调整会出现停顿的情况.最后,对于一些复杂网络的优化问题,BP算法受到学习速率的限制需要花费几个小时,甚至更长的时间来完成训练任务.此后于1989年,Cybenko、Funahashi、Hornik等人相继对BP神经网络的非线性函数逼近性能进行了分析,并证明了对于具有单隐层、传递函数为sigmoid的连续型前馈神经网络可以以任意精度逼近任何复杂的连续映射[38-40].根据研究结果显示,只要隐层神经元的个数足够多,BP神经网络就能够保证对复杂连续映射关系的刻画能力,具有重要的理论和现实指导意义.继BP之后,为模拟生物神经元的局部响应特性,Broomhead和Lowe于1988年将径向基函数引入到了神经网络的设计中,形成了径向基神经网络RBF[41].后来,Jackson和Park分别于1989年和1991年对RBF在非线性连续函数上的一致逼近性能进行了论证[42-43].RBF神经网络是一种3层的前向网络,其基本工作原理是:利用RBF构成的隐藏层空间对低维的输入矢量进行投影,将数据变换到高维空间中去,以使原来线性不可分的问题能够变得线性可分.图5为径向基神经网络的基本结构示意图.由于输入层在RBF网络中仅仅起到信号的传输作用,故而输入层和隐含层之间的连接权值都为1,隐含层实现对输入特征的非线性投影,而输出层则负责最后的线性加权求和.RBF网络中待学习优化的参数包括:基函数的中心和方差以及隐含层到输出层的连接权值.输出层负责通过线性优化策略来实现对权值的优化,学习速度通常较快;而隐含层则需要采用非线性优化的方法对激活函数的参数调整,故而其学习速度较慢.RBF网络的参数学习方法按照径向基函数中心的选取有不同的类型,主要包括自组织选取法、随机中心法、有监督中心法和正交最小二乘法等.以自组织法为例,其学习主要包括两个阶段,第一阶段为无监督和自组织学习阶段,用以确定隐含层基函数的中心及方差;第二阶段是有监督学习过程,可实现隐含层到输出层之间的连接权值的求解.RBF网络有很快的学习收敛速度,一个很重要的原因在于其属于局部逼近网络,不需要学习隐含层的权值,避免了误差在网络中耗时的逐层传递过程.RBF网络也是神经网络真正走向实用化的一个重要标志,其已被成功应用于非线性函数逼近、模式分类、控制系统建模、时变数据分析和故障分析诊断等工程领域[43-45].应当指出的是蔡少棠等人提出了细胞神经网络(CellularNeuralNetworks)[46-47],Zhang等人提出了小波神经网络[48],Jiao等人提出了多小波神经网络[49],Yang等人提出了脊波神经网络[50],这些模型在非平稳、非线性、非高斯信号与图像处理中表现出良好的应用潜力和价值.此后,神经网络与机器学习和模式识别的融合呈现出前所未有的局面,SVM、PCA、ICA、LDA等模型得到广泛关注和研究,表现出良好的性能,有力促进了这一领域的进展.其中,薄列峰等人提出的大规模SVM[51-53]是这方面的典型代表.进入21世纪以来,国内外在神经网络的理论和应用研究上也取得了若干突破性成果.特别应当指出的是,香港中文大学的Xu提出了Bayes学习机和Y-Y机,并证明了EM算法的收敛性[54-55],清华大学Zhang等人提出了PLN神经网络模型[56],中国科学院半导体研究所王守觉等人对神经网络的硬件实现及其在模式识别领域的应用进行了广泛而深入的研究[57-58]等.复旦大学陈天平教授、西安交通大学徐宗本教授在多层和径向基神经网络的逼近性能以及Cohen—Grossberg和具有时延的Hopfield神经网络的稳定性方面开展了相关研究,并得出了一些具有指导意义的结论[59-62].在生物神经网络模型与机理方面,张香桐、郭爱克、汪云九、陈琳、汪德亮、刘德荣等人做了先驱性的工作,赢得了国际同行的赞誉.伯明翰大学的姚新将进化计算的搜索机制引Page8入到人工神经网络中,提出了进化人工神经网络的概念,并且对进化神经网络进行集成以提高网络性能[63-64].萨里大学的金耀初利用多目标遗传算法进行神经网络的正则化和集成,并且将网络用于复杂系统的建模和控制当中[65-66].中国科学技术大学陈国良院士提出了主从通用神经网络模型,并且开发出了通用并行神经网络模拟系统,为神经网络提供了高级描述语言以及编辑和可执行环境[67-68].上海交通大学赵同和戚飞虎等人提出了基于遗传算法的协同神经网络中参数的优化方法[69-70].清华大学吴佑寿等人,中国科学院自动化研究所戴汝为、刘迎建等人在汉字图像识别上取得了较Hopfield网络更优的性能.华中科技大学廖晓昕和四川大学章毅在神经网络的稳定性和收敛性方面进行了深入研究.东南大学曹进德等人对具有时延的CNN网络、回归神经网络、Cohen—Grossberg网络和联想记忆网络等的稳定性和周期性进行了深入的研究[71-74].南京大学周志华等人于2001年提出了用遗传算法来进行多个神经网络的选择性集成的模型GASEN[75-76],并证明集成部分网络比使用单个网络或者集成所有网络有更强的泛化能力;于2003年分别提出了用于解释集成神经网络功能的方法REFNE[77],此方法可以提取出具有高保真度或强泛化能力的规则来提高集成网络的可理解性,以及可用于医疗诊断的模型C4.5Rule-PANE[78],此模型结合了集成神经网络的强泛化能力和C4.5规则推理的高度可理解性;于2004年提出了一种全新的决策树算法NeC4.5[79],在UCIMachineLearningRepository上取得了较传统C4.5方法更优的分类性能;同时于2006年提出了通过训练代价敏感的神经网络来解决样本不平衡问题的新方法[80].西南大学廖晓峰等人在带时滞神经网络的鲁棒性和稳定性方面做出了突出贡献,研究成果在模式识别和自动控制领域得到了广泛应用[81-83].南京航空航天大学陈松灿PARNEC团队相继提出了ICBP(ImprovedCircularBackPropa-gation)、DLS(DiscountedLeastSquares)-ICBP、ChainedDLS-ICBP和Plane-Gaussian等神经网络模型,用以提升神经网络的泛化和适应能力,并更好地解决局部极小值问题[84-87].香港中文大学王军对递归神经网络及其在线性规划、最短路径寻优、降秩矩阵伪逆求解等问题的应用上进行了深入的研究,推动了神经网络在工程领域的应用[88-90].西安交通大学郑南宁使用确定性退火方法训练径向基神经网络,取得了较传统BP算法更好的学习精度和泛化能力,同时降低了学习所需的时间[91].此外,国内外一些学者和专家也出版了关于神经网络方面的系统论著.在国内,主要包括西安电子科技大学焦李成所编著的《神经网络系统理论》[92]、《神经网络的应用与实现》[93]和《神经网络计算》[94]、北京邮电大学钟义信等人合编的《智能理论与技术———人工智能与神经网络》[95]、四川大学章毅编写的《ConvergenceAnalysisofRecurrentNeuralNetworks》[96]、中国科学院计算所史忠植编写的《神经网络》[97]、西南交通大学靳蕃等人编著的《神经网络与神经计算机》[98]、南京大学周志华编著的《神经网络及其应用》[99]、复旦大学张立明编著的《人工神经网络的模型及其应用》[100]、中国科学院自动化研究所黄秉宪编著的《脑的高级功能与神经网络》[101]、北京工商大学韩力群编著的《人工神经网络教程》[102]和《人工神经网络理论、设计及应用》[103]、清华大学袁曾任编写的《人工神经元网络及其应用》[104]、北京理工大学陈祥光和裴旭东编著的《人工神经网络技术及应用》[105]、北京交通大学罗四维编著的《人工神经网络建造》[106]、浙江大学杨建刚编著的《人工神经网络实用教程》[107]、合肥工业大学高隽编著的《人工神经网络原理及仿真实例》[108]、上海海事大学朱大奇和史慧编著的《人工神经网络原理及应用》[109].在国外,主要包括美国俄克拉何马州立大学的Hagan等人编写的《NeuralNetworkDesign》[110]、加拿大麦克马斯特大学Haykin编写的《NeuralNetworksandLearningMachines》[111]和《NeuralNetworks:AComprehensiveFoundation》[112]、美国路易斯维尔大学的Zurada编写的《IntroductiontoArtificialNeuralSystems》[113]、美国卡内基梅隆大学的Mitchell编写的《MachineLearning》[114]、美国休斯顿大学的Freeman和Skapura合著的《NeuralNetworks:Algorithms,Applications,andProgrammingTechniques》[115]、美国乔治亚南方大学的Fausett编写的《FundamentalsofNeuralNetworks:Architectures,AlgorithmsandApplications》[116]、澳大利亚莫纳什大学的Veelenturf编写的《AnalysisandApplicationsofArtificialNeuralNetworks》[117]、荷兰阿姆斯特丹大学的Krose等人编写的《AnIntroductiontoNeuralNetworks》[118]、英国佩斯利大学的Fyfe编写的《ArtificialNeuralNetworks》[119]、美国麻省理工学院的Kasabov和Arbib分别编写的《FoundationsofNeuralNetworks,FuzzySystems,Page9andKnowledgeEngineering》[120]和《TheHandbookofBrainTheoryandNeuralNetworks》[121]、加拿大萨斯喀彻温大学的Gupta等人编著的《StaticandDynamicNeuralNetworks:FromFundamentalstoAdvancedTheory》[122]、由Taylor编辑出版的《MethodsandProceduresfortheVerificationandValidationofArtificialNeuralNetworks》[123]、西班牙拉科鲁尼亚大学的Rabual和Dorado合著的《ArtificialNeuralNetworksinReal-lifeApplications》[124]、俄罗斯莫斯科物理技术学院的Galushkin编写的《NeuralNetworksTheory》[125].以上著作从神经网络的基本原理、网络设计优化以及网络的应用等角度对神经网络做了系统的梳理和阐释,是学习和研究神经网络的重要参考书籍.同时,神经网络也为一些学科的发展奠定了坚实的基础,形成了新的理论体系和方法论.其中,西安电子科技大学焦李成等人编著的《自适应多尺度网络理论与应用》[126]和《智能目标识别与分类》[127]、清华大学杨行峻编著的《人工神经网络与盲信号处理》[128]、清华大学阎平凡和张长水编著的《人工神经网络与模拟进化计算》[129]、中国科学技术大学丛爽编著的《神经网络、模糊控制及其在运动控制中的应用》[130]、东北大学虞和济编著的《基于神经网络的智能诊断》[131]、哈尔滨工业大学权太范编著的《信息融合神经网络———模糊推理理论与应用》[132]、华中科技大学廖晓昕编著的《StabilityofDynamicalSystems》[133]和《动力系统的稳定性理论和应用》[134]、中国科学院大学的刘德荣等人编著的《QualitativeAnalysisandSynthesisofRecurrentNeuralNet-works》[135]、国防科学技术大学的胡德文等人编著的《神经网络自适应控制》[136]、中国科学院自动化所戴汝为院士编著的《人工智能》[137]、清华大学罗发龙和李衍达院士合著的《神经网络信号处理》[138]、清华大学阎平凡等人编著的《神经网络与模糊控制》[139]和《人工神经网络与模拟进化计算》[140]、东北大学张化光编著的《递归时滞神经网络的综合分析与动态特性研究》[141]、中国科学院合肥智能所黄德双编著的《神经网络模式识别系统理论》[142]是这方面的典型代表.3深度学习研究进展神经网络曾是机器学习领域中一个特别火的研究方向,但由于其容易过拟合且参数训练速度慢,后来又慢慢淡出了人们的视线.传统的人工神经网络相比生物神经网络是一个浅层的结构,这也是为什么人工神经网络不能像人脑一样智能的原因之一.随着计算机处理速度和存储能力的提高,深层神经网络的设计和实现已逐渐成为可能.2006年,一篇题为《ReducingtheDimensionalityofDatawithNeuralNetworks》的文章在《科学》杂志上发表[12],掀起了深度学习在学术界和工业界的研究热潮,其作者是来自加拿大多伦多大学的教授Hinton和他的学生Salakhutdinov.他们在文中独辟蹊径阐述了两个重要观点:一是多隐层的神经网络可以学习到能刻画数据本质属性的特征,对数据可视化和分类等任务有很大帮助;二是可以借助于无监督的“逐层初始化”策略来有效克服深层神经网络在训练上存在的难度.正如之前提到的,神经网络的训练算法一直是制约其发展的一个瓶颈,网络层数的增加对参数学习算法提出了更严峻的挑战.传统的BP算法实际上对于仅含几层的网络训练效果就已经很不理想,更不可能完成对深层网络的学习任务.基于此,Hinton等人提出了基于“逐层预训练”和“精调”的两阶段策略,解决了深度学习中网络参数训练的难题[143-145].继Hinton之后,纽约大学的LeCun、蒙特利尔大学的Bengio和斯坦福大学的Ng等人分别在深度学习领域展开了研究,并提出了自编码器[146-150]、深度置信网[151-155]、卷积神经网络等深度模型[156-160],在多个领域得到了应用.2015年CVPR收录的论文中与深度学习有关的就有近百篇,应用遍及计算机视觉的各个方向.以下将对深度学习在过去十年内的发展进行一定的梳理,并对一些典型的深度学习模型进行回顾和分析.自编码器(Autoencoder)是一种无监督的特征学习网络,它利用反向传播算法,让目标输出值等于输入值,其结构如图6所示.对于一个输入x∈首先将其通过一个特征映射得到对应的隐藏层表示h∈m,隐藏层表示接着被投影到输出层y∈Page10且希望输出与原始输入尽可能相等.自编码器试图学习一个恒等函数,当隐藏层的数目小于输入层的数目时可以实现对信号的压缩表示,获得对输入数据有意义的特征表示.通常隐层权值矩阵和输出层权值矩阵互为转置,这样大大减少了网络的参数个数.当输入数据中包含噪声时,自编码器的性能将会受到影响.为了使自编码器更加鲁棒,2008年Vincent及Bengio等人提出了去噪自编码器(DenoisingAutoencoder)的概念,在输入数据进行映射之前先对其添加随机噪声,然后将加噪后的数据进行编码和解码操作,并希望解码出来的输出信号能够逼近原来的干净输入信号[161].去噪自编码器的原理如图7所示.图7中,x是原始信号,x是加噪后的信号,h是编码后的信号,y是解码后的信号,d(x,y)是原始信号和解码后信号的差异,通常希望其越小越好.通过在原始信号中加入一定量的随机噪声来模拟真实数据中存在的干扰,可以更加鲁棒地从数据中学习到有意义的特征.如果将稀疏性引入到自编码器中还可以得到另一种被称为稀疏自编码器(SparseAutoencoder)的网络,这种网络限制每次获得的编码尽量稀疏,从而来模拟人脑中神经元刺激和抑制的规律.稀疏自编码器的优化模型如下:min其中,h=WTx为编码后的信号,通过式(21)中的第二项可以约束编码信号足够稀疏,来获得对原始信号更加紧凑简洁的表示.同时,将若干个自编码器堆叠在一起可以形成栈式自编码器,这种深层网络能学习到输入信号的层次化表示,更有利于提取数据中所蕴含的抽象特征[162].一个简单的栈式自编码器的结构如图8所示.首先,将原始数据x输入到栈式自编码器中,通过第一层的编码得到原始数据的一阶特征表示h1,然后将此一阶特征作为下一个自编码器的输入,对其进行进一步的编码得到二阶特征h2,如此重复进行直到编码完毕.编码后的各阶特征便构成了对原始数据的层次化描述,可以用于后续的分类和识别任务中.在训练阶段,首先从第一层开始,按照单个自编码器的训练方式逐层训练网络参数,接着将最后一层的输出和期望输出的误差进行逐层反向传播,微调网络各层的参数.深度信念网(DeepBeliefNetwork,DBN)是由Hinton在2006年提出的,它是一种生成模型,通过训练神经元之间的权重,可以让整个神经网络按照最大概率来生成训练数据,其结构如图9所示[10].DBN是由多层RBM堆叠而成的,神经元可以分为显性神经元和隐性神经元,显性神经元用于接受输入,隐性神经元用以提取特征,最顶上的两层连接是无向的用以组成联想记忆单元.DBN的参数学习过程分为无监督贪婪逐层训练和ContrastiveWake-Sleep调优两个阶段.在第一阶段,首先充分训练第一个RBM,接着固定第一个RBM的权重和偏置,将其隐层神经元的状态作为第二个RBM的输入向量,再对第二个RBM进行训练,如此依次进行.如果顶层的RBM除了有显性神经元还包括代表分类标签的神经元,则需要将其一起进行训练.第二个阶段是参数的调优过程,分为Wake(认知过程)和Sleep(生成过程)两个子阶段.在Wake阶段,外界的输入特征在向上权重的作用下产生出各层的输出,再基于梯度下降法来修改各层的下行权重;在Sleep阶段通过顶层表示和向下权重生成底层的状态,同时修改向上的权重.DBN在特征提取过程中,先将输入信号拉成向量再投入到网络中,忽略了数据中存在的空间结构,在处理图像和视频这样的二维或高维信号时会存在一定问题.同时,当输入信号的维数过高且网络的深度过Page11深时,网络中需要训练的参数会很多,这给存储和计算提出了很高的要求.目前,更为常用的一种深层网络(也是首个被真正成功训练的深层网络)是卷积神经网络(ConvolutionalNeuralNetworks,CNNs),它在处理手写体识别等图像处理任务中表现出了优越的性能.卷积神经网络采用了局部感受野、权值共享以及时间或空间亚采样的结构思想,使得网络中自由参数的个数大大减少,降低了网络参数选择的复杂度,其在识别具有移位、伸缩或扭曲不变的二维模式时有很好的性能,最为典型的例子便是用于美国大多数银行支票上手写数字识别的LeNet-5[163-164].卷积神经网络的基本原理如图10所示.CNN是一种多层网络结构,每一层是由包含多个独立神经元的若干二维平面组成的,这些二维平面通常被称作卷积核.以图10为例,输入图像首先和3个可调节的卷积核进行卷积运算,在C1层得到对应的3个特征映射图,然后对每一幅特征映射图进行池化后经过Sigmoid函数产生S2层的3幅特征映射图.这些特征映射图继续通过滤波操作后得到C3层,再按照和S2层一样的处理方法得到S4的结果.在最后一层将所有像素光栅化拉成一个列,并输入到传统的神经网络之中得到最终的输出结果.CNN一个很重要的特点就是通过局部感受野、权值共享以及时间或空间亚采样3种思想减少了网络中自由参数的个数,获得了某种程度的位移、尺度、形变不变性.CNN的训练算法上仍然采用传统的误差反向传播思想,通常具有较快的收敛速度.2014年,Mnih等人针对图像分类任务提出了基于视觉注意的递归神经网络,进一步降低了卷积神经网络在处理图像时的计算开销[165].通过视觉注意机制有选择地从图像中提取有利于分类的特征,可以更准确地对待识别物体进行描述,同时避免了冗余的全局处理.深度学习思想的提出是对传统特征选择与提取框架的突破,正对包括计算机视觉、自然语言处理(NaturalLanguageProcessing,NLP)、生物医学分析、遥感影像解译在内的诸多领域产生越来越重要的影响.Krizhevsky等人训练了一个包含65万个神经元的深层卷积网络来对包括1000类的120万幅图像进行分类,并取得了比经典方法更优的分类性能[166-167].Farabet等人提出了多尺度卷积神经网络来捕捉区域的纹理、形状和文本特征用以场景标注,避免了手工特征的设计和组合所存在的问题[3].Ciresan等人提出基于GPU并行加速的卷积神经网络,用以自动学习交通标识符的特征,在德国交通标识符数据库上取得了比人工判读更高的识别率[168].Tang等人提出了鲁棒波尔兹曼机(RobustBoltzmannMachine,RoBM),使得波尔兹曼机对干扰更加鲁棒,在人脸数据集上的去噪和修补任务中取得了更好的效果[33].Mohamed等人提出了用深度信念网来进行语音建模,克服了传统的隐马尔可夫模型在建模时所施加的条件独立假设,能提取出深层的表示特征用以识别任务[7].Socher等人提出了基于递归自编码器(RecursiveAutoencoders,RAEs)的自然语言释义检测(ParaphraseDetection),借助于无监督的RAEs来学习句子的特征向量,并将学习到的特征用以度量两个句子中词和短语的相似性,在MSRP释义语料库上取得了较经典方法更优的性能[148].Glorot等人基于堆栈去噪自编码器从评论数据中无监督地学习特征用于情感分类,在亚马逊等工业数据集上取得了较经典方法更优的分类精度[169].Zhao等人将2013年ImageNet竞赛上获胜的卷积网络用以显著检测建模,通过全局和局部文本的联合学习在多个测试数据集上取得了较传统方法更优的检测性能[6].Williamson等人使用深度神经网络估计语音的非负激活矩阵,用以从噪声中提取干净语音信息,并获得了比Masking和NMF方法更好的提取效果[170].徐宗本等人提出用卷积神经网络来预测图像块运动模糊的概率分布,在单幅图像的非均匀运动去模糊问题上取得了较为理想的结果[171].Ouyang等人将行人检测问题中的特征提取、变形和遮挡处理以及分类4个模块统一于深度学习框架之下,通过各部分之间的协同来达到整体性能的提升,并在最大的行人检测数据库Caltech上,以9%的优势超越之前最好的方法[172].类似地,在姿态估计问题上也将视觉表象得分、表象混合类型和变形这3类信息结合起来,统一于多源深度模型之中,在3组基准数据集上较现有方法性能提高了8.6%[173].Wang等人通过离线的方式从自然图像中训练了用于描述待跟踪物体特征的堆栈去噪自Page12编码器,在复杂场景中可以提取出更加通用的特征用于分类,并在一些具有挑战性的视频序列上获得了比经典方法更准确的跟踪精度和更低的时间开销[174].Sun等人将卷积神经网络和受限玻尔兹曼机结合起来,组成了混合的深度神经网络用于人脸验证,在LFW数据集上获得了更优的验证性能[175].Wan等人提出用深度学习模型来尝试解决“语义鸿沟”问题,并在基于内容的图像检索问题上验证了所提思路的有效性[176].Dong等人提出利用卷积神经网络来直接学习从低分辨到高分辨图像的映射关系,并且将传统基于稀疏表示的方法统一于此框架之下,通过联合优化的方式得到了更好的超分辨重建效果[177].在通用结构的设计上,Jia等人开发了深度卷积网络模型Caffe,可用于大规模工业应用领域,并且已被用作多个问题的求解方案[178].Eitel等人提出了基于双层CNN的RGB-D物体识别网络结构,并在含有噪声的RGB-D物体数据库上取得了最优的识别结果[179].Palangi等人将深度学习的概念和序列建模的方法结合起来,用于提升多观测向量下的压缩感知问题的求解性能[180].为避免无参考图像质量评价过程中繁琐的手工特征设计,Kang等人提出将特征学习和回归过程统一到CNN的优化框架下,并在LIVE数据集上取得了最优性能,且该方法可用于图像局部质量的评估[181].Chen等人提出通过学习的方式来让CNN选择最优的局部接收场,并用于汉字手写体识别,极大地提升了传统CNN的性能[182].Maturana等人将体积占用图和3DCNN进行耦合,设计出了可用于检测植被覆盖地区潜在的被遮挡障碍物的系统,并将其应用到激光雷达点云下的自主飞行器安全降落区域的检测中,取得了较为理想的效果[183].Tomczak将分类受限玻尔兹曼机(ClassificationRestrictedBoltzmannMachine)作为独立的非线性分类器用于5类不同的医学问题领域,并通过在模型中添加正则项来获得稀疏解[184].Zhang等人提出了Coarse-to-fine的自编码网络CFAN用于人脸对准,首先用第一组堆栈自编码网络(StackedAuto-encoderNetworks,SANs)来快速预测脸部的特征点,之后用第二组堆栈自编码网络来对其修正,在3组数据集上CFAN都取得了实时且最优的性能[185].Mi等人将堆栈自编码器用于垃圾邮件检测任务,取得了较朴素贝叶斯、支撑向量机、决策树、集成、随机森林和传统神经网络更优的性能[186].在遥感领域,Yue等人将卷积神经网络用于高光谱图像分类[187]、Zhou等人将自编码器用于高分辨遥感影像的检索任务[188],并获得了较为满意的分类和检索结果.目前,深度网络的自动特征提取能力正受到自然、生物医学和遥感等多个领域的广泛关注,并且基于深度网络的方法在多个任务上都显示出了优越的性能,在未来将会有更加广阔的应用前景[189-190].近年来深度学习的研究方兴未艾,这方面的书籍也不断涌现.其中具有代表性的著作有加拿大蒙特利尔大学的Bengio编写的《LearningDeepArchitecturesforAI》[11]、美国伊利诺伊大学的Ohlsson编著的《DeepLearning:HowtheMindOverridesExperience》[191]、美国麻省理工学院的Buduma编著的《FundamentalsofDeepLearning》[192]、微软公司的Deng和Yu合著的《DeepLearning:MethodsandApplications》[193]和《AutomaticSpeechRecognition:ADeepLearningApproach》[194]、美国伊利诺伊大学的Nath和Levinson合著的《AutonomousRoboticsandDeepLearning》[195]等.4总结和展望作为联接主义智能实现的典范,人工神经网络采用广泛互联的结构与有效的学习机制来模拟人脑智能信息处理的过程,是人工智能发展历程中的重要方法,也是类脑智能研究中的有效工具.在神经网络七十年的发展历程中,曾经几遭冷遇又几度繁荣.本文回顾了神经网络在过去七十年的发展历程,介绍了神经网络在各个发展阶段所取得的成果和面临的挑战.未来基于神经网络的类脑智能的研究还有许多亟待解决的问题与挑战:(1)认知神经网络尽管深度神经网络在语音识别和图像/视频识别等任务中显示出很大的优势,现有的人工神经网络结构还远远不及生物神经网络结构复杂,仍然是对生物神经系统信息处理的初级模拟,这是制约神经网络智能化水平的一个重要瓶颈.目前深层神经网络仅能完成一些简单的语音与视觉理解任务,在理论上还存在很多局限,训练网络的学习算法目前也十分有限.另一方面,神经认知计算科学对视觉注意力、推理、抉择、学习等认知功能的研究方兴未艾.如何从脑科学和神经认知科学寻找借鉴,从理论上发展出功能更加强大的类脑计算模型如认知神经网络,来解决人工智能面临的局限,将有可望实现更高层次Page13的类脑智能.(2)主动神经网络生物个体在与环境接触过程中,智能水平会得到提高.人脑可以在没有监督信息时主动地从周围环境中学习,实现对客观世界中物体的区分.因此,如果要实现更加高级的智能行为,现有神经网络的发展需要突破利用神经元与网络结构的结构模拟思路,从结构模拟向功能模拟乃至行为模拟转换,借鉴人与环境之间的交互过程,主动且自动地完成增强学习,以摆脱对监督信息的依赖,在更严苛的环境下完成学习任务,这也是实现高级类脑智能的可能途径.(3)感知-理解-决策神经网络类脑智能行为可以大概归结为“感知”、“理解”与“决策”这3个方面.目前的神经网络模型的功能大都局限在对数据的理解层面,而事实上一个高级的智能机器应该具有环境感知与推理决策的功能.如何发展具有环境感知、数据理解以及推理决策能力的网络模型,也是实现高级类脑智能的必然要求.(4)复杂神经网络实现机器计算能力的提高曾经将神经网络重新拉回大众关注的视野.对于许多互联网公司来说,如何实现对海量大数据的快速高效训练是深层神经网络走向实用化的重要标志.现有的Hadoop平台不适合迭代运算,SGD又不能依并行方式工作,而GPU在训练DNN时仍然显得比较吃力.同时,平台的能耗问题也成为制约其进一步发展的主要因素.为迎接未来深度学习在产业化过程中的挑战,高性能并行加速计算平台的开发成为当务之急.另一方面,生物神经元之间的连接带有随机和动态性,而不是如人工神经网络那样确定和一成不变,如何用计算机硬件或者算法来模拟这一过程虽极具挑战但意义重大.(5)深度神经网络深层神经网络一个最主要的特点在于其具有大量可调的自由参数,这使得其构建起的模型具有较高的灵活性.但另一方面却缺乏有力的理论指导和支撑,大多数情况下仍过分依赖于经验,带有一定程度的随机性.如此复杂的模型很容易在特定数据集上得到近乎理想的拟合效果,然而在推广泛化性能上却往往很难得到保障.为防止过拟合带来的问题,今后应当在数据的规模、网络的结构以及模型的正则化等方面开展工作,使得深度神经网络更好地发挥其功能.(6)大数据深度学习深度学习的兴起很大程度上归功于海量可用的数据.当前,实验神经科学与各个工程应用领域给我们带来了呈指数增长的海量复杂数据,通过各种不同的形态被呈现出来(如文本、图像、音频、视频、基因数据、复杂网络等),且具有不同的分布,使得神经网络所面临的数据特性发生了本质变化.这给统计学习意义下的神经网络模型的结构设计、参数选取、训练算法,以及时效性等方面都提出了新的挑战.因此,如何针对大数据设计有效的深度神经网络模型与学习理论,从指数增长的数据中获得指数增长的知识,是深度学习深化研究中必须面临的挑战.
