Page1基于条件对数似然函数导数的贝叶斯网络分类器优化算法王中锋王志海(北京交通大学计算机与信息技术学院北京100044)摘要通常基于鉴别式学习策略训练的贝叶斯网络分类器有较高的精度,但在具有冗余边的网络结构之上鉴别式参数学习算法的性能受到一定的限制.为了在实际应用中进一步提高贝叶斯网络分类器的分类精度,该文定量描述了网络结构与真实数据变量分布之间的关系,提出了一种不存在冗余边的森林型贝叶斯网络分类器及其相应的FAN学习算法(Forest-AugmentedNaveBayesAlgorithm),FAN算法能够利用对数条件似然函数的偏导数来优化网络结构学习.实验结果表明常用的限制性贝叶斯网络分类器通常存在一些冗余边,其往往会降低鉴别式参数学习算法的性能;森林型贝叶斯网络分类器减少了结构中的冗余边,更加适合于采用鉴别式学习策略训练参数;应用条件对数似然函数偏导数的FAN算法在大多数实验数据集合上提高了分类精度.关键词机器学习;数据挖掘;分类器;贝叶斯网络;鉴别式训练策略1引言分类器的设计是数据挖掘和机器学习领域的主要研究内容之一.该领域提出了大量的分类器模型和学习算法.在这些模型中,贝叶斯网络分类器不但理论基础坚实,而且在实际应用中有较强的抗噪声性能和健壮性能,得到了多年的持续研究,提出了许多的学习算法[1].但是,在这些学习算法中,大多数仅是将贝叶斯网络分类器看作贝叶斯网络来处理,而没有考虑其作为分类器的特殊需求[2-4].一般而言,以贝叶斯网络的表达能力为评价标准指导设计的贝叶斯网络分类器学习算法归为生成式学习策略,以分类器的精度为评价标准设计的归为鉴别式学习策略[4].在理想状态下,与数据变量分布一致的贝叶斯网络同时也是分类精度最高的贝叶斯网络分类器.但在实际应用中,两者难以保持一致.这主要是因为:(1)在现实中,迫于计算机的性能,从计算理论的角度考虑,搜索到精确的结构问题是一个NP难题[5];(2)在应用中,采样工具的选择,实验人员的熟练程度,统计方法的局限性,都会导致训练数据不能准确地代表整体数据分布,且这种差异是不可预期的[6];(3)在训练数据的数目一定的情况下,贝叶斯网络分类器的结构越复杂,条件概率表就越庞大,对网络中各个参数的估计就会越不可靠,其分类性能就有可能下降.考虑到这些实际情况,通常在设计贝叶斯网络分类器模型和算法时,限制网络结构的形式,来降低与样本的拟合程度,提高其泛化性能.这类模型和算法有朴素贝叶斯网络分类器(NaveBayes,NB)、树型限制性贝叶斯网络(Tree-AugmentedNaveBayesClassifier,TAN)分类器[2]、双层贝叶斯分类器(Double-LevelBayesianNetwrokAugmentedNaveBayes,DLBAN)[7]、消极贝叶斯网络分类器学习算法(LazyBayesianRules,LBR)[8]、超父算法(Super-ParentTAN,SP)[9]、平均1-依赖估计算法(AggregatingOne-DependenceEstimators,AODE)[10]、子结构学习算法(SubstructureLearningAlgorithm)[11]等.对于这些限制了网络结构形式的贝叶斯网络分类器模型,最接近数据变量分布的网络结构不一定就是分类精度最高的,因此需要用鉴别式学习策略来设计算法.由于鉴别式学习策略是直接以提高分类器的精度为目标设计学习算法的,在通常情况下采用鉴别式学习策略得到的分类器精度比采用生成式学习策略得到的高[3-4,12-16].近年来,Pernkopf等人[13]提出了一些贝叶斯网络分类器鉴别式结构学习算法,Greiner等人[3-4]提出了ELR参数学习算法,Jing等人[14-15]提出了BNB和BAN参数学习算法,Su等人[16]提出了DFE参数学习算法.这些算法都是基于鉴别式学习策略设计的.但是,本文研究表明:限制性贝叶斯网络分类器鉴别式参数学习策略仅适用于比实际数据变量分布简单的网络结构.贝叶斯网络结构与实际数据变量分布之间存在3种的关系,分别是简单、等于和复杂.当贝叶斯网络结构缺少表示变量之间实际存在的依赖关系的边时,网络结构简单于变量分布;当贝叶斯网络结构恰好能够表示变量之间的依赖关系时,网络结构等于变量分布;当贝叶斯网络结构存在表示变量之间依赖关系的边,而实际变量之间不存在依赖关系时,网络结构复杂于变量分布,并称这些边为冗余边.由于以往基于鉴别式学习策略设计的参数学习算法假设能够在各种网络结构上提高分类精度,所以它们仅对网络结构与实际变量分布之间的关系进行了定性的描述.本文首次定量描述了它们之间这3种关系,提出了一种森林型贝叶斯网络分类器(Forest-AugmentedNaveBayes,FAN),来提高鉴别式参数学习策略的适应性,并提出了一种基于条件对数似然函数偏导数的FAN分类器学习算法,来识别贝叶斯网络结构中的冗余边.本文的主要贡献包括:(1)定量描述贝叶斯网络结构与所描述的数据变量实际分布之间的关系;(2)提出一种森林型贝叶斯网络分类器FAN,FAN模型能够为鉴别式参数学习策略性能的发挥提供必要的保证;(3)提出一种FAN分类器学习算法,该算法首次应用条件对数似然函数偏导数指导贝叶斯网络分类器算法优化;(4)实验验证本文论点的正确性及所提算法的有效性.本文第2节介绍相关工作;第3节定量描述贝叶斯网络结构与所描述的实际数据变量分布之间的关系;第4节提出森林型贝叶斯网络分类器FAN;第5节提出一种FAN分类器学习算法;第6节实验验证森林型贝叶斯网络分类器和FAN算法的有效性;第7节总结全文.2贝叶斯网络分类器贝叶斯网络分类器是一种采用贝叶斯网络进行表示的分类器函数.其学习算法的设计可以应用对Page3数似然函数为评价标准,也可以应用条件对数似然函数为评价标准.通常,采用前者的称为生成式学习策略,采用后者的称为鉴别式学习策略.在正式讨论本文的内容前,为叙述的方便,先约定文中符号的基本含义.变量用大写字母表示(例如:A,Bi,Y),其状态或取值用相应的小写字母表示(例如:a,bi,y).变量的集合用大写黑体表示(例如:犃,Π),变量集合的值用相应的小写黑体表示(例如:犪,π).并用花体字母(例如:,)表示统计模型或图模型.2.1分类器的定义不失一般性,首先给出分类器的定义:若问题域为类对象(或者事物的状态),所有可能的类标签为c1,c2,…,ck,且令C={c1,c2,…,ck},则属性(或特征、变量)对象信息可表达为一个向量,每一个对象都是由一个相应的属性值向量犪∈犃来描述的.给定一个属性值向量,分类任务就是判定这个向量所表达的对象属于类标签c1,c2,…,ck中的哪一个.于是,一个分类器可视为一个映射f:犃→C.对于数据集合D,条件变量犃=(A1,A2,…,An)和类变量C的联合概率分布可以表示为贝叶斯网络=〈,Θ〉,其中,是一个有向无环图,其中的结点都对应于D中的随机变量,有向边表示其终点对应的随机变量依赖于起点对应的随机变量;Θ={θ1,θ2,…,θn}是网络中的参数,由一组条件概率表构成,表θi对应着变量Ai,定义了图中结点间依赖强度的大小.从而,一个贝叶斯网络对应于一种变量的联合概率分布,即P(A1,A2,…,An)=∏n其中:n表示中随机变量的数目;Πi表示随机变量Ai在中的直接父结点对应的变量的合取.进而,基于贝叶斯网络的分类器可以定义为函数f(·).f(犪)=argmaxc∈C{P(c)∏n2.2学习策略贝叶斯网络分类器学习策略有两种:分别是生成式学习策略和鉴别式学习策略.生成式学习策略是先训练一个性能较好的贝叶斯网络,然后再将此网络应用于分类问题.鉴别式学习策略是直接生成一个性能较好的分类器.一般来讲,在分类器的分类精度方面,鉴别式学习策略比生成式学习策略得到的分类器更有竞争力,这是因为生成式学习策略过多地考虑了与类变量无关的属性变量之间的依赖关系.例如,图1给出了一个应用两种策略有可能得到的不同网络结构,图1(a)是应用生成式学习策略得到的,图1(b)是应用鉴别式式学习策略得到的.图1(a)是一个好的贝叶斯网络,却不一定是一个好的贝叶斯网络分类器;图1(b)是一个好的贝叶斯网络分类器,却也不一定是一个好的贝叶斯网络.生成式学习策略通常是以最大化似然函数标准指导学习算法的设计,鉴别式学习策略通常以最大化条件似然函数标准指导学习算法的设计.两者的区别在于采用了不同的评估函数.为了应用上的方便,似然函数常转换为等价的对数似然函数(LogLikelihood,LL)的形式.给定m个实例的数据集合D={犪1,…,犪i,…,犪m},则有对数似然函数表示为LL(fD)=∑m容易看出,当θ(ai,πi)=^PD(ai,πi)时,对数似然函数取得极大值.因此可简单地用观察到的样本出现频率估计贝叶斯网络的参数(Observed-Fre-quencyEstimation,OFE)[16].鉴别式学习策略[15]直接以最大化条件似然函Page4数为标准优化分类器性能.同样,条件似然函数也可以表示为对数条件似然函数(LogConditionalLike-lihood,LCL)的形式,LCL(fD)=∑m虽然式(4)和式(3)表现形式类似,但当θ(ai,πi)=^PD(ai,πi)成立时,只能保证对数似然函数取得极大值,却不能保证对数条件似然函数取得极大值,因此式(4)不能分解为线性形式或任何相近的形式直接计算.3网络结构与变量分布的关系在应用中,由于很多实际原因,往往只能训练得到限制了网络结构形式的贝叶斯网络.这种网络结构与实际数据变量的分布并不一致.为了进一步提高分类器的性能,本文定量地定义了贝叶斯网络结构与实际变量分布之间的关系,方便具体的操作.定义1.假设数据集合D符合某一联合概率分布P,称两个贝叶斯网络和相对于概率分布P是等价的,当且仅当目相同,且一一对应;(1)和中结点的数目与P中随机变量的数(2)和能准确地表示联合概率分布P,记为=|P(简记为=).当=时,说明和关于概率分布P是等价的,但它们有可能表现出不同的结构形式.这是因为在P中条件独立的两个变量在和中对应的两个结点间可能存在边也可能不存在边.定义2.对于一个关于联合概率分布P的贝叶斯网络,假设其中的每一对变量都存在着一条有向边,则称这个贝叶斯网络是联合概率分布P的一个完全贝叶斯网络,记为c.即便在联合概率分布P中条件独立的两个变量,在c中对应的两个结点间也必须存在边,且所有这样的结点间都是相互弱连通的.定义3.对于一个联合概率分布P,所有描述它的贝叶斯网络集合记为犅犛(P)={|=|Pc}.定义4.在犅犛(P)中边数目最少的称为P的最简贝叶斯网络,记为t,即t中所有的边对描述数据集合D的联合概率分布P都是必不可少的.一般地,在以往的研究中,用表示训练得到的网络结构,用表示符合数据集合D内变量实际联合概率分布P的网络结构.将两者的关系描述为这里的可以视为t.当缺少足够的边来描述D中变量间的依赖关系时,记为<;当能够近似描述D时,记为≈.定义中所表达的“≈”近似这一概念很难把握.现在精确定义为,当=t时,=,并追加当∈犅犛(P)且≠t时,>,对于这种结构,又称其为存在冗余边的结构,简称冗余结构.本文将两者的关系重新描述为图2是式(6)中3类关系的示例图,若图2(a)表示与实际变量分布完全一致的贝叶斯网络分类器结构,则缺少边〈A1,A2〉的图2(b)表示比实际变量分布简单的贝叶斯网络分类器结构,而添加边〈A1,A3〉的图2(c)表示存在冗余结构的贝叶斯网络分类器结构.Page5因为在大多数应用问题中,c是无意义的,t是难以保证的,所以在分类任务中,通常采用固定父结点数目的贝叶斯网络结构.Greiner等人[4]利用ELR算法进行了两类实验:第1类采用的网络结构简单于数据集实际分布的网络结构;第2类采用的网络结构接近于数据集实际分布的网络结构.这些实验具有一定的局限性.因为,即便对于结构简单的NB网络或TAN网络[2],也可能由于存在多余的结点与冗余的边而导致其比实际结构复杂.假设={1,2,3},={1,2,3},即由1、2和3构成,由1、2和3构成,设i和i分别包含相同的结点(i=1,2,3),且假设这3组子结构所含结点数目基本相同.当<时,有可能是因为{1<1,2<2,3<3},也有可能是因为{1<1,2<2,3>3},{1<1,2>2,3<3}或{1>1,2<2,3<3}情况引起的.很明显,后3种情况出现的概率更大.通过以上讨论得知,在所采用的网络结构之中通常存在<,=或>情况.应用贝叶斯网络结构与实际变量分布之间的关系,可以分析出,在一般的情况下,限制了父结点数目的贝叶斯网络结构中不能避免存在冗余的边.下面提出一种不含有冗余边的贝叶斯网络分类器.4森林型贝叶斯网络分类器森林型贝叶斯网络分类器FAN是一种限制网络形式的贝叶斯网络分类器应用模型,其限制策略包括两方面:首先,其结构的属性子网拓扑形式与TAN结构的属性子网近似,但比TAN结构更为灵活,内部结点可以有一个父结点或没有父结点,即整体上可以是一棵树,也可以是由多棵树组成的森林.其次,类变量是属性子网中所有结点的父结点.这样可以保证分类结果能够考虑到每一个特征变量的影响.FAN分类器定义为函数f(·),f(犪)=argmaxc∈C{P(c)∏n图3是一个FAN的结构图示意图.从图中可以看出,FAN是一种介于NB和TAN网络结构之间的限制性贝叶斯网络结构.因为对于任一特征变量ANB∈NB,其|ΠNB|=1,对于任一特征变量ATAN∈TAN,|ΠTAN|=2,且对于任一特征变量AFAN∈FAN,1|ΠFAN|2,所以|ΠNB||ΠFAN||ΠTAN|,因此,可以将NB看做是结构最简单的FAN,将TAN看做是结构最复杂的FAN.5森林型贝叶斯网络分类器学习算法这一节提出一种在现有贝叶斯网络分类器结构训练基础上学习森林型贝叶斯网络分类器的算法,该算法的关键是识别网络结构中的冗余边.当贝叶斯网络结构中有冗余的边的时候,不仅仅是增加计算量的问题,甚至会误导参数学习,降低分类器的性能.因而,常常需要识别网络结构中是否有冗余的边.极大化对数条件似然函数的过程与提高贝叶斯网络分类器分类精度的过程是一致的.梯度上升法以对数条件似然函数的导数为方向极大化函数值,所以对数条件似然函数的偏导数的计算与网络中结点的父结点集合关系紧密.因此可以应用对数条件似然函数的偏导数与网络中结点的父结点集合之间的关系,消除网络结构中冗余的部分,提高鉴别式参数学习方法所得到的分类器精度.条件概率表内的每一个参数都表示一个随机变量Ai和属性变量集合Πi间的随机关系.随机变量Ai是固定不变的,属性变量是一个集合,在贝叶斯网络中也就是Ai的父结点集合,当其中有结点的存在与否对对数条件似然函数的导数没有影响时,认为它们是冗余的信息,并记Ai的这些父结点为sub(Πi).即当LCL()(Θ)θ(aiπi)=LCL()(Θ)成立时,sub(Πi)集合内所包含的结点不应成为变量Ai的父结点.接下来,有两个技术方面的问题需要解决:一是如何计算对数条件似然函数的偏导数,二是以什么地方的导数值为判断的依据.关于第1个问题,可以应用梯度方法鉴别式学习贝叶斯网络参数时的研究成果.梯度方法研究中Page6提出的计算对数条件似然函数的偏导数的方法如下[4,17].用一个贝叶斯网络分类器的网络表示这个数据集合,设条件概率中的各个参数为变量,并计算出这些变量的偏导数.对于其中的一个参数βa|π,为了保证βa|π0且∑aβa|π=1,将其变形为参数“θa|π”,使满足可得出,当θ的定义域是实数时,β能够满足要求.在梯度算法运行中的任一步,参数θ的变化方向可以表示为对数条件似然函数的偏导数,因而结点Ai在取值为ai且父结点集合取值为πi时,对应于数据集合D的导数为LCL()(Θ)θ(aiπi)=∑〈a1,a2,…,an,c〉∈D对于任一训练实例〈犪,c〉,关于参数θ(ai|πi)的偏导数即为LCL(〈犪,c〉)(Θ)θ(aiπi)=[PΘ(ai,πia,c)-PΘ(ai,πia)]-θ(aiπi)[PΘ(πia,c)-PΘ(πia)](10)将式(10)代入式(9)可以计算出函数各个参数的偏导数.关于第2个问题,可以在最大似然函数估计参数之处求偏导数,来判断当时的网络结构与条件对数似然函数的关系.为了发现这些冗余结构,需要寻找在对数条件似然函数最大化过程中冗余结构对函数导数产生影响的位置,计算其导数值,进而应用导数性质分析出冗余部分.可以从对数条件似然函数公式的变形看出LCL(fD)=∑〈a1,a2,…,an,c〉∈D因为等号右边第一项为对数似然函数[2]LL(D)=∑〈a1,a2,…,an,c〉∈D所以,对数条件似然函数可表示为LCL(fD)=LL(D)-∑〈a1,a2,…,an〉∈D上式可以将对数条件似然函数理解为调整后的对数似然函数.因此,最大化对数条件似然函数的过程就是在最大对数似然的基础上,最小化∑〈a1,a2,…,an〉∈D所以在对数条件似然函数最大化过程中,可以应用最大似然函数估计参数之处做判断网络结构对对数条件似然函数导数有影响的点求偏导数.根据以上分析,本文首次基于对数条件似然函数的偏导数提出了一种森林型贝叶斯网络分类器学习算法FAN,该算法能够识别网络中冗余的边,并删除其表示的本不存在的依赖关系.算法的详细描述见算法1所示.算法1.FAN算法.输入:训练数据集合D输出:一个贝叶斯网络分类器f1.应用TAN算法训练一个贝叶斯网络结构.2.应用OFE算法估计贝叶斯网络结构的网络参数,3.分别以条件概率表中各项为变量,计算对数条件似4.对于贝叶斯网络中的每一个结点对应的属性变量5.对于属性变量Ai的在贝叶斯网络中的父结点集合Πi内的每一个结点对应的变量Aj.6.如果将Aj放入集合sub(Πi)后,对于新的贝叶斯网7.应用鉴别式参数学习算法重新估计的网络参数,生8.返回贝叶斯网络分类器f.FAN算法步1根据训练数据集合生成一个贝叶斯网络结构;步2采用生成式方法估计条件概率表的初始值;步3计算对数条件似然函数对应于各个参数项的偏导数;步4~步6,对贝叶斯网络参数中每一项都进行考察,若其父结点集合存在对对数Page7条件似然函数相对于这一项的偏导数无影响的结点,则删除;步7重新应用鉴别式参数学习算法估计网络参数,生成贝叶斯网络分类器;步8返回训练好的贝叶斯网络分类器.FAN算法是一种应用条件对数似然函数偏导数设计的FAN分类器学习算法,该算法能够使贝叶斯网络结构更适合于鉴别式参数学习算法性能的发挥.虽然鉴别式参数学习方法比生成式参数学习方法适应性更强,但要求采用的网络结构不能比实际网络结构复杂,这个问题看似容易解决,但目前的结构学习算法都很难保证,因而需要剔除传统方法生成的贝叶斯网络结构中冗余的边.6实验许多研究通常假设贝叶斯网络结构中不存在冗余的边.然而,这种假设在大多数现实问题中具有不同程度的局限性.可以验证当网络结构近似于或简单于实际数据变量分布时,鉴别式参数学习策略比生成式参数学习策略得到的分类器分类精度高.因此,只需验证在更广泛的情况下,鉴别式参数学习策略比生成式参数学习策略得到的分类器分类精度低,就可以得出通常情况下贝叶斯网络结构中是存在冗余的边的结论.由于基于梯度方法的鉴别式训练算法的运算量大,其实验结果只是在小数据集合上得到了验证.对于自动训练算法,较难学习到恰好符合实际分布的贝叶斯网络结构,所以不易确定网络结构是简单于还是复杂于实际的分布.因此,实验中需要以某一实际联合概率分布的近似描述为基准,删除一些边,生成较简单的网络结构.这里采用TAN结构做为基准网络,没有采用NB结构,是因为在TAN结构中每个属性变量的父结点集合除了类变量之外还能有一个属性变量,但NB网络结构中的属性变量只有类变量一个父结点,若采用NB网络结构为基础进行分析则类同于属性选择.这样设计的实验与以往的实验不同,以往多是直接假设TAN网络结构简单于或近似于实际的分布进行分析的,比如Greiner等人[4]在验证ELR算法理论时的第一组实验同时采用了NB结构和TAN结构,认为这两种结构一般都比实际数据集合的分布简单.比较而言,在基准结构上删除边后作为简单的网络结构来应用更为准确.在实验中,使用DFE算法进行鉴别式参数学习,这是因为ELR算法训练速度太慢,一般只适用于小型数据集.为了扩大本文的实验范围以及说明属性变量数目与算法的关系,在实验中采用更为有效的DFE算法.DFE算法的分类精度一般与ELR算法基本相当[16].序号数据集合实例类值属性缺损值1Audiology2Chess3Connect-44LungCancer5Lymphography6PrimaryTumor7Soybean8SolarFlare9SolarFlare-x10SpliceJunctionGene3177360No11Zoology12Adult13AnnealingProcesses898638Yes14BalanceScale15BreastCancer(Wisconsin)69929Yes16Echocardiogram17GlassIdentificationA214710No18GlassIdentification21479No19Heart20HepatitisPrognosis155219Yes21HypothyroidDiagnosis3163225Yes22Ionosphere23IrisClassification24LaborNegotiations57216Yes25New-thyroid26PenDigits27PimaIndiansDiabetes76828No28Satellite29Shuttle30Sign31SonarClassification208260No32Syncon33WineRecognition178313No实验是在Weka-3-4-11[18]系统平台之上设计与实现的.数据集合来源于UCI的数据库站点①.考虑到在属性变量较多的数据集合上训练的网络结构可能出现冗余边的概率较大,为了强调鉴别式参数学习策略在有冗余边的结构上训练的分类器的性能,选择的数据集合在本文参考文献之中所使用过的基础上适量提高了属性变量多的集合的比重,Greiner等人[4]以Friedman等人[2]的实验数据为基础做的实验,Friedman等人[2]共采用了25个数据集合,数据集合的属性变量数目范围从4个~36个,平均每①AsuncionA,NewmanDJ.UCIMachineLearningReposi-tory.http://www.ics.uci.edu/~mlearn/MLRepository.Page8个数据集合拥有属性变量的数目为15.68,本文采用了33个数据集合,数据集合的属性变量数目范围从4个~69个,平均每个数据集合拥有属性变量的数目为23.85.很明显,Greiner等人[4]和Friedman等人[2]实验选择的数据集合维度较低,代表性不足.表1列出了这33个数据集的具体信息,包括实例数目、类值数目、属性变量数目以及是否有缺损值.其中前11个数据集合只有名称型数据,其它数据集含有连续型属性.实验使用Weka系统中无监督的离散化方法进行预处理.另外,有12个数据集合具有缺损值,在实验中并没有进行特殊处理.表2列出了应用OFE算法估计参数(简称OFE)得到的分类器的误分类率、应用DFE算法估计参数(简称DFE)得到的分类器的误分类率和FAN算法得到的分类器的误分类率.No.126.4602±2.370579.7345±1.413224.1593±0.740428.8203±0.79737.6951±0.20698.0218±0.1519323.7737±0.014323.5484±0.005823.5750±0.0097448.1250±3.563052.5000±4.635146.2500±1.3975517.2973±1.227415.4054±1.208713.9189±0.7704653.2153±0.875156.7552±0.849956.2242±0.991678.6383±0.527912.2694±0.74239.8683±0.2450815.8963±0.206115.7955±0.120515.7955±0.120590.9791±0.03941.3391±0.06441.3391±0.0644105.0299±0.07844.7026±0.03594.6900±0.0385115.9406±2.10032.5743±1.12892.3762±0.54231215.9719±0.063416.0100±0.036316.0080±0.0383134.3207±0.288248.0401±1.867725.0557±0.71301413.3760±0.764013.7920±0.485313.7280±0.5355154.1774±0.50984.6352±0.23943.8627±0.14311639.3893±2.783938.7786±2.720433.1298±1.02411743.6448±0.781941.4953±1.296741.3084±1.42511818.5047±1.886620.0000±0.51199.0654±1.02381922.4445±1.245018.9630±1.153517.8518±1.76462020.5161±1.241018.0645±1.580314.3226±1.6699212.8517±0.13307.9861±0.11097.9229±0.1918227.4644±0.68028.0342±0.54808.1481±0.5908238.6667±2.10827.8666±0.86927.3333±0.81652416.1404±2.602221.0526±3.508814.0351±2.7739255.9535±0.68999.2093±1.00837.3489±0.2080263.7391±0.08633.8828±0.11003.8664±0.08262725.7292±1.068225.1042±0.464024.0625±0.41592812.7055±0.168712.4911±0.054312.4911±0.0543296.0069±0.00245.8642±0.00285.8642±0.00283027.8973±0.168427.9946±0.158127.9946±0.15813122.1154±2.229225.0961±1.964724.1346±1.2900322.5333±0.54525.3333±0.48591.1000±0.3028338.5394±1.74977.9776±0.47003.8202±0.4700图4是采用OFE算法和DFE算法得到的贝叶斯网络分类器误分类率的曲线图和散点图.在误分类率曲线图中,横坐标是数据集合的序号,按其属性变量数目从少到多排列;纵坐标表示这两个算法生成的贝叶斯网络分类器在各个数据集上的平均误分类率的大小.DFE算法生成的分类器的误分类率多项式趋势线用实线表示,OFE算法生成的分类器的误分类率多项式趋势线用虚线表示.图4OFE算法与DFE算法比较的误分类率曲线图和散点图从图4(a)可以看出,在数据集合的属性变量数目较少时,这两个算法生成的分类器的误分类率并无明显差异.随着属性变量数目的增加,DFE算法的误分类率有增长的趋势,而OFE算法的误分类率仍无显著变化.在图4(b)所表示的误分类率散点图内,除了在两个属性变量数目较多的数据集合之上应用DFE算法的效果较差之外,在其它数据集合之上的分布基本在对角线附近.统计结果表明,在18个数据集上DFE算法生成的贝叶斯网络分类器的平均误分类率比OFE算法的高,进一步对之进行双尾配对t检验,可知有10个数据集合在显著性水平0.05下误分类率显著上升;在另外15个数据集上,DFE算法的平均误分类率比OFE算法的低,同样进行双尾配对t检验,有6个在显著性水平0.05下误分类率显著下降.这就说明在27个数据集上基于Page9鉴别式学习策略的DFE算法生成的分类器的误分类率没有显著下降,即两个算法的性能基本接近.Greiner等人[4]已经验证了在<和≈情况下,鉴别式方法生成的分类器性能优于生成式方法生成的分类器性能.但在27个数据集上鉴别式方法生成的分类器性能并不优于生成式方法生成的分类器性能,所以这些网络中可能包含>的情况.显然,本文实验中采用的大部分数据集合生成的贝叶斯网络结构存在冗余的部分,并不像通常认为的,学到的网络结构比实际的复杂只是个别现象,并说明了鉴别式参数学习策略对限制性贝叶斯网络结构的修正是有限的.因此,FAN分类器是一种比TAN分类器更适合鉴别式参数训练的分类模型.FAN算法运行中,式(8)是判断随机变量Ai的父结点集合内是否有多余结点的理论依据.然而,现实数据集合中既可能有噪声又可能有缺损数据.因此,在实验中设置了一个误差范围(±0.5),当比较结果在这个范围内时,就认为式(8)成立.不同的随机变量可能存在不同的父结点,并且这些父结点具有多种的取值组合形式.随着父结点数目的增多,组合方式就会迅速增加,出现差异的参数个数也就随之迅速增多.为了控制父结点数目的增多对度量结果的影响,实验中以各个父结点产生的差异率作为比较标准,即相对于具有相应父结点的不同参数的导数个数与随机变量Ai的父结点取值数目乘积的比值.结合前面的分析,当比值小于设定的阈值时,就认为式(8)成立,以此作为识别结构中是否有冗余边的依据.图5是我们的FAN算法和DFE算法所得到的贝叶斯网络分类器误分类率的曲线图和散点图.误分类率曲线图的坐标同上.DFE算法的误分类率多项式趋势线用实线表示,而FAN算法的误分类率多项式趋势线用虚线表示.从图5(a)可以看出,FAN算法误分类率多项式趋势线较为平稳,受数据集合的属性变量数目变化的影响较小;而除了具有中小规模属性变量数目的数据集之外,DFE算法性能均不如FAN算法.从图5(b)也可看出,FAN算法生成的分类器的误分类率整体上比DFE算法的低.在25个数据集上FAN算法比DFE算法得到的贝叶斯网络分类器的平均错误率都有所降低,占总数据集合数目的75.76%.在显著性水平0.05之下的双尾配对t检验结果表明,FAN算法在12个数据集合之上的误分类率都有显著降低,占总数据集合数目的36.36%.这就验证基于对数条件似然函数偏导数的变量间依赖关系度量方法,能够在贝叶斯网络结构中辨别通常的度量方法不能有效识别的冗余边,所设计的森林型贝叶斯网络分类器在多数实际应用中能取得较好的性能.图5FAN算法与DFE算法比较的误分类率曲线图和散点图7总结与展望本文研究了贝叶斯网络在分类问题中的应用.定量描述了网络结构与实际数据变量分布之间的关系,指出当网络结构比实际分布复杂时,即网络中存在冗余边时,将会限制鉴别式参数学习策略性能的发挥,进而提出一种不含有冗余边的森林型贝叶斯网络分类器和学习算法FAN.FAN算法首次应用了对数条件似然函数的偏导数来优化算法的设计.最后,用实验验证了已有的限制性贝叶斯网络分类器中普遍存在冗余边;在有冗余边的网络结构上,鉴别式学习策略的性能弱于生成式学习策略的,这就说明了本文所提FAN分类器的应用价值;同时,验证了FAN算法的有效性,说明了基于对数条件似然函数设计贝叶斯网络分类器是一种有效提高分类Page10器性能的途径.本文下一步将研究鉴别式参数学习策略不适合在有冗余边的网络结构上训练的原因,分别从两方面进行探索:第1个原因可能是因为,在贝叶斯网络分类器结构中,随着冗余边增多,进行参数估计时需要的训练数据数目也将增多,若训练数据固定,分类器参数估计的可靠性将下降;第2个原因可能是因为条件对数似然函数是不可分解的,在利用爬山算法逐步逼近最优解时,若网络结构中冗余边增多,有可能增加爬山算法陷入局部最优的可能性.致谢本文研究得到了田盛丰教授的热情指导.田教授在本文研究进程中,提出了很多宝贵的建议,开拓了我们的研究思路.在此表示衷心的感谢.同时也感谢匿名审稿人对本文提出的宝贵意见!
