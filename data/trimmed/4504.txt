Page1基于自适应归一化RBF网络的犙-犞值函数协同逼近模型1)(苏州大学计算机科学与技术学院江苏苏州215006)2)(吉林大学符号计算与知识工程教育部重点实验室长春130012)3)(江苏省软件新技术与产业化协同创新中心南京210046)摘要径向基函数网络逼近模型可以有效地解决连续状态空间强化学习问题.然而,强化学习的在线特性决定了RBF网络逼近模型会面临“灾难性扰动”,即新样本作用于学习模型后非常容易对先前学习到的输入输出映射关系产生破坏.针对RBF网络逼近模型的“灾难性扰动”问题,文中提出了一种基于自适应归一化RBF(ANRBF)网络的Q-V值函数协同逼近模型及对应的协同逼近算法———QV(λ).该算法对由RBFs提取得到的特征向量进行归一化处理,并在线自适应地调整ANRBF网络隐藏层节点的个数、中心及宽度,可以有效地提高逼近模型的抗干扰性和灵活性.协同逼近模型中利用Q和V值函数协同塑造TD误差,在一定程度上利用了环境模型的先验知识,因此可以有效地提高算法的收敛速度和初始性能.从理论上分析了QV(λ)算法的收敛性,并对比其他的函数逼近算法,通过实验验证了QV(λ)算法具有较优的性能.关键词强化学习;函数逼近;径向基函数;灾难性扰动;协同逼近1引言强化学习(ReinforcementLearning,RL)是一类由学习环境状态到动作的映射方法.在与环境交互中,Agent选择动作,环境作出反应,到达新的状态,并对每个状态或状态动作对通过值函数评价其好坏,最终通过值函数确定到达目标的最优策略.目前强化学习方法被广泛地应用于工业控制、仿真、博弈等领域[1-4].在离散状态空间强化学习系统中,值函数通常利用查询表(Lookup-Table)的方式进行存储,状态和动作是表的两个维度,其估计值与表格中的表项相对应.这种方法非常适合于离散的小状态空间任务,而对于大的或连续空间任务,会面临“维数灾”,从而导致收敛速度慢甚至无法收敛.目前解决“维数灾”问题主要采取3种方法:(1)采取聚类等编码方式将任务转化成查询表强化学习可以解决的问题[5];(2)对任务进行分解,采用分层、并行等技术来提高强化学习方法的执行效率[6-8];(3)采取函数逼近方法对强化学习中的值函数或策略进行建模,利用样本来不断调整参数,逐渐逼近真实的问题模型[9].函数逼近通常包括带参函数逼近和无参函数逼近两种[10-11].带参数函数逼近其函数形式和参数个数事先预定,初始模型限制了学习效果,这种逼近方法非常容易陷入局部极小值.相比而言,无参函数逼近方法其模型根据样本的个数和形式不断调整,因此灵活性和逼近精度都大大提高.由于带参的函数逼近模型方法简单,容易在理论上保证其收敛性,使得其应用广泛.Tadic[12]提出了一种与线性函数逼近器相结合的时间差分(TemporalDifference,TD)学习方法.Sherstov等人[13]提出了一种基于粗糙编码的在线自适应线性函数逼近方法.Sutton等人[14]将梯度下降线性函数逼近器结合于时间差分算法中,提出了一种梯度时间差分学习方法.而后,Sutton等人[15]对梯度时间差分算法做进一步改进,提出了GTD2及带有梯度校正的线性时间差分方法(linearTDwithgradientCorrection,TDC).Sutton等人提出的系列时间差分方法使得离策略时间差分学习算法的不稳定问题,在一定程度上得到解决.另外,Maei等人提出了GQ(λ)(GeneralQ(λ))算法[16]和Greedy-GQ算法[17],并于2009年将文献[14-15]中的线性函数逼近模型扩展到非线性函数逼近模型[18].Bonarini等人[19]提出了一种基于模糊逻辑的Q学习算法,并验证了算法的有效性.Heinen等人[20]提出了一种基于概率神经网络的强化学习方法,该方法增量式地逼近问题的值函数.目前无参函数逼近模型主要包括高斯过程函数逼近模型和核函数逼近模型两种.Engel等人[21]在无参函数逼近器的基础上,提出了高斯过程时间差分算法,利用高斯过程来逼近强化学习中的值函数.Ormoneit等人[22]提出了一种基于核的强化学习方法.Xu等人[23]提出了基于核的最小二乘时间差分方法(Kernel-basedLeastSquaresTD,KLSTD),将基于核的逼近器与最小二乘相结合,取得了一定的效果.在KLSTD基础上,Xu等人[24]提出了KLSPI及KLSTD-Q算法,并证明了算法的有效性.而后Taylor等人[25]证明了KLSPI及KLSTD-Q算法的等价性.对于无参函数逼近模型来说,由于强化学习的样本是在线获得的,因此难以保证其收敛.径向基函数RBF(RadialBasisFunction)网络逼近模型是一种局部逼近神经网络,可以用来解决连续空间强化学习问题.该模型既利用了核函数等机制来提高模型的表达能力,又具有线性逼近模型的简单性.针对RBF网络逼近模型应用于强化学习会出现“灾难性扰动”等问题,Barreto等人[26]提出了基于RBF网络的RGD-TD(0)算法,经扩展得到RGD-Sarsa(λ)等系列算法,并从理论和实验两方面验证了算法的有效性.这些算法一定程度上解决了RBF网络逼近模型中的“灾难性扰动”问题,但是算法的收敛速度却不理想.Page3针对RBF网络逼近模型的有效性及存在的问题,本文提出了一种基于自适应归一化径向基函数RBF(AdaptiveNormalizedRBF,ANRBF)网络的Q-V值函数协同逼近模型及对应的协同逼近算法———QV(λ).对于连续状态,利用ANRBF网络来进行编码和特征提取,得到特征向量.进一步经过归一化处理,能够得到平滑的逼近模型,在一定程度上可提高模型的抗干扰性.为了有效地提高逼近模型的灵活性,该网络逼近模型可以自适应地调整隐藏层节点的个数、宽度和中心.另外,Q-V值函数协同逼近模型利用状态值函数来塑造奖赏,将环境模型知识以奖赏的形式传递给学习器,从而有效地提高了算法的收敛速度和初始性能.将QV(λ)算法应用于连续状态空间强化学习标准验证仿真平台MountainCar,实验表明,QV(λ)算法在时间、空间及收敛性上都具有较优的性能.2相关理论2.1问题描述强化学习问题通常可以用马尔可夫决策过程来建模.定义1.在t时刻,系统的n个状态可以用实数向量表示为狓t=[狓1(t),狓2(t),…,狓n(t)]T∈不考虑时刻t,则可以表示为狓=[狓1,狓2,…,狓n]T.定义2.设P为强化学习任务,P的状态空间是由n维状态向量狓=[狓1,狓2,…,狓n]T各分量而构成的子状态空间X(P)={狓|狓=[狓1,…,狓n]T,狓i∈Di,i=1,…,n},其中Di是状态向量第i个分量的值域.定义3.一个四元组M={X,U,R,T}为马尔可夫决策过程,其中X={狓t|t∈(可以为连续的),U={狌i}k间,R:X×U×X是状态迁移函数.X2.2GD-Sarsa(λ)线性带参Q值函数逼近器是通过n维基函数向量(狓,狌)与n维参数向量狑建模得到,其计算公式为其中(狓,狌)为状态动作对〈狓,狌〉的特征向量,狑为参数向量.Sutton等人在式(1)的基础上,提出了GD-Sarsa(λ)算法[1],该方法利用梯度下降方法来迭代更新值函数的参数向量,其迭代公式如式(2):其中狑t和狑t+1为参数向量.α∈[0,1]为学习率.时间差分TD误差为δt=狉t+1+γQt(狓t+1,狌t+1)-Qt(狓t,狌t),狉t+1为立即奖赏,γ∈[0,1]为折扣率,Qt(狓t,狌t)和Qt(狓t+1,狌t+1)分别为在t时刻状态动作对〈狓t,狌t〉及〈狓t+1,狌t+1〉的估计值.向量犲t为t时刻的资格迹,具体更新如式(3):式(3)采用累加迹(accumulatingtrace)的更新方式.λ∈[0,1]为资格迹衰减因子.通过对函数f的参数向量狑t的每一个分量求偏导,得到梯度向量,如式(4):狑tf(狑t)=3犙-犞值函数协同逼近模型3.1犙-犞值函数协同机制在强化学习中,通常利用值函数来评估策略,具体可以分为动作值函数和状态值函数两种.根据两类值函数的不同特性,提出一种协同机制来解决连续状态空间的强化学习问题.针对连续状态空间的强化学习任务,其中X={狓|狓=[狓1,…,狓n]T,狓i∈Di,i=1,…,n}为连续状态空间,U={狌i}kQ-V值函数协同模型,如式(5):烄Qi(狓)=狑T烅V(狓)=狑T烆其中包含k个Q值函数和1个V值函数,(狓)=[1(狓),…,m(狓)]T∈征向量,狑i=[狑i1,…,狑im]T∈值函数的权值向量,狑l=[狑l1,…,狑lm]T∈函数的权值向量.在强化学习算法中,通常利用TD误差来修正值函数的参数向量,动态有效地降低函数逼近模型的“灾难性扰动”.为了使得TD误差相对稳定,Ng等人[27]利用塑造奖赏机制来重塑TD误差.通过将模型知识以奖赏的形式调整TD误差,使得Agent根据新TD误差更新值函数,最终能够减少次优动Page4作的选择次数,从而加快算法的收敛速度.另外,Randlv等人[28]同时也指出,如果塑造奖赏机制使用不当,将会减缓算法收敛速度.本文在文献[27-28]的基础上,提出了一种利用状态值函数对奖赏进行塑造的方法.基于状态值函数的塑造奖赏定义如下.定义4.基于状态值函数V:X有界实值映射F:X×U×X态空间,U为离散动作空间,状态值函数塑造奖赏(ShapingRewardbasedontheStateValueFunction,SR-SVF)定义如下:其中,γ∈[0,1]为折扣率,V(狓t)和V(狓t+1)分别为状态狓t和狓t+1的状态值函数.定义5.考虑t时刻的状态狓t,Agent根据当前行为策略选择一个动作狌t=狌i∈U(1ik)作用于环境,状态转移至狓t+1,立即奖赏值狉t+1,并根据行为策略选择状态狓t+1下的动作狌t+1=狌j∈U(1jk).在t时刻,利用塑造奖赏机制的协同TD误差(CollaborativeTemporalDifferenceError,C-TDE)计算方法如式(7):δt=狉t+1+γQj(狓t+1)-Qi(狓t)+F(狓t,狌t,狓t+1)=狉t+1+γQj(狓t+1)-Qi(狓t)+γV(狓t+1)-V(狓t)=狉t+1+γ(Qj(狓t+1)+V(狓t+1))-(Qi(狓t)+V(狓t))为了简化表示,式(7)中值函数Qi、Qj以及V都省略时间步下标t.由于协同逼近模型的特殊性,本文对资格迹重新定义,如式(8):犲j(t)=其中j=1,2,…,l,犲j(t)是在t时刻第j个子模型的权值向量的资格迹.3.2基于ANRBF网络的犙-犞值函数协同逼近模型在基于函数近似的强化学习算法中,状态特征的提取直接影响着所逼近的值函数模型的质量.本文利用高斯径向基函数构建RBF网络,用于对状态特征进行提取,并对基函数进行归一化处理,通过引入自适应机制,最终构建一个ANRBF网络.下面给出基于ANRBF网络的Q-V值函数协同逼近模型的结构图及描述,如图1所示.图1给出的Q-V值函数协同逼近模型是一个n×m×(k+1)的三层归一化带反馈机制的神经网络,该网络可以在线自适应地调整隐藏层各节点的图1基于ANRBF网络的Q-V值函数协同逼近模型结构图权值.描述如下:(1)左边层为输入层,输入一个n维状态向量狓=[狓1,狓2,…,狓n]T∈(2)中间层为隐藏层,包括激活函数、归一化、特征向量3个部分.激活函数共有m个隐节点,采用高斯径向基函数为激活函数.激活函数定义如式(9):ψi(狓)=exp-∑其中n维向量犮i=[犮i1,…,犮in]T、σi=[σi1,…,σin]T分别表示第i个高斯径向基函数的中心及宽度.通过归一化方法对激活函数的输出进行归一化处理得到状态的特征向量,其中特征向量的第i个分量如式(10):i(狓)=ψi(狓)∑(3)右边层为输出层,包含l=k+1个输出,分别是k个动作值函数和1个状态值函数,值函数模型构建如式(5)所示,其中式(5)中的狑ij(i=1,2,…,l,j=1,2,…,m)是输出层的权值.基于ANRBF网络的Q-V值函数协同逼近模型的描述如下:(1)将状态向量狓=[狓1,…,狓n]T∈入,经过式(9)所示的激活函数计算得到特征向量ψ(狓)∈(2)将ψ(狓)代入式(10)所示的归一化公式进行处理,输出特征向量(狓)∈(3)将(狓)代入式(5)所示的值函数模型,得到Q和V值函数的近似值.(4)根据Q值函数的估计值,结合动作选择策略选择一个动作,获得后续状态并立即奖赏.利用式(6)、(7)来计算协同TD误差δ.然后根据式(8)所定义的资格迹将当前得到的协同TD误差反向传播至整个状态及状态动作空间,最后根据式(2)自适应地调整输出层权值及隐藏层各激活函数的中心和宽度.Page5协同逼近模型的动态结构主要包括4个方面的调整:(1)调整输出层权值向量狑i=[狑i1,狑i2,…,狑im]T,i=1,2,…,l.方法为狑i=狑i+αδ犲i,其中犲i=[犲i1,犲i2,…,犲im]T,资格迹的更新策略为i=1,2,…,l,如果i=ID(狌t)或者i=l,那么犲i=max(γλ犲i,(狓)),否则犲i=γλ犲i.其中ID(狌t)表示当前动作狌t在动作集合中的编号.(2)如果δ>η1且j=1,2,…,m,max(j(狓))<η2,则增加一个新的隐节点,m=m+1.初始化该隐节点所含的RBF激活函数的中心向量、宽度向量以及对应的输出层权值向量:狌=1,…,n,犮mu=狓u,σmu=0.1,i=1,…,l,狑im=0,犲im=0.其中η1和η2为预先设定的阈值.(3)调整中心犮j=[犮j1,…,犮jn]T,j=1,2,…,m.假设j=1,2,…,m,犮z=argmax1,…,n,犮zu=犮zu-=犮zu+β1δ狑iz(1-z(狓))z(狓)其中β1为预先设定的参数.(4)调整宽度σj=[σj1,…,σjn]T,j=1,2,…,m.假设j=1,2,…,m,σz=argmaxu=1,…,n,σzu=σzu-=σzu+β2δ狑iz(1-z(狓))z(狓)其中β2为预先设定的参数.上述ANRBF协同逼近模型的动态结构调整中,第(1)、(3)和(4)步都是采用梯度下降的方法来进行调整的,第(1)步的调整策略如式(4)和(8)所示,第(3)和(4)步调整策略的详细推导如下:犮zu=犮zu-=犮zu+β1δ狑iz=犮zu+β1δ狑iz同理,σzu=σzu-=σzu+β2δ狑iz(1-z(狓))z(狓)i(狓)为当前选择的第i个动作的Q值函数,其中狑Tz(狓)形式如式(10)所示.4犙-犞值函数协同逼近算法4.1犙犞(λ)根据Q-V值函数协同逼近模型,提出了一种基于ANRBF网络的协同逼近算法QV(λ),假设A=δnew>η1∧j=1,2,…,m,max(j(狓))<η2,B=δ狑iz(1-z(狓))z(狓),算法的详细描述如下.算法1.Q-V值函数协同逼近算法QV(λ).输入:任务环境模型输出:各状态对应的策略1.初始化ANRBF网络隐藏层各节点激活函数;2.构建如式(5)所示的Q-V值函数协同逼近模型;3.i=1,2,…,l,狑i=0∈4.repeat(对每一个情节)5.当前状态狓及动作狌;6.repeat(对该情节中的每一步)7.执行动作狌,观察狉,狓;8.将数据〈狓,U(狓)〉输入当前值函数模型;9.狌←通过ε-greedy等策略选择狓下的动作;10.收集数据〈狓,狌,狉,狓,狌〉;11.计算TDE,δold=狉+γQID(狌)(狓)-QID(狌)(狓);12.计算SR-SVF,F(狓,狌,狓)=γV(狓)-V(狓);13.计算C-TDE,δnew=δold+F(狓,狌,狓);14.更新资格迹,i=1,2,…,l,15.16.17.将δnew反馈给Q-V值函数协同逼近模型;18.i=1,2,…,l,狑i=狑i+αδnew犲i;19.调整协同逼近模型的隐藏层结构,20.21.22.23.24.25.26.27.狓=狓,狌=狌;28.直到狓是终止状态;29.直到运行完设定情节数或满足其他终止条件.算法对Q-V值函数协同逼近模型的调整主要Page6包含两个方面:(1)对输出层权重的调整;(2)对隐藏层结构的调整.其中,调整策略是文献[26]中所给出的RGD策略.假设1.C-TDE满足δnewη1或者j=1,2,…,m,max(j(狓))η2,即C-TDE不大于阈值η1,或者逼近模型隐藏层输出的最大分量不小于阈值η2,其中η1和η2为事先设定的阈值.假设2.令犮z=argmax(j(狓)),即犮z和σz分别是隐藏层各激活函数中离狓最近的一个基函数的中心向量和宽度向量.定理1.在假设1和假设2成立的前提下,RGD调整策略有效,即逼近模型关于状态向量狓收缩.证明.下面从中心向量和宽度向量的更新过程来分别证明定理1的正确性:①由于(狓)经过归一化处理,z=1,2,…,n,z(狓)∈[0,1],有(1-z(狓))z(狓)σ-2②根据RGD调整策略,在状态狓下选择的动作为狌(在动作集合中的编号记为i=ID(狌)),只有当δ狑iz>0时,才调整犮zu的大小.又因为Δ犮zu=β1δ狑iz(1-z(狓))z(狓)(狓u-犮zu)σ-2负取决于(狓u-犮zu)的正负;③根据①和②可知:犮z的狌分量犮zu向状态向量狓的狌分量狓u方向调整;④在③的基础上,考虑中心向量的每一维,可得中心向量犮z始终向状态向量狓的方向调整;⑤因为z(狓)∈[0,1]且σzu>0,则(1-z(狓))z(狓)(狓u-犮zu)2σ-3RBF的宽度向量的狌分量σzu将被缩小.⑥由③可知,激活函数的中心向量调整的方向与状态向量狓保持一致;由⑤可知激活函数的宽度向量调整是收缩的.综上所述,RGD调整策略有效,且逼近模型关于状态向量狓收缩.4.2算法收敛性分析下面从Q-V值函数逼近模型的结构和权值调整两个方面来分析Q-V逼近模型的收敛性.参考文献[23,29]给出了引理1,该引理主要用来辅助选择Q-V逼近模型隐藏层激活函数的中心向量.引理1[23,29].假设Q-V逼近模型隐藏层激活函数的候选中心向量集为犡={狓1,狓2,…,狓n},基于ALD方法得到t-1时刻的中心向量集为犡t-1={狓1,狓2,…,狓m},1<mn.考虑一个新样本狓t,采用文献[23]中所给出的ALD特征选择方法,当min犮∑j则犡t=犡t-1∪{狓t},其中犮=[犮1,…,犮j],μ为预先设定的阈值,则Q-V逼近模型隐藏层的中心向量能够稳定.文献[23,29]给出了引理1的证明.根据引理1所给出的策略,Q-V逼近模型选择隐藏层激活函数的中心向量,则可以保证模型的隐藏层结点数及中心向量能够稳定.基于引理1及定理1,本文给出了定理2,其证明在Q-V逼近模型的隐藏层结构趋于稳定的情况下,QV(λ)算法能够收敛.定理2.假设Q-V逼近模型的隐藏层趋于稳定,在确定性MDP中,若算法具有相同的样本序列,则根据C-TDE更新值函数的QV(λ)算法与利用TDE结合V值函数进行更新的GD-Sarsa(λ)算法具有相同的收敛性.证明.考虑具有连续状态空间X和离散动作空间U的强化学习问题,分别用L和L表示GD-Sarsa(λ)算法和QV(λ)算法的学习器.狓∈X,狌∈U,Q(狓,狌)=狑TQ0(狓,狌)+V(狓)=ωTQ(狓,狌)=ωTQ(狓,狌)=Q0(狓,狌)=ωTQ(狓,狌)表示QID(狌)(狓).假设在状态狓下采取动作狌转移到状态狓,立即奖赏为狉,在狓下根据行为策略选择动作狌,则该样本数据可以用五元组〈狓,狌,狓,狉,狌〉表示.基于此五元组样本数据,L和L分别更新权值,更新公式如式(11)和式(12):其中δQ(狓,狌)=狉+γQ(狓,狌)-Q(狓,狌)为TDE,δQ(狓,狌)=狉+γQ(狓,狌)-Q(狓,狌)+F(狓,狌,狓)为C-TDE.犲ID(狌)和犲ID(狌)分别是值函数模型中的第ID(狌)个子模型的资格迹.状态动作对〈狓,狌〉先后两次值函数的差分别记为ΔQ(狓,狌),ΔQ(狓,狌).根据式(11)和式(12)的权值更新调整Q值函数的更新,Q值函数的更新量计算如式(13)和式(14):给定相同的学习样本序列,接下来利用归纳法证明L和L的等价性,即证明狓∈X,狌∈U,ΔQ(狓,狌)=ΔQ(狓,狌).证明过程如下:①当Q(狓,狌)和Q(狓,狌)都是初始值时,则Page7ΔQ(狓,狌)=ΔQ(狓,狌)=0;②假设当前存在狓∈X,狌∈U,ΔQ(狓,狌)=ΔQ(狓,狌),则在此基础上,根据新的样本数据〈狓,狌,狓,狉,狌〉,利用TDE及C-TDE可得δQ(狓,狌)=狉+γQ(狓,狌)-Q(狓,狌)δQ(狓,狌)=狉+γQ(狓,狌)-Q(狓,狌)+F(狓,狌,狓)根据上述计算,可得δQ(狓,狌)=δQ(狓,狌),即在新样本数据下,L和L所计算的TDE和C-TDE相同.此外,由于具有相同的样本序列,L和L中资格迹更新也是相同的,又根据每个时间步L和L获得相同的TDE和C-TDE,因此,L和L资格迹在每个时间步具有相同的更新,即对于任一时间步,犲ID(狌)=犲ID(狌).再根据式(13)和式(14)可得,狓∈X,狌∈U,ΔQ(狓,狌)=ΔQ(狓,狌).综上所述,在相同的样本序列下,L和L等价,即QV(λ)算法与GD-Sarsa(λ)算法具有一致的收敛性.定理3.在式(15)所给的条件下,如果学习率α随时间衰减,则利用梯度下降方法更新参数的QV(λ)算法能够收敛至一个局部最优解.证明.根据文献[10]得知,采用梯度下降方法的线性学习算法,例如Sutton等人提出的GD-Sarsa(λ)算法在满足式(15)的条件下,如果学习率α随时间衰减,则能够保证该算法能收敛至一个局部最优解.又根据定理2,在具有相同样本序列的情况下,QV(λ)算法与GD-Sarsa(λ)算法具有一致的收敛性.而在强化学习算法中,任意初始化值函数都不会影响算法的收敛结果,因此,在学习率α满足式(15)且随时间衰减的情况下,QV(λ)算法一定能够收敛至一个局部最优解.5实验及结果分析5.1实验描述及设置MountainCar问题是一个经典的具有连续状态空间、离散动作空间的强化学习问题,如图2所示.图2描述的是一个动力不足的小车,在山谷中的任何一点通过前后摆动的方式最快到达山顶的G点的问题.其中状态定义如式(16):其中y为小车的水平位移(-1.2y0.5),v为小车的水平速度(-0.07v0.07).系统的动力学特性如式(17):其中g=0.0025为与重力有关的常数,狌为控制动作,取3个离散值,+1,0和-1,分别代表全油门向前、零油门和全油门向后.另外,小车运动过程中的奖赏函数如式(18):其中下标t表示时间步.实验采用ε-greedy作为行为策略[1],每次实验的情节数为1000,每个情节的最大时间步数为1000.每次实验随机生成小车的初始状态,当小车到达G点或者时间步数超过1000时,情节结束,并开始下一个情节的学习.算法的性能指标包括3个方面:(1)收敛速度,即算法能够在多少个情节内收敛;(2)收敛结果,即算法收敛后,小车从初始点到达目标点G所用的平均时间步;(3)初始性能,即每次实验的前20个情节中,小车从初始点到达目标点G所用的平均时间步.将本文所提的QV(λ)算法与GD-Sarsa(λ)算法[1],Greedy-GQ算法[17]以及RGD-Sarsa(λ)算法[26]在MountainCar问题中进行对比分析.并着重分析资格迹、学习率、SR-SVF及自适应机制对算法性能的影响.Page85.2实验分析在QV(λ)算法及RGD-Sarsa(λ)算法中,分别采用8个等距的高斯RBFs来对二维连续状态空间的每一维进行划分,RBFs的数量为8×8=64,宽度向量为σ=[0.1,0.1]T.另外ε=0.0,α=0.9,λ=0.9,γ=1.0.在GD-Sarsa(λ)中,采用10个9×9的Tilings来划分状态空间,并利用文献[1]中给出的最优参数:另外,ε=0.0,α=0.14,γ=1.0,在采用替代迹的情况下,λ=0.9,在采用累加迹的情况下,λ=0.3.在Greedy-GQ算法中,ε=0.0,α=0.1,γ=1.0.从图3和图4中可以看出,QV(λ)算法在收敛速度和初始性能上明显优于其他3个算法.图4算法性能比较(续)采用上述给出的RBFs配置,ε=0.0,α=0.9,λ=0.9,γ=0.99.分别考虑两个不同的立即奖赏模型:(1)根据式(18)给出的奖赏模型;(2)奖赏模型为:当y<0.5时,狉t=0;当y0.5时,狉t=1.实验结果如图5所示,左图为第1种奖赏设置下的算法执行结果,右图为第2种奖赏设置下的算法执行结果.从图5可以看出,带有SR-SVF机制的QV(λ)算法在收敛速度和初始性能方面明显优于不带SR-SVF图5SR-SVF机制对QV(λ)算法的性能影响分析机制的QV(λ)算法.在采用第1种奖赏模型的情况下,由于奖赏是-1和0,是一种惩罚型奖赏,SR-SVF机制对算法收敛速度和初始性能影响较小;当采用第2种奖赏模型时,由于奖赏是0和1,是一种鼓励型奖赏,只有到达目标状态时,才能有益于算法的学习,因此SR-SVF机制可以在学习初期通过环境模型知识构造塑造奖赏,并传递给学习器,从而能够有效地提高算法的收敛速度和初始性能.Page9下面重点分析学习率α对算法性能的影响.在QV(λ)算法中,本文定义了新的资格迹,λ=0.9,α分别取值0.1,0.5,0.9,1.0;在GD-Sarsa(λ)算法中,采用替代迹,λ=0.9,α分别取值0.01,0.05,0.14,0.18,其余参数设置不变;在Greedy-GQ算法中,α的取值分别为0.005,0.015,0.05,0.1,其余参数不图6算法关于学习率的敏感度分析表1给出了QV(λ)算法的平均性能.当RBFs设置为5×5=25时,带有自适应机制的QV(λ)算法大约需要229个情节收敛,且收敛后大约需要73个时间步到达目标点,而不带自适应机制的QV(λ)算法收敛后大约需要275个情节收敛,且收敛后大约需要142个时间步到达目标点;当RBFs设置为表1自适应特性对犙犞(λ)算法的性能影响分析275±15142±15收敛情节数平均时间步6结束语本文利用ANRBF网络将状态空间映射到高维特征空间,通过一组高斯基函数的线性组合构建Q-V值函数协同逼近模型.该模型既具有非线性逼近模型的强表达能力,又具有线性逼近模型的简单性及良好的收敛性.ANRBF网络可以有效地提高算法的鲁棒性和灵活性,从而在一定程度上解决了强化学习逼近模型所面临的“灾难性扰动”问题.通过Q值函数和V值函数构造协同TD误差,从而有效地提高算法的收敛速度和初始性能.此外,本文定变;在RGD-Sarsa(λ)算法中,采用替代迹,λ=0.9,α的取值分别为0.1,0.5,0.9,1.0,其余参数不变.实验结果如图6所示,与基于线性函数近似的GD-Sarsa(λ)、Greedy-GQ算法以及基于非参函数近似的RGD-Sarsa(λ)算法相比,本文所给出的QV(λ)算法对于不同学习率具有较强的鲁棒性.3×3=9时,带有自适应机制的QV(λ)算法大约需要252个情节收敛,且收敛后大约需要75个时间步到达目标点,而不带自适应机制的QV(λ)算法大约需要269个情节收敛,且收敛后大约需要276个时间步到达目标点.义了一种新的资格迹更新方法,并用来处理基于连续编码的Q-V值函数协同逼近模型的时间信度分配问题,实验结果表明具有较优的效果.考虑到强化学习的自学习和在线学习特性,如何设计有效的逼近模型来更好的适应和处理在线强化学习问题,有待进一步研究.
