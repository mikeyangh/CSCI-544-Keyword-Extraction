Page1基于SSVM的递归统计不相关特征抽取算法任世锦1),2)王小林1)吕俊怀1)张晓光2)1)(徐州师范大学计算机学院江苏徐州221116)2)(中国矿业大学机电工程学院江苏徐州221008)摘要文章旨在研究数据分布未知的高维、小样本问题的特征抽取算法.基于支持向量机原理和特征统计不相关思想,提出基于散度支持向量机(SSVM)的递归统计不相关特征抽取算法,解决现有算法抽取特征之间存在相关性、算法受到样本分布影响等问题.针对高维小样本问题,使用PCA把SSVM优化问题变换到同构低维空间;给出边界鉴别向量集的递归求取方法,把模式高维特征投影到边界鉴别向量集,实现了统计不相关特征的抽取;分析了算法的收敛性和终止条件.文中使用核方法把线性SSVM推广到非线性SSVM,通过KPCA方法把非线性SSVM优化问题转换到低维空间中的等价优化问题,在低维空间抽取不相关非线性特征.仿真结果证明了文中算法的有效性.关键词散度支持向量机(SSVM);分类;特征抽取;统计不相关边界鉴别向量;主元分析(PCA)1引言特征抽取方法能够降低数据存储空间以及消除样本中冗余信息和噪声,便于人们对数据进行分析和理解,解决了因样本维数过高而导致分类器训练代价过大、泛化性能低等问题[1-3].常见的特征抽取算法有主元分析(PCA)、线性鉴别分析(LDA)、独立主元分析(ICA)和基于支持向量机(SVM)方法等[2-6],其中,LDA和SVM在机器学习领域中引起了人们极大关注.LDA能够求取反映样本集全局信息的鉴别向量,具有较高的分类性能,被认为是目前最为有效的特征抽取算法,LDA及其改进算法的基本思想就是根据Fisher鉴别准则求取多个最佳鉴别向量,然后再将模式高维特征向量投影到该最佳鉴别向量集上,构成低维鉴别特征空间[6].鉴于LDA算法不支持高维小样本数据以及F-SLDA算法抽取的特征分量之间存在统计相关等问题,人们提出了很多改进算法,如广义LDA、零子空间(Null-subspace)LDA、基于广义SVD的LDA、统计不相关LDA等[6-9].然而LDA存在单类样本为高斯分布时才能取得最优解、不能准确区分线性可分样本等问题[10-11].SVM是基于结构风险最小化原理的有监督分类方法,具有支持小样本数据、无需假定样本分布、计算量与维数无关等优点[5,12].然而SVM只能抽取样本集局部信息,存在超平面法线方向与样本分布不一致的问题,影响了泛化性能的提高[11].对二分类问题,SVM超平面法向量与LDA最佳鉴别向量具有相同的物理意义.基于上述考虑,文献[4]通过把样本高维特征投影到SVM分类超平面法线方向上,构成一维特征空间,取得了较好的分类性能;类似F-SLDA算法[6],文献[5]提出基于SVM的递归提取正交边界鉴别向量方法,其性能优于SVM算法.然而上述算法性能受到SVM性能和抽取特征之间存在相关性的影响.其原因是:(1)SVM只利用了样本集的局部信息,影响了其泛化性能,因此与常见的特征抽取算法相比,文献[4]算法性能并没有显著的提高.理论分析和实验证明,利用样本集全局信息能够有效提高SVM的性能[11,13];(2)基于统计不相关的鉴别向量集优于正交鉴别向量集的模式识别理论[6-7,14],文献[5]抽取的特征之间存在相关性,影响了算法性能的提高.基于上述分析,本文提出基于散度支持向量机(ScatterSupportVectorMachine,SSVM)的统计不相关特征抽取算法.首先提出了SSVM,该方法实现了SVM与LDA之间的折中,保证了SSVM性能的优越性;给出基于SSVM的递归抽取统计不相关边界鉴别向量集算法,证明了算法的收敛性结论以及算法终止条件;并通过核方法把线性抽取算法推广到非线性抽取算法.本文算法继承了SVM和LDA的优点,通过使用PCA/KPCA把高维空间的SSVM优化问题变换到同构低维空间中等价优化问题,解决了小样本情况下SSVM优化问题求解的问题,有效地克服现有算法的缺点.使用UCI数据集和XM2VTS人脸图像进行了仿真实验,结果说明本文算法是可行有效的.下面首先介绍SSVM原理.2散度支持向量机(SSVM)LDA通过如下形式的Fisher准则求取鉴别向量,即其中,犛b和犛w分别为类间、类内散度矩阵.设样本总体散度矩阵为犛t,满足关系:犛t=犛b+犛w;狑为鉴别向量.上面优化问题具有如下等价形式[15]:由于LDA充分利用样本集全局信息以及类别信息,鉴别向量方向与样本分布一致.受LDA和文献[16]启发,文献[11]提出MCSVM(MinimumClassvariance,SVM),其优化问题通过狑T犛w狑代替SVM优化问题狑T狑项得到.MCSVM考虑了样本集局部和全局信息,实现FLDA与SVM之间的折衷,具有更优的分类性能[11].对两类样本集{(狓j,yj)}N+1},假设分类超平面模型具有如下形式:受文献[11,13]的启发,本文通过求解如下改进SVM优化问题P1得到狑和b的最优解,即这里,ξ1,ξ2,…,ξN是非负松弛变量;C为误差惩罚因子;犛t=1阵,需要满足狑T犛t狑>0.从上式可以看出,优化问题Page3P1就是使用狑T犛t狑代替传统SVM优化问题的狑T狑,且满足约束条件狑T犛t狑>0.由于式(1)和(2)所示优化问题的等价关系,优化问题P1表示的改进SVM具有与MCSVM类似特性,因而在理论上保证SSVM具有更优的分类性能.由于犛t为样本集的总体散度矩阵,代表样本的全局信息,因此,本文把这种改进的SVM称作散度支持向量机(SSVM).条件构造Lagrange方程,即根据式(3)所示优化问题P1的目标函数和约束L=min狑,b,αj,βj其中,αj和βj(j=1,2,…,N)是Lagrange乘子.式(4)对狑,b,αj,βj求导并等于0得把式(5)~(7)代入优化问题P1,根据KKT条件,优化问题P1转换为如下形式,即P2:maxαj0αjC,j=1,2,…,N烄s.t.烅∑N烆=0j=1对于SSVM,其决策函数为g(狓)=sign(狑T狓+b)=sign1根据KKT条件,支持向量狓k其对应的ak≠0,ξk=0.设支持向量集为犛犞,对支持向量狓k∈犛犞有下式成立:那么b的最优值可由下式求取,即b=1需要注意的是,对于高维小样本数据集,即样本数据维数dN,犛t是往往是奇异的,这样就不能直接通过优化问题P1求取向量狑i的最优解.然而,对于高维小样本数据集,在第3节将证明,通过PCA对数据集进行维数约简,把优化问题P1转换为低维空间上等价的优化问题,从而求取SSVM的最优解.下面讨论小样本情况下的SSVM优化问题求解问题.3小样本情况下SSVM的求解由函数分析理论可知,犛t在空间Rd是有界、紧的、正定的算子,根据Hilbert-Schmidt定理,矩阵犛t的特征向量是Rd上的正交基[17].令犇表示犛t非零特征值对应特征向量张成的空间,犇⊥表示犛t的零特征值对应特征向量张成的空间,则空间犇⊥表示空间犇的补空间.因此,对任意向量狑∈Rd可以表示为[16]定义映射L:Rd→犇,对任意的狑∈Rd,均存在唯一的映射狏使得下式成立,即这样,映射L把优化问题P1从空间Rd转换为低维空间犇的优化问题P3.很显然,如果优化问题P1和P3等价,那么对高维空间P1的求解就转化为对低维空间犇上P3进行求解,解决了小样本情况下因犛t奇异而导致的SSVM难以求解的问题.定理1给出优化问题P1等价于P3的结论.定理1.设犛t为奇异散度矩阵,设犇和犇⊥分别表示犛t非零、零特征值对应特征向量张成的空间,那么空间Rd上的优化问题P1等价于如下所示空间犇上的优化问题P3,即其中,狏T犛t狏>0.证明.根据式(9),对任意狑∈Rd可以表示为根据函数理论[17],显然有下式成立,即因此有则式(4)所示的Lagrange方程转化为L=min狑i,b,αj,βj=狏T犛t狏+C∑NPage4如果对∈Rd有同样本狓j,狓k均有上的投影均为常数.令c=根据上式,式(13)可以写为L=min狑,b,αj,βj从式(14)可以看出,上式与优化问题P3对应的Lagrange方程完全一致,而式(14)表示的Lagrange方程又与优化问题P1等价.因此,优化问题P3与优化问题P1是等价的.令犛t非零特征值对应的特征向量作为变换矩阵犘,且特征向量的数目Kd-1.在很多情况下,可以假定训练样本是独立、同分布的,此时K=d-1.由于犘的列向量是空间Rd的正交基,变换矩阵犘就是空间犇到RK的一对一映射,根据函数分析理论,在PCA变换矩阵犘下空间犇与RK同构[17],即根据式(16)、式(11)所示的优化问题P3可以转换为如下所示的等价优化问题P4,即其中,珘犛t=犘T犛t犘;狓~矩阵上的投影,且狓~类似第3节,优化问题P4的对偶问题为P5:maxajs.t.∑N根据上面的讨论,对小样本问题,为求出优化问题P1的最优解,首先把样本通过PCA变换矩阵犘映射到空间RK上,然后在低维特征空间RK中求取优化问题的最优解.然而,如何在空间RK求取统计不相关的边界鉴别向量集以及算法是否收敛,是本文需要研究的关键问题.下节给出算法实现以及收敛性分析.4递归统计不相关特征抽取算法分析4.1算法的实现1},SSVM的分类超平面为y=狑T1狓+b1.分类超平面法向量狑1包含了样本类别边界信息,具有区分不同类别数据的能力.对二分类样本,狑1与LDA鉴别向量具有相同的物理意义,因此,本文把狑1称作边界鉴别向量.设狑1为规格化向量,即狑1=1,样本(狓i,yi)在向量狑1方向上的投影为设训练样本集{(狓i,yi)}N对上式左边同乘狑1可得很显然,狑1狑T1狓i为在向量狑1方向上对狓i的逼近.根据PCA理论,假设在向量狑1方向上样本集犡的逼近^犡构成空间犛犠1,样本集犡与逼近^犡之间的误差珟犡构成残差空间犛r,满足下面等式:它们之间的几何关系如图1所示.令狑1狑T1=犐+犐狑1,犐是d×d单位矩阵,犐狑1是非零矩阵,那么式(20)可写为根据图1所示,样本狓i与逼近狓^i及其误差狓~间满足如下关系,即根据式(21)、(22),狓~根据训练样本{(狓~可求取边界鉴别向量狑2;然后根据式(23)递归求取边界鉴别向量狑3,狑4,…,狑l;最后把空间Rd模式高维特征投影到边界鉴别向量集{狑1,狑2,…,狑l}上,构成低维特征空间Rl.在定理2给出低维空间中的特Page5定理2.对训练样本集{(狓k,yk)}N征之间是否存在相关性的结论.yk∈{-1,1},基于上述方法使用SSVM递归求取边界鉴别向量狑1,狑2,…,狑l,且狑i=1,i=1,2,…,l.对任意两个边界鉴别向量狑i,狑j,样本犡=[狓1,狓2,…,狓N]在狑i,狑j方向上投影得到的特征是不相关的,即i犡,狑Tj()犡=狑Tcov狑T证明.根据训练样本集,由SSVM算法求取的狑1作为第1个边界鉴别向量.根据式(23),狓k在狑1方向上逼近残差为对残差样本集{(狓~1求取狑2作为第2个边界鉴别向量.然后基于式(25)递归求取边界鉴别向量狑1,狑2,…,狑l,狑i=1,i=1,2,…,l.对式(25)左边乘以狑T1,有下面等式成立:即狑1正交于样本残差空间.根据式(5)可知,狑2=t∑N犛-1k=1可得αkyk狓~1对任意的狑i,狑j,i≠j,i,j=1,2,…,l,同理可证明有下式成立:不相关的特征,定理2证明了本文算法能够从样本集抽取统计综合以上所述,本文给出如下递归统计不相关特征抽取算法实现步骤:假设有N个训练样本{(狓j,yj)}N其中d是输入数据的维数,根据SSVM原理,本文提出的算法实现步骤如下:1.初始化.给出算法终止条件,迭代次数r=1,令狑r表示迭代计算得到的边界鉴别向量.2.确定本文SSVM模型参数.基于第2节SSVM算法,使用交叉校验方法确定SSVM的模型参数,根据上述样本训练SSVM,并把求取得到的超平面法向量狑作为第1个边界鉴别向量狑r;r←r+1.3.求取边界鉴别向量狑r.使用式狓~r计算逼近残差样本集{(狓~rSSVM优化问题求解,把求取的超平面法向量作为第r个边界鉴别向量狑r.4.结束判断.判断是否满足算法结束条件,如果不满足,则r←r+1,转到步3继续递归求取统计不相关边界鉴别向量;否则,r←r-1,转入步5.5.抽取特征.把样本投影到边界鉴别向量变量集{狑1,狑2,…,狑r},构成r维特征空间,即抽取r个统计不相关特征.6.分类器训练:在低维特征空间建立分类器模型.7.算法结束.本文递归统计不相关特征抽取算法比较简单,易于实现.算法实现的关键是确定合适的终止条件,而算法的收敛性决定算法终止条件的存在,下面分析算法的收敛性.4.2算法的收敛性分析SSVM基于结构风险最小化原理,其结构风险越小表示边界向量狑l包含的分类信息越多.如果SSVM的结构风险随着l的递增而增大,则边界鉴别向量狑l包含的分类信息越少,即算法收敛.基于以上分析,定理3给出算法收敛的结论.{狑l,bl,ξl如果l-1犛t狑l-1+C∑N12狑T成立,那么算法是收敛的.定理3.令{狑l-1,bl-1,ξl-1证明.由式(24)可知,狓l对上式左边乘以狑T对式(28)进行迭代计算得到如下等式,即由于犛t是已知正定矩阵,显然下式成立:满足以下约束条件:根据式(29),有下面等式成立:yi(狑Tl狓lyi(狑T由SSVM算法可知,{狑l,bl,ξlj0,yi(狑Tξl根据式(30),SSVM的最优解{狑l,bl,ξlN}对样本集{(狓tξl示的约束条件,t=1,2,…,l.同理,{狑l-1,bl-1,ξl-12,…,ξl-1ξl-1而对样本集{(狓lPage6成立:l-1犛t狑l-1+C∑N12狑T上式表明,随着抽取特征数量l的递增,结构风险也随之增加,边界鉴别向量狑l包含的分类信息也减少,算法逐渐收敛.定理3虽然给出了算法的收敛性结论,但不能直接作为算法终止条件.定理4给出算法终止的条件.定理4.对本文递归不相关特征抽取算法,设每次使用的训练样本集为{(狓l2,…,m为抽取特征的数量,有如下关系成立:对ε>0,必存在一个正整数m使得下式成立:证明.根据式(28)有下式成立:即狓li单调递减.注意到狓i=狓1i,根据SSVM算法可知由于犛-12,…,狓1狓1很显然狓2i∈犛,i=1,2,…,N.同理可以证明狑2∈犛.由于犛是有限维空间,一定存在一个正整数m使得下式成立:显然,上式等价于式(32).很明显,式(32)成立时,根据本节的分析可知,算法是收敛的.证毕.本文提出3种算法终止条件:(1)确定抽取的特征数量.该方法难以确定合适的特征数量;(2)根据定理4,设定阈值ε>0,当maxi=1,2,…,N狓~l终止,其中,狓~l残差;(3)基于分类精度不会随着统计不相关特征增加而下降的原理[5],首先设定分类精度变化阈值θspe,分类精度变换小于阈值θspe时算法终止,避免了抽取太多特征付出计算代价太大的问题.5非线性特征抽取算法对大量的非线性模式数据,需要把上述线性特征抽取算法推广到非线性特征抽取算法.本文首先通过核方法把上面线性SSVM推广到非线性SSVM,然后基于非线性SSVM递归抽取特征.首先把上面训练样本{(狓i,yi)}Ni=1,其中(·):Rd→犎为高维非线性映射,犎yi)}N空间表示Hilbert空间,满足关系(狓i)T(狓j)=KH(狓i,狓j),KH(狓i,狓j)为满足Mercer条件的核函数.在Η空间内散度函数定义为其中,犿=1在犎空间内,SSVM的优化问题具有如下形式,即P6:min狑i,bi,ξjs.t.yj(狑T(狓j)+b)1-ξj众所周知,Η空间的维数很高,犛t几乎总是奇异的,不能满足条件狑TP6直接求取狑的最优解.由于犛t是犎空间中的有界、正定紧的自伴算子,根据Hilbert-Schmidt定理[17],犛t的特征向量组成犎空间的正交基.令犇、⊥分别为犛t非零、零特征值对应特征向量张成的犇互补空间,那么Η空间的向量狑可以表示为类似定理2,可以证明,线性SSVM优化问题P6可以转化为空间犇的等价优化问题.设犛t的非零特征向量的数目为K,且KN-1,那么犇的维数也为K.根据函数分析理论,犇与欧氏空间RK同构,存在同构映射其中,变换矩阵犘的列向量由犛t的非零特征向量组成.优化问题P6转化到RK等价优化问题P7,即P7:min狑,b,ξjs.t.yj狇T狕j+()b1-ξj烅烄烆这里,犛)t=犘T犛t犘;狕j=犘T狓()j是通过KPCA变换矩阵把狓()j投影到RK,有关KPCA的详细介绍可参考文献[18].这样对非线性SSVM优化问题P6的求解问题可转化为如式(36)所示RK中优化问题P7进行求解.由于犛)t非奇异,最优分类超平面法向量为Page7狇=∑Nαjyj犛)-1j=1非线性特征抽取方法为,首先把样本{(狓j,yj)}Nj=1映射到高维特征空间{((狓j),yj)}N通过KPCA非零特征向量组成的变换矩阵犘把Η空间样本投影到K维特征空间{(狕j=犘T(狓j),yj)}Nj=1.类似4.1节,在K维特征空间中通过优化问题P7递归求取统计不相关边界鉴别向量狇i,i=1,2,…,L.算法终止条件可与4.2节提出的方法类似.根据以上分析,可以通过核方法把线性特征抽取算法推广到非线性特征抽取问题.6仿真实验为说明本文算法的有效性,首先以UCI中Heart-disease和Breastcancer数据集为例,研究算法抽取特征数量与分类精度之间关系.Heart-disease数据集包含270个样本,每个样本有13个属性;Breastcancer数据集包括277个样本,每个样本有9个属性.由于SVM与递归支持向量集维数约简算法(RSVM)在文献[5]做了详细的比较,因此,本文算法只与RSVM进行比较,在低维特征空间中使用最近邻分类器对测试样本分类.本文SSVM算法使用高斯核函数,使用5-fold交叉校验方法确定模型参数σ和C.在仿真实验中,从所有的数据集随机选取50%作为训练样本,其余作为测试样本,此过程重复10次,计算分类精度的平均值作为最终分类精度.仿真程序在matlab7.0环境下运行,并使用其自带的优化工具包,仿真结果如图2、图3所示.从图中可以看出,(1)RSVM分类精度随着抽取特征数量的增加会出现下降的现象,给确定最佳特征数量带来了困难.本文算法分类精度在开始时会随着抽取特征数量递增快速增加,当达到或接近最佳特征数时分类精度增加得非常缓慢,甚至是不变,从而说明本文提出的判断算法终止条件的合理性;(2)本文算法优于RSSVM算法.为进一步说明算法的有效性,本文算法与KFDA[2]、KRDA[19]、SVM和RSSVM算法对比.实验选用UCI数据集Breast-cancer、Heart-disease、Sonar、Cancer、Ionosphere、Wasteplant和Wave-form以及XM2VTS人脸数据库进行镜片检测[19].Wasteplant数据集有13个类别.为简便起见,将其合并成正常情况和非正常情况两类,正常情况有图2Heart-disease抽取特征数量与分类精度图3Breast-cancer抽取特征数量与分类精度332个样本,非正常情况包括性能超过平均值的正常情况、低输入的正常情况、二沉池故、暴雨以及固体溶度过负荷,分别对应的样本数量为116、65、7、3、4.UCI数据集统计特性如表1所示.XM2VTS数据库共有2360幅图像,其中1518幅人脸图像有镜片,842幅无镜片.人脸图像的分辨率720×576,每个图像被旋转和缩放,使得所有眼睛中心放置在特定像素上,然后修剪为85×156,通过直方图均衡化技术把图像强度规格化到均值为0、方差为1,部分图像如图4所示.实验中,如果某个人出现在训练样本集中,则从测试数据中剔除此人图片.所有算法采用高斯核函数,使用交叉验证法确定SVM、RSSVM以及SSVM正则化参数σ和C,KRDA与KFDA参数确定方法参考文献[20].实验中,UCI数据集所有的数据属性被规格化为[-1,1],在低维特征空间中使用最近邻分类器估计精度.样本选取方法同上,测试样本的平均精度如表2所示.数据集IonosphereWasteplantCancerWaveformPage8图4XM2VTS数据库部分人脸图像表2分类器测试精度率比较精度/%KFDABreastcancer73.3±4.7Heart-disease82.8±3.3Wasteplant74.8±3.5CancerWaveformIonosphereXM2VTS从表2可以看出,本文算法明显优于SVM、RSVM和KFDA算法,对绝大部分数据集优于KRDA算法,只有个别数据集分类精度低于KRDA算法.出现上述现象的原因是,一方面SSVM具有类似MCSVM的特点,因而具有更高的分类性能;其次,本文算法抽取的特征之间不存在冗余信息,可以有效提高分类样本精度.7结语本文提出了一种基于SSVM的递归统计不相关特征抽取算法,基于核方法把线性特征抽取方法推广到非线性特征抽取,通过PCA/KPCA变换解决了高维特征空间中因散度矩阵奇异性导致SSVM优化问题无法求解的问题,证明了算法的收敛性以及抽取的特征具有统计不相关性,给出了算法终止条件.文中还给出了递归非线性特征抽取算法的实现方法.本文给出的线性、非线性特征抽取方法能够抽取不相关特征,有效地提高分类器的性能,克服了现有算法存在的问题,具有重要的理论和应用价值.需要指出的是,本文算法计算代价较大,如何使用现有的快捷优化算法,提高算法的速度,对于算法的实际应用具有重要的意义.
