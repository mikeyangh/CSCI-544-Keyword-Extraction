Page1使用异构互联网图像组的视频标注王晗吴心筱贾云得(北京理工大学计算机学院智能信息技术北京市重点实验室北京100081)摘要标注用户视频中的事件是一项极具挑战性的工作.目前的研究主要关注如何从大量的已标注视频中获取视频相关概念,并用来标注未知的用户视频.现实场景下的视频具有复杂性和多样性的特点,建模需要收集大量已标注的视频训练样本,这个过程非常费时费力.为了缓解这一问题,作者利用大量互联网图像来建立模型,这些图像数据涵盖了各种环境下的各种事件.然而,从互联网上得到的知识变化多样且有噪声,如果不加选择而盲目进行知识迁移,反而会影响视频标注的效果.因此,作者提出了一种联合组权重学习框架来权衡互联网上不同但相关的图像组,并用这些知识建立视频标注模型.在该框架下,作者采用联合优化的方法来获得不同图像组的权重,每一个权重值表示了相应的图像组在知识迁移中所起的作用.为了解决视频与图像特征的异构问题,作者建立了一个共同特征子空间来连接视频和图像这两个特征空间.两个视频数据库上的实验结果表明了文中方法的有效性.关键词知识迁移;视频标注;互联网图像搜索引擎;共同特征子空间1引言移动设备和数码设备的广泛普及使得用户可以随时随地拍摄视频图像,形成海量数据.对视频的人工标注常常出现缺少标注或者标注过于主观的情况,导致传统的基于文本的视频检索方法越来越不适用于海量视频数据的检索[1-2].另外,由于现实世界中的视频往往是非常复杂的,存在剧烈的摄像机运动以及很多的类内变化,给海量视频检索带来了困难.然而,收集涵盖各种情况下足够多的视频又费时费力,因此我们试图从日渐成熟的互联网图像搜索引擎中获得大量的图像知识,并把这些知识迁移到视频中.实际上从互联网上获得的图像比实验室生成的数据更具有多样性,更有利于完成现实世界的任务.为了从互联网上获得所需知识,我们使用与事件相关的关键字在互联网图像搜索引擎上收集图片.现实世界中的视频具有复杂性和多样性的特点,单一的检索词并不能很好地描述复杂场景下的视频.例如,可以由关键词“篮球”联想到“篮球赛”、“NBA”、“Kobe”等等.在本文中,为了搜集到尽可能全面的事件知识,使用了联想关键词对图像进行检索.从互联网上为每一个关键词收集一组图片,每一个事件都对应了多组从互联网检索来的图片.我们将互联网上获得的已标注图像数据称为源域(sourcedomain),将未标注的用户视频数据称为目标域(targetdomain).尽管从互联网很容易获得大量的标记数据,然而用户向互联网上传图像时标注的随意性和主观性,使得与所检索事件相关性较小的噪声图像大量存在.在这样的情况下,使用蛮力迁移(brute-forcetransfer)很可能会降低视频分类器的分类效果,这样的知识迁移称为负迁移(negativetransfer).由此可见,如何总结图像知识并选择最有效最相关的那部分数据来对未知视频进行标注显得尤为重要.为避免负迁移,我们提出了一种联合分组权重学习方法,即把互联网上获取的图像集(源域)按照联想词的不同进行分组,并根据这些源域的分组数据与视频数据(目标域)的相关性,给不同组赋予不同的权重.通过为每一类事件的分组,自动学习一个相关性权重来提高目标域从源域中获取有效知识的概率.在知识从图像迁移到视频的过程中,如何解决图像与视频特征空间的异构问题是知识迁移的又一个难点.这里需要设计一个“翻译器”,来建立两个异构特征空间之间的联系.我们引入典型相关性分析(CanonicalCorrelationAnalysis,CCA)方法来帮助知识从图像迁移到视频.使用CCA,源域的图像特征和目标域的视频特征通过两个投影矩阵同时被映射到一个共同的特征子空间.借助这个共同子空间,从源域上学到的分类器可以直接分类目标域的数据.图1所示是我们提出的使用互联网图像知识来标注用户视频的迁移学习框架.2相关工作迁移学习在语音识别[3]、文本分类[4]和图像标注[5]等方面有广泛的应用.通过知识迁移,人们可以使用先验知识来更快更有效地解决新问题.近年来,知识迁移在多媒体内容分析领域引起了学者的广泛重视[6-9].Yang等人[10]使用自适应SVM(A-SVM)方法来提高对视频事件的检测效果,新的分类器是从现有的源域SVM分类器上自适应学习得来的.Duan等人[11]通过最小化正则项误差,同时优化源域分类器的线性组合得到目标域分类器.目前有关如何利用互联网中的图像资源来标注视频的工作较Page3少.Ikizler-Cinbis等人[12]利用互联网图像迁移得到行为识别模型,但他们的工作没有利用视频的运动信息来指导视频标注,对于某些动作的识别率较低,例如:“站起”、“坐下”等动作类别.Duan等人[13]提出了一种新的使用互联网图像进行用户视频事件识别方法,该方法把视频特征和图像特征放在一个统一的框架下进行学习,但他们把视频特征和图像特征分开处理,并没有考虑到这些异性特征之间的内在联系.Wang等人[14]提出使用关键字从互联网检索相关图像并结合一些少量带标注的目标域数据来标注视频.他们的方法联合学习了视频特征和图像特征,但目标域的函数需要少量标注视频进行辅助学习,并不是完全的无监督学习.3问题描述我们的目标是借助源域和目标域的知识来学习目标域的决策函数ft(·).在迁移学习机制中,假设目标域中的一些未标注数据在训练阶段是可见的.这种源域中有标注数据,目标域中没有标注数据的知识迁移也被称为直推式知识迁移(transductivelearning)[15].在迁移学习中,源域中学习得到的决策函数可以通过一些未标注的目标域数据更好地迁移到目标域中.我们的方法通过权衡源域中不同图像组并通过建立源域和目标域共同的子空间,逐渐地将源域知识迁移到目标域.所得到的目标域的决策函数能具有更好的泛化性.我们提出的框架不仅能挖掘出源域中最有用的那部分知识,也提供了将知识从源域迁移到目标域的有效方法.注图像数据;目标域注的用户视频.P(Xs)和P(Xt)分别表示源域数据的特征空间χs和目标域数据特征空间χt的分布.我们使用不同的联想关键词来收集图像集.一个关键词检索得到的一个图像集合被称为一个组.每一个组对应于相应事件的一个概念集.由此,对于从互联网图像搜索引擎得到的图像样本根据它们所对应的联想词的不同自动划分为不用的图像组.借助于多组图像,可以获得一个事件各方面的知识.将一个事件的第g个图像组定义为Xg={狓g{1,…,G},其中狓g特征,ds表示图像特征的维度,Ng表示第g组图像样本的数目.另外,我们还将目标域中Nt个未标注定义源域的视频定义为Xt={xt中的第i个视频特征,dt表示目标域视频特征的维度.4从网络图像到用户视频的知识迁移4.1共同特征子空间由于图像特征和视频特征存在于异性的特征空间中,在源域的图像上学习到的分类器并不能直接在目标域的视频上进行分类.为了克服这种不适用性,我们引入共同特征子空间的概念.在目标域和源域的特征空间之间建立共同特征子空间,使得不同特征空间中的特征可以在这个子空间上进行对照.任意的源域样本(图像)和任意目标域样本(视频)可以通过两个映射矩阵投影到这个子空间上.设两个映射矩阵分别为狑s∈共同特征子空间的维度.定义映射函数的一般形式为狓可以是目标域或者是源域的样本.本文使用典型相关性分析[16-18](CanonicalCorrelationAnalysis,CCA)方法来学习这两个映射矩阵狑s和狑s.CCA通过假定不同特征空间中的特征来自于同一个样本,从而达到在不同的特征空间中联合降维的目的.由于要求不同的特征来自同一个样本,传统的CCA方法通常是有监督的方法.而本文中目标域的视频数据都是无标注的,所以不能直接使用CCA建立两个域的链接.实践中我们发现了视频与它关键帧的对应关系,这种关系为CCA提供了一种自然的监督信息,从而可以在图像特征空间(来自于关键帧)和视频特征空间(来自于相应的视频)之间建立起一一对应的关系.给定N个样本对{(犛1,犜1),…,(犛N,犜N)},其中犛i∈(关键帧)特征空间的样本和视频特征空间的样本.CCA的目标是利用典型相关性学习两个映射矩阵狑s∈dc×ds和狑t∈这里E^表示经验期望值,犆犛犜表示图像特征空间犛Page4和视频特征空间犜的相关性矩阵,犆犛犛和犆犜犜分别表示图像特征空间犛和视频特征空间犜的自相关矩阵.4.2预分类器定义源域第g组图像的预分类器为狑=[w1;w2]为预分类器模板,狓s,g是第g组图像中的第s个图像样本.ψ(狓s,g)和υ(狓s,g)分别是共同特征和图像特征.注意到,目标域关键帧中的图像特征分布与源域的图像特征分布在某种程度上是不一样的.因此,使用DASVM[19]来优化预分类器的模板w1和w2,使它们更适应于目标域的数据.在该方法中,源域数据只在初始化组预分类器时使用.在完成初始化后,源域样本就逐渐地被目标域样本替代,从而得到最终的分类平面.4.3联合组权重本节主要讨论如何将上一节得到的预分类器整合起来得到目标分类器.我们提出一种新的联合组权重学习方法,根据不同组与目标域的相关性进行加权整合起来.每组的权重代表了这个组对分类目标视频的贡献.在联合组权重学习中,将视频的目标分类器定义为这里αg>0是第g组的权重,将α归一化:∑G基于对不同组的平滑假设,既需要最小化目标函数在标注的源域数据上的误差,也需要最小化不同组分类器在目标域数据上的差距.学习计算框架表示如下:(Ω(ft)+λLΩL(ft)+λTΩT(ft)+λGΩG(ft))minft这里λL,λG,λT>0为平衡参数.下面详细介绍式(5)中的每一项内容.Ω(ft)=1α=(α1,α2,…,αg)T为所有分组的权重向量.的损失函数:ΩL(ft)是目标域分类函数在源域的标注数据上这里的xsi表示源域中的第i幅图像.ysi是xsi的事件标签,Ns是源域中训练样本的数量,即Ns=∑G这个正则项强制目标函数在源域样本上的决策值尽可能接近源域样本的真实值.如果仅仅使用源域中的已标注数据训练目标函数会导致目标函数在训练数据上的过拟合,从而降低了目标函数的泛化性能.在一些传统的直推式学习方法中,目标域的未标注数据也能提供一些约束信息从而提高分类效果.由此,我们也考虑使用一个分组损失函数ΩG(ft)来保证目标函数在分组上的平滑性:ΩG(ft)=∑Nt这个损失函数约束同一个事件在不同的分组应该具有相似的决策值.从域适应的角度来看,假定属于同一事件类别的不同预分类器对于目标域的未标注样本应该具有相似的决策制.例如,假设源域的第k个分类器和第g个分类器属于同一个事件,那么我们认为fk上,引入ΩG(ft)来惩罚那些远远偏离大部分事件相关组的图像组.我们还使用目标域的未标注样本来增强所学得模型的泛化性能,表示为正则项:整合以上所有各项,得到如下优化问题:minαs.t.∑G式(9)中的优化问题可以通过二次优化算法得到解决.5实验5.1数据库CCV数据库[20]包含了4659个训练视频和4658个测试视频,所有视频被标注为20个语义类别.由于我们关注于事件标注,并没有考虑该数据库中的非事件视频(如“playground”、“bird”,“beach”,“cat”和Page5“dog”).为了便于图像检索,将“weddingceremony”,“weddingreception”和“weddingdance”合并为一个关键字“wedding”,将“non-musicperformance”和“musicperformance”合并为“performance”.最终,得到如下事件类别:“basketball”、“baseball”、“soccer”、“iceskating”、“biking”、“swimming”、“graduation”、“birthday”、“wedding”、“show”、“parade”.Kodak数据库是Kodak公司历时一年多从大约100位真实用户中收集的视频.同样地,在我们的实验中只考虑6类事件相关类别(“sports”、“birthday”、“wedding”、“show”、“parade”、“picnic”).对于每一个用户视频,我们使用两种特征:视频特征和图像特征.在CCV数据库上提取144维三维时空兴趣点特征作为CCV的视频特征[21],在Kodak数据库上提取96维梯度方向直方图(HOG)和108baseballgamesfoullinesoftballdemonstrationprocessionprotest图2实验中所使用的联想关键词5.2实验设置我们使用词袋模型对图像和视频特征进行聚类.提取所有图像的SIFT特征并进行k-means聚类,得到2000个视觉单词.根据这些视觉单词进而量化得到一个2000维的图像/关键帧特征.同样,我们对CCV和Kodak数据库上的视频特征进行聚类,分别得到5000维和2000维的视频特征.对于一个给定的事件,使用5个联想关键词在维光流直方图(HOF)[13]作为Kodak数据库上的视频特征.在每一个视频上随机抽取一帧作为该视频的关键帧,并在这个关键帧上使用DoG检测子检测显著区域提取128维的SIFT特征作为图像特征[22].本文实验中的网络图像均是使用谷歌图像搜索引擎进行关键字检索收集得来的.根据CCV和Kodak所对应的的事件类型,从搜索引擎上收集13类事件图像:“basketball”、“baseball”、“soccer”、“iceskating”、“biking”、“swimming”、“graduation”、“birthday”、“wedding”、“skiing”、“show”、“parade”、“picnic”.图2显示了实验中所使用的联想关键字.每一个事件对应于多组知识.每组知识对应于一个联想关键词在图像搜索引擎上检索到的图像集.所有的这些图像集构成了源域数据.图3显示了“basketball”对应的图像组.最左一列显示了对于事件“basketball”所使用的联想关键词.每一行显示了与该联想关键词所对应的检索图像.第3组NBAbikecandlecelebrateskatesledfootballrealmadridACmilanswimmingsynchronisedaquaticscookoutfood搜索引擎中检索图片.对于每一个关键词返回的检索结果,收集前300张图片作为一个源域图像组.为了获得每一个图像组的分类器,使用该组所包含的300张图像作为正样本,从其它事件的任意图像组中随机抽取300张图像作为负样本.在训练阶段,对于CCV数据库,使用数据库给定的4659个视频作为未标注的目标域训练样本;对于Kodak数据库,使用数据库中全部195个视频作为未标注训练样本.我们将本文的方法与标准SVM方法、DASVM方法[19]、DSM方法[13]进行比较.以上这些方法可以在目标域没有任何标注数据的情况下,对目标域数据进行分类.对于标准的SVM和DASVM等方法,在每一个图像组上学习一个分类器,再将单组分类结果进行融合得到多组分类的结果.在DSM方法Page6中,使用非线性核χ2并使用SVM训练得到预分类器.本文使用AveragePrecision(AP)评价所有方法的标注结果,并将mAP(meanAveragePrecision)定义为所有事件的平均AP值.5.3实验结果在CCV和Kodak数据库上将我们的方法与现有的一些方法进行比较.图4和图5显示了这几种方法的标注性能.在表1列出了mAP结果.数据库CCV8.5210.9012.6316.48Kodak23.2928.6331.5433.54从表1可以看出本文的方法在视频标注上的有效性.在CCV数据库上,本文的方法在性能上分别比SVM、DASVM和DSM相对提高了46.95%,24.03%和7%.在Kodak数据库上,分别相对提高了40.40%、14.21%和3.7%.实验还验证了共同特征子空间在CCV和Kodak数据库上的有效性.图6和图7显示了不同特征的标注性能.本文测试了3种特征:SIFT特征(SIFT)、中间特征(inter)[5]和共同特征(common).可以发现,在大多数事件中共同特征取得了与SIFT特征等同的效果,特别是在一些与动态信息相关的事件中,如“birthday”、“parade”和“picnic”,共同特征起到了非常重要的作用.将SIFT和共同特征融合对事件进行标注时,取得了最好的标注效果.图6共同特征子空间在CCV数据库上的标注性能(AP)图7共同特征子空间在Kodak数据库上的标注性能(AP)为验证分组个数对标注结果的影响,我们列出了在使用不同组数时的实验结果.从图8中可以看出,多组图像上的实验结果始终比单组图像要好.这说明我们的分组机制可以获得对视频标注有用的信息.同时也发现mAP并不随着组数单调递增.一种可能的解释是,知识迁移的有效性取决于在源域数据与目标域数据的相关性.当这两个域的相关性越大时,知识迁移越有效.最后我们验证了式(9)中各个正则项的有效性.表2列出了当λG=0和λT=0时标注的结果.从结果中可以看出,当ΩG(ft)和ΩT(ft)从优化函数中移除时,mAP结果大大降低了.同时也列出了当所有组的权重都相等αg=1/G的结果,可以看出不相关的图像噪声在很大程度上影响了标注的效果.Page7数据库CCV10.3010.1311.4016.48Kodak24.9020.9019.7933.546总结本文提出了一种联合组权重学习的框架,利用互联网上不同分组图像的知识来完成用户视频的标注.在这个框架下,通过不同的联想词,将所得到的图像集划分为不同的组.根据不同图像组与视频之间的相关性,通过一种联合优化学习的方法,根据源域各组之间、源域与目标域之间的关系,自动学习各组的权重.实验结果表明:(1)对多种相关的知识进行权衡能够得到更好的结果;(2)给不同组赋予不同的权重对知识的有效迁移有重要的作用,通过实验证明本文对组的赋值策略是有效的;(3)在图像与视频之间进行知识迁移时,应考虑使用图像与视频特征之间的关系来提高标注效果.
