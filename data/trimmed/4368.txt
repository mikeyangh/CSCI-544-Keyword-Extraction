Page1同质球形邻域投影郑忠龙1)俞牡丹1)陈中育1)杨凡1)杨杰2)1)(浙江师范大学计算机科学研究所浙江金华321004)2)(上海交通大学图像处理与模式识别研究所上海200040)摘要在有监督学习模式下,当样本数量在类与类之间的分布具有较大的不平衡现象时,一些传统算法如LDA的性能会受到很大影响.在iid条件下,可以认为每类数据具有独特性,且类与类之间彼此独立.基于此,提出了同质球形邻域算法IHSN(IsotropicHyperSphereNeighborhood).通过在Rn-1空间中构建n个同质的正则单纯形,作为样本在嵌入空间中的同质球形邻域,利用带约束的最小二乘回归法可求得数据空间与嵌入空间的映射函数.所提出的IHSN算法有两种实现形式:基于流形学习的IHSN-ML、基于KL散度的IHSN-KL.IHSN-ML具有闭式解,速度快;IHSN-KL可解释性好,精度更高.在IRIS和PIE-CMU数据集上的实验,验证了所提算法的有效性.关键词流形学习;内蕴结构;降维;有监督学习1引言降维或者维数约简(DimensionalityReduction)Page2代表性方法有主成分分析PCA(PrincipalCompo-nentAnalysis)[1]、局部保持投影LPP(LocalityPreservingProjections)[2]以及线性判别分析LDA(LinearDiscriminateAnalysis)[3-4].PCA与LPP为无监督学习模式,LDA为有监督学习模式.LDA通过类内散度最小化同时类间散度最大化,在分类问题方面取得优异的成果.目前,涌现出了诸多针对PCA、LDA及LPP方法的改进方案[5-11].线性降维算法在处理非欧空间时,具有很大的局限性[12].流形学习ML(ManifoldLearning)算法是一类非线性方法,在非线性降维、数据的可视化等方面取得了不少成果[13-15].该类算法的假设前提是数据处于一个光滑的低维流形上[16],其优点是能够保持流形在某些假设前提下的拓扑结构[17-18].有监督思想近年来已被引入到流形学习理论中[19-20],提高了流形学习在分类任务中的性能.同类数据具有同质性,因而我们有理由相信,同类数据可以用一个有界的集合或者邻域来刻画.并且,由于不同类数据之间的差异性,我们可以认为,这些邻域之间都是独立的.对于这些邻域,我们可以用各向同质的超球体IHS(IsotropicHyperSphere)来描述,这些同质的有界邻域本文称之为“同质球形邻域”IHSN(IHS-Neighborhood).这些IHSN在仿射变换下具有不变性,自身具有对称性与自相似性.基于同质球形邻域的思想,本文尝试寻找一种线性投影,使得在适当的约束条件下,能够将给定的数据集中各类数据投影到各自的球形邻域中,使得类内散度尽可能小,各类类间散度尽可能的均衡.为此,本文提出了两种方法以实现上述思想.2IHSN算法原理2.1IHSN的构建在有监督学习模式中,同类数据具有同质性,因而我们有理由相信,同类数据可以用一个有界的集合或者邻域来刻画,并且,由于不同类数据之间的差异性,我们可以认为,用来刻画这些数据的邻域之间具有独立性.在模式分类任务中,为了更为有效地对数据进行分类,有监督学习方法通常会对类间散度和类内散度进行处理,一般是最大化所有类间散度、最小化所有类内散度[17].以LDA为例,如果给定的各类数据的方差具有较大的不同,即样本不平衡情况下,其分类的效果将受到很大的影响.图1所示为三类样本数据进行有监督学习.假定有三类数据都分布在各自的有界邻域内,利用LDA方法可以将这三类数据降维至二维空间(对于LDA而言,二维将是Bayes意义下的最优空间).由于LDA是最大化总的类间散度,因而可能会得到如图1(a)所示的结果,可以看出,这样的分类模型的泛化能力是比较弱的.图1(b)所示为较为均衡的情况,此时的分类任务将变得较为简单,泛化风险也更小.图1数据不平衡和平衡示例(实线为类间间隔)不妨对给定的数据集做如下假设:同类数据具有同质性;不同类数据之间具有独立性.在特征/降维空间中,我们希望同类数据落在一个有界邻域内,并且,不同类数据之间尽可能平衡地分开,以达到好的泛化性能.采用同质球形邻域IHSN可以满足上述假设,这些IHSN在仿射变换下具有不变性,自身具有对称性与自相似性.定义1.在Rn-1维空间中,存在n个点,它们两两之间的距离为r.这n个点的集合称之为n-单纯形(n-RS,RegularSimplex).当r=1时,称之为n-标准单纯形.性质1.n-标准单纯形是对这n个点最为平对照n-RS与IHSN的概念和性质,采用n-RS的顶点作为IHSN的中心点不失为一个好的选择.即以n-RS的顶点为中心,r为半径构造超球体作为一种邻域来构造IHSN,所构造的这些邻域如同各向同质的容器(ISOtropicContainer,ISOC),一般来衡和对称的一种分割形态.Page3存放某类数据.当n=3时,所构造的3-RS如图2所示.对于任意的n-RS的构造,如算法1所述.算法1.正则单纯形的构建算法.1.初始化.ISOC间距ELength=L,半径r,嵌入空间2.令犞犕犪狋(1,1)=-1,犞犕犪狋(1,2)=1;3.FORi=1TOd4.HValue=i+15.犞犕犪狋(i,i+1)=HValue6.犞犕犪狋(i,i+2(d+1))=HValue/(i+1)7.ENDFOR8.犞犕犪狋的列为ISOC的中心点.2.2基于流形的IHSN2.2.1拓扑结构的保持流形学习的特点是希望在特征空间中保持原始数据的拓扑结构.ISOMAP则通过计算所有点对之间的测地距离(GeodesicDistance)来得到低维的嵌入结果.与LLE[14]和LE[21]算法相比,ISOMAP在整体拓扑结构的保持上具有的一定的优势.2.2.2IHSN-ML算法本节将阐述IHSN-ML(IHSN-ManifoldLearning)算法.给定样本集合X=[X1,X2,…,Xc]∈RD×n,包含c类,每类Xi中包含有ki个样本,即∑c计算非线性映射:假定利用算法1构建了c个ISOCs,表示为[ISOC1,ISOC2,…,ISOCc],寻找一个线性映射犃将γi平移至ISOCi中,得到Yi.IHSN-ML算法优化带有约束条件的最小二乘回归模型:式(1)是一个严格凸问题,可以用梯度下降法求解[22].本文采用更为快捷的Lagrange方法:f(犃,λ)=trace{(犢-犃T犡)T(犢-犃T犡)}+∑c-1式(2)的对偶形式为g(λ)=min犃f(犃,λ)=trace(犢犢T-犢犡T(犡犡T+Λ)-1(犢犡T)T-bΛ)其中,Λ=diag(λ).式(3)对λ求一阶偏导和二阶偏导(Hessian矩阵)有2g(λ)λiλj=-2((犡犡T+Λ)-1(犢犡T)T·可得算法2.IHSN-ML算法.1.由算法1构建ISOCs;2.由流形学习算法计算γi;3.通过式(7)计算投影矩阵犃.2.3基于散度的IHSN2.3.1KL散度KL(Kullback-Leibler)散度,亦称KL距离或者相对熵,用来衡量两个概率分布的差异性.给定两类数据Ci和Cj,其高斯分布分别为N(μi,Σi)和N(μj,Σj),则Ci和Cj之间的对称KL散度定义为KL(ij)=tr∑i∑-12.3.2IHSN-KL算法本节将阐述实现IHSN算法的第2个思路,基于KL散度的IHSN-KL方法.Page4IHSN-KL方法的思想是直接在数据空间与IHSN特征空间中寻找一种线性映射犘∈RD×(c-1),而不经过流形学习进行降维.在犘的作用下,两类数据Ci和Cj的分布分别为N(犘Tμi,犘TΣi犘)和N(犘Tμj,犘TΣj犘).则在Rc-1空间中,Ci和Cj之间KL散度为fij(犘)=tr犘T∑i()犘犘T∑j()犘-1其中,μij=(μi-μj).对式(8)进一步整理,有fij(犘)=tr犘T∑i()犘-1(犘TΦij犘其中,Φij=μijμTij+∑j在多类情况下,IHSN-KL的目标函数为犘=argminf(犘)=∑c其中,σij=VerMati-VerMatj.2.3.3IHSN-KL优化求解度的迭代算法进行求解,有由于式(10)没有闭式解,因而本文采用基于梯其中,f犘=∑c犘=θ1(犘)fijθ1(犘)犘=2Φij犘Ψi-2∑iθ2(犘)犘=2Φji犘Ψj-2∑jΨi=犘T∑i()犘-1,Ψj=犘T∑j()犘-1.在采用梯度下降法求解时,需要给定一个合理的初始值,然后进行迭代求解.3仿真实验将提出的IHSN-ML、IHSN-KL两种算法在IRIS、PIE-CMU[23]两个测试集上进行仿真实验,采用的比较算法包括PCA、LDA、OLDA、LPP和OLPP[24].如果某个算法涉及图的构建,如无特别说明,统一采用0-1权值矩阵来构建[24].3.1IRIS数据集IRIS数据集中共计150个样本,分为3类,每类50个.训练集60个样本,每类20个,随机抽取,剩余的样本为测试集.重复50次这样的随机实验.降维后的特征空间为二维.分类时采用最近邻分类器(NearestNeighborClassifier,NNC).需要说明的是,对于IHSN-ML、IHSN-KL两种算法,分类时只需计算与各ISOC中心点的距离即可,因而可以提高测试的速度.在IRIS数据集上的平均分类精度如表1所示.3.2PIE-CMU数据集PIE-CMU数据集包含68个人的超过4万张人脸图像.参照文献[25]的实验环境设置,重复20次训练集和测试集的随机划分,取平均的实验结果,错误率参见表2,括号中的数字表示维数.算法名称5Train10Train20Train30TrainPCA69.91(338)55.74(654)38.09(889)27.87(990)LDA31.52(67)22.44(67)15.38(67)7.77(67)OLDA30.77(67)20.60(67)13.15(67)6.53(67)LPP30.83(67)21.14(67)14.06(146)7.13(131)OLPP21.42(108)11.35(265)6.51(493)4.83(423)IHSN-ML24.04(67)13.63(67)7.53(67)4.96(67)IHSN-KL24.80(67)12.97(67)6.10(67)4.23(67)4结束语针对LDA等传统有监督学习算法在样本严重不平衡时效果急剧下降的特点,提出了IHSN-ML、IHSN-KL两种算法.所提算法的核心思想是通过在R(n-1)空间中构建n个高度同质的正则单纯形,作为数据在嵌入空间中的映射结果,利用带有约束Page5的回归模型寻找最优的投影矩阵,从而获得较为平衡的类与类之间的判别特性.
