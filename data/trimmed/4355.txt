Page1基于选择性集成策略的嵌入式网络流特征选择潘吴斌1),3)程光1),3)郭晓军1),2),3)王艳1),3)1)(东南大学计算机科学与工程学院南京210096)2)(西藏民族学院信息工程学院陕西咸阳712082)3)(计算机网络和信息集成教育部重点实验室(东南大学)南京210096)摘要机器学习在网络流量分类中存在特征选择度量指标单一、类别不平衡和概念漂移等问题,使得模型复杂度提高、泛化能力下降.该文提出基于选择性集成策略的嵌入式特征选择方法,根据选择性集成策略选取部分特征选择器集成,再改进序列前向搜索和封装器组合方法二次搜索最优特征子集.实验结果表明该算法在保证分类效果的同时有效降低了特征子集复杂度,从而达到了分类效果、效率和稳定性的最优平衡.关键词选择性集成;特征选择;嵌入式;稳定性1引言流量分类技术在网络测量与安全领域应用广泛,一方面,根据应用实时性要求优化网络通信资Page2关特征会增加模型复杂度、降低模型可信度,导致分类效果和效率同时下降.然而,特征选择方法可以有效地消除冗余和不相关特征,选取最优特征子集.当前,借助特征选择方法还存在一定的局限性:(1)概念漂移使得特征选择结果很难保持稳定,特征属性及其数目随之改变;(2)不同的特征选择方法缺少统一的评价指标.当前特征子集的好坏主要由分类性能来评价,而各个特征子集的分类性能不稳定,有时会出现极个别分类性能较低的现象;(3)有些机器学习算法获得的分类准确率也不稳定,这与机器学习算法数据预处理过程有很大关系,比如C4.5决策树会预先离散化数据.本文受选择性集成思想[3]和Embedded方法[4]启发,提出基于选择性集成策略的嵌入式特征选择方法,采用选择性集成方法选取部分特征选择器集成,再通过改进序列前向搜索和封装器组合方法进一步搜索特征子集,该方法可以获得特征较少且稳定的最优特征子集.另外,采用离散化方法分割连续型数据与类分布一致,简化数据,减少噪声数据,提高机器学习算法分类效果和效率.本文研究贡献主要在于以下几点:(1)本文提出基于选择性集成策略的嵌入式特征选择方法.一方面,单个特征选择方法很难获得稳定的最优特征子集,选择性集成策略综合多个特征选择器的优点,获得比集成全部更好的性能.另一方面,采用穷举式搜索耗费时间长,将集成后的特征采用启发式搜索进行二次特征选择,快速剔除不相关特征.最后,将准确率最高的特征子集作为全局特征子集,有效提高特征子集的稳定性.该方法有效消除不相关和冗余特征,获得稳定的最优特征子集,有利于提高模型泛化能力、分类效果和效率.(2)本文引入稳定性度量评估特征子集.当前特征选择方法采用单一度量标准(如相关性、一致性、分类精度)选取特征,且没有度量指标评估特征子集,文中将特征加权出现频率作为稳定性度量标准,从整体上评估特征子集,有效应对分类过程中概念漂移问题.(3)本文采用最小描述长度原理的离散化方法解决机器学习方法分类准确率不稳定性的问题.有些机器学习方法分类连续型数据的效果和效率低下,离散化方法将连续属性转化为有限的区间,达到分类准确率与离散区间的最优平衡.根据信息论原理将连续属性分割成多个离散区间,以最小描述长度为控制离散化算法的结束准则.该方法有利于简化数据,减少数据中的噪声,提高机器学习方法的分类准确率.本文第2节综述网络流量分类中特征选择研究的现状;第3节描述基于选择性集成策略的嵌入式特征选择方法;第4节引入两项评价指标,包括性能(平均准确率、查准率、查全率和综合评价F-Measure)和稳定性;第5节给出实验数据集、简要说明实验环境和流程;第6节从准确性和效率性进行实验分析,并提出离散化方法解决机器学习分类准确率不稳定性问题;第7节总结全文并展望未来的工作.2相关研究目前,基于统计特征的机器学习流量分类方法研究广泛[5-7],但很少有研究人员关注分类过程中存在的类别不平衡性和概念漂移问题[8].一般情况下,类别不平衡性主要通过重取样、特征选择和改进分类算法等方法来解决,但当特征维数较高时,重取样和改进算法作用不明显[9].所谓概念漂移就是类别分布随时间发生变化,使得分类模型很难保持较高的分类准确率.这些影响使得基于统计特征的机器学习方法很难获得较高的分类效果和效率.特征选择方法可以很好的解决维数灾难问题,但现有的单个特征选择方法[10]未考虑特征属性与应用间的内在关联,仅从单一度量指标评估特征,泛化能力和稳定性不高,存在一定的局限性.Li等人[11]考虑到不同时间域和空间域对流量分类效果的影响,采用FCBF和对称不确定性度量选择特征子集,由于单个FCBF特征选择方法对于多个数据集很难保持较高的分类性能,没有很好的解决概念漂移问题.张宏莉等人[12]提出了基于Bagging集成学习的分类方法,虽然Bagging集成学习可以提高总体分类精度,但会使实例较少的类别分类准确性降低,类不平衡问题仍然存在.deSouza等人[13]提出一种动态Adaboost集成学习算法,并对IP分组来简化分类模型,但分类模型中的IP群组不能用于其他网络环境,概念漂移问题仍然存在.Zhang等人[8]提出了一种采用加权对称不确定性和ROC曲线下面积度量的混合特征选择算法,不需要改变类别分布就能提高少数类的查全率和查准率、以及分类的字节准确率,有效解决类别不平衡性,但没有解决动态数据流引起的概念漂移.Fahad等人[14]提出一种多种特征选择方法集成的混合式特征选择方法,该方法有利于简化分类模型,减少模型建立和分类时间,但是该方法耗费时间长,且没有考Page3虑类不平衡性和概念漂移问题.与前述工作相比,本文针对流量分类中类不平衡和概念漂移问题,提出基于选择性集成策略的嵌入式特征选择方法(embeddedFeatureSelectionusingSelectiveENsemble,FSEN).FSEN采用选择性集成思想结合部分特征选择器的优点,通过启发式搜索快速剔除不相关和冗余特征,再采用离散化方法将连续型数据优化为离散型数据,简化数据,减少数据噪声,有效提高分类效率和效果.另外,由于特征子集缺乏统一的评价标准,以及特征子集质量对分类的重要性,本文引入一种稳定性度量评价概念漂移引起的特征子集动态变化现象.3基于选择性集成的嵌入式特征选择FSEN算法主要包括两部分:第1部分,将多个特征选择器选取的特征子集根据评价指标进行排序,再根据选择性集成策略选择部分特征选择器,从已有的特征选择器中将作用不大和性能不好的特征选择器剔除,将保留的特征选择器集成;第2部分,采用朴素贝叶斯算法评估序列前向搜索产生的特征子集,以分类准确率下降为结束准则,再比较多个数据集的最优特征子集选出全局特征子集,提高特征子集稳定性,FSEN流程如图1所示.该方法不仅可以有效剔除不相关和冗余特征,还能提高特征选择的稳定性,使分类准确率与稳定性达到最优平衡.3.1选择器集成选择器集成过程中采用选择性集成(selectiveensemble)策略,简单地说,选择性集成就是对同一问题的多种方法进行适当的选择,将所选择的结果进行结合获得比集成全部方法更好的效果[3].特征选择过程中的选择性集成就是从一组特征选择器中选择部分集成,假定在m个特征属性上的期望输出D=[d1,d2,…,dm],其中dj表示第j个属性的期望输出,dj∈{-1,1}(j=1,2,…,m).令犳i表示第i个特征选择器的实际输出,犳i=[fi1,fi2,…,fim]T,其中fij表示第i个特征选择器在第j个属性上的实际输出,fij∈{-1,1}(i=1,2,…,N;j=1,2,…,m).当第i个特征选择器在第j个属性上的实际输出正确时,fijdj=1,否则fijdj=-1.这样,第i个特征选择器在这m个属性上的泛化误差为Error(x)定义为和向量犛狌犿j代表所有个体特征选择器在第j个属性上的实际输出的和,即则集成在第j个属性上的输出为Sgn(x)定义为因此,集成的泛化误差为假设集成中剔除第k个特征选择器,则新集成在第j个示例上的输出为新集成的泛化误差为从式(6)和式(8)可知,如果E^不小于E^,说明剔除后的集成比原来的集成更好,即当犛狌犿j>1时,剔除掉第k个特征选择器不影响dj,又由于函数Error(x)和Sgn(x)的性质:Page4Error(Sgn(x)-Error(Sgn(x-y))=可得j∈{j犛狌犿j1}由于fkjdj=-1,式(11)满足结果.理论分析表明集成部分特征选择器优于集成所有特征选择器.特征选择过程中通过对特征选择器排序来选择性集成部分特征选择器,首先,根据准确率评估准则对特征选择器排序,然后,根据指定的特征选择器个数部分选取.3.2启发式搜索FSEN算法获取的最优特征子集机械式组合容易引起冗余,无法获得较优的特征子集.假设特征集中有n个特征,那么存在2n-1个非空特征子集,搜索策略就是从2n-1个候选特征子集中寻找最优特征子集.因此,本文改进序列前向搜索算法进一步精选特征子集,每次从未选入的特征中选择一个特征,使它与已选入的特征组合在一起时判据值J最大,直到判据值J降低为结束准则.设特征集F=f1,f2,…,f{集F0=,已选入了k个特征的特征子集记为Fk,把未选入的n-k个特征Fj(j=1,2,…,n-k)逐个与已选入的特征Fk组合计算判据值J,若J(Fk+x1)J(Fk+x2)…J(Fk+xn-k),则x1选入,下一步的特征组合为Fk+1=Fk+f1,该过程一直进行到最大判据J值降低为止,从而避免搜索整个特征空间,该算法时间复杂度n(n-1)/2,搜索过程如表1所示.迭代次数当前特征子集评估值最优特征子集1233.3FSEN算法算法1的伪代码描述了基于选择性集成策略的嵌入式特征选择方法的具体执行过程.行1~4采用5种特征选择器提取特征子集,包括相关性、信息增益、统计、一致性度量,每种算法是各个度量指标的代表性算法,包括FCBF[15],InfoGain,GainRatio,Chi-square,CBC.GainRatio作为一种补偿措施来解决InfoGain偏向选择取值多的属性的不足,但它也有可能导致过分补偿,因此两种算法可以互为补充.行5~8根据选择性集成策略选取部分特征选择器,行5评估每个特征子集,行6选择评估指标最高的3个特征子集对应的选择器,行7合并特征子集的特征,行8返回相关性较高的特征子集,但其中有冗余特征还会降低分类性能,如何消除这些冗余特征是整个特征选择过程的关键.行10~16采用启发式搜索策略从行8返回的特征中选择最优特征子集,直至加入特征后分类准确率下降.行11采用序列前向搜索方法产生特征子集,行12根据朴素贝叶斯算法评估每轮特征子集S的分类准确率,行13选出最高的分类准确率,行17根据分类准确率找出不同数据集的全局特征子集.剔除了不相关和冗余特征的全局特征子集有利于简化分类模型,提高分类准确率和稳定性.算法1.FSEN算法伪代码.输入:数据集Data:Trafficdatasets特征选择器T:FiveFeatureselectors特征子集Subset:Featuresubset特征集合F:Featuresinsubsets输出:全局特征子集Globalfeaturesubset1.fordatainDatado//Part1SelectorsEnsemble2.fortinTdo3.Subset[optimal]··=FindOptimalSubset(data,t)4.endfor5.β··=Evaluate(Subset[optimal])6.β[top]··=FindTopThree(β)7.F∩=F{β[top]}8.returnF//Part1finished9.repeat//Part2SecondaryFeatureSelection10.forfinFdo11.Subset··=GenerateSubset(F)12.θ··=Evaluate(Subset)13.Subset[best]··=FindBestSubset(Max(θ))14.F-=f15.endfor16.untilF∈‖θ[iteration+1]<θ[iteration]17.Subset[global]=FindOptimal(Subset[best])18.endfor19.returnSubset[global]//Part2finishedPage54评价标准4.1性能指标准确率常用于评价识别新流量的能力.假设N为流量样本数,m为应用类型数.nij表示实际类型为i的应用被标记为类型j的样本数.真正TP代表实际类型为i的样本中被正确标记的样本数,TPi=nii.假负FN代表实际类型为i的样本中被误标识为其他类型的样本数,FNi=∑j≠i类型为非i的样本中被误标识为类型i的样本数,FPi=∑j≠i体准确率(OverallAccuracy)、查准率(precision)、查全率(recall)和综合评价(F-Measure)的形式化描述.查准率和查全率体现了识别方法在每个单独协议类别上的识别效果,整体准确率体现了识别方法的总体准确率,F-Measure是查准率和查全率的综合评价指标.一个好的方法不仅要求具有较高的总体准确率,还应该在各个类别上具有较高的查准率、查全率和F-Measure,特别当样本类别分布不均匀时,查准率、查全率和F-Measure可以准确获知每个类别的分类情况.4.2稳定性概念漂移是实际分类过程中最常见的问题,类别分布随时间发生改变,特征选择方法很难选取稳定的特征子集来保持较高的分类精度.文献[16]采用汉明距离(HammingDistance)和Tanimoto系数作为稳定性度量,但只适用于固定大小的特征子集.另外,由于不同特征选择方法度量标准不统一,无法比较.本文提出的稳定性度量可以统计不同大小的特征子集,也可以比较不同特征选择方法获得的稳定性.另外,针对出现频率高的特征对稳定性贡献大,采用加权方式突出其稳定性作用.因此,有必要评估特征子集的稳定性,特征选择的稳定性主要研究当样本类别分布发生变化时,特征选择算法的鲁棒性.特征选择方法不仅要获得很高的分类准确率,可靠的稳定性也必不可少.令特征子集S={S1,S2,…,Sn},集合X={f|f∈S,Ff>0}=∪n出现次数为Ff,总出现次数N=∑y∈X征一致性为最小出现次数Fmin=1,最大出现次数Fmax=n.Cf()i=0表示fi出现次数为1;Cf()i=1表示fi出现次数为n,平均一致性为C(S)=1带权重的一致性为即CW(S)=∑f∈X如果()CWS=0,当且仅当N=X,每个特征只出现一次;如果()CWS=1,当且仅当N=nX,每个特征只出现一次;如果N>X,肯定有特征出现超过一次,()CWS>0.5实验5.1实验数据集目前还没有一个权威的数据集来评测流量分类性能,采用不同数据集进行分类研究,可以有效验证该算法的有效性,本文采用CERNET华东(北)地区网络中心采集的CERNET数据集(CNT)和Moore_Set[15](MS)两组数据集.CNT数据集是采用tcpdump抓取华东(北)网络中心16个C类地址2014年4月2日13:00约60分的双向全报文数据,大小为30GB,构成5个数据集,采用改进OpenDPI获取五元组标准集[17],再利用tcptrace获取双向流的110种统计特征,CNT数据集共包含352884个完整的双向流网络流样本,被分为7类,流样本类别具体分布如表2所示.MS数据集是在同一结点处随机抽样产生,数据集只选用语义完整的TCP双向流作为网络流样Page6本,每条流包含249项特征属性,共包含9种类型376832个网络流样本,被分为5个数据集,每类网络流的数量和所占的比例见表3.类别数目百分比/%类别数目百分比/%HTTP27495877.92QQ73122.07Flash121103.43Bittorent27960.79SSL95592.71Nomatch4435012.57ICMP29370.83Total352884100.00类别数目百分比/%类别数目百分比/%WWW32809287.07P2P20940.56MAIL285677.58Data-base26480.70FTP-control30540.81FTP-data57971.54FTP-pasv26880.71Services20990.56Attack17930.48Total376832100.005.2实验流程图2描述了特征选择算法的具体流程[18].首先,评价函数对产生的特征子集进行评价,直至评价结果满足结束准则为止,否则继续评价下一组特征子集.然后,根据选取的特征子集建立分类模型,根据模型识别新的应用,再计算分类准确率,最后,对同一算法产生的多个特征子集计算稳定性.机器学习算法分类精度采用十折交叉验证进行评估,十折交叉验证是将数据集分成10份,轮流将其中9份作为训练数据,1份作为测试数据,10次结果的正确率的平均值作为对算法准确率的估计.本文基于Weka-3.7.10[19]二次开发,在eclipse上调用Weka的API完成特征选择和流量分类任务.所用实验平台运行Windows7操作系统,CPU为表4分类准确率数据集方法FSEN96.2296.5496.4295.6496.4997.4698.8698.4397.4297.98FCBF92.2195.0192.5192.4394.5296.9894.6093.6595.4894.91InfoGain44.3377.5376.1494.5728.3690.8689.0496.5085.2383.91GainRatio92.6894.0990.8692.794.3710.3710.2484.1389.6888.23Chi-square59.5595.5990.0295.4995.6196.2695.0596.9691.9939.99CBC35.5013.0213.545.5318.4821.5893.5876.9331.5179.43Original12.0311.1419.9311.6218.1257.8960.7084.4574.5179.29注:Original代表未进行特征选择的特征全集.由表4可以看出FSEN特征选择后的分类准确率均高于其他特征选择算法,而且准确率比较稳定;FCBF获得较稳定的分类准确率,但总体分类准确率低于FSEN,Original分类准确率不高,因为特征全集中存在不相关和冗余特征,与Original比较可以看出,有些数据集特征选择后的分类准确率反而IntelCorei5-3210,2.5GHz,内存为DDR31600MHz4GB,Java开发平台eclipse-4.2.2.6实验分析6.1准确性当前网络上运行着大量的应用,同时新应用不断出现,每个应用都有独特的流统计特征,流统计特征随着时间推移发生概念漂移,使得分类器很难保持较高的分类准确率.概念漂移导致类分布不断变化,使得特征选择方法难以获得稳定的特征子集,因此,有必要选择稳定的特征子集,使其能在很长一段时间维持稳定的分类准确率.将FSEN算法在CNT数据集上进行特性选择,采用朴素贝叶斯算法获得分类准确度,并与5种常用特征选择算法(FCBF[15],InfoGain,GainRatio,Chi-square,CBC)进行对比,其中FCBF在文献[15]中被采用,分类准确率如表4所示.为了进一步验证算法的可行性,再统计MS数据集分类准确率,如表4所示.变低,特征选择后的分类模型反而变差了,说明有些特征选择方法鲁棒性差.另外,从表4中可以看出,FSEN和FCBF算法分类准确率相对稳定,其余算法随着时间推移准确率有较大变化,很不稳定,说明传统的特征选择方法无法获取稳定的特征子集应对概念漂移问题.而FSEN分类准确率稳定在95%以Page7上,因为FSEN算法综合多个特征选择方法的优点,剔除概念漂移产生的局部最优特征,获得稳定的特征子集.为了进一步验证FSEN算法应对概念漂移的有效性,采用平均准确率和稳定性从整体上评价特征选择方法,如图3所示.图3可以看出FSEN算法的平均分类准确率和稳定性均高于其他算法,其他算法存在分类准确率不稳定现象,FCBF虽然获得了较高的分类准确率,但稳定性相对较低;而Chi-square和InfoGain算法的分类准确率和稳定度均高于60%.从稳定性来看,FSEN算法明显高于其他算法,因为FSEN算法图3平均准确率和稳定性表5查准率和查全率方法FSEN96.2698.0396.8698.1095.8897.94FCBF93.3495.1295.9893.3493.3284.32Chi-square64.1989.1196.9697.0264.2289.10GainRatio74.9656.5395.8069.2074.9856.50InfoGain87.2584.0596.0497.0287.2484.08CBC17.2160.6196.5684.8417.2060.60表5可以发现FSEN算法的查准率和查全率较为接近,表明分类性能稳定.然而,FCBF具有较高的查准率及较低的查全率,表明其中有应用类型分类精度不高,需要对该类型进行增量学习.从特征数目来看,FSEN算法选取的特征子集数目最小,因为FSEN算法借助集成学习的优势,保留了不同数据集的本质特征,对部分相关的特征予以剔除,具体特数据集CNTServerPort表6FSEN算法产生的特征子集选取稳定的特征子集作为不同数据集的全局特征子集.综合来看,FSEN算法具有较好且稳定的特征选择能力,达到分类准确率和稳定性的最优平衡.一个好的识别方法不仅要有较高的识别准确率,还应该在每个待识别的应用上具有较高的查准率、查全率和F-Measure.各个应用的样本分布不均匀,对每个应用的查准率、查全率和F-Measure特别重要.分类准确率只能综合评价整个数据集的识别精度,查准率、查全率和F-Measure可以有效评价各类的分类情况,各个特征选择算法的准确率、查准率、查全率和特征数目如表5所示.征描述如表6所示.CNT数据集的特征子集是ServerPort(端口号)和Misseddata(丢失字节数),虽然基于端口号的分类方法由于动态端口号而失效,但端口信息仍然是重要的特征;Misseddata表示实际收到的字节数与期望收到的字节数的差值;MS数据集的特征子集是ServerPort,Ave_seg_size(平均包大小)和Init_win_bytes(初始窗口字节Page8数),ServerPort是重要特征,Ave_seg_size表示平均包大小,不同的应用包大小差别很大,Init_win_bytes表示初始窗口发送的字节数,两组数据集包含的特征区别性都很强,而且特征之间也不存在冗余性.说明FSEN算法选取的特征作为全局特征子集,不受概念漂移作用的影响.图4描述了数据集CNT1和MS1各个类别的综合评价F-Measure,从CNT1图中可以看出,FSEN算法除了类SSL和QQ的F-Measure低于Chi-square算法以外,其余类的F-Measure都高于其图4综合评价F-Measure6.2效率性指标选取不同大小的CNT数据集(10000,20000,40000,80000,160000)执行FSEN特征选择,每个过程执行10次取平均值,各个特征选择算法的运行时间如图5所示.另外,采用FSEN特征选择在3个CNT数据集上执行特征选择,特征子集的模型建立时间和分类时间分别如图6、图7所示.从特征选择执行时间可以发现FCBF、Gain-Ratio、Chi-square和InfoGain所需的时间较少,而CBC的执行时间较长,FSEN执行时间长是因为集成了多个特征选择算法,可以采用并行计算来加快他算法,特别是类Flash和Bittorrent的F-Measure明显高于其他算法.另外,从MS1图中可以看出,FSEN算法的F-Measure在各个类别上都超过60%,除了类DataBase和Services的F-Measure略微低于FCBF算法以外,其余类别的F-Measure都是最高的.因为FSEN算法集成多个特征选择方法的优点,同时兼顾了多种度量标准,而不是从单一的度量考虑.综合来看,FSEN算法的F-Measure明显优于其他算法,在各个类别上都获得较高的分类性能,有效处理了类不平衡问题.Page9处理速度.从模型建立时间来看,FSEN低于其他特征选择算法,但相差不大.而从分类时间来看,FSEN算法明显低于其他算法.综合来看,FSEN算法不论是模型建立时间还是分类时间都少于其他特征选择算法,主要是因为FSEN算法产生的特征数目较少,简化了分类模型,同时提高了分类准确率和稳定性,最终达到分类准确率和稳定性的最优平衡.6.3特征离散化采用5种常用的机器学习算法(C4.5,Bayes-Net,KNN,NB和SMO)分类FSEN算法获取的特征子集,结果显示NB和SMO算法的识别精度不太稳定,如图8所示.不管采用什么流统计特征,C4.5[20]、BayesNet和KNN分类准确率总高于NaiveBayes和SMO,发现前3种算法分类前对数据进行“离散化”预处理[21].机器学习算法主要用于处理离散型数据,虽然可以分类连续性数据,但效果和效率低[22].由于流统计特征中存在连续型数据,导致机器学习分类性能下降.离散化可以简化数据,消除噪声,使得分类器更快、更精确、鲁棒性更好.同时,最小化类和属性之间的相互依赖.因此,采用一种基于最小描述长度原理的启发式离散化算法[23]来解决.离散化方法根据信息论原理将连续属性分割成多个离散区间,以最小描述长度为控制离散化算法的停止指标,在分类错误与离散区间之间找到一个最优平衡.采用离散化方法后获得的分类准确率如图9所示.通过比较不同机器学习算法采用离散化预处理前后的分类性能,对比图8、图9可以发现离散化后的NB和SMO分类准确率明显提高,另外3种算法保持较高的分类准确率.离散化有助于分割连续型数据与类分布一致,简化分类模型,同时消除数据中部分噪声,提高机器学习算法分类效果和稳定性.7结论特征选择从高维数据中选取最优特征子集,有利于提高模型鲁棒性,减少模型建立时间和分类时间,从而提高分类准确率和泛化能力.本文提出了一种基于选择性集成策略的嵌入式特征选择方法,通过准确率、稳定性和时间性能比较不同特征选择算法的性能.实验结果表明该特征选择算法稳定性强,而且特征子集较小,有效简化分类模型,提高分类效果和效率,使分类准确率与稳定性达到最优平衡.另外,采用离散化方法简化数据,有效减少数据噪声,进一步提高机器学习算法分类稳定性.下一步主要研究将结合本文提出的嵌入式特征选择算法,采用集成学习与代价敏感学习方法,通过特征选择和机器学习紧密结合解决流量分类中的类不平衡和概念漂移问题.致谢审稿人对论文提出了宝贵意见,在此表示感谢!
