Page1图形处理器通用计算关键技术研究综述王海峰2),3)陈庆奎1),2)1)(上海理工大学光电信息与计算机工程学院上海200093)2)(上海理工大学管理学院上海200093)3)(临沂大学信息学院山东临沂276002)摘要当前图形处理器的通用计算取得长足发展,为适应通用计算图形处理器在硬件体系结构和软件支持方面完成相应调整和改变,面对各种应用领域中数据规模增大的趋势,多GPU系统和GPU集群的研究应用日趋增多.以流处理器及图形处理器硬件体系为依据,介绍学术和工业领域中流处理器及图形处理器体系变化趋势.从软件编程环境、硬件计算与通信等方面展开讨论,阐述通用计算中图形处理器的关键问题,包括编程模型及语言的发展和方向,存储模型的量化研究、访存模式和行为的优化以及分布式存储管理的热点问题,典型通信原型系统的对比及通信难点的分析,GPU片内和片间的负载均衡,可靠性和容错计算,GPU功耗评测及低功耗优化的研究进展.综述在海量数据处理、智能计算、复杂网络、集群应用领域中图形处理器的研究进展及成果.总结在通用计算发展中存在的技术问题和未来挑战.关键词图形处理器;通用计算;可编程性;GPU集群1引言计算机图形处理器GPU(GraphicsProcessingUnit)具有极高计算性能和相对廉价的成本,以超过摩尔定律的速度更新硬件,从2003年后在通用计算领域图形处理器GPGPU(General-PurposeComputingGraphicsProcessingUnit)取得长足发展.在学术界人们研制了以下典型处理器芯片:斯坦福大学的Imagine、Merrimac及国防科学技术大学的飞腾FT64[1],而飞腾处理器已成功应用于“天河一号”超级计算机的设计中.在工业界Nvidia、AMD等公司持续更新其GPU硬件产品,利用推广软件的方式来扩大各自硬件产品的生态圈,目前Nvidia在通用计算领域中成果显著.国内吴恩华教授[2-3]总结了2004年之前GPU的发展历史和现代GPU基本结构,阐述了通用计算的技术原理、软件工具及详细的发展之路,总结了各种应用领域的进展,如流体模拟、代数计算、数据库应用、频谱分析等.国外Owens研究团队综述了2008年之前GPGPU的硬件及软件方面的应用进展[4-5],分别从GPU体系、计算编程模型、软件开发环境和编程语言等方面作了详细介绍;然后重点分析了GPU计算中的关键技术和算法问题,即原语法和排序、数据库查询、微分方程、线性代数中的算法,这些算法注重提高数据并行性、减少SIMD线程分歧执行、最大化算术密度及提高流带宽4个方面的优化研究;最后介绍在游戏物理、科学计算领域中的具体实例.本文重点关注最近3年内GPGPU发展过程中关键技术的研究与进展,特别是由于海量数据处理的需要,单节点GPU受到硬件限制,多GPU系统和GPU集群的应用研究成为新热点.主要包括以下几个方面:(1)GPU芯片特点及体系更新,以此为基础展开存储、通信及编程模型等方面的论述;(2)GPU体系变化对编程模型的影响,编程语言的发展趋势;(3)对存储模型及性能优化的分析;(4)通信是影响并行计算性能的重要因素,分垂直和水平通信两个方面阐述现有研究成果及存在的问题;(5)在单GPU和GPU集群范围内讨论负载均衡问题;(6)可靠性分析及容错计算;(7)低功耗优化及绿色计算.本文第2节介绍GPU体系的发展变化,特别是工业界中典型产品的对比;第3节介绍GPGPU中关键技术的研究及进展,指出现有研究成果存在的问题和发展思路;第4节简述非图形计算应用和集群应用中的新成果;最后总结现有研究面临的挑战和展望未来工作.2GPU体系及发展2.1GPU体系的通用计算历程从芯片体系的演化能看出GPU向通用计算的发展趋势.在早期固定图形流水线模式下,随着图形计算需求复杂性的提高,处于图形流水线中的顶点处理器、几何处理器、像素与子素处理器等可编程性得到增强[3],不仅使处理器摆脱固定流水线的禁锢成为可能,而且逐步表现出通用计算的能力.为解决GPU片内负载均衡问题,统一渲染处理器(ShaderProcessor)取代各种可编程部件,这就出现了通用计算GPU的雏形[5].而流处理器在图形处理器中的应用奠定GPU通用计算的基础.流处理器是在流计算模型上充分考虑并发和通信的计算体系.对计算单元精简动态控制设计,如去除分支预测、乱序控制逻辑、内存预取等功能,通过降低硬件复杂性达到提高运算单元密度的目的;为维护数据局部性和并发性,流处理器有多层存储体系,一般分片上本地存储器、流寄存器文件和片外存储器3部分.流处理器的控制部分即流控制器,负责指令发射、流数据装载和与主机通信;外部存储控制器负责片外存储器与外部DRAM的数据交换.各种流处理器的主要差别是算术计算单元的类型和数量、ALU集群数量以及各级存储器的容量与带宽.2.2学院派流处理器Imagine[6-7]是斯坦福大学的研究项目,并且在Intel公司和德州仪器公司的支持下实现第一款流处理器芯片.Imagine中有8个ALUClusters,而每个计算簇中计算单元不对等,有6个浮点计算单元,Page3其中有3个加法器、2个乘法器.为解决片外通信昂贵、芯片上通信面积大于计算面积的问题,Imagine通过3级存储结构减少片外通信,以充分提高片内运算能力.Merrimac项目中基于流处理器构建超级计算来提高科学计算性能,在Merrimac中通过网络连接16个流处理器,每个流处理器中有16个ALUClusters,计算簇内设计对等计算单元,有利于负载调度,适合计算密集型的科学计算任务[8].国防科学技术大学张春元教授领导的MASA团队设计了可编程64位流处理器MASA[9-10],MASA继承了流体系的基本特征和层次带宽等特性,改进存储模型和计算功能单元.采用共享存储空间和非阻塞传输技术来解决中间数据导致的标量核与访存带宽压力提高的问题;计算单元中增加特定操作,并保证对等性,进一步提高核运算的性能.国防科学技术大学杨学军院士[1]领导开发的飞腾64位流处理器,在流体系结构基础上设计针对科学计算的加速方案,提供消息传递和流通信两种通信方式,并且利用自主研发的网络接口可将大量飞腾处理器连接起来构成超级计算机.超级计算机“天河一号”中应用大量飞腾处理器,标志着国产流处理器逐渐成熟并走向工业应用道路.2.3工业图形处理器在工业界中Nvidia公司的GPU是应用最广泛的图形处理器,体系结构经历GT80、GT200和Fermi三代发展.2006年,GT80首次将顶点与像素处理器统一为渲染处理器,统一的目的是为解决片内负载不均衡的问题;渲染处理器把矢量处理器拆分为标量处理器,因此渲染处理器本质是标量线程处理器,并采用单指令多线程执行模式.GT200的设计中继续增加片上计算面积,扩充片内SM和SM内线程数量.片上计算单元数量增加后出现两个问题,(1)对线程和指令调度及仲裁机制提出更高要求,GT200通过指令双发射机制来满足大量计算单元需要的指令和数据;(2)芯片功耗损失增大.从GT200开始考虑向用户提供动态频率与电压机制来优化能耗.Fermi体系如图1(a)所示,增加计算簇内计算单元的密度,流多处理器SM中由8个流处理器增加到32个,并且流处理器与CUDA紧密结合,改为CUDA核心,体现向通用计算发展的决心;提高计算簇之间任务并行性,允许同时执行多个核函数;提高单个计算簇内的线程并行度,在单个SM中提供双Warps调度器和分配单元;在Fermi体系中为降低片外通信代价引入真正的缓存体系,为整个芯片中各个计算簇提供一个768KB的共享二级缓存;为解决各种应用程序对缓存的不同需求,增加片内存储器的灵活配置机制.即单个SM中的64KB片内缓存可配置为16KB共享内存和48KB一级缓存或者48KB共享内存和16KB一级缓存[11].总之,Nvidia在改进存储模型的数据局部性、提高线程吞吐量的调度机制及减少片外通信代价等方面取得较好成绩.Nvidia的主要市场对手是AMD,以Radeon系列产品为例介绍AMD芯片.AMDGPU属于矢量处理器[12],5个流核心(StreamCore)组成1个超长指令体系的线程处理器,其中4个普通流核心负责矢量并行操作,1个特殊流核心(T-Stream)处理特定数学计算.多个线程处理器组成一个单指令多数据处理器阵列及计算单元(ComputeUnit).存储系统由全局显存、一、二级缓存、共享存储器和寄存器构成,每个计算单元中有一级缓存和共享存储器,多个计算单元共享一个二级缓存[13].AMD与Nvidia明显区别是流处理器体系不同,AMD以计算单元规模来提高性能,采用单指令多数据结构,趋向指令并行方向;Nvidia则在有限计算单元前提下优化并行体系来提高性能,采用多指令多数据,偏向线程并行度方向,因此NvidiaGPU更高效地执行分支程序,通用计算能力较强.其次,Nvidia缓存及管理机制比较强大,如图1(a)所示.最小线程执行粒度不同,执行线程粒度越小并行度越高,掩盖延迟的能力越强.Nvidia执行线程粒度是16个线程,而AMD是64个线程.未来Nvidia继续向通用计算发展,而AMD则用自身技术优势关注片内异构核融合设计.AMD在2011年7月上市的代号为Fusion的APU,在解决CPU与GPU片间通信瓶颈和处理标Page4量和矢量负载都表现不凡.但是,现在APU中x86与SIMD核心之间的存储区域未能实现无缝集成,成为下一代Fusion研究重点[13].Intel公司也尝试进入图形处理器领域,提供代号Larrabee的高端GPU体系设计方案,然而最终并未投片生产.Larrabee中标量和矢量计算单元都有寄存器,标量单元是顺序执行、双发射CPU核心,支持预取、任务与指令调度和缓存一致性策略等功能;矢量处理器单元负责处理各种从简单到矢量计算的指令;Larrabee具有完整一、二级缓存及管理机制,因为基于x86结构的设计,所以更具有通用计算能力[14].虽然Larrabee未能成功以GPU形式出现,但是Intel宣布已经集成到“高性能协处理器家族”中,SC2011大会后发布了KnightCorner50核的原型芯片①.总之,通过GPU芯片体系发展可以看出如下趋势:增加计算资源密度,提高存储体系性能和功能,增强通信能力和可靠性,降低功耗等.因此,下一节重点讨论相关技术.3GPGPU关键技术及进展3.1GPU编程模型3.1.1片内编程模型从片内与片间两个角度分析编程模型的发展.对于早期GPU片内,在数据并行编程模型基础上,程序员将各种算法映射到绘图流水线中,以此挖掘GPU硬件并行性.随着可编程部件功能增强,出现了以顶点、子素处理器等硬件为基础的渲染模型(ShaderModel).为解决片内负载均衡问题,统一各种可编程部件后出现了统一渲染模型(UnitedShaderModel)[5].CTM[15]对GPU片内编程模式有重要影响,抽象化GPU硬件来降低对底层的依赖,把开发控制策略返还编程人员.此后,由于更适合通用计算的流处理器SP(StreamProgramming)[2]的出现,流式编程模型(StreamProgrammingModel)逐渐成熟,并在Brook和CUDA编程语言中应用[16-17].流编程模型的优点是捕捉应用程序的两种局部性:(1)核内局部性.在核的执行过程中所有引用的数据都集中在核内;(2)生产消费关系局部性.由于数据在各个核之间流动,有效组织核之间的逻辑顺序能把生产消费关系控制在局部范围内.3.1.2片间编程模型GPU长期以协处理或加速器的方式存在,因此GPU与CPU之间的耦合关系对片间编程模型有影响,如NvidiaGPU与CPU之间是松耦合关系,而Larrabee与Fusion是紧耦合关系.松耦合体系侧重向用户提供灵活的编程模型,如OpenCL[18]存在数据并行模型、任务并行模型和混合编程模型,根据不同应用程序的性能特点灵活选择适当编程模型;CUDA也针对片间松耦合体系,基本编程模型是数据并行模型[19],而且是细粒度数据并行.此外CUDA中提供流的机制,用户运用多流可以达到任务并行执行,因此在某种程度上CUDA给出一种任务并行编程的途径.在松耦合体系中存在一个问题,由于CPU即是计算管理者又是执行者,因此CPU未被充分利用.而片间紧耦合体系有效解决了该问题,平衡编程模型能在GPU与CPU之间合理分配计算负载,充分利用CPU计算资源;实现以线程为调度单位的细粒度数据和任务并行计算.3.1.3编程模型发展方向未来编程模型预计向两个方向发展.为降低并行程序设计复杂性和提高非专业开发人员的效率,GPU片内要出现元编程模式.元程序设计是编写生成代码的程序.在元编程模式中自动生成任务划分、通信等代码,编程人员更专注于特定领域的算法设计.GPU片间通信能力增强后,片间编程模型必然向分布式方向演化.如CUDASA中的分布式编程模式能有效解决负载均衡和全局存储管理、通信的问题,降低GPU集群应用程序的开发难度[20].3.2GPU开发语言3.2.1GPU开发语言GPU开发语言从专门的着色语言、面向流计算模型的流语言,发展到现在的面向通用计算领域的编程语言.以下分3个方面介绍:GPU编程语言起源着色语言(ShadingLan-guage),如GLSL、HLSL、Cg[5].着色语言是独立图形处理硬件的高级编程语言,为了开发者灵活、方便控制并行图形渲染.存在两个局限性:(1)对底层图形库依赖性强,可移植性差;(2)开发难度大,不仅要掌握并行开发技术,而且要了解硬件结构和图形库细节.针对流编程模型出现一些流编程语言,如StreamIt、StreamC/KernelC、Brook/BrookTran.其中StreamIt[21]是麻省理工学院为RAW流处理器开发、以Java为基本语法的流处理语言,允许在状①http://www.cnbeta.com/articles/162339.htmPage5态空间表示基础上进行一系列变换.StreamC/KernelC[22]是斯坦福大学为Imagine设计的流编程语言,分为流与核两种指令类型,在编译阶段能够优化流调度、访存调度和通信调度.Brook[23]是在C/Fortran基础上扩展的另一种流编程语言,提供实时运行库,允许用户创建和管理流.流编程语言为GPU通用编程语言的发展奠定基础.随着非图形计算应用的增多,各种领域的用户对通用编程语言的需求增大.两大GPU厂商为扩大各自硬件产品的生态圈,分别推出不同通用计算语言,Nvidia的CUDA和AMD的Brook+.CUDA以C语言为基础,由于具备了以下两个优点,从而迅速增加了用户数量:(1)封装图形编程底层转化算法;(2)降低编程难度,用户无需考虑计算模型和操作资源的限制,利于集中设计特定应用领域算法.AMD的Brook+是从斯坦福大学的流编程语言Brook基础上扩展而来,提供流机制、各种数据对象和核并行计算方式,在AMD各产品系列中得到应用.学术界也有较好研究成果,清华大学Hou等人[24]提出BSGP通用编程语言,将BulkSynchronousProgramming(BSP)模型应用到流处理器编程中,增加若干关键词Spawn、Barrier、Require、Fork、Kill等,与CUDA相比不仅获得相似计算性能,而且降低了编程及维护的复杂性.3.2.2编程语言发展方向编程语言在向抽象化发展.因为CUDA与OpenCL用户仍然需要管理存储器、建立、调度核运算,所以逐渐有研究者关注提高抽象设计能力的编程语言,如PyCUDA[25]、JCUDA[26]、hiCUDA[27]等,将现有并行编程语言与灵活的脚本语言相结合,创建更抽象的编程语言.PyCUDA通过Python脚本编程实时生成CUDA代码,以提高代码抽象性的方法来降低编程复杂性和改善编码效率;JCUDA在Jini接口上建立Java与CUDA的接口,通过在Java应用程序中嵌入CUDA代码来加速关键算法.hiCUDA是基于编译标示符的高层语言,在现有计算和数据模型基础上允许用CUDA编程.Glift[28]提供灵活、高效的数据结构实现机制,通过添加库函数、元语言等方法开发软件,属于一种抽象编程语言.总之,目前抽象通用编程语言处在初级研究阶段,大多数研究成果仅实现简单数组计算,缺乏高级功能,还存在较大研究空间.GPU编程语言以提高开发效率和降低编程难度为目标,从面向GPU硬件的低级语言到通用编程语言,再向更抽象、快捷的脚本语言发展.3.3存储模型与性能分析现代GPU存储子系统由不同存储区域构成,主要分为片外和片内存储区两部分,如图2所示.片外存储区由通用存储器与常量存储器构成,通用存储器是早期纹理存储器演化而来,如Nvidia的全局显存;片内存储区包括缓存与用于局部数据共享和重用的快速存储器.片内存储器具有局部特性,在体系中属于一个多线程计算单元,即Nvidia流多处理器或AMDSIMD引擎.GPU存储子系统有以下特点:(1)GPU无主动发起访问片外存储器的能力,与CPU存在较高的通信和访存耦合度;(2)GPU片上缓存是不满足数据一致性的二维纹理Cache,而且容量小.因为GPU存储子系统性能决定计算性能,所以厂商一直致力于GPU存储性能的提高.增加片外存储控制器寻址范围,Nvidia的Fermi可访问6GB全局显存;增强片内缓存硬件功能,如Fermi的二级缓存体系和可灵活配置的一级缓存机制;本节主要对GPU存储模型分两方面讨论,GPU片上存储管理与分布式存储管理.3.3.1GPU片上存储管理存储性能一直是GPU计算性能的瓶颈,定量分析是一种常用的研究方法.因为厂商未公开片内存储器硬件机制,所以需要合理的黑盒模型来分析存储性能,如3C模型分析片内缓存行为[29].在建立量化分析模型时需要考虑应用程序的访存特性,根据不同访存特征对应用程序进行分类,针对不同类别来分析性能损失.由于应用程序访存模式的多样性,因此提高定量分析模型的普适性成为一个难点.建模过程中需要综合考虑,不能忽视任何一个区域对计算性能的影响,如片内共享存储器、共享存储器冲突、全局显存合并访存及分支指令的执行机制等对存储性能的影响[30].除定量分析之外,存储模型Page6还能实现特定功能,如实现高效、透明的片内外数据传输.根据存储体系的物理组织特点,建立树状层次模型,节点记录存储区的存取速度、容量等属性,并把GPU的核心抽象为工作线程,通过树中各层节点操作解决数据交换任务[31].由于GPU存储体系要面临海量线程并发操作,在各种应用中访存模式对性能影响敏感,因此有学者关注访存模式的研究,如分析聚集和散列操作中随机访问模式的性能[32],小缓存访问模式、流式顺序访问模式和无序随机访问模式对性能的影响[33].Jang等人[12]针对嵌套循环程序中基于数组的访问模式,建立严格数学模型,并通过数据转换方法正确映射到不同的存储区中,在AMD和Nvidia两种GPU上均获得较好优化效果.CUDA中为全局显存提供了一种合并访存模式,能够合并多线程的访存操作来降低访存延迟.然而在多线程环境中合并访存机制存在同步问题,Ha等人[34]分析证明合并访存具有良好同步性,由同步而引起的性能损失较小.通常GPU片上缓存及局部快速存储器成为应用程序性能优化的焦点,需要重点考虑数据访问局部性和重用性两种因素.GPU中数据访问侧重于空间局部性,在早期子素处理器和纹理缓存体系中Govindaraju用3C模型分析缓存行为,提高数据访问局部性[29];在聚集和散列操作中采用多轮计算的方法优化数据访问局部性,这是一种用计算资源降低访存延迟的均衡思想[32].对于数据局部性韩博引入一种度量方法,即访问密度.访问密度是在同一时间单位内数据单元被不同线程访问的次数[33],这种衡量数据访问局部性的方法对提高缓存性能具有重要意义.数据重用中的挑战是动态重用性,在编译前复杂算法中的数据存取模式是不确定的,如应用广泛的Sum-product算法[35].对于GPU中共享存储区综合考虑时间和空间局部性,设计缓存预取和替换算法来解决动态数据重用.在硬件方面由于流处理器数量的激增,必然出现复杂线程仲裁机构,需要更复杂的片上存储系统.Nvidia开始提供可编程共享存储器SharedMemory,使软件管理缓存成为可能.目前Fermi一、二级缓存的出现,必然使得通过优化缓存来提高计算性能成为以后的研究热点.对于GPU众核多线程体系,软件显式管理缓存尚存在一些难点:(1)对不规则存取模式的缓存优化,如光线跟踪算法;(2)缺少基本硬件缓存操作原语;(3)在GPU中缺少线程之间的细粒度同步机制,导致更新缓存状态效率较低.3.3.2分布式存储管理随着节点内多GPU和GPU集群的发展,GPU分布式存储管理成为一个重要研究方向.为用户提供透明管理的地址空间和数据一致性的维护是两个倍受关注的问题.统一地址空间的建立和维护一直是解决分布式透明化管理的主要思路.在多GPU环境中Moerschell等人[36]建立全局纹理存储空间,借鉴CPU对缓存状态的跟踪方法,通过目录记录分布式节点中存储器使用状态来维护数据一致性.针对Fermi体系CUDA4.0提出GPU统一寻址空间的概念,支持多个GPU及CPU统一调用全局显存和CPU内存的功能.分布式GPU存储系统本质上是异构存储体系,数据一致性维护和通信管理的复杂性大.文献[37]提出一个非对称分布式共享存储模型,CPU负责数据一致性维护和通信,GPU处于完全被动状态.并且向开发者提供基于CUDAAPI来降低分布式管理的复杂性及开发成本.分布式存储管理需要解决的另一个问题是提高可扩展性,通过降低存储管理的难度来达到推广多GPU和GPU集群应用的目的.CUDASA和Zippy都属于这类开发框架,特别是CUDA4.0对虚拟统一地址的解决方案都反映这类问题的重要性.现有研究成果存在一些不足,对于动态负载或不规则负载任务,因为未能有效解决动态数据传输问题,所以数据一致性维护尚且存在较大研究空间.3.4通信模型及分析GPGPU的通信指发生在GPU片内或片间控制信息和数据的交换过程,主要分垂直通信和水平通信.垂直通信是GPU片内存储体系中各种存储器之间的数据传输,由于GPU不能主动发起通信请求,具有与CPU之间存在控制耦合度高的限制.水平通信指节点内多GPU之间或GPU集群中节点之间的通信.因为网络设备与GPU存储器之间无直接通道,需要经过发送端和接受端的CPU内存及网络信道进行通信,所以水平通信控制复杂度高.3.4.1GPU垂直通信GPU垂直通信的研究集中于提高透明化通信和降低GPU通信依赖性两个方面.GPU物理存储体系的层次特性使抽象树成为解决垂直通信的直观方法.并行编程语言Sequoia中就提供单GPU存储体系的树形模型,在编程中允许用户有效地控制存储体系中各层次之间的数据移动和定位,实现垂直通信的透明化操作.树形存储模型易扩展到集群中,Page7完成集群节点内的垂直通信任务.将集群节点的存储体系抽象为树形结构,把垂直通信的操作转化为对树的操作,提高数据传递的抽象性和性能[38].研究者也在尝试寻找一种软件机制来解决垂直通信的控制耦合问题.通过操作系统提供的回调机制,在GPU核函数中设置回调函数的执行条件,当核运算满足触发条件时执行回调函数,并由其发起通信请求和数据交换工作[39].然而,在增强GPU垂直通信的独立性方面还存在难点,需要继续探索.3.4.2GPU水平通信水平通信中的关键问题是控制通信管理的复杂性.在多GPU或GPU集群应用开发中用户以显式方式控制水平通信,必然存在大量通信与计算逻辑代码紧密耦合的现象,不仅开发难度大,而且维护成本高.为解决该问题,研究人员关注通信接口和通信框架的研究及应用工作,出现cudaMPI[40]、DCGN[41]、CUDASA[20]、Zippy[42].以下围绕这些典型系统来讨论水平通信.通用性是水平通信框架和接口库的一个设计目标.分层策略是控制软件复杂性的主要方法,从接近通信底层的角度设计水平通信接口具有较好的通用性,cudaMPI就是以该方式实现的分布式共享存储结构的通信接口库[40].cudaMPI分别在CUDA和OpenGL基础上设计出类似MPI的两个通信接口库cudaMPI和glMPI,提供点对点和集合通信两种模式及异步传输功能.扩展性是另一个设计目标,通用性与扩展性相互促进,cudaMPI最靠近通信管理底层,不仅通用性好而且扩展性强.抽象性是又一个设计目标,抽象性与通用性相互制约.通信接口的抽象性可以将数据一致性和局部性维护等细节隐藏起来,允许在实现通信功能时不需要了解源节点、目的节点等细节信息,为用户提供一个简洁、抽象的接口集.在抽象性设计中Zippy和CUDASA表现较好,均采用全局数组策略GA(GlobalArray)来实现水平通信功能.CUDASA在CUDA基础上增加总线层和网络层,将应用层与底层通信隔离.在集群中选择一个物理节点来管理任务调度和通信,并建立一个分布式共享存储池,由各节点中的部分片外显存组成共享存储池,分布式共享池统一编址,水平通信被转化为针对全局数组的操作.该方式不仅能提高数据局部性,而且可实现节点间的透明通信.CUDASA的通信方式直观、简捷.缺点是由于采纳集中式管理,当集群规模增大后容易出现通信拥塞.Zippy与CUDASA不同之处在于分布式管理模式,有效避免集中式通信管理中的性能瓶颈.建立两种分层的局部和全局数组,局部数组位于各GPU纹理存储器中,分布在各节点的部分纹理存储器构成全局数组.Zippy提供一个虚拟全局存储空间,图3解释了Zippy透明通信过程.图3中有两个全局数组,Zippy有两种全局数组通信方式:(1)在一个全局数组内传递数据,例如GA1的Region1传送到Region2;(2)在两个全局数组之间传递数据,比如GA1的Region1传输到GA2的Region3.在Zippy基础上用户不需要关注水平通信的底层细节,只要显式控制全局数组中的数据传输就能准确地控制通信的内容和时机,因此Zippy有效地将数据局部性控制权转移到应用层.Zippy的缺点是无动态通信能力,数据一致性机制设计过于简单,造成性能损失.GPU水平通信的最大挑战是动态通信.由于GPU缺乏主动通信能力,GPU与CPU之间存在很高通信耦合度,当复杂算法产生动态数据时动态通信更难实现.为解决该问题,Stuart等人[41]设计DCGN通信接口,DCGN主要用多线程轮询的方法实现动态通信.在分布式环境中每个节点都有一个线程(CommThread)负责轮询通信状态,当节点1检索到发送请求,而节点2检索到接受请求后,数据从GPU传送到CPU主存中,然后经过网络发送到节点2的主存,最后节点2的通信进程将接收到的数据发送到GPU显存,如图4所示.DCGN通过轮询机制实现GPU和CPU之间解耦合,以损失计算性Page8能和降低通信稳定性为代价来实现动态通信.总之,水平通信的研究围绕3个目标:抽象性、通用性、扩展性,表1提供定性分析现有研究成果的性能对比.通用性对GPU高性能计算的发展起到重要支撑作用,cudaMPI、DCGN、CUDASA等框架的设计都体现出对通用性的需求.因为cudaMPI最靠近通信底层,所以该接口的通用性最好;另一方面,CUDASA提供最靠近用户的应用层接口,此外在CUDA基础上进行扩展,因此通用性较好;DCGN采用多线程轮询机制,实现机制比较复杂导致通用性最差.为降低计算和通信的耦合度而提高数据通信的抽象性,使各种应用领域的开发人员集中于算法设计.在抽象性方面CUDASA具有严格的分层结构,隐藏水平通信的细节,抽象性最好;DCGN以原语和接口方式实现复杂动态通信,因此抽象性较好;由于cudaMPI用户必须掌握各种通信细节,如自行维护数据一致性和局部性等,所以抽象性最差.在扩展性方面现有研究只是处于原型阶段,都没有出色表现.因为cudaMPI为底层支持,能向更抽象层扩展,所以其扩展性较好;DCGN实现复杂度高,其扩展性表现最差.总之,提高水平通信框架或接口的扩展性仍是一个开放性课题.性能分类好较好一般较差通用性cudaMPICUDASAZippyDCGN抽象性CUDASADCGNZippycudaMPI扩展性cudaMPICUDASAZippyDCGN3.5负载均衡提高数据处理的吞吐量是GPU设计的主要目标,尽量降低计算单元的复杂性,以此提高计算单元密度.为降低硬件设计复杂性,早期GPU负载均衡的功能被固化在硬件层.但是应用层仍然需要解决负载均衡,无效负载均衡策略导致GPU无法充分发挥计算能力[43],比如图形处理中的光线跟踪[44].随着GPU硬件发展,如复杂同步机制和原语操作的出现,负载均衡出现从硬件层向软件层过渡的趋势.此外,节点内多GPU系统和集群应用的推广,负载均衡必然成为未来GPU通用计算中的一个活跃课题.本节从负载均衡的方法和粒度两个方面讨论,方法分静态与动态负载均衡,任务调度的粒度分节点内细粒度与节点间粗粒度.3.5.1静态与动态负载均衡静态负载均衡法简单、易实现,最典型代表是静态任务列表方案.由于该方案与锁无关,同步成本低,计算性能较好,因此成为CUDA中默认调度方案[45].文献[46]在编译前提取代码中的性能特征,将任务分配转化为一个分类问题,用主成分法建立分类预测器,经过训练后完成实时任务的分配.静态法无法有效处理负载未知和动态负载的计算任务,需要动态负载均衡DLB(DynamicLoadBalance)机制来解决.随着散列Scatter操作和硬件原子操作的出现,在硬件方面已经达到软件实现动态负载均衡的要求.传统SMP系统的DLB方案可应用到GPU通用计算领域.动态均衡中最基本的数据结构是队列,在队列基础上有多种动态均衡策略.由同步锁机制分为阻塞式和非阻塞式任务队列,阻塞式任务队列需要共享访问,同步锁代价比较高,易造成计算性能损失;非阻塞任务队列是一种锁无关的并发访问共享任务队列的方案,利用GPU的CAS操作(CompareAndSwap)解决访问冲突,另外由懒惰更新机制降低冲突[47].从队列管理方式的角度分为集中式和分布式,由于存在严重锁竞争,集中式比分布式队列性能差.分布式队列出现各种扩展形式,任务窃取(TaskStealing)允许不同处理器之间通信和窃取邻居队列中的计算任务,提高处理器利用率.但是每个处理器要考虑最坏情况下的任务负载,导致各处理器必须维护较大任务队列,从而浪费存储资源.任务共享策略(TaskDonation)可解决该问题,任务共享适合缺乏处理器局部性的GPU体系,各处理器分别维护较小任务队列,当队列Pi任务溢出时采用RoundRobin方式选取任务接受处理器Pr,然后溢出的任务由Pr执行.电影实时渲染ReyesRen-dering中任务窃取与任务共享机制比集中式任务队列提高100倍的处理器利用率,在存储器利用率方面任务共享优于任务窃取机制[47].从队列层次上分为单层和多层队列.在解决光线跟踪计算中分别建立全局和局部双层任务队列,一个GPU对应全局任务队列而GPU中的每个执行单元对应一个局部队列,取得高于硬件调度的性能结果[48].3.5.2负载均衡粒度负载均衡的粒度分GPU内细粒度和GPU之间粗粒度.在细粒度负载均衡的层次上,Cederman等人[45]比较多种调度方案,以八叉树空间剖分算法验证得出任务窃取获得最优性能,静态任务队列性能较好;当处理单元增多时阻塞式任务队列性能最差,非阻塞式任务队列性能较好而扩展性较差.粗粒度调度分为GPU之间和GPU与CPU之间两种情况.光线追踪的负载均衡是解决计算任务在多个同Page9构GPU处理器之间的分配[48].而CPU与GPU异构处理器之间的任务调度是当前热门问题,如在计算机游戏设计领域[49].Gregg等人[50]研究在运行时刻动态调度GPU和CPU的方法,以基准程序的历史计算数据和计算任务的输入数据为动态决策基础,重点考虑两种异构处理器之间的竞争状态来设计动态调度算法.Jimenez等人[51]用预测方法解决任务分配,根据历史性能数据建立预测模型,从全局范围内调度计算资源.文献[46]将GPU和CPU中的任务分配问题转化为一个分类问题,构建层次预测器.第一层是简单预测器,决定任务是否完全在GPU或在CPU中执行,第二层是个复杂预测器,用支持向量机中的线性核预测分类,该方法性能优化率达到85%.Qilin[52]是在GPU异构计算体系中自适应分配计算任务的原型系统.实验发现不同算法中GPU与CPU之间粗粒度的任务调度对计算性能的影响大.故首先将任务分为在GPU和CPU中计算的两个子任务TG((1-β)N),Tc(βN),自动任务映射转为求最优划分点β的问题.通过离线学习构造拟合函数,用训练后的拟合函数实现运行时刻动态分配任务.3.6可靠性与容错计算在GPU计算过程中存在逻辑计算和访存错误等暂时故障,但是厂商提供较少的故障检测支持[53].通用计算的发展对GPU计算的可靠性和容错性提出更严格要求,特别是多GPU系统和集群应用的推广,然而可靠性分析及容错方案的研究处在起步阶段.下面分GPU片内与分布式两个方向讨论.3.6.1GPU可靠性与容错GPU片内可靠和容错研究集中在访存容错性和计算可靠性两个方面.在访存容错性研究中以软件方式实现显存ECC校验,在应用程序读写显存的位置添加错误检查函数,由错误检测函数完成ECC校验,软校验降低ECC校验的硬件成本[54].Haque通过统计方法量化访存故障并建立可靠性模型,在CUDA和OpenCL两个平台上设计开源存储测试程序MemtestG80[55].冗余计算是解决计算可靠性的基本思路,研究焦点是寻找性能与可靠性之间最优解的问题.R-Nave、R-Scatter和R-Thread是3种应用层的冗余计算策略[53].R-Nave是最简单的复算方式,一个任务运算两次并比较结果,计算吞吐量降为未增加冗余时的一半;后两种模型利用潜在的计算资源来减少冗余计算造成的性能损失.R-Scatter以指令级并行交错执行,而R-Thread以线程级并行交错执行原始和冗余程序.相对而言,硬件冗余复算具有透明度高和复算性能损失小的优点.文献[56]介绍一种可在未来GPU体系中应用的硬件复算方案.在充分考虑时间局部性的基础上,同时启用两个计算核心,一个核心负责计算,另一个负责复算.Ameya①分析了4种基本容错策略的性能损失:(1)Re-execution策略直观便捷,适合GPU容错计算,然而容错代价较大;(2)空间冗余策略可处理Re-execution不能解决的固定硬件故障检测问题,容错代价较低;(3)数据多样性策略是执行两次增加随机干扰的特定输入数据,通过比较输出结果来判断是否存在故障.因为要创建随机比较数据集,所以容错代价高于Re-execution;(4)算法容错策略是基于校验值的特定应用层容错方案,具有硬件依赖性小和容错代价低的优点,但是缺乏普适性.国内徐新海博士等人基于Gregerson的时间双模冗余法实现面向GPGPU的故障检测,将GPU从异构系统中分离出来后分析GPU瞬时故障的传播规律,提出Lazy的容错思想,把故障检测和恢复控制在不可靠数据范围内明显降低容错代价[57].3.6.2GPU分布式可靠性与容错计算目前多GPU系统和GPU集群的可靠性及容错研究处于初级阶段,主要集中在故障规律的分析、仿真与检测等方面.通常基于特定概率分布对大规模集群系统的故障规律进行分析,如指数分布等,然而这种假设在实际应用中不能准确反映真实情况.Haque等人[55]对蛋白质折叠网Folding@home中分布在世界各地的5000多GPU统计了故障规律,证明假设故障规律法存在严重缺陷,这项成果对GPU集群的可靠性研究具有重要价值.传统集群容错方法开始应用于GPU集群容错计算方面,国防科学技术大学较早涉及异构GPU集群的容错计算研究,用Brook语言实现了应用层检查点恢复原型系统HiAL-Ckpt[58].在HiAL-Ckpt中将GPU集群分为CPU控制的Master层和GPU计算的Slaver层,以在应用程序中以插入编译指示符的方式增加检测点功能,而CPU运行状态保存到计算机外存中,GPU运行状态和数据保存到主机内存里,实验表明这种分层的检查点技术容错①http://homepages.cae.wise.edu/ece753/papers/Paper_4.Page10代价较小.3.7低功耗及优化研究GPU芯片中计算资源密度大,在取得高计算性能的同时功耗也大于CPU,高功耗对稳定性与可靠性也有重要影响.在硬件方面,GPU厂商试图从芯片设计中降低功耗,例如Nvidia在GTX200增加动态频率门限功能,当GPU处于空闲状态时降低频率;在软件方面,支持比较简单的功耗优化,如驱动程序检测到GPU处于轻负载状态时降低计算频率.现在GPU低功耗研究处于起步阶段,大多数研究集中在能耗评测分析方面.以测量统计的方法研究GPU通用计算中的功耗,这种方法直观、准确性高.评测Nvidia两种芯片体系GTX200与GF100指令级功耗,比如通信、浮点算术和访存指令等[59];CUDA中不同计算程序能耗情况[60].此外,研究者关注程序特征对性能和功耗的影响[61],根据通信、计算、访存3个特征将程序分为3类,研究在各种频率下不同程序功耗情况[62];研究方法也在不断向精确方向改进,出现一些实验结合统计分析的功耗建模及优化研究成果.Ma等人[63]通过统计分析建立GPU功耗模型,预测目标GPU运行时功耗;利用历史数据建立的功耗模型,控制GPU中活动SM数量来获得性能与功耗的平衡[64].这些功耗预测模型为未来GPU功耗优化奠定基础,然而基于统计分析方法得出的预测模型,具有较大局限性,存在精确性低的缺点.为提高统计法功耗预测的精度和可行性,从PTX指令层统计动态指令数构建功耗预测模型,用GPU仿真器验证该方法能提高预测精度[65].将源代码切片作为功耗分析粒度并建立功耗预测模型,该方法使开发者迅速了解应用程序的能耗行为,成为一种在开发过程中快捷优化程序的功耗预测法[66].现在还出现深层次分析优化功耗的研究,国防科学技术大学的Wang等人提出合并多个独立核函数的方法实现节能,将核函数聚合转为为一个动态规划问题,该方法可以嵌入到编译器的能耗优化设计中[67].国防科学技术大学的林一松等人[68]从CUDA线程Warp调度角度,分析程序的计算与访存并行度,建立严格的功耗理论模型,在此模型上求解并行度与频率的关系,给出有效优化应用程序功耗的策略.最后仿真实验表明,动态调整计算、访存频率的功耗优化策略有效.综合考虑CPU-GPU的异构体系,使用AOV网分析CUDA计算任务中各子任务的依赖关系,降低非关键路径中子任务的冗余能耗,达到不影响程序执行性能前提下的节能目的[69].Gebhart等人[70]深入GPU芯片体系中尝试通过增加寄存器文件缓存和将线程调度改为双层调度的方式优化计算功耗,并取得较理想效果.4GPGPU的应用及进展随着GPGPU在软、硬件方面的发展,GPGPU在非图形计算领域的应用取得长足进展,特别是海量数据管理的GPU并行研究与应用.另外,GPU集群的应用逐渐开展.在这些应用领域中编程模式、存储性能优化、通信优化和负载均衡等关键技术都得到应用和发展.4.1GPU非图形计算应用4.1.1传统数据库应用由于海量数据处理的需求,在传统数据库管理中GPU得以推广应用.原语方法是一种GPU解决大数据集并行处理的基本方法,例如Harris实现数据处理原语库Cudpp①.香港科技大学的研究员利用GPU设计原语集合,如Map、Split、Scatter、Gather、Sort等,实现无索引嵌套循环Join、索引嵌套循环Join、归并Join和散列Join并行算法[71].但是原语法需要开发者熟悉GPU底层开发的知识,因而降低GPU并行数据库应用的开发效率.文献[72]尝试解决该问题,设计GPU计算的SQL接口,数据库开发人员无需掌握诸如CUDA之类的GPU编程语言,使用更抽象的SQL语言设计基于GPU的数据库应用程序.He等人[73]用提高空间和时间局部性的优化策略解决了大数据负载的内存墙问题,并以分层方法开发基于GPU的关系数据库管理系统GDB.4.1.2数据挖掘应用并行和分布式数据挖掘的研究吸引大量研究人员的目光,然而基于GPU的并行数据挖掘研究尚少.典型数据挖掘算法的GPU移植,比如K-Means聚类算法[74]、密度链聚类算法CUDA-DClust[75]等.其中最典型的研究成果是周国亮博士等人[76]实现的GPU并行方体算法GPU-Cubing.在联机分析处理OLAP和数据仓库领域,方体计算是一个核心问题,对于大数据量实时计算是一项非常有挑战的工作.算法采用自底向上、广度优先的划分策略,在计算过程中多个分区被分配给不同的线程块同步处①http://code.google.com/p/cudpp/Page11理,获得了至少两倍以上加速比.Fang等人[77]研制GPUMiner,利用CUDA实现K-Means聚类算法和Apriori频繁集挖掘算法;在DirectX基础上实现数据挖掘的可视化,允许用户与GPUMiner进行在线交互操作,提高数据分析的准确性和算法的收敛速度.GPU对数据挖掘应用算法的性能提升带来巨大机遇,以K-Means聚类算法为例,GPU算法分别比单核CPU提高200~400倍,比8核CPU算法提高6~12倍.此外,以下应用广泛的算法均能移植到GPU中优化性能,时间序列数据分析中的隐马尔科夫模型的GPU并行算法,贝叶斯网络分析算法及支持向量机的训练和分类算法等.4.1.3人工智能应用在人工智能领域中存在一些计算密度高的优化算法,当问题规模增大后比较适合移植到GPU集群中.李建明[78-79]等学者将并行遗传算法,并行蚁群算法移植到GPU中,以CUDA线程为调度基本单位,不仅保持抑制早熟的特性,而且提高了算法运行速度.文献[80]提出GPU并行鱼群算法,利用CUDA线程块模拟鱼群,由于忽视线程块之间通信,未能充分挖掘GPU并行计算能力.4.1.4复杂网络应用复杂网络的理论研究中有较多计算密度大的经典算法,比如复杂网络的社区发现算法和链路预测算法等.由于这类算法具有数据密集和计算密集的特点,适合数据并行划分的原则,因此GPGPU并行化复杂网络算法具有重要意义.在文献[81]中讨论随机图的染色算法在GPU中的详细设计方案,并与多核CPU中的多线程算法进行性能对比,证明GPU并行算法效果优于CPU多线程算法.4.2GPU集群应用在GPGPU的发展过程中,存储容量的限制始终是个关键问题.尽管硬件厂商不断提高显存容量,然而GPU局部存储器的扩充受到集成电路的制约,GPU显存扩容有限.为克服GPU通用计算应用中的硬件壁垒,多GPU系统和GPU集群的应用逐渐增多.Fan等人[82]最早提出GPU集群解决科学计算问题,此后科学计算领域中逐渐出现GPU集群计算的应用.对统计物理学中的IsingModel提出元Spin的概念,建立元Spin与具体Spin之间的映射,根据映射关系划分并行计算任务[83].文献[84]对比CELL、GPU、CPU3种集群的偏微分方程计算,实验表明GPU集群取得最优加速效果.GPU集群在有限元计算中将MG方法中不同网格分配到各个CPU和GPU计算单元中,在集群范围内合理分配计算负载获得理想加速比[85].在仿真计算中GPU集群处理流体表面流动的可视化问题[86],利用LBM格子波尔兹曼模型模拟流体流动和空中污染物的扩散仿真,在具有30个节点的GPU集群中取得了比同规模CPU集群高4.6倍的性能优势[87].另外,GPU集群在工业应用领域也逐渐开展起来,陈庆奎研究团队在上海市重点科技攻关项目支持下构建了基于GPU集群的3G视频质量分析系统[65-66].该系统由IP网络数据包分拣节点、H.264视频质量参数提取节点和基于GPU的视频质量分析节点,GPU集群能够完成实时海量视频流质量分析任务,当系统采用27个节点构成9条视频分析流水线时可完成1万条视频流的质量分析,满足电信企业3G骨干网上的视频质量分析需求.5结束语图形处理器通用计算经过多年的发展,未来仍将继续保持高速发展.GPGPU的关键技术研究中存在如下挑战:(1)由于GPU硬件标准很难统一,因此导致编程模型中难以合理分配异构计算资源.而对硬件体系不敏感、高度抽象的通用编程语言一直会存在调试困难的挑战;(2)各种领域中的软件和算法向GPU移植的难题始终限制GPGPU发展,设计高可靠性和复用性的公共并行算法库是一种有效的解决途径;(3)有效地解决分布式共享存储模型中的数据一致性问题及造成的性能损失;(4)设计优化控制GPU缓存的普适性算法,对提高各类算法性能有重要价值;(5)GPU集群节点之间动态通信尚未完全解决,然而大部分算法需要节点之间动态交换数据.虽然CUDASA与Zippy等系统实现节点之间的透明通信,但是节点之间只能进行静态通信.尽管DCGN对动态通信提出解决方案,由动态通信引起的数据一致性问题并未有效解决;(6)对于大量实时计算任务,GPU集群的稳态可用度必须大于0.9999.由于GPU与CPU的编程模式和存储模型都不同,因此传统可靠性解决方案不适合GPU集群.为GPU集群建立合适的可靠性模型,在满足可靠性约束下优化容错代价成为一项长期研究课题;(7)GPU集群的能耗优化问题,以及考虑性能、可靠性等约束的能耗优化;(8)CPU与Page12GPU协同计算问题.在CUDASA中开始关注GPU与CPU协同计算问题,将大规模计算任务分配给分布式环境中的CPU与GPU节点,实现协同计算中的负载均衡.但是协同计算中的很多问题尚未解决,比如对于集群中异构节点的同步处理,尚未出现保证计算性能和稳定性的同步策略.另外,随着“天河一号”的出现,大规模集群计算系统中CPU和GPU协同计算存在较多挑战,国防科学技术大学杨灿群研究员总结在天河设计中存在的主要问题:自适应任务均衡模型,优化垂直通信的软件流水机制,解决GPU片间访存冲突的流式数据读取策略,减少进程与线程迁移开销的亲和调度,动态性能测量及确定大规模系统的故障定位问题①.总之,对于大规模GPU与CPU协同计算问题,将会在通用编程模型及语言、分布式存储模型优化、水平通信优化、可靠性及容错、低功耗计算、集群中间件等方面取得新的进展和突破.GPU通用计算的应用领域尚需拓展,复杂网络研究中社会网络、生物蛋白网络等均存在数据量大、计算密度大的重要算法,因此对复杂网络并行算法的研究是个崭新的交叉研究方向.致谢感谢匿名审稿人和编辑提出的宝贵意见!
