Page1基于数据差异的连续数据保护恢复算法王超李战怀张小芳侯利曼(西北工业大学计算机学院西安710072)摘要连续数据保护系统在进行数据恢复时,首要任务是从历史时刻中快速识别出可恢复时刻点,总恢复时间通常与恢复时刻点识别过程中所检测的历史数据版本成正比.然而,基本数据恢复方法的恢复效率低,无法适应现代存储系统对可用性和可靠性的要求,恢复时间和数据损失之间的矛盾日益突出.通过对邻近算法的改进和完善,提出了一种支持多间隙复杂情况的恢复算法———RM-CBDD.RM-CBDD通过分析并消除恢复起止时刻之间两种类型的差异数据实现恢复.实验结果表明,在多间隙复杂情况下,RM-CBDD算法的恢复效率明显优于基本方法和WDRS算法,有效降低了二分探查最佳恢复时刻点的时间开销.关键词连续数据保护;数据恢复;数据差异;块数据;可用性1引言面对病毒入侵、人为破坏[1]、硬件失效以及自然Page2保护方法的重要指标.RPO描述了灾难后可接受的数据丢失量,RTO描述了灾难后恢复数据所需要的时间.根据RPO的不同,将目前常见的块级数据保护技术分为定期备份(PeriodicalBackup)[4-6]、快照(Snapshot)[7]和连续数据保护(ContinuousDataProtection,CDP)三类.连续数据保护(ContinuousDataProtection,CDP)技术[3,8],可以在不影响正常数据业务的前提下,通过实时跟踪并记录所有数据的更新,能够在出现数据损坏或丢失等问题时,将数据恢复到任意历史时刻.传统的块级连续数据保护技术,其实现机制通常是按照时间顺序、以单一写请求为单位保存每个数据块的所有更新,在数据恢复时则只需替换所有时间戳在恢复时刻点之前的数据块即可.与前两种技术相比,块级连续数据保护技术可以在数据失效后及时恢复至以单一写请求为单位的任意历史时刻,数据丢失量最小.在数据失效后,必须在识别出一个可恢复时刻点之后,才能开始执行数据恢复操作.虽然连续数据保护技术可以将数据恢复至任意历史时刻,但却不能帮助可恢复目标时刻点的识别,连续的历史时刻反而使得恢复目标时刻点的识别变得更加复杂.为了快速识别可恢复时刻点,通常采用二分探查的方式,在初始时刻和发生数据失效的时刻之间进行探查,探查过程中将数据依次恢复至每个探查的时刻点,并对数据的完整性进行检测,然后根据检测结果判断下一次二分探查的方向.由于探查过程需要执行多次数据恢复,并进行完整性检测,因此,相比执行数据恢复操作所花费的时间,识别恢复目标时刻点则要耗费更多的时间.可见,降低总恢复时间的关键在于降低恢复时刻点识别所需要的时间.系统管理员在选择恢复目标时刻点时,需要在总恢复时间和丢失数据量之间进行权衡,探查的时刻点越多,通常意味着消耗更多的时间,但却可以识别出丢失数据更少的恢复时刻点.随着现代存储系统对可用性和可靠性要求的不断提高,恢复时间和数据损失之间的矛盾日益突出.然而,块级连续数据保护机制的基本恢复方法,在恢复数据时简单地基于初始备份依次Redo所有时间戳在恢复目标时刻点之前的历史数据,恢复效率极低,已经无法满足现代存储系统的需求.由于连续数据保护系统需要保存各个数据块的所有更新,历史数据需要占用大量的存储空间,限制了块级连续数据保护技术的应用,所以,目前大部分的研究[8-11]集中于解决传统块级连续数据保护机制的高存储开销问题.然而,对于如何快速地识别出数据损坏之前的最后一个可用数据镜像对应的时刻点,即最优恢复时刻点,相关的研究却非常有限.SWEEPER[12]根据对多种系统事件的监控分析,有选择地针对特定时刻点所对应的数据镜像进行完整性检测,在不降低对RPO要求的前提下,减少了所需检测的数据镜像个数,从而加快恢复时刻点的识别,最终达到降低恢复时间的目的.与SWEEPER通过减少探查次数的思路不同,提高数据恢复效率,降低每一次探查中恢复数据的时间开销,则是解决恢复时刻点识别问题的另一途径.快照技术中的相关研究结合数据的分布特征提高快照的检索效率.如THVFS[13]利用目录、文件版本之间的相关性提高快照检索效率;Skippy[14]利用内存快照HotData的特性,提出了一种分层次的索引结构,提高长期快照的检索效率;结合数据分布特征和检索模式的分层二维索引结构HCSIM[15]能够提高高频快照索引的存储效率和检索效率.尽管这类技术可以提高快照检索效率,进而提高恢复速度,但却都是基于快照技术的相关研究,并不适合在数据块级的连续数据保护系统中使用.WDRS[16]对块级连续数据保护系统的恢复过程进行了优化,通过位表的方式,保证了在执行数据恢复的过程中,数据卷中的每一个逻辑块均被恢复一次.因此,WDRS的恢复时间与数据卷的大小关系密切,而恢复起止时刻的位置对其的影响并不明显.WDRS恢复效率稳定,且优于基本恢复方法,但仍旧无法适应恢复时刻点识别的要求.针对可恢复时刻点的快速识别问题,侯利曼等人[17]提出了“基于数据差异的CDP邻近时间点恢复”算法,简称“临近算法”.临近算法通过分析两相邻时刻的差异数据,采用剔除多余数据、写入缺少数据的方法,消除差异数据,对恢复起止时刻相距较近的情况,数据恢复效率极高.在通过二分探查的方式识别恢复时刻点时,随着二分探查次数的增加,相邻两次探查时刻点之间的时间间隔呈指数次幂减少,差异数据量随之骤减,临近算法大幅度提高了单次探查的效率,进而加速恢复时刻点的识别.然而,临近算法仅根据时间间隔较小的两个临近时刻点之间的关系,在最多存在一个间隙的情况下,对可能出现的5种简单情况作了分析,然后分别对这5种情况给出了恢复方法.在连续数据保护系Page3统的实际运行过程中,出现多次无规律数据恢复的情况是很常见的,即多间隙复杂情况,这时,仅支持最多一个间隙的5种简单情况的临近算法就无法适用.鉴于临近算法仅支持最多一个间隙的简单情况,而无法在实际环境中应用的问题,本文对临近算法进行了改进与完善,提出了一种支持多间隙复杂情况的恢复算法———RM-CBDD,并进行了详细的实验验证及分析,本文的主要贡献如下:(1)通过形式化建模,对临近算法给出的5种时间点划分进行了详细的分析及证明.(2)以消除恢复起止时刻之间差异数据的思想为指导,对临近算法进行了完善及改进,提出了支持多间隙复杂情况的RM-CBDD算法,通过形式化的方式给出了RM-CBDD的数据记录及数据恢复过程,并对算法的复杂度进行了比较分析.(3)设计并实现了同时支持基本方法、临近算法、WDRS和RM-CBDD4种恢复算法的原型系统,通过重放Trace数据的方式,对4种算法的恢复效率进行了比较,分析了间隙个数对恢复效率的影响.最后,通过模拟二分探查寻找可恢复时刻点的方式,比较了除临近算法外的3种恢复方法在恢复时刻点识别中的效率.2基本模型2.1基本模型对于由相同大小且相互独立的数据块组成的数据存储,每个数据块都包含相同大小的确定数据,并且由逻辑块地址(LogicalBlockAddress,LBA)唯一识别.假设A是该数据存储中所有逻辑块地址的全集,D表示数据块中所有可能的数据的集合.定义1.设St为集合A和集合D上的二元关系,是由形式为〈a,d〉的二元序偶所组成的集合.对于确定的时刻t,逻辑块地址a唯一标识了数据块以及保存在该数据块上的确定数据d,因此二元关系St是一个函数,表示数据存储在时刻t的镜像.用St(a)表示逻辑块地址a所标识的数据块在时刻t的数据镜像.定义2.写请求集合R是由形式为〈t,a,d〉的三元序偶组成的集合,记录了某一时间区间内,由逻辑块地址和时间戳标识的数据更新的集合.R(t1,t2]表示时间区间(t1,t2]内数据存储系统所产生的所有写请求的集合.定义3.逻辑块地址集合B记录了某一时间区间内被更新的所有逻辑块地址,用B(t1,t2]表示时间区间(t1,t2]内被更新的逻辑块地址集合,即对写请求集合R(t1,t2]在逻辑块地址属性上进行投影运算B(t1,t2]=B(R(t1,t2])=∏LBA在某一时间区间内,数据存储可能对同一数据块提交多个写请求,但只有最后提交的写请求的结果会反映到最终时刻的镜像中,由这些有效写请求所更新的逻辑块地址及数据所构成的形式为〈a,d〉的二元组所组成的集合,即为该时间区间上的增量.数据存储在时间区间(t1,t2]的增量,可以通过对写请求集合R(t1,t2]的运算得到.首先,对于B(R(t1,t2])中的每一个逻辑块地址,分别从集合R(t1,t2]中选择出时间戳最大的写请求,然后将选出的写请求所组成的集合在逻辑块地址属性和数据属性上执行投影运算,即得到写请求集合R(t1,t2]在时间区间(t1,t2]的增量.定义4.定义数据存储在时间区间(t1,t2]的增量I(t1,t2]=ΔS=St2-St1,即I(t1,t2]=I(R(t1,t2])定义5.对于逻辑块地址全集A的子集A,定义/运算为St/A=St∩(A×D),AA,特殊的,当A=B(t1,t2]时,St2/B(t1,t2]=I(t1,t2].为了叙述方便,使公式更加清晰,对于符号S、B、R和I以及任意时刻Tx和任意时间区间(Ty,Tz],在下文中采用简写Sx来表示STx;采用简写B(y,z]、R(y,z]和I(y,z]来分别表示B(Ty,Tz]、R(Ty,Tz]和I(Ty,Tz].2.2临近算法的形式化分析及证明连续数据保护系统在恢复数据时,通常需要通过完整性检测工具对历史时刻的数据镜像进行多次检测,以便确定可恢复时刻点.由于每次检测都需要将数据恢复至待检测时刻,多次数据恢复需要耗费大量的时间.然而,相邻两次探查时刻点之间通常相隔较小,数据差异并不大,通过对差异数据进行分析及处理,可以有效提高恢复效率.定义6.任意一次恢复操作之后,如果有写请求产生,则此次恢复的起始时刻与目标时刻之间的写请求就构成了一个间隙.由起止时刻为时刻点,x和y的恢复x→y所形成的间隙称为“间隙x→y”.如图1(b)所示,连续数据保护系统在T2时刻Page4执行了目标时刻为T1恢复,即T2→T1,并在此次恢复完成之后继续提交写请求,则时间区间[T1,T2-]之间的写请求构成了一个间隙,即间隙T2→T1.对于恢复路径中包含时刻点T2+的所有时刻点,即时间区间[T2+,T3],在恢复时都需要剔除该间隙内的无效写请求.侯利曼等人提出的“临近算法”,根据时间间隔较小的两个临近时刻点之间的关系,在不多于一个间隙的情况下,对可能出现的5种简单情况作了分析,基于这5种特殊情况的临近算法可以大幅度提高连续数据保护系统的恢复效率.下面对“临近算法”的5种情况分别进行了形式化分析.(1)无间隙后临近时间点恢复对于无间隙后临近时间点恢复T1→Ta,如图1(a)所示,当前数据卷状态所对应的时刻为T1,目标时刻为Ta,T1和Ta之间不存在间隙并且Ta>T1,则恢复T1→Ta可按照式(1)进行.证明是简单的,式(1)用时间点T1到Ta的增量替换该时间段内发生改变的数据块中的数据,这与典型的Redo操作的恢复结果一致.(2)无间隙前临近时间点恢复对于无间隙前临近时间点恢复T1→Tb,如图1(a)所示,当前数据卷状态所对应的时刻为T1,目标时刻为Tb,T1和Tb之间不存在间隙并且Tb<T1.此时首先需要将时间区间(Tb,T1]内被更新的数据剔除,然后将被剔除数据块在Tb时刻的数据镜像写入数据卷.恢复T1→Tb可按照式(2)进行.Sb=(S1-S1/B(b,1])∪(I(0,b]/B(b,1])∪证明.通过关系代数可以容易的证明.T1时刻数据卷的状态S1,可以用初始时刻状态(S1-S1/B(b,1])=(S0-S0/S(0,1])∪(I(0,b]/(B(0,b]-B(b,1]))(3)S0表示为将式(3)代入式(2)可得到Sb=(S0-S0/S(0,1])∪(I(0,b]/(B(0,b]-B(b,1]))∪(I(0,b]/B(b,1])∪(S0/(B(b,1]-B(0,b]))(4)对于式(4)中的第一部分和最后一部分,第二部分和第三部分,分别结合计算后可得出这与采用Redo方法的恢复结果一致.证毕.(3)有间隙后(间隙内)临近时间点恢复对于间隙内临近时间点恢复T3→Tc,如图1(b)所示,当前数据卷状态所对应的时刻为T3,目标时刻点为时间区间(T2+,T3]上的Tc.首先需要将时间区间(Tc,T3]内被更新的数据剔除,然后将被剔除的数据块在Tc时刻的数据镜像写入数据卷.恢复T3→Tc可按照式(6)进行.Sc=(S3-S3/B(c,3])∪(I(2+,c]/B(c,3])∪对于恢复T3→Tc,与无间隙前临近时间点恢复类似,在写入被剔除的数据块镜像时,只需要先从间隙前后的两段时间区间((T2+,Tc]和(T0,T1])内构建被剔除的数据块在Tc时刻的镜像.证明过程与无间隙前临近时间点恢复类似.(4)有间隙后(间隙外)临近时间点恢复对于间隙外临近时间点恢复T3→Td,如图1(b)所示,数据卷当前状态所对应的时刻点为T3,目标时刻点为时间区间(T1,T2-]上的Td.对于该恢复,不仅要将时间区间(T1,Td]内的数据写入数据卷,还要剔除时间区间(T2+,T3]内被更新的数据,如式(7)所示.Sd=(S3-S3/(B(2+,3]∪B(1,d]))∪I(1,d]∪可见,对于间隙外临近时间点恢复T3→Td,是写入数据与剔除数据并存的一种情况,根据无间隙前和无间隙后临近时间点恢复可以容易证明.(5)有间隙前临近时间点恢复对于间隙前临近时间点恢复T3→Te,如图1(b)所示,数据卷当前状态所对应的时刻点为T3,目标时刻点为时间区间(T0,T1]上的Te,根据式(8)进行恢复.对于恢复T3→Te,与无间隙前临近时间点恢Page5复类似,只需要剔除间隙前后的两段时间区间((T2+,T3]和(Te,T1])内被更新的数据块.因此,根据无间隙前临近时间点恢复可以容易的证明.3RM-CBDD算法在恢复数据时,RM-CBDD(RecoveryMethodforCDPBasedonDataDiscrepancies,RM-CBDD)以快照时刻为基础,首先找到恢复起止时刻的参照时刻点,并通过递归算法剔除由间隙引入的无效写请求之后,得到了参照时刻点与恢复起止时刻之间的两个写请求集合,然后通过对两写请求集合的分析,找出恢复起止时刻之间的差异数据,最后分别以不同的方法对“多数据”和“少数据”两种类型的差异数据进行处理,即可完成数据恢复.3.1数据记录3.1.1恢复映射序列虽然临近算法具有很高的恢复效率,然而,临近算法仅支持最多不超过一个间隙情况下的5种简单情况,无法对连续数据保护系统实际应用中常见的多间隙复杂情况进行处理.为了实现对多间隙复杂情况的恢复,RM-CBDD通过恢复映射序列,记录连续数据保护系统启动后的每一次恢复.图2(a)展示了一个存在3个间隙的恢复过程,从初始时刻T0到最终时刻T19,依次经历了T7→T3、T12→T1和T18→T9三次恢复,形成了3个间隙.由于这种无规律间隙的存在,通过恢复映射序列,如图2(b)所示,RM-CBDD记录了自初始时刻T0以来执行过的所有恢复的起止时刻,以便在数据恢复时作为参考.3.1.2插入快照以初始时刻为共同起点,当前时刻与目标时刻很有可能共同经历一段相同的写请求序列,也就是说,在剔除间隙引入的无效写请求之后,初始时刻与恢复起止时刻的两个写请求集合之间,可能存在交集.并且,随着初始时刻与恢复起止时刻之间间隔的增加,两写请求集合之间的交集也随之增大.RM-CBDD算法需要遍历历史数据,通过对初始时刻与恢复起止时刻之间的两写请求集合进行分析,以确定两时刻点之间的差异数据.而由于两写请求集合交集的存在,使得需要遍历的历史数据增加、对写请求进行分析的复杂度增大.为了降低获取差异数据的时间开销,RM-CBDD在历史数据中,按照一定的时间间隔,在日志链条中插入相应时刻的快照,并将恢复起止时刻与初始时刻之间共同经历的写请求序列中时刻最晚的快照时刻作为参照时刻点,以取代初始时刻.这样,以参照时刻为基准的两写请求集合的交集减小,并且最大不超过两快照时刻之间的时间间隔,达到降低获取差异数据时间开销的目的.并且,在对差异数据中的“多数据”进行处理时,参照时刻之前的数据可以直接通过参照时刻的快照获得,所需遍历的写请求集合也随之减小,对提高“多数据”的处理效率也起到了一定作用.如图2(a)所示,在日志链条中,每隔d个写请求就插入相应时刻的快照Si,如果被间隙打断,则从间隙之后重新对d进行计数.为了节约存储空间,在实现时,快照只保存指向相应写请求数据的指针.定义7.设SN和SN是由快照时刻点组成的集合,SN(t1,t2]记录了不存在间隙的时间区间(t1,t2]内的所有快照时刻点;在剔除时间区间(t3,t4]内由间隙引入的无效写请求后,SN(t3,t4]则记录了该时间区间内的所有快照时刻点.3.2构建写请求集合3.2.1确定参照时刻点参照时刻点,即当前时刻Tc和恢复目标时刻Tt与初始时刻T0之间的两个快照时刻点集合的交集中,时间戳最晚的时刻点,如果交集为空,则取初始时刻T0为参照时刻点.然而,考虑到多间隙的存在,快照时刻点集合需要剔除间隙中的无效快照时刻点,因此采用递归的方式,分别以当前时刻点Tc和恢复目标时刻点Tt为起点,向初始时刻T0方向遍历,根据恢复映射序列,将间隙逐一剔除,最终获得两个快照时刻点集合.具Page6体的递归算法如下:之间不存在间隙时:(1)基本情况.任意时刻点t与初始时刻点T0间至少存在一个间隙时:(2)递归步骤.当时刻点t与初始时刻点T0之设〈x→y〉为时间区间(T0,t]上t→0方向的第一个间隙,则在确定了恢复起止时刻与初始时刻之间的快照然后,对时刻点y继续过程(1)、(2).时刻点集合后,参照时刻点Tbase如式(9)所示.Tbase=σmax(timestamp)((SN(0,t]∩SN(0,c])∪{T0})(9)3.2.2构建写请求集合R(0,t]和R(base,t]分别表示从初始时刻T0和参考时刻Tbase到任意时刻t之间的所有写请求构成的集合;由于多间隙的存在,在恢复数据时,需要将时刻t的恢复路径中的所有间隙剔除,用R(0,t]和R(base,t]表示剔除间隙之后的写请求集合.为了获取恢复起止时刻之间的差异数据,以实现数据恢复的目标,需要先分别获得恢复起止时刻与参照时刻之间的写请求集合.此时仍然需要剔除由间隙所带来的无效写请求,因此同样采用递归的方式,分别对当前时刻点Tc和目标时刻点Tt,以恢复映射序列为参照,对历史数据进行遍历,将间隙逐一剔除,最终获得两时刻点分别基于参考时刻Tbase的写请求集合R(base,c]和R(base,t].具体的递归算法如下:(1)基本情况.任意时刻点t与参照时刻点Tbase之间不存在间隙时间至少存在一个间隙时:(2)递归步骤.当时刻点t与初始时刻点T0之设〈x→y〉为时间区间(Tbase,t]上t→0方向的第一个间隙,则然后,对时刻点y继续过程(1)、(2).以目标时刻点T10为例,如图2(b)所示,以当前时刻点T19为起始时刻的恢复T19→T10,其快照时刻集合分别为SN(0,19]={Tsn1,Tsn2,Tsn5}和SN(0,10]={Tsn1,Tsn2,Tsn5};恢复起止时刻T19和T10的参考时刻为Tbase=Tsn5;以参照时刻点Tsn5为基准的写请求集合为R(base,19]=R(sn5,9]∪R(18,19]和R(base,10]=R(sn5,10].3.3获取差异数据RM-CBDD算法的关键在于找出数据存储在当前时刻的镜像Sc与目标时刻的镜像St之间的差异数据.实际上,差异数据是由时间区间(Tbase,Tt]和(Tbase,Tc]内产生的不同写请求所形成的,差异数据分为“少数据”和“多数据”两种类型,图3展示了目标时刻和当前时刻写请求集合之间的关系.3.3.1少数据对于当前时刻镜像Sc比目标时刻镜像St少的数据,即St-Sc,用St-Sc集合中元素替换Sc集合中相应逻辑块地址所标识的元素即可;在执行数据恢复时,则只需要将这一部分增量直接提交至数据卷.如图3所示,“少数据”是由图中①这部分写请求所引起,这些写请求对逻辑块地址集合B(R(base,t]-R(base,c])所对应的逻辑块进行了更新,其增量为以图2(a)中的恢复T19→T10为例,其“少数据”的增量为I(R(base,10]-R(base,19])=I(R(9,10]).3.3.2多数据对于当前时刻镜像Sc比目标时刻镜像St多出的数据,即Sc-St,首先需要在历史数据及初始镜像中获取Sc-St中数据块在目标时刻t的镜像,然后从集合Sc中将Sc-St剔除,并将获取到的镜像添加至Sc中;执行数据恢复时,在获取到目标时刻Tt的镜像后,将这部分数据以增量的形式提交至数据卷即可.因为不同的写请求可能会对同一数据块进行更新,所以图3中①和③两部分写请求存在对相同逻辑块进行更新的可能,即存在(B(R(base,c]-R(base,t]))∩(B(R(base,t]-R(base,c]))≠的可能.注意到,在“少数据”中已经对图中③部分的逻辑块进行了处理,因此,在“多数据”中并不需要重复处理,即需要剔除的逻辑块地址集合简化为B(R(base,c]-R(base,t])-B(R(base,t]-R(base,c])(11)可见,与“少数据”不同,“多数据”的情况变得复杂,处理“多数据”的关键在于获取这部分数据所对Page7应数据块在恢复目标时刻Tt的镜像.简单的方法是按照时间先后顺序,直接将历史写请求集合R(base,t]提交至数据卷,即对集合R(base,t]中的写请求执行Redo操作,但由于存在对同一逻辑块地址的多次写操作,还可能重复处理“少数据”中的部分逻辑块,因此效率不高.为了获得最佳效率,RM-CBDD按照以下方法获取式(11)中逻辑块地址在恢复目标时刻Tt的镜像:(1)首先按照时间的逆序,即Tt→Tbase,遍历写请求集合R(base,t]∩R(base,c],得到部分逻辑块地址在时刻Tt的镜像:I(R(base,t]∩R(base,c])/(B(R(base,c]-R(base,t])-B(R(base,t]-R(base,c]))(12)(2)对于无法在R(base,t]∩R(base,c]中获取到的逻辑块,表明时间区间(Tbase,Tt]内并没有写请求对其进行更新.因此,参照时刻Tbase的数据镜像Sbase中相应逻辑块镜像即为剩余逻辑块地址在目标时刻Tt的镜像:Sbase/(B(R(base,c]-R(base,t])-B(R(base,t]-R(base,c])-B(R(base,t]∩R(base,c]))根据式(12)和式(13),RM-CBDD按照式(14)对“多数据”进行处理(I(R(base,t]∩R(base,c])/(B(R(base,c]-R(base,t])-B(R(base,t]-R(base,c])))(∪Sbase/(B(R(base,c]-R(base,t])-B(R(base,t]-R(base,c])-B(R(base,t]∩R(base,c])))(14)以图2(a)中的恢复T19→T10为例,RM-CBDD获取的“多数据”逻辑块地址在恢复目标时刻T10的镜像为(I(R(sn5,9])/(B(R(18,19])-B(R(9,10])))(Ssn5/(B(R(18,19])-B(R(9,10])-B(R(sn5,9]))).3.4RM-CBDD算法综合3.3节中对“少数据”和“多数据”的处理过程,RM-CBDD从当前时刻对应镜像Sc恢复到目标时刻镜像St的过程可以形式化地表示为St(=Sc-Sc/(B(R(base,c]-R(base,t])∪B(R(base,t]-R(base,c])))∪I(R(base,t]-R(base,c])(I(R(base,t]∩R(base,c])/(B(R(base,c]-R(base,t])-B(R(base,t]-R(base,c])))(Sbase/(B(R(base,c])-B(R(base,t])))(15)如图4所示,RM-CBDD的恢复过程如下:请求集合R(base,c]和R(base,t];类型的差异数据;①根据3.2节找到参照时刻点Tbase,并获得写②根据3.3节获取“少数据”和“多数据”两种③根据式(10),将“少数据”直接写入数据卷;④根据式(14),将获取到的“多数据”数据块在目标时刻Tt的镜像提交至数据卷.3.5恢复性能分析对于RM-CBDD算法,其恢复时间由读取元数据、根据元数据获取写请求集合及差异数据、读取历史数据并写入数据卷几部分组成.其中,需要读取的元数据包括恢复映射序列以及与恢复相关的时间区间的写请求描述符,这些与真正需要恢复的数据相比,数据量要小很多;另外,获取写请求集合以及差异数据的过程都是在内存中完成,与大量的磁盘I/O操作相比,是可以容忍的.因此,影响数据恢复时间的主要因素是读取历史卷以及写数据卷这两部分开销.类似的,基本方法和WDRS的恢复时间也主要由这两部分组成.对于基本方法、WDRS和RM-CBDD3种算法,表1列出了在恢复数据时需要读取和写入的数据量.恢复方法基本方法WDRSRM-CBDD|B(R(0,c]-R(0,t])∪B(R(0,t]-R(0,c])|RM-CBDD算法需要从历史数据读取和向数据卷写入的数据块个数,实际上就是当前时刻与恢复目标时刻的数据镜像之间存在差异的数据块个数;基本恢复方法在恢复数据时,首先需要将初始备份数据恢复至数据卷中,然后按照写请求所产生的时间先后顺序,将初始时刻与目标时刻之间的所有写请求依次提交至数据卷中.因此,基本恢复方法需要读写的数据块个数为数据卷的数据块个数与目标时刻之前所有写请求个数之和;WDRS方法则是按照时间的逆序,通过位表标记已处理的数据块,使得Page8数据卷中的所有数据块在恢复时均只被处理一次,因此所需要读写的数据块个数始终保持为数据卷的数据块个数.在最差的情况下,当前时刻点与恢复目标时刻点的间隔足够远,使得两时刻点的镜像中所有数据块都存在差异.这时,RM-CBDD算法所需要读写的数据块个数达到最大,即数据卷的数据块个数,但仍然不超过WDRS.通常情况下,恢复起止时刻之间的间隔并不会很远,尤其是在二分探查最佳恢复时刻点时,随着探查次数的不断增加,恢复起止时刻之间的间隔呈指数次幂减小,RM-CBDD算法的所需读写的数据块个数随之锐减.4实验评估4.1实验环境为了对RM-CBDD的恢复性能进行评估,我们在Windows平台下,利用磁盘过滤驱动技术,在逻辑卷层实现了支持基本方法、临近算法、WDRS和RM-CBDD4种恢复算法的块级连续数据保护原型系统.原型系统在逻辑卷层截获上层应用的写请求,并在历史数据卷中按照写请求产生的时间先后顺序进行记录.为了获得更加客观的实验数据,操作系统、日志卷、数据卷和历史卷都分别绑定不同的硬盘,实验在浪潮AS300N存储服务器上进行,具体的实验环境如表2所示.部件CPUIntelXeon5504×2内存DDRIII16GB硬盘操作系统Windows2003R2EnterpriseX64Edition实验采用存储性能理事会发布的Trace数据,该Trace数据采集自某大型金融机构的OLTP应用,数据块大小为512字节,平均每分钟提交写请求70000次左右,约34MB,每小时更新数据约2GB.根据以下实验的不同要求,按照不同的策略将Trace数据通过Replay的方式在数据卷上进行重放,然后分别采用基本恢复方法、WDRS和RM-CBDD3种算法,按照特定的恢复序列执行恢复操作,同时记录下每次恢复的恢复时间.4.2恢复效率比较为了评估RM-CBDD的恢复性能,首先构造实验环境.如图5所示,以初始时刻0min为起点,按照每次Replay60min数据,紧接着向前恢复30min的策略,重复5次后形成了4个间隙,间隙将整个时间轴划分为[240+,300]、[180+,240]、[120+,180]、[60+,120]和[0,60]5个时间区间,数据卷的最终状态所对应的时刻为270min.在该实验环境下,分别采用基本方法、临近算法、WDRS和RM-CBDD4种算法,以270min作为恢复的起始时刻,时间区间[240+,300]、[180+,240]、[120+,180]、[60+,120]、[0,60]上以5min为间隔的65个时刻点为目标时刻(以时间区间[240+,300]为例,有240+,245,250,…,300共计13个目标时刻),分别执行恢复操作(每次恢复操作前,都需要将数据卷恢复至时刻270min所对应的状态),记录下每次恢复操作的恢复时间,结果如图6所示,其中(a)、(b)、(c)、(d)、(e)依次对应于以上5个时间区间.特殊的,临近算法最多支持一个间隙,但其所关注的是恢复起始时刻和目标时刻之间差异数据内的间隙,虽然历史数据中共存在四个间隙,但由于恢复起始时刻270min,与时间区间[240+,300]和[180+,240]上的恢复目标时刻之间,最多只存在一个间隙,因此临近算法在这两个时间区间上依然适用.从结果中可以看出,不论哪一个时间区间,RM-CBDD算法的恢复效率最高,恢复时间均远低于基本恢复方法和WDRS.与RM-CBDD方法相比,WDRS的恢复时间最高为RM-CBDD的12.7倍,平均1.83倍;基本方法则差距更大,最差时恢复时间为RM-CBDD的66.4倍,平均6.5倍.RM-CBDD算法在恢复数据时,通过分析当前时刻与目标时刻数据卷之间的数据差异,分别对“少数据”和“多数据”两种类型的差异数据进行处理,消Page9图64种算法的恢复效率比较除恢复起止时刻之间的数据差异达到恢复数据的目的.因此,恢复起始时刻与目标时刻之间写请求的多少,直接关系到差异数据的多少,进而影响到恢复时间.从图6中可以看出,从[240+,300]至[0,60]的5个时间区间,恢复起始时刻与目标时刻之间的时间跨度依次增大,导致差异数据的增加,进而使得恢复时间增加.其中,时间区间[240+,300]中目标时刻点265min和275min与恢复起始时刻时间跨度最小,恢复时间也最小;而时间区间[0,60]中的目标时刻0min和60min则时间跨度最大,恢复时间也最大.特殊的,由于临近算法与RM-CBDD都是通过分析处理差异数据进行恢复,所以,在恢复目标时刻在时间区间[240+,300]和[180+,240]上时,两种方法的恢复时间几乎相同,曲线则近似重合.WDRS在恢复数据时,并不关注当前时刻数据卷的状态,通过位表标记已处理的数据块,使得数据卷中的所有数据块在恢复时均只处理一次.因此,在图6中可以看出,不论哪一时间区间,WDRS每次恢复的恢复时间都保持在40s上下,曲线近似水平.而对于基本方法,在恢复数据时不仅需要首先将初始镜像恢复至数据卷,然后还要按照先后顺序对初始时刻至目标时刻的写请求依次执行Redo操作,因此,目标时刻距初始时刻越远,需要执行Redo操作的写请求越多,恢复时间越长.从结果中可以看出,图6(a)中时间区间[240+,300]与初始时刻0min的时间跨度最大,恢复时间大多在200s以上;Page10而图6(e)中时间区间[0,60]与初始时刻0min的时间跨度最小,恢复时间最高未超过110s.特殊的,恢复至目标时刻0min,只需要恢复初始镜像即可,然而此时恢复起止时刻之间的间隔最大,RM-CBDD所需处理的差异数据量最多,所以此时基本方法的恢复效率略好于RM-CBDD.4.3间隙对恢复效率的影响为了分析间隙对恢复效率的影响,首先构造实验环境.如图7所示,以初始时刻0min为起点,首先Replay80min数据,再向前恢复40min;然后Replay40min数据,向前恢复20min,依次类推,Replay至155min时结束.为了排除数据分布对实验结果的影响,在Replay数据时,对时间区间[40,80]与[80+,120]、[100,120]与[120+,140]、[130,140]与[140+,150]、[145,150]与[150+,155],两两重放相同的数据.在该实验环境下,分别采用基本方法、WDRS和RM-CBDD3种算法,以155min作为恢复的起始时刻,时间区间[0,40]上以5min为间隔的9个时刻点为目标时刻,执行恢复操作(每次恢复操作前,都需要将数据卷恢复至恢复起始时刻155min所对应的状态),分别记录下每次恢复操作的恢复时间.类似的,再分别以150min、140min、120min和80min作为恢复的起始时刻,执行上述恢复操作并记录恢复时间.从图7中可以看出,以155min作为恢复起始时刻,它与初始时刻0min之间存在4个间隙,因此每次恢复都经历了4个间隙;依次类推,以150min、140min、120min和80min作为恢复起始时刻的恢复,则依次经历了3个、2个、1个和0个间隙.因为不同的数据分布会对恢复起止时刻对应镜像之间的数据差异产生影响,所以为了观察不同间隙个数对恢复效率的影响,在以上构建实验环境的过程中,对Replay数据进行了特殊处理.这样,即便恢复起始时刻不同,但对于相同的目标时刻,恢复过程中所处理的数据都是相同的,进而可以更好地观察间隙个数对恢复效率的影响.实验结果如图8所示.根据间隙个数的不同,基本方法、WDRS和RM-CBDD3种恢复算法在图8中都分别有5条曲线与之对应.从图中可以明显看出,每种恢复算法的5条曲线都近似重合,由于排除了数据分布可能对实验结果造成的影响,可见间隙的个数对恢复效率并没有显著影响.RM-CBDD算法的恢复过程也印证了这一点:通过读取恢复映射序列等元数据,在内存中进行一定的运算后,便得到了写请求集合,即消除了间隙的影响,并且与需要消除的差异数据相比,读取元数据和内存操作所消耗的时间则要少得多,表现在整个恢复时间中就不明显了.4.4寻找最优恢复时刻点一旦系统管理员发现数据发生了损坏,就需要寻找并确定一个能够获得干净数据的恢复时刻点.通常,系统管理员在选择恢复时刻点时,需要在恢复时间(进行一致性检测的数据版本的个数)和数据损失量之间进行权衡.由于肩负着尽快恢复数据的使命,对于系统管理员,可恢复时刻点的识别通常是一个手工的、易错的、耗时繁琐的过程.虽然连续数据保护系统提供了以写请求为精度向前恢复的能力,但仍旧无法解决识别恢复时刻点所面临的矛盾.只有在恢复时刻点确定之后,才能用一致的干净数据替换损坏的数据,整个恢复过程才得以完成.通常,在寻找恢复时刻点时,系统管理员会在连续数据保护的初始时刻与发生数据损坏的时刻之间进行二分探查,对于每一个二分探查的时刻点,都需要通过完整性检测工具对该时刻点的数据镜像进行检测,以判断下一个探查时刻点,在找到满足RPO的时刻点时完成探查.为了评估RM-CBDD在可恢复时刻点识别中的效率,我们重新根据图5构造了实验环境,然后分别采用基本方法、WDRS和RM-CBDD3种恢复算法,以300min作为恢复的起始时刻,在5个连续的Page11时间区间[0,300]、[0,200]、[0,100]、[0,50]和[0,25]上,通过二分探查的方式分别对可恢复时刻点进行探查.与4.2节中的实验不同,在同一时间区间内的每次探查之前,数据卷保持上次探查结束的状态即可,只在对下一个时间区间开始探查之前,才需要将数据卷恢复至起始时刻300min所对应的状态.实验中并没有采用真实的完整性检测工具,因此,对于以上5个探查区间,每次二分探查之后的下一个探查时刻点,分别采用一直向前探查、一直向后探查和前后交替探查3种方式进行模拟.此外,实验中所找到的恢复时刻点,要求与最佳恢复时刻点之间的时间间隔不超过1min,在对每个时间区间的二分探查过程结束后,数据卷的状态即为所找到的恢复时刻点所对应的状态,探查恢复过程结束.实验中记录下探查每个时间区间所花费的总恢复时间(不包含对各探查时刻对应数据镜像进行完整性检查的时间),结果如图9所示.如图9所示,不论向前探查、向后探查,还是前后交替探查,RM-CBDD所消耗的总恢复时间都远远小于其他两种算法.其中,探查8次的平均总恢复时间为78.03s,探查6次的平均总恢复时间仅为33.15s,WDRS所消耗时间分别是RM-CBDD的4.32倍和7.56倍,基本方法分别是13.47倍和13.48倍.由于WDRS的恢复时间非常稳定,每次恢复所消耗时间均在40s上下.因此,如图9所示,不论向前、向后还是前后交替探查,WDRS的总恢复时间根据探查次数的不同而随之增减,与每次恢复起止时刻之间的数据量以及初始时刻与目标时刻之间的时间间隔均无关.对于基本恢复方法,在向前探查时,时间区间[0,300]的总恢复时间为482s,而向后探查竟高达2184s,前后交替探查则以1426s介于两者之间.这是因为,随着探查次数的不断增加,向前探查使得探查的时刻点不断向前移动,与初始时刻0min的间隔不断减小;而向后恢复则恰恰相反,每次探查的时刻点不断向后移动,与初始时刻0min的间隔不断增大;由于基本方法的恢复时间是由恢复初始镜像和执行Redo操作两部分组成的,向前探查使得每次需要Redo的写请求不断减少,而向后探查则不断增多,前后交替探查介于两者之间;由此形成了图9所示的结果.与基本方法不同,探查方向的变化并不会对RM-CBDD的恢复时间造成明显影响.根据二分查找的性质,随着探查次数的不断增加,相邻两次探查时刻点之间的时间间隔呈指数次幂不断减小,两时刻点之间的历史写请求随之骤减,使得RM-CBDD所需要处理的差异数据骤减,进而恢复时间骤减,效率提高非常明显.实验中,RM-CBDD在时间区间[0,300]上进行了9次探查之后,找到了与最佳恢复时刻点之间间隔小于1min的恢复时刻点,其中每次探查所消耗的恢复时间如图10所示.可以明显看出,随着探查次数的增加,恢复时间随之骤减,并且在第8次探查时已经低于1s.注意到,构建实验环境所Replay的数据,平均每分钟产生70000个写请求,如果要寻找到最佳恢复点,只需要继续进行不超过17次的Page12探查,消耗不超过17s的时间.而如果采用WDRS或者基本方法继续寻找最佳恢复时刻点,所消耗的时间则是不可忍受的.图10RM-CBDD在时间区间[0,300]上每次5总结以在元数据中增加恢复映射序列以及在历史写请求中插入快照为基础,RM-CBDD算法采用递归算法获取恢复起止时刻基于参照时刻的两个写请求集合,然后通过对两个写请求集合的分析处理,找出了恢复起止时刻之间两种类型的差异数据,即“少数据”和“多数据”,最后针对两种类型差异数据的特点分别进行了处理,消除了恢复起止时刻之间的数据差异,实现了数据恢复.通过对“临近算法”的完善与改进,RM-CBDD不仅支持存在多间隙的复杂实际应用环境,并且继承了“临近算法”的高恢复效率.实验数据表明,在多间隙复杂情况下,RM-CBDD的恢复效率明显高于基本方法和WDRS,有效降低了二分探查最佳恢复时刻点的时间开销.
