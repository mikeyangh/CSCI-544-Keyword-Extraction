Page1神经网络极速学习方法研究邓万宇1)郑庆华1)陈琳2)许学斌1)1)(西安交通大学电信学院计算机系智能网络与网络安全教育部重点实验室西安710049)2)(西安邮电学院计算机科学与技术系西安710061)摘要单隐藏层前馈神经网络(Single-hiddenLayerFeedforwardNeuralNetwork,SLFN)已经在模式识别、自动控制及数据挖掘等领域取得了广泛的应用,但传统学习方法的速度远远不能满足实际的需要,成为制约其发展的主要瓶颈.产生这种情况的两个主要原因是:(1)传统的误差反向传播方法(BackPropagation,BP)主要基于梯度下降的思想,需要多次迭代;(2)网络的所有参数都需要在训练过程中迭代确定.因此算法的计算量和搜索空间很大.针对以上问题,借鉴ELM的一次学习思想并基于结构风险最小化理论提出一种快速学习方法(RELM),避免了多次迭代和局部最小值,具有良好的泛化性、鲁棒性与可控性.实验表明RELM综合性能优于ELM、BP和SVM.关键词极速学习机;正则极速学习机;支持向量机;结构风险;神经网络;最小二乘1引言单隐藏层前馈神经网络(Single-hiddenLayerFeedforwardNeuralNetwork,SLFN)之所以能够在很多领域得到广泛应用,是因为它有很多优点:(1)具有很强的学习能力,能够逼近复杂非线性函数;(2)能够解决传统参数方法无法解决的问题.但另一方面缺乏快速学习方法,也使其很多时候无法满足实际需要.对于SLFN的学习能力,很多文献分别从紧集(compactinputsets)和有限集(infiniteinputsets)两种输入情况进行了深入讨论.Hornik研究表明:如果激励函数连续、有界且不是常量函数,那么SLFN能够在紧集情况下逼近任何连续函数[1];Leshno在Hornik基础上的进一步研究表明:使用非多项式激励函数的SLFN能够逼近任何连续函数[2].在实际应用中,神经网络的输入往往是有限集,对于有限集情况下SLFN的学习能力,Huang和Babri等进行了研究,结果表明:对于含有N个不同实例的有限集,一个具有非线性激励函数的SLFN最多只需N个隐藏层结点,就可以无误差地逼近这N个实例[3-4].这就是说,一个具有N个隐藏层结点的SLFN,即使输入权值随机取值,它也能够准确拟合N个不同的实例,更明确地讲就是:SLFN的学习能力只和隐藏层结点的数目有关,而和输入层的权值无关.虽然这一点对于提出一种新的学习算法很有启发,但并未引起研究者的注意,迭代调整的思想一直坚持到现在,很多算法都只是围绕这一思想进行技巧性的改进.不同于传统的学习方法,Huang基于以上研究结论为SLFN提出了一种称为极速学习机(ExtremeLearningMachine,ELM)的学习方法[5]:设置合适的隐藏层结点数,为输入权和隐藏层偏差进行随机赋值,然后输出层权值通过最小二乘法得到.整个过程一次完成,无需迭代,与BP相比速度显著提高(通常10倍以上).但是ELM是基于经验风险最小化原理,这可能会导致过度拟合问题[6].此外因为ELM不考虑误差的权重,当数据集中存在离群点时,它的性能将会受到严重影响[7].为了克服这些缺点,我们结合结构风险最小化理论以及加权最小二乘法对ELM算法进行改进,使得ELM在保持“快速”这一优势的前提下,泛化性能得到进一步的提高.2SLFN的统一模型对于N个不同样本(狓i,狋i),其中狓i=[xi1,xi2,…,xin]T∈Rn,狋i=[ti1,ti2,…,tim]T∈Rm,一个隐藏层结点数目为珦N、激励函数为g(x)的SLFN的统一模型为∑珦Ni=1βigi(狓j)=∑珦N其中犪i=[ai1,ai2,…,ain]T是连接第i个隐藏层结点的输入权值;bi是i个隐藏层结点的偏差(bias);βi=[βi1,βi2,…,βim]T是连接i个隐藏层结点的输出权值;犪i·狓j表示犪i和狓j的内积.激励函数g(x)可以是“Sigmoid”、“Sine”或“RBF”等.上述N个方程的矩阵形式可写为其中犎(犪1,…,犪珦N,b1,…,b珦N,狓1,…,狓N)=g(犪1·狓1+b1)…g(犪珦N·狓1+b珦N)熿g(犪1·狓N+b1)…g(犪珦N·狓N+b珦N燀熿β=燀E(犠)表示期望值和实际值之间的误差平方和,问题求解就是寻找最优的权值犠=(犪,犫,β)使代价函数E(犠)最小,其数学模型可表示为argmin犠=(犪,犫,β)E(犠)=argmin犠=(犪,犫,β)‖ε‖2,s.t.∑珦Ni=1βig(犪i·狓j+bi)-狋j=εj,j=1,2,…,N其中εj=[εj1,εj2,…,εjm]是第j个样本的误差.为例进行研究,但所得结论仍适用于多维情况.3BP为了方便讨论,在后文中将以一维输出(m=1)由Rumelhart和McClelland提出的BP神经网络模型是目前应用最广泛的模型之一[8],BP训练方法是通过反向误差传播原理不断调整网络权值使得实际输出与期望输出之间的误差平方和达到最小或小于某个阈值.当犎未知时,通常采用梯度下降法Page3迭代调整犠:其中η代表学习速率.基于梯度下降法的BP存在以下缺点:(1)训练速度慢.因为需要多次的迭代,所以时间消耗很长.(2)参数选择很敏感,必须选取合适的η与犠初值,才能取得理想的结果.若η太小,算法收敛很慢,而η太大,算法不太稳定甚至不再收敛;(3)局部最小值.由于E(犠)非凸,因此在下降过程中可能会陷入局部最小点,无法达到全局最小[9];(4)过渡拟合.在有限样本上训练时,仅以训练误差最小为目标的训练可能导致过渡拟合.4ELMSLFN提出了ELM学习算法.为了解决以上问题,Huang基于以下定理为定理1[5].对于任意N个不同样本(狓i,狋i),其中狓i=[xi1,xi2,…,xin]T∈Rn,狋i=[ti1,ti2,…,tim]T∈Rm,N个隐藏层结点和一个任意区间无限可导的激活函数g:R→R,则SLFN在犪i∈Rn和bi∈R任意赋值的情况下,所形成的隐藏层矩阵犎可逆,即方程组有精确解,代价函数E(犠)=0.定理2[5].给定任意N个不同样本(狓i,狋i),任意小误差e>0,及在任意区间无限可导的激活函数g:R→R,总存在一个包含珦N(珦NN)个隐藏层结点的SLFN,使得在犪i∈Rn和bi∈R任意取值情况下,误差E(犠)e.定理1和定理2的详细证明可参考文献[4-5,10].定理表明:只要隐含层结点数足够多,SLFN就能在输入权随机赋值情况下逼近任何连续函数.但为了使SLFN具有良好的泛化性能,通常珦NN.当输入权以随机赋值的方式确定后,所得隐藏层矩阵犎便是一个确定的矩阵,因此训练SLFN就转化为计算犎β=犜的最小二乘解问题.关于ELM的细节请参考文献[5].与BP相比ELM需要调整的参数只有隐含层结点个数珦N,目前虽没有精确估计珦N的方法,但珦NN大大缩小了搜索范围,在实际应用中珦N可以通过交叉验证的方式确定.在标准UCI数据集上的大量实验表明ELM训练速度快,泛化性能良好,但ELM仍有一些缺点:(1)ELM仅考虑经验风险,没有考虑到结构化风险,因此可能导致过度拟合问题;(2)ELM直接计算最小二乘解,用户无法根据数据集的特征进行微调,可控性差;(3)当数据集中存在离群点时,模型性能将会受到很大影响,鲁棒性较差.为了克服这些缺点,我们把结构风险最小化理论以及加权最小二乘法引入到ELM中,提出一种正则极速学习机(RegularizedExtremeLearningMachine,RELM).5正则极速学习机(RELM)根据统计学理论可知,实际风险包括经验风险和结构风险两种成分[11].一个具有较好泛化性能的模型应该能权衡这两种风险,并取得最佳的折中.RELM将同时考虑这两种风险因素,并通过参数γ调节两种风险的比例,RELM的数学模型可表示为argminβE(犠)=argminβs.t.∑珦Ni=1βig(犪i·狓j+bi)-狋j=εj,j=1,2,…,N,其中,误差的平方和‖ε‖2代表经验风险;‖β‖2代表结构风险,它源于统计理论中边缘距离最大化原理[12-13];而γ则是两种风险的比例参数,通过交叉验证的方式确定γ来获得两种风险的最佳折中点.为了获得一个抗干扰模型,我们为不同样本的误差进行加权,‖ε‖2被扩展为‖犇ε‖2.其中犇=diag(v1,v2,…,vN)表示误差的权值对角阵.RELM的模型进一步修正为argminβs.t.∑珦Ni=1βig(犪i·狓j+bi)-狋j=εj,j=1,2,…,N.上式是条件极值问题,通过拉格朗日方程转换为无条件极值问题进行求解:(β,ε,α)=γPage4其中α=[α1,α2,…,αN];αj∈Rm(j=1,2,…,N)代表拉格朗日乘子.求拉格朗日方程的梯度并令其为0:把方程(5c)代入方程(5b)得把式(6)代入方程(5a)得表达式(7)只含有一个珦N×珦N(珦NN)矩阵的逆操作,所以计算β的速度非常快.5.1无权RELM在实际应用中,如果数据集中离群点很少,对模型没有太大影响,那么为了加快训练速度,可以认为每个样本的误差权值相同,此时矩阵犇=diag(v1,v2,…,vN)将是一个单位阵,无须计算.我们称这种情况的RELM为无权RELM,无权RELM算法可归结如下:算法1.无权RELM.给定一个训练集={(狓i,狋i)|狓i∈Rn,狋i∈Rm,i=1,2,…,N}、激励函数g(x)及隐藏层结点数珦N,(1)随机指定输入权值犪i和偏差bi(i=1,2,…,珦N).(2)计算隐藏层输出矩阵犎=(3)计算输出权值β:β=犐通过观察不难看出,RELM与ELM计算量基本一样.其实ELM是未加权RELM的一种特殊情况.定理3.当γ→时,未加权RELM将退化为证明.若γ→,则犐ELM.β=犐5.2加权RELM与无权RELM相反,如果数据含有离群点,那么使用加权RELM有一定的抗干扰能力,这可以从后面“SinC”数据集离群点加入前后的实验对比中看出.加权RELM需要计算误差的权值,权值计算已有很多论述[7,14],这里采用文献[15]提到的方法:其中εj=-αjγ,它是无权RELM计算得到的样本误差,^s是误差εj的标准偏差(standarddeviation)估计,可通过公式^s=1.483MAD(xj)计算.MAD(MedianAbsoluteDeviation)表示绝对中位差.根据高斯分布可知:基本不存在大于2.5^s的误差,因此常量c1和c2通常被置为c1=2.5,c2=3[7].综上所述,RELM算法可归结如下:算法2.加权RELM.给定一个训练集={(狓i,狋i)|狓i∈Rn,狋i∈Rm,i=1,2,…,N}、激励函数g(x)以及隐藏层结点数珦N,(1)随机指定输入权值犪i、偏差bi(i=1,2,…,珦N)并且计算隐藏层输出矩阵犎.(2)β=犐(3)α=-γ(犎β-犜)T.(4)εi=αi(5)^s=1.483MAD(狓j).(6)犇=diag(v1,v2,…,vN):(7)β=犐加权RELM多了计算权值的过程,时间消耗有所延长,因此如果实际应用中对训练时间要求很强,那么用无权RELM比较合适.在下面的实验中,除为了验证RELM的鲁棒性在“SinC”数据集上采用加权RELM和ELM进行比较外,其它数据集的实验一律采用无权RELM和ELM进行比较.RELM与ELM相比,具有如下特点:(1)方程组的解是犎β=犜的一个加权最小二‖犎β^-犜‖=‖犎(犎T犇2犎)犎T犇2犜-犜‖这个解不但可以达到最小的训练误差,同时对乘解:离群点具有一定的抗干扰能力.(2)通过引入调节参数γ,代价函数不仅包括经Page5验风险,还包括结构风险,这使得方程组的解不仅获得尽可能小的训练误差,而且能使边缘距离最大化,从而具有更好的泛化性能:犎犐argminβγ‖犎β-犜‖+‖β‖6性能评估这里我们通过实验结果比较RELM、ELM、BP和支持向量机(SupportVectorMachine,SVM)[12-13]的性能.RELM、ELM和BP的执行环境是Matlab7.0,SVM的执行环境是C语言.RELM由我们自己实现,ELM的源代码可以从Huang的个人主页直接下载①,而BP算法已经集成在Matlab自带的神经网络工具箱中,可以直接使用.BP算法有很多变种,我们选择最快的Levenberg-Marquardt算法来进行实验.SVM算法我们采用C语言实现的SVM包:LibSVM②.RELM、ELM和BP的激励函数都选择“Sigmoid”函数:g(x)=1/(1+exp(-x)),而SVM的核函数选择径向基函数.实验数据的输入一律归一化到[0,1]范围内,而输出则归一化到[-1,1]范围内.值得指出的是,这里汇总的实验结果都是每种算法能够达到的最优实验结果.对于SVM,我们采用Hsu和Lin提出的排列组合方式[16]选择最优的表14种算法在“SinC”数据集上的性能比较RELMELMBPSVM为了比较RELM和ELM算法的鲁棒性,“SinC”训练集中加入了一些离群点后进行重新实验.实验结果见图1,从图中可以看出ELM的预测曲线明显脱离实际曲线,说明其受到离群点的干扰很大.而RELM的预测曲线仍能完好地拟合实际曲线,说明RELM具有一定的抗干扰能力.6.1.2实际回归问题我们在13种真实数据集③上将RELM与ELM、BP、SVM进行比较,数据集信息见表2.4种算法的RMSE见表3.从表3可以看出,RELM在大多数据集上的测试RMSE比ELM、BP、SVM小,参数γ和C:γ=[24,23,…,2-10],C=[212,211,…,2-2].共有15×15=225种组合,对每一种组合(γ,C),进行50次随机实验,并对最佳平均值进行汇总.对于RELM,我们采用类似于SVM的方式选择最优的参数γ和隐藏层结点数珦N:γ=[2-50,2-49,…,250],珦N=[5,10,…,珦Nmax](珦Nmax根据具体数据集设定).对于所产生的每个组合(γ,珦N),进行50次随机实验,并对最佳平均值进行汇总.对于ELM和BP,隐藏层结点的个数初始取5,每次递增5,并基于5-折交叉验证的方法选择最优(接近)的数目,然后进行50次实验并将最佳平均结果进行汇总.6.1回归问题6.1.1人工数据:“SinC”“SinC”函数表达式:y(x)=sinx/x,x≠0数据产生方法:在区间(-10,10)内随机产生5000个训练样本和测试样本,并在所有训练样本上附加取值范围为[-0.2,0.2]的随机噪声,而测试数据无噪声.各种算法的性能见表1.从表1可以看出RELM的RMSE(RootMeanSquareError,均方根误差)比ELM小,分别为0.0078和0.0097;不过RELM训练时间比ELM稍长;RELM的RMSE明显比BP算法和SVM算法要小,而训练时间却比BP和SVM缩短了上百倍.由此可见在“SinC”数据集上,RELM综合性能最好.0.00780.00970.01590.0130说明其有更好的泛化性能(如果两种算法的RMSE相差大于0.005时,较好的RMSE加粗表示);表4汇总了4种算法的时间消耗,从表4可以看出RELM的训练速度和ELM相差无几,却比BP和SVM快很多倍.但是由于BP具有最紧凑网络结构(隐藏层结点数最少),在4种算法中BP测试时间最短;表5汇总了4种算法的标准偏差.①②③lin/libsvm/Page6图1加入离群点前后“SinC”曲线拟合效果比较表2回归数据集信息数据集AbaloneDeltaailerons3000412960ServoDeltaelevators4000551760Breastcancer10094320Computeractivity4000419280Bankdomains4500369280CensusAutopriceTriazines表34种不同算法的均方差(RMSE)比较SVM的RMSE训练样本测试样本AbaloneDeltaailerons0.04090.04810.04180.04290.04230.04310.02800.0388Deltaelevators0.05440.05920.05340.05400.05500.05680.00170.0179Computeractivity0.04640.04700.04640.04700.03160.03820.03210.0185Census(house8L)0.07180.07460.07180.07460.06240.06600.06240.0360AutopriceTriazinesMachineCPU0.03520.08260.05740.08110.03320.05390.01680.0220ServoBreastcancer0.22780.26430.22780.26430.24700.26790.21450.201Bankdomains0.04540.04670.04540.04670.04060.0360.0410.015Californiahousing0.10890.11800.10890.11800.12170.12670.12130.1170Stocksdomain0.05030.05180.05030.05180.02510.03480.0220.0132Page7表44种不同算法的时间比较SVM的时间训练样本测试样本AbaloneDeltaailerons2.0850.0120.5100.1690.0450.0480.0460.048Deltaelevators0.9040.0090.8490.4450.2130.1550.2210.155Computeractivity51.0910.0520.7690.2290.2240.1300.2970.132Census(house8L)6.1100.0358.5233.1420.8180.4770.8270.484AutopriceTriazinesMachineCPUServoBreastcancerBankdomains5.6860.0351.2180.2290.4870.1670.5480.169Californiahousing4.9480.03656.2007.9190.8470.2300.9400.234Stocksdomain0.7940.0050.0520.0480.0130.0230.0140.023表54种不同算法的标准偏差(standarddeviations)比较SVM的标准偏差训练样本测试样本AbaloneDeltaailerons0.00150.00150.00120.00100.00300.00350.00280.0034Deltaelevators0.00070.00030.00060.00050.00280.00290.00250.0029Computeractivity0.00070.00070.00150.00160.00050.00330.00040.0034Census(house8L)0.00110.00500.00130.00130.0010.00170.0020.0026AutopriceTriazinesMachineCPU0.01920.07150.00830.01800.00600.01560.00560.0147ServoBreastcancer0.15290.09620.01150.01510.01210.01670.01270.0167Bankdomains0.00060.00040.00050.00080.00060.00090.00060.0008Californiahousing0.00450.00260.00120.00110.00210.00330.00200.0026Stocksdomain0.00120.00220.00160.00220.00110.00160.00140.0015前面提到当γ→时,RELM将退化为ELM.为了说明这一点,我们以数据集“Triazines”为例展示RELM的性能(RMSE)随γ变化情况.如图2所示,可以看出RELM的性能首先随着γ的增大不断提高(越小越好),当γ=2-2时,RELM的性能达到最好,比ELM提高了0.05.之后,随着γ的增大,RELM的性能不断降低,并逐渐与ELM的性能曲线重叠在一起,这说明当γ→时,RELM退化为ELM,由此可见RELM的精度至少能与ELM相当.6.2分类在这一部分,我们来给出RELM在分类中的性能测试.数据集信息见表6,包括Banana数据集①以及一些来自UCI的数据集②.Banana数据集中冗余数据已被删除,只选用其中的5200个不同的训练样本.由于Forest-type数据集规模很大,基于梯度的BP算法在传统PC上会产生内存溢出,因此我们直接引用文献[17]中的实验结果.SVM的参数设置是C=10.不同算法的性能见表7,从表7可以看出RELM的运行速度和ELM几乎一样快,并获得更好的泛化性能.①②Page8表6分类问题数据集的信息数据集训练样本测试样本属性类别数据集训练样本测试样本属性类别Diabetes57619282Shuttle435001450097Satimage44002000367BananaSegmen表74种算法在分类问题上的性能比较数据集算法RELM0.0110.00278.6978.191.0902.55DiabetesSatimageSegmentShuttleBananaForest-typeRELM1.64816.18092.3391.780.0970.105RELM1.3610.58192.2590.330.0120.0137结束语RELM打破了传统BP算法的参数迭代调整的思想,从而获得了快速学习的能力,从不同数据集的实验可以看出RELM比BP、SVM速度提高很多倍(通常是10倍以上),而泛化性能却比BP、SVM高,这无疑为神经网络应用到实时环境提供了有效途径.与ELM相比,RELM不但继承了ELM快速训练的特点,还通过引入参数γ使得模型可以根据数据集的特点进行微调,从而得到更好的泛化性能,增强了系统的可控性.另外如果数据集中存在明显的噪声,那么可以使用加权RELM,达到降噪的目的,从对“SinC”数据集的实验可以看出加权RELM对噪声有一定的抗干扰能力.在我们下一步研究中,我们打算将在线学习与RELM相结合,提出一种能够处理流数据的RELM;将SOM[18]、主动学习等技术与RELM相结合以期得到更好的性能.在应用方面,我们打算将RELM应用到文本分类、协作过滤、数字水印等领域,尤其是文本分类与协作过滤都存在高维稀疏问题,传统方法非常耗时,如何研究一种有效的快速Sparse-RELM将是另一个值得研究的问题.
