Page1基于深度学习的鲁棒性视觉跟踪方法高君宇杨小汕张天柱徐常胜(中国科学院自动化研究所模式识别国家重点实验室北京100190)摘要传统的视觉跟踪方法(如L1等)大多直接使用视频序列各帧内的像素级特征进行建模,而没有考虑到各图像块内部的深层视觉特征信息.在现实世界的固定摄像头视频监控场景中,通常可以找到一块区域,该区域中目标物体具有清晰、易于分辨的表观.因此,文中在各视频场景内事先选定一块可以清晰分辨目标表观的参考区域用以构造训练样本,并构建了一个两路对称且权值共享的深度卷积神经网络.该深度网络使得参考区域外目标的输出特征尽可能与参考区域内目标的输出特征相似,以获得参考区域内目标良好表征的特性.经过训练后的深度卷积神经网络模型具有增强目标可识别性的特点,可以应用在使用浅层特征的跟踪系统(如L1等)中以提高其鲁棒性.文中在L1跟踪系统的框架下使用训练好的深度网络提取目标候选的特征进行稀疏表示,从而获得了跟踪过程中应对遮挡、光照变化等问题的鲁棒性.文中在25个行人视频中与当前国际上流行的9种方法对比,结果显示文中提出的方法的平均重叠率比次优的方法高0.11,平均中心位置误差比次优的方法低1.0.关键词深度学习;卷积神经网络;视觉跟踪;鲁棒性;L1跟踪系统;计算机视觉1引言视觉跟踪是当前计算机视觉领域的研究热点,其广泛应用于物体智能识别、人机交互、车辆定位等方面[1].视觉跟踪还可应用于智能视频监控技术中,服务智慧城市的发展和公共安全的需求[2].虽图1视频序列中可能出现的多种挑战(遮挡、光照变化、尺度变化、姿态变化等)大多数跟踪算法都可划为判别型和生成型方法[11-12,16].判别型方法将跟踪问题建模为一个二元分类问题,用以找到区分目标物体和背景的决策边界.Avidan[11]将许多弱分类器组合成一个强分类器,提出了一种全局跟踪方法.Babenko等人[12]提出了一种基于在线多示例学习的跟踪方法.Grabner等人[13]提出了一种在线boosting方法来更新可区分的特征.Struck跟踪系统[5]利用一类核函数实现结构化输出跟踪结果.Kalal等人[14]提出了P-N学习算法,通过对潜在正负样本结构信息的学习构造出目标跟踪的分类器.黄福珍等人[15]提出了一种基于LevelSet的人脸跟踪方法,利用图像帧间差分快速检测出运动区域,并根据人脸图像的投影映射规则确定人脸所在的外接矩形,从而判定跟踪过程中的人脸位置.与判别模型相反,生成跟踪模型通过学习一个模型来表示目标物体,然后使用此模型以最小的重构误差搜索图像区域,以达到跟踪目的.Mei等人[16]提出了L1跟踪系统,通过对目标进行稀疏表然近年来关于视觉跟踪算法的研究取得了较大的进展[3-10],但其在现实中应对各类复杂场景时仍然面临着巨大的挑战,例如遮挡、亮度变化、姿态变化、尺度变化等,如图1所示,在一个视频序列中可能出现多种跟踪挑战.所以,如何使跟踪算法更具鲁棒性以适应上述挑战仍然是目前研究聚焦的核心.示,达到跟踪目的.Frag跟踪系统[17]通过使用局部块的直方图表示对物体的外观进行建模,以解决部分遮挡问题.Jia等人[18]提出了一种自适应的结构化局部稀疏外观模型进行跟踪.Zhong等人[19]使用基于稀疏表示的局部块间协同的模型以获得跟踪的鲁棒性.Zhang等人[20]在在线跟踪中引入了多专家重建机制,通过求解一个熵最小化问题重建当前的跟踪系统.肖国强等人[21]提出了一种基于中心宏块的视频目标跟踪算法,引入了一个中心宏块的概念,通过两个层次的相似性度量,以建立相邻帧之间目标的对应关系.判别模型和生成模型中大多数方法直接使用视频图像序列中的像素值进行建模,当跟踪过程中出现严重的遮挡、复杂背景等较大挑战时,浅层的像素级特征无法很好地应对.而经过学习得到的深层视觉结构特征利于处理这些问题.近年来,深度学习框架已经应用于计算机视觉领域并取得了良好的效果.其中,卷积神经网络(ConvolutionalNeuronNetworks,CNN)由卷积层和全连接层构成,通过共Page3享权重和池化层(poolinglayer)来降低参数的数目和提升效果,具有良好地学习图像深层视觉特征的能力.因此,本文提出了一种新颖的深度卷积神经网络结构,利用视频场景中的区域位置特性以提高视觉跟踪的鲁棒性.本文提出了一种基于深度学习的跟踪算法.该算法基于如下观察:在一个固定摄像头的视频场景中,可以找到一块较好的区域,例如一块位置适中、表现平整的马路等,如图2所示(黑白图①).图2中8张图片内矩形框所围成的区域即本文选择的部分参考区域.在参考区域中目标物体通常会有清晰、易分辨的表观,而当目标物体出现在参考区域外时,由于非参考区域可能具有复杂的环境,目标物体的表观易于呈现出不清晰、不完整的情况,如图3所示.这个发现启发本文在跟踪过程中利用参考区域的位置特性以增强目标的可识别性,即将目标样本映射图28个视频场景中的参考区域包含了遮挡、亮度变化、姿态变化等阻碍跟踪性能的情况,使得学习到的模型具备适应各类问题的鲁棒性.到参考区域内,以获得参考区域内目标样本的良好表观特性.同时,本文改进了传统的用于分类的卷积神经网络,构造了一个两路对称且权值共享的网络模型,使其能学习到增强目标可识别性的权重参数,并将此网络应用到视觉跟踪过程中.本文将出现在参考区域的目标样本和不在参考区域的同一目标的样本构成样本对,作为训练深度网络的输入.此网络分为对称且权值共享的两条路径,每路都由3层卷积层和一层全连接层组成.两种类型的样本分别通过这两条路径并在全连接层输出固定维度的特征(如10×10),通过最小化欧氏距离函数,使负样本输出的特征尽可能与正样本相似.由于是固定摄像头场景下的视觉跟踪且选取的参考区域固定,所以该网络学习出的权重参数对于出现在该摄像头内的目标物体具有通用性.各摄像头对应的深度网络进行训练的样本对数量的均值为40530对,其中充分①根据投稿格式要求,文中均使用黑白图片.为了便于读者理jygao/result-gjy.htmlPage4图3参考区域内外的目标样本差异在跟踪过程中,深度网络输出的相应特征可以提高许多直接使用浅层特征的跟踪系统的鲁棒性,如L1跟踪系统.本文的实验部分以L1跟踪系统[3]为例,提出了一种基于深度学习的L1跟踪系统(DeepLearningL1tracker),通过使用模板特征和每一帧内粒子采样块对应的输出特征,通过求解一个1规则化最小二乘问题,实现用模板特征集对目标特征进行稀疏表示.本文提出的基于深度学习的L1跟踪算法具有以下优点:(1)模型在应对遮挡、光照变化、物体姿态改变等方面具有较高的鲁棒性.由本文学习得到的卷积神经网络模型可以应用于其他已有的跟踪模型中进行特征提取;(2)经过深度学习训练得到的模型在该摄像头内进行目标跟踪具有很强的通用性;(3)本文提出的模型在应对初始帧内目标模糊或残缺的状况时较其他模型更优.通过在25个视频中与前文提及的目前国际上最为先进的9种方法进行对比,本文提出的方法在平均重叠率和平均中心位置误差的评价指标上超过了其他跟踪系统.2相关工作本文将提出的深度学习模型应用于一类基于稀疏表示的生成型跟踪系统中,如L1跟踪系统[3].近年来,基于稀疏表示的跟踪方法得到了较大的发展.文献[22-23]利用粒子滤波方法中目标周围的采样粒子具有相似性和依赖性的关系,提出了一个由多个粒子共同构建的协同稀疏模型.文献[24]构造了一个结构化的多任务稀疏学习模型,提升了跟踪过程的鲁棒性.文献[25]采用部分匹配进行稀疏表示的方法,良好地解决了部分遮挡问题.文献[25]构造了一个一致性低秩稀疏模型,利用粒子采样中粒子之间的固有关系提高了跟踪过程的鲁棒性.王宇霞等人[26]提出了一种基于自重构粒子滤波算法的目标跟踪,该算法能够通过分裂跟踪器以应对复杂多变的跟踪环境,同时,合并过程能够从多个跟踪器中选出最优跟踪器,利用合并冗余的跟踪器以达到减少计算量的效果.另一方面,近年来深度学习已经开始应用于视觉跟踪.但这类方法依然存在两个问题:(1)由于深度学习模型规模巨大,所以需要大量的样本进行训练;(2)深度学习庞大的规模导致其在应用中产生了大量的时间开销.为了解决缺少样本的问题,Wang等人[27]使用大量的辅助图像离线训练了一个栈式去噪自编码器,对目标候选进行特征提取,并将这样的特征应用于粒子滤波的框架中,同时,在跟踪过程中更新自编码器.更多的学者选择使用卷积神经网络进行视觉跟踪.Fan等人[28]同样利用大量的辅助图像训练深度卷积神经网络,并将模型应用于行人跟踪中,取得了较好的效果.Zhou等人[29]使用多个神经网络的聚合体进行目标跟踪.然而,这些方法由于缺少大量跟踪过程中的实际数据,所以效果提升的程度有限.为此,Li等人[30-32]设计了层次较浅的卷积神经网络,设定了一个特殊的损失函数,并以在线的方式对跟踪过程中产生的样本进行训练.还有另外的一些方法试图解决上述的两个问题.Hong等人[33]利用卷积神经网络提取特征,并使用在线更新的SVM对跟踪过程中的样本进行分类,将正样本的特征进行反向传播,从而得到正样本对应的显著图并以此显著图来进行判别式跟踪.WangPage5等人[34]利用离线训练好的卷积神经网络模型提取层次特征并用以进行在线跟踪.Chen等人[35]利用类似于文献[27]的方式训练了一个卷积神经网络,而Hu等人[36]则训练了一个卷积深度置信网络(ConvolutionalDeepBeliefNetwork,CDBN)应用于跟踪过程.Zhang等人[37]使用目标区域中随机提取的归一化图像块作为卷积神经网络的滤波器,从而实现了不用训练卷积神经网络的快速特征提取.Kuen等人[38]提出了一种通过强短时限制和栈式卷积自编码器学习到目标表示的不变性.文献[39]利用了一种半监督的深度学习方法进行目标跟踪.3基于深度学习的视觉跟踪方法本节首先介绍了基于参考区域特征变换的深度网络模型,训练好的深度网络模型可以应用于许多图4深度网络的架构在第一个卷积层中,x0通过一个权重矩阵犠1得到了96个特征映射,犠1包含96个子矩阵,即犠1=[W1第一个卷积层中每个卷积核对应的参数,卷积核的1;W1通道数和尺寸分别为3和11×11,采样间隔为4.因此输出的96个特征映射{x1应的和经过非线性激励函数计算得到的.每个x1过式(1)进行计算:本文选用sigmoid函数作为激活函数f(·),对自变量逐元素进行计算,其表达式见式(2).“”代表三维卷积.注意到传统的卷积神经网络模型[40]中含有偏置项b,其输出为f(Wx+b),但是Wx+b传统的跟踪方法(如L1跟踪系统).之后介绍本文据此深度网络模型改进了的L1跟踪算法.3.1基于参考区域特征变换的深度网络图4展示了本文提出的基于参考区域特征变换的深度网络架构.输入data=(x0,y0),其中x0表示出现在参考区域中的样本,y0表示未出现在参考区域中的同一目标的的样本.x0所对应样本的表观大部分清晰、易于分辨,y0所对应样本中包含了大量的遮挡、光照变化、姿态变化等情况.将x0、y0根据通道数、高度和宽度进行尺寸归整,使得x0,y0∈犚3×100×100,即n0=3×100×100.两者各自通过对称且权值共享的三层卷积层、三层池化层和一层x0全连接层,并在全连接层输出各自的特征,维度为100.由于x0,y0通过的路径相互对称且对应层权值共享,所以下面本文只介绍如何获得x0对应的输出特征.可以写作珮Wx~,所以本文省略了b以便于表达.本文使用最大池采样[41]的方式对特征映射进行下采样,下采样滤波器的尺寸为3×3,采样间隔为2,得到x1得到一个完整的特征映射x1∈犚96×11×11,其维度为i∈犚11×11.通过将所有的x1n1=96×11×11.在第二层中,为了能更充分地利用输入信息x1,本文在每个x12.之后将x1与256个卷积核依次进行卷积,对应的权重参数为犠2=[W2犚96×5×5每个卷积核的通道数和尺寸分别为96和Page65×5,采样间隔为1,可得这一层采用尺寸为3×3的滤波器进行下采样,采样间隔为2,因此x2i组成x2∈犚256×5×5,其维度为n2=256×5×5.x2在第三层中,本文将x2映射到x3,且这一层不包含池化.32个卷积核依次与x2进行卷积,卷积核的通道数和尺寸分别为3×3,每个输入x2补的尺度为1.卷积核对应的权重参数记为犠3=[W31;W3i∈犚5×5,将所有的x3其中x332×5×5=800维的向量,即x3∈犚800.最后一层是全连接层,从而输出原始数据的最终特征x4,其维度为n4=100.使用的权重矩阵为犠4∈犚n4×n3.同样,y0也经过相同的深度网络进行变换,最后得到其在最后一个全连接层的输出特征y4.对于训练数据中的所有样本对,本文可以得到两个对应的特征集合X4={x4}m示训练实例的数目.本文的目标函数是最小化正、负样本之间的欧氏距离:式(6)中·2表示2-范数.通过优化式(6)可以获得上述整个深度网络的权重,学习到的权重具有将样本映射到参考区域中的功能,本文将训练好的卷积神经网络用在L1跟踪系统的框架下进行特征提取.3.2改进的L1跟踪系统本文参考了Bao等人[3]提出的APG-L1跟踪系统,本文主要的改动是利用训练好的深度网络对采样图像块和模板提取特征(而不是原始像素)并进行稀疏表示,且不需要考虑单位模板和遮挡情况的处理.3.2.1粒子滤波在视觉跟踪中,粒子滤波是估计下一帧目标位置后验概率的重要方法,包括预测和更新两步.本文用xt表示在t帧时目标物体的状态,用y1:t-1={y1,y2,…,yt-1}表示1~t时刻所有的观测.粒子滤波通过下面两个概率进行预测和更新:p(xt|y1:t-1)=∫p(xt|xt-1)p(xt-1|y1:t-1)dxt-1(7)t帧内目标的最优状态可以通过估计其最大后验概率求解:x要性采样技术时,后验概率可以用一组具有不同权重的粒子St={x1简化的条件下满足wi3.2.2基于特征的稀疏表示t,t2稀疏表示的目的是为了计算粒子xt的似然分布,即p(zt|xt).在t帧,给定目标模板的特征集Tt=[t1样粒子的状态且令Ot={y1选的特征集.由于经过深度网络训练出的特征在应对遮挡问题时十分有效,所以本文不使用单位模板.目标候选特征可以由目标模板特征集近似线性表示如式(9):式中犪=(a1,a2,…,an)T称为目标系数向量.本文希望犪是稀疏的,另外,本文给犪附加了非负约束以增强L1跟踪系统的鲁棒性[16].因此,对于每一个目标候选特征yi数最小化问题实现,并加以非负约束:最后,xi式(11)中,α是一个常数用来控制高斯核的形状,Γ是正则因子,ci是式(10)所求得的最优解.此时,帧t内目标状态的最优解可由式(12)求得另外,本文引入了一种模板更新机制[16]以适应跟踪过程中的光照和姿态变化等,但由于模型中使用的是图像块的特征,所以本文调整了模板更新的阈值.本文还参考了Mei等人[42]提出的最小误差边界,用以加速求解上述1-范数最小化问题.该理论得出每一个粒子xip(zt|xi其中q(zt|xiq(zt|xit)<采样中出现.本文使用两阶段的重采样方法以降低跟踪过程中所需粒子数目[42].a^通过式(14)求得Page7相比于APG-L1[10]跟踪算法,本文的主要改动是使用基于参考区域特征变换的深度卷积神经网络模型对模板和粒子采样的图像块提取特征以进行稀疏表示,并且由于训练好的卷积神经网络具有应对遮挡的能力,所以在L1框架中本文不考虑遮挡情况,见算法1.算法1.基于深度学习的L1跟踪算法.输入:当前帧Ft;输出:根据式(12)求得的x1.fori=1toNdo2.根据xi3.求解式(14)对应的问题;4.根据式(13)计算qi.5.endfor6.根据q值,以降序方式对粒子进行排序;7.令i=1且τ=0.8.whilei<Nandqiτdo9.求解式(10)的最小化问题[3];10.根据式(11)计算似然分布pi;11.τ=τ+12.i=i+1;13.endwhile14.对于ji,令pj=0;15.更新模板特征集Tt-1;16.根据p更新粒子集St-1.4实验结果与分析由于本文提出的深度网络应用于固定摄像头场景,并且需要大量已标注的样本进行训练,所以本文使用北京大学数字视频编解码技术国家工程实验室(NELVT)的PKU-SVD-B数据集进行验证.该数据集也成功应用于2015年全国研究生智慧城市技术与创意设计大赛中.实验使用了该数据集中25个行人目标的视频.这些视频具有多类场景并包含了大量的遮挡、光照变化、姿态变化、密集背景、初始帧目标模糊或残缺等情况.本次实验中,本文使用伯克利大学视觉与学习中心(BVLC)提供的开源深度学习架构Caffe[43]训练本文的深度神经网络.在之后的L1框架中,本文令λ=0.2,模板个数m=10,粒子数目n=600.最后的实验结果与当前国际上流行的9种跟踪算法进行对比,这些算法分别简写为OAB[13]、FragLST[18]、SDG[19]、MIL[12].4.1定量分析为了定量分析每个跟踪系统的性能,本文使用中心位置误差和重叠率两种度量方式.这两种度量方式各有侧重,中心位置误差是跟踪结果和实际情况中心点间的欧氏距离.重叠率是PASCAL竞赛中目标检测的评分标准[44],即对于给定的跟踪边界框ROIT和实际情况的边界框ROIGT,通过使用score=本文在25个图像序列上与9种当前流行的跟踪系统对比,结果见表1和表2.表中粗体和粗斜体的数据分别表示最好和次好的结果.根据平均重叠率的衡量标准,本文提出基于深度学习的视觉跟踪方法(DDL1)在25个测试视频中有16个排名第一,5个排名第二,DDL1方法在整个数据集上的平均重叠率为0.68,比次优的Struck方法高0.11,比L1方法高0.16,比最低的TLD方法高0.26.总的来说,本文提出的基于深度学习跟踪算法较好地超过了当前流行的其他算法.根据平均中心位置误差的衡量标准,本文提出的DDL1跟踪方法在25个测试视频中有10个排名第一,4个排名第二,在整个数据集上的平均中心位置误差为17.65,比次优的Struck方法低1.0,比L1方法低50.31,比最低的TLD方法低68.52.图5展示了其中18个视频的逐帧中心位置误差,可以看出,在跟踪过程中,本文提出的方法的误差较其他9种方法保持在一个较低的水平.4.2定性分析图6展示了25个视频序列上的10个跟踪系统的部分跟踪结果,图内这些视频序列的名称均为原数据集视频名称的缩写.下文将根据跟踪过程中的主要困难对各跟踪结果进行分析.(1)遮挡.hsln13.11为多人交错行走,目标在过程中多次被严重遮挡,L1、Frag、OAB与TLD算法在23帧、44帧、61帧和65帧附近跟丢目标,其余算法成功地进行了跟踪,但效果没有本文提出的方法好.ygq5.10中目标沿路旁行走,在39帧时,TLD算法跟丢了目标,而DDL1方法可以一直紧凑地跟踪目标.视频dnm1.3中,目标被一辆汽车部分遮挡,Frag和TLD算法分别在25帧和29帧处跟丢,DDL1方法效果显著.视频bwb2.2和视频bwb2.3中行人被树枝遮挡,本文提出的方法较好地应对了这类问题.Page8表125个视频上10个跟踪系统的平均重叠率表225个视频上10个跟踪系统的平均中心位置误差视频bwb2.2bwb2.3bwb2.4dcm6.11dcm6.12dmn3.2dnm1.3dnm1.4hsln13.11hsln13.15hslw14.11jcrn9.3jcrn9.7jcrw10.3jcrw10.7jcrw10.15wmhbe11.11wmhbe11.15wmhbw12.11wmhd7.11wmhd7.15ygn4.12ygq5.10ygq5.11ytw8.7视频bwb2.2bwb2.3bwb2.4dcm6.11dcm6.12dmn3.2dnm1.3dnm1.4hsln13.11hsln13.15hslw14.11jcrn9.3jcrn9.7jcrw10.3jcrw10.7jcrw10.15wmhbe11.11wmhbe11.15wmhbw12.11wmhd7.11wmhd7.15ygn4.12ygq5.10ygq5.11ytw8.7(2)突然运动.hslw14.11中目标在15帧时由静止突然转身移动,OAB算法当即跟丢,Struck、MEEM和LST算法在15帧后跟丢,L1、TLD、SDG、MIL算法在20帧后出现较大误差,本文的算法与Frag跟踪系统表现最好.(3)姿态变化.ygn4.12中目标骑行转弯,姿态不断变化.Frag、L1、LST和SDG分别在5帧、18帧、28帧和30帧后跟丢目标.本文的方法取得了极好L10.650.340.740.750.710.120.820.510.160.670.490.160.390.780.810.720.270.630.540.710.690.230.870.250.16L13.735.16.65.18.654.93.323.8296.537.512.589.4104.410.88.223.1177.753.2220.626.321.3294.63.1161.117.7的效果.视频ygq5.10中,目标斜向横穿马路,姿态发生了较大的变化,L1、Frag、TLD算法分别在20帧、24帧、25帧时跟丢目标,本文构建的算法可以一直跟上.(4)尺度变化.视频bwb2.4中,目标从远处骑行到近处,尺度发生了很大的变化,Frag、OAB、TLD、LST算法依次跟丢了目标,本文提出的基于深度学习的跟踪方法取得了很好的效果.Page9图518个视频中10种方法的逐帧中心位置误差(推荐观看彩色版以获得最佳效果)Page10Page11Page12Page13图625个视频中10种方法的跟踪结果(推荐观看彩色版以获得最佳效果)(5)目标初始帧模糊.dmn3.2中目标初始帧仅为出现在视频边界附近的一小部分模糊的侧身,且dmn3.2视频场景复杂,人流巨大.大多数跟踪算法均在12帧附近跟丢,本文的算法获得了最好的跟踪效果.ytw8.7中目标初始帧位于人流量很大场景中,且初始状态为被严重遮挡,仅露出头部和颈部,最终只有本文的算法可以跟上.这也正体现了本文提出的基于深度学习算法的鲁棒性.(6)遮挡和姿态变化.dnm1.4中目标骑行过程中转了一个较大的弧度,且被附近建筑遮挡,其他方法均在20帧后出现了较大的偏差,有的跟踪框不断变小有的变大.(7)遮挡和尺度变化.dcm6.12中目标由远及近骑行,在14帧时被行驶的汽车遮挡住了下半身,TLD算法当即跟丢,Frag、SDG、LST跟踪系统也在47帧、62帧、70帧时跟丢目标.dcm6.11中,行人由近及远行走,在40帧时被其他行人部分遮挡,Frag、MEEM、TLD相继跟丢目标,本文的跟踪系统获得了最佳效果.(8)亮度变化和尺度变化.wmhbe11.15视频中目标从较远的树荫处走到较近的明处.L1、MEEM、Frag、SDG、OAB等均在100帧之后出现较大误差,本文的方法具有显著的效果.(9)遮挡、突然运动和姿态变化.wmhbw12.11中目标在静止一段时间后突然斜向穿越马路,且被路旁的汽车严重遮挡.L1、MEEM、Frag、MIL、OAB、TLD和SDG方法均在102帧后跟丢.本文的方法成功地跟踪了目标,效果仅次于Struck跟踪系统.5结束语本文提出了一个新颖的基于深度学习的视觉跟踪方法,其核心思想是利用固定摄像头下视频场景Page14中一块利于分辨目标的区域构造训练样本,并试图学习到一种映射变换模型,将不在参考区域中的样本映射到参考区域中.由于参考区域具有良好的位置特性,所以训练出的深度网络具有增强目标可识别性的特点.该网络可以提高多种传统跟踪系统的鲁棒性,如L1跟踪系统.通过实验,本文在25个行人目标视频中与其他9种近年来国际上流行的跟踪系统进行比较,结果显示,本文构建的算法超越了这些已有算法.在后续的工作中,本文将不仅考虑参考区域中样本和不在参考区域中样本构成的样本对,还要考虑以背景作为负样本与目标样本构成样本对进行训练,这样可以更好地增加目标与背景之间的可区分性.另外,利用跨域映射的跟踪思路[45],对于出现在不同摄像头的同一样本,可以训练一个基于多摄像头场景参考区域的统一模型,使得训练好的模型在多摄像头间具有良好的通用性.致谢在此诚挚地感谢北京大学数字视频编解码技术国家工程实验室(NELVT)提供的PKU-SVD-B数据集.并对给本文给出宝贵意见和建议的应龙师兄等表示感谢!
