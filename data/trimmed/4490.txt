Page1基于摄像机动态标定的交通能见度估计郜园园2)1)(北京工业大学电子信息与控制工程学院北京100124)2)(浙江农林大学信息工程学院杭州311300)摘要为克服雾天能见度检测仪价格昂贵、检测范围小等缺点,该文结合雾天光线传输模型与摄像机几何光学模型提出一种交通能见度估计算法.该算法通过动态标定交通摄像机内外参数来计算路面区域内点到摄像机的距离,利用场景透射率得到大气消光系数并估计交通能见度.首先,算法基于活动图提取感兴趣区域,再根据区域内平均像素拟合曲线是否满足刃边函数对雾天进行识别;其次,通过暗原色先验原理估计交通场景中每一个点的透射率,并且选取道路上4组透射率差值最大的点对摄像机内部参数标定;然后,提取消失点及车道边缘线完成摄像机外部参数动态标定;最后,通过路面上点到摄像机的距离以及相应的场景透射率估计交通场景的能见度.该文将雾天多个交通场景下能见度值计算结果与人工观测、物理仪器测量等方法进行了比较,结果证明了该文方法的有效性与实时性.关键词交通能见度;摄像机动态标定;雾天光线传输模型;感兴趣区域;暗原色先验原理1引言能见度与地面交通安全密切相关[1].雾霾天气条件下交通场景能见距离大大降低,驾驶员容易对场景距离过度估计,而交通能见度信息实时测量能够提醒司机调整车速从而避免交通事故.测量交通能见度通常可采用目测法,但该方法客观性较差;也可采用测量仪器,如激光能见度测量仪、大气透射仪,该方法应用较广,但设备普遍比较昂贵且测量范围较小;另外基于双目摄像机和单目摄像机的方法也逐渐被采用,该方法主要是利用计算机视觉技术对交通场景图像进行处理.基于双目摄像机的方法由于道路图像中的像素在颜色与纹理上相似性较强,因此匹配困难,算法运行时间较长,实时性不佳.基于单目摄像机的方法因其设备简单、检测范围大近年来被广泛采用.Bush等人[2]采用单目摄像机获取道路图像,并基于小波变换对图像边缘进行检测(对比度大于5%)来估算交通能见度;Hautiere等人[3-4]基于单目摄像机的交通监控技术实现对大气交通能见度距离的估计,并以实验验证了该方法的有效性;Hallowell等人[5]将场景对比度同视觉范围关联起来,通过附加传感器估计能见度;另外,结合sobel算子、高通滤波器等估算能见度的方法[6-7]也取得了较好的效果.摄像机标定技术在基于图像的参数测量方面起着关键作用[8].对于交通能见度的测量,目前有许多学者逐渐采用摄像机标定的方法.摄像机标定方法分为传统摄像机标定方法[9]、自标定方法[10]和主动视觉标定法[11].Todd等人[12]针对交通场景中的摄像机,提出一种基于大气散射理论的内外参数标定方法.该算法不需要借助于车道线等先验信息,通过估计道路边缘位置来校正摄像机.Hautiere等人[13]提出一种利用摄像机标定自动检测雾天能见度的方法.他们根据Koschmieder’s定律,结合国际照明委员会(InternationalCommissiononIllumination,CIE)给出的对比度阈值0.05估计能见度距离.相比其他方法该方法约束较少,仅利用车载单目摄像机以及天空和道路的部分场景图像就可以实现能见度距离的估计.但是该方法不适用于轻雾天气下的道路能见度检测.Gallen等人[14]结合摄像机标定与前向散射法提出一种适用于夜间环境下的雾天能见度距离静态估计方法.随后,Hautiere等人[1,15]利用摄像机标定与场景对比度分布提出一种基于非线性概率模型驱动的能见度估计方法.模型中考虑到场景中的朗伯表面,因此该方法对场景亮度变化具有很好的鲁棒性.之后该方法被应用于实际的高速路能见度检测,能见度估计距离可高达15km,平均误差为30%[16].光线传输模型将场景的距离信息、透射率以及消光系数(与能见度有关)联系在一起,本文结合雾天光线传输模型与摄像机几何光学模型对交通道路中的摄像机内外参数进行标定,进而利用场景透射率计算大气消光系数并估计交通能见度.特别地,本文在所构建的单目摄像机模型中考虑到均质雾天条件,使该方法无需借助于道路特征等先验知识且能够适用于非结构化道路环境.本文第2节介绍本文算法的基本假设与所提出的能见度估计模型;第3节通过活动图和区域搜索算法提取感兴趣区域以识别当前天气;第4节分析基于均质雾天的交通摄像机单目模型;第5节叙述基于暗原色先验原理的透射率计算方法、基于均质雾天的摄像机内外参数标定方法、标定点选择方法以及交通能见度计算方法;第6节给出摄像机内外参数标定结果和能见度检测结果,并与人工观测、物理仪器测量等方法做了比较;最后是结论与展望.2算法基本假设与能见度估计模型(1)算法基本假设根据实际情况对算法做出如下基本假设:①根据中国气象局的官方统计,均质雾天在所有天气类型中比例达到40%.因此,均质雾天是一种普遍的气象条件,本文算法应用面较广.②道路形状假设.使用道路形状假设可以有效简化道路模型,道路形状有直线、回旋曲线、抛物线及其他特殊的形状.本文采用直线道路模型.③道路平坦假设.本文将道路近似为平面,运动车辆均行驶在道路平面上,可简化图像坐标到世界坐标的转换.④道路特征一致假设.本文对所提取的感兴趣区域使用了纹理特征分析,需假设图像路面区域具有一致的特征.⑤考虑制作工艺等原因,对于大多数摄像机,摄像机内参存在如下关系αx=αy=α.Page3(2)能见度估计模型根据国际照明委员会(CIE)给出的能见度定义[17],气象能见度Rv与大气消光系数k满足以下公式1924年,雾天光线传输模型被Middleton[18]提出,光线传输过程见图1.模型中指出雾天时视觉传感器接收到的总辐射为入射光衰减后到达传感器的强度与进入成像系统的大气光强度之和.因此,观测到的光强度与物体固有光强度之间满足其中:x表示图像路面上的一点;I(x)是指该点观测到的图像光强度;J(x)是指物体该点固有光强度;A表示大气光强度;t(x)为点x场景透射率值,且t(x)满足其中:k为大气消光系数;d(x)为点x到摄像机的距离.因此得到大气消光系数k=-可以看出,场景中的距离信息、透射率和大气消光系数通过雾天光线传输模型联系起来.本文在此基础上根据提出的交通摄像机动态标定算法,计算出路面区域内点到摄像机的距离,利用场景透射率计算大气消光系数并最终估计交通能见度.本文所提出的能见度计算方法分为3个模块,包括均质雾天识别模块、摄像机标定模块和交通能见度估计模块,如图2所示.均质雾天识别模块根据活动图以及区域搜索算法提取感兴趣区域进而对当前天气进行识别;摄像机标定模块首先利用暗原色先验原理得到交通场景中各点的透射率,然后选择道路上的8个点对摄像机初步标定,最后提取消失点及车道边缘线对摄像机内外参数动态标定;交通能见度估计模块利用路面上点到摄像机的距离以及场景透射率计算大气消光系数进而估计能见度值.图2基于摄像机动态标定的能见度估计流程3基于区域选择的均质雾天识别均质雾天时由于道路、天空区域的纹理、光照的均匀统一性,自上而下地图像均质部分的像素以刃边函数(EdgeSpreadFunction,ESF)的形式变化(式(3)).利用这一规律,均质雾天识别模块通过提取图像纹理特征搜索感兴趣区域,然后统计该区域内平均像素的变化规律是否满足刃边函数,从而实现均质雾天天气识别.3.1背景生成与更新、纹理特征提取摄像机标定时,由于标定点的选取是在背景路面中进行的,因此首先需要生成与更新背景图像.背景生成与更新一般是通过判断图像特征变化识别背景与前景,常用的算法有平均帧差法、混合高斯模型法[19].考虑到时间消耗,本文采用平均帧差法得到交通场景的背景图像并实时更新.图3(a)、(b)、(c)为本文在快速路上采集的3种雾天交通场景下的原Page4始图像,图3(d)、(e)、(f)分别为对应的背景图像.为了提取图像均质部分区域(包括天空区域、道路区域),算法通过计算图像灰度共生矩阵提取纹理特征,包括熵(Entropy,ENT)、方差(Variance,VAR)、角二次距(AngleSecondaryMatrix,ASM)、逆差距(InverseDifferenceMatrix,IDM)和对比度(Contrast,CON).3.2基于图像纹理特征的区域搜索算法(1)根据场景活动图确定道路区域把给定时间内视频图像中运动和静止的物体分别用白色像素和黑色像素显示在一幅图上,该图称为活动图.活动图[20]通过检测交通视频图像中运动的车辆判定车辆位置,确定车道以及路面信息,进而获取运动区域A.该区域采用多帧取差求平均法其中:N为帧数;Ii为第i帧的像素.本部分首先生成场景活动图,然后通过区域生长算法(质心生长法)产生连续的道路区域.图4(a)为交通场景下的原始图像,图4(b)、(c)分别是(a)图对应的交通视频序列在第10、100帧时生成的活动图.其中路面为白色像素点,黑色像素点是交通场景中除路面外的其余部分.(2)感兴趣区域搜索典型交通场景的部分纹理图像如图5所示,本部分基于天空和路面图像纹理特征提取感兴趣区域,具体过程如下[21]:第1步.从下到上逐行检测交通场景活动图中的白色像素点,并将白色像素点行数最小值保存,设为L.考虑到道路交通图像中以路面和天空区域为主,因此,这一步通过检测白色像素点以初步确定候选天空区域,其纵坐标范围在0~L,则候选路面区域纵坐标范围在L+1~240.第2步.通过上一步可确定候选天空的范围,但在实际交通图像中,该范围内大多包含房屋、灯、树木等,因此必须去除上述噪声点提取准确的天空区域.处理步骤为:选择8×6图形窗,按照像素从上到下逐行扫描,并计算每个图形窗的纹理特征,将其与天空模板比较,存储纹理差最小的图形窗,算法扫描到L行结束.用Ai表示第i行搜索的窗口区域,Pi为对应窗口区域内像素的平均值,i=1,2,…,L.第3步.在候选路面区域内(纵坐标为L+1~240),对活动图上出现的白色像素点每行随机选择一个8×6窗口区域,记为Aj,Pj为Aj区域内像素的平均值,j=L+1,L+2,…,240.3.3曲线拟合与天气识别结果通过以上步骤选取了感兴趣区域,该区域内包含天空区域与路面区域.在感兴趣区域内逐行计算平均像素,根据像素变化是否符合刃边函数判定当前天气状况是否为均质雾天.本部分实验采用的视频序列均拍摄于北京城市快速路上的固定摄像机.算法经过不同天气、不同场景视频图像测试,涵盖晴天、雪天、均质雾天(2个交通场景)3种天气条件.如不特别说明,本文图像尺寸均为320×240像素.图6给出了晴天、雪天和均质雾天的背景以及对应的感兴趣区域搜索结果,图中显示的该区域均已被放大.其中,均质雾天下选取了2个不同场景.感兴趣区域内像素平均值变化的拟合曲线如图7所示.从图7(a)可以看出,晴天时感兴趣区域的拟合曲线多处出现跳跃、不连续现象.这是由于晴天条件下光照分布不均匀,近处物体获得的光照可能大于远处物体的光照.从图7(b)可以看出,雪天时感兴趣区域的拟合曲线仍然出现不连续现象,像素变化无明显规律,这是由于雪天时虽未明显受到太阳光的影响,但由于雪在路面上分布不均匀致使路面光照反射不均匀,像素变化杂乱无章.从图7(c)、(d)可看出,均质雾天下,虽然在不Page5图63种天气条件下背景图像及所提取的感兴趣区域图7感兴趣区域内像素的曲线拟合结果同的交通场景下,拟合曲线都是连续的刃边函数形状曲线,但是曲线拐点所在的位置不同.采集两种场景下的雾雨混合天气交通视频,将视频图像中的雨滴作为噪声点,用以验证算法的鲁棒性.两种雾雨混合天气场景下的背景图像以及提取的相应感兴趣区域如图8所示.图9为对应场景下的拟合曲线.根据图9显示结果可以看出,雾雨混合天气下所提取的感兴趣区域拟合曲线由于受到雨滴等因素的干扰,像素波动相对于图7而言较大,但仍近似以刃边函数的形状变化.实验结果表明本文提出的区域搜索算法具有较好的鲁棒性,通过感兴趣区域识别天气状况的应用范围较为广泛.图8场景3雾雨混合天气下背景及相应区域提取结果Page64交通摄像机单目模型如果只依靠单目摄像机,通过交通场景图像中的点不能直接获得图像的深度信息,但当道路中的点假设在一个平面上时,同时参考场景中的先验信息(如车道线等)可获得图像道路上点的深度信息[9].本节结合摄像机线性模型提出一种交通摄像机单目模型.4.1摄像机传统坐标系(1)图像坐标系u-v在图像平面上建立的以像素为单位度量的坐标系统.(u,v)表示图像坐标系坐标,u,v分别对应每个像素的行数和列数,且均为整数.(2)成像平面坐标系x-y区别于图像像素平面坐标,成像平面坐标以物理单位度量,如厘米.x,y轴分别平行于u,v轴,每个像素在x,y方向上的物理尺寸用dx,dy表示.摄像机光轴与图像平面的交点为O1,通常在图像中心位置,如图10所示.(3)摄像机坐标系Xc-Yc-ZcXc轴与Yc轴分别平行于成像平面坐标系的x轴与y轴.Zc轴为摄像机光轴,且垂直于图像平面.Oc表示坐标系原点,即摄像机光心.(4)世界坐标系Xw-Yw-Zw世界坐标系是场景中用来描述摄像机位置的基准坐标系,同时可用来确定物体相对位置.为简化坐标变换公式,具体在应用时可选择环境中一些具有特定意义的点作为世界坐标系的原点.摄像机传统坐标系如图10所示.摄像机线性模型(针孔模型)实现了世界坐标与图像坐标的转换.4.2均质雾天下交通摄像机单目模型在摄像机线性模型基础上,作者给出一种均质雾天下的交通摄像机单目模型[21].设场景中摄像机距离路面的垂直高度h,车道边缘线L1,L2,摄像机俯角φ(垂直偏转角度),偏角θ(摄像机所在铅垂面与车道边缘线夹角).世界坐标系原点为摄像机光轴与路面的交点,Yw轴为摄像机所在铅垂面与路面的交线,Zw轴垂直于路面.摄像机光心到世界坐标系原点距离表示为F,且F=h×cscφ.如图11所示.Page7另外,为简化模型,在摄像机传统坐标系基础上增加U-V-W坐标系.该坐标系中U轴平行于Xw,V和W轴是通过Zw和Yw轴旋转φ角得到的.图像像素坐标与世界坐标之间的关系满足其中,ax=f/dx以及ay=f/dy均为摄像机内参,(u0,v0)为图像像素坐标系原点位置,(u,v)是图像像素坐标系内的任意一点.通过U-V-W坐标系与Xc-Yc-Zc之间的旋转关系,可以得到摄像机坐标与世界坐标存在如下关系:结合上述公式整理得到Yw=将式(8)代入式(7),求出ZcZc=sinφ×[sinφ×αy-cosφ×(v-v0)]5基于均质雾天的摄像机参数校正摄像机标定模块首先利用暗原色先验原理计算场景中各点的透射率,并结合交通摄像机单目模型给出参数α、u0、v0以及φ的标定公式;其次选取路面上具有特定透射率的8个点初步标定摄像机内部参数;最后提取车辆运行轨迹得到消失点,利用车道边缘线实现摄像机外部参数动态标定.5.1基于暗原色先验原理计算场景透射率暗原色先验原理是He等人[22]根据大量图像数据统计得出的一种经验性规律.该原理表明户外无雾图像在非天空的局部范围内,至少在RGB中一个颜色通道上有一些灰度值很低的像素.对于图像J,点x处的暗原色定义为其中:c∈{R,G,B};x,y是图像上的坐标点;Jc(y)表示点y上J的某一颜色通道;Ω(x)表示以点x为中心的一块方形区域.在雾霾天气条件下,由于图像被雾干扰而受到大气白光成分的充斥,导致这些暗像素的强度值变高.因此,雾天退化图像中的暗像素可以直接用来估计雾天光线的透射率.本部分利用暗原色先验原理计算场景透射率,透射率t(x)为对于图像的天空部分暗原色先验一般不适用,而雾天天气条件下的天空光照贴近于大气光强度A,满足将式(12)代入式(11)得出天空区域的透射率趋于0.因此式(11)对天空区域与非天空区域均适用.本文对3个不同雾天场景下的背景图像,按照暗原色原理(式(11))分别求取图像上各点的透射率,利用SoftMatting(软抠图)[22]优化后将透射率映射到[0~255]范围内,透射率显示结果如图12所示.图123个场景下的原始图像与相应的透射率图像5.2校正摄像机内参数α,狌0,狏0和φ5.2.1摄像机内参数校正摄像机内参数校正方程的具体推导过程详见附录,本部分仅说明校正方程生成的原理.路面区域点到摄像机的距离d(x)可由该点的摄像机坐标得到由式(3)通过变换得出路面上任意一点到摄像机的距离d(x)=-lnt(x)/k.对于场景中的任一点,由于消光系数k是未知的,只通过式(3)中的距离d(x)仍不可求.为减少未知数,选取路面上的两个点x1和x2消去大气消光系数k,整理得到u1-u(u2-v(Page8其中,式(14)是一个二次方程混合三角方程.方程中的参数α,u0,v0和φ均未知(四元方程),较难求解.文中通过选取具有特殊透射率值的路面区域点以简化方程求解.具体方法是按照一定的约束同时选取图像上的4个点将方程的二次项部分消掉,从而把二次方程转化为一次方程.其中,约束条件为所选取的4个点必须满足两两位于路面区域的同一行,同时结合局部纹理特征以排除噪声干扰.设搜索得到的4个点分别为x1=(u1,v1),x2=(u2,v2),x3=(u3,v3)以及x4=(u4,v4),根据约束关系有v1=v2,v3=v4,则同一行的2个点分别有(u1-u0)2+v1-v((u3-u0)2+v3-v(令lnt(x1)[lnt(x2整理得到如下一次方程2(u3-Bu4)[1-B为了求出参数u0,v0需要构建2个一次方程,类似地在图像其余场景点内重新选择两对位于道路区域同一行的4个路面坐标点,假设此时所选择的这4个点分别为x5=(u5,v5),x6=(u6,v6),x7=(u7,v7)和x8=(u8,v8),且满足约束v5=v6,v7=v8,得到2(u7-Du8)[1-D其中,C=(17)、方程(18)求出摄像机参数u0,v0后,将结果代入式(14)、(15),可计算出其他摄像机参数.5.2.2标定点选取与摄像机标定过程选取路面上具有特定透射率的8个点,分4步校正摄像机内参数.(1)计算参数u0,v0首先对路面区域上每一行选取两个透射率差最大的点对,根据本文第3节区域搜索算法可知,路面区域在图像上从L+1~240行.为保证算法的鲁棒性,本文进一步对每一个选取点做噪声检测,以去除路面上可能存在的车道线损坏等干扰噪声,具体方法为:①选取点3×3小范围内计算纹理特征并匹配,不满足条件则丢弃.②选取点3×3小范围内计算像素平均灰度值并匹配,不满足条件则丢弃.路面图像经过自顶而下的搜索之后,产生点对集合,设为(Pi,1,Pi,2),其中i=L+1,…,240.其次,在所搜索的点对集合中选取4对透射率差值最大的点,分别设为(x1,x2),(x3,x4),(x5,x6)和(x7,x8).将点对坐标值以及相应透射率值代入式(17)和式(18),计算参数u0,v0.(2)校正摄像机内参α从点对(x1,x2),(x3,x4),(x5,x6)和(x7,x8)中选择一对具有最大透射率差的点,将点对坐标值以及相应透射率值代入式(15)中,求解参数α.(3)校正摄像机内参φ从上述8个点中选取两个点,路面上这两点满足纵坐标不同且透射率值相差最大.把这两点的坐标和计算出的透射率值代入式(14)中,求解参数φ.(4)完成摄像机内参数初步校正最后,对视频序列每帧图像计算内参u0,v0,α和φ,取平均值后作为摄像机内参标定结果.摄像机内参标定完成后,采用坐标变换公式可将图像坐标系中路面上的点对应转换到摄像机坐标,转换公式如下Xc=sinφ×α-cosφ×(v-v0)Yc=sinφ×α-cosφ×(v-v0)Zc=sinφ×[sinφ×α-cosφ×(v-v0)]-5.3校正摄像机外参数犺,θ和犱本节首先选择路面区域中的两个点计算摄像机高度h;然后通过Camshift算法跟踪车辆的运行轨迹估计场景消失点xe,结合场景活动图得到与摄像机距离最近的车道边缘线L1.在图像坐标系中计算L1与X轴的交点xL的横坐标u(xL),通过xe和u(xL)计算摄像机水平转角θ和与路边的间距d.(1)摄像机安装高度h基于本文第3节得到的活动图和路面纹理特征进一步选取路面上的两个点p1,p2,根据式(2)变换Page9得出场景距离,结合式(13)计算得到lnt(p1)lnt(p2)在已求得摄像机参数α,u0,v0以及φ的情况下,式(20)仅含有一个未知数h,通过多帧取平均即得到h值.(2)基于车辆行驶轨迹的消失点提取在非结构化道路中往往无法通过车道线提取消失点.由于任意两条平行直线在无穷远处相交于消失点,而实际行驶中大多数车辆运行的轨迹近似于平行直线,可以在背景提取的基础上通过Camshift算法跟踪车辆轨迹提取消失点.Bradshi[23]提出了连续自适应均值漂移算法(ContinuouslyAdaptiveMeanShift,Camshift),可自适应跟踪并调整目标窗口大小,实现了目标的快速定位.基于Camshift算法的消失点提取流程见图13.图14给出了基于Camshift的轨迹提取、消失点估计结果.图14基于Camshift的轨迹提取、消失点估计结果(3)校正摄像机转角θ根据第2节均质雾天摄像机单目模型得到消失点xe(ue,ve)横坐标表达式则摄像机参数θ将消失点横坐标ue、摄像机内参数α,φ代入式(22)得到转角θ.(4)校正摄像机与路边的间距d图15为摄像机在路面的垂直投影与车道边缘线L1,L2的鸟瞰图,其中L1,L2与Xw,Yw轴分别相交于Q1,Q2,Q3和Q4.根据几何关系有Yw(Q1)-Yw(Q2)=Xw(Q2)cotθ,结合摄像机单目模型,得到u(Q2)=u0+其中,u(Q2)表示点Q2在图像像素平面的横坐标.由式(23)知,通过求解u(Q2)可估计摄像机与路边的间距d值.首先估计车道边缘线L1,L2的位置,在场景活动图中沿底部统计任一点与消失点xe连线上的像Page10素灰度.将数据归一化到[0,1],通过低通滤波器滤波,得到一个以u为自变量的信号序列,如图16所示.统计信号序列所有强度大于0.5的波峰,如果存在N个波峰(up),则必存在N-1个波谷(uv),记波峰与波谷车道线间距为Δu,uv的平均值为u-v估计各车道线区域和车行道边缘:u-u^v(i)=u-其中,当i=0,i=Nv+1时,式(24)分别表示车道边缘线L1,L2,相应的车道区域划分结果如图17所示.为计算交点Q2的横坐标,需要求出L1与Xw轴在图像平面上的方程,联立方程求出未知数:其中,v=v0,v=示Xw轴与路边线L1在图像平面上的方程.通过式(25)求解u(Q2),结果代入式(23)得到摄像机到路边的距离d.综上可知,摄像机的7个内外参数:u0,v0,α,φ,h,θ以及路边距d均已求出.6实验结果与分析实验采集快速路上的3个典型均质雾天视频(视频由路边摄像机拍摄,拍摄时间在上午9点到10点),3个视频序列依次标记为V1,V2,V3,对应摄像机(摄像机内参相同、外参不同)分别为C1,C2,C3.3个视频各包含1000帧图像,且视频图像尺寸统一规范化为320×240像素.图18给出了3个视频序列第800帧时的原始图像.由于摄像机标定算法需采用背景图像,而背景生成与更新往往存在时延,本文实验中摄像机参数的标定和能见度估计都在第500帧之后进行.本文算法运行环境的处理器为英特尔奔腾4内存2.0GHz的PC机,采用VC6.0与OpenCV混合编译环境.验证实验分为3个部分:第1部分是根据所提出的算法获取视频V1,V2和V3各自第800帧图像路面区域内的4组标定点以及对应生成的4个摄像机标定方程,并给出摄像机7个内外参数的标定结果;第2部分是将本文算法与Todd[12]、郭凡等人[24]提出的标定算法进行比较以验证本文算法的有效性;第3部分是将标定结果应用到雾天能见度检测上,计算得到3个视频的能见度序列第500~1000帧的统计结果,并将结果与能见度仪检测的数据做比较.设提取的8个满足约束关系的标定点为xj中i=1,2,…,8;j=1,2,3,分别对应V1,V2,V3.表1、表2、表3给出了3个视频序列在第800帧时的图像标定点搜索结果;利用标定点搜索结果确定的摄像机标定一次方程、二次方程和三角方程见式(26)~(28);表4给出了3个摄像机C1,C2和C3在视频第800帧时摄像机内外7个参数的标定结果,表5给出了视频序列在第1000帧结束时参数均值化后的结果.图19给出了3个视频序列中的车道分界线提取结果.选取点x11x12x13x14Page11选取点x21x22x23x24选取点x31x32x33x34表4本文方法第800帧犆1,犆2,犆3各参数标定结果第800帧u0C1C2C3表5本文方法第1000帧参数标定平均结果(多帧平均)第1000帧u0C1C2C3将本文算法与Todd算法、郭凡算法提出的标定算法分别进行对比研究.郭凡所提出的基于消失点的摄像机标定方法需要已知车道线信息,但在大雾天气条件下车道线不易检测,因而该算法具有一定的局限性,不适用于非结构化的道路环境.Todd算法虽然适用于非结构化道路,但因摄像机模型中未引入均质雾天,计算过程较为繁琐.表6给出了这两种算法在3个视频的第800帧标定结果.算法Todd(C1)14311812.03726.238.10.48Todd(C2)15212111.23666.348.10.59Todd(C3)1491219.23676.137.10.58郭凡(C1)14811610.23685.539.10.47郭凡(C2)1481208.43695.440.90.57郭凡(C3)15011913.03666.538.30.54从表5、表6可以看出,本文所提出的基于均质雾天的摄像机标定算法与Todd算法、郭凡算法计算结果相近,比较结果验证了算法的有效性.图20给出了3个不同的视频序列在3种不同算法(图中分别用不同曲线表示)下的摄相机内外参数u0,v0,α,φ,h,θ以及路边距d的统计结果.其中,每一个参数统计结果从左到右3幅图所采用的视频序列号依次为V1,V2,V3.每一幅子图横坐标表示视频帧的数目(从第500帧到第1000帧),纵坐标表示相应参数的校正结果.从图中可以看出,本文算法与其他两种算法相比标定结果较为稳定.本文利用标定后的摄像机计算路面区域内点到摄像机的距离,结合场景透射率计算大气消光系数并估计雾天道路交通能见度.由于每一帧图像可以计算若干个能见度,为了增强能见度计算的准确性,本文将数据取平均后作为当前帧图像的能见度计算结果,多帧图像能见度值取均值最终作为当前视频下的交通能见度的估计值.统计3个视频从第500帧到第1000帧的能见度值计算结果,并将结果与基于人工观测的数据和基于能见度仪采集的数据(作为基础数据)做了比较.虽然这两种估计交通能见度的方法是目前主流方法,但是存在观测结果主观性大、设备昂贵、检测范围小等缺点.图21(b)、(d)、(f)中实线表示本文算法得到的能见度检测结果与能见度仪数据的误差,虚线表示人工观测数据与能见度仪监测数据的误差.从图中可知,本文算法交通能见度估计准确率高于90%,满足能见度检测要求.另外,本文算法运行环境中处理器为英特尔奔腾4内存2.0GHz的PC机,对一帧320×240像素图像处理,雾天识别、摄像机参数标定和能见度估算3个模块各耗时10ms、15ms、5ms,共30ms(0.03s),满足视频实时性处理要求.Page12图20本文算法、Todd算法、郭凡算法3个均质雾天场景下摄像机参数标定结果比较Page13图21基于雾天摄像机校正的能见度检测结果(第800帧)7结论与展望本文结合雾天光线传输模型与摄像机几何光学模型动态标定交通摄像机内外参数,通过计算路面区域点到摄像机的距离,利用场景透射率计算大气消光系数并估计交通能见度.文中通过活动图和区域搜索算法提取感兴趣区域并判断出当前天气是否为均质雾天.本文将算法中摄像机动态标定模块与Todd、郭凡提出的标定算法进行了比较研究,统计了3种算法下3个不同的交通视频序列的能见度值,结果表明本文算法在非结构化道路环境下进行能见度检测的优越性.最后把能见度估计结果与能见度仪检测的数据进行了比较分析,从对比结果可以看出,本文算法所计算的交通能见度准确率高于90%,能见度估计误差在±10%以内,达到了检测的要求;在算法速度方面,一定条件下达到了视频实时性处理要求.因此,本文算法具有造价低、处理速度快、动态适应性强等优点,适用于雾天交通场景下的能见度实时检测,在视频交通参数检测方面具有广阔的应用前景.
