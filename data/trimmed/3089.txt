Page1求解非线性回归问题的Newton算法韩敏王亚楠(大连理工大学电子信息与电气工程学部辽宁大连116024)摘要针对大规模非线性回归问题,提出基于静态储备池的Newton算法.利用储备池搭建高维特征空间,将原始问题转化成与储备池维数相关的线性支持向量回归问题,并应用Newton算法求解.鲁棒损失函数的应用可抑制异常点对预测结果的干扰.通过与SVR(SupportVectorRegression)及储备池Tikhonov正则化方法比较,验证了所提方法的快速性、较高的预测精度和较好的鲁棒性.关键词支持向量回归;静态储备池;Newton算法;鲁棒性1引言回声状态网络(EchoStateNetwork,ESN)是一种发展迅速的新型递归网络,具有结构简单、训练速度快、可提前确定稳定性等优势.ESN由输入层、中间的递归网络层和输出层构成,各层之间通过权值相连.输入和输出间的部分又称储备池,网络的许多特性都通过储备池实现.储备池状态本质上为输入信号的高维显现,表征输入信号的特性[1].当储备池的内部连接权值全部为零时,ESN演化为一种前向网络———极限学习机(ExtremeLearningMachine,ELM)[2].ELM的结构和性质与ESN极其相似,但较ESN更适用于处理静态问题.文献[3]在ESN网络储备池的基础上,提出SVESMs(SupportVectorEchoStateMachines)模型,利用ESN网络储备池搭建特征空间,并在其中运用线性支持向量回归(SupportVectorRegres-sion,SVR)技术,对混沌时间序列进行预测.但是,对于如何结合储备池的特性对模型进行优化求解,文章并未进行深入分析.而目前得以广泛应用的SVM分解算法,如SMO(SequentialMinimalOpti-Page2mization)算法对线性SVM的训练速度较慢,且训练时间随超参数C的增大成比例增长[4],无法体现出储备池将非线性问题线性化的优势.Newton算法具有至少二阶的超线性收敛速度,适合求解线性优化问题.Mangasarian等人将Newton算法用于线性分类SVM,显示出很强的运算优势[5].Chapelle提出了针对非线性分类问题的原始SVM递归有限Newton算法,算法的运算复杂度与常规非线性SVM分类方法相同[6].薄列峰等在已有研究之上,将Newton算法推广到非线性SVR问题的求解上,得到和常规分解算法相似的结果[7].本文在前人研究的基础上,将支持向量回归思想应用于静态储备池中,并结合Newton算法对非线性回归问题求解.该方法首先通过储备池将复杂非线性回归问题转化成与储备池维数相关的线性优化问题,然后运用Newton算法求解,并采用Wolfe-Powell法搜索Newton迭代步长.构造一个分段多项式函数,将它和开平方函数一起作为静态储备池的鲁棒损失函数.数值实验结果表明,所提算法在训练速度上优于常规SVM方法,并具有较好的鲁棒性.2静态储备池模型ELM的网络方程如下:其中,狌、狓分别为输入向量和储备池状态向量,犠in是输入权值矩阵,犫in、b代表偏置,狑为输出权值向量,y为网络输出,下标i=1,…,N表示第i个样本,N为样本总数.输入矩阵犠in和偏置向量犫in的值在初始时刻随机给定,网络训练的目的是求输出权值狑(含偏置b).令犡=[狓1,狓2,…,狓N]T,期望输出为狔d=[yd1,yd2,…,ydN]T,取代价函数为min狑∈其中,C∈Tikhonov方法求得输出权值为[3]运用对偶原理,可得式(3)的对偶式为[6]相应的对偶解为不论是通过求取代价函数得到最优输出权值狑,还是求其对偶形式得到最优Lagrange乘子α,最终得到相同的预测结果.在传统的SVM中,由于犡=[Φ(狌1),Φ(狌2),…,Φ(狌N)]T,Φ(·)是将输入从原始空间映射至Hilbert空间中的一未知映射.由于犡状态未知,SVM需通过求内积,即应用核函数计算出Gram矩阵犡犡T,得到式(6)中的α,进而求得预测输出.选择不同的核函数,相当于选择不同的映射Φ(·)或不同的内积.由于式(6)中α的运算复杂度高达O(N3),计算量随样本规模增大而变大.这就是在数据规模较大时,SVM运算速度慢的原因.在ELM中,犡为储备池状态矩阵,可通过式(1)计算得到.相比于SVM,可将ELM网络的储备池映射看作一种特殊的映射Φ(·),并在储备池空间运用线性支持向量回归技术.这正是本文算法应用的基本思想.在储备池中,由于储备池状态可计算得到,能够通过计算输出权值狑来得到预测输出.从式(4)可以看出,计算狑的运算复杂度为O(d3)(d一般在100~1000之间).因此,在数据规模很大时,储备池方法仍具有较快的训练速度.3静态储备池模型Newton算法本节将Newton算法应用于静态储备池的求解中.将式(3)推广可得代价函数的广义形式如下其中,f表示任意损失函数.在式(3)中,f为二次损失函数.此时,运用Tik-honov算法,通过矩阵Cholesky分解快速求得最优权值.但是,当数据含有噪声特别是存在异常点时,二次函数会带来误差的超线性增长,对训练产生较大干扰,对预测结果具有不利影响.为了提高模型的抗干扰能力,需选用新的鲁棒损失函数.3.1鲁棒损失函数为降低异常点对预测结果的干扰,所选损失函数在误差较大时应为线性或近似线性的增长.同时,Newton算法要求损失函数为二次光滑的凸函数.而损失函数本身应为非负偶函数.综上所述,所选用的损失函数应满足以下条件:(1)f(x)为偶函数,且恒有f(x)0;(2)f(x)是二次光滑条件的凸函数;(3)在误差较大时,f(x)呈线性或近似线性增长.Page3遵照上面3个要求,本文选用一个二次分段光滑多项式函数和开平方函数一起作为静态储备池模型的损失函数.(1)多项式函数在文献[8]的基础上,构造出一个分段二次光滑多项式函数,如下式f(x)=其中,h>0.式(8)显然满足条件(1)和(3),下面,证明其为二次光滑凸函数.引理1.若f(x)在开凸集S上具有二阶连续偏导数,则f(x)为S上凸函数的必要条件是:f(x)的黑塞矩阵2f(x)在S上处处半正定[9].命题1.式(8)所示的函数f(x)一定是二次光滑凸函数.证明.由于函数f(x)在各分断区间内均为二次光滑,且在分段点±1f±1()h=1f-1()h=-1,f(0)=0,f1()h=12f±1()h=2f(0)可知f(x)在(-,+)为二次光滑.又因为烆烄烅2f(x)=且当-10,当0<x<1根据引理1可知,函数f(x)定为凸函数.综上所述,结论得证.(2)开平方函数开平方损失函数定义如下τ>0.开平方函数是在二次损失函数基础上,通过添加正的常数项后开方得到的.容易看出,它满足本文鲁棒损失函数选择的所有条件,并在误差较大时呈近似线性增长.图1为ε-不敏感损失函数、开平方函数、分段多项式函数和二次损失函数曲线图.从图中可知,在误差较大时,二次损失函数取值迅速增大,而其它3种函数呈线性(或近似线性)增长.同时,由于ε-不敏感损失函数为不连续,无法用Newton算法进行求解.3.2Newton算法具体步骤鲁棒损失函数确定后,应用Newton算法求解模型最优输出权值狑.Newton下降方向计算公式为在式(10)中,2Lp(狑k)为黑塞矩阵,Lp(狑k)为方向导数,k为迭代步长.Newton下降方向确定之后,应用Wolfe-Powell线性搜索法计算出迭代步长,它能够在获取有效步长的同时,兼顾算法的快速性.基于静态储备池的Newton算法具体步骤如下:δ1<δ2<1.同时,选取适当小的ε1>0,令k=0.1.赋初值.任意给定向量狑1及δ1,δ2,满足0<δ1<0.5,2.k··=k+1,计算黑塞阵2Lp(狑k),其必为对称正定.用Cholesky分解法从式(10)中求得犱k.3.判断犱kε1·max(1.0,狑k)是否满足.满足则求得最终解狑=狑k,计算结束.否则转步4.4.得到犱k后,用Wolfe-Powell算法确定迭代步长.取函数(α)=Lp(狑k+α犱k),令αk=1,若满足(αk)=(Lp(狑k+αk犱k))T犱k,转步8.不满足则转步5.5.任取β>0,ρ,ρ1∈(0,1).先取α(0)±1,±2,…}中满足式(11)中第1个不等式的最大值后,再Page4令i=0.6.若α(i)步8;否则取β(i)7.取α(i+1)±2,…}中满足式(11)中第1个不等式的最大值.令i··=i+1,转步6.8.令狑k+1··=狑k+αk犱k,转步2.4仿真实例为验证本文方法的有效性,将其应用于10组基准数据①的实验仿真中,数据见表1所示.并与SVR和Tikhonov方法进行比较,以验证所提方法的快速性.同时,将本文方法和Tikhonov方法分别用于两组含异常点的数值实验中,以验证所提方法的鲁棒性.实验结果表明,本文所提方法具有较快的训练速率、较高的预测精度和较好的鲁棒性.数据类型AutoPriceBreastcancer10094320TriazinesMachineCPU10010960AbaloneDeltaailerons3000412950Cpuactivity40004192120Deltaelevators4000551760Calhousing80001264080Census4.1基准数据举例将本文方法应用于十组基准数据仿真中,并与SVR[2]和Tikhonov方法比较.SVR和静态储备池的模型复杂度对照见表2所示.AutoPriceBreastCancerTriazinesMachineCPUAbaloneDeltaaileronsCpuactivityDeltaelevators260.38CalhousingCensus下面将根据数值实验的结果从效率和预测精度两方面对3种算法进行简要评价.效率主要指训练时间,所用的时间越短则效率就越高.而预测精度则选用均方根误差ERMSE进行定量说明其中,S为测试样本个数,Oi是实际观测值,Pi为对应的预测输出值.ERMSE越小则精度越高.3种方法的实验仿真结果如表3、表4所示.表3Tikhonov、SVR和Newton算法训练时间对照表数据AutoPrice0.00120.00420.00150.0018BreastCancer0.00060.00640.00060.0013Triazines0.00160.00860.00190.0019MachineCPU0.00120.00180.00240.0015Abalone0.01151.61230.02500.0435Deltaailerons0.01650.67260.06400.0992Cpuactivity0.14161.01490.64352.0120Deltaelevators0.09881.1210.33210.5058Calhousing0.196074.1840.99132.2597Census表4Tikhonov、SVR和Newton算法测试犈RMSE对照表数据AutoPrice0.09440.09370.09630.0958BreastCancer0.26230.26430.26220.2678Triazines0.20040.18290.19900.2000MachineCPU0.5350.08110.05330.0564Abalone0.07680.07840.07710.0768Deltaailerons0.03910.04290.03900.0392Cpuactivity0.03450.04700.03470.0344Deltaelevators0.05310.05400.05330.0531Calhousing0.12690.11800.12680.1274Census在表3、表4中,加粗的数字表示性能最好.从表3可以看出,3种方法的训练时间随样本规模的增大表现出较大差异.其中,Tikhonov算法的训练时间最短,且随数据样本的增加略微增长.Newton算法的训练时间也较短,随着数据样本和储备池维数的增加而增长.SVR的训练时间最长,训练速率随着样本数增多而显著变慢.而从表4可以看出,对不同的数据,本文所提算法均具有较高的预测精度.对实验结果进行简要分析.由于Tikhonov方法只需进行一次Cholesky分解即得最优解,所以它的训练速度最快.而静态储备池Newton算法将非线性①http://www.niaad.liacc.up.pt/~ltorgo/Regression/ds\_Page5问题转化为与储备池规模相关的线性无约束优化问题,且储备池维数远小于数据样本个数,所以在样本量较大时仍具有很高的训练速度.而SVR的SMO算法,本质上是对Lagrange乘子α进行运算,计算复杂度为样本规模数的三次方,因此,当数据样本较多时,SVR的训练时间显著增长.4.2Newton算法鲁棒性评估在实际中,由于噪声或外界干扰的出现,经常会遇到训练数据包含异常点的情况.所谓异常点是指与既定模型输出偏离较大的数据点.它的存在会使训练结果与期望值之间产生较大偏差.因此,对异常点抑制能力的大小,实际上反映了算法鲁棒性的强弱.本小节,将本文的Newton算法和Tikhonov算法分别应用于两组含异常点的数据仿真中,并对结果进行简要评价.4.2.1SinC函数SinC函数定义如下[10]在[-15,15]区间的SinC曲线上随机取300个数据点作为训练样本,并在相应的输出上添加标准差0.1的高斯白噪声.另取(-11,2.5)和(0,-1.5)两点作异常点,将它们和前面300个数据点一起用作训练样本,如图2所示.分别将Newton算法和Tikhonov正则化方法用于SinC函数逼近,并对3001个数据点进行测试,得到仿真结果如图3、图4所示.从图中可以看出,Tikhonov正则化算法的预测曲线在两个异常点附近与期望结果产生了较大偏离,而本文所提算法有效地克服了异常点的干扰,预测结果十分理想.4.2.2Motocycle数据Motocycle数据是统计学中较为常用的一组实际测试数据.由于其数据分布不规则,具有异方差性,在某种意义上可将其视为含异常点的数据[10].分别将Newton算法和Tikhonov算法用于Motocycle数据仿真中,训练样本和测试样本均为实测值,得仿真结果如图5所示.Page6从图5可以看出,在数据分布较密集的始端和末端,两种方法均实现了较为理想的逼近效果.而在数据点分布比较分散、不规则的中间阶段,Tikhonov方法的预测曲线受异常点的拉动影响较大,在[30,40]区间上出现一段明显的下移.而Newton方法的预测输出受异常点的影响相对较小.从SinC函数和Motocycle数据仿真结果中可以看出,鲁棒损失函数的选用有效地降低了异常点对训练过程的影响,使得本文算法具有较好的鲁棒性.5结论本文将鲁棒损失函数应用于静态储备池中,并运用Newton算法对复杂非线性回归问题进行求解.通过储备池将原空间的非线性数据关系转化为储备池空间的线性数据关系,由于储备池状态可计算得到,可运用最优化算法对储备池输出权值进行优化求解.鲁棒损失函数的运用可有效抑制异常点的干扰,使得本文方法具有较强的鲁棒性.将所提方法应用于十组基准数据的实验仿真中,并和Tikhonov方法及SVR进行比较.实验结果表明本文所提方法在处理大规模数据问题时,具有较高的训练速率和预测精度.同时,通过对两组含异常点的数据进行实验仿真,验证了所提方法较好的鲁棒性能.致谢作者感谢各位审稿人提出的宝贵意见及为此文付出劳动的编辑部同志们!BackgroundThisworkissupportedbyNationalNaturalScienceFoundationofChina(60674073),NationalHighTechnologyResearchandDevelopmentProgram(863Program)ofChina(2007AA04Z158),NationalKeyTechnologyR&DProgramofChina(2006BAB14B05)andNationalBasicResearchPro-gram(973Program)ofChina(2006CB403405).Theyaimtoefficientlysolvethelargescalenonlinearregressionproblem.
