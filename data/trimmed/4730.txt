Page1基线与增量数据分离架构下的分布式连接算法樊秋实周敏奇周傲英(华东师范大学数据科学与工程研究院,上海高可信计算重点实验室上海200062)摘要在大数据背景下,数据库系统表连接操作的效率急需优化,尤其对于基线与增量数据分离的数据库系统来说,其连接操作更是成为其性能的主要瓶颈.为了有效提升事务处理的性能,在基线与增量数据分离的数据库系统架构中,通常将基线数据存储于磁盘中,增量数据存储于内存中,进而获得较高的事务处理吞吐量和可扩展性.Hbase、BigTable、OceanBase等系统是典型的基线与增量数据分离的数据库管理系统,但是他们的表连接效率较低,其主要原因包括:每次表连接前必须先合并基线数据和增量数据;数据存储模式更为复杂,导致过大的网络开销.该文提出了一种基线与增量数据分离架构下的排序归并连接优化算法.该算法对连接属性做范围切分,在多个节点上并行做排序归并连接.该算法无需在连接前合并基线数据和增量数据,进而实现对基线和增量数据并行处理,同时也避免了大量非连接结果集数据的基线与增量合并操作.并在开源的数据库OceanBase上实现了该算法,通过一系列实验证明,该算法可以极大提高OceanBase数据库的表连接处理性能.关键词分布式连接;增量数据;并行处理;排序归并连接1引言随着大数据时代的到来,如何更好地管理和利用大数据已经成为普遍关注的话题.在2008年,《Nature》推出了BigData专刊①;2011年2月,《Science》也接着推出专刊:DealingwithData②.美国奥巴马政府在2012年提出了“大数据研究和发展倡议”,共投资了2亿美元,正式开始“大数据发展计划”,并计划利用大数据技术实现在科学研究、生物医学、环境等领域的突破.当数据的规模达到PB、EB或ZB的时候,传统关系型数据库将面临极大的考验.所以近几年NoSQL[1]数据库得到了迅速的发展,比如HBase[2],MongoDB[3]等,这些数据库解决了大规模数据集合多重数据种类带来的挑战,攻克了一些大数据应用难题.但是NoSQL数据库的数据模型比较简单,以牺牲数据库的高度数据一致性实现更高的数据库性能.因此对于数据一致性要求高的应用来说,传统的分布式关系数据库模型是一个更好的选择.例如谷歌公司最新公开的分布式关系型数据库F1,由前Facebook工程师创办的号称世界上最快的分布式关系型数据库MemSQL,还有已经全面支撑支付宝业务的分布式关系型数据库OceanBase[4].这些数据库在一定程度上解决了工业上对数据库的需求.连接操作是关系型数据库最重要的数据分析操作之一.以某国有银行历史库交易为例,所有的查询SQL中单表查询占11.3%,两张表连接的查询占65.2%,多表连接的查询占23.5%.如何在大数据量的情况下保证连接操作的正确性与实时性是一个很难解决的技术.这一问题对具有基线数据和增量数据分离架构的数据库来说更为突出.对于该架构的数据库来说,基线数据是存储在磁盘上的数据,增量数据采用类似于Log-StructuredMergeTree[5]的架构,在内存中存储一段时间内对数据库的修改数据,当修改增量到达一定大小时,增量数据会被物化到磁盘上.与很多基于Log-StructuredMergeTree架构的数据库(例如LevelDB,HBase,Cassandra等)相似,这种架构的数据库能够极大地提高事务的处理性能,并且具有良好的可扩展性.但是它的缺点是在做连接操作之前必须先把基线数据和增量数据合并,这通常会导致连接操作的效率低下.OceanBase就是典型的基线与增量分离架构的分布式关系数据库,在超大表做连接的情况下,OceanBase的处理时间是在分钟级别的.对于一个实时的系统来说,这显然是不能接受的.可以看出,基线与增量数据分离架构的数据库具有以下缺点:(1)查询处理效率低下,每次查询前都要先将增量数据和基线数据合并;(2)数据模式复杂,有的数据在内存中,有的数据在磁盘中,并且由于数据分布在不同的机器上,导致处理数据时产生了很多额外的开销;(3)网络负载远大于传统的数据库.以上3个缺点导致了在该架构下,连接操作的效率非常低下.针对这一问题,本文提出了一种分布式连接算法,该算法基于MapReduce框架,将基线数据和增量数据同时分发到多个节点,每个节点内先不合并数据,而是对增量数据和基线数据并行的做排序归并连接.该算法有以下贡献:(1)解决了在基线与增量数据分离架构下连接效率低下的问题;(2)提供了一种如何在该架构下避免在连接前先合并数据的思路;(3)在开源的数据库OceanBase上实现了该算法,并通过一系列测试证明该算法的正确性和高效率.本文第2节介绍3种基本的连接算法的实现以及优化;第3节形式化定义了该算法所解决的问题;第4节介绍算法在开源数据库OceanBase上的详细实现;第5节分析该算法的正确性和算法效率;第6节补充算法实现过程中对增量数据的一些特殊处理;第7节通过一系列实验结果验证算法的效率;第8节总结本文所做的工作以及未来研究的方向.2相关工作传统的连接算法主要有3种:嵌套循环连接、哈希连接、排序归并连接.这3种算法有着各自的特色,随着数据库技术的发展,对这3种算法的优化也有很多.下面简要介绍一下这3种算法.2.1嵌套循环连接嵌套循环连接(NestedLoopJoin)是一种相对稳定、简单的表连接方法.它使用两层嵌套循环即外①②Page3层循环和内层循环,从而得到最终的连接结果集.在处理一些选择性强、约束性高,并且最终结果集较小的查询时,嵌套循环连接能够显示出较高的性能.嵌套循环连接由驱动表和被驱动表循环比较得到连接结果,当驱动表的记录较少,被驱动表连接列有唯一索引时,两张表记录比较的次数会较少,所以嵌套循环连接的效率变得很高.当使用了嵌套循环后,数据库不需要等到全部循环结束再返回结果集,而是可以不断地将查询出来的局部结果集返回,所以嵌套循环连接在返回前几行的记录方面上是非常快的.但是,当驱动表的记录很多,或者是被驱动表的连接列上没有索引时,两张表循环比较的时间变长,从而导致嵌套循环连接的效率变得十分低下.在共享内存架构中,减少高速缓存缺失和充分利用SIMD技术是提升嵌套循环连接算法性能的两种手段.Zhou和Ross[6]提出了利用SIMD技术优化嵌套循环连接的3种方式:复制外层循环、复制内层循环和旋转方式.而Shatdal等人[7]提出了基于块的处理方式,使得连接时高速缓存缺失尽量少.在无共享架构下,通常采用复制分片将一张表的数据复制到所有节点上,Hadoop框架[8]中采用分布式缓存,Spark框架[9]中则采用广播变量.2.2哈希连接哈希连接由构建阶段和探测阶段两个阶段组成.(1)构建阶段:选择一张小表作为驱动表,使用特定的哈希函数计算连接列的值,最终产生一张哈希表.由于计算是在内存里面进行的,因此该阶段的时间较短.(2)探测阶段:使用相同的哈希函数计算被驱动表的连接列上的值,将计算的结果与第1阶段形成的哈希表进行探测,并返回符合条件的记录.当被驱动表和驱动表的某些记录在连接列上的值不相等时,这些记录会在探测阶段被丢弃.在大数据量时,哈希连接只会用小表的连接列构建一个哈希表,所用内存不多,不会发生排序溢出.所以在大数据量时,采用哈希连接会更好.但由于哈希算法所特有的特点,哈希连接只适用于等值连接.多核大内存计算环境成了哈希连接的一个优化的新切入点.Boncz等人[10]中提出的radix连接算法就是充分利用了硬件特征,极大地优化了连接算法在内存中的Cache缺失和TLB缺失.但是,Blanas和Patel在文献[11]中提到,radix算法的连接效率与数据在内存中并且没有分区阶段的哈希连接算法相比,并没有显著的提高,因为在没有分区阶段时,该算法能充分利用硬件的预取而提高一定的效率.然而,Balkesen等人[12]通过论文实验证明:即使是无分区阶段的连接算法,性能上还是比不上radix连接算法.他指出Blanas只是利用小表建立哈希表,因此减少了分区阶段在连接算法中的占比,明显低估了radix连接的整体性能.2.3排序归并连接排序归并连接指的是两个表连接时,先通过连接列排序后,再通过合并操作来得到最后返回的结果集的方法.在对排序归并算法的研究中,Kim等人[13]指出,排序归并连接的效率与SIMD宽度相关,当宽度超过256位的时候,排序归并连接的性能能够超过哈希连接.而Albutiu等人[14]提出的MPSM并行排序归并算法则充分考虑了NUMA架构,他强调MPSM算法能够超过哈希连接算法的性能,即使在没有SIMD的支持下.2.4分布式计算在NoSQL领域,基于Spark内核的数据仓库基础架构Shark[15]对连接性能做了很大的优化.Shark中有两种分布式连接算法,分别为map连接,shuffle连接.map连接的思路是将小表的全部数据广播传输到所有含有大表分区数据的节点上,每个节点并行做连接.shuffle连接的核心是将两个表的记录都按照相同的哈希函数在连接属性上进行切分,最后在切分后的第3批节点上并行地做连接操作.对于基线与增量数据分离架构的数据库来说,优化连接性能显得更加复杂.以HBase为例,HBase不能支持where条件,orderby查询,只能按照主键和主键的range来查询.但是可以通过将Hive和HBase结合,使用Hive提供的HQL语言实现HBase大表连接查询,同时使用底层的MapReduce计算框架处理连接查询任务,将满足条件的结果存放在HBase表中.虽然有很多对连接算法的优化,但是大部分优化算法都没有考虑到基线数据与增量数据分离的情况.本文提出的算法本质上是shuffle连接,但是在连接之前没有把所有的基线数据和增量数据都合并,而是对基线数据和增量数据并行处理,减少了数据的交互,同时也保证了数据的正确性.3问题定义互联网企业的很多应用都有这样一个特点:数Page4据的总量很大,但是一段时间内的修改增量相对很小.这种情况下,基线与增量分离架构的数据库是很好的选择.这类数据库的设计思路在很早就已经被提出了,例如谷歌的BigTable[16].对于BigTable来说,每一个节点有一个memtable,当该节点接收到一个写请求,它会将数据写到内存中的memtable里去.当memtable成长到一定规模会被冻结,冻结的memtable会转换成SSTable形式写入GFS.当该节点接收到一个读请求,这个读操作会查看所有SSTable文件和memtable的合并视图.在国内对数据库的研究中,OceanBase是最典型的基线与增量分离的数据库.OceanBase是阿里巴巴研发的可扩展型关系数据库,它以成本低,高可扩展性,高可用性和高可靠性著称.现已支持了阿里集团包括支付宝在内的许多业务.OceanBase的实现借鉴了BigTable的思路,但又有一些自己的改动.它的基线数据分布式地存在多台机器上,这些机器被称为ChunkServer(CS).而它的所有的增量数据集中的存在一台机器上,这台机器被称为UpdateServer(UPS).如图1.在基线与增量分离的架构下,事务处理的速度明显加快,但查询处理方面,尤其是对连接操作的处理,还是存在很大问题的:(1)对于读请求来说,每次读操作都要做一次基线数据与增量数据的合并,由于基线数据和增量数据在不同的机器上,每次合并都要有网络的开销.在查询的数据量很少的时候,这对查询性能的影响不是很大,但是当表的数据上千万或者上亿行的时候,每一行都要做一次基线数据与增量数据的合并,这中间的网络开销是非常大的;(2)在连接算法的选择上,大多数数据库采用的是排序归并连接算法.传统的实现是先把各个表的基线数据和增量数据合并,然后把各个表合并后的数据串行发送到一台主节点上面,再在该主节点的内存里对各个表的数据排序,最后对排完序的结果做归并连接.这种实现方式的优点是简单、稳定.缺点是性能太差了,连接的处理时间过长,无法支持对实时性要求高的系统.针对以上两个问题,本文提出了一种在基线与增量分离架构下对连接操作的优化算法.由于算法在开源数据库OceanBase上做了实现,为了更直观的介绍该算法,本文在这里对问题进行一些形式化定义:(1)OceanBase集群.现有一个集群,集群有n台机器.节点M代表MergeServer,节点U代表UpdateServer,节点Ci至节点Cj代表ChunkServer.其中,节点U集中的存储所有的增量数据,节点Ci至节点Cj分布式的存储基线数据.(2)数据分布.现有R表和S表在属性a上面做自然连接.R表的增量数据全部存在节点U上面.R表的基线数据共有m个tablet:R1,R2,…,Rm,分别存在节点Ci至节点Cj上面.S表的增量数据全部存在节点U上面.S表的基线数据共有m个tablet:S1,S2,…,Sm,也分别存在节点Ci至节点Cj上面.(3)连接操作.由于篇幅所限,本文只介绍自然连接下算法的实现,现有R表和S表在连接属性a上做自然连接:selectfromRinnerjoinSonR.a=S.a;(4)连接结果.将满足条件的R表和S表的数据返回给节点M.节点M再将结果返回给客户端.4算法介绍算法基于MapReduce[17]框架,在多个节点上并行地做排序归并连接.首先对连接属性做范围(range)切分,为每个节点分配一个范围.其次在多个节点之间做基线数据混洗(shuffle),将连接属性值在同一范围的记录传输到相同的节点上,同时根据连接属性范围信息对增量数据做切分,将一个范围内的所有增量数据发送到该范围对应的节点上.最后在每个节点上并行地做排序归并连接,并将连接的结果发送给主节点.4.1算法实现由于OceanBase是国内著名的基于基线与增量分离的分布式数据库,并且已经在阿里巴巴公司的很多线上系统得到了应用.所以从实用性上考虑,算Page5法在开源的数据库OceanBase上做了实现.Ocean-Base的基线数据是分布式存储的,每一台存储基线数据的机器叫做ChunkServer.它的增量数据是集中式存储在一台机器上,该机器叫做UpdateServer.算法中还有一个主节点的角色,负责发送请求和合并子节点返回的连接结果.在OceanBase中,MergeServer充当主节点角色.根据第3部分的问题定义,算法实际分为5个阶段,具体流程如图2所示.(1)并行统计各个ChunkServer上面两张表在连接属性a上的数据分布,并将各个节点上的数据分布信息统一发送到一台MergeServer上面.由于ChunkServer存的数据只是基线数据,想要得到正确的连接属性a上的数据分布,还需要获得增量数据中连接属性a上的数据分布信息;(2)MergeServer根据每个ChunkServer传输来的a的统计信息,对连接属性a进行范围划分,每个ChunkServer对应一个范围.并把范围划分的结果发送给所有的ChunkServer以及UpdateServer;(3)各个ChunkServer之间做数据的交互,最终每个ChunkServer都得到了对应的连接属性范围内里的R表和S表的基线数据.同时,UpdateServer也根据连接属性的范围信息,将相应的增量数据发送到相应的ChunkServer上面去;(4)每个ChunkServer都获得了自己对应的连接属性范围上的4个部分数据:R表的基线数据和增量数据,S表的基线数据和增量数据.所有的ChunkServer并行地对自己的4个部分数据做排序归并连接,并将连接的结果发送给MergeServer;(5)MergeServer整合所有的ChunkServer发送过来的合并的结果,并将结果返回给客户端.4.2统计信息计算在算法的开始阶段,主节点向各个ChunkServer发送计算统计信息的请求,也会同时把R表和S表在各个ChunkServer上的数据分布信息发送给UpdateServer.这里的数据分布是以R表和S表在各个ChunkServer上的主键范围来标识的.如图3.这样,UpdateServer和ChunkServer就会并行地处理请求.首先看ChunkServer这端,每个ChunkServer要把自己在相应主键范围内的R表和S表的基线数据从磁盘中读到内存里面,并计算连接属性上的统计信息.由于基线数据是以SSTable的形式存储的,一个SSTable最大为256MB,所以在一个主键范围内可能有多个SSTable.算法的实现是在ChunkServer上开启一个线程池,用多个线程并行地计算统计信息.统计信息的计算有两种方法:抽样统计和直方图统计.前者扫描的数据少,速度快,但后者比前者更精确.这里由于数据已经被读到内存中并且是多线程计算,所以算法采用了直方图的形式来计算统计信息.每个ChunkServer计算完基线数据的统计信息后,不能立刻将结果发送给MergeServer,因为还需要获得增量数据的统计信息.再看UpdateServer的处理,UpdateServer根据R表和S表的主键范围,对增量数据进行范围遍历.增量数据存在UpdateServer内存中的数据结构memtable里面,memtable的本质是一个B+树,每张表的每一个主键对应一个叶节点,一个叶节点指向一个行操作链表,链表的每一个元素都是一个cell,记录了对该行的某一列上的修改.如图4所示.算法使用一个类似bitmap的数据结构来存储对memtable遍历的结果,每个主键对应一个值,如果该行上面的修改没有涉及到连接属性,则该行对应的值为0,否则,该行对应的值为1,并且将该行在Page6连接属性上的最新值也存起来.遍历结束后,UpdateServer把遍历结果发送给相应的ChunkServer.由于在memtable中,只有被修改的行才会作为叶节点,没有被修改的行不会在memtable上出现,所以最后遍历的结果不会很大,同时遍历都是在内存中完成的,整体速度不会太慢.ChunkServer会根据接收到的UpdateServer上的遍历结果,对基线数据的直方图进行修改,将修改后的统计信息发送给MergeServer.同时,ChunkServer也会根据遍历结果中所有对应的值为1的主键,来修改基线数据中的对应行.如图5所示.4.3增量数据划分在算法的第3阶段中,UpdateServer要根据连接属性的范围分布信息,将每个连接属性范围内的增量数据发送到对应的ChunkServer上面去.但是根据memtable的构造,有些行只修改了连接属性之外的属性,所以这些行的增量数据中没有连接属性的值.这个时候没有办法确定把这些行的增量数据发到哪个ChunkServer上面去.基于这种情况,本文提出了“最大范围”算法,来保证UpdateServer发给ChunkServer的增量数据包含了该ChunkServer在自己的连接属性范围上的所有增量.最大范围算法:由于每个ChunkServer对应一个连接属性上的范围,而UpdateServer只能把一个主键范围上的增量数据发给ChunkServer,所以如何确定这个主键范围是该算法的核心.最大范围算法的思路是由各个ChunkServer发给UpdateServer多个子主键范围,UpdateServer整合所有的主键范围,生成每个ChunkServer对应的最终的主键范围,并保证该ChunkServer对应的连接属性范围上的所有增量都在这个最终主键范围内.最大范围算法分为两个阶段.(1)第1阶段ChunkServer生成多个子主键范围.下面介绍一下该阶段的伪代码子主键生成伪代码:输入:data_scanner_(存储基线数据),join_column_id输出:Buffer1,Buffer2,…,Buffer5(存储不同范围1.WHILE(不停地从data_scanner_中取一行记录:row)2.ObObjjoin_value=row.raw_get_cell(join_column_3.Intlocation=join_range_partition(join_value);4.SWITCH(location)5.Case1:6.Buffer1.add_row(row);7.Break;8.…9.Case5:10.Buffer5.add_row(row);11.Break;12.RowKeyRangesub_range1=Buffer1.get_sub_join_13.…14.RowKeyRangesub_range5=Buffer5.get_sub_join_在做数据shuffle前,每一个ChunkServer的内存中都有R表和S表在相应主键范围上的基线数据.并且这些基线数据在连接属性上的值都是最新的(上文中计算统计信息的时候已经将连接属性上的增量数据与基线数据合并了).每个ChunkServer同时也拥有连接属性范围的切分信息:CS1对应[a_min1,a_max1],CS2对应[a_min2,a_max2],CS3对应[a_min3,a_max3],CS4对应[a_min4,a_max4],CS5对应[a_min5,a_max5].每个ChunkServer在做数据Shuffle之前会在内存中申请5个Buffer:Buffer1,Buffer2,Buffer3,Buffer4,Buffer5.这5个Buffer分别对应上述的5个连接属性上的range.在做数据Shuffle之前,ChunkServer会遍历内存中的基线数据(R表和S表的数据并行处理,现以R表为例),根据每一行在连接属性的值,将该行追加到相应的Buffer里面.如图6所示.这样在基线数据遍历结束的时候,5个Buffer里面都被填充了很多行,由于基线数据是按照主键排序的,所以此时每个Buffer中的数据也是按照主键排序的.将每个Buffer内的第1行和最后一Page7图6CS1根据连接属性的值将自己的基线数据分成5块行的主键取出来,这样就产生了5个子主键范围:[k_min1,k_max1],[k_min2,k_max2],[k_min3,k_max3],[k_min4,k_max4],[k_min5,k_max5].例如对于Buffer1来说,若Buffer1内存储的主键集合为{2,3,5,7,9,11,15},则Buffer1生成的子主键范围就是[2,15].子主键范围生成后,每个ChunkServer封装一个特殊的包(SP)发送给UpdateServer,特殊包SP里面包含的信息有:机器ip,表的id,每个连接属性上的range对应的主键range.ChunkServer发送完SP包之后再做数据的shuffle,将Buffer里的全部数据发送给相应的ChunkServer.(2)第2阶段UpdateServer接收到了所有ChunkServer发送的SP包之后,根据连接属性的range,把所有SP包的信息整合起来.如图7所示.整合结束时,5个连接属性的range分别对应了R表和S表的5个最终主键范围.例如[a_min1,a_max1]对应了R表的[e_min1,e_max1]和S表的[e_min1,e_max1].此时,UpdateServer会根据这些最终主键范围遍历memtable,将范围内的增量数据发给相应的ChunkServer.4.4基线数据与增量数据归并连接当算法进行到第4阶段的时候,每个Chunk-Server的内存中已经有了4块数据.对R表和S表的基线数据做归并连接(如下面伪代码所述):从每张表的基线数据中取一行记录开始匹配,如果符合连接条件,则根据该行的主键到该表的增量数据中二分查找,如果找到,则将该行的增量数据和基线数据合并,并把合并后的结果放到结果集中.如果不符合连接条件,则将连接字段较小的记录抛弃,从这条记录对应的表中取下一条记录继续进行匹配,直到整个循环结束.如图8所示.归并连接伪代码:输入:row_store_r(存储R表数据),row_store_s(存储输出:result_buffer(存储连接操作的结果)1.WHILE(row_store_r和row_store_s都还有数据)2.从row_store_r中取一行记录:row_r3.从row_store_s中取一行记录:row_s4.根据join_column_id从row_r里面获得该行在5.根据join_column_id从row_s里面获得该行在6.IF(join_value_r<join_value_s)7.从row_store_r中取下一行记录8.ELSEIF(join_value_r>join_value_s)9.从row_store_s中取下一行记录10.ELSEIF(join_value_r==join_value_s)11.根据row_r和row_s的主键到R表和S的增12.IF(R表的增量Buffer中找到了row_r的主键)13.将row_r与该行的增量数据合并14.IF(S表的增量Buffer中找到了row_s的主键)15.将row_s与该行的增量数据合并16.将合并后的row_r与row_s存到result_buffer中.5算法分析5.1算法正确性算法通过以下细节保证了结果的准确性.Page8(1)在统计连接属性上的信息时,不是只把对基线数据的统计结果发送给UpdateServer,而是将连接属性上的增量数据也算入到统计信息里面.这样保证了在切分连接属性range的时候,能够根据真正的统计信息,做出正确的、均匀的range切分.(2)最大范围算法的正确性:在基线数据的Shuffle过程中,每个ChunkServer对应一个连接属性上的range,每个ChunkServer会接收到其他ChunkServer发送来的属于该range里的数据块,以及自己本地的属于该range里的数据块.这些数据块都有主键范围,取这些主键范围的并集,得到最终的主键范围.虽然最终的主键范围并不精确,因为它是不连续的.但是该ChunkServer在连接属性range上的所有数据都在这个主键范围内.所以最终在做mergejoin的时候,每一行基线数据肯定能够从增量Buffer中找到对应的增量数据(如果该行被修改了的话).5.2算法效率分析与传统的排序归并连接算法相比,改进后的算法极大地提高了连接的效率.设两张表的基线数据行数为nBase,增量数据行数为nIncrease,连接结果的行数为nResult,每一行数据在网络上的传输时间为tnet,每一行增量数据和一行基线数据合并的时间为tmerge,内存中每排序一行数据的时间为tsort,排序归并连接时每处理一行数据的时间为tjoin,节点的个数为N.则可得到传统的连接算法下的连接时间T_old:T_old=(nBase+nIncrease)×优化后的算法处理时间T_new:T_new=由公式可以看出,优化后的算法明显地减少了基线与增量数据合并所用的时间,当节点个数N较大时,T_new远小于T_old.下面从3个方面详细介绍下算法如何提升了连接效率:(1)MergeServer在切分连接属性范围的时候,会根据每个ChunkServer上的统计信息,尽量使得切分后的每个范围里的数据量是平均的.如果总的数据量是m,对m行数据做排序归并连接的时间为s,范围的个数为r.则如果每个范围里的数据量是平均的,最终的连接处理时间为s/r.如果每个范围里的数据量是不平均的,假设最大的范围的数据量为m1,则最终的连接处理时间为s×m1/m.同时,在给每个CS分配范围的时候,如果某个范围内的数据大部分都在ChunkServer1上面,就把该范围分配给ChunkServer1.这样就使得网络间Shuffle的数据量最少,减少做数据Shuffle所用的时间.(2)使用最大范围算法避免了UpdateServer将所有的增量数据发给每个ChunkServer.如果没有最大范围算法,为保证结果的正确性,UpdateServer需要将每张表所有的增量数据发送给每个Chunk-Server,这样在做mergejoin的时候,基线数据才能够从增量Buffer中找到自己的增量数据.但是每张表的所有增量是很大的,这会导致UpdateServer与ChunkServer之间的网络开销特别大.如果使用最大范围算法,UpdateServer只需要将一个主键范围内的增量数据发送给ChunkServer,虽然这个主键范围内的数据会比ChunkServer真正需要的增量数据多,但是与所有的增量数据相比,最大范围算法提升的效率还是很大的.(3)4.1节中的算法在很多处都采用了并行的实现:第1阶段ChunkServer和UpdateServer并行地处理MergeServer发送的请求;第3阶段ChunkServer之间基线数据的Shuffle和UpdateServer内增量数据的划分是并行的;第4阶段多个ChunkServer之间是并行地做mergejoin的.并行的处理能够充分的利用分布式数据库的特点,极大地提高算法的效率.5.3算法适用性之所以选择在OceanBase上实现该算法,是因为OceanBase的增量数据全部集中在一台机器上,基线数据分布式地存储在多台机器上.基线数据和增量数据不在一台机器内,真正实现了物理上的分离.这一特性能够更好地符合该算法的特点,使得该算法在OceanBase上的优化效果更好.对于其他的分布式数据库来说,该算法也是适用的,只是需要在实现的时候根据数据库的特点做不同的修改.以HBase为例,Hbase的基线数据和增量数据是存储在一台机器上的,则只需要在算法的第3阶段增加不同机器之间增量数据的交互,算法的整体思路在Hbase还是可以实现的.Page96对增量数据的特殊处理增量数据一般存储在内存中,不同的数据库采用不同的数据结构来存储增量数据.以OceanBase为例,增量数据是存储在一个叫做内存表的结构里面.下面详细介绍一下内存表的架构.6.1内存表结构内存表本质上是一个B+树.它的非叶子节点是由表id和主键组成的,叶节点则存储某张表的某一行的所有增量.例如想要更新表t1的主键为R1的行的某一个非主键列c1,则首先根据t1的表id查找B+树,找到t1对应的节点,再通过R1查找该节点的孩子节点,如果找到某个叶子节点对应的主键为R1,则在该叶子节点指向的链表的尾部再增加一个链表节点,该链表节点的值为c1更新后的值.这里需要指出的是,叶子节点指向的链表是不存储该行的所有列的值的,它只存储该行被修改的列的值.如果某一列未被修改过,它是不存在内存表中的.可以看出,一张内存表是可以存储数据库中所有表的增量数据的.数据库刚启动的时候,内存表是初始为空的.当有事务来临时,如果事务更新的行在内存表里面不存在,则在内存表内新增节点.随着事务越来越多,内存表的大小也会逐渐增加,当增加到一个阈值的时候,会触发内存表的冻结操作.冻结操作会将该内存表转存到磁盘上,并且在内存中新建一张空的内存表.所以,OceanBase系统运行时最多只有一个内存表,但是可能有多个被转储到磁盘中的冻结内存表.针对OceanBase内存表的这些特性,本文在算法设计的时候提出了最大范围算法来进行增量数据的分发.由于内存表的结构限制,OceanBase只支持通过主键或者主键范围向内存表拉取增量数据.而本文算法在实现的时候,是为每个节点分配一个连接列上的范围,根据这个连接列范围向内存表拉取该范围内的增量数据.这里就出现了问题:当连接列不是主键的时候,无法通过连接列的范围向内存表拉取该范围内的增量数据.解决这个问题的关键就是:通过连接列的范围生成一个近似的主键范围,然后通过这个近似的主键范围向内存表拉取增量数据,并且保证连接列范围内的基线数据所对应的增量全部都在该近似的主键范围内.上文提出的最大范围算法就是用来求出这个近似的主键范围的.6.2数据修改操作增量数据的类型有很多种,OceanBase支持4种对数据修改的操作:insert,replace,update和delete.这4种操作在memtable上增加的增量数据是不同的.所以在算法的第1阶段,UpdateServer给ChunkServer发送遍历memtable的结果的时候,需要对不同类型的增量数据进行特殊的处理:(1)因为delete操作在memtable的表示是只在该行的行操作链表中增加一个删除的标识,所以即使该行的行操作链表中没有连接属性上的值,也要把该行对应的bitmap里的值置为1,并且连接属性的值为空.(2)对于insert操作来说,基线数据是没有新插入的行的,所以ChunkServer在根据UpdateServer的遍历结果修改基线数据的时候,需要在基线数据中添加那些新插入的行.(3)对于replace操作来说,基线数据中可能有该行,也可能没有.所以ChunkServer在根据UpdateServer的遍历结果修改基线数据的时候,如果某一个主键对应的bitmap里面的值为1,但是在基线数据中找不到该主键,同样也需要在基线数据中添加一行.7实验评估为了验证算法的正确性与效率,本文设计了一系列实验.实验的思路是通过对比OceanBase开源的版本与经算法改进后的版本对相同SQL的处理时间与处理结果,最终得出结论.7.1实验环境实验有两个OceanBase集群,每个集群都有一台UpdateServer,一台RootServer,10台Chunk-Server和一台MergeServer.其中一个集群使用的OceanBase开源的0.4版本.另一个集群使用的是实现了上述算法的版本.实验采用的数据是使用数据生成器随机生成的数据,数据集的大小从1GB到6GB不等.为了保证两个集群使用同样的数据,我们首先把随机生成的数据存到文件中,然后使用工具ObImport将该文件导入两个集群中.7.2实验变量(1)做连接的表的数据量;(2)集群中Chunk-Server的个数;(3)基线数据与增量数据的比例.7.3实验流程在两个集群中建立相同的两张表,插入相同的Page10数据,执行相同的查询:selectfromtable_Rinnerjointable_Sontable_R.col2=table_S.col2;比较在不同数据量、不同ChunkServer个数的图9实验对比结果实验结果分析:(1)当数据行数较少时,两个集群的处理时间相差不大.但是,当数据行数超过1500万行的时候,开源版本集群的处理时间出现大幅增长,而算法改进后集群的处理时间呈近线性增长.这是因为开源版本集群只在一台MergeServer上做排序归并连接,当MergeServer的内存不能容纳所有的数据的时候,一部分数据会被物化到磁盘上,等需要的时候再从磁盘中取出来,这极大地影响了连接处理的效率.(2)ChunkServer的个数对开源版本集群的处理时间几乎没有影响.而算法改进后集群的处理时间随着ChunkServer的个数增加而呈近线性减少.这是因为无论有多少台ChunkServer,开源版本集群只在一台MergeServer上做连接.而算法改进后集群则是在多个ChunkServer上并行地做连接.情况下,两个集群的处理时间与处理结果.最终得到的结论如图9.(3)当增量数据占总数据量的比例越来越大时,开源版本集群的处理时间呈近线性增长,这是因为要做合并操作的增量数据越来越多,导致网络传输量增加.而算法改进后集群的处理时间呈先下降后上升的趋势,这是因为当基线数据减少时,ChunkServer之间数据交互的数量变少,整体的处理时间下降.但是当增量数据越来越多的时候,增量数据的网络传输时间增加,导致整体的处理时间上升.7.4TPC-H测试除了通过对比实验来测试算法的优化性能,本文还参考了TPC-H基准测试对算法优化后的OceanBase进行了进一步测试.TPC-H基准测试是由TPC-D发展而来,包括22个查询,其主要评价指标是各个查询的相应时间,其度量单位是每小时执行的查询数.本文使用了TPC-H不同大小的数据集,修改了查询语句.最终生成的测试结果如图10.Page11通过以上实验和测试可以看出,改进后的算法的确极大地提高了OceanBase对连接操作的处理效率.并且ChunkServer个数越多,改进后的算法效率越高.8总结以及未来工作本文提出了一种分布式环境下的排序归并连接算法,该算法充分利用了基线数据与增量数据分离的特点,通过对连接属性做范围切分,并行地处理基线数据和增量数据,避免了在做连接前把大量的基线数据和增量数据合并导致的查询效率低下的情况.通过在开源数据库OceanBase上实现该算法并做了一系列的测试,充分验证了所提连接算法能够极大地提升大表连接的效率.本文重点讨论了两张大表连接的情况,对于多表连接,如何根据统计信息、中间结果集的大小来确定连接的顺序,以及如何对多表的数据同时进行MapReduce操作等优化方法,将是未来连接优化算法研究的重点.致谢在导师周敏奇教授的细心指导下,我才能顺利地完成本文的工作.写论文期间,周教授严谨的治学态度极大地影响了我,让我学到了很多.在这里衷心感谢3年来周老师对我的关心和指导.同时,周傲英教授也对我的工作和论文提出了许多宝贵的意见,让我少走了很多弯路,提高了学习的效率,在这里表示衷心的感谢.在写论文期间,实验室的很多小伙伴,如周欢等对我的工作、生活给予了热情的帮助,让我感受到同窗的温暖,在此向他们表达我的感谢!
