Page1一种基于线性函数逼近的离策略犙(λ)算法傅启明1)刘全1),2)王辉1)肖飞1)于俊1)李娇1)1)(苏州大学计算机科学与技术学院江苏苏州215006)2)(吉林大学符号计算与知识工程教育部重点实验室长春130012)摘要将函数逼近用于强化学习是目前机器学习领域的一个新的研究热点.针对传统的基于查询表及函数逼近的Q(λ)学习算法在大规模状态空间中收敛速度慢或者无法收敛的问题,提出一种基于线性函数逼近的离策略Q(λ)算法.该算法通过引入重要性关联因子,在迭代次数逐步增长的过程中,使得在策略与离策略相统一,确保算法的收敛性.同时在保证在策略与离策略的样本数据一致性的前提下,对算法的收敛性给予理论证明.将文中提出的算法用于Baird反例、Mountain-Car及RandomWalk仿真平台,实验结果表明,该算法与传统的基于函数逼近的离策略算法相比,具有较好的收敛性;与传统的基于查询表的算法相比,具有更快的收敛速度,且对于状态空间的增长具有较强的鲁棒性.关键词强化学习;函数逼近;离策略;Q(λ)算法;机器学习1引言强化学习(ReinforcementLearning,RL)是一种从环境状态到动作映射的学习,并期望动作从环境中获得的累积奖赏最大[1-2].从20世纪80年代末开始,随着对强化学习的数学基础研究取得突破性进展后,对强化学习的研究和应用日益开展起来,强化学习成为目前机器学习领域的研究热点之一.近年来在工程应用、模式识别、图像处理、网络优化等领域都得到广泛应用.在强化学习中,常用查询表(Lookup-Table)来描述状态到动作的映射,它是一个二维的表格,状态和动作是表的两个维度,在学习之后,给定相应状态动作对一定的回报值,Agent可以根据对应状态动作对的回报值来选取动作.对于低维状态空间,它是一个非常简洁有效的方法.然而,对于高维大状态空间,容易出现“维数灾”的问题,同时方法本身也存在以下3个问题:(1)存储空间.Lookup-Table是以表的形式存储从状态到动作的映射,当状态空间较大时,如何存储这个表是需要解决的问题;(2)表值的查询和修改.当状态空间较大时,如何能够及时准确地查询和修改表值也是需要解决的问题;(3)在学习之后,对于学习过程中没有学习到的状态,Agent无法根据查询表选取最优动作,这一问题在连续状态空间任务中更为明显[3].针对以上3个问题,在强化学习中引入函数逼近方法.通过函数逼近,使学习的经验信息能够从状态空间子集泛化至整个状态空间,采取带有一组参数的近似函数来描述强化学习中的状态值函数(或动作值函数),Agent根据近似函数选择最优动作[4-5].根据目标策略和行为策略是否一致,将强化学习分为在策略(On-Policy)学习和离策略(Off-Policy)学习.如果在学习过程中,动作选择的行为策略和学习改进的目标策略一致,该方法就被称为在策略学习,如Sarsa学习,否则被称为离策略学习,如Q学习.将函数逼近用于在策略强化学习,Tsitsiklis等人[5]在20世纪90年代从理论和实验两个方面证明了方法的稳定性和收敛性.然而对于离策略强化学习方法,由于目标策略和行为策略的不一致,使得行为策略所生成的样本数据与目标策略所需要的样本数据的分布是不一致的,导致方法无法收敛.比如,将基于函数逼近的Q学习用于著名Baird反例,算法无法收敛[6].Gordon[7]对经典Q学习算法做出尝试改进,提出的算法能够稳定收敛,但是性能不够理想.Tadic[8]利用线性函数将函数逼近和TD学习相结合用于有限维度状态空间问题,并证明了算法有效性,但试验结果表明,与传统的TD学习相比,该算法的执行效率不高.Lagoudakis等人[9]利用最小二乘法求解策略迭代问题,但该算法只能用于环境模型已知的情况.Precup等人[10]提出将函数逼近用于离策略的TD(λ)算法,并证明了算法收敛性,但该算法对于初始行为策略有一定的要求,算法在执行过程中可能因为初始行为策略不满足要求,而出现无法收敛的情况.Geramifard等人[11]利用最小二乘法实现TD问题增量式求解,该方法主要是对在策略TD方法的改进,将离线更新方法改为在线增量更新方法.Sutton等人[12-13]利用线性函数将TD学习与函数逼近相结合,提出GTD、GTD2及TDC算法,并在实验和理论上均证明这几种算法的收敛性,但与传统的TD方法相比,算法在执行效率上没有较大的提高.Maei等人[14]提出一种增量式的离策略greedy-GQ算法,但算法要求用于生成样本数据的行为策略必须是稳定性策略.本文针对基于查询表的Q学习算法在大状态空间中所存在的3个问题,提出一种基于线性函数逼近的离策略Q(λ)算法(Off-PolicyQ(λ)algorithmbasedonGradient-Descent,GDOP-Q(λ)).针对经典的基于函数逼近的Q(λ)算法,由于目标策略需要的样本数据与行为策略生成的样本数据分布不一致,从而导致无法收敛的问题,GDOP-Q(λ)算法引入重要性关联因子.在此基础之上定义一种新的离策略下的资格迹(EligibilityTraces),使得在策略与离策略相统一,并在迭代次数逐步增长的过程中,利用梯度下降方法求解一组最优参数,确保算法的收敛性.同时在保证在策略与离策略的样本数据一致性的前提下,对算法的收敛性给出了理论证明.将GDOP-Q(λ)算法用于Baird反例、Mountain-Car及RandomWalk仿真平台,实验结果表明,该算法与传统的基于函数逼近的离策略算法相比,能够较好的收敛;与传统的基于查询表的TD(λ)系列算法相比,具有较好、较快的收敛速度,且对于状态空间的增长具有较好的鲁棒性.2离策略强化学习原则上任何函数逼近方法都可以用于监督学习,比如人工神经网络、决策树以及各种多元线性回Page3归方法等.然而,对于强化学习来讲,并不是所有的方法都适用.大部分的逼近方法,比如人工神经网络、遗传算法等,都假设有一个稳定静态的训练集,所有的逼近操作都是基于稳定静态的训练集.但是在强化学习中,学习的过程是在线的、动态的,逼近方法所需要的训练集是Agent与环境交互动态实时得到的,因此就需要逼近方法能够有效地从增量式的训练数据中进行学习.在强化学习中,还需要逼近方法能够解决目标函数不确定的问题,例如,在GPI(GeneralPolicyIteration)控制问题中,当目标策略π改变时,方法要能够逼近新的动作值函数Qπ.或者即使目标函数是确定的,如果Agent与环境交互得到的样本数据是基于自举式(Bootstrapping)方法得到的,那么样本数据对应的状态(动作)值也是不稳定的,这就要求逼近方法也要能够对这类问题具有较好的鲁棒性.2.1梯度下降法与线性函数逼近梯度下降法是利用负梯度方向来决定每次迭代的搜索方向,使得每次迭代能使待优化的目标函数呈非递增变化,它是2范数下的梯度下降法.它的简单形式如式(1)所示.其中f(k)是目标函数,kf(k)是f(k)的梯度.在基于梯度下降的逼近方法中,线性函数逼近是最常用、最重要的逼近形式.由于其函数构造形式简单,计算量较小,近年来,在基于函数逼近的强化学习方法中得到广泛的应用.本文也采用线性函数来构造动作值函数(Q值函数),如式(2)所示.考虑在标准的强化学习框架下,Agent与环境交互构成一个有限马尔科夫决策过程(MarkovDecisionProcess,MDP).在策略π下,每个时间步t,环境状态为st(st∈S,S为状态空间),Agent选择的动作为at(at∈A,A为动作选择空间),状态动作对(st,at)由特征向量(st,at)表示((st,at)={(1),(2),…,(n-1),(n)},(st,at)∈写为t;环境给出一个奖赏值rt(rt∈),动作值函数记为Q(st,at),简写为Qt;Qt是关于参数向量θt(θt=(θt(1),θt(2),…,θt(n))T且θt∈向量t的函数,记为Qt(θt),Qπat)的期望值.随着Agent与环境的不断交互,可以得到一组状态、动作、回报值的序列,s1,a1,r2,s2,a2,r3,…,将该序列表示为一组四元组(s1,a1,r2,s2),(s2,a2,r3,s3),…,由于状态动作对(st,at)用t表示,因此将序列简化为(1,r2,2),(2,r3,3),….Qt(θt)=θTtt≈Vπt=E∑其中γ是折扣因子.Agent对于环境未知,仅从观察的一组状态转移序列中估计动作值.在策略TD(0)中,一步TDError及线性TDSolution如式(3)、(4)所示.定义1.在策略TD(λ)中,λ-TDError及线性λ-TDSolution如式(7)、(8)所示.t(θ)=R(n)rt+1+γrt+2+γ2rt+3+…+γn-1rt+n+γnθTt+n(5)δλ=Rλt(θ)-Qt(θ)=Rλt(θ)-θTt=-θTt+(1-λ)λ0[rt+1+γθTt+1]+(1-λ)λ1[rt+1+γrt+2+γ2θTt+2]+(1-λ)λ2[rt+1+γrt+2+γ2rt+3+γ3θTt+3]=-θTt+(γλ)0[rt+1+γθTt+1-γλθTt+1]+(γλ)1[rt+2+γθTt+2-γλθTt+2]+(γλ)2[rt+3+γθTt+3-γλθTt+3]=(γλ)0[rt+1+γθTt+1-θTt]+(γλ)1[rt+2+γθTt+2-θTt+1]+(γλ)2[rt+3+γθTt+3-θTt+2]≈∑T-1=∑T-1=∑T-1其中R(n)Rλt(θ)是λ-回报值,犃=犈t∑T-1b=犈t∑T-1是当前θ的差分值,目标需使得犈[δλ]等于0.因此,取其2范数描述当前θ值与目标θ值的距离,即如式(9)所示.i=tt(θ)是n步截断回报值,简称n步回报值,{i=ti=tPage4根据梯度下降法,在每一次迭代过程中,对θ的更新如式(10)所示.其中αt是步长参数.将式(9)代入式(10),在每个时间步t对θt的更新如式(11)所示.θt+1=θt+αtθ(犈[δ]T犈[δλ])=θt+2αtθ(犈[δλ])犈[δλ]=θt+2αt(犈[(θδλ)T]T)犈[δλ]=θt+2αt犈t∑T-1考虑式(11)中存在两个期望的乘积,无法通过迭代的方法逼近最优θ,因此考虑采用样本值计算其中一个,迭代计算另一个.考虑用样本值计算{犈t∑T-1ut为第t个时间步后对犈[δ]的估计.i=tθt+1=θt+αt∑T-1其中u1=0,βt>0,αt>0,βt,αt是步长参数.2.2离策略强化学习算法离策略学习是一种将目标策略与行为策略分离的学习方式.在将函数逼近与离策略学习结合时,面临这样一个问题———逼近求解的所需要的样本数据概率分布与行为策略生成的样本数据概率分布不一致.因此,当求解的θ收敛时,目标策略却无法收敛.本文主要以Q(λ)算法为基础,分析离策略强化学习算法运行流程及发散原因,认为解决这一问题主要有以下两种途径:(1)在学习过程中,利用与目标改进策略接近的行为策略来生成样本数据;(2)将目标改进策略与行为策略以某种方式进行关联,使得生成的样本数据与改进目标策略逼近所需要的数据在概率分布上保持一致.假设π是目标改进策略,b是行为策略,本文引入重要性关联因子ρ.定义2.假设环境状态为st,动作为at,在目标策略π和行为策略b下,在状态st下选择at的概率分别为π(st,at)和b(st,at),且满足π(st,at)>0,b(st,at)>0,则重要性关联因子ρt=π(st,at)/b(st,at),简称关联因子ρt.在Q(λ)算法中,利用关联因子,重写式(5)~(8)、(12)、(13),如式(14)~(19)所示.珚R(n)t(θ)=rt+1+γrt+2ρt+1+…+γn-1rt+nρt+1ρt+2…珔δλ=∑T-1{其中犃=犈t∑T-1b=犈t∑T-1θt+1=θt+αt∑T-1Precup等人在2000年首次提出离策略n步回报,并证明Eb{珚Rλt|s=st,a=at}=Eπ{Rλt|s=st,a=at},st∈S,at∈A[15],本文将这观点延伸至基于线性函数逼近的Q(λ)中.由式(13)和式(19)可知Δ珔θt=αt珚Rλt-θT()ttρ1ρ2…ρt=α珚Rλt-θT()tt∏t定理1.在策略与离策略算法关于样本数据分布的一致性证明.令Δθt与Δ珔θt分别是在策略TD(λ)与离策略Q(λ)算法在一个情节结束之后关于θ的增量和,假设两个算法分别从(s0,a0)开始,策略b和π分别是Q(λ)算法的行为策略和目标策略,ρt是关联因子,则Eb{Δ珔θt|s=st,a=at}=Eπ{Δθt|s=st,a=at}.证明.根据式(20)、(21),重写Δθ与Δ珔θ,并令Eπ{Δθt}=Eπ{Δθt|s=st,a=at},Eb{Δ珔θt}=Eb{Δ珔θts=st,a=at},得Eb{Δ珔θt}=Eb∑T因此,要证明Eb{Δ珔θt}=Eπ{Δθt},即要证明{Eb∑T令Ωt为Agent从(s0,a0)开始在t时刻所能得到的状态转移序列集合,ω是其中的一个样本序列,即ω∈Ωt,pb(ω)为在行为策略b下得到ω序列的概率,则t=1Page5t=1t=1t=1i=1{Eb∑T=∑Tt=1∑ω∈Ωt=∑Tt∑ω∈Ωt∏tπ(sj,aj)∏tb(sj,aj)(Eb{珚Rλt|s=st,a=at}-θTt)=∑Tt∑ω∈Ωt∏t(Eb{珚Rλt|s=st,a=at}-θTt)又因为st∈S,at∈A,Eb{珚Rλt|s=st,a=at}=Eπ{珚Rλt|s=st,a=at},可得{Eb∑T=∑Tt=1(Eb{Rλt|s=st,a=at}-θTt)=∑T=Eπ∑T由定理1可知,在Q(λ)中引入关联因子,在任意t时刻,对于θ的差分累加和与在策略TD(λ)中θ的差分累加和是一致的.进一步可认为,在引入关联因子的Q(λ)算法中,行为策略所生成的样本数据与改进目标策略所需要的样本数据的分布是一致的,确保算法不会因为样本数据的不一致性导致无法收敛.3犌犇犗犘-犙(λ)算法t=1第2节中主要从向前的观点证明引入重要性关联因子的Q(λ)能够取得与在策略TD(λ)算法分布一致的样本数据,确保算法不会因为样本数据的不一致性导致无法收敛.本节将利用向后的观点具体阐述GDOP-Q(λ)算法的执行流程,并证明算法的收敛性.3.1犌犇犗犘-犙(λ)在向前的观点中,主要引入n步回报值,即对当前动作值的估计是基于后续n个奖赏值,但是在实际执行过程中,无法利用增量的方式计算后续n个奖赏值的估计.Sutton等人在文献[16]中提出了在概念和计算上比较方便的向后的观点.利用向后的观点,可以轻易实现算法的增量式更新.在向后观点中,资格迹是一个新引入的参数,它反映了当前状态(或状态动作对)之前n个状态(或状态动作对)对当前TD-Error的影响程度.定义3.假设当前时刻t,状态动作对(st,at),(st,at)是关于(st,at)的特征向量,(st,at)={(1),(2),…,(n-1),(n)},犲t是在策略下的资格迹,犲t={犲t[1],犲t[2],…,犲t[n]},犲t是离策略下的资格迹,犲t={犲t[1],犲t[2],…,犲t[n]},则犲t[i]=IsstIaat+犲t[i]=ω=其中γ是折扣因子,λ是衰减因子,ω是迹因子,ρ(st,at)是重要性关联因子,Ixy是一致性标识函数,如果x=y,Ixy=1,否则Ixy=0.利用定义3给出的离策略下的资格迹,下面给出完整的GDOP-Q(λ)算法.算法1.GDOP-Q(λ)算法.1.初始化.任意值初始化资格迹犲和犲,θ=0,π是greedy策略,b是ε-greedy策略.2.在每个时间步t,状态动作对(st,at),(st,at)是关于(st,at)的特征向量,判断当前动作at是否是贪心动作,即判断是否Q(st,at)=maxaQ(st,at).如果判断成立,则ω=ωρ(st,at),对于任意(i)∈(st,at),有否则,ω=1,且犲t=0.犲t[i]=γλ犲t-1(i)+1,犲t[i]=γλ犲t[i]ω+犲t[i],3.采用动作at,得到立即奖赏rt+1,环境迁移至下一个状态st.如果at是贪心动作,转至步4,否则,转至步2.4.计算Q(st,at)=θT(st,at),δt=rt+1-Q(st,at).5.对于所有at=A(st),A(st)是状态st下动作集合,计算Q(st,at)=θT(st,at).6.取st下的贪心动作,at=argmaxaQ(st,at).7.计算δt=δt+γQ(st,at),θt=θt-1+αtδt犲t,αt=kαt-1,(k∈(0,1]),如果θ收敛,算法终止,否则转至步2.3.2收敛性分析向前观点主要从理论的角度理解算法,向后的观点主要从概念和计算的角度描述算法.算法1给出了GDOP-Q(λ)算法完整的执行流程.接下来再Page6次利用向前观点,证明算法的收敛,下面给出算法收敛性定理.定理2.GDOP-Q(λ)算法的收敛性.根据向前的观点,利用式(18)、(19)计算θ值.设其中βt=ηαt,η>0,βt,αt∈(0,1],且∑进一步假设(t,rt,t)满足独立同分布,令犃=Et∑T-1b=Et∑T-1且假设犃是满秩矩阵,则θ必定收敛,且满足犃θ-b=0(λ-TDSolution).证明.首先利用式(19)、(20),将μt,θt合并写成一个长度为2n的向量,ρTt=(狏Tt,θTt).狏t=μt/槡η,同时构造一个长度为2n的向量,犵T且ρt+1=ρt+αt槡η(Gt+1ρt+犵t+1),其中Gt+1=-槡ηI,t∑T-1烄(γλ)i-t∏i-t∑T-1烆i=t设犌是Gt的期望,犵是gt的期望,则.则由犌ρ+犵=0可以推导出犃θ-b=0,其中ρT=(狏T,θT).令ρt+1=ρt+αt槡η(犌ρt+犵+(Gt+1-犌)ρt+(犵t+1-犵))=ρt+αt(h(ρt)+Mt+1),其中αt=αt槡η,h(ρt)=犌ρt+犵,Mt+1=(Gt+1-犌)ρt+(犵t+1-犵).令Γt=σ(ρ1,M1,ρ2,M2,…,ρt,Mt).若能证明ρt+1=ρt+αt(h(ρt)+Mt+1)收敛于犌ρ+犵=0,则可以推出犃θ-b=0,定理得证.下面证明ρt+1=ρt+αt(h(ρt)+Mt+1)收敛.为此,利用文献[17]给出的定理2.2.根据定理2.2,只需要验证以下4个条件:(1)函数h(ρt)满足Lipschitz条件且h(ρ)=limr→h(rρ)/r存在.h(ρ1)-h(ρ1)2=g+犌ρ1-(g+犌ρ2)2验证:对于ρ1,ρ2,有可见函数h(ρt)满足Lipschitz条件.其次,limr→h(rρ)/r=limr→犌ρ,由于犵有限,因此,limr→犵/r=0,从而h(ρ)=limr→h(rρ)/r存在成立.(2)数列(Mt,Γt)是熵差分序列,存在τ满足犈[Mt+12Γt|]τ(1+ρt验证:Mt+12=(Gt+1-犌)ρt+(犵t+1-犵)2令τ=max(Gt+1-犌)2,(犵t+1-犵)Mt+12τ(1+ρt(3)由定理条件可知,∑条件成立.(4)常微分方程ρ·=h(ρ)存在一个全局稳定近似解.验证:由线性微分方程组理论可知,如果犌的所有特征值的实部均为负数,便可推出ρ·=h(ρ)=犵+犌ρ满足条件.问题可转化为验证犌的所有特征值都小于0.由犌=-槡ηI-犃|犌|=-槡ηI0-犃T(-槡ηI)(-犃)=(-槡η)n(-1)n(1/槡η)n犃T犃=犃T犃.由于犃≠0,且犃T=犃,得犌=犃2≠0.设犌的所有特征值为λ1,λ2,…,λ2n,则犌=∏2nλk≠0,则犌的所有特征值均不为0.k=1设λ是犌的一个特征值,狓是标准特征向量,则犌狓=λ狓,且狓T狓=1.因此狓T犌狓=狓Tλ狓=λ狓T狓,得狓T犌狓=λ.令狓=x1λ=狓T犌狓=(狓T1,狓T2)-槡ηI-犃=(狓T1,狓T2)-槡η狓1-犃狓2=-槡η狓T1狓1-狓T1犃狓2+狓T2犃T狓1由于狓T1犃狓2=狓T2犃T狓1,得因此,λ的实部就是-槡η狓12,又因为G的所有特征值均不为0,故λ≠0.因此λ的实部Re(λ)<Page70,因此问题得证,满足第4个条件.根据文献[17]给出的定理2.2,且定理满足以上4个条件,则ρt+1=ρt+αt(h(ρt)+Mt+1)收敛于犌ρ+犵=0,因此可推出犃θ-b=0.证毕.4实验结果分析为了验证算法的有效性,本文以强化学习中经典的Baird反例、MountainCar和RandomWalk为例.其中Baird反例是经典的用于验证基于函数逼近的离策略学习算法无法收敛的实例,如图1所示.MountainCar是强化学习中另一个经典的连续状态空间、情节式任务(情节式任务是指包含终结状态的强化学习任务),其中状态的表示较Baird反例和RandomWalk更为复杂,状态空间也更大,如图2所示.RandomWalk是一个典型的无折扣、情节式的强化学习任务,经典的RandomWalk是在一条直线上包含5的状态,在直线的两端各有一个终结状态.当Agent到达最右端的终结状态时,立即奖赏为1,其它情况下,立即奖赏为0,如图3所示.为了验证算法对于状态空间增长的鲁棒性,本文将经典的RandomWalk扩展为19个状态,并验证算法的有效性.实验1是强化学习中经典的Baird反例,由Baird在1995年提出,用于说明经典的基于函数逼近的离策略强化学习算法无法收敛.该实验包含6个状态,状态表示函数及状态之间的转移情况如图1所示,且对于所有状态转移的立即奖赏值都是0.因此,该实验中每个状态的真实状态值(或者动作值)都是0,状态表示函数的参数θ值都是0.将经典的离策略动态规划算法、基于函数逼近的Q(λ)算法———Watkins’sQ(λ)算法[18]及本文提出的GDOP-Q(λ)用于Baird反例,其中γ=0.99,α=0.01,λ=0.1,实验结果如图4~图6所示.图中横坐标表示算法所执行的episode数目,纵坐标表示θ值,以θ(1)的值为例.实验表明,对于Baird反例,经典的离策略动态规划算法和Watkins’sQ(λ)算法无法收敛.而本文提出的离策略GDOP-Q(λ)利用关联因子,保证行为策略所生成的实验数据和目标Page8策略所需要的数据一致,在算法中通过修正资格迹et给出新的θ更新策略.实验结果表明,对于小状态空间的Baird反例,算法大约在20个episode之后就呈现收敛状态,且θ(1)的值为0.实验2将Least-SquaresSARSA(λ)[19]、Greedy-GQ[14]、基于查询表的SARSA(λ)、Watkins’sQ(λ)及GDOP-Q(λ)算法用于MountainCar,比较算法性能,其中γ=1.0,α=0.5,λ=0.95,ε=0.01.MountainCar是强化学习中一个经典连续状态空间案例,如图2所示.案例中小车由于动力不足,无法直接通过加速达到右侧坡顶,因此需要借助左侧的小坡,通过不断的前后加速到达坡顶.小车的动作分为前后加速和不加速3个动作,状态属于一段连续的状态空间,小车到达右侧坡顶获得值为1的立即奖赏,其它情况立即奖赏为0.实验结果如图7~图11所示,图中横坐标表示算法所执行的episode数目,纵坐标表示小车爬上坡顶所需要的步数.通过比较,不难发现,其中Watkins’sQ(λ)算法无法收敛,其它算法包括本文提出的GDOP-Q(λ)算法都能收敛.比较图10和图11,Least-SquaresSARSA(λ)算法的收敛性能和收敛速度均优于基于查询表的SARSA(λ)算法,且基于查询表的SARSA(λ)算法前期振幅较大.这是由于在大规模状态空间中,利用有限训练集得出最优参数可以泛化至整个状态空间,能够保证算法在有限的训练次数后得到较好的收敛性能.比较图9和图11,可以看出,在当前状态空间下,Greedy-GQ的收敛速度又要优于Least-SquaresSARSA(λ)算法,但两者的收敛结果类似.比较图8和图9,GDOP-Q(λ)算法略优于Greedy-GQ.这是因为利用相关性因子,保证行为策略所生成的实验数据和改进策略所需要的数据一致,进而保证算法收敛.综合比较所有算法性能图,GDOP-Q(λ)算法能在保证收敛的情况,具有更好的收敛性能.Page9实验3主要用于检验算法在状态空间发生变化时收敛性能的鲁棒性.将基于查询表SARSA(λ)、Least-SquaresSARSA(λ)及GDOP-Q(λ)算法用于RandomWalk,其中γ=1.0,α=0.5,λ=0.95,ε=0.01.经典的RandomWalk中包含5个状态,两端各有一个终状态,呈线性排列,状态之间的转移情况如图3所示.当到达右侧终结状态时,立即奖赏为1,其它情况下,立即奖赏为0.为了验证算法对于状态空间发生变化时的鲁棒性,将经典的5个状态的RandomWalk扩展为19个状态.实验结果如图12所示,图中每个点的数据值是对每个算法重复执行100次之后状态值标准差(RootMeanSquare,RMS)的平均值,其中横坐标表示算法执行的episode数目,纵坐标是在当前episode数下算法执行100次之后RMS的平均值.对于5个状态的RandomWalk,3个算法都取得类似的实验结果.但是当RandomWalk中包含19个状态时,不难发现,对于Least-SquaresSARSA(λ)和GDOP-Q(λ)算法,在收敛性能上与只有5个状态的RandomWalk下的性能基本一致,GDOP-Q(λ)在收敛速度和收敛性能上略优于Least-SquaresSARSA(λ).但是对于基于查询表的SARSA(λ)算法,在相同的训练次数下,在收敛速度和收敛性能上都明显下降.这是由于基于函数逼近的学习算法所求的θ值与具体的状态无关,仅与组成状态的特征相关.在相同的训练次数下,基于函数逼近的学习算法可以取得更好的收敛性能.因此,GDOP-Q(λ)的收敛性能在状态空间发生变化时具有较强的鲁棒性.图12基于查询表SARSA(λ)、Least-SquaresSARSA(λ)及GDOP-Q(λ)算法在状态空间发生变化时的收敛图5结束语针对当前强化学习领域中经典的基于查询表的强化学习算法在大规模状态空间中收敛速度慢或无法收敛的问题,本文提出利用函数逼近方法求解强化学习策略.并进一步针对将函数逼近用于经典离策略强化学习算法———Q(λ)算法无法收敛的问题,引入重要性关联因子,并证明利用重要性关联因子可以将在策略与离策略相统一.基于以上分析,提出一种基于线性函数逼近的离策略Q(λ)算法———GDOP-Q(λ),并证明算法的收敛性.本文以经典的Baird反例、MountainCar以及Randomwalk为例,将GDOP-Q(λ)与经典动态规划算法、基于查询表在策略SARSA(λ)算法、Least-SquaresSARSA(λ)、Greedy-GQ、Watkins’sQ(λ)算法相比较.实验结果表明,该算法不同于经典的Watkins’sQ(λ)算法,在3个例子中都能够收敛;同时与Least-SquaresSARSA(λ)算法相比,算法性能基本一致,收敛速度略有提高;与基于查询表的SARSA(λ)相比,在状态空间发生变化,算法具有较强的鲁棒性.在强化学习领域,如何平衡利用(Agent采用已知的最优策略)和探索(Agent探索新的策略,该策略可能优于当前的最优策略)问题是强化学习领域中的一个难点.该问题在本文中并没有给出明确的解决方案,在本文的实际实验中仅是采用ε-greedy方法来平衡.然而ε-greedy方法没有针对状态本身的不确定性给出明确的探索或者利用信号,具有一定的随机性,弱化了算法的收敛性.因此,下一步的工作将从平衡探索和利用的角度,结合离策略的学习算法,提出改进策略,进一步加快算法的收敛速度.
