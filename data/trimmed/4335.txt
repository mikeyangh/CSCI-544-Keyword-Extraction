Page1内存OLAP多核并行查询优化技术研究焦敏1),2)张延松1),2),3)王珊1),2)陈红1),2)1)(中国人民大学数据工程与知识工程教育部重点实验室北京100872)2)(中国人民大学信息学院北京100872)3)(中国人民大学中国调查与数据中心北京100872)摘要随着以大内存和多核为代表的计算机硬件技术的发展,以cache-conscious算法为中心的查询优化技术逐渐转向以multicore-conscious为中心的查询优化技术,来提高多核处理器的并行处理性能.该文的研究目标是具备复杂星型连接特点的联机分析处理OLAP技术,以查询执行代价最大的星型连接为研究对象,提出同时满足cache-conscious和multicore-conscious的多核并行连接算法DDTA-MPJ.该算法包括基于事实表水平分片和维属性列共享访问模式的查询内多核并行算法IntraDDTA-MPJ、基于QuerySlots的查询间多核并行算法InterDDTA-MPJ以及中位数多核并行算法Median-MPJ.实验结果表明该算法具有良好且稳定的并行查询处理性能,线性查询处理模型能够更好地利用多核处理器的先进性能.关键词多核敏感算法;星型连接;多核并行;并行中位数聚集计算1引言20世纪70年代,IBM公司的SystemR原型系统奠定了关系数据库的技术基础,并推动其占据主流的数据库市场.在关系数据库技术越来越成熟的同时,计算机硬件技术的发展为数据库引入越来越多新的特性和技术.目前,数据库查询优化技术是以磁盘存储和I/O代价模型为核心的,难以充分发挥大内存和多核处理器这些硬件的性能优势.面向大内存,数据库需要根据内存访问特性设计新的存储模型;面对多核处理器越来越多的计算核心,数据库需要将原有的串行查询处理引擎升级为新的并行查询处理引擎.面向内存数据库的cache-conscious查询优化技术主要针对多级cache结构优化内存访问性能,主要体现在cache-conscious索引机制的研究,cache-conscious的连接优化技术以及并发查询时面向page-color的优化技术等.研究的方法是对磁盘关系数据库中传统的索引结构和哈希连接算法进行内存优化.面对当前多核处理器技术,内存数据库的查询优化技术不仅需要在多级缓存结构上进行数据访问优化,而且需要面向多核处理器进行并行访问和并行计算模型上的优化,同时还要提高共享cache在多核并行处理时的利用率.本文以数据仓库和联机分析处理(On-lineAnalyticalProcessing,OLAP)的星型存储模型为基础,研究基于大内存和多核处理器平台的OLAP并行查询优化技术.优化面向内存访问特性的存储模型,设计面向负载特性的内存行存储、列存储及混合存储模型;基于具备内存随机访问特性的DDTA-JOIN算法[1]提出了多核并行DDTA-MPJ算法,通过内存中事实表动态水平分区和维表属性列共享访问技术提供良好的并行扩展能力,增强频繁访问数据集在多核共享cache中的数据局部性.OLAP的并行处理能力不仅取决于连接操作的并行效率,还取决于聚集操作的可并行性.OLAP中可分布式计算的聚集函数,如sum、count以及average等能够充分利用多核并行计算能力;对于OLAP中不可分布计算函数,例如百分位数(包括中位数)等需要集中处理的聚集函数,难以发挥多核处理器的并行计算优势.百分位数是统计分析中非常重要的函数,它用来衡量数据分布的均衡状态.百分位数在统计、经济、教育、医学、房地产等领域应用非常广泛,在分析处理中被越来越广泛地使用.但是这类函数和数据位置密切相关,不能简单地利用多核技术将数据集水平分割后再进行并行处理.通常只能将全部数据集排序后,按照位置找到所需的数值.但是在大内存多核处理器平台上,这种方法效率很低.本文提出了以多核并行处理为基础的迭代中位数算法,能够利用多核计算资源实现中位数并行计算,从而支持通用的多核内存OLAP查询处理.本文主要创新体现在以下几点:(1)根据数据仓库星型模型的访问特征设计了面向多核处理器的适于并行处理的存储模型:采用行/列混合存储模型和面向多核优化的谓词向量压缩技术.事实表采用行存储模型,适合基于动态水平分区的多核并行扫描操作;维表采用列存储模型,可以提高频繁访问的维表数据的存储效率;通过谓词向量压缩技术将多核并行查询所依赖的多个共享维表列缩减为位图向量,进一步减少多核并行处理时的cache开销.(2)提出了面向内存OLAP的多核并行DDTA-MPJ(DirectlyDimensionalTupleAccessingforMulticoreParallelJoin)算法,实现了查询内多核并行和查询间多核并行处理.通过键值-地址映射机制将内存维向量转换为公共向量集,消除并发查询的私有数据集(如查询时所需的私有哈希表),减少高负载并发查询处理时对cache及内存资源的过度消耗,提高了OLAP查询处理的性能和多核处理器的利用率.(3)提出了面向多核处理器的中位数并行聚集计算技术,提高了依赖全局排序的不可分布聚集函数(如中位数、百分位数等)在多核处理器上的并行处理能力.本文第2节介绍相关工作;第3节阐述多核并行内存OLAP关键技术;第4节详细介绍多核并行DDTA-MPJ算法;第5节描述实验设计与实验测试结果;第6节总结本文的工作.Page32相关工作2.1基于列存储的查询优化技术列存储已成为分析型数据库普遍采用的存储模型,如C-store[2]、SybaseIQ[3]、MonetDB[4]等都采用列存储模型来支持面向少量属性的列式分析处理.通过将列数据连续存储来获取更好的压缩性能以及数据访问时的cache性能和I/O性能.但列存储中相关属性的访问要转化为列间操作,并产生大量列连接中间结果,相对于行存储的流水线处理模式而言,列存储会消耗更多的内存资源.PAX[5]将一个数据页中的元组按照列属性组织在一起,实现页内按列访问,增加了页内记录的空间局部性.MonetDB/X100[6]提出了向量计算模型,通过positional-join算法访问cache中的向量来消除传统列存储在多列处理时的物化代价.文献[7]进一步提出了块内行-列转换技术,根据不同处理特点可以动态选择优化的存储模型.Datamorphing[8]根据负载特征将关联列组合在一起存储,当访问关联属性时可以提高cache性能.但它是针对特定查询负载的,需要对数据的物理分布预先进行处理.对于内存数据仓库,查询负载具有相对稳定的特点,可以采用类似技术将关联度高的属性组合在一起存储,提高多属性访问时的内存带宽效率和cache性能.InfoBright[9]把64KB个的行记录组成一组,称为数据包,然后将每个数据包中的数据按列存储,相当于水平存储和列存储的组合策略.Vertica[10]的事实表采用列存储,按照相关属性分成几个投影,从而可以增强查询时的数据局部性,提高I/O效率.基于列存储的查询优化技术有两种:一种是采用列存储数据而查询分析采用行存储处理技术,如Greenplum,InfoBright,C-store等.这种技术在内存中将数据由列存储转换为行存储,以流水线方式完成OLAP查询处理.另一种是采用列存储数据,查询分析采用列代数查询处理引擎,如MonetDB、invisiblejoin[11]等.2.2多核并行查询优化算法多核处理器采用共享最后一级cache(LastLevelCache,LLC)的硬件架构,优化cache性能是提高多核处理性能的基础.相对于cache容量而言频繁访问数据集越小越容易提高其在cache中的驻留能力.因此在设计算法时需要最小化各种频繁访问的数据集,例如查询处理时用到的连接哈希表和分组聚集哈希表等中间数据结构.对于哈希连接操作而言,哈希表是连接操作中的强局部性数据集,radix-cluster[12]通过以基数为分区的方法来控制哈希分区的最优数据集大小,提高哈希表的cache访问性能.在多核并行查询优化技术方面,以MonetDB为代表的列存储数据库将列数据按照处理器核心进行逻辑分区来支持连接操作的并行化.但局部连接结果需要合并后才能执行后面的操作,在整个查询执行计划中仍然具有较高的串行负载比例.IBM的BLINK[13]技术使用非规范化的存储模型将复杂模式转换为一个大的连接表,然后以“memorybank”为单位进行压缩后再划分列存储单元.各列之间通过positionaljoin算法来支持行式访问.这种行式处理技术消除了复杂的计算,具有接近线性的并行加速比.但由于要将较小的维表属性物化到庞大的事实表中,复杂谓词(如字符串比较等)需要进行大量的重复计算,同时也造成存储空间的极大膨胀,多核并行的优化收益受到冗余数据访问代价的影响.DDTA-JOIN通过谓词向量和键值-地址映射来实现join-free(消除传统哈希连接代价)的星型连接,在不改变模式的前提下获得与BLINK一样的线性处理性能,并且降低了复杂谓词的操作代价.在多核并行算法方面DDTA-JOIN与BLINK具有相同的加速比,但却大大降低了BLINK所造成的存储空间的膨胀,提高了存储效率.对于通用的多核并行查询优化算法,需要从查询执行计划优化、线程调度优化和并行连接操作优化几个方面来提高多核处理效率.在多核平台上,文献[14]提出并行的基于动态规划的连接枚举器,对于复杂的查询可以产生优化的查询执行计划,文献[15]针对多线程查询优化技术进行研究,文献[16]提出基于并行分区,内存缓冲池和适于多线程处理的数据结构的系统架构.在研究连接操作的多核并行优化算法上,文献[17]在两个不同的多核硬件平台上,分别对无分区的哈希连接,共享分区的哈希连接,无共享分区的哈希连接以及Radix哈希连接进行实验得出这样的结论:传统多核并行优化技术所采用的哈希算法是基于哈希数据预分区的,因此需要付出预处理代价,而这个代价高于并行哈希算法所带来的cache命中率的提高.简单的哈希连接算法在多核平台上具有更好的综合性能,包括降低计Page4算代价、数据同步代价和优化cache访问模式,这比单纯地考虑cachemiss的算法更加适合多核平台.2.3中位数优化算法研究在内存OLAP应用中,可分布聚集计算函数能够充分利用多核性能来提高查询效率.文献[18-19]在多核平台上研究了这类聚集函数的优化技术.针对中位数等需要全局排序的聚集函数,OLAP的并行化处理性能会受制于串行算法这个瓶颈.文献[20]提出在无共享(Shared-Nothing,SN)集群上使用迭代中位数逼近算法来剪裁远离全局中位数的数据,减少全局合并中位数计算的数据传输代价.在多核并行中位数计算时,数据分片可以采用根据内存偏移地址段的逻辑划分来消除SN集群中的物理划分代价,同时需要根据共享cache容量对迭代剪裁次数和全局合并数据集大小之间进行优化选择,以获得最佳的整体并行处理性能.文献[21-23]针对不同应用场景下的中位数算法进行了研究,但在多核OLAP应用场景下的并行中位数计算的研究还没有成熟的技术,通常采用的是集中式排序方法获得中位数,没有充分对中位数计算进行并行化处理.3内存OLAP实现技术多核内存OLAP在存储模型上采用维表内存列存储,事实表内存行存储的混合存储模型,事实表可以根据负载的数据访问特性采用和DataMorphing类似的行/列存储模型,提高数据访问时的cache性能.内存OLAP查询优化技术主要有两个方面:一是提高查询处理时数据访问内存的带宽效率,增强内存访问性能;二是优化OLAP的星型连接操作,提高连接操作在CPU上的代码执行效率.3.1存储模型实现技术(1)存储模型我们所研究的多核内存OLAP采用磁盘-内存二级存储层次上的行/列混合存储模型.事实表和维表的原始数据存储于PostgreSQL中.系统启动时事实表从磁盘加载到内存并转换为内存存储模型,在加载时可以根据查询负载中属性之间的协同访问频率来动态选择行存储、列存储和聚集列存储.如果事实表中所有属性经常共同访问,可以采用行存储;如果部分属性经常共同访问,可以采用聚集列存储;如果属性间相关性不大,可以采用独立的列存储.维表从磁盘加载到内存并转换为内存列存储模型,数据宽度较大的字符型属性可以通过数据压缩技术来进一步减少内存存储开销.访问维表上时主要集中在对谓词属性和低势集分组属性的访问上,事实表和维表主-外键的约束决定了维表属性是频繁访问的数据集,因此采用列存储模型来减少频繁访问数据集的大小.与键值-地址映射相结合,维表可以作为多核并行OLAP的共享哈希表,采用列存储保证在连接操作时具有较小的参照数据集,在高负载多核并发查询处理时可以避免产生私有哈希表,从而减少内存开销和对cache的争用.事实表由定长的外键属性和度量属性组成,当事实表采用行存储时,能够根据内存事实表起始地址计算出每一个分片的起始地址,在多核并行处理时可以按内存行偏移地址动态划分水平分片,实现查询处理负载并行化.由于维表采用内存列存储和键值-地址映射的连接技术,因此事实表不需要按外键键值进行哈希分区,可以简化查询处理,减少数据分区代价.(2)事实表聚集存储表扫描操作的效率和性能取决于有效访问属性宽度总和占记录总宽度的比例,本文将此比例称为访问密度.例如,在SSB(StarSchemaBenchmark)数据库中,事实表有17个属性,但在全部测试查询中只有7个属性(属性个数比例为41.2%)被使用,访问密度为34.2%.我们按照事实表属性的访问频度进行聚类的方法来垂直划分事实表,将整个事实表划分为频繁访问和非频繁访问两个垂直分片.频繁访问垂直分片中包含维属性、频繁访问的度量属性和附加的分片连接属性,非频繁访问分片中只包含度量属性和连接属性,即:FF(D1,D2,…,Dn,Mi,…,Mp,Key)∪其中,(Mi,…,Mp)∪(Mj,…,Mq)=(M1,M2,…,Mm).当事实表由磁盘向内存加载时完成事实表聚集列存储,两个聚集存储表在内存中通过键值-地址映射的方式实现两个表按相同的内存偏移地址进行访问.不同于传统的基于分区连接的访问技术,这种方法消除了大事实表分区之间的连接代价,从而降低了查询中需要跨聚集表访问所造成的记录重构代价.未来研究中,可以进一步根据事实表属性间的关联性实现更细粒度的垂直分区,如将频繁访问的外键、组合访问的度量属性以及独立访问的度量属性Page5分别作为聚集列存储粒度来提高内存访问效率.3.2查询处理模型实现技术(1)查询处理模型我们所研究的多核内存OLAP的基础算法是DDTA-JOIN,该算法是基于行扫描方式的连接优化技术,它将OLAP中星型连接对应的查询树转换为基于键值-地址映射的维表记录直接访问或谓词向量过滤操作,实现join-free查询处理.整个OLAP处理过程分为3个阶段:DDTA-JOIN连接阶段,group-by操作阶段和order-by操作阶段,从而可以将复杂的多维数据处理转换为类似单表扫描操作的线性处理过程,更加适合在多核处理器上并行处理.(2)键值-地址映射(Key-AddressMapping)多核内存OLAP中,维表采用内存列存储,每个维属性被存储在独立的内存数组中.各维表的主键大多数没有特殊的语义信息,一般为连续递增的数值.数据仓库中数据的只读特性保证了维表记录的稳定性,因此在维表主键和内存维属性数组下标之间能够建立一一映射关系,即fMap为映射函数,则有fMap(x|x∈FactDi)=AddressOf(T|T∈Di∧T(PK)=x),FactDi为事实表中第i个维属性,Di为第i个维表,T为维表记录,AddressOf(T)为该维表记录所有属性值的内存偏移地址.通过键值-地址映射机制,事实表记录中的外键值对应为维属性的偏移地址.星型模型可以看作是一个虚拟连接表,而事实表中的外键值相当于内置的维属性内存地址指针.因此基于事实表外键与维表主键值之间的连接操作可以简化为直接按内存偏移地址进行访问操作,这个连接操作仅需要一个CPU指令周期即可完成.而传统的时间复杂度为O(1)的哈希连接(包图1存储模型和DDTA-JOIN算法(4)Predicate-Vector谓词向量压缩技术在OLAP查询中,维属性的用途是连接操作过滤器,并提供查询所需要的分组属性值.由于谓词属括键值哈希映射,哈希探测和键值匹配等操作)则需要多个CPU指令周期来实现以哈希探测为基础的连接操作,这种键值-地址映射的方式有效地降低了连接操作的CPU代价.(3)DDTA-JOIN算法如图1所示,DDTA-JOIN完成多维数据分析处理模型中事实表与全部维表的连接操作.在维表内存列存储和键值-地址映射函数两种机制的支持下,事实表被映射成一个虚拟的连接表,复杂的多表连接操作被规范化为在虚拟连接表上的选择和投影操作.采用与join-order相似的谓词树优化技术来优先处理低选择率的谓词操作,从而可以减少无效谓词的对维属性列的访问和谓词处理代价.例如事实表记录(1,3,2,3)在扫描时,维属性值1,3,2分别映射为内存维表D1,D2,D3上的维属性数组下标0,2,1.查询中的谓词表达式按选择率被优化为(D1.A>a∩D2.B=b)∩(D3.C>c∪D3.D<d),因此最终执行的谓词表达式为(D1_A[0]>a∩D2_B[2]=b)∩(D3_C[1]>c∪D3_D[1]<d),其中D1_A表示维表D1中属性A对应的数组,以此类推.当谓词表达式的结果为TRUE时,按外键映射地址直接访问分组属性列中指定位置的数据并与度量属性值合并为连接结果,然后再进行group-by操作.DDTA-JOIN与传统的多表连接操作相比最大的区别是将复杂的连接树扁平化为线性的谓词表达式运算,使维表向量成为不同查询的共享数据.在多核并行处理时,每个事实表分片可以直接映射到共享的内存维表列中,不需要构建额外的哈希表;同时多核并行访问可以增强维表列的访问频率,提高维表列在cache中的数据局部性强度,提高cache命中率.性是频繁访问数据集,因此减少谓词属性数据集的大小对提高cache利用率和CPU效率具有重要的促进作用.Page6谓词向量数据压缩技术(predicate-vector)是将维表上全部的谓词操作进行预计算,如果当前维属性满足全部n个谓词操作条件则在位图(bitmap)中将该记录置1,反之则置0,即bitPredVector[i]=δ1δ2…δn(Ti).通过在维表上预处理谓词操作的方式,可以获得该维表谓词操作结果的位向量Pred-Vector[]={0,0,1,0,1,…},用谓词向量代替维属性即可完成谓词的判断操作.采用谓词向量技术后,执行该维表上所有谓词操作所需要的频繁数据集大小为Nbit(N代表维表记录行数).这样做极大地减小了查询处理过程中的频繁数据集,而且不需要对谓词操作进行重复计算,在查询处理时进一步降低了共享cache的空间开销.在SF=8的SSB测试集中,维表总行数为858555,所产生的谓词向量总大小为0.1MB,而我们在实验中所用的处理器L2cache为4MB,谓词向量极大地缩减了对共享cache的需求.在并行OLAP中,谓词向量作为查询的共享访问数据集只需要保留一份,不会因处理线程的增加而增加.4多核并行DDTA-MPJ算法及实现根据文献[17]的结论,我们采用两个关键技术来保证多核并行时多表连接查询算法的性能:一是采用谓词向量和共享内存分组维属性列作为连接操作中的公共连接过滤器和分组属性抽取器.这种方法不需要按连接属性为每个查询处理线程进行数据分区和创建私有哈希表,从而可以减少数据分区预处理代价和大量私有数据所造成的cache争用;二是在键值-地址映射机制的支持下,将事实表与维表之间的哈希连接操作简化为按映射地址直接进行内存访问,同时将连接操作优化为直接按位置访问谓词向量.由于线程间共享谓词向量能够增大其在cache中的访问频率,提高数据的cache驻留性.这样可以减少每个线程的数据空间开销,降低线程在共享cache中的空间争用,从而可以最小化事实表与维表之间的哈希连接代价.OLAP中有两类并行处理需求:查询内并行和查询间并行.查询内并行是将一个较大的查询任务分解为多个较小的、可并行处理的查询任务,从而减少单个查询的执行时间;查询间并行是通过并发查询处理技术同时完成多个查询任务,减少并发用户的等待时间,提高多核处理器的效率.多核并行内存OLAP能够极大地提高ad-hoc查询性能,从而使传统的OLAP突破性能瓶颈,服务于更广大的分析用户.由于查询负载不断增加,这使得OLAP并发查询性能变得更加重要.影响多核并行算法性能的主要因素有:查询处理中并行化负载与串行化负载比例,并行处理过程中私有数据空间开销所导致的cache容量冲突以及内存带宽争用所产生的数据访问延迟.DDTA-MPJ将查询树扁平化,可以最小化查询处理过程中的串行负载;同时采用共享维表和谓词向量压缩技术最小化查询处理所需的私有数据,从而降低了对内存带宽的需求.DDTA-MPJ针对查询特点提出三种查询处理算法:查询内多核并行算法(IntraDDTA-MPJ),查询间多核并行算法(InterDDTA-MPJ)和中位数多核并行算法(Median-MPJ).4.1查询内多核并行算法IntraDDTA-MPJ在多核并行内存OLAP查询执行的3个阶段中,DDTA-JOIN是在数据子集上完成并行处理,Group-by操作和Order-by操作完成全局分组和排序任务.因此IntraDDTA-MPJ可以采用两种并行实现技术:第1种方案是完全并行模式,如图3(a)所示.将内存事实表按可用的处理器核心动态划分为若干个水平分片,然后在每个水平分片上并行执行查询处理任务.各个并行子查询任务共享访问维表向量,消除子查询的私有数据集开销,提高数据访问的局部性;各子查询维护独立的分组聚集表并生成已排好序的查询结果子集,最后将各个子查询的结果子集进行全局归并排序,生成最终的查询处理结果.算法1.完全并行IntraDDTA-MPJ算法.IntraDDTA-MPJ(SQLStatementST){//将SQL分解为各个维表上的子查询SQLParser(ST)→STD={STD1,STD2,…};//生成各维表位图谓词向量GenPreBM(STD)→PreBM={PB1,PB2,…};//根据事实表行数和线程数初始化查询处理线程,分Page7GetFactTableRows()→FRows;GetThreads()→th;InitQueryThreads(FRows,th,PreBM)→{Qth1,Qth2,…};//并行查询处理ParaDDTA_JOIN()→RS={RS1,RS2,…};GlobalMerge(RS)→FinalRS;RETURN(FinalRS);}这种方案能够实现任务的最大化并行,将串行结果集的合并负载降到最低.但每个查询处理任务需要重复使用基于哈希的group-by数据结构,当查询结果集较大时,各个子查询独立的哈希聚集表会图3IntraDDTA-MPJ算法完全并行方案中数据之间独立,在共享哈希聚集表上无写访问冲突,但需要为每个查询处理线程维护私有的哈希聚集表,这种方案适合于查询结果集较小的应用场景;n+1并行模式在哈希聚集阶段采用共享的数据空间,节省了线程的私有数据存储空间,减少了连接处理阶段的并行处理资源.但会产生线程间的共享访问冲突,需要并发控制机制来产生较大的cache争用问题.第2种方案为n+1并行模式,如图3(b)所示.将全部的多核处理线程分为两个部分,n个线程用于并行的DDTA-MPJ查询处理,1个线程用于group-by和order-by操作.这种方案能够减少并行查询处理时的资源争用,合理分配不同操作符的处理资源.StagedDB[24]为不同的操作符分配独立的处理线程,这些处理线程供全部查询任务共享.n+1并行模式吸收了操作符共享并行的思想,通过DDTA-MPJ算法减少了查询处理中的操作符数量,同时为并行收益最大的操作符分配最多的处理资源,从而可以提高并行查询处理的性能.保证正确性;同时也需要独立的线程来处理分组和排序操作.这种模式适合于查询结果集较大的应用场景,它是以牺牲部分并行处理资源来减少查询子任务在cache中的开销.文献[18]提出了根据group-by结果集大小选择最佳查询处理策略.我们在未来研究中会根据查询结果集的估算动态选择完全并行模式或者n+1并行模式来优化查询处理性能.Page84.2查询间多核并行算法InterDDTA-MPJ在磁盘数据库并发查询中,表扫描的代价较大,相对而言查询处理的执行代价较小,因此研究主要集中如何在并发查询之间共享表扫描操作.并行的收益取决于TI/O/TQ,即在一个I/O延迟内能够完成多少并行查询任务.其中TI/O表示一次磁盘I/O访问时间,TQ表示并行查询时间.对于内存数据库而言,查询的延迟主要由cachemiss决定,由于在cache上无法创建可管理的缓存区,因此无法保证查询处理时所需的较小数据集能够被“pin”在cache中.cache替换算法LRU在对弱局部性数据(不经常访问的数据)顺序扫描时会将强局部性数据(经常访问的数据)替换出cache,造成“cachepollution”.当前多核平台上的cache优化算法要么需要操作系统层次上的支持(如pagecoloring技术),要么需要针对特定处理类型的指令级进行优化,难以具有良好的通用性.因此,在当前硬件和操作系统的基础上,提高cache性能的关键是减小查询处理时强局部性数据集的存储空间从而可以提高它在cache中的驻留性.本文所研究的并发查询优化技术主要集中在优化查询处理模型、减少查询所需私有数据的存储空间、优化并发查询的数据局部性强度上.研究的模型和算法不依赖于具体的操作系统和硬件配置,算法具有较好的通用性.磁盘数据库的代价模型以I/O为核心,其理论基础是磁盘缓冲区可控制,系统能够保证查询所需的数据被“pin”在内存中.对cache层而言,没有软件级的替换算法,因此我们采用数据距离(DataDistance,DD)来评估多核内存OLAP的查询代价.例如将某个操作对应的数据按照cacheline大小划分为数据子集{D1,D2,…,Dn},多个数据子集可以存在于同一个cacheline中,一般以LLC的命中率作为代价评估函数的依据.因此数据操作OP的代价为其中DD(Di)=1,Di缺失时访问特征和cache替换策略下数据缺失的概率.例如在哈希连接过程中,表扫描是顺序操作,因此下一个数据在cacheline中缺失的概率为1,而哈希表中频繁访问数据项在cacheline中缺失的概率则小于1.η需要通过统计或经验值进行估算来调整代价模型与真实查询代价的拟合程度.在高负载并发查询处理时,总的私有哈希表空间随查询数量而线性增长,因此降低哈希表中数据缺失概率的关键是减少哈希表的空间大小.文献[25]提出了基于共享哈希表的OLAP并发查询优化技术,将共享哈希表作为并发查询的过滤器,提高哈希表的共享性,消除并发查询的私有哈希表空间.由于OLAP查询选择率较高,高并发查询负载的私有哈希表可能大大超过原始维表大小.本文通过键值-地址技术将内存列存储维表作为并发查询的公共哈希表,以事实表外键作为哈希键值,维表列作为哈希桶,保证在高并发查询负载下公共哈希表空间的最小化,从而可以减少高并发查询时的哈希表访问代价.在DDTA-JOIN连接操作中,事实表记录FTuple的数据距离为|FTuple|/64B(cacheline的长度为64B),因此减小事实表记录宽度能够增加当前记录在cacheline中的命中率.维属性的数据距离可以通过谓词向量来减小,从而提高cacheline的命中率.由于OLAP以事实表行扫描为基础,各线程都需要进行顺序表扫描操作,因此对于并发查询而言,事实表的第一个扫描线程加载到cache中的数据需要较长周期被替换出cache.对于在相同事实表上的多个并发查询处理线程而言,第1个扫描内存事实表的线程为其后的线程提供了更高的cache命中率.虽然在内存数据库中难以设计类似磁盘数据库的共享扫描机制,但在线程并行的粒度上同时执行多个查询任务有助于提高cache数据的共享程度.5.3节的实验结果也证明了基于批处理的并发查询处理模式能够高效地利用共享数据来提高cache命中率,从而提高并发查询性能.根据不同的线程并行策略,我们提出3种InterDDTA-MPJ算法.(1)Query/Thread如图4所示,事实表和维表被多个查询处理线程共享访问,每个查询处理线程实现完整的DDTA-JOIN处理,维护独立的哈希group-by结果集.各个查询处理线程之间不需要严格的同步,新的查询请求可以动态提交,其cache性能受cache替换策略的影响.(2)QuerySlots并行Query/Thread降低了CPU代价但加大了对内存访问带宽的需求,降低了并行查询性能.通过QuerySlots(查询槽)将并发查询分组,如图5所示,每组查询作为一个slot,分配一个查询线程,然后分组批量执行查询.这样可以提高事实表扫描操作的共享性,增加查询处理时CPU的负载,降低对内存Page9图4Query/Thread算法图5QuerySlots算法能收益.带宽的需求;同时也可以提高cache数据的共享度,降低线程管理的开销.但是QuerySlots中多个查询所对应的哈希group-by结果集为私有数据,并且批量执行的查询数量也增加了线程对私有数据空间的需求,导致比Query/Thread并行方法更大的cache容量冲突.QuerySlots模式由于事实表共享访问可以带来收益,但同时也由于私有数据空间增加需要付出额外的cache代价,因此需要综合评估它的性(3)IntraInterDDTA-MPJ图6是在IntraDDTA-MPJ基础上实现的分组批量并行查询,每个线程在独立的数据分片上完成局部查询,然后通过全局合并完成最终的查询处理.这种方案既提高了长程查询(long-runquery)的处理性能,又能较好地满足并发查询处理的需求.方案(1)的每个线程执行的查询彼此独立,没有Page10图6IntraInterDDTA-MPJ算法同步代价,线程数量可以根据负载强度确定.在实验中通过对并发查询强度的测试来寻找加速比性能曲线的拐点,从而测试出系统最大的查询并行支持度.方案(2)和(3)采用固定的线程数量减少线程同步和维护代价,查询通过QuerySlots进行分组管理,这两种查询方式的任务集中并可以周期性地批量执行.为了收集实时产生的查询请求,我们将方案改进为每个查询分组初始化两个同构的slot,每个分组仍然分配一个查询处理线程.这两个同构的slot分别称为执行slot和聚集slot,执行slot用于执行并行查询处理,聚集slot用于收集用户实时提交的并发查询请求.当前的并行查询执行完毕后,自动地切换slot,执行slot与聚集slot完成功能互换并开始新的并行查询处理.通过QuerySlots自动替换机制,当系统的并发查询强度很大时可以实现循环的并发查询处理,充分利用系统的计算资源.4.3中位数多核并行算法Median-MPJOLAP中常用的两类聚集函数:可分布聚集函数和不可分布聚集函数.并行计算的关键是聚集函数的可分布计算特征.前一类聚集函数可以采用4.1节所示的并行算法在数据水平分片的基础上进行并行处理和全局聚集结果归并;而后一类聚集函数需要集中式处理,难以充分发挥现代多核处理器的并行处理性能.根据多核处理器的特点,我们将不可分布计算函数进行优化处理来提高查询性能.本文以中位数计算为例讨论基于全局排序的聚集函数的多核并行算法,其原理可以直接应用于百分位数和其他依赖全局排序的聚集函数.中位数是在全局排序数列中位于中间位置的数值.当数列的个数为奇数时,中间位置的数值就是中位数;当数列个数为偶数时,中位数则是处于中间位置两个数值的平均数.在多核处理器中可以将待排序数据在内存中按数据偏移地址进行逻辑水平分段(不进行物理划分以节约内存空间),并由多核处理器并行排序,然后通过多路归并排序方法生成全局排序数列后取中位数结果.分段并行排序提高了排序操作的性能但多路归并排序在多核处理器的共享cache中产生大量的数据迁移,降低了多核处理器的效率.我们在文献[20]基于SN集群上提出的迭代中位数逼近算法的基础上,面向多核处理器,提出了中位数排序序列剪枝算法,通过剪裁各个排序序列中全局中位数之前的数值来缩小全局排序侯选集的大小,从而减少全局排序时所需归并的数据量,降低内存带宽延迟.算法2.中位数多核并行算法Median-MPJ.ParallelHandleRecordSet(RecordSetRS){Page11ParallelSortSubSetASC(SS0,SS1,…,SSn)}m=total/2;//total为待排序数据总数RecurseGetMedianValue(){//比较各子数据集中值,得到最小的中值//计算全局剪裁的数据个数IF(pn<m)图7迭代并行中位数计算在多核并行中位数计算过程中,使用内存地址指针可以动态实现对分布式候选排序集的剪裁(调整排序序列起始位置指针);同时通过全局变量传递同步消息,具有较小的通讯开销.在处理过程中,剪裁是迭代的可并行处理过程,全局归并是一个串行处理过程.迭代的次数越多,通讯和CPU开销越大,但各处理线程中排序序列越接近全局中位数位置,全局归并排序的数据量越小.因此在多核并行中位数算法中,关键问题是需要根据实验测试并权衡迭代处理代价和全局数据归并代价对全局并行计算收益的影响.5实验5.1实验平台和系统设计多核内存OLAP是面向多核平台上的分析型}图7显示了迭代中位数计算的工作原理,我们以3个并行处理线程为例:(1)将数据按处理线程n(n=3)等分为分布式计算数据集,计算出全局数据项个数和中位数的位置m;(2)对分布式数据集进行并行排序生成序列S1,S2,S3,以m/n为度量在各排序序列中划分出剪枝候选集SS1,SS2,SS3;(3)取S1,S2,S3序列中的最小值(图7例中为min(17,15,19=15)),并以此最小值为阈值在候选序列S1,S2,S3中剪裁掉小于该阈值的序列并计算全局剪裁的数据个数pn(5+6+4=15);(4)分布式计算数据集S1,S2,S3起始位置指针跳过剪裁数据序列而生成新的逻辑数据序列;(5)迭代地在分布式排序数据集上进行中位数剪枝操作.当剪裁掉的数据项个数逼近全局中位数位置m时,即m-pnλ时,将各候选集中的前m-pn个序列进行全局归并排序,全局序列中的第m-pn位置即为全局中位数.内存数据库技术,初始化时事实表和维表存储在磁盘上,在系统运行时动态地加载到内存分别构造事实表内存行存储和维表内存列存储.我们在SSB测试标准上用C++开发了以图1所示算法为基础的多核内存OLAP原型系统,其中事实表采用聚集列存储模型,维表采用基于字典表压缩技术的列存储模型,我们在这个原型系统上进行了并行查询处理性能的测试.实验的硬件平台为一台HP350服务器,配置两个IntelXeonQuad-core四核处理器,处理核心主频为2.26GHz,每两个处理器核心共享4MBL2cache.操作系统为Ubuntu11.04,内存为6GB,72GB双硬盘镜像.5.2IntraDDTA-MPJ查询处理性能首先测试了IntraDDTA-MPJ并行查询性能,将数据量为8GB的SSB数据库的事实表动态地划分为2、4、8和16个水平分片,然后在每个分片上并Page12行地执行查询处理,最后将各子查询结果集进行全局归并.总体看来,IntraDDTA-MPJ算法能够获得很好的并行加速性能.从图8的并行查询执行时间和图9显示的并行加速比曲线来看,当线程数不超过4时,并行查询的加速比随并行处理的粒度增长而增长,与理想的线性加速比曲线(虚线部分)基本吻合.原因是两个处理器共有4个独立的L2cache,操作系统分配的查询处理子线程会选择能够独占L2cache的处理核心,因此不会产生线程间的L2cache竞争,体现出良好的并行加速比;当并行线程数量为8时,每两个处理器核心共享一个L2cache,会产生L2cache争用,降低了并行查询处理的性能;当并行线程超过处理器物理核心数量8时,多个线程共享处理核心,查询的并行性能仍然有一定的“边际效应”:随着并行任务的增加缓慢增长到最大并行加速比5.83,然后呈现缓慢下降趋势.其主要影响因素是cache争用、内存带宽争用以及线程切换代价.在DDTA-MPJ算法中存在两种不同局部性特征的数据:顺序访问的事实表是弱局部性数据,随机访问的维表是强局部性数据,这两种局部性数据需要通过“cache”优化策略来减少“cachepollution”带来的负面影响.当在处理器核心上运行多个线程时,需要优化数据在cache层上的访问策略来提高性能;同时也要针对内存带宽进行优化,减少带宽瓶颈所产生的性能影响.在未来工作中将针对cache替换策略和内存带宽性能做更加深入的研究工作.我们将多核并行IntraDDTA-MPJ算法与MonetDB做了性能比较,如图10所示,实验证明我们算法的总体平均执行时间要优于MonetDB,特别是Q4.1和Q4.2查询.IntraDDTA-MPJ在事实表行扫描操作的基础上将事实表与维表之间的连接操作转换为CPU代码效率极高的直接内存访问,因此查询处理时间相对稳定,受选择率影响但波动不大,是一种类似BLINK常量查询时间的算法.查询处理时间主要体现为不同选择率所产生的分组聚集时间代价,传统的连接操作代价被最小化,在多核并行处理时也能够保持接近常量执行时间的特性,查询时间的可预期性非常高.而MonetDB的查询性能受较多因素影响,如选择率,查询中要访问列的大小和数量,谓词操作代价(AND或OR操作)等.MonetDB的列存储机制在低选择率且列间连接操作较少的应用场景上具有优越的cache性能,图10可以看到MonetDB在查询Q2.x,Q3.2和Q3.4(Q3.3执行时间超长,可能是系统bug,实验中未统计其执行时间)中具有很好的性能.但在Q1.2,Q1.3和Q4.x中,由于查询包含了多个事实表列属性之间的连接操作以及物化的连接索引,这些都对MonetDB的性能产生了消极影响,使其在较低选择率的情况下仍有较长的查询处理时间.图10IntraDDTA-MPJ与MonetDB并行查询执行时间比较5.3InterDDTA-MPJ查询处理性能我们在实验中测试了两种查询词并发查询处理方案,IntraInterDDTA-MPJ可以看作是查询内并行与查询间并行的组合,在本文中不作进一步测试.(1)Query/Thread每个查询使用独立的线程完成查询处理任务,实验中并发查询的数量从1增长到1024,查询任务Page13采用SSB的13个测试查询,按照round-robin的方式循环调用.图11显示了查询处理时的并行加速比曲线,当并发查询超过物理核心数8时,并发查询线程相当于在处理器核心之间进行轮流切换,并发查询的收益增速放缓,并行加速比逐渐稳定在6.2这个最大值,当并发查询负载继续增加时,基本保持稳定状态.当使用8个并发查询时Query/Thread的加速比为4.08,低于IntraDDTA-MPJ算法的加速比5.58,原因是IntraDDTA-MPJ算法采用共享谓词向量技术来降低L2cache的数据访问冲突;而Query/Thread为不同的并发查询,采用直接访问维表列的方法,因此它具有更大的L2cache数据访问冲突.但IntraDDTA-MPJ算法需要合并线程间的查询结果,因此线程数量增长时全局数据合并开销也同步增长,其最大并行加速比为5.83,而Query/Thread并发查询处理不需要进行全局结果的归并,会随着并发查询线程的增加缓慢达到更高的并行加速比6.2.(2)QuerySlots并行如图9所示,实验平台配置有4个L2cache和8个核心,当采用4个线程处理查询时不会产生L2cache争用,可以获得最大的并发处理效率.当采用8个线程处理查询时可以充分利用多核处理器来获得很高的并发处理性能.因此我们预设4个和8个并发处理线程,同时将查询分成4组和8组,每组分配一个线程来分别获得最大的并发处理效率和并发处理性能.实验中设置两组查询测试,第1组设置4个slot,每个slot的查询任务量从1增长到256,第1组的查询任务总量从4增长到1024.查询时循环调用SSB的13个标准测试查询,每个slot中的查询任务各不相同;第2组采用8个slot,每个slot的查询任务量从1增长到128,第2组的查询任务总量从8增长到1024.QuerySlots为每个slot中的查询维护独立的group-by操作对应的哈希聚集表,slot越多,QuerySlots中私有哈希聚集表的空间开销越大.QuerySlots算法对slot中的查询任务批量处理,使顺序访问的事实表记录的利用率最大化,减少了线程切换代价,但同时私有数据空间的增大也加大了cache共享冲突的概率.从图11的并发查询结果可以得到这样的结论:在高负载的并发查询处理任务中,QuerySlots算法能够获得更高的并行加速性能.Query/Thread算法在并发任务数量超过处理器核心数量时,并行加速比逐渐趋于稳定.而QuerySlots算法在初期有一个波动,主要是由于slot中各查询执行时间不同,因此包含查询时间较长的slot组的性能会受到影响,呈现波动趋势.当查询负载增加时,每个分组中的查询执行时间相对平均,加速比曲线逐渐趋于平稳.与图9结论相近,QuerySlot-4(预设4个查询分组)方法的加速比接近线性加速比4,QuerySlot-8(预设8个查询分组)方法在具有较大并发查询任务(512个并发查询)时达到最高加速比6.245,而IntraDDTA-MPJ在32线程时达到最大并行加速比5.83.相对于IntraDDTA-MPJ而言,IntraDDTA-MPJ能够在高负载时获得更好的并行处理效率.QuerySlot-8并发查询处理比Query/Thread方法使用更少的线程,因此线程切换开销更低,最大加速比性能也高于Query/Thread方式.QuerySlot-4在4个L2cache的硬件配置下获得最高的并行处理效率,即并行处理效率=并行加速比/处理线程数=3.907/4=97.68%,但没有获得最大并行处理性能.增加并行处理线程虽然增加了多核并行处理时的额外开销,但能够获得更高的并行处理性能.综合图9与图11的实验结果,在当前4个独立L2cache数据通道和8个物理核心的硬件配置下,多核并行性能主要由硬件结构特点决定.与L2cache数据通道数量相同的并行处理粒度能够获得最佳的接近于线性的加速比,但不是最大的并行加速性能.最大并行加速性能主要受物理核心数量的影响,并在超出物理核心数量的并行负载下逐渐达到最大并行加速比,该加速比数值会低于物理核心的数量.DDTA-MPJ算法以表扫描操作为基础,在并发查询处理时表现出比较稳定的性能曲线,是一种可预估算时间代价的查询处理算法.但由于算法将复杂的连接运算转换为直接地址访问操作,因此在查Page14询总负载中使内存带宽的需求更加突出,在并行查询处理时需要更大的cache容量和更高的内存带宽支持.实验的平台是2个核心共享L2cache,这样的架构也降低了多核处理器可共享访问的cache总容量,制约了并行性能的进一步提高.新的处理器能够支持更大容量的共享cache和更高的内存带宽性能,能够进一步发挥DDTA-MPJ算法的优势.5.4中位数多核并行算法Median-MPJ性能我们在8核处理器平台上用8GB的SSB测试数据进行并行中位数计算实验.在实验中将需要计算的数据列全部加载到内存并通过划分数据逻辑分区的方式对数据进行水平分段,然后对分段数据进行并行排序,最后应用迭代并行中位数算法.在实验中我们设置2n个并行处理线程,在线程数超过128时我们以64线程为步长减小线程增幅以获得更细粒度的并行加速比曲线.从图12所示的并行中位数计算执行时间和图13显示的并行加速比曲线来看,并行中位数加速比达到最大值9时的线程数达到128个,也就是说中位数计算的并行收益超过可分布式并行聚集计算的InterDDTA-MPJ的并行加速比,也高于物理核心数量8.当线程数超过128个时,加速比曲线逐渐呈下降趋势.相比线性处理的IntraDDTA-MPJ算法,中位数算法分段排序的代价是O(nlogn),因此分段排序的并行收益较大.当并行线程较多时,分段排序的性能会有较高的收益.影响并行中位数计算的因素包括迭代剪裁操作的代价和全局归并的代价.当并行线程较多时,并行排序的时间代价降低,同时并行负载比重下降,但全局归并的串行负载比重会上升,从而对并行加速比产生负面影响.通常的观念是并行处理线程数量与物理线程数量相同时(多核处理器中一个处理核心支持一个物理线程)并行收益最高,当并行处理线程超过物理线程时,线程之间采用轮流切换的方式,会影响整体并行处理的收益.从我们的实验结果来看,在8个物理核心的多核处理器平台上,并行中位数算法在128个处理线程时获得较高的并行加速比(并行加速比为9,超过物理核心数量8),能够更好地发挥多核处理器的性能.6结束语本文针对内存OLAP提出了多核并行查询优化技术,充分发挥了多核处理器的优势,提高了查询处理性能.我们所提出的DDTA-MPJ算法采用了优化的存储模型,分别实现了查询内多核并行,查询间多核并行以及不可分布聚集函数的多核并行.DDTA-MPJ通过优化多维数据上的连接操作来提高CPU效率,优化维表存储结构实现维属性直接按地址随机访问.这样可以充分发挥内存的随机访问性能,也充分利用了多核的特性,简化了查询处理模型,消除了私有哈希表所带来的cache冲突,提高了cache性能.实验结果证明DDTA-MPJ在大内存、多核平台上具有良好的并行处理性能,并且能够与现有的磁盘数据库技术保持良好的兼容性.多核内存OLAP是内存数据库在多核时代的技术延伸,它结合最新的硬件技术,在存储访问性能和并行处理性能两个物理层面来进一步优化数据库的性能,也将成为新一代高性能内存分析处理的基本特征.随着多核处理器技术的发展,在未来的多核和众核计算平台上,单节点能够提供几十甚至上百个并行计算核心或物理线程,数据库面临着节点内的大规模并行处理需求.因此需要研究更加简单和易于扩展的并行数据库软件架构,来更高效率地利用处理器强大的并行计算资源,提高数据库在高性能计算平台上的处理能力.Page15
