Page1基于隐式用户反馈数据流的实时个性化推荐1)(中山大学信息科学与技术学院广州510006)2)(广东东软学院国际合作部广东佛山528225)摘要大多数的传统推荐系统是基于用户评分构建,并采用离线批量的训练模式.该文研究以下两个问题:(1)基于隐式用户反馈构建推荐系统.与显式评分相比,隐式反馈存在范围更广且更易于收集;(2)基于反馈数据流进行实时推荐,以此来保障更强的推荐时效性.为了克服由隐式反馈本质特征导致的不平衡类标问题,直接对可观察的用户选择行为进行概率建模,在训练时无需引入负样本.为了提高训练效率并及时抓住用户兴趣的变化,该文提出的在线学习算法在强化学习用户新倾向的同时弱化了学习用户惯常行为与噪声,通过比较反馈发生概率与用户置信度来为每一个反馈动态调节学习步长.最后,该文设计了在线评价机制,并在两个真实数据集上进行了丰富的实验.实验结果验证了所提方法的有效性,并展示了其在推荐精度、推荐多样性、可解释性、训练效率、健壮性以及冷启动适应能力等多个方面的优势.关键词隐式反馈;在线学习;推荐系统;大数据1引言推荐系统已经成为解决信息过载问题的重要工具.它通过对用户兴趣倾向进行建模,向用户主动地推送其感兴趣的信息,进而提供个性化服务.近年来,推荐系统的研究已经取得了一系列进展[1].大数据时代对推荐算法与推荐模型提出了新的需求与挑战.1.1大规模隐式反馈数据利用大多数传统的推荐系统[2-3]是基于用户评分来构建的.用户评分虽然能显式地反应出用户的喜恶倾向,但并不总是可得的.与之相对的,隐式用户反馈(ImplicitUserFeedback)普遍存在于互联网生活中.用户在观看电影、收听音乐或浏览朋友圈时的多种形式的选择行为都可以看成是隐式反馈.由于它是用户在使用系统服务的过程中不自觉留下的,所以它的收集成本很低且不影响用户体验.也正因为隐式反馈具有范围广且易于收集的优良特性,其数据规模往往较大.这种反馈虽然能隐含地表达出用户正的倾向(喜欢),但不能表达用户负的倾向(不喜欢).这种只有正反馈而没有负反馈的设置成为了传统推荐模型的巨大障碍[4].因此,基于隐式反馈构建推荐系统的研究具有理论意义和极强的现实应用价值[5].1.2在线学习与实时推荐推荐的时效性会极大地影响推荐质量与用户满意度[6].通过整合时间信息,动态推荐模型[7-8]能够提高推荐准确率.但是,这些模型大多采用离线批量的训练模式,因此存在以下局限性.(1)训练效率低下与高时效性需求的冲突批量训练是指通过收集一批训练样本进行一次性学习得出模型参数的训练方式.为了保证训练出的模型的可用性,我们需要隔一段时间后再重新训练.这种方式的训练效率较低.特别是当数据规模较大时,一方面,训练时间不可能被无限压缩;另一方面,频繁的重新训练将导致计算成本过高.因此,离线批量的训练模式难以满足高时效性的需求.(2)信息利用不及时进而影响推荐质量离线训练也就意味着无法及时利用最近的用户反馈信息,因为在训练时来不及收集这些信息.只有等到下一次重新训练时,这些信息才能发挥作用,但有可能到那时,这些信息已经“过时”了.然而,最近的用户反馈极具价值,因为它们反映了用户的最新兴趣或当前需求.这种信息损失势必影响推荐质量.此外,离线训练还是导致冷启动问题的根本原因之一.之所以难以对“新用户”进行推荐是因为在训练过程中推荐系统没有学习过该用户的反馈.而事实上,该新用户很可能在最近的几分钟内已经提供了反馈信息,只是离线系统还未来得及加以利用.在线学习与实时推荐的框架可以克服以上两点局限.如图1所示,用户反馈数据以数据流的形式持续进入系统,对于每一个用户反馈,推荐系统都能够及时接收并在线更新模型参数,提供实时的推荐服务.该框架避免了重新训练,因而显著地提高了训练效率,从根本上解决了训练效率低下与高时效性需求间的矛盾[9].该框架能够在最短时间内捕获用户兴趣的新变化,同时压缩了冷启动空间,因此能提高推荐质量.但是,在线流式计算系统对噪声较为敏感,因为采用流式的数据处理方式则难以进行数据清洗和噪声过滤,与此同时,在线环境更容易受到恶意攻击(比如由爬虫发起的大量随机访问).这就对在线推荐模型的健壮性,特别是抗攻击能力提出了更高的要求[10].在此背景下,本文研究了如何基于隐式用户反馈数据流来构建在线的推荐模型,提供实时的个性化推荐服务.本文工作致力于提高推荐系统在适用范围、训练效率以及推荐质量等多方面的能力,主要贡献包括:(1)提出了隐式反馈推荐模型,通过在概率生成模型的基本框架下最大化可观察用户的选择行为发生的概率,将推荐问题转化为优化问题,进而解决了隐式反馈数据导致的不平衡类标问题;(2)通过对用户敏感程度以及从众程度进行个性化设置,给出了整合隐式用户反馈、产品特征、流行度等多种异构信息的应用范例;(3)采用在线学习方式,提出了以较低的训练开销处理大规模流式反馈数据的在线推荐模型.该模型采用动态学习步长,在及时抓住用户及环境动Page3态变化的同时,降低了噪声的影响;(4)设计了在线评价机制并在两个真实数据集上进行了丰富的实验,验证了本文提出的模型不仅在推荐质量与训练效率上存在明显优势,更在推荐结果多样性、可解释性、健壮性以及冷启动适应能力等方面具备竞争力.2相关工作2.1基于隐式反馈的推荐传统推荐模型难以直接应用于隐式反馈推荐场景,这是因为隐式反馈数据本身只包含用户正的倾向而缺乏用户负的倾向.Pan等人[4]最早将这一问题定义为OneClassCollaborativeFiltering.更一般地,可将其概括为不平衡类标问题(UnbalancedClassProblem)[1],其核心在于,数据集中正负样本极度不平衡导致难以训练.目前,应对此问题的总体思路是引入负样本.总结起来有3种方式:(1)基于规则指定负样本.在某些特殊的场景中,可以大概率地认为某些未知标签的样本是负的.比如认为夹在两条被转发的微博之间的其他未被转发的微博为负的,因为很可能用户浏览了它们但并不喜欢[11].由于依赖于领域知识,该方法的推广性不强;(2)从未知标签样本中随机抽样作为负样本.这种方法一般假设未知标签样本中大多数为负样本,所以随机抽样得到的样本很可能是负的[4,9,12];(3)将未知标签样本都作为负样本,但设置较小的权重.权重反映了这些样本是负样本的置信程度[4,13].其矛盾点在于:为了训练不得不引入负样本,又无法保证引入的负样本是“真负”.本质上这些方法都试图去平衡这一矛盾.此外,引入负样本会增加训练负担,数据规模较大时将影响训练效率.本文所提出的模型则采用概率生成模型的技术思路,通过最大化已观察到的用户反馈发生的概率来直接对用户选择倾向进行建模,无需负样本即可进行训练,因此天然地适用于隐式反馈推荐场景.2.2动态推荐与实时推荐近年来,有着完备的概率解释,较高预测准确率和良好可扩展性的概率矩阵分解(ProbabilisticMatrixFactorization)[2]已经逐渐取代传统的基于邻居的协同过滤算法[3]并成为主流模型.在此基础上,通过整合时间信息,timeSVD++[7]与DMF[8]等动态模型进一步地提高了预测准确率.但是这些动态模型依然遵循离线批量训练模式.为了降低离线训练方式高昂的重计算开销,Abernethy等人[14]提出了在传统矩阵分解模型上使用在线梯度下降算法(OnlineGradientDescent,OGD)来实时地更新模型参数.Zhao等人[15]从交互的角度出发来设计推荐策略,用在线学习的方法使推荐系统能够在尽可能短的交互过程中掌握用户兴趣.这些实时推荐模型都是基于用户评分的.Diaz-Aviles等人[9]在基于隐式反馈数据流构建实时推荐模型的方向上进行了积极的探索,提出的模型RMFX采用抽样负样本的策略,对用户转发微博的行为进行排序学习.这些模型都采用固定学习步长的方式来进行在线学习,将所有用户反馈(包括噪声)不加区别地对待,因此限制了学习效果.而本文提出的在线学习方法根据用户反馈的价值自动调节学习步长,能有效地提高学习效果.3模型本文要研究的问题是如何基于隐式用户反馈数据流构建在线推荐模型,该模型能够利用用户反馈从而在线更新模型参数并产生实时推荐结果.本节首先介绍了基于隐式反馈的统一推荐模型IFRM.然后给出了通用应用范例展示如何灵活地整合用户反馈、产品描述以及流行度等多种异构信息.最后介绍了IFRM的在线学习算法.3.1隐式反馈推荐模型(ImplicitFeedbackRecommendationModel,IFRM)隐式反馈数据的特点是只有正样本而缺乏负样本,为了避免在训练时需要引入负样本,我们采用概率生成模型直接对用户选择行为进行建模.其核心点在于对观察到的用户选择行为发生的原因进行合理假设.我们假设用户选择行为受用户对产品的“选择倾向度”决定,而这种倾向性程度是相对而言的.比如,用户之所以会选择某产品是因为用户对该产品具有比一般产品更高的选择倾向.基于此,首先给出如下形式化的定义.定义1.用户i选择产品j发生的概率Prij由Δij决定.Δij描述了用户i对产品j的相对选择倾向程度.Δij与选择倾向度Aij以及平均选择倾向度Ai相关:Page4其中,M是总产品数,φ(x)=其作用是将Δij归一到(0,1)区间.值得注意的是,这里也可采用标准sigmoid函数φ(x)=ex/1+ex,但本文采用的形式利于在线学习(详见3.3节),也能增强推荐结果的可解释性(详见用户置信度θ的计算及相关实验).可以将Aij看成是与用户i和产品j相关的函数,其形式可根据具体应用场景中可利用信息的不同而灵活设计(见3.2节),在此暂不详细展开讨论.将观察到的用户选择行为的集合记为O={〈i,j〉|用户i选择了产品j},假设选择行为是相互独立的,则似然概率为P(OΘ)=∏〈i,j〉∈O其中,Θ泛指模型参数,与Aij的具体设计相关.应用贝叶斯公式并假设Θ服从均值为0,方差为σ2的高斯先验分布,可得后验概率:P(ΘO)∝P(OΘ)P(Θ)=∏〈i,j〉∈O训练的目标即为最大化后验概率,对式(4)取对数并取反后,得到如下等价的优化目标:argminΘL·其中·2控制参数复杂度.观察优化目标可知IFRM具有如下3点优势:(1)仅依赖于用户选择行为集O,训练时无需负样本,因此天然适用于隐式反馈推荐场景;(2)提供了概率框架但未具体定义模型参数Θ,因此可推广性强,3.2节给出了推广应用范例;(3)目标为连加形式,易于在线学习,详见3.3节.转化为最优化问题后,可以使用随机梯度下降法训练得到模型参数Θ.具体来说,首先随机初始化全部的模型参数,重复地从训练集O中抽取一个样本,沿着导数负方向更新相关参数,直至算法收敛或达到迭代次数上限.在推荐时,由Θ可以计算出相应的选择倾向度Aij,向用户i推荐具有较高Aij的产品.3.2IFRM应用范例本小节将介绍一种可适用于大多数隐式反馈推荐场景的IFRM应用范例,展示如何通过合理设计选择倾向度Aij来整合多种异构信息.首先,用户的选择倾向受喜爱程度P影响,受潜在要素模型[2]的启发,可在K维潜在特征空间对用户和产品进行重新表达,令Ui=(Ui1,Ui2,…,UiK),Vj=(Vj1,Vj2,…,VjK).假设用户i对产品j的喜爱程度Pij由潜在特征共同决定:Pij=Ui·Vj=KUikVjk.∑k=1其次,用户选择行为与上下文条件I相关[11].比如,用户喜欢某演员,因此倾向于观看由该演员出演的电影.将产品的描述性特征抽取出来,可以得到产品的特征向量犉j=(Fj1,Fj2,…,FiQ).举例来说,某动作科幻电影的特征向量在“类别=动作”以及“类别=科幻”这两个特征分量上的值为1,在“类别=喜剧”等其他关于类别的特征分量上的值为0.为了平衡各特征的影响,建议对特征值进行归一化处理.由于不同的用户对于不同的上下文特征的敏感程度是不同的,本文引入个性化的敏感程度概念,定义用户i的敏感程度向量为犛i=(Si1,Si2,…,SiQ).用户敏感度与产品特征共同起作用:Iij=Si·Fj=QSiqFjq.∑q=1最后,社会环境E也会影响用户选择行为[11].比如,用户倾向于观看近期热播的电影或收听时下流行的歌曲.利用社交信息可以对社会环境对用户产生的影响进行量化估计,但许多应用场景中没有社交环境.因此本文采用时下流行度(TemporalPopularity,TP)对社交影响进行简单的近似.受用户数量规模的限制,这个估计是有偏的,但实验表明是有效的.记TPj为产品j的时下流行度,其值为最近时间窗中被选择的次数.社会环境对于较为从众的用户影响较大,而对于不从众的用户影响较小.为此,本文引入了个性化的用户从众度ci,并令Eij=ci·TPj.线性模型具有泛化能力强且易于求解的优点,本文采用线性模型来整合P、I以及E这3个影响因素:在式(6)中,U,V,S,c是需要估计的模型参数;φ1为权重,用来权衡各影响因素发挥的作用,可φ2根据领域先验知识进行设置,本文实验在两个数据,φ3集上对参数调节产生的影响进行了细致的分析和讨论.在实际应用中,也可以增加一个burn-in离线训Page5练过程,使用EM算法自动找到适用于具体应用场景的较优取值,避免手动调节.虽然该应用范例具有通用性,但仍建议读者根据应用场景的特性进行更合理的假设.此外,建议各要素初始化值的规模应相当,利于模型自行调优.将式(6)代入IFRM框架,得到如下模型:argminU,V,S,cL·其中Aij由式(6)定义,λ1,λ2是正则化系数,使用随机梯度下降法在导数负方向上分别对模型参数U,V,S,c进行迭代更新,即可求得局部最优解.3.3在线隐式反馈推荐模型(onlineImplicitFeed-backRecommendationModel,oIFRM)IFRM仍采用离线批量训练模式,这种训练模式在训练效率与推荐质量上存在局限性,本小节将介绍如何将IFRM扩展为在线模型.由式(5)和式(7)可以看出,IFRM的优化目标由损失项与正则项构成,其中损失项以连加的形式整合了所有用户反馈,这为实现在线学习提供了便利.基本方案是采用在线梯度下降法来对每一个用户反馈在线地更新模型参数[14].使用这种学习方式,用户反馈逐一流入推荐系统并仅被学习一次.由于模型不能进行重新学习,把握好这唯一的学习机会就显得特别重要.因此,如何确定学习步长成为了在线更新的关键所在.过大的学习步长意味着较强的训练程度,存在过训练(OverTraining)的风险;而过小的学习步长意味着较低的训练程度,可能导致欠训练(UnderTraining).已有的方法[9,14]依赖人工设置统一的学习率和迭代次数.这样做有3点缺陷:(1)实际使用过程中需要进行大量的调参实验;(2)不同的用户反馈对系统的价值是不同的,因此全局统一的学习步长势必会影响学习效果;(3)对所有用户反馈(包括噪声)一视同仁,将大大降低模型的鲁棒性,而抗噪和抗攻击能力对于在线系统来说是极其重要的.为了克服这些缺陷,我们应该根据每一个反馈对系统的价值来动态调节学习步长.从价值角度可以将用户反馈归为3类:(1)反映用户新兴趣的反馈.这类反馈对系统而言具有很高的价值,因此需要通过加强学习来迅速抓住用户兴趣的变化;(2)符合用户习惯行为的反馈.因为推荐系统已经较好地掌握了此类的用户行为,所以这类反馈对系统的价值不高,应该弱化学习以提高学习效率(减少训练迭代次数);(3)噪声.这类反馈对系统的影响是负面的,所以应该忽略以提高模型的鲁棒性.于是,问题的关键在于如何判别出不同的反馈类型.反馈发生的概率与用户置信度这两个概念与反馈类型息息相关.能够反映用户新兴趣的反馈具有较小的发生概率(用户不同以往的行为)以及较大的置信度(一直以来用户的此类行为都较为可靠),在训练时应增加迭代次数以达到强化学习的目的.符合用户惯常行为的反馈具有较大的发生概率,噪声特别是由恶意用户发起的攻击则具有较小的置信度.对于这两类反馈在训练时应减少迭代次数以达到弱化学习的目的.综上分析,可以通过比较反馈发生的概率与用户置信度来自动控制每一个反馈的学习步长.这是oIFRM在线学习算法的核心所在(算法1第4行).下面介绍反馈发生的概率与用户置信度的具体定义.一方面,IFRM具备完备的概率解释(见3.1节),可以将用户选择概率Prij作为反馈发生的概率.使用φ(x)=下特性:当Prij≈0.5时,Aij≈Ai,用户i对产品j与一般产品相比没有明显区别,此时推荐系统最不确定用户是否会选择该产品;当Prij接近1时,推荐系统认为用户很可能选择该产品,反馈行为发生的可能性很高;当Prij接近0时,推荐系统认为用户很可能不会选择该产品,反馈发生的概率较低.综上所述,Prij符合反馈发生概率的直观解释.另一方面,引入用户置信度变量θ来衡量用户行为的可靠程度.如果用户i的大部分行为符合推荐系统预期,则系统认为用户i的反馈可靠度较高,此时对用户i的反馈较为重视;反之,如果用户i存在较多出乎系统预料的反馈,则用户i的置信度不高,其反馈对系统的影响也较小,以此屏蔽由恶意用户发起的攻击,进而提高系统的抗攻击能力.算法1展示了oIFRM在线学习算法,使用3.2节给出的通用应用范例(式(7))构造模型.对模型参数进行随机初始化并归一化后,顺序地从隐式反馈数据流中读入一个用户反馈〈i,j〉,首先根据Prij来调节θi.新的θi为所有Pri的平均值.然后更新相关参数,通过比较Prij和θi来控制学习步长.Page6算法1.oIFRM在线学习算法.输入:隐式反馈数据流FS={…,〈Useri,Itemj〉,…}实时输出:模型参数U,V,S,c1.初始化:随机构造U,V,S并归一化2.从FS中顺序读取一个反馈〈i,j〉:3.根据〈i,j〉发生的估计概率Prij调节θi4.DOWHILEPrij<θi:5.6.7.8.9.ENDWHILE10.直到FS为空4数据集与源码本文使用两个数据集进行实验.其中Last.fm是国外知名音乐网站,豆瓣是国内知名影评网站.收听音乐与观看电影是较典型的两类隐式用户反馈,特别适合用来研究隐式反馈推荐系统.从Last.fm官方提供的API(http://www.last.fm/api)收集了132个用户的收听记录(〈用户,歌手,歌曲,时间戳〉的四元组)时间跨度由2005年2月14日到2005年7月31日.本实验将歌曲看作产品(Item),将歌手信息作为描述歌曲的特征(Feature).将所有收集到的记录按时间排序得到隐式反馈数据流.从豆瓣网抓取了6250个用户从2005年5月3日到2006年12月31日的所有观影记录,并抓取了相关电影的介绍页面,从中提取出14496个特征,包括主演(14233个),类别(37种),语言(130种),发行地区(96个).详细统计信息如表1所示.数据集LastFM豆瓣本文提供了数据集以及源码(下载地址见论文首页),已经对数据集属性部分进行了归一化处理,并用Python语言实现了算法1,并采用在线评价的方式进行测试(详见5.1节).代码的详细使用说明请参考下载包中的readme.txt.5实验本节将详细报告实验方法和相关结论,从多个角度展示oIFRM的表现.5.1实验设置传统的实验设计大多采用离线方式,先将数据集切分为两部分,使用一部分训练模型,用另一部分进行测试.这种离线测试的方式并不能很好地评价模型的线上表现.工业界常用A/B测试来比较不同算法,但其代价较大.本文通过模拟真实在线环境,从而设计了一种可以离线比较不同推荐模型线上表现的实验框架,如图2所示.为了克服隐式反馈的数据缺乏负样本因而难以评价的困难,我们对于每个反馈,随机从全体产品集合中抽样1000个产品,与该用户真实选择的产品一起组成长度为1001的候选列表,该方法已被广泛地用于评价隐式反馈推荐模型[9,16].本实验的任务是重排候选列表得到TopN推荐列表,并尽可能的令用户真实选择的产品排在前面.该任务与图1描述的在线学习与实时推荐框架是一致的.采用推荐系统中常用的3个指标来进行评价,包括反映召回率的Rank值,反映准确率的TopN命中率(HitRatio)以及反映推荐多样性的覆盖率(Corverage).Rank值为用户真实选择产品j在推荐列表中的百分比位置.如果j排在推荐列表首位,则Rank=0;如果排在末位,则Rank=100%.命中率指系统每次推荐给用户N个产品,导致用户选择的推荐占总推荐次数的比例.覆盖率指推荐过的产品占全部产品的比例.为了使评价结果更具有统计意义,在该实验中,平均每5000次连续的推荐得到1次评价(对应图3中的1个点).Page7图3Top30实时推荐效果动态比较5.2对比算法根据隐式反馈推荐的特点以及实时推荐的要求,选择以下6种算法进行对比:(1)iMF(implicitMatrixFactorization)[13]将已观察到的用户反馈置为1,其他置为0,并设置相对小的置信度权重,将问题转化为0-1矩阵分解.(2)BPR(BayesianPersonalizedRanking)[12]采用随机抽样抽取负样本的方式将问题转化为pairwise排序学习.(3)bIFRM(batchIFRM)使用随机梯度下降算法采用离线批量的训练方式求解IFRM模型参数.在实验中,每收集到5万个用户反馈后将新收集到的反馈加入原训练集后重新训练.(4)TP(TemporalPopularity)是IFRM的一个组成部分.通过统计滑动窗口中产品被选择的次数来向用户推荐时下最流行的产品.由于音乐及电影的推荐时效性要求都较强,TP在本实验中具有竞争力.Page8(5)RMFX(StreamRankingMatrixFactoriza-tion)[9]是较新的在线隐式反馈推荐模型,与oIFRM不同,RMFX需要抽取负样本并使用固定的学习步长.(6)DMF(DynamicMatrixFactorization)[8]在非负矩阵分解的基础上结合线性动态系统来对用户选择行为进行动态建模.虽然整合了时间信息,但DMF仍采用离线批量的训练模式.它是冷启动相关实验中的主要对比基线.5.3参数调节首先,通过多次实验,调节对比算法的参数使其达到效果最优.根据文献[8-9],我们经验地设置潜在特征维数为50,迭代次数为30,学习率为10-4.表2列出了oIFRM可以调节的参数,并给出了可直接适用于多种应用场景(包括2个数据集以及多种实验设置)的默认值.5.4节中报告的全部实验结果都直接使用了该组默认参数.由多次实验可以看出,oIFRM的参数容易理解和设置,具有较强的适用性表3调节oIFRM应用范例3方面因素的权重对效果提升的影响(“S”表示显著水平犘值小于0.001)数据集φ1φ2φ3LASTFM豆瓣5.4实验结果与分析实验1.推荐质量与可解释性.表4列出了6种算法的整体推荐效果,分析得到如下结论:(1)在线模型比离线模型在推荐质量上更具优势,这是因为在线学习能及时利用反馈信息,而离线训练在推荐时效性上存在局限性;(2)本文提出的模型IFRM在两种训练模式下与对比算法相比都取得了更好的推荐效果;(3)设置N=5,20,30,各模型在不同的TopN推荐中趋势一致,因此后面实验中只报告N=30时的结果.图3以动态的方式展示了各模型的线上表现,可得以下观察结果:(1)在线学习的优势在LastFM数据集上更为和推广能力.参数DWλ1λ2αθ,φ33.2节给出的IFRM应用范例引入了参数φ1来权衡各因素的影响.我们使用网格搜索策略φ2来对其进行调节,实验结果如表3所示.为了得到统计意义上稳定的实验结果,我们只记录了模型稳定后(最后10万次反馈)的评价结果,将实验重复了10次并进行t检验.观察发现,喜爱程度、上下文条件与社会环境都会影响用户选择.后续实验分别在两个数据集上选取了能够取得最好效果的设置.Top30推荐命中率平均较TP提升/%t检验P值15.315.453.84.80.110.44.453.80.00.3-9.4-0.10.1-0.30.10.3明显.因为电影更新速度较慢且用户兴趣较为稳定,而用户对音乐的兴趣更易发生变化,此时,在线学习能更及时地捕获这种变化;(2)对比3种在线模型,oIFRM除了在豆瓣数据集上命中率与TP较为接近外,整体上表现出了明显的优越性.这是因为豆瓣本身就会在首页向用户推荐近期上映的电影,所以TP策略已经可以达到较高的推荐命中率.但是,TP策略不利于发现用户个性化的需求,所以在推荐召回率以及多样性等指标上明显不及oIFRM.RMFX同样倾向于推荐热门产品,这是因为受优化目标驱使,更新算法不断加大热门产品的潜在特征,使热门产品更容易被推荐;Page9表4实时Top犖推荐整体效果比较稳定后推荐命中率Top20数据集LASTFM豆瓣(3)对比3种离线模型,bIFRM能产生更精准的推荐.值得注意的是,测试时随机采样作为负样本与BPR的采样过程看似一致,但实际上,BPR每次只采样1个负样本,与测试场景中采样大量(1000个)负样本是有本质区别的.相反,IFRM则使用“平均选择倾向度”来近似大量负样本的统计均值,因此更加契合测试场景,并取得了更好的推荐质量,同时这种采样大量负样本的设置也更贴近真实应用场景;(4)通过整合多种异构信息,IFRM的推荐结果更加多样化.由于初期缺乏足够的反馈来建模用户敏感度和从众度,oIFRM倾向于推荐时下流行的产品(TP起主导作用),随着学习到的用户反馈逐渐增多,oIFRM的推荐结果也越来越个性化.跟踪用户从众度c,记录其统计值如图4所示.oIFRM在可解释性方面具有如下优势:(1)对应用环境的理解.平均而言,Last.fm用户比豆瓣用户更不从众.在LastFM数据上,用户平均从众度持续下降到0.7以下,而在豆瓣数据上,该值稳定在1.0左右.Last.fm用户从众程度的差异性较大,部分用户有自己较为独特的品味;而豆瓣用户的从众度分布图4个性化的用户从众程度的动态统计则比较集中,其中大多数倾向于观看时下流行的电影(均值线与最大值线非常接近),因为观看电影需要花费更多的时间成本和经济成本;(2)对用户的理解.通过持续学习,从众度最大值与最小值之间的差距逐渐变大,从众度的个性化程度也越来越高,oIFRM对用户的理解程度也越来越深.在对用户进行推荐时,可以依据其从众度选择不同的推荐策略,比如向比较从众的用户推荐热门产品,并强调产品的流行度,向较个性的用户推荐小众产品,并强调产品特色.增强推荐结果的可解释性将有助于提高推荐接受率和用户满意度.该实验验证了个性化用户从众程度是有效的.实验2.训练效率与吞吐量.实验2通过比较oIFRM与其他在线推荐模型的训练效率来验证oIFRM处理大规模数据流的能力,并进一步给出系统吞吐量的估计.由于离线模型需要重训练,其时间开销与在线模型不具有可比性,因此本文仅选用在线模型TP和RMFX作为对比算法.实验部署在配备酷睿i5双核处理器2GB内存的PC上,所有算法使用Python语言实现.需要Page10说明的是,用C语言实现并部署在高性能服务器上能进一步提高训练效率,因此这里的报告结果仅是保守估计.在实际应用中,还可以使用抽样或并行计算技术[17]来进一步减少训练的时间开销,提高系统的吞吐量.在一般情况下,潜在特征维数D对训练时间的影响较大,因此本文考察D=5,50,200时各模型进行一次更新的平均时间开销.为了使结果更可靠,我们将实验重复10次,统计结果在表5中列出.TP的优势非常明显,其每次更新的时间复杂度仅为O(1).与RMFX相比,oIFRM将时间开销缩减了约50%,这是因为RMFX对所有反馈采用固定的迭代次数,而oIFRM区别对待每一个反馈,使学习更加有效(有些价值较低的反馈被忽略).值得注意的是,潜在特征维数D对oIFRM的学习效率影响不大,甚至设置更大的D时,训练时间不增反降.经过调试模型我们发现,较大的D导致个性化程度较高,进而导致用户置信度θ持续变小,此时迭代次数也相应变小.表5一次更新的平均时间开销(单位:ms)模型/数据集D=5D=50D=200对于在线系统来说,吞吐量是一个重要指标.经估算,oIFRM的数据吞吐能力为429个用户反馈每秒,已经能够满足一般的应用需求.实验3.系统健壮性.由于难以对数据流进行在线清洗,因此对于在线系统来说,健壮性显得尤其重要.当面临恶意攻击时,在线推荐系统的推荐质量将受到影响,更甚者可能导致系统崩溃.为此,实验3通过模拟恶意攻击情景来评估各模型的抗攻击能力.对于隐式反馈推荐系统来说,爬虫的随机访问(点击)是一种常见的攻击来源.我们随机选择了两个用户作为攻击者,分别在两个时间点各发起1万次随机访问.观察图5可以发现,RMFX在第8万和第16万次反馈后推荐命中率明显下滑,恰好与预先设置的两个攻击时间点吻合.而oIFRM几乎没有受到任何影响.通过监测用户置信度θ的变化我们发现,与正常情况相比(图6中实线),发动攻击之后(图6中虚线),这两个用户的置信度有明显下降.这意味着系统已经判别出用户行为的异常并屏蔽了该用户的反馈(攻击),进而保证了系统的推荐质量.可见,引入用户置信度有助于提高模型的鲁棒性.在实际应用中,可以通过监测θ来识别异常用户.此外,由于θ表示系统对用户进行推荐时的信心,可以使用其他模型对θ值较低的用户进行推荐.实验4.热启动性能提升以及冷启动适应能力.由于离线批量训练是导致冷启动问题的根源,因此设计实验4,其目的在于验证在线学习是否能有效地提高冷启动的适应能力.对比算法选用较新的动态模型DMF.该模型同样整合了时间信息,但采用离线训练模式.为了定义冷启动用户和产品,遵循传统的测试方法,根据时间将前30万反馈数据作为训练集,余下的作为测试集.统计信息如表6所示.设User为训练集的用户集合,Item为训练集的产品集合.对测试样本进行筛选构造出以下4种场景:(1)热启动场景,要求用户和产品都在训练集中出现过,TEST1={〈i,j〉|i∈UserANDj∈Item};(2)冷启动用户场景,要求用户未在训练集中出现过,TEST2={〈i,j〉|iUser};(3)冷启动产品场景,要求产品未在训练集中出现过,TEST3=Page11{〈i,j〉|jItem};(4)冷启动场景,要求用户或产品其中任意一方是新的,TEST4={〈i,j〉|iUser训练集用户数训练集产品数测试集用户数测试集产品数冷启动用户数冷启动用户比例/%冷启动产品数冷启动产品比例/%表6数据划分统计信息LastFM豆瓣数据集LastFM豆瓣表7在各场景下oIFRM对推荐效果的改进AverageRankDMF在训练集上进行离线训练,然后分别用4个测试集来进行测试.oIFRM则采用在线学习与在线评价结合的方式(图2).评价指标采用侧重召回率的AverageRank(AR)以及侧重准确率的AverageHitRatio(AHR).实验结果由表7给出,分析可得如下结论:(1)oIFRM在4种场景下都能取得更好的推荐精度.这是因为oIFRM能及时利用最新的用户反馈信息;(2)oIFRM的冷启动适应能力更强,因为在线学习可以极大地压缩冷启动范围,有些用户或产品在DMF看来是全新,但在oIFRM看来并不是;(3)oIFRM在冷启动产品场景下优势尤其明显.在LastFM数据上AHR由0提升至0.1192;在豆瓣数据上AHR大幅提升485%;(4)oIFRM在冷启动用户场景下优势较不明显.这是因为新用户反馈的累积过程较为缓慢,限制了在线模型的学习;(5)oIFRM在LastFM数据上比在豆瓣数据上效果提升更为显著,这是因为LastFM用户反馈频率较高,每位用户的平均反馈数高达465.83.可见,在频繁交互的应用场景中,oIFRM的优势更容易体现,而在低频交互的应用场景中,oIFRM应对冷启动问题有微弱优势.实验5.动态调整学习步长的有效性.根据反馈的价值,动态自动地调节学习步长是oIFRM的主要创新点.设计实验5的目的在于验证动态学习步长能有效地提高学习效果.实验在LastFM数据集上进行,基于一样的oIFRM模型(包括所有参数设置也完全一致),比较采用动态学习步长的更新方式和采用固定学习步长的更新方式的推荐质量.实验结果如图7所示,其中,IterX表示对每个反ORjItem}.这4个测试集中的样本占全部测试样本的比例在表7中“数据比例”一项列出.11464(AR)0.14550.39730.43150.41000.12770.09450.57470.1590图7动态学习步长与固定学习步长效果对比Page12馈都固定迭代学习X次.观察实验结果可知,使用动态的学习步长能够显著地提高召回率、准确率以及多样性.在采用固定学习步长的对比算法中,Iter30效果最差,过大的学习步长导致了过训练.Iter5,Iter10以及Iter20的学习效果差别不明显,都略好于对比基线TP.这说明如果采用固定学习步长的在线更新方式,将迭代次数设置为10左右能取得较优的推荐精度.但事实上,无论如何设置这一参数,学习效果总存在瓶颈.这是因为固定的学习步长无法满足所有反馈的学习要求.动态的学习步长帮助oIFRM突破了这一瓶颈,在各评价指标上都取得了更明显的改进.与此同时,我们记录了动态学习步长的oIFRM对每个反馈的训练迭代次数,计算得出均值为17.38,标准差为6.61.这说明本文提出的在线学习算法是有效的,通过差异化训练能够提高在线模型的训练效果.6总结本文研究了基于隐式用户反馈数据流的在线推荐模型.首先,本文提出了基本的隐式反馈推荐框架IFRM,通过最大化用户选择行为的发生概率将推荐问题概率建模为优化问题.其次,本文在IFRM框架下给出了通用的应用范例,展示了如何灵活地整合包括用户反馈、产品特征以及流行度在内的多种信息源.再次,本文进一步地提出了在线的隐式反馈推荐模型oIFRM,采用动态的学习步长在强化学习用户兴趣变化的同时降低了噪声的影响.最后,本文设计了在线评价机制并进行了丰富实验,从推荐召回率、准确率、多样性、可解释性、吞吐量、健壮性以及冷启动适应能力等多方面验证了所提供模型的优越性.未来,我们将尝试对社会影响力进行更细粒度地建模,并将oIFRM应用于社交网络推荐场景.在实际应用中,产品的展现位置很可能会影响用户的选择行为,因此可以结合考虑产品展现位置并通过对展现概率进行估计来改善推荐质量.致谢本文部分工作是在作者访问中国人民大学的萨师煊大数据研究中心时完成的.该中心获国家高等学校学科创新引智计划(111计划)等资助.在此,我们向对本文工作给予支持和建议的同行表示感谢.特别感谢评审老师提出的宝贵意见!
