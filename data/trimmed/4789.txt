Page1基于强化学习的SBS云应用自适应性能优化方法闫永明张斌郭军孟煜(东北大学计算机科学与工程学院沈阳110819)摘要自适应的调整云应用所占用的资源是一种有效的保障云应用性能的方法,但传统的决策方法面向基于服务的系统(Service-BasedSystem,SBS)时会存在一些问题,例如基于应用系统性能模型的决策方法不能很好适应云环境下SBS的动态变化,基于智能优化算法的决策方法效率较低.该文提出了一种基于强化学习的SBS云应用自适应性能优化方法.在该方法中,该文建立了自适应基本要素之间相互关系的特征描述框架,利用高层次的系统行为指标(如响应时间、用户并发量、资源量等)来描述系统性能的优化目标等.为了应对云环境以及SBS的动态变化,该文的方法采用了无模型(model-free)的在线学习算法,当用户并发量发生变化导致系统的预期行为发生偏差时,该方法通过不断重复“执行-积累-学习-决策”的过程,可以不断的积累经验数据并优化决策结果.为了保证自适应优化的高效性,该文提出了一种引导算子,可以有效的缩小候选自适应动作的范围,提高算法的学习效率.该文实现了以一个SBS为例的原型框架,使用该框架的实验结果证明了该文提出方法的有效性.关键词自适应;强化学习;资源调整;云应用;基于服务的系统;云计算1引言当前很多云计算平台(AmazonEC2、MicrosoftAzure、Rackspace、阿里云等)都为部署在上面的应用系统提供了通过资源调整进行云应用系统性能优化的能力,这种能力使得云应用系统可以更加有效地应对云环境以及用户请求的实时变化,为云应用系统的性能保障提供了更加有效的支持[1-3].基于服务的系统(Service-BasedSystem,SBS)在各种应用领域(如科研、通信、金融)发挥着越来越重要的作用[4].同时,也有越来越多的服务提供者选择将SBS部署在云环境中(通常采用将具体组件服务部署到多台虚拟机(VirtualMachine,VM)上的方式[5]),以减少硬件的运营成本.为了保障部署在云环境下的SBS的服务质量,则可以通过调整组件服务所在的虚拟机的资源(CPU、内存等)来改善组件服务的服务能力,从而实现保障SBS整体性能的目的[6-7].然而对组件资源进行自适应调整需要确定调整动作(如组件服务需要增加内存或增加CPU、给组件服务增加300MB或500MB内存).但云环境下资源的使用状态是动态变化的,一个虚拟机能够增加多少资源(如CPU、内存等)是受其所在物理机的资源使用情况所左右的,如果在系统运行之前预定义自适应调整,很有可能会出现自适应调整无法执行、不能实现预期优化目标等情况.例如预定义的自适应调整是给组件S1增加300MB内存,若S1所在物理机的剩余内存不足300MB时,则该自适应调整无法执行;若S1需要增加500MB内存才能使系统的响应时间满足服务等级协议(ServiceLevelAgreement,SLA)的要求,则该自适应调整不能实现预期的优化目标.所以,为了在云环境下对SBS进行更有效的自适应性能优化,需要基于动态的自适应决策.动态的自适应决策通常可以利用应用系统性能模型[8-11]或者智能优化算法[12-15]来实现.其中,利用应用系统性能模型进行自适应决策是在离线阶段建立应用系统资源、负载、性能三者关系的模型,然后在线阶段以系统当前的负载和性能目标作为性能模型的输入,计算得到资源的调整量;采用智能算法进行决策是将自适应方案的生成转换为最优化问题,即从所有可能的调整动作中选出满足优化目标且调整代价最小的动作组合.但是云环境以及部署在云环境中的SBS是动态变化的(如虚拟机资源增减、组件服务发生替换等),离线模型可能会存在不能很好地适应变化后环境的情况.而利用智能优化算法求解的效率通常很难满足在线自适应调整在高效性方面的要求(尽量减少SLA违例的时间),例如一个SBS由i个组件服务组成,每个组件服务所在的虚拟机都可以调整CPU和内存资源,CPU的调整方案为P种(可调整的CPU数量为0~P),内存的调整方案为Q/M种(可调整的内存总量为Q,最小调整量为M).那么智能优化算法需要在(P×Q/M)i种动作组合中进行选择,这将耗费大量的计算时间.针对这些问题,本文提出了一种基于强化学习的云应用自适应方法,该方法在动态决策的过程中积累相应的经验数据,并利用历史经验在线学习到资源调整的效果,通过不断重复“执行-积累-学习-决策”的过程,从而生成更好的资源调整方案,即通过在线学习不断优化决策结果.本文采用了Q-learning算法,该算法是一种无模型(model-free)的在线学习算法,在自适应的过程中不需要建立应用系统的性能模型,可以更好地应对自适应过程中的动态变化.此外,本文提出了一种引导算子,可以有效地缩小候选自适应动作的范围,从而提高算法的学习效率,保证自适应优化的高效性.本文的主要贡献有:Page3(1)本文提出了一种基于强化学习的自适应方法,可以在动态决策的过程中积累相应的经验数据,并通过经验数据不断地优化决策结果,能够在自适应优化的过程中更好地应对云环境以及SBS的动态性;(2)本文采用形式化的描述方法构建了云服务运行时自适应优化的表示模型,建立了可以完备描述自适应基本要素之间相互关系的特征描述框架;(3)为了保证应用系统运行时自适应优化的高效性,本文提出了一种引导算子,该算子通过自适应动作所包含的特征对自适应动作进行筛选,可以有效地缩小候选自适应动作的范围,达到提高算法学习效率的目的.本文第2节对所提出的方法进行了概述;第3、4、5节分别对自适应基础信息、自适应动态决策和在线学习过程进行描述;第6节给出相关的实例分析;第7节为实验及其结果分析;第8节介绍本文相关研究的现状;最后总结全文.2基于强化学习的自适应2.1基于强化学习的自适应模型基于强化学习的决策可以看作是一个Agent(应用系统也可以扮演Agent的角色)与部署环境之间的互动模型(interactionmodel)[16],如图1所示.在该模型中,Agent可以立即监测到软件系统运行环境的状态,并获得当前环境的回报值(根据当前环境状态计算得到的一个数值).基于当前的环境状态(State)st和回报值rt,Agent选择一个动作(Action)at,并通过执行动作at来影响环境.在动作执行完成后,环境将转换到下一状态st+1,对应一个新的回报值rt+1,Agent将基于st+1和rt+1选出一个新的动作at+1.这种Agent和环境之间周期性的持续互动是实现自适应自主性的基础.但是上述模型应用在面向云应用的自适应性能优化中会存在以下问题:(1)在云应用系统性能符合SLA要求时,持续的频繁调整会导致不必要的成本,包括决策和执行时所使用的计算资源和额外增加的资源的成本;(2)由于基于资源调整的自适应优化过程中存在数量众多的调整动作,且在动作选择时有可能会选择对云应用系统性能造成不良影响的动作(例如在SBS系统正常运行时,选择并执行减少VM资源的动作可能会导致云应用系统性能下降,发生SLA违例),从而影响强化学习算法的收敛速度和自适应的优化效果.为了解决上述问题,本文采用如图2所示的模型.针对第一个问题,该模型采用了事件触发机制(如图2中的|E|所示),即Agent根据环境状态st和回报值rt选择一个动作at前,需要先判断是否有触发事件(Event)被触发,如果触发事件被触发,则开始执行系统的自适应调整,从而避免了云应用系统的性能符合SLA要求后,还继续进行“无效”的调整.针对第2个问题,本文利用被触发的事件对所有可执行的Action进行筛选,保留与触发事件有相同目标趋势的Action作为候选集(例如云应用的响应时间超过SLA规定的上限时触发事件e被触发,e的目标趋势是提升组件服务的性能,所以将增加资源类的Action作为候选集),即对Agent进行了引导(shaping),限定其在特定范围的Action集合中进行选择,从而提升强化学习算法的学习效率.为了将上述的Agent-环境互动模型应用到面向SBS云应用的自适应资源调整中,本文假设SBS系统是由i个组件服务c1,c2,…,ci构成,其部署的云环境包含j个物理机pm1,pm2,…,pmj,各物理机上的可分配的资源量为Rα为资源类型.并设触发事件的有限集合为E,自适应方案的有限集合为P,每个自适应方案(p∈P)包含m个自适应调整{a1,a2,…,am},对于每一个am,其调整的资源量为rm,k(即am调整的资源需要由物理机pmk提供).当一个触发事件(e∈E)被触发时,如果Agent总是可以选出一个最佳的自适应方案(p∈P),那么可以认为Agent拥有一组最优的自适应方案,使得SBS系统的响应时间满足SLA的约束时,自适应调整的资源量满足当前云环境的可用Page4D:E→Ps.t.RTsla_lowRTRTsla_up∑ml=1资源约束.则该模型的形式化描述如下:式(1)中的D为一个决策器,表示在SBS运行期间可以将每一种触发事件(e∈E)映射到一个合适的自适应方案上(p∈P);式(2)为SBS系统响应时间RT的约束;式(3)表示一台物理机内的自适应调整的资源量不能超过其最大可用资源数量.所以,面向SBS云应用的自适应决策的主要目标是找到一个最优的决策器D,在面对环境和用户访问动态变化时,决策器D可以自动地制定自适应优化方案.2.2方法概述在Agent-环境互动模型的基础上,本文提出的基于强化学习的SBS云应用自适应的基本过程如图3所示.主要分为监测、触发、决策、执行、评估和学习6个阶段:监测阶段主要是获得与SBS系统相关的云环境状态信息(如VM占用的资源量等)以及SBS系统的运行状态信息(如响应时间、用户并发量等),然后将这些信息传递到触发阶段.在触发阶段时,需要判断监测信息是否满足触发事件中的触发条件,若满足触发条件则进入决策阶段.另外,如果有自适应调整动作未执行完成时,则不触发新的触发事件.图3自适应基本过程3.1关键性能指标假设预期系统行为可以通过一系列的指标来进行描述,这些指标通过获取系统性能特定的方面获得,例如CPU利用率、内存占用率、系统响应时间、服务等级(SLA)等[6],并将这些指标称为关键性能指标.KPI可以用来描述单个组件或整个系统的状态,例如可以使用c.KPI来表示组件服务c的KPI值.在决策阶段时,Agent根据监测信息和决策经验信息选出最优的自适应调整动作.在执行阶段时,Agent执行上一阶段选择的自适应调整动作,完成对云环境中虚拟机资源的调整.当自适应调整动作执行完成后,Agent将利用适应度函数(fitnessfunction)评估调整后的云环境所能获得的回报,并利用Q-learning对所执行的自适应调整的经验(即评估阶段获得的回报值)进行累积,为以后的决策提供经验.在上述过程中,监测和执行均可以通过使用现有的软件来完成,例如利用collectd获得应用系统性能信息,利用libvirtAPI获得VM资源占用量和调整VM的资源.触发则是依次判断监测的数据是否满足相应触发事件.所以,在接下来的第3、4、5节中将重点介绍自适应基础信息形式化描述、如何在决策的过程中使用Q-learning学习到的经验和如何累积自适应调整之后的经验.3自适应基础信息本节介绍了基于强化学习自适应过程中的基础信息,包括:系统行为的描述指标(KeyPerformanceIndicator,KPI)、SBS系统的部署信息(包括物理机、虚拟机和组件服务的部署信息)、自适应优化目标(Goal)和触发事件(Event),并在此基础上对Q-learning的State和Action进行了形式化描述.KPI的定义包括名称、数据类型、组合方程和边界值(RelevanceMargin).其中组合方程用来说明如何计算系统全局或部署了组件服务的单个虚拟机的KPI值,而边界值则用来说明两个同类型的KPI值是否相同,即当两个同类型的KPI值的差值小于边界值时,认为这两个KPI值相同.本文中所定义的KPI如下所示:Page5KPIcpu_use:doubleCombFuncSumRMargin0.01KPImem_use:doubleCombFuncSumRMargin0.1KPIuser_con:intCombFuncSumRMargin0KPIresp_time:doubleCombFuncTreeRMargin0.01在本文中,我们假设系统级的KPI值都可以通过系统中每个单独的组件服务的KPI值组合获得,即系统级的KPI值可以通过c.KPI对应的组合方程计算得到,其中组合方程是单调递增或单调递减的,如CPU、内存、网络和用户并发量的KPI值.但对于响应时间的KPI值,由于本文讨论的云应用系统是由多个组件服务构成的SBS系统,其业务流程中会存在顺序、循环、并行和选择这4种结构,可以采用标记树对SBS的组合流程进行等价转换并计算其响应时间[9].3.2系统部署信息系统部署信息描述了云环境中与SBS云服务部署相关的物理机、虚拟机以及SBS系统中组件服务的部署信息,以虚拟机的部署信息为例,其具体形式如下:VirtualMachineSpecificationVM_nameDeployment:(component1,…,componentn)Resource:((Resource1,Count),…,(Resourcen,Status:[RUNNING/UNUSED/UNALLOCATED]Deployment和Resource分别描述了该虚拟机上部署的组件服务的情况和该虚拟机的资源信息.Status描述了虚拟机的运行状态,共有3种类型:RUNNING、UNUSED和UNALLOCATED,分别表示该虚拟机处于运行、停止和未分配状态,其中,处于UNALLOCATED状态表示未创建该虚拟机的运行实例.3.3自适应优化目标和触发事件每个自适应目标使用KPI来描述特定的自适应优化目标,并使用Above、Below和Between关键字来确定自适应目标可接受的指标的范围,每个自适应目标可以通过人工设定或从SLA约束中自动抽取等方式获得,本文采用人工设定的方式.具体形式如下:Goalgoal_name:kpi_nameAbovethr_lowerGoalgoal_name:kpi_nameBelowthr_upperGoalgoal_name:kpi_nameBetweenthr_lower其中:Above目标表示应该使KPI的值高于所给定的阈值thr_lower;Below目标表示应该使KPI的值低于所给定的阈值thr_upper;between目标表示应该使KPI的值保持在所给定的阈值区间.触发事件为自适应调整的触发条件,每一个触发事件关联着一个特定的自适应目标,并说明事件触发后自适应调整所应达到的目标趋势,具体形式如下:Eventgoal_name.event_name:kpi_name[>/<]thrTargettarget_name其中:goal_name.event_name为自适应目标和触发事件名称的组合,描述了两者的对应关系;kpi_name[>/<]thr为判定条件,表示当KPI的值大于/小于阈值thr时满足触发条件;Target表示触发事件触发后自适应调整所应达到的目标趋势,包括两种类型:IMPR_PERF和DEGR_PERF,分别表示触发事件触发后应执行提升或降低组件服务性能的自适应调整.在实际系统中使用上述触发事件时,为了避免系统KPI值在短时间内超过阈值所造成的不必要的触发.本文的触发机制采用基于时间窗的方法,即当时间窗内的系统KPI值有x%超过触发事件中的阈值时,才认为触发事件会被触发.其中,时间窗的宽度和x的值可以根据经验来设定.根据自适应目标抽取对应的触发事件,触发事件的判定条件可以依据自适应目标中的关键字(Above、Below和Between)和阈值来确定,具体的对应规则如表1所示.在确定触发事件的判定条件后,可以根据判定条件确定触发事件的目标趋势,具体为当判定条件为“>”时,目标趋势为“IMPR_PERF”;当判定条件为“<”时,目标趋势为“DEGR_PERF”.Betweenthr_lowerthr_upper<thr_lower>thr_upper3.4State和Action的形式化表示State和Action的形式化表示定义了系统的问题空间和解决方案空间,是设计性能优化方法中基于Q-learning动态决策的关键步骤.本文设环境状态的集合为S={s1,s2,…,sn},每个环境状态si包括云环境中与SBS系统相关的VM的资源拥有量和SBS系统当前的负载所处的区间,其形式可以表示为Statestate_nameResource:(Resourcevm1,Resourcevm2,…,Resourcevmn)Page6Load:load_type其中,Resource为与SBS系统相关的VM的资源拥有量,本文为了减少环境状态的数量,将VM的资源拥有量设定为固定的6种:NANO、MICRO、SMALL、MEDIUM、LARGE和EXLARGE.采用这种设定,一方面是参考亚马逊EC2中虚拟机实例的设定,另一方面是通过对SBS系统进行基准测试,参考基准测试数据所设定给的(例如,通过基准测试可以得到当VM的CPU数量为8,内存为8GB时,再继续增加VM的资源不会使SBS的性能有明显的提升),具体如表2所示.Load是SBS系统当前的负载类型,即当前的并发用户数量所处的区间,其设定类似于虚拟机资源类型的设定,具体如表3所示.NANOMICROSMALLMEDIUMLARGEEXLARGE对于Action,本文设自适应动作的集合为A={a1,a2,…,am},每个自适应动作aj包括自适应原语和动作执行后的目标趋势,具体形式如下:Actionaction_namePrimitive:primitive_name(object,ori_res,des_res)Target:[IMPR_PERF/DEGR_PERF]其中,Primitive为自适应原语,即每个自适应调整方式的具体执行指令,其第1个参数为调整的目标VM,第2、3个参数分别为VM调整前后所占有的资源数量,例如addRes(VM1,SMALL,MEDIUM是将VM1的资源从SMALL调整为MEDIUM.Target是自适应原语执行后所实现的目标趋势,与触发事件中的目标趋势相同.当SBS由i个组件服务构成时,环境状态的集合S中共有xi×y个环境状态,动作集合A中共有i×x(x-1)×y个自适应动作.其中,x为虚拟机资源类型,y为负载类型.在本文中x和y的值分别为6和5,即根据本文的设定,共有6i×5个环境状态以及30i×5个自适应动作.在确定所有环境状态和自适应动作后,Agent需要生成(6i×5)×(30i×5)维的矩阵犙,其元素犙(s,a)为环境处于环境状态s时选择自适应动作a的经验值.在选择自适应动作时,由于计算所有自适应动作的选择概率需要耗费大量时间和资源,所以本文通过两个步骤来减少候选自适应动作的范围:首先在确定当前的环境状态后,可以选出当前环境状态下可以执行的自适应动作作为候选自适应动作集,该候选集中共有30i个自适应动作;然后利用引导算子(具体见第4节)对候选集进行筛选,最终的候选集中共包括15i个自适应动作.4自适应动态决策当触发条件被触发后,自适应动态决策过程开始执行,即根据当前的环境状态和决策经验选择合适的Action,主要分为两个步骤:首先,利用环境状态和引导算子从自适应动作集合A中选出候选动作集合Ac(AcA),以减少候选Action的搜索空间.然后,利用软最大化(soft-max)方法在Ac中选择合适的待执行Action.其中,引导算子从自适应动作集合A中选出候选动作集合Ac(AcA)的原则为:对于任意aj∈A,若aj中的Target与被触发的触发事件中的Target相同,则将aj加入候选集Ac中.例如,当触发事件Eventresp_time_goal1.event2:resp_time>19sTargetIMPR_PERF被触发时,从集合A中挑选Target为IMPR_PERF的Action放入候选动作集合Ac.构建候选集Ac可以视为对Agent进行引导(shaping),限定了Agent在特定范围内选择Action,避免随机选择Action降低Q-learning的学习效率.利用软最大化方法选择待执行Action时,认为在环境状态s时选择自适应动作a(a∈Ac)的概率与eQ(s,a)τ成正比,具体为式中:犙(s,a)为环境处于环境状态s时选择自适应动作a的经验值;τ(τ>0)是选择参数,当τ较大时表示任意自适应调整被选中的概率几乎一样,减少τ则拥有最大值的自适应调整被选中的可能性更大,若τ→0则总是选择最优的行为.利用式(4),Page7Agent将计算动作候选集Ac中每个自适应动作的选择概率,并选择概率值最大的自适应动作作为结果.若多个自适应动作的选择概率相同时,则随机选择一个自适应动作,被选择的自适应动作将在执行阶段被执行.5在线学习当选择的自适应动作a执行完成后,环境状态由s转换为s,Agent可以利用适应度函数来获得相应的回报r(s,a),并利用获得的回报r(s,a)来更新Q值(Q-values),以完成在线学习过程.在线学习过程中,合理高效的适应度函数使自适应动作的选择准确、高效,本文将自适应动作a执行后的代价和收益作为其执行后的回报,即利用自适应动作执行所需要付出的成本和对系统性能影响来评价自适应动作是否合理.适应度函数跟所调整的资源的成本、系统性能的变化量以及所调整资源是否满足资源量的约束这3个属相相关,可以定义为其中:ωc和ωw是适应度函数相关属性的权重;cΔR是调整资源的成本;ΔW是SBS系统性能的变化量;RP是资源调整量超过可利用资源量时的惩罚.调整资源的成本cΔR是指自适应调整在执行前后不同资源变化量的费用之和,即式中:CPU、Mem分别表示处理器、内存等资源;Ur表示每种资源的单位价格,可通过建立资源的定价模型来确定,如市场上一条容量为4GB的DDR3内存条的价格是220元左右,则可以算出内存的单位价格为220元/4096MB=0.054元/MB;m,m分别图4地听服务器端业务流程表示自适应调整在执行前和执行后VM占有的各类资源的总量.当增加资源时,cΔR为负数,释放资源时,cΔR为正数.SBS系统性能变化量ΔW是指自适应调整导致的SBS系统的不同性能指标(如响应时间、可靠性和吞吐量等)的变化,在本文中主要是指响应时间的变化量.当自适应动作调整的资源量超过可用的资源量时,惩罚量RP为一个负数,否则RP为0.在学习阶段,利用获得的回报r(s,a)来更新Q值的过程如式(7)所示:Q(s,a)=(1-α)Q(s,a)+α(R(s,a)+式中:α(0α1)是一个常数步长参数,用来控制已有经验和当前所获得经验之间的权重,其中,(1-α)Q(s,a)为已有经验,α(R(s,a)+γmaxaQ(s,a))目前回报和未来经验折扣估计值的和,即当前所获得的经验;γ(0γ1)是折扣因子,用来控制未来动作选择的权重.更新后的Q值将会在下一次决策中被使用,使Agent能够选出最优的自适应动作.6实例分析本文以一个景点语音导游系统(地听)为例,该系统分为服务器端和移动端两部分,其中服务器端是一套SBS系统,共由7个组件服务构成,分别是:景区定位(c1)、上传图像解析(c2)、图像特征匹配(c3)、GPS定位识别景点(c4)、当前景点语音播放列表确定(c5)、当前景点语音播放传送(c6)和后续路线推荐(c7),整体的业务流程如图4所示.在本文中,将重点关注地听系统服务器端的自适应性能优化过程.Page86.1系统部署信息系统部署在由6台物理机(PhysicalMachine,PM)组成的云环境中,其中3台物理机为配置相同的服务器,标记为PM1~PM3,并在这3台物理机上共创建7台配置相同的虚拟机,分别标记为VM1~VM7(在PM1上创建VM1和VM2,在PM2上创建VM3~VM5,在PM3上创建VM6和VM7),然后在每台虚拟机上分别部署SBS系统的1个组件服务,如在VM1上部署c1.另外3台物理机则分别用来进行自适应决策、存储SBS系统的业务数据及运行状态的监控数据和产生访问负载及监测SBS系统的运行状态.以PM1,VM1和c1为例,相关的系统部署信息如下所示:PhysicalMachineSpecificationPM1Deployment:(VM1,VM2)Resource:((CPU,2.5GHz×2×4),(MEM,8GB))VirtualMachineSpecificationVM1Deployment:(c1)Resource:((CPU,1×2),(MEM,2GB))ComponentSpecificationc1Deployment:VM1,Status:[USING]6.2自适应优化目标设定和触发事件生成在地听系统自适应优化的过程中,自适应优化的目标是要保证系统的响应时间不能超过SLA中规定的阈值.而与该目标相关的指标,除了系统的响应时间外,还有用户的并发访问量以及虚拟机的资源利用率,如CPU和内存的占用率.对于用户并发访问量,当系统当前的用户并发访问量超过系统所能承受的上限时,则很容易导致系统的响应时间变长,所以需要设定用户并发量相关的自适应目标,以保证在出现并发用户数超过系统当前承载能力的时候可以进行及时的调优.对于虚拟机的资源利用率,若虚拟机的资源利用率一段时间内都处于较高的状态时,如CPU利用率大于80%,则说明部署在虚拟机上的组件服务已经处于较高的负载状态,很有可能会出现或已经出现过载的情况,从而导致服务响应时间违反SLA规定,所以也需要设定虚拟机资源利用率的自适应目标,使虚拟机可以保持在适当的负载状态.根据上述因素,本文将设定3类自适应目标:响应时间类、用户并发量类和虚拟机资源利用率类.其中,响应时间类的自适应目标可以根据SLA中关于系统响应时间的约束进行设定,以生成系统级的自适应目标.以系统的响应时间自适应目标为例:Goalsys_resp_time_goal:resp_timeBetween7s19s用户并发量类的自适应目标可以分为系统级和组件服务级两类,每类的设定方式也与响应时间类的自适应目标类似.以系统的用户并发量自适应目标和组件服务c1的用户并发量自适应目标为例:Goalsys_user_con_goal:user_conBelow300Goalcomp_user_con_goal1:c1.user_conBelow300对于虚拟机资源利用率的自适应目标,其设定完全是根据专家经验,例如服务器的CPU利用率应保持在20%~80%之间.此外,本文中主要考虑CPU和内存指标,结合系统部署信息,可以为每一个虚拟机设定相应的自适应目标,即人工设定每类资源利用率的具体范围,所有虚拟机的资源利用率类的自适应目标都以该范围为标准进行设定.以虚拟机VM1为例,其资源利用率类的自适应目标如下:Goalvm_cpu_usage_goal1:vm1.cpu_usageBetween20%80%Goalvm_mem_usage_goal1:vm1.mem_usageBetween20%80%根据上述的自适应目标,按照3.3节所述的自适应触发事件抽取规则,可以生成相应的触发事件.以系统响应时间的自适应目标为例,抽取的自适应触发事件如下:Eventsys_resp_time_goal.event1:resp_time<7sTargetDEGR_PERFEventsys_resp_time_goal.event2:resp_time>19sTargetIMPR_PERF当运行阶段所获得监测指标使得多个触发事件被触发时,将按照响应时间类、用户并发量类和虚拟机资源利用率的优先级进行触发.若还存在系统级和组件服务级的触发事件同时被触发,则系统级的自适应目标相关的触发事件将优先触发,例如:当通过监测发现系统的用户并发量和组件服务c1的用户并发量都不符合要求时,则优先触发系统级的触发事件(resp_user_con_goal.event),首先保证系统的整体性能.此外,为了避免由于KPI的瞬时变化所导致的不必要的优化,本文将根据一段时间内KPI值的变化情况对触发事件进行触发,例如在30s内,如果有80%以上的系统响应时间监测值超过了19s,则触发resp_time_goal1.event2事件,开始对系统进行优化.Page9对于Action,以增加和减少VM1资源的自适应6.3State和Action根据3.4节中State的形式定义和所设定的虚拟机资源类型以及负载类型,可以生成环境状态的集合.假设所有虚拟机的初始资源类型为MICRO,负载为TYPE_I时,环境状态为States1Resource:(MICRO,MICRO,MICRO,MICRO,Load:TYPE_I动作为例:Actiona1Primitive:addRes(VM1,MICRO,SMALL)Target:[IMPR_PERF]Actiona2Primitive:reduceRes(VM1,MICRO,NANO)Target:[DEGR_PERF]其中:自适应动作a1是将VM1的资源从MICRO类型增加至SMALL类型,由于增加资源,所以可以提升系统的性能,即Target为IMPR_PERF;而自适应动作a2是将VM1的资源从MICRO类型减少至NANO类型,降低系统的性能,Target为DEGR_PERF.其它State与Action与上述例子类似.7实验本节将通过实验分析不同参数对本文提出的方法的性能的影响,以及应用于部署在云环境中的SBS系统时的有效性,主要包括实验环境配置、实验设计和实验结果与分析三部分内容.7.1实验环境配置实验所用的云环境与第6节中的系统部署环境一致,共包括6台物理机(PhysicalMachine,PM),各物理机通过带宽1GB/s的局域网相连,其中物理机1~3为硬件配置相同的服务器(CPU:IntelXeonE5-2620v224核,64GB内存,1000Mbps网卡,1TB磁盘),并在这些服务器上创建大小相同的虚拟机(类型为MICRO),用于部署SBS系统的组件服务.物理机4用于分析SBS系统的监控数据,并根据系统的运行状态进行自适应决策,生成相应的自适应调整方案.物理机5用于存储SBS系统的业务数据及运行状态的监控数据.物理机6用于监测SBS系统的运行状态并产生访问负载.具体部署结构如图5所示.7.2实验设计本文的实验主要是为了分析不同参数对基于Q-learning的自适应性能优化方法性能的影响以及在SBS自适应性能优化中应用的有效性,具体如下:(1)分析不同因素对算法收敛性的影响.①分析参数设定对算法收敛性的影响.对SBS系统施加恒定的压力,对比在不同参数(α,γ,τ)情况下基于Q-learning方法的回报累积情况,以选出较优的参数;②分析引导对算法收敛性的影响.对SBS系统施加恒定的压力,对比方法在是否采用引导的情况下的回报累积情况.法的有效性:(2)验证基于Q-learning的自适应性能优化方①在不同压力的场景下,检验本文提出的方法能否使SBS系统的响应时间恢复到SLA的约束范围内(7s~19s),并对比同样情况下不使用自适应优化方法时SBS系统的响应时间状态;②在不同压力的场景下,对比本文提出的方法和另外两种动态决策方法(分别为文献[8]中利用应用系统性能模型的方法;文献[12]中基于遗传算法的方法)作用下的系统响应时间的情况以及使用的资源成本co.对于实验内容2的压力场景,本文根据数理统计[17]中对时间序列(用户访问序列可以视为一个时间序列)的分类来设计负载场景.数理统计中将时间序列分为平稳型和非平稳型,其中非平稳型又可以根据序列变化的趋势分为随机型和具有一定变化规律的类型(如具有线性递增特征的类型、具有线性递减特征的类型和具有季节特征的类型).对于非平稳随机型场景,本文在0~60min的时间内随机生成100~200区间的任意并发数,且持续很短的时间,具体如图6所示.在随机型负载场景中,本文对比了使用本文的方法和不使用时SBS系统的响应时间状态,具体如图7所示.Page10在图7中,上方的虚线为负载随时间变化的情况,下方为SBS的响应时间的变化情况,其中标注“▲”的位置为该时刻进行了资源调整.由于本文提出的自适应方法为了避免由于KPI瞬间变化所导致的不必要的优化,所以在随机型场景中很多时刻并未触发自适应优化.在不触发自适应优化的情况下,系统的响应时间(ResponseTime,RT)会随着负载的随机变化而变化,具体分布情况如表4所示.表4随机型场景中SBS的响应时间分布情况(单位:%)有自适应18.3无自适应26.7如表4所示,在随机型场景中,有无自适应优化的情况下,系统响应时间符合SLA要求的比例(7RT19)分别为68.3%和63.3%,满足用户要求(RT19)的比例分别为81.7%和73.3%.采用自适应优化使SBS系统响应时间符合SLA约束和用户要求的比例分别提升了5%和8.4%.可见,本文提出的自适应优化方法在随机型负载场景中对SBS系统的性能改善不大.所以,根据随机型场景的实验结果,本文将重点验证所提出的方法在其它负载场景中的有效性,分别是:具有平稳型特征的负载场景(简称平稳型场景)、具有线性递增非平稳型特征的负载场景(简称线性递增型场景)、具有线性递减非平稳型特征的负载场景(简称线性递减型场景)和具有季节性非平稳型特征的负载场景(简称季节型场景).其中,平稳型场景包括4种负载方式,分别标记为负载I、负载II、负载III和负载IV.在虚拟机资源为初始分配量的情况下,负载I将使得SBS系统的响应时间低于SLA约束的下限阈值;负载II将使得SBS系统的响应时间保持在SLA的约束范围之内;负载III将使得SBS系统的响应时间超过SLA约束的上限阈值;负载IV将使得SBS系统的响应时间比在负载III情况下更多地超过SLA约束的上限阈值.具体并发数量见表5.负载I负载II负载III负载IV100对于平稳型场景,将按照II→III→IV→I的顺序对SBS系统进行施压,且一段时间内一直持续施加一种负载;线性递增型场景是在0~60min期间内,用户并发量从负载I增长到负载IV,总体处于线性递增的趋势.线性递减型则与之相反,总体处于线性递减的趋势;季节型场景是用户并发量具有季节波动性,一个周期为30min.在一个周期内,两种类型的用户并发量的最小值为负载I,最大值为负载IV,具体如图8所示.对于使用的资源成本co,具体如式(8)所示.其中:RVα用的资源量;Ur表示每种资源的单位价格;T为使用该虚拟机资源的时间.7.3实验结果与分析7.3.1收敛性分析算法的收敛性对于是否能够在合理时间内找到近似最优解至关重要,直接影响算法的可用性.本组实验考察了影响Q-learning算法收敛速度的主要参数(学习步长参数α、折扣因子γ和选择参数τ)在取不同值时以及是否对算法进行引导时,算法的回报累积情况.Page11图84种负载场景(1)参数设定对算法收敛性的影响本实验对于参数的设定将分为6种情况,具体如表6所示.设定I0.3设定II0.3设定III0.3设定IV0.5设定V0.5设定VI0.5在实验中,将对SBS系统施加恒定的压力(用户并发量为200),SBS系统的响应时间将会超过SLA的约束,使得自适应优化被触发,在迭代30次后累积回报值的具体情况如图9所示.从图9中可以看出,参数设定为(0.3,0.7,0.8)时算法的回报值累积曲线在其它参数设定曲线的上方,则认为该参数设定使得算法具有较好的收敛性,所以本文将采用设定I.(2)引导对算法收敛性的影响本实验主要是在恒定负载的情况下,所有虚拟机的初始资源类型为SMALL时,对比有无引导时基于Q-learning决策算法的回报累积情况.其中,无引导是指在选择Action时,从当前State对应的所有action中进行选择;有引导是指在选择Action时,从与被触发Event有相同Target的Action集合中进行选择.在迭代30次后回报的具体情况如图10所示.从图10中可以看出,有引导时的算法的回报值累积曲线在没有引导的回报值累积曲线上方,则说明有引导时算法具有更好的收敛性.Page127.3.2有效性验证为了验证本文提出的自适应优化方法的有效性,本文将对比4种压力场景中,有自适应优化和无自适应优化情况下SBS系统的响应时间RT,以及对比本文提出的方法和另外两种动态决策方法作用下的系统响应时间的情况以及使用的资源成本,具体如下:(1)有无自适应情况对比在4种压力场景中,SBS系统在有自适应优化和无自适应优化情况下的系统响应时间情况如图11有无自适应时SBS的响应时间对比图11所示.与图7类似,在图11(a)~(d)中,上方的虚线为负载随时间变化的情况,下方为SBS的响应时间的变化情况,其中标注“▲”的位置为该时刻进行了资源调整.从图11(a)中可以看出,在平稳型的负载场景下,当SBS系统的并发访问量从负载II增加到负载III后,系统的响应时间也随之增加,并超过了SLA约束的上限阈值.在对SBS系统进行优化后,其响应时间在第14min时恢复到SLA的约束范围内.当SBS系统的并发访问量从负载III增加到负载IV时,以及并发访问量从负载IV减少到负载I后,对系统进行自适应优化后均可以使系统的响应时间恢复到SLA的约束范围内.而未进行自适应优化时,在施加负载III、负载IV和负载I期间,系统响应时间一直处于高于上限阈值和低于下限阈值的状态.与在平稳型场景中的情况类似,SBS系统的响应时间在其它3种场景中的情况如图11(b)、(c)和(d)所示,当有自适应优化发挥作用时,SBS系统的响应时间会很快地恢复到SLA的约束范围内,而没有自适应优化时,SBS系统的响应时间会长时间地处于SLA违例状态.每种场景中响应时间的具体分Page13布情况如表7所示.如表7所示,在不同的负载场景中,采用自适应优化时,SBS系统响应时间符合SLA约束(7RT19)的比例分别提升了75%、38.3%、40%和15%,符合SBS系统使用用户要求(RT19)的比例分别提升了43.4%、35%、38.4和10%.可见,本文提出的方法是有效的.表7有无自适应时SBS响应时间分布情况(单位:%)平稳型有自适应3.395.01.7线性递增型有自适应8.390.01.7线性递减型有自适应3.393.33.4季节型有自适应5.088.36.7(2)不同决策方法对比在本小节实验中,将在不同的压力场景下,对比本文提出的方法和另外两种动态决策方法(分别为图12不同方法作用下SBS的响应时间对比文献[8]中利用应用系统性能模型的方法,简称PEM;文献[12]中基于遗传算法的方法,简称GA)作用下的系统响应时间的情况以及使用的资源成本co,具体见图12、表8和表9.表8不同方法作用下SBS响应时间分布情况(单位:%)平稳型线性递增型线性递减型季节型从图12可以看出,在4种不同的压力场景中,本文提出方法和其它两种方法均可以使SBS系统的响应时间满足SLA的约束.其中,本文提出的方Page14法和基于PEM的方法都可以很快地使系统响应时间恢复到SLA的约束范围内,如表8所示,两种方法使得SBS系统响应时间符合SLA约束和SBS系统使用用户要求的比例相差不大.但基于PEM的方法由于离线建立的性能模型不能很好地适应环境的动态变化,每次调整后系统的响应时间变化幅度都较大,而这也代表着使用了更多的资源.如表9所示,在4种场景中,基于PEM的方法的资源成本co比本文方法分别高出57.74%、55.22%、69.64%和83.35%.可以看出,本文提出的方法对比基于PEM的方法在决策效率方面相差不大,但使用的资源成本更低.平稳型359756743368线性递增型261740622482线性递减型454677124340季节型294959072781综上,本文提出的方法在4种典型负载场景中均可以达到优化系统响应时间的目的,且在拥有较高决策效率的同时,可以保证云应用占用较少的资源即可以满足性能需求.综合考虑效率和资源成本时,本文提出的方法优于其它两种方法.8相关研究自适应决策是自适应过程中的关键步骤,可以通过多种方式实现,一种方式是依据预先设定的自适应策略进行决策,还可以利用人工智能、控制论等技术进行决策.(1)基于策略的自适应决策方面自适应策略的描述通常采用ECA(Event-Condition-Action)的形式[18],或者采用更为简单的if-else形式,连同系统的因果网络一起构建基于模型的推理机制[19].此外,IBM还提出了ACPL(AutomaticComputingPolicyLanguage)语言,用来描述自主计算中的策略[20].在此基础上,文献[6]的自适应决策机制包括离线和在线两个阶段,在离线阶段生成一组ECA形式的自适应规则,在线阶段时根据当前的系统状态和策略中所定义的目标对自适应规则集合进行匹配的方式生成自适应调整方案.文献[9]提出了一种SBS运行时自适应框架MOSES(MOdel-basedSElf-adaptationofSOASystems),利用为每个抽象服务建立的最优自适应策略模型选取动作模式,从而达到最佳性能.文献[10]提出了一种可执行的建模语言EUREMA(ExecUtableRuntimEMegAmodels),可以按照模型驱动的工程方法来简化自适应工程的开发.文献[11]给出了一组特定的MAPE-K模板(Monitor-Analyze-Plan-ExecuteplusKnowledge),这些模板包括特定行为模板和特定属性模板,前者用来对MAPE-K循环回路中的不同的组件进行建模,后者用来验证自适应行为的正确性,并利用这些模板构建了一个自适应系统.文献[8]则通过建立应用系统的负载、资源和性能之间的关系模型,来制定应用系统的优化策略.但是这类方法大多需要对系统或者环境进行建模,而在离线阶段建立的模型不能很好地适应发生变化后的环境或应用系统.(2)结合人工智能技术进行自适应决策方面智能优化算法在自适应决策中得到了较多的应用,例如文献[13]在决策时考虑了工作流使用资源的成本和所花费时间之间的平衡,采用基于启发式的多完整关键路径方法进行求解;文献[14]通过提出的声明式服务编排语言DSOL(DeclarativeServiceOrchestrationLanguage)将自适应调整方案的生成问题表示为自动规划问题,采用智能规划技术进行求解;文献[12]在自适应决策过程中同时考虑了违反SLA约束的代价及执行自适应调整动作的代价,将自适应调整方案的生成问题建模为单目标优化问题,通过智能算法对该问题进行求解.文献[15]提出了一种基于服务选取的SBS资源分配模型,并采用支持向量回归(SupportVectorRegression,SVR)构建组件服务的性能模型,并以性能模型为基础进行分配方案的求解.但利用智能优化算法求解通常都会存在计算效率问题,需要进行相应的优化.还有学者利用规划、模糊推理和基于用例的推理(Case-BasedReasoning,CBR)方法来进行自适应决策,例如文献[21]将特征模型和在线机器学习结合起来,即将领域专家的知识体现在特征模型中,然后通过在线学习来改进自适应决策的准确性和效率;文献[22]构建了一种自调整的模糊控制器(Self-TuningFuzzyController,STFC)方法,STFC利用模糊规则推理得到资源的调整量;文献[23]通过计算监测到的系统状态和用例库中用例的相似度,找到最相似的用例,并执行用例中的自适应动作来对系统进行优化,然后将优化的结果更新至用例库中.而这类方法优化效果的好坏很大程度取决于人工设定知识(如规则、用例库等)的质量.Page15与本文的方法类似的还有文献[24-26],这些方法都采用了基于强化学习的方法来调整系统的资源,达到保障应用系统性能的目的.其中,文献[24]所调整的资源是以虚拟机为粒度,即增加、减少虚拟机的个数,而文献[25-26]是通过调整单个虚拟机的资源来增加其处理能力.但是这些方法都是将优化的对象视为一个整体,当这些方法应用于由多个组件服务组成的SBS时,由于环境状态和自适应动作的增加,可能会出现决策效率较低的情况.而本文通过环境状态和引导算子对自适应动作集合进行筛选,有效地缩小候选自适应动作的范围,保证自适应决策的高效性.在针对SBS的自适应资源分配方面,文献[27]采用了基于反馈控制的方法,并利用径向基函数(RadialBasisFunction,RBF)神经网络在应用系统运行时自适应的调整控制参数;文献[28]首先建立每个组件服务的RAT(Resource-Allocation-Throughput)模型,并将模型扩展至整个SBS,从而分析SBS所承受的负载和分配的资源之间的关系,然后将资源分配问题转化为线性规划的优化问题并进行求解.但在这两个方法中,都需要在离线阶段建立与SBS相关的模型,当SBS的组件服务发生动态变化时,离线模型可能需要重新训练,而采用Q-learning这种无模型的方法则可以避免此类问题.所以,针对现有的方法中存在的一些问题,本文提出了一种基于强化学习的云应用自适应方法,而强化学习可以较好的满足自适应决策过程对于动态性、高效性等方面的要求[29].本文采用了Q-learning算法,该算法是一种无模型(model-free)的在线学习算法,在自适应的过程中不需要建立应用系统的性能模型,可以更好地应对自适应过程中的动态变化.此外,本文还提出了一种引导算子,可以有效地缩小候选自适应动作的范围,从而保障算法的学习效率.9结论本文提出了一种基于强化学习的SBS云应用自适应优化方法,建立了自适应基本要素之间相互关系的特征描述框架,并详细描述了如何在决策的过程中使用经验和如何累积自适应调整之后的经验.通过实验表明,本文提出的方法是有效的,并具有以下优点:(1)采用了事件触发机制,可以避免云应用系统的性能符合SLA要求后进行“无效”的调整;(2)采用了无模型的在线学习算法(Q-learning算法),可以更好地应对自适应过程中的动态变化;(3)提出了一种引导算子,利用被触发的Event对所有可执行的Action进行筛选,限定其在特定范围的Action集合中进行选择,从而提升学习算法的效率.
