Page1一种基于随机游走模型的多标签分类算法郑伟王朝坤刘璋王建民(清华大学软件学院北京100084)(清华信息科学与技术国家实验室北京100084)(信息系统安全教育部重点实验室北京100084)摘要在数据挖掘领域,传统的单分类和多分类问题已经得到了广泛的研究.但是多标签数据的普遍存在性和重要性直到近些年来才逐渐得到人们的关注.在多标签分类问题中,由于标签相关性的存在,传统的单分类和多分类问题的解决方法,无法简单地应用于多标签分类问题.文中提出了一种基于随机游走模型的多标签分类算法,称为多标签随机游走算法.首先,将多标签数据映射成为多标签随机游走图.当输入一个未分类数据时,建立一个多标签随机游走图系列.而后,对图系列中的每个图应用随机游走模型,得到遍历每个顶点的概率分布,并将这个点概率分布转化成每个标签的概率分布.最后,基于多标签随机游走算法,文中给出了一种新的阈值学习算法.真实数据集上的实验表明,多标签随机游走算法可以有效地解决多标签分类问题.关键词多标签;分类算法;随机游走;阈值学习1引言数据分类(dataclassification)是数据挖掘(datamining)的一个重要研究方向.一直以来,数据分类问题和方法受到了人们的广泛关注和研究.传统数据分类问题的研究目标是如何将每条数据准确地划分到某一类中.如果候选类别只有一个,则分类目标转化为判断未分类数据是否属于该类别,这类问题被称作单分类问题(single-classclassification)或二值分类问题(binaryclassifica-tion).如果候选类别有多个,在传统的分类问题中,分类器仅能在这些候选类别中选择一个作为输出,这类问题被称作多分类问题(multi-classclassifica-tion).多分类问题可以比较容易地转化成单分类问题.单分类问题和多分类问题统称为单标签分类问题(single-labelclassification).它们和本文研究的多标签分类(multi-labelclassification)问题有着本质的区别[1].在实际应用中,普遍存在如下情况:一条数据可能同时属于多个不同的类别.这类数据被称作多标签数据.例如,Lewis等研究了路透社的804414条新闻,发现平均每条新闻同时属于2.6个不同的类别[2];在ACMComputing分类体系中,存在着一级类别11个、二级类别81个,而作者可以为每篇文章选择多个不同的类别[3];Snoek等人通过分析43907个从非洲、中国和美国收集的音频片段以及与这些音频片段相关的101个标签,发现平均每个音频片段具有4.4个不同的标签[4].这样的分类问题被称作多标签分类问题(见定义1).和传统的单标签分类问题相比,多标签分类问题存在着显著的区别,类别间的相关性(relevance)和共现性(co-occurrence)直接导致传统的单标签分类方法不能被直接应用到多标签分类问题中[1,5].多标签分类问题正逐渐成为当前的一个研究热点.多标签分类问题的形式化定义如下所示.定义1.已知一个定义在实数域R上的d维输入数据空间,记作X=Rd;一个包含q个标签的标签集合,记作Y={λ1,λ2,…,λq}和一个包含m个训练数据的训练集合,记作D={(xi,Yi)|1im,xi∈X,YiY}(1)其中xi是输入空间X中的一个训练数据,Yi是xi的真实标签集合(actuallabelset).多标签分类问题指:根据训练数据D学习分类函数f:X→2Y,当输入一个未分类数据x∈X时,通过函数f得到x的预测标签集合PxY,使得Px与x的真实标签集合Yx最为接近.易知,单标签分类问题是多标签分类问题的一个特例.当训练和测试数据都满足|Yi|=1时,多标签分类问题退化成多分类问题.特别地,当q=1时,多标签分类问题退化为单分类问题.多标签排序问题是与多标签分类问题直接相关的一类问题,其形式化定义如下.定义2.已知输入空间X、标签集合Y和训练数据集合D如定义1所示.多标签排序问题指:根据训练数据D学习函数g:X×Y→R,当输入一个未分类数据x∈X时,对于任意的y∈Y,得到一个置信系数g(x,y),并根据该置信系数对Y中的所有标签进行排序,使得此排序结果与真实结果最为接近.不同的指标被用于度量分类或排序结果的正确性,例如Precision/Recall/F-Measure、Subsetaccu-racy、HammingLoss、One-Error、RankingLoss、Coverage、AveragePrecision等[5](见5.1节).多标签分类问题主要有两大类解决方法:基于问题转化的方法和基于算法转化的方法[5](见第2节).本文提出了一种基于随机游走模型的多标签分类算法MLRW(Amulti-labelclassificationalgorithmbasedontherandomwalkmodel).主要贡献有(1)提出了一种新的多标签分类算法MLRW.MLRW在预测未分类数据时,除了能够给出分类结果,还可以结合条件概率模型,给出该数据具有每个标签的概率分布.(2)基于MLRW算法,提出了一种分类阈值学习方法,该方法可以解决多标签分类算法中的阈值设置问题.(3)真实数据集上的实验结果表明,MLRW算法和分类阈值学习方法能有效解决多标签分类问题和多标签排序问题.本文第2节介绍与本文有关的研究工作;第3节和第4节分别给出MLRW算法和阈值学习方法;第5节给出MLRW算法的相关讨论;第6节介绍实验方法和实验结果;最后总结全文.2相关工作近年来,多标签分类和排序问题受到了人们的广泛关注和研究.其解决方法主要分为基于问题转Page3化的方法(ProblemTransformationbasedmeth-ods,PT)和基于算法转化的方法(AlgorithmAdap-tationbasedmethods,AA).2.1基于问题转化的方法PT类方法的主要目标是将一个多标签分类问题转化成一个或一组单标签分类问题,从而运用已有的单标签分类方法解决该问题.BR(BinaryRelevance)是一种典型的PT方法,它将每个标签的预测看作一个独立的单分类问题,并为每个标签训练一个独立的分类器,用全部的训练数据对每个分类器进行训练.这种算法忽略了标签之间的相互关系,往往无法达到令人满意的分类效果.文献[6]通过拷贝(copy)和带权重拷贝(copy-weight)的方法,对BR进行改进,将原训练集合中的一条多标签数据拆分成多条单标签数据,并给予相应的权重.Hullermeier等提出了基于标签对比(pairwisecomparison)的分类方法.通过对比标签集合中任意两个标签之间的关系,建立q(q-1)/2个分类器.每个分类器在两个标签λi和λj之间投票,然后组合这些投票结果作为最终的多标签分类结果[7].假设多标签分类算法中采用的基础分类器(baseclassifier)的复杂度为O(t(D)),其中函数t(D)表示分类器在训练集合D上建立分类模型的复杂度,则基于标签对比的多标签分类算法的复杂度为O(q(q-1)/2·t(D)).LP(LabelPowerset)是另外一种被广泛使用的PT方法.它将训练数据中的每种标签组合进行二进制编码,从而形成新的标签.在LP中,多标签数据被以这种方式转化成单标签数据.LP算法的显著缺点是不能预测新的标签组合.Read等将概率分布模型应用到LP中,当对未分类数据进行预测时,可以预测出训练集合中未出现的标签组合[8].但是LP算法的复杂度较高,达到O(min{2q,m}·t(D)),可以通过剪枝[8]或随机标签组合[9]的方法在一定程度上降低复杂度,但降低的幅度有限.2.2基于算法转化的方法AA类方法的主要目标是,通过改变已有的单标签分类算法,使其能够处理多标签数据.典型的AA算法有以下几种:基于单标签分类算法AdaBoost.M1,Schapire等提出了适用于多标签数据的AdaBoost.MH算法[10],该算法使用每个多标签训练数据生成q个新的单标签训练数据.该算法的主要缺点是,显著地增加了训练数据的数量,进而增加了建模时的消耗.人工神经元网络也可以应用到多标签分类问题中.Zhang等人通过定义针对多标签数据的全局优化函数,使得人工神经元网络能够处理多标签数据[11].该算法基本思想是,如果很多实例同时具备两个标签,那么这两个标签中的一个出现了,另外一个也很可能同时出现.经典的kNN方法也可以应用到多标签分类问题中,例如文献[12]中介绍的MLkNN算法.MLkNN通过统计方法,得出每个标签的先验概率.当输入一个未分类数据x时,对标签集合Y中的每个标签λ,分别计算x具有标签λ和不具有标签λ的概率,进而预测x是否具有标签λ.C4.5决策树也可应用于多标签分类问题中,只需要将单标签分类问题中熵的定义扩展到多标签分类问题.Clare等定义多标签分类问题中的熵为MLEntropy=∑y∈Y而后便可以基于熵计算信息增益,从而对多标签数据建立决策树[13].此外,经典的Bayes等算法也可以通过修改而被用于多标签分类问题中.此外,基于已有的多标签分类算法,Tsoumakas等提出了二层的多标签分类模型,第一层中采用BR、决策树或SVM等进行k-fold交叉训练;在第二层中,采用BR、SVM等算法,使用第一层训练后得到的各标签的得分或概率分布作为输入,来预测最终的标签输出结果[14].2.3随机游走模型随机游走模型的基本思想是,从一个或一系列顶点开始遍历一张图.在任意一个顶点,遍历者将以概率1-α游走到这个顶点的邻居顶点,以概率α随机跳跃(teleport)到图中的任何一个顶点,称α为跳转发生概率.每次游走后得出一个概率分布,该概率分布刻画了图中每一个顶点被访问到的概率.用这个概率分布作为下一次游走的输入并反复迭代这一过程.当满足一定前提条件时,这个概率分布会趋于收敛.收敛后,即可以得到一个稳定的概率分布.随机游走模型广泛应用于数据挖掘和互联网领域,PageRank算法可以看作是随机游走模型的一个实例[15].Zhang等人使用该模型从书评中挖掘关键词[16];Zhu等人提出了有吸收状态的随机游走模型,该模型可以用于文本自动摘要(textsummariza-tion)和基于社会网络的分析与挖掘[17].本文使用收Page4敛后的概率分布来刻画未分类数据具有每个标签的概率.3MLRW和阈值学习算法3.1随机游走图的生成MLRW算法首先将训练集合D映射成d维度空间中的多标签随机游走图.我们使用随机游走模型的原因是:该模型通过点与点之间的连通性准确地刻画训练数据之间的相关性,进而刻画候选标签之间的相关性.MLRW的基本思路是:将集合D中的每个训练数据x∈X映射为图中的一个点v.如果两个训练数据xi、xj具有相同的标签,则将这两个训练数据对应的顶点vi、vj相连.形式化地,已知训练集合D如式(1)所示,则由训练集合D导出的多标签随机游走图记作:E={(vi,vj)|vi,vj∈V,Yi∩Yj≠,i≠j}(4)如无特别说明,本文余下部分使用vi表示训练数据xi在随机游走图上对应的顶点.例如,式(3)表示每个训练数据xi将对应图G中的一个顶点vi,这些顶点构成了图G的顶点集合V.接下来,我们计算随机游走图G上的权重矩阵犠.如式(5),边的权值即为训练数据对应顶点在d维空间中的距离,记作dis(vi,vj).本文采用欧式距离作为距离函数.Wij=不失一般性,可以假定图G是连通的.如果G中存在不连通的子图,则说明标签集合Y中存在相互独立的标签子集,G中的每个连通分量对应Y中的一个独立子集.此时,我们可以根据G中的连通分量,将标签集合Y拆分成多个互不相交的子集,并对每个子集分别应用MLRW算法.因此,本文后面的内容都将基于图G是连通图这一前提展开.例如,给定一个标签集合Y={λ1,λ2},训练集合中包含6条数据(如表1),训练数据x1,x2,x3有相同的标签λ1,则将这3个点两两相连,连接它们的边的权重即为这3个数据的特征向量在输入空间中的欧式距离.同理,x3,x4,x5,x6同时具有标签λ2,把它们两两相连,则由训练集合D导出的随机游走图如图1所示.123456可以看出,随机游走模型无法直接应用于传统的单标签分类问题.如果将单标签数据映射为随机游走图,得到的将会是不连通图,这不满足随机游走算法的收敛条件(见定理1).然而,将多标签数据映射为随机游走图时,满足随机游走模型的收敛条件,因而得以应用.3.2多标签随机游走过程3.2.1随机游走过程随机游走过程需要4个输入参数:邻接矩阵犘(adjacentmatrix),初始概率分布向量狊0,跳转发生概率α(teleportingprobability),发生跳转时跳转到图中每个顶点的概率分布向量犱.每次游走过程后的输出概率分布向量记作狊,则狊的计算法方法为将向量狊作为式(6)的输入狊0,反复迭代式(6)直至收敛,将此时的概率分布向量记作π,满足式(7)中的向量π即为稳定的概率分布向量.为了应用式(6),首先基于权重矩阵犠计算邻接矩阵犘.基本思想是,对任意顶点v,在v的所有邻居顶点中,如果一个顶点距离v越远,则游走到这个顶点的概率就越低,如式(8)所示.Mij=对矩阵犕进行归一化处理:此时的概率分布矩阵犘即为算法输入的邻接矩阵.Page53.2.2多标签随机游走图系列当输入一个未分类数据x时,将x对应的顶点记作u,MLRW将以u作为起始点应用q次随机游走模型.具体地,在第k次应用随机游走模型时,将u与所有具有标签λk的点相连得到多标签随机游走图Gk.我们将这些图的集合{Gk}(k=1,2,…,q)定义为多标签随机游走图系列(如图2所示).图2输入未分类数据x时(将x对应的点记作u):(a)将u与具有标签λ1的点(v1,v2,v3)相连,得到图G1;(b)将u与具有标签λ2的点(v3,v4,v5,v6)相连,得到图G2定义3.已知多标签随机游走图G、标签集合Y和一个未分类数据x,则定义由G和x导出的多标签随机游走图系列为T={Gk|k=1,2,…,q},其中其中,u是未分类数据x对应的顶点,vi是训练数据xi对应的顶点,Yi是xi的真实标签集合.接下来,我们对T中的每个图以u为起点应用本文3.2.1节所描述的随机游走过程.此时,我们还需要计算初始向量狊0.首先计算狊0,狊0是一个m维向量,它的第i个元素为对狊0使用类似于式(8)~(10)的方法进行归一化,即得到初始向量狊0.在本文中,我们假设从某个顶点出发跳转到图中任意一个顶点的概率是相等的,得到随机跳转到每个顶点的概率分布向量:此外,我们还得知α的一般取值为0.15[16],本文的实验部分将对α取值对结果的影响进行讨论.至此,我们已经得到了随机游走过程所需的所有输入,将它们代入式(6)中,可以得到概率分布向量狊,通过反复迭代式(6)直至收敛,可得出最终的概率分布向量π.π刻画了将未分类数据x对应的顶点u与具有标签λk的数据对应的顶点相连(记作x<λk)时,以u点为起点游走到图Gk中每个顶点的条件概率.我们将该条件概率记作其中,π(i)表示向量π的第i个元素.将每个标签对应的点取其条件概率的平均值,即为以u点为起点遍历图Gk时游走到每个标签的平均条件概率:P(λj∈Yx|x<λk)=avg{P(vi|x<λk)|λj∈Yi}3.2.3条件概率模型根据条件概率模型,未分类数据x具有标签λj的概率可以采用以下公式计算:P(λj∈Yx)=∑1kq因此,我们还需要求出未知数据与具有标签λk的点相邻的先验概率P(x<λk).在本文中,使用u点和具有标签λk的数据对应顶点的平均距离来刻画这个先验概率.即平均距离越大,则该先验概率的值就越小.为此,首先计算一个临时变量,记作而后,使用类似于式(8)~(10)的方法对式(19)进行归一化,即可得到所需的先验概率P(x<λk),将其代入式(18),得到最终的概率分布结果.MLRW算法的形式化描述如图3所示.3.3图剪枝标签集的势指平均每条数据具有的标签数[5],记作c.我们发现,当训练集中标签的势较大时,图G中边的数量会大大增加.这是因为,平均每个标签关联的数据为O(mc/q),则每个点平均具有边O(mc2/q),图G中边的总数为O(m2c2/2q).由此可以看出图G中的总边数随c的增大而快速增大,当c槡q时,MLRW算法的空间消耗快速上升.因此,我们对图G进行如下剪枝,以降低算法的空间消耗.定义4.已知图G=(V,E),其上的权重矩阵为犠,则图G上的Top-k剪枝指的是,对每个顶点vi∈V,将其相关联的所有边{(vi,vj)|i≠j,(vi,vj)∈Page6E},按照其权重Wij排序,保留其中权重最小(即距离最近)的k条边,将其它边从图G中删除.我们将在本文实验部分对剪枝的影响进行讨论.算法1.多标签随机游走算法MLRW.输入:训练数据集合D,随机跳转发生概率α,输出:x具有Y中每个标签的概率分布P(λj∈Yx)MLRW(D,α,x,Y)1.初始化数组PC、PP//临时保存条件概率、先验概率2.构造随机跳转到每个顶点的概率分布向量犱//式(15)3.fork←1toqdo4.根据D,x,λk构造随机游走图Gk//式(11)~(13)5.根据Gk计算邻接矩阵犘6.根据Gk计算随机游走初始向量狊0//式(14)7.应用随机游走模型得出条件概率分布向量,记作犙:犙(j)=P(λj∈Yx|x<λk),j=1,2,…,q//式(16)~(17)8.将向量犙保存到数组PC中,即PC[k]=犙9.计算先验概率并保存在数组PP中:PP[k]=P(x<λk)10.endfor11.foreachλj∈Ydo11.根据PP,PR计算概率分布P(λj∈Yx)//式(18)12.endfor4分类阈值学习方法由式(18),当输入一个未分类数据x时,可求出x具有每个标签的概率分布.通过该概率分布,可以得到一个排序后的标签集合.此时,为了决定每个标签的取舍,还需要为每个标签给定一个阈值,将概率大于阈值的标签集合作为x的预测标签集合Px.多标签分类中的阈值确定问题,同样得到了人们的广泛研究.例如,Fan等提出了SCutFBR算法[18],Tang等人提出了基于训练数据的阈值学习方法[19].但是,这些阈值学习的方法由于没有与具体的分类方法相结合,往往难以取得好的效果.本文基于MLRW算法,给出一种新的阈值学习方法.具体地,首先对训练集合D进行随机采样,生成采样集合D.对D中的每一个数据xi,以xi对应的顶点为输入应用MLRW算法,由式(18)可以得到一个q维的概率分布向量,记作犘i.而后我们使用这|D|个向量通过如下操作得到一个q维的接受阈值(acceptthreshold)向量和一个q维的拒绝阈值(rejectthreshold)向量,分别记作犘a、犘r,如其中,犘a(j)表示向量犘a的第j个元素,其它类同.最终的阈值向量为这两个阈值的平均:当输入一个未分类数据时,首先通过算法1得到x具有每个标签的概率,而后与阈值向量犘T比较,进而确定每个标签的有无.5算法讨论5.1收敛性定理1.MLRW算法是收敛的.证明.(1)因为向量犱中不包含非零元素,并且0<α<1,所以从任意点开始,随机跳跃到图Gk中的任意点都是可能的,故邻接矩阵犘是不可规约的(irreducible).(2)当随机游走过程遍历到某一顶点后,再次遍历到这个顶点所需的步数是不确定的,故整个随机游走过程是一个非周期的过程(aperiodic).(3)显然,当图中任意一个顶点被遍历后,都可能在有限步数内再次遍历这个顶点,且再次遍历之前经过的步数是不完全相同的(positiverecurrent).由以上3点,可以得出MLRW算法是各态历经的(ergodic)[20],故此算法是收敛的.即存在向量π,满足式(7).5.2复杂度分析定理2.MLRW算法的时间复杂度为O(qlogm),空间复杂度为O(q|E|).其中,q表示标签集合Y的大小,E表示由训练集合D导出的随机游走图G中边的集合,m是训练集的大小.证明.算法1的第3~6行循环的复杂度由算法第6行随机游走的迭代次数决定.根据文献[21],MLRW算法中随机游走的迭代次数为O(log|E|)=O(logm).|Y|=q,故MLRW算法的时间复杂度为O(qlogm).算法1中需要存储的变量为转移矩阵犘、随机跳转向量犱、每次迭代后的概率分布向量狊、数值α和λk.其中,转移矩阵犘的大小等于图Gk中边的数目.故算法的空间复杂度为O((m+m+|E|)q+1+1),由于图Gk连通,所以总的空间复杂度为O(q|E|).证毕.6实验6.1数据集和度量标准本文采用yeast数据集①,该数据集是对啤酒酵①数据集可在http://mulan.sourceforge.net/datasets.htmlPage7母菌细胞基因表达的研究结果.经过微阵列实验(microarrayexperiments),大量的基因片段(大约3300个)被按照功能进行分类,其中的2417条数据构成了yeast数据集[22],其统计数据如表2所示.其中标签密度等于标签集的大小q除以标签集的势c,表示每个标签出现的平均概率.数据集名称yeast1500917103140.304.20本文使用平均精度(Avg-Precision)、(One-Error)、结果覆盖长度(Coverage)等指标对实验结果进行度量,它们的定义分别为[23]One-Error=1Avg-Precison=1其中ri(λ)表示标签λ的排名.本文实验环境为IntelCore2.33GHz的CPU,4GB内存,1.5TB硬盘的PC机.操作系统为Ubuntu9.10,Java版本SunJDK1.6.0.6.2实验结果6.2.1对比实验我们基于MuLan①实现了MLRW算法.Mu-Lan是一个基于Weka②的开源项目,它实现了一些最近提出的多标签分类和排序算法.实验中采用的对比算法有Homer、BR(BinaryRelevance)、CLR(CalibratedLabelRanking)、MLkNN(Multi-Labelk-nearestneighbor)、RAkEL(Random-kLabelsets)、LP(LabelPowerset)等(见第2节).其中Homer算法中Cluster的数量为3,MLkNN中k=10,其它均采用默认参数.BR、CLR、IncludeLabels、RAkEL、LP算法的基础分类器(baseclassifier)采用SVM分类器,该SVM分类器采用线性核函数,常数c的值为1.Homer分类算法采用CalibratedLabel分类器作为基础分类器.在对比实验中,我们将原有数据集中的训练集和测试集混合,随机重新采样排序,然后用10-fold交叉验证(crossvalidation)的方法对结果进行验证.对以上实验重复进行10次,取其平均值.MLRW算法中剪枝粒度k设定为100.如表3所示,MLRW算法可以达到较好的平均精度和较小的误差.MLRW算法的平均精度与MLkNN算法几乎相同,但MLRW算法的结果覆盖长度比较小,也就是说,使用MLRW算法可以用较小的误差找全所有的正确标签集合.而较低的One-Error值(相比MLkNN领先5.6%),则说明MLRW算法给出的排名最靠前的标签(top-onere-latedlabel)不在该数据实际标签集合中的概率较低.这在信息检索应用中非常重要,因为大多数用户往往只关心排名靠前的检索结果[24].Homer0.25018.23020.6955BR0.36647.50700.6613CLR0.25976.79710.7097MLkNN0.28447.41430.7284LP0.52679.59650.5633RAkEL0.29117.65430.7096MLRW0.24517.41000.70696.2.2剪枝粒度对实验结果的影响通过实验我们发现,剪枝粒度越大(k值越小),随机游走过程中,达到稳定前迭代的次数就越多.从图4中可以看出,随着k值的增大.迭代数目明显减小.这是因为,剪枝粒度的减小,图G中边的数量增加,图的连通性增强,邻接矩阵犘中每一列的方差减小,遍历到每点的概率趋于平均,因此收敛速度加快.通过对比剪枝粒度对算法精度的影响,我们发现,改变剪枝粒度的大小(k值),对实验精度的影响极小,算法的平均精度维持在69.75%±0.5%(如图5所示).可以认为,剪枝的粒度只会改变算法的收敛速度,不会对算法的精度造成大幅度影响.①②Page86.2.3α取值对实验精度的影响由式(6)得知,α表示游走过程中随机跳跃到某个点进行遍历的概率.α值越大,则随机跳跃到其它点的概率就越大.当取k=100时,α变化对算法精度的影响如图6所示.从中可以看出,α取值的变化对算法精度的影响比较小(不超过±3%).当算法进行迭代时,尽管随α的变化,实验结果也会有所变化,但是由于该数据集中标签集的势较高(c=4.20),图G的连通性较强,因此,α取值的变化对收敛后的概率分布影响不大.7结论本文介绍了一种基于随机游走模型的多标签分类算法MLRW,能够有效解决多标签分类和排序问题.今后将考虑标签数量和分布对分类结果的影响.同时,在今后的工作中也将考虑进一步提高算法的效率.
