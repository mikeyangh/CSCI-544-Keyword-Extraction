Page1科学计算双路并行I/O优化方法曹立强莫则尧沈卫超夏芳陈军(北京应用物理与计算数学研究所高性能计算中心北京100094)摘要科学计算数据集由数据和元数据组成.一般条件下,数据的尺寸较大,元数据尺寸较小.传统的高性能计算机并行文件系统可以高效率地读写大块连续数据,但是无法高效率地读写大量较小块的元数据.一旦大块数据和小块元数据两类读写特征混杂在一起,元数据将较严重地干扰并行I/O,造成性能的下降.为此,文中提出数据与元数据分治的双路并行I/O方法.该方法在高层I/O库中建立内存文件系统与并行文件系统两级存储,在存储资源之间并行迁移科学计算元数据.一方面降低较频繁读写元数据的I/O延迟,另一方面改变科学计算数据的存储特征与存储模式,从而提高科学计算应用、尤其是数据分析与可视化等读入密集型应用的I/O效率.测试表明,双路并行I/O方法可提高写性能8%~13%,提高读性能89%到1.01倍.关键词并行I/O;高层I/O库;性能优化;数据格式;双路并行I/O1引言科学计算数据场由数据集合组成.数据集合包含数据与元数据,其中数据承载计算,元数据记录数据的名称、类型、位置和属性等信息.在实际应用中,科学计算程序多使用HDF5(HierarchicalDataFormat5)①和NetCDF(NetworkCommonDataForm)②等高层库读写数据集合.这些高层I/O库有面向领域的存储模型,可以简化研制应用的复杂度,提高科学数据共享水平.科学计算应用大量的读写数据集合.从I/O特征角度,多数元数据尺寸较小,低于KB量级,在文件中存储位置分散,对I/O性能、尤其是大规模并行I/O性能有一定影响.在BG/L系统上,Yu等人[1]使用FLASHI/O和HOMME等应用程序分析ParallelNetCDF和HDF5的读写性能.相同条件中,使用HDF5的并行输出带宽仅相当于MPI-I/O峰值输出带宽的10%左右,使用ParallelNetCDF的并行输出带宽仅相当于MPI-I/O峰值带宽的20%~30%.由于元数据存取的需求,高层I/O库的读写数量比MPI-I/O高2倍以上,并且多数情况下的存取粒度较小,位置规律不明显.这是目前高层I/O库效率较低的主要原因之一.两阶段I/O(TwoPhaseI/O)[2]与数据过滤(DataSieving)[3]等方法可提高小块数据的I/O性能,但是这些方法主要面向密集非连续存储的特征,与元数据的小块松散非连续特征不一致.在BG/P平台的FlashI/O测试程序中,Latham等人[4]依次使用集合I/O、列存储、非阻塞I/O等方法优化高层I/O库ParallelNetCDF性能.实验环境中,非阻塞I/O方法有最高的带宽和最好的可扩展性.在搭配65536处理器时取得近8GB/s的输出带宽,接近文件系统读写带宽的80%.然而,非阻塞I/O是一种使用内存缓存数据的I/O方法.当数据规模较大时,非阻塞I/O会较大幅度降低系统可用内存数量.数据的层次化存储是充分利用计算机资源,提高I/O带宽、缩短I/O延迟的有效方法.其主要挑战一方面在于构建有效的层次存储结构,另一方面在于高效利用近线存储资源.文件系统缓存与I/O代理(I/ODelegationCache,IODC)[5-6]可降低I/O延迟,提高I/O效率.然而许多科学计算数据的规模较大,其中大部分应用对数据有一过性访问的特征.缓存与I/O代理的容量较小,在没有应用指导数据替换策略时,往往不能充分发挥缓存与代理的性能.IOFSL(I/OForwardingScalabilityLayer)[7-8]是美国Argonne实验室、LosAlamos实验室和Sandia实验室等机构联合研制的可扩展I/O框架.该框架位于高层I/O库与文件系统之间,使用专有的I/O节点聚合、转发高层I/O调用,提高效率.I/OForwarding可以降低I/O节点竞争,减少数据拥塞,提高大规模并行条件下的可扩展性.然而,I/OForwarding增加了数据的移动距离,当I/O连续性较好,形成流水线时,IOFSL有较高的效率.一旦I/O连续性较弱或者完全随机时,IOFSL的效率将受到影响.较长时间以来,并行科学计算应用的I/O面临瓶颈制约.本文在应用并行I/O过程中使用内存文件系统与并行文件系统构建两级存储结构.改变高层I/O库的数据、元数据混合存储为数据、元数据分类存储.在并行I/O过程中迁移规模较小但是存取频次却较高的元数据文件到局部内存文件系统.一方面降低元数据I/O的总延迟,另一方面改善数据在文件系统的存储布局,提高应用程序可获得的I/O带宽.本文的创新主要在于:(1)数据迁移技术在并行I/O领域的应用.已有的数据迁移技术一般应用虚拟化存储等领域,在保障存储资源性能的同时降低单位成本.本文将其应用于并行I/O性能优化中,提高粒度较小,但存取较频繁的元数据I/O性能,提高并行I/O效率;(2)本文将数据、元数据分类存储模式与并行I/O结合,提出元数据分组并行I/O模式,在并行环境中保障元数据存储一致性;(3)本文利用高性能计算机的局部内存文件系统改变并行I/O特征,优化数据、元数据的存储布局,加速数据I/O.本文第2节介绍高层I/O库;第3节分析高层I/O库的存储特征;第4节介绍双路并行I/O库DCPL(DoubleChannelParallelI/OLibrary)的结构设计;第5节是DCPL的串、并行实现;第6节是DCPL的单节点性能分析;第7节是并行应用测试;第8节是结论.2高层I/O库简介科学计算高层I/O库主要管理集合形式的科①②Page3学计算数据.科学计算数据场由大量的数据集合组成.一个数据集合可以抽象表示为〈数据,元数据〉二元组.其中元数据是用于管理数据的数据.有了元数据,高层I/O流程可以分为两部分:数据I/O与元数据I/O.以读数据为例,高层I/O库的读写步骤一般如图1所示.图1的调用分为6个步骤:第1步,科学计算程序调用高层I/O接口;第2步,高层I/O库定位元数据;第3步,高层I/O读取元数据并解析其中关于数据的描述,获取数据类型与存储位置等信息;第4步,根据元数据指示,高层I/O库验证数据的偏移、类型与数量;第5步,高层I/O库读取数据,并根据需要解码、转换类型;第6步,高层I/O库返回调用.使用高层I/O库写数据的过程与读数据过程相似,也包含写元数据和写数据两个步骤.一些数据管理能力较强的高层I/O库,例如HDF5等,使用B树等索引算法为元数据建立索引,提高上述过程中第2步元数据定位的速度.并行环境中,高层库的I/O模式主要有两种:进程-文件对应(File-per-process)模式和共享文件(Sharedfile)模式.虽然同为并行I/O,但是前者进程间无共享数据.参与并行I/O的进程读写各自文件.元数据与数据是私有的,有进程属性,其流程与串行过程相似.共享文件模式中,并行文件系统提供基本的全局文件视图.高层I/O库在进程间管理、分配元数据与数据.数据输出时,它一般采取元数据串行I/O、数据并行I/O的方式保障数据存储一致性.参与并行I/O的主进程不仅要管理元数据,还要在进程间切分数据集合,然后由主、从进程共同写数据.数据读入时,仍然由主进程读取元数据,然后切分数据集合,主、从进程并行读入数据.在科学计算领域,目前高层I/O库的两个比较重要实现是HDF5和NetCDF.HDF5是美国国家超算应用中心(NationalCenterforSupercomputerApplications,NCSA)面向科学数据存储研制的一种文件格式.HDF5支持集合数据的I/O.一个HDF5数据集合由数据与元数据两部分组成.其中的元数据包括数据的名称、属性、类型、编码方式、I/O方式等说明信息.HDF5使用B树为数据集建立索引,支持十亿量级的数据查询与PB以上量级的数据存储.HDF5兼容多种操作系统,支持多种I/O方式,包括系统调用、标准I/O、MPI-I/O等.此外,HDF5格式有跨平台和自描述特征.使用HDF5存储的科学数据,可以不依赖文件外的信息还原到其他节点.并行HDF5是HDF5的一种I/O方式.它支持进程并行读写数据集中的数据.目前,HDF5在科学数据存储领域有较广泛的应用.除了较多的计算应用外,一些知名的项目还包括HDF-EOS和BioHDF等[9-10].NetCDF是美国大气研究综合协会(UniversityCorporationforAtmosphericResearch,UCAR)研制的一种跨网络数据存储格式.它支持数据集合的跨平台存储与共享.一个NetCDF的数据集合包括数据与元数据两部分.其中数据有唯一的名称,并使用唯一的ID指代,元数据包含维长、变量名称与属性等信息.NetCDF兼容多种操作系统,有跨平台和自描述特征.它使用XDR①或者HDF5存储数据.一个NetCDF文件,可以不依赖文件外的信息还原到其他节点.ParalleNetCDF是对NetCDF的并行扩展,它在并行I/O中支持共享文件模式的数据集共享[11].目前,NetCDF和ParallelNetCDF在环境和气象科学计算领域有较广泛的应用.3高层I/O库的存储特征元数据占据文件的一部分存储资源.它改变了科学计算数据在文件中的存储布局.以Lared-P[12]科学计算应用的HDF5数据为例.Lared-P是基于JASMIN框架[13-14]研制的并行计算程序.它在笛卡尔坐标系内求解电磁场Maxwell方程、静电势Poission方程和粒子云运动等方程,保存计算结果用于可视化分析与程序重启动.分析Lared-P中HDF5相关的调用,80%以上用于存取数据的名称、位置、类型和属性等元数据信息.解剖典型可视化文件中数据、元数据分布如图2所示.图中每个块大小8KB,浅色代表数据,深色代表元数据.①ExternalDataRepresentation.http://tools.ietf.org/html/Page4从存储规模上分析,Lared-P数据场的元数据仅占总数据存储规模的4.2%.较少的元数据间隔存储于数据中,一方面降低了文件系统缓存效率,另一方面增加了I/O操作的不连续性.并不是所有数据场的数据与元数据交替存储,但是这种大量数据与较少元数据交替存储的形式在科学计算数据场中长期存在.这种数据与元数据交替存储的形式,不仅不能充分利用当前并行文件系统的带宽,也不利于数据再分析中快速获取数据场的概要信息.4双路并行I/O的结构设计高层I/O库位于并行应用与并行文件系统之间,它可以屏蔽文件系统的差异.利用这一性质,我们设计双路并行I/O库DCPL(DoubleChannelParallelI/OLibrary).DCPL面向科学计算数据集合存储需求.支持结构网格、非结构网格和无网格数据存取与管理.与HDF5和NetCDF相似,一个DCPL数据集合包括数据和元数据两部分,其中的元数据包括网格类型、属性、编码方式等说明信息.DCPL使用B+索引算法对数据集合建立索引,提高数据管理能力.目前,DCPL已经在JASMIN等编程框架中提高大规模并行应用的I/O效率.DCPL在单节点内的结构如图3所示.使用DCPL输出时,数据直接输出到文件系统.元数据首先暂存于内存文件系统,待DCPL关闭时迁入到文件系统,形成独立的元数据文件.读入过程则反之.元数据文件首先从文件系统迁入到内存文件系统,然后是元数据与数据的读入.内存文件系统有高带宽、低延迟的特征,但是它的容量较小,而且不能永久保存数据.并行文件系统有较高的带宽,但是多数条件下延迟较高,I/O性能受I/O特征影响较大.元数据的暂存与迁移不仅改进程-文件对应模式中,参与并行I/O进程在局部的内存文件系统与全局文件系统间并行迁移元数据.此时,每一个进程对应两个文件:直接读写的数据文件和迁移的元数据文件.进程-文件对应模式面临较多数量元数据文件的迁移问题.为此,我们设计元数据分组并行I/O模式作为进程-文件对应模式的补充.元数据分组并行I/O模式中,数据I/O模式与进程-文件对应模式一致,但是它的元数据I/O分组,一组对应一个文件.组内主进程维护元数据并行迁移过程中存储一致性.其体系结构如图4所示.变了数据和元数据的存储布局,还改变元数据的I/O特征.在大部分应用场景中可提高I/O性能.双路并行I/O为不同的并行I/O模式设计不同的元数据迁移方法.共享文件并行I/O模式中,双路并行I/O采取元数据串行、数据并行的方法保障数据存储一致性.此时,主进程节点内存文件系统与并行文件系统构成两级存储结构.主进程不仅在打开与关闭文件过程中迁入、迁出元数据,还负责管理数据的全局视图.参与I/O的从进程从主进程获取局部数据视图与相关元数据,并行读写数据.图4中,数据文件为进程私有,元数据文件在组内共享.应用程序仍然按照进程-文件对应模式并行打开文件.高层I/O库DCPL屏蔽数据与元数据的存储位置,为组内进程提供一致的数据视图.输出时,各进程在内存文件系统缓存元数据,直至文件关闭.然后将元数据文件合并迁移到并行文件系统.在保障数据存储的同时降低总的写延迟.读入时,参与并行I/O的进程首先将与进程对应的元数据迁移到局部内存文件系统,由内存文件系统提高元数据读入性能.数据的读写操作不受元数据迁入迁出影响,按需读入或写出.Page5图4分组并行模式的双路I/O体系结构元数据迁移不仅优化了数据、元数据的存储布局,还改变了科学计算数据管理模式.使用双路I/O方法后,数据场的存储形式由单一文件变成元数据、数据文件两类.应用程序仅需加载容量较小的元数据文件即可获得数据场的描述.这对于数据分析与数据再利用等的读入密集型应用有较好的加速效果.双路并行I/O提高了并行I/O效率.然而,元数据迁移弱化了并行进程的读-写时序关系,在系统故障时容易发生数据、元数据不一致现象.因此我们在DCPL的数据中设定元数据迁移成功标志.如果标志为真,那么表示数据与元数据一致,数据场完整;如果标志为假,那么表示有元数据缺失,场内数据不完整.需由应用程序处理场内数据不完整现象.5双路并行I/O的实现元数据层次存储是双路并行I/O性能优化的核心技术.我们使用内存文件系统tmpfs①与并行文件系统lustre[15]两级存储.Tmpfs是建立在操作系统虚拟内存设备上的文件系统.它是Linux系统缺省自带资源之一,一般用于存储临时的系统数据.Tmpfs的存储调度服从操作系统内存管理,容量根据可用内存数量自动调节.由于通用性好,性能高,许多系统服务依赖tmpfs交换数据.对象存储并行文件系统lustre在高性能计算机中有较广泛应用.它由对象存储设备(ObjectStorageTarget,OST)、元数据服务器(MetaDataServer,MDS)和客户端组成.目前,大型lustre并行文件系统可扩展至数百服务器与上万客户端,提供数十PB以上量级存储.在双路并行I/O中,我们使用tmpfs临时存储元数据,缩短延迟;使用lustre存储数据,提高带宽.元数据采用层次存储之后,数据读写流程有一定的变化.双路I/O不仅同时管理数据与元数据文件,还需要在内存文件系统与并行文件系统之间迁入、迁出元数据文件.为此,双路并行I/O使用memory与changed双开关结构指示元数据状态.当memory值为真,说明元文件存储于内存文件系统;memory值为假,说明元数据文件存储于并行文件系统.changed为真,说明元数据文件有修改,关闭时须迁移元数据文件;changed为假,说明元数据文件无修改,关闭时不需要迁移元数据文件.图5(a)是串行条件下打开元数据文件的流程图,其中load函数判断数据完整性标志,将元数据文件从文件系统迁移到内存.程序首先根据空闲内存容量和/dev/shm设备读写权限判断内存文件系统tmpfs是否可用,然后决定是否使用双路I/O.对于使用双路I/O的情况,如果需要创建元数据文件,①Thetmpfsdocumentation.https://www.kernel.org/doc/Page6那么在内存文件系统中创建它,并且置changed和memory开关为true,如果不需要创建元数据文件,那么从并行文件系统中加载它,置changed开关为false,memory开关为true.图5(b)是串行条件下关闭元数据文件流程图,其中save函数将元数据文件从内存文件系统迁移到外存文件系统,然后设置迁移成功标志;remove函数从内存文件系统删除元数据文件.关闭时,首先判断元数据文件是否在内存中.如果不在内存中,使用文件系统的调用关闭文件.如果在内存中且有修改,那么关闭文件后将文件迁移到文件系统,如果在内存中但是没有修改,那么关闭后从内存文件系统中删除元数据文件.元数据分组并行模式中,参与分组的各进程将元数据文件聚合存储于一个文件中.文件格式定义如下.首先是文件头,元数据文件头中记录了参与分组的进程数量,其后是分属于各个进程的元数据存储二元组〈len,offset〉列表.列表中每一项与一个进程对应,其中len代表进程内元数据长度,offset代表本进程元数据在文件中的起始位置.再后是按列表依次存储的各进程的元数据.元数据分组并行I/O过程中,DCPL采用MPI-I/O的文件视图与集合I/O[16]迁移元数据.文件视图可以在进程间协调分配存储空间,屏蔽各进程读写位置的差异,提高I/O操作的并行性.数据并行迁出部分代码如图6中所示.图7并行分组I/O中元数据的迁入通过元数据的分组并行迁入与迁出,双路I/O不仅可避免元数据并行读写的拥塞,还可根据系统配置和并行程序的元数据规模调节分组和分组内进程数量,提高计算节点与I/O服务器的匹配关系.此外,元数据分组还降低系统中文件数量,提高数据在并行文件系统上的可管理能力.6串行测试使用串行测试程序对比DCPL、HDF5和MPI-I/O的性能.与DCPL和HDF5相比,MPI-I/O仅具有数据读写功能,我们使用它对比DCPL和…对于每一个进程:…//切分存储空间进程结束…元数据分组迁出的步骤如下:第1步,主进程输出元数据文件头与二元组列表;第2步,由各进程根据二元组列表并行进程创建MPI数据类型,在文件中切分存储空间;第3步,各进程设定文件视图,屏蔽存储位置的差异;第4步,通过系统调用与MPI_File_write_all迁出元数据;第5步,主进程在文件头中设置元数据迁移成功标志.数据迁入时,第1步,主进程读入文件头,判断元数据的完整性;第2步,各进程读入二元组列表;第3步,各进程根据二元组列表并行进程创建MPI数据类型,在文件中切分存储空间;第4步,通过系统调用与MPI_File_read_all并行迁入元数据.如图7所示.HDF5的数据I/O带宽.6.1测试环境测试的硬件环境是一个部署有lustre并行文件系统的MPP系统.该系统有3880个节点.每个节点采用2个IntelXeonX5670六核处理器,主频2.93GHz,配置48GB内存,总计46560个处理器核与181.9TB内存.系统内并行文件系统配置96台服务器,每个服务器采用2个IntelXeonE5640四核处理器,主频2.67GHz,配置12GB内存和两个16TB的RAID6盘阵,总计3PB存储资源.客户端与服务器之间采用QDR的infiniband互连.在较早的并行文件系统性能测试中,它的单进程持续带宽Page7约为450MB/s左右,多进程并行I/O峰值带宽超过60GB/s.6.2单节点性能测试与分析我们设计并实现的节点内测试程序maskio由负载计时程序和掩模文件组成.负载计时程序具有DCPL、HDF5和MPI-I/O三种读写接口.与DCPL与HDF5相比,MPI-I/O只有基础的I/O功能,因此DCPL与HDF5输出块分割的三维数据场作为负载;MPI-I/O仅依次输出场中的数据块,不输出元数据.真实的应用程序的I/O特征往往没有规律可循.我们使用掩模文件记录每一次读写操作的相对位置.掩模文件产生于真实的应用,它记录应用程序每一次读写操作发生位置.输出时,maskio按参数指定大小和数量顺序输出.输入时,maskio按照掩模文件指示的位置读入数据.6.2.1单节点性能测试设定数据块总量为5000,大小均匀,测试长度从4KB扩展到16MB的多种情况.由5000个数据块构成的数据场中,包括名称、属性与索引在内的元数据总量为1.8MB.在数据块从4KB增长到16MB的过程中,相应的数据场存储规模从20MB增长到80GB.DCPL、HDF5和MPI-I/O三者写性能对比如图8(a)所示,读性能对比如图8(b)所示.如图8(a)所示,输出过程中,与HDF5相比,节点内的DCPL平均带宽提高13%.相比于MPI-I/O,由于没有任何元数据开销,平均带宽是HDF5的2.15倍.图8(b)中,与HDF5相比,单进程DCPL的读带宽平均提高到3.51倍.16KB时最高,可达7.11倍;1MB与4MB的加速比分别为2.72和1.76倍.这说明在较广泛的数据区间内,双路I/O优化技术对读操作有良好的加速效果.6.2.2单节点性能分析DCPL的读写时间由数据、元数据读写时间和元数据迁入迁出时间组成.5000个数据块的数据块名称、类型、说明与属性等元数据量为1.8MB.在lustre缓存的影响下,这些数据从tmpfs迁出到lustre并行文件系统的时间为0.003s左右.Lustre缓存不命中时,从并行文件系统迁入到tmpfs的时间为0.07s左右.将迁入迁出时间算入DCPL的元数据开销,比较DCPL、HDF5和MPI-I/O的数据、元数据输出时间如图9(a)所示,读入时间如图9(b)所示.在图9(a)中,文件系统缓存屏蔽了元数据层次存储的效果.仅分析元数据输出开销时,DCPL与HDF5相近.然而,元数据层次存储后,数据存储布局连续性有提高,所以DCPL的数据输出时间仍然少于HDF5.图9(b)说明了HDF5读入性能低的主要原因.HDF5的元数据读入开销随着数据块大小的增长而增加,DCPL的元数据读入开销相对稳定.两者平均Page8有7.5倍差距,最大可达12.69倍.这是DCPL读性能好于HDF5的主要原因.此外,DCPL数据存储连续性较好,这是DCPL读入性能好于HDF5的次要原因.7并行应用测试在上一节说明的测试环境中,我们使用Lared-P程序与TeraVAP程序测试双路并行I/O与HDF5的加速比.我们设定Lared-P程序数据场大小为20483,单时刻重启动数据规模386GB.使用元数据分组并行I/O模式,6个进程对应一个元数据文件,测试实际应用可获得的并行I/O性能.3072个进程到24576个进程的性能对比如图10所示.图10双路并行I/O与HDF5的重启动I/O带宽比较如图10,双路并行I/O(DCPL)的平均加速效果,写操作平均提高8%,读操作平均提高89%.12288进程时读加速比最高,提高116%.该测试说明,双路并行I/O方法有较好的加速效果,尤其是数据的读入操作,最高可提高1倍以上.分析读、写加速效果不平衡的原因.Lared-P数据场为一次性的写出与读入.写出时,文件系统客户端缓存屏蔽了元数据迁移的效果,因此加速比受到限制;读入时,缓存不易命中,元数据迁移有明显的加速效果,因此有较高的加速比.TeraVAP软件是中国工程物理研究院高性能数值模拟软件中心研制的数据并行可视化工具,它支持HDF5、NetCDF等多种格式数据的并行可视化.使用TeraVAP绘制一帧图片需要经历数据初选、读入、渲染、绘制等过程.我们将DCPL格式移植到TeraVAP,与HDF5对比测试.测试数据集源自于基于JASMIN框架的JEMS-FDTD并行程序.JEMS-FDTD是一个使用Maxwell方程求解电磁场反射强度的程序.设定数据场大小为10243,单时刻数据规模56GB.使用32~512进程并行伪彩色绘制电场强度,数据I/O时间与可视化时间对比如图11所示.图11测试中使用了TeraVAP自带的计时器,其中I/O时间包括了数据初选与读入时间之和.DCPL平均缩短TeraVAP的I/O时间2.01倍,对总绘图时间也有71%的加速效果.8结束语高层I/O库是科学计算数据集合管理工具,是许多大型科学计算应用存取数据的基础.本文根据数据、元数据I/O特征差异,在科学计算应用的并行I/O过程中构造内存文件系统和并行文件系统两层存储,迁移规模较小,存取频次较高的元数据文件到内存文件系统.降低元数据I/O延迟,优化数据、元数据存储布局,提高并行I/O效率.在元数据并行迁移过程中,本文使用集合I/O为元数据协同分配存储空间,并行读写,提高迁移效率.测试表明,双路并行I/O方法有较好的加速效果,可提高万核应用程序的并行写带宽8%~13%,并行读带宽89%到1.01倍.在具有较复杂模型的大规模并行计算应用中,数据、元数据无序读写是一种比较普遍的现象.双路并行I/O可以提高这些应用程序的并行读写带宽,提高应用的执行速度,因此具有较广泛的应用前景.
