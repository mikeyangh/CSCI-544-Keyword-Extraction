Page1面向GPU异构并行系统的多任务流编程模型董小社1)刘超1)王恩东2)刘袁1)张兴军1)1)(西安交通大学计算机科学与技术系西安710049)2)(高效能服务器和存储技术国家重点实验室济南250013)摘要传统并行编程模型和框架不能有效利用和发挥GPU异构并行系统特点,应用开发难度大,性能优化困难,文中采用混合编程模型思想,建立了一种以协处理器为中心的GPU计算核心与CPU控制相融合的多任务流编程模型.模型将并行任务与CUDA流相结合,利用系统硬件并行性特点实现程序任务级和数据级并行;采用任务间消息通信和任务内数据共享通信方式,既保证对传统并行应用的继承又降低了不同存储空间给应用开发带来的复杂性和难度.基于该编程模型实现了一个运行时支持系统原型,测试结果表明可保证高效的数据通信,且能充分利用系统计算能力,提高了应用程序运行效率.关键词GPU;异构并行;编程模型1引言近年来,GPU凭借其强大的浮点计算能力在高性能计算领域得到了广泛应用,其并行数据处理能力已远远超过传统CPU的计算能力,以“通用主处理器+加速协处理器”为特征的新型异构并行结构也成为高性能计算机系统发展的一个新方向.2010年11月世界超级计算机Top500中排名第一的“天河一号(TH-1A)”超级计算机便采用了这种异构结构[1].GPU众核处理器为高性能计算机系统的发展带来了契机,但同时在软件及应用程序开发上也面临着更多的挑战和困难[2].传统并行编程模型主要基于同构CPU构成的并行系统,不能有效利用和发挥异构并行系统中GPU的加速计算能力.由于GPU结构的特殊性,使用GPU运行的程序需要按照专用的编程模型或框架进行开发,如CUDA和OpenCL等,并且GPU显存与系统主存间显式的数据交互特点要求应用程序开发者对数据进行合理的划分和管理,增加了新系统下应用开发和移植的难度.为了在有效利用GPU异构并行系统计算性能优势的同时,提高应用开发移植效率,本文建立了一种面向GPU异构并行系统的多任务流编程模型.该模型采用混合编程模型思想,将传统的消息传递模型与GPU专用编程模型相结合,通过GPU对应用计算核心进行加速,减少程序运行时间.同时,提供单个任务流内CPU-GPU数据共享机制,隐藏数据传输,降低应用开发难度.本文第2节介绍相关领域的研究工作;第3节介绍编程模型总体设计结构;第4节介绍编程模型的运行时支持系统;第5节对所实现的原型系统进行测试评价;最后在第6节进行总结.2相关工作随着GPU在通用计算领域的不断发展,许多研究组织和个人针对GPU异构并行系统的编程开发与性能优化进行了深入的研究.Leung等人[3]通过修改R-Stream编译器,将高级语言程序自动转化成CUDA程序.多伦多大学的研究人员提出了一种以制导语句为基础的编程语言hiCUDA[4],通过在串行程序中加入制导语句形成hiCUDA源程序,然后通过源到源编译器将源程序转为CUDA程序.Chen等人[5]通过对UPC语言进行扩展,将并行执行的循环体使用GPU加速执行.Harvey等人[6]设计开发的Swan和Martinez等人[7]实现的CU2CL则将CUDA程序转换为OpenCL程序,从而支持不同架构的GPU以及多核CPU或DSP等多种加速处理单元.类似的还有OpenMPC[8]、OpenMPtoGPGPU[9]等,这些研究工作都通过建立新的编程语言或是扩展已有的编程语言,使用源到源编译技术对源程序进行转换,自动生成可运行于GPU的程序.采用这种方法,开发者不必掌握GPU专用编程语言,降低了程序开发难度,但自动转换方式的有效性难以保证,而且通常还需要使用者对GPU结构有一定了解,从而进行制导语句的标注.对于一些复杂应用,转换所生成程序的性能也不是很高.另一方面研究采用混合编程模型的思想,将已有的通用编程模型与GPU专用编程模型相结合,进行程序开发.Wang等人[10]介绍了UPC/CUDA混合编程模型,并对FT和MG算法进行了实现和优化.FLAT[11]编程框架将MPI通信操作写在GPU运行的kernel程序中,通过运行时系统将kernel中的通信操作转化为CPU上执行的操作完成消息通信.Barak等人[12]开发的MGP(ManyGPUsPackage)通过在上层扩展OpenMP,在下层建立支持OpenCL的运行时系统实现多CPU/GPU环境下的应用程序开发.HybridOpenCL[13]建立了一个与MPI/OpenCL类似的程序开发和运行系统,方便OpenCL应用程序向集群环境移植.cudaMPI[14]将MPI通信接口与CUDA数据传输进行封装,形成统一的用户编程接口,方便用户进行应用开发.MVAPICH2-GPU[15]将涉及显存的数据传输集成到MPI发送接收操作中,在程序运行时自动检测消息数据的位置并进行操作,简化了用户编程,但其实现依赖于CUDA提供的UVA(UnifiedVirtualAddress)支持,且运行时对地址指针的自动检测过程会给消息传输操作带来较多的额外开销.MPI-ACC[16]则通过增加新的MPI_Data_Attributes属性类型来指示消息数据的位置,避免了自动检测带来的开销,在数据传输中使用多数据块和流水线传输等优化手段降低延迟,并且支持CUDA和OpenCL等不同GPU编程模型.但这些混合编程模型中GPU访问的数据仍然需要用户通过显式数据拷贝操作来完成,增加了开发负担,且容易造成冗余的数据拷贝.MPI/CUDA混合编程模型也是目前GPU异构并行系统下使用最为广泛的一种编程方法.传统的MPI/CUDA混合编程模型还存在一些不足:(1)没有考虑GPU结构的发展对任务并行性的支持,如NVIDIAFermi架构GPU.传统的混合Page3编程模型中多个任务占用一个GPU时各自拥有独立的GPU上下文,抢占使用GPU,切换开销大且无法利用单个GPU并行执行计算核心;(2)数据通信没有考虑GPU显存,涉及显存的数据通信需要用户显式完成,增加程序开发难度且存在冗余的数据传输过程.基于以上分析,本文建立的多任务流编程模型采用消息传递与CUDA相结合的混合编程方式,将并行任务以线程方式实现并与GPU运行的CUDA流相结合形成任务流,建立单一GPU上下文降低开销,并且可以利用Fermi架构GPU并发执行kernel的特性①.同时结合显存实现任务流间消息传递通信和任务流内数据共享通信,提高通信效率,隐藏主存/显存间显式传输,降低编程开发难度.3编程模型总体设计3.1多任务流模型结构GPU异构并行系统结构具有以下特点:(1)硬件的多级并行性.不同处理器之间的并行以及单个处理器内部的多个处理单元的并行;(2)共享式存储与分布式存储相结合.每个GPU拥有可直接访问的独立显存,同时GPU与CPU共享系统主存;(3)GPU提供主要计算能力.系统通过GPU加速计算,降低程序运行时间.针对GPU异构并行系统结构特点,本文在逻辑抽象基础上以协处理器为中心,采用混合编程模型思想,建立GPU计算核心与CPU控制相融合的多任务流编程模型,该模型总体结构如图1所示.由于GPU没有自主控制能力,基于GPU系统的程序由CPU运行的逻辑控制部分和GPU运行的计算核心部分共同构成.因此,本文将CPU和GPU协作完成且按照程序书写顺序串行执行的一段程序指令集称为一个任务流(Task-stream).在多任务流模型中,并行应用程序由多个任务流组成,每个任务流包括CPU运行的主控程序和GPU运行的计算kernel.主控程序按照传统的消息传递编程模型进行开发,实现处理器间的任务级并行,计算kernel按照CUDA编程模型进行开发,实现GPU内众多处理单元并行计算的数据级并行.整个应用程序表现出任务级与数据级两层并行特征,充分利用和发挥硬件多级并行性的优势.每个任务流控制使用的GPU采用逻辑GPU的概念,多个逻辑GPU在物理上可以是同一个,如图1中Task-stream0、Task-stream1和Task-stream2都使用GPU0.3.2数据通信方式多任务流模型中数据通信包括两方面:并行任务间通信和单个任务内主控程序与计算kernel间通信.由于GPU异构并行系统具有分布式存储与共享存储相结合的特点,因此数据通信采用消息传递和数据共享相结合的通信方式,如图2所示.并行任务间使用消息传递通信方式,并且与传统的消息通信接口MPI保持一致,方便用户掌握和使用,也保证了所开发应用具有良好的可扩展性以及对已有并行应用的继承.单个任务流内提供数据共享的通信机制,使得主控程序与计算kernel可以通过对共享数据的访问隐藏主存/显存间显式的数据传输,降低不同存储给编程开发带来的复杂性,减轻应用开发负担.4编程模型运行时系统4.1运行时系统软件结构本文基于NVIDIAGPU平台设计实现了多①FermiArchitectureWhitepaper.http://www.nvidia.com/Page4任务流编程模型运行时支持系统GMMP(GPUMemorycombinedMessagePassing).该系统将MPI作为下层多节点并行和通信环境,节点内并行通过Pthread以线程方式实现,再结合GPU运行所需的CUDA软件环境,其软件结构如图3所示.GMMP系统主要包括任务流管理和任务流通应用程序通过GMMP在每个计算节点上创建一个MPI进程———mpi_process,在每个MPI进程中再创建若干线程———Pthread.线程执行任务流的主控程序,控制使用一个GPU,并与特定的CUDA流关联,任务流的计算kernel将属于所关联的CUDA流.一个计算节点上的所有任务流属于同一个MPI进程,称为任务流集———task-stream-set.因此,GMMP中Pthread与Task-stream对应,mpi_process与task-stream-set对应.信,任务流通信又包括消息通信和数据共享.4.2并行任务流管理并行任务管理最基本的工作包括3个方面:并行任务的创建、结束和同步.GMMP多任务流采用了进程、线程、流相结合的方式,因此并行任务管理也呈现出mpi_process、Pthread及cuda_stream相结合的多层特点.GMMP并行任务流管理总体实现方式如图4所示,通过MPI、Pthread及CUDA等下层软件环境提供的接口分别完成进程、线程以及流的创建、结束和同步操作.多线程同步没有直接的同步调用,本文通过线程条件变量进行实现.线程与CUDA流的关联则是通过线程私有变量来完成.GMMP中每个任务流拥有一个ID,称为gmmp_rank,gmmp_rank由任务流所属MPI进程的mpi_rank和该任务流在进程中的序号local_thread_rank决定.假设一个MPI进程中有N个线程,即N个任务流,则gmmp_rank=mpi_rank×N+local_thread_rank.4.3消息通信GMMP消息通信包括节点间(inter-node)通信和节点内(intra-node)通信.节点间通信发生在不同的MPI进程间,借助下层MPI消息通信操作完成.节点内消息通信,本文通过在系统主存中建立消息缓冲区完成.GMMP消息通信实现如图5所示.MPI环境提供了统一、高效的消息传递接口,节点间通信使用MPI作为下层通信环境,不但降低了GMMP实现的难度且能有效利用软件资源保证通信的高效完成.节点内通信,发送方和接收方通过共享的消息缓冲来完成消息传递操作,避免了数据在不同缓冲间的冗余传输,降低了通信延迟;GMMP使用页锁定内存(pinned-memory)作为消息缓冲区,进一步降低主存/显存间的通信延迟.随着GPU软硬件技术的发展,系统可针对多个GPU提供P2P式传输,即不同的GPU间通过系Page5统总线或是互连网络直接进行数据传输,如图4中虚线箭头所示,不再通过主存间接完成,GPU间的通信速度将获得更大提高.4.4数据共享根据CPU和GPU分别只能直接访问主存和显存的特点,本文采用主存-显存存储映射方法实现任务流内CPU和GPU对数据的共享访问,如图6所示.共享空间由主存/显存间相互映射的一对空间构成,系统维持映射空间数据的一致,保证CPU与GPU分别访问相应的数据时程序行为的正确性.GMMP针对共享空间建立了映射表,记录映射空间的实际地址及状态信息.共享空间数据的一致性维护是GMMP要解决的一个关键问题,在主存/显存映射方式中,共享数据存在如下几种状态:(1)IN_HOST//最新数据在主存空间中(2)IN_DEVICE//最新数据在显存空间中(3)IN_ALL//主存、显存中数据一致本文建立了3种映射空间的数据更新策略.4.4.1简单一致性更新基于程序中计算kernel的执行,应用程序在执行计算kernel前进行p_host→p_device的数据更新,计算kernel完成后进行p_device→p_host的数据更新,从而保证CPU或GPU在访问共享空间时访问到最新数据,其状态转换如图7所示.这种更新策略操作简单,不需要额外信息,但由于无论CPU或GPU是否对共享数据进行了修改都要进行数据更新,造成重复的更新操作,效率不高.因此,按照计算kernel对共享空间读写属性,对简单一致性更新策略进行改进:(1)kernel执行前只对读取的共享空间更新;(2)kernel结束后只对写过的共享空间更新.这样可以减少数据重复更新的次数,降低更新开销.实现时用户可使用制导语句标明计算kernel对共享数据的读写属性或通过编译自动识别读写属性.4.4.2读写一致性更新基于程序对共享空间的读写访问属性,按照不同的读写操作进行状态转换和数据更新.应用程序对共享空间的访问包括CPU读/写及GPU读/写,4种访问操作下的状态转换如图8所示.读写一致性更新中,共享空间存在3种状态.当访问操作不改变共享空间的状态时,不引发数据更新操作;写操作可能改变状态,但不引发数据更新;读操作在改变状态时引发数据更新.这种读访问更新数据的方式保证程序访问最近最新的数据.该策略区分程序对共享空间的读写操作,避免了重复的数据更新,但实现复杂,需要提供较多的读写访问信息辅助完成.4.4.3异步一致性更新CUDA环境提供了主存-显存间的异步传输功能,采用异步传输可以重叠数据通信和其它程序操作,隐藏主存-显存间的传输延迟.异步更新策略在读写一致性更新策略的基础上引入异步传输特性,如图9所示.Page6与读写一致性更新策略中读操作才引发数据更新不同,此时写操作引发异步数据更新,而相应的读操作则检查和等待异步传输完成.两个操作之间程序可以执行其他指令,隐藏数据通信延时.5验证与测试5.1实验环境测试环境包括5个计算节点,通过Infiniband连接,每个节点有2个4核IntelXeon5660CPU、4GB内存和2个NVIDIATeslaC2050GPU.操作系统为RedHatLinux5企业版,安装CUDAToolkit、CUDASDK4.0和openMPI1.3.3.5.2通信测试使用ping-pong测试程序对节点内和节点间点到点通信延时进行测试,并与使用MPI/CUDA混合编程模型方式的测试结果进行比较.图10(a)和(b)分别显示了小消息和大消息的节点内通信延时,pinned表示在GMMP中使用页锁定内存作为消息缓冲.GMMP小消息通信延时要稍大于MPI/CUDA方式,这主要是消息缓冲的加锁/解锁操作带来的开销.随着通信数据量的增大,GMMP减少了数据传输过程,额外开销影响减小,延迟好于MPI/CUDA方式.图11是节点间通信的延迟,由于GMMP节点间通信借助下层的MPI接口实现,通信过程与MPI/CUDA方式相同,两者通信延迟也基本相同.图12是5个节点,10个GPU的集合通信操作-广播通信延时,GMMP广播延时与MPI/CUDA基本相同,保持了较高的通信性能.5.3多任务流应用测试5.3.1单GPU多任务测试Fermi架构GPU支持相同GPU上下文的多个计算kernel并发执行,这里参照和修改CUDASDK中concurrent_kernel测试程序,分别在前代GPU产品TeslaC1060和Fermi架构GPU产品TeslaC2050上运行,多个并行任务占用一个GPU,运行时间如图13所示.C1060GPU不支持并行kernel执行,随着任务数增加,多个任务的计算kernel在一个GPU上串行执行,运行总时间也逐渐增多.在C2050上MPI/CUDA是多GPU上下文,多个kernel仍然串行执行,而GMMP中不同任务流的kernel属于同一个GPU上下文的不同CUDA流,可以并行执行,因此相比MPI/CUDA减少了运行时间.Page7图13单GPU多任务concurrent_kernel程序运行时间5.3.2更新策略测试使用分块矩阵相乘程序(BlockMatrixMultiply,BMM)对不同一致性更新策略进行测试,矩阵规模为8192×8192单精度浮点数,运行于多个节点,每个任务单独使用一个GPU,程序运行时间如图14所示.简单一致性更新存在较多的重复数据更新过程,程序运行效率最差;改进后的简单一致性更新减少了重复更新次数,运行时间有了较大降低.读写一致性更新避免了重复更新,程序性能有了进一步提高,与MPI/CUDA方式基本相同;异步一致性更新通过异步传输隐藏部分通信延时,性能较读写一致性更新有改善.5.3.3N-body应用测试N-body问题描述了具有相互引力作用的N个粒子运行轨迹的计算过程,广泛应用于物理学、静电学和旋涡流体动力学等领域中.N-body问题最直接的算法是PP(particle-particle)算法.本文采用PP算法,并且使用读写一致性更新方式对N=65536,迭代500步的N-body问题进行测试,运行时间如表1所示.并行任务数表1中,mpi/cuda和gmmp均使用GPU加速计算,最多创建10个并行任务分别占用5个节点的10个GPU.随着任务数增多,程序运行时间下降,但并行加速比减少,这主要是由于每个任务负责的计算量已经很小,GPU控制与通信等开销的相对比例增加.因此在使用GPU异构并行系统运行程序时,并行任务的数量要使得每个GPU分配有足够的计算量,从而保证对GPU计算资源的充分利用.我们也对只使用CPU运行的mpi程序进行了测试,使用5个节点40个CPU核运行40个并行任务的运行时间为3112.17s.gmmp使用10个GPU的运行时间相比于MPI使用40个CPU核加速比提升达到近70倍.6结论本文针对GPU异构并行系统,建立了一种多任务流编程模型,设计实现了编程模型运行时支持系统原型GMMP.该模型采用消息传递+CUDA的混合编程模型思想,利用多级硬件并行性特点实现应用程序的任务级和数据级并行.模型采用任务间消息传递与任务内数据共享的通信方式,既保证了并行应用的高效实现,方便应用开发移植,又降低了程序开发负担.对运行时支持系统GMMP的测试表明,GMMP保持了较好的通信性能,可以更好利用GPU新特性,充分使用GPU提供的计算能力Page8对程序计算核心进行加速,提高应用整体运行效率和性能.
