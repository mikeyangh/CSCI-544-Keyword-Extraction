Page1特征加权距离与软子空间学习相结合的文本聚类新方法王骏王士同邓赵红(江南大学数字媒体学院江苏无锡214122)摘要文本数据维数高、数据分布稀疏、不同类别的特征相互重叠,这为聚类分析提出了挑战.针对文本数据的这一特点,将特征加权技术与软子空间相结合,基于模糊聚类的算法框架,提出了一种适用于高维文本数据的软子空间模糊聚类新方法.首先,基于加权范数理论,提出了新的特征加权距离计算方法.接着,将其与软子空间学习的理论框架相结合,提出了面向模糊聚类的新的目标学习准则.通过向约束条件中引入熵指数r,从而扩展了模糊指数m的取值范围,并给出了物理解释.基于Zangwill收敛定理对算法的全局收敛性给出理论证明.实验表明,文中算法可以使软子空间学习和聚类分析同时进行,其性能比现有的相关算法有了较大的提高.关键词模糊聚类;文本聚类;软子空间;特征加权距离;全局收敛性1引言在对文本数据进行聚类分析时,通常需要选择聚类分析[1]是实现文本挖掘的重要手段.但是,Page2合适的距离度量来评价样本点之间的相似或相异程度.为了合理计算高维文本数据之间的距离,在聚类过程中引入特征权重来强化重要特征的积极作用,削减冗余特征的不利影响,已经成为一种常用的方法[2-8].例如,参考文献[4-5]分别提出了两种基于特征加权的模糊聚类方法.它们的共同点在于,在聚类之前首先使用一个有监督或无监督的学习过程得到能够反映数据集内部结构的特征权重向量.在此基础上,形成特征加权的距离函数.然后基于FCM算法框架得到数据集的模糊划分.特征权重的学习过程揭示了数据集里各个类别的结构特征,这有利于后继的聚类过程.但是,在聚类过程中特征权重不再发生变化.因此,聚类过程受先前学习结果的制约.与上述两种方法不同,Huang等人[6-7]提出了自动特征(变量)加权技术.通过向k-means或FCM中引入特征权重向量来表示整个数据集上各特征的重要程度.通过循环迭代过程,在聚类分析的同时实现了特征权重的无监督学习,这成为处理高维数据的另一种代表性方法.上述方法的共同点在于,算法在整个特征空间中对特征进行加权,各个类别使用相同的特征权重向量.现实生活中,由于不同主题的文本,其关键词各不相同,每个关键词描述不同主题的能力也有很大差异,因此进行特征权重学习时,整个数据集采用单一的特征权重向量无法与数据集的结构特点相吻合.近年来,软子空间聚类技术[9-11]成为学术界的研究热点.其基本思想是,给数据集中的各类别赋予不同的特征权重向量,用来表示聚类过程中各特征对此类别贡献的大小.在聚类过程中,每一维特征对各个类别都有不同的贡献,因此每一类都有不同的特征权重向量,从而在整个特征空间中形成了若干个“软子空间”[9],聚类过程就是在各个“软子空间”中进行的.Gan等人[10]提出的模糊子空间聚类算法FSC、Jing等人[9]提出的熵加权k均值聚类算法EWKM、Chen等人[11]提出的特征组k均值聚类算法FG-k-means就是其典型代表.令人遗憾的是,现有的软子空间聚类算法大多建立在数据集硬划分的基础上.以FSC为例,它具有以下缺点:首先,聚类结果对初始点敏感,如果初始点集选择不合适,会出现若干聚类合并的现象.其原因在于,FSC本质上采用硬划分方法,这与k-means是一样的.因此也不可避免地继承了k-means的缺点.其次,数据集的维数过高时,目标函数过早收敛,从而影响了算法的性能.根据高维本文数据集的结构特点,本文将特征加权距离、软子空间技术和模糊聚类理论相结合,提出了一种新的熵指数软子空间模糊聚类算法.其特点在于,在完成聚类任务的同时,算法可以自动进行特征权重的无监督学习,这对文本挖掘等应用领域有着重要的意义.从形式上看,加入新的模糊特征权重矩阵犠,使目标函数中出现了两个模糊矩阵,这是目前的研究不曾涉及问题.在约束条件中,模糊隶属度的幂指数从1推广到了正实数,模糊指数m的取值范围也被推广到m>0.本文第2节回顾FCM算法;第3节基于加权范数理论,提出特征加权距离的定义;将其与软子空间技术结合,同时向约束条件中引入熵指数r,提出本文的新算法,讨论参数的选取,并从信息论的角度给出相应的物理解释;第4节应用Zangwill全局收敛定理,进一步证明本文算法具有全局收敛性;第5节给出实验分析;最后是结论.2模糊C均值算法FCM设X={狓1,狓2,…,狓n}为欧氏空间犚d中的一个有限数据集,c为一整数,满足2c<n.用Rcn表示所有的实c×n的矩阵集合.令Mfcn表示数据集X上划分矩阵的集合,犝=[uji]∈Rcn是模糊划分矩阵,其每一行元素定义了X中的一个模糊聚类,uji表示第i个样本隶属于聚类j的程度,满足如下条件:0uji1,j=1,2,…,c;i=1,2,…,n(1a)∑c0<∑n犞=(狏1,狏2,…,狏c)T∈Rcd表示聚类中心,‖·‖为犚d上的内积范数,m为模糊指标,Mfcn为满足式(1)的所有模糊矩阵犝的集合.FCM的目标函数JFCM:Mfcn×Rcd→犚定义为j=1根据式(1)、(2),可得JFCM取局部极小值的必要条件如下:使用式(3a)、(3b)进行Picard迭代,使目标函Page3数收敛于局部极小点或鞍点,从而得到X的一个模糊划分矩阵犝.从数学的角度来看FCM,可以把FCM看作在式(1)的约束条件下,求解模糊划分矩阵犝,使式(2)的目标函数取极小值.这是一个非线性规划问题.对FCM收敛性的研究也是研究的难点之一.几经反复,Hathaway、Bezdek等人[12]终于证明了算法的目标函数最终收敛于其局部最小值点或鞍点.3熵指数软子空间模糊聚类算法EI-FWD-SSFC3.1基于加权范数理论的特征加权距离在数学上,一个二元实值函数dist(x,y)作为拓扑空间中的距离,应该满足以下3个条件:(i)非负性.dist(x,y)>0,x≠y,dist(x,x)=0;(ii)对称性.dist(x,y)=dist(y,x);(iii)三角不等式.dist(x,y)dist(x,z)+本文的特征加权距离建立在加权范数理论的基dist(z,y),z.础上,首先介绍加权范数[13]的定义.犚d规定定义1.设犃∈犚d×d是正定矩阵,对任意狓∈定理1.设狓=(x1,x2,…,xd)T,狔=(y1,则‖狓‖犃是一种向量范数,称为加权范数.y2,…,yd)T,w1,w2,…,wd为正实数,满足∑d1,0则wdist(狓,狔)=∑d离,其中α>1.等式成立.证明.非负性和对称性易证,下面证明三角不设狓,狔,狕∈犚d,根据以上加权范数的定义,有其中犃=diag(wα对角元的对角矩阵.显然犃正定.根据向量范数的相关性质[13],有即wdist(狓,狔)wdist(狓,狕)+wdist(狕,狔)成立.定理得证.3.2算法描述对FCM的目标函数(2),本文作以下改进.首先,用本文提出的特征加权距离来改进原FCM中的距离计算公式,并添加适当的正则化项.由于不同类别所处的软子空间使用不同的特征权重向量,因此形成了模糊特征权重矩阵犠.其次,向约束条件中加入熵指数r,使算法更灵活通用.设X={狓1,狓2,…,狓n}为欧氏空间犚d中的有限数据集.c为一整数,满足2c<n.犞=(狏1,狏2,…,狏c)T∈Rcd为c个聚类中心.实数m、r、α满足m>r>0,α>1.εw为任意正实数,εu为任意实数.犝为模糊划分矩阵,满足如下条件:0uji1,j=1,2,…,c;i=1,2,…,n(7a)∑c0<∑nM犝为满足以上条件的模糊划分矩阵犝的集合.犠=[wjh]c×d为模糊特征权重矩阵,满足如下条件:0wjh1,j=1,2,…,c;h=1,2,…,d(8a)∑dM犠为满足以上条件的模糊特征权重矩阵的集合.将特征加权距离与软子空间相结合,构造一般化的目标函数如下:Jm(犠,犞,犝)=∑cj=1h=1h=1其中,dji=wdist2(狓i,狏j)=∑d义域Df={(犠,犞,犝)|犠∈M犠,犞∈Rcd,犝∈M犝}.(这里f∑c∑cj=1∑dwα运算.即(f1∑cj=1∑d和(f2∑cj=1∑d本文中,我们仅研究以f1∑c正则化项的情况.即h=1Page4Jm(犠,犞,犝)=∑c对于以f2∑c况,我们将另文发表.下目标函数(9b)取极值的必要条件如下:通过Lagrange乘子法,得到在约束条件(7)、(8)狏j=∑nwjh=uji=∑c与FCM类似,EI-FWD-SSFC通过以上3个必要条件之间的Picard迭代,使目标函数(9b)收敛,从而找到数据集犡的一个模糊划分.EI-FWD-SSFC算法描述如下.算法1.EI-FWD-SSFC.输入:D为数据集;c为聚类数目输出:犝为模糊划分矩阵;犠为特征权重矩阵;步骤:1.随机选择c个样本点初始化聚类中心矩阵犞(0);2.通过wjh=1/d(j=1,2,…,c,h=1,2,…,d),初始3.通过式(10c)计算uji,初始化犝(0);4.根据式(9b)计算obj_fcn(0)=Jm(犠(0),犞(0),犝(0));5.设置目标函数变化阈值eps,最大迭代次数max_iter,6.while(k<=max_iter)7.根据式(10a)更新犞(k);8.根据式(10b)更新犠(k);9.根据式(10c)更新犝(k);10.根据式(9b)计算obj_fcn(k)=Jm(犠(k),犣(k),犝(k));11.error=|obj_fcn(k)-obj_fcn(k-1)|;12.if(error<eps)break;13.k=k+1;14.end15.输出犠,犞和犝.算法首先通过随机选择c个数据点初始化犞,wjh=1/d初始化犠,通过式(10c)来初始化模糊划分矩阵犝.随后算法通过Picard迭代,依次计算中心点犞、模糊特征权重矩阵犠以及模糊划分矩阵犝,直到收敛.使用式(10c)重新计算模糊隶属度uji,实际上就是根据样本点狓i到各类别中心点狏j的特征加权距离,重新确定各样本点从属于每个类别的模糊度.使用式(10b)更新模糊特征权重矩阵犠即为对特征权重进行无监督学习的过程,从而得到更合理的特征权重分布.在计算wjh时考虑了当前迭代中每个样本点从属于每一个聚类的模糊程度,以及各个样本点到各个聚类中心的距离.3.3ε狑和ε狌参数的选择对于高维稀疏数据来说,可能会出现某一维特征上的数据全为0的情况,这时式(10b)中∑nji(xih-vjh)2为0;此外,当数据集某一类中某um一维特征上的数据全为0时,∑n能接近0.参数εw的引入避免了式(10b)中分母为0.特别的,当εw→时,根据式(10b),wjh→1/d,此时算法退化为FCM.i=1不妨设当h=h0时,∑n1,2,…,c.根据式(10b),此时有对于任意l=1,2,…,d,0∑n∑n(xil-vjl)2成立,因此有1/dwjh0i=1即参数εw对wjh0的上限起着重要的调节作用.显然,其上限与数据集的结构特点有关.聚类过程中,当聚类中心狏j与某一样本点狓i过于接近而几乎重合时,它们之间的加权距离dji为0,εu的引入也同样避免了分母为0,通常取接近于0的任意正实数.当εu取负值时,式(9b)可视为采用Page5i=1了ε不敏感距离.当εu→时,根据式(10c),uji→c-1/r,此时算法将迅速收敛到整个测试集的中心狏0=∑nn.3.4模糊指数犿和熵指数狉的大小关系与FCM类似,实数m是模糊划分矩阵犝的模糊指数.约束条件中熵指数r的引入使m的范围由FCM中的m>1扩展为m>r>0.当m法退化为FSC;当m收敛到整个测试集的中心;当m>0,r>0且m<r时,由于不能保证目标函数(9)是犝上的凸函数,算法的收敛性无法保证.实数α是模糊特征权重矩阵犠的模糊指数.当α→1+时,软子空间将演变为硬子空间;当α→时,wjh→1/d,即各维特征的权重相等,此时算法退化为FCM.针对FCM模糊指标m的选取问题,于剑从理论的角度给出了选取准则[14-15].对于软子空间聚类算法,由于新增加了计算特征权重矩阵犠这一步骤,使得问题更加复杂.本文根据多个测试集的实验,当满足1.3m法可以取得比较理想的性能.当然,更深入的理论方法还有待进一步挖掘.3.5对约束条件的解释在信息论中,经常使用熵来描述随机变量的不确定性程度.根据信息论的相关概念,数据集样本点狓i的Havrda-Charvat熵定义为[16]β(狓i)=1H(r)显然,如果把矩阵犝看作概率矩阵,当满足约束条件(7b)时,H(r)中样本点狓i从属于各个划分的不确定性最小.故EI-FWD-SSFC算法的实质就是在数据集X中关于各个样本点的模糊隶属度uji的Havrda-Charvat熵具有最小值的情况下,求解目标函数(9b)的最小值.此时数据集X中各样本点所具有的Havrda-Charvat熵总量亦最小.4EI-FWD-SSFC的收敛性参考文献[10,12]中分别证明了FSC和FCM的收敛性.但是引入模糊特征权重矩阵犠后,目标函数中出现了两个模糊矩阵.此外,向约束条件中引入指数r后,问题发生的变化,因此有必要重新证明算法的收敛性.4.1Zangwill收敛定理要判断一个迭代算法是否全局收敛,只要判断其是否符合Zangwill收敛定理的3个条件即可.首先介绍Zangwill收敛定理如下.引理1(Zangwill收敛定理).假设点狕(0)∈V,由点-集映射A:V→P(V)定义的迭代算法产生迭代序列{狕(k)},解集ΩV.如果:(3)若狕Ω,映射A在狕处是闭的.(1){狕(k)}ΓV,其中Γ是紧集.(2)存在连续函数J:V→犚,满足:(a)若狕Ω,则对于任意狔∈A(狕),J(狔)<J(狕);(b)若狕∈Ω,算法终止,或对于任意狔∈A(狕),J(狔)J(狕).则算法终止于解集Ω,或{狕(k)}的任一收敛子序列的极限是解集Ω的一个点.通常把满足上述引理中条件(2)的函数J称为是A在V上的Zangwill函数[17-18].4.2EI-FWD-SSFC全局收敛性证明在证明之前,还需引入几个定义和定理.定义2.设A:V→P(V)是一点-集映射,如果在点狓∈X处,有{狓(k)}X,狓(k)→狓,狔(k)∈A(狓(k)),狔(k)→狔,使得狔∈A(狓),则称点-集映射A在狓∈X处是闭的如果它在X中的每一点是闭的,则称点-集映射A在X上是闭的.定义3a.定义映射T1:M犠×M犝→Rcd:其中狏j=(vj1,vj2,…,vjd)T∈犚d,j=1,2,…,c,通过式(10a)计算而得.定义3b.定义映射T2:Rcd×M犝→M犠:其中狑j=(wj1,wj2,…,wjd)T,j=1,2,…,c,通过式(10b)计算而得.定义3c.定义映射T3:M犠×Rcd→M犝:定义4.定义映射Tm:M犠×Rcd×M犝→M犠×其中犝=[uji]c×n通过式(10c)计算而得.Rcd×M犝如下:Tm(犠,犞,犝)={(犠,犞,犝)|犞=T1(犠,犝),定义5.如果(犠(k),犞(k),犝(k))∈Tm(犠(k-1),犠(k-1),犝(k-1)),k=2,3,….这里(犠(1),犞(1),犝(1))为M犠×Rcd×M犝中的任意值,则序列{(犠(k),犞(k),犝(k))}称为EI-FWD-SSFC迭代序列.Page6定理2.设X={狓1,狓2,…,狓n},定义Ξ(犝)=Jm(犠,犞,犝)为Ξ:M犝→犚上的函数,其中犠∈M犠,犞∈Rcd,α>1,m>r>0,εw>0,εu>0为定值.当且仅当犝=T3(犠,犞)时,犝为Ξ在M犝上的全局最小值点.定理3.设X={狓1,狓2,…,狓n},定义Θ(犠)=Jm(犠,犞,犝)为Θ:M犠→犚上的函数,其中犞∈Rcd,犝∈M犝,α>1,m>r>0,εw>0,εu>0为定值.当且仅当犠=T2(犞,犝)时,犠为Θ在M犠上的全局最小值点.定理4.设X={狓1,狓2,…,狓n},定义Ψ(犞)=Jm(犠,犞,犝)为Ψ:Rcd→犚上的函数,其中犠∈M犠,犝∈M犝,α>1,m>r>0,εw>0,εu>0为定值.当且仅当犞=T1(犠,犝)时,犞为Ψ在Rcd上的全局最小值点.定理5(EI-FWD-SSFC收敛定理).设X={狓1,狓2,…,狓n}包含至少c个独立的点,c<n,定义Ω={(犠,犞,犝)∈M犠×Rcd×M犝|Jm(犠,犞,犝)<Jm(犠,犞,犝),犝≠犝;Jm(犠,犞,犝)<Jm(犠,犞,犝),犠≠犠;Jm(犠,犞,犝)<Jm(犠,犞,犝),犞≠犞}(15)为如下最优化问题的解集:其中,Jm(犠,犞,犝)如式(9)计算.令(犠(0),犞(0),犝(0))为根据映射Tm进行迭代的起点,犠(0)∈M犠,犞(0)∈犚cd,犝(0)=T3(犠(0),犞(0)).则迭代序列{(犠(k),犞(k),犝(k))},k=1,2,…,终止于解集Ω中的一个点(犠,犞,犝),或者存在一子序列收敛于Ω中的一个点.证明.(1)设conv(X)={狓∈犚d|狓=∑nαi0;狓i∈X犚d}为数据集X上的凸包.由式(10a)易得,狏(k)j∈conv(X),j=1,2,…,c,k=1,2,…;犞={狏1,狏2,…,狏c}∈[conv(X)]T.显然,(犠(k),犞(k),犝(k))∈M犠×[conv(X)]T×M犝M犠×Rcd×M犝,k=1,2,….而M犠、conv(X)、M犝均为有界闭集,故它们均为紧集,M犠×[conv(X)]T×M犝亦为紧集.(2)Jm为Tm在X上的Zangwill函数.由于犞(k+1)=T1(犠(k),犝(k)),根据定理4,有Jm(犠(k),犞(k+1),犝(k))Jm(犠(k),犞(k),犝(k)).同理,根据定理3,可得如下不等式:Jm(犠(k+1),犞(k+1),犝(k))Jm(犠(k),犞(k+1),犝(k));根据定理2,可得如下不等式:Jm(犠(k+1),犞(k+1),犝(k+1))Jm(犠(k+1),犞(k+1),犝(k)),即Jm(犠(k+1),犞(k+1),犝(k+1))Jm(犠(k),犞(k),犝(k))下面证(犠(k),犞(k),犝(k))Ω时,取严格不等号.用反证法.假设存在(犠(k),犞(k),犝(k))Ω,使Jm(犠(k+1),犞(k+1),犝(k+1))=Jm(犠(k),犞(k),犝(k))成立,则必有Jm(犠(k),犞(k),犝(k))=Jm(犠(k),犞(k+1),犝(k))=Jm(犠(k+1),犞(k+1),犝(k))根据定理4,有犞(k+1)=T1(犠(k),犝(k))为Ψ(犞)=Jm(犠(k),犞,犝(k))的全局最小值点.又根据式(17)中Jm(犠(k),犞(k),犝(k))=Jm(犠(k),犞(k+1),犝(k))成立,根据全局最小值点的唯一性,有犞(k)=犞(k+1).同理可得,犠(k)=犠(k+1),犝(k)=犝(k+1).所以有(犠(k),犞(k),犝(k))=(犠(k+1),犞(k+1),犝(k+1))(18)由于(犠(k),犞(k),犝(k))Ω,由解集Ω的定义,存在以下3种情况:①存在犞≠犞(k),使Jm(犠(k),犞(k),犝(k))Jm(犠(k),犞,犝(k))(19a)②存在犠≠犠(k),使Jm(犠(k),犞(k),犝(k))Jm(犠,犞(k),犝(k))(19b)③存在犝≠犝(k),使Jm(犠(k),犞(k),犝(k))Jm(犠(k),犞(k),犝)(19c)成立.下面以①为例进行讨论.当存在犞≠犞(k),使式(19a)成立时,根据犞(k)=犞(k+1),有犞≠犞(k+1).根据定理4,犞(k+1)=T1(犠(k),犝(k)),有Jm(犠(k),犞,犝(k))>Jm(犠(k),犞(k+1),犝(k))综合式(19a)、(20),有Jm(犠(k),犞(k),犝(k))>Jm(犠(k+1),犞(k+1),犝(k+1))这与式(17)矛盾.同理,根据②、③均可推出矛盾.故假设不成立,即当(犠(k),犞(k),犝(k))Ω时,式(16)取严格不等号.故Jm是Tm在X上的Zangwill函数.(3)根据定义3,显然T1:M犠×M犝→Rcd和T2:Rcd×M犝→M犠是连续映射.下面根据定义2证明T3:M犠×Rcd→M犝是闭映射.不妨设成立;成立;因为Page7uji=limk→u(k)又limk→(d(k)ji+εu)=limk→d(k)即成立.所以,T3:M犠×Rcd→M犝为闭映射.易得Tm:M犠×Rcd×M犝→M犠×Rcd×M犝亦为闭映射[10].当然在{Df-Ω}上是闭映射,其中Df如式(9(b))定义.综合上述,定理5得证.5实验5.1模拟数据实验本实验用来验证算法EI-FWD-SSFC在进行有图1效聚类的同时,可以成功地发现各类别包含的重要特征.所使用的模拟数据集采用参考文献[19]的方法生成,由1000个包含50个特征的样本点构成,包含两类,具有以下特点:(1)它包含了不止一个类别.(2)在各类别的相关特征所构成的特征子空间中,数据点的坐标值较为集中;在各类别的无关特征所构成的特征子空间中,数据点的坐标值在某一区间均匀分布,并且接近于0.(3)不同类别所具有的特征子集相互重合.模拟数据的这种空间分布特点与文本数据的结构是类似的.相关信息见表1.类别11,3,11,16,17,19,21,22,24,25,27,29,31,33,21,2,3,5,810,11,16,18,19,20,21,23,24,27,本实验中,参数设置如下:m=1.5,r=1.1,α=3,εu=10-14,εw=0.1.图1(a)和图1(b)分别给出了聚类结果各类中每一维特征的权重.对照表1,我们发现,在聚类结果中,权重大的特征恰为表1所列的生成数据时各类别的相关特征,可见本文算法可以成功地发现数据集中各类别包含的相关特征.Page85.2真实数据实验5.2.1测试样本集文本聚类算法的性能受多个因素影响,比如测试集包含类别的数量、各个类别之间语义上的差别、每个类别的大小等等.为研究这些因素如何影响本文算法,我们基于标准文本数据集classic、k1b、NG20①建立测试样本集.通过计算tf·idf,去除表2测试集信息数据源测试集classick1bNG205.2.2评估指标与测试平台本文从RandIndex[20]指标和算法的运行时间这两个方面来评估算法的性能.RandIndex指标用来评价聚类结果与外部准则(externalcriterion)相吻合的程度.对于所给点集X={狓1,狓2,…,xn},假设U(e)={u(e)和U={u1,u2,…,uC}是数据集X上两种不同的划分,对于1i≠iR,1j≠jC,有∪R∪Cj=1uj,u(e)是聚类结果,令a为在U(e)和U中均出现在同一类中的点对的数量,b为在U(e)中出现在同一类中、而在U中出现在不同类中的点对的数量,c为在U(e)中出现在不同类中、而在U中出现在同一类中的点对的数量,d为在U(e)和U中均出现在不同类中的点对数量,则RandIndex通过如下式子进行计算:显然,RandIndex值介于[0,1]之间.当两种分类情况完全一致时,RandIndex的值为1.本文所用实验平台如表3所示.5.2.3本文算法和FSC性能比较本实验考查经过充分迭代后,本文算法和FSCidf值过高和过低的特征.本文建立如表2所示的9个测试集,这9个测试集分为3个系列.每一系列中,依次向前一测试集增加1~2个类别,从而构成新测试集.每个测试集所包含的类别数量由字母代号后的数字给出.此外,我们兼顾平衡数据集与非平衡数据集对算法性能的影响,K4和K6为非平衡数据集,其它的都是平衡数据集.200300400200360560200400600性能的差异.为了避免迭代终止阈值的影响,我们设置算法迭代次数为50次,使算法充分迭代后收敛.本文算法参数设置为m=1.5,r=1.1,α=3,εu=10-14,εw=0.1;FSC参数设置为α=3.5,ε=0.1.在每个测试集上分别运行算法10次,得到聚类结果RandIndex指标的平均值以及每次迭代所需要的平均时间,如表4所示.测试集RandIndexC20.83590.51750.180.18C30.67080.38600.410.41C40.68280.29720.890.90K20.84850.49910.260.28K40.79540.40201.251.26K60.64220.28313.723.76NG20.73710.49950.460.45NG40.66610.28242.472.38NG60.74450.27297.047.06从表4中我们不难发现,对于具有上千个特征的高维文本数据,本文算法在不增加时间开销的前提下,可以取得比FSC更好的效果,这说明使用模糊划分矩阵犝来代替FSC中的二值划分矩阵有效地提高了算法的性能.下面我们以目标函数的取值变化为研究对象,分析FSC性能下降的原因.以测试集NG2为例,图2给出运行本文算法和FSC时目标函数的变化过程,其它测试集也可得到类似结果.①http://glaros.dtc.umn.edu/gkhome/fetch/sw/cluto/Page9图2如图2(b)中,本文算法的目标函数随着迭代的推进而递减,经过10次左右的迭代后基本达到平衡状态,这一过程也是算法寻找数据集上最优划分的过程.而在迭代过程中,FSC目标函数的取值始终不发生变化(如图2(a)),寻找更优划分的过程也因此而停止.下面我们分析原因.以数据集NG2为例,对其进行预处理后,数据集是一个具有6163维的高维矩阵.特征数量过多,即d值过大,使用wjh=1/d(j=1,2,…,c,h=1,2,…,d)对特征权重矩阵犠进行初始化[10],wjh值就会非常小.FSC更新二值划分矩阵时使用的是最近邻原则,可以将uji的更新公式看成是一个对wjh变化不敏感的滤波器.当wjh值很小时,其绝对变化量也非常小,这不足以使dji发生很大的变化从而使uji值也随之变化.当划分矩阵犝停止更新时,目标函数逐渐减小的过程也因此终止,算法也过早地陷于局部极值而不能自拔.而本文算法采用式(10c)更新模糊划分矩阵犝,其定义域为连续空间,犠,犞任何细微变化都会使犝进行更新,从而使聚类结果更趋于合理,所以迭代过程可以向前推进.5.2.4特征数量的变化对不同特征加权算法的影响本实验研究算法在充分迭代的前提下,特征数量的变化对不同特征加权算法的影响.使用表2中最为复杂的测试集NG6,指定迭代次数为50次,依次改变特征数量,从200递增到3000.分别运行本文算法(m=1.5,r=1.1,α=3,εu=10-14,εw=0.1)、WKMeans(β=2)[6]、EWKM(γ=10000)[9]、FSC(α=3.5,ε=0.1)[10]各10次.采用实验平台如表3所示,得到平均运行时间和RandIndex指标分别如图3(a)和图3(b)所示.从图3(a)中我们可以看出,在相同迭代次数的前提下,EWKM需要耗费更多的时间,虽然4个算Page10法每次迭代的时间复杂度均为O(nkd),其中n、k、d分别为样本点、类别、特征的数量.但是,熵运算需要更大的运算量,因此EWKM需要更多的时间.图3(b)给出了在特征数量逐渐增加的情况下,分别运行这4个算法所得结果的RandIndex值.比较本文算法和WKmeans,充分体现了特征加权与软子空间相结合的优势:本文算法在不同的软子空间中进行聚类,不同的类别使用不同的权重向量进行特征加权;而WKmeans算法虽然进行了特征加权,但是没有考虑到不同类别中特征权重的差异,所以聚类效果不理想.将本文算法与FSC相比,显然,用模糊划分矩阵代替二值划分矩阵,有利于算法性能的提高.随着特征数量的增加,FSC聚类结果的RandIndex指标有逐步减小的趋势.此外,FSC和EWKM均为硬聚类算法k-means与软子空间相结合的产物,其效果不如本算法.另一方面,对于特征数量的增加,本文算法的聚类结果较其它算法更加平稳,其RandIndex指标维持在0.75左右,可见本文算法对特征数量的变化表现出了较强的鲁棒性.6结论将特征加权距离与软子空间相结合,本文提出了一种适用于高维文本数据的模糊聚类新算法.其特点是:算法在进行聚类的同时,可以成功地进行特征权重的无监督学习,这对于文本数据挖掘等应用有着重要的意义.从形式上讲,本文算法的目标函数中出现了两个模糊矩阵;此外目标函数的约束条件中引入了熵指数,从而扩大了模糊指数m和α的选取范围.本文从信息论的角度,给出了其物理解释;并应用Zangwill定理就算法的全局收敛性给出了理论证明.通过在模拟数据集和真实数据集上的实验表明,本文算法的性能较现有的特征加权算法有了很大的提高.在实验过程中,我们发现合理选择m、r、α的值对算法性能有较大影响,因此,如何合理选取软子空间聚类算法中的参数将是我们进一步要解决的问题.
