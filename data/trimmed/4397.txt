Page1基于Parray数组类型的矩阵乘法实现崔翔1),2)李晓雯2)陈一1)(北京大学信息科学技术学院高可信教育部重点实验室北京100871)2)(河南大学计算机与信息工程学院河南开封475000)摘要介绍针对异构集群体系结构特点设计的编程接口Parray.Parray使用数组类型对数据的物理存储和逻辑结构进行分离.Parray使用统一的线程数组类型表示各种进程(线程)的创建以及它们之间的控制流转.通过矩阵乘法实例演示Parray程序设计的特点:该程序由一个单CPU线程程序演变为多CPU线程程序、再演变为GPU线程程序———程序的各次演变仅通过数组类型的变化和代码的细微修改即可完成.介绍使用Parray实现的高性能GPU矩阵乘法,在天河1A单节点上的测试性能和CUBLAS4.0相当,同时该代码可以工作于不同物理存储方式的数组.关键词GPU集群;程序设计;矩阵乘法;编程接口;性能优化1引言的主流架构.异构集群的体系结构特点表现为:异构的数据存储和异构的计算单元.程序员需要针对异构集群的体系结构特点进行使用GPU加速的异构集群逐渐成为超级计算程序设计以得到高性能的代码[1-3].针对其异构数据Page2存储的特点,数据通常需要进行多维度的划分.针对其异构计算单元的特点,程序员需要创建和调用不同种类的线程.目前程序员进行异构集群程序设计通常使用MPI+Pthread+CUDA的方式[4].此方式的弊端表现在:(1)缺乏对多维数组操作的有效支持.C语言并无对多维嵌套数组操作的有效支持,此处所讲的数组可以是分布在集群中不同节点存储器的数组,也可以指单机上分布于不同GPU设备存储器的数组.在MPI+Pthread+CUDA方式下,多维数组的操作通过物理寻址实现,代码表现为复杂的偏移计算表达式和多维嵌套循环.并且,代码只能工作于数组的特定数据物理排列方式;(2)缺乏对不同种类线程调用的统一表示方式.MPI进程、Pthread线程和CUDA线程的创建和调用方式各不相同,因此,程序员需要学习和组合各种异构计算单元的操作方式.GPU集群编程模型和方法的研究面临一个两难的选择:一方面,程序员希望有较高的抽象层次而不必关心系统低层的细节特征;另一方面,当前新体系结构的发展趋势要求在源程序一级区分多种不同类型的处理器和数据存储传输方式,而不是强迫程序员使用某种特定的方式.传统上,侧重前一种选择的思想是“隐藏”(hiding),不少基于变量共享或共享存储的编程模型如OpenMP、OpenCL和PGAS类语言都属于这一设计;侧重后一种选择的思想是“暴露”(exposure),比如MPI和CUDA都暴露了相当多的低层系统特征.不当的暴露会使并行编程过于困难,而过度的隐藏会使源程序严重缺少并行化信息并过于依赖低层优化从而难以达到高性能.这也是当前GPU集群通常采用MPI+Pthread+CUDA方式进行程序设计的原因,因为针对“暴露”的体系结构特征编程可以取得良好的性能.GPU集群设计的初衷是取得高性能,而GPU集群的特点决定了:如果要在其上得到高性能的代码,则必须要考虑其异构特点并针对其体系结构特点进行程序设计而不能忽视之.因此,GPU集群编程工具设计应该尽量减轻程序员的编程难度、简化编程,同时“暴露”与性能相关的体系结构特征,使程序员能进行有效控制,以得到顶级性能的代码.基于上述思想,我们实现了GPU集群上的编程工具Parray.Parray是一种针对异构集群体系结构特点设计的编程接口[5-6],通过引入数组类型对C语言进行扩充.Parray使用数组类型对数据的物理存储和逻辑结构进行分离,为多维数组维度逻辑操作提供程序语言层面的支持;Parray使用统一的线程数组类型表示各种进程(线程)的创建以及它们之间的控制流转.本文首先介绍Parray的数组类型.之后,通过矩阵乘法实例演示Parray程序设计的特点:该程序由一个单CPU线程程序演变为多CPU线程程序、再演变为GPU线程程序.由于使用Parray针对数据的逻辑结构进行编程,代码可以运行于不同的数组物理存储方式;由于对不同种类线程使用统一的数组表示方式,程序的各次演变仅通过数组类型的变化和代码的细微修改即可完成.本文最后介绍使用Parray实现的高性能GPU矩阵乘法,在天河1A单节点上的测试性能和CUBLAS4.0相当,同时也给出该代码工作于不同物理存储方式数组的测试性能.使用Parray实现的另一个典型代码为GPU集群上的FFT,该代码在天河1A上进行了全系统性能测试;由于篇幅限制,此工作将在其他论文中进行介绍.2Parray的数组类型Parray是一种针对GPU和GPU集群的编程接口[5-6].Parray通过数组类型,实现了数组数据物理存储与逻辑结构概念上的分离,本节将介绍Parray中各种不同的数组类型和使用方法.同时,Parray以统一方式表示各种进程(线程)的创建以及它们之间的控制流转.2.1自然数组自然数组指的是数组数据元素的逻辑下标和物理偏移相一致的数组类型.如图1所示,为Parray的自然数组示例代码.1.#parray{pinnedfloat[[3][2]][4]}犃2.#create犃(x)3.for(inti=0;i<$dim(犃)$;i++)x[$犃[i]$]=i-1;4.printf("arrayaccess:%d\n",x[$犃[5][2]$]);5.printf("numberofrows=%d\n",$dim(犃_0)$);6.#destroy犃(x)在Parray代码中,与Parray有关的成分通常为以“#”起始的一段代码或以成对的“$”括起的代码.这样,Parray编译器可以对该代码方便的进行预处理,提取Parray有关的语言成分,并展开成为目标C语言代码.在该代码片段中,第1行首先声Page3明一个数组类型犃,该类型为[[3][2]][4]维度的浮点数(可以视为6行4列的数组,其中行维度6又被分解为3和2两个维度),且存储的物理位置为不分页内存(pinned表示);该行代码只是声明一种类型,即看待物理存储器中数据存储的一种方式,而并未分配其需要的真正的物理存储空间.而后,第2行代码使用Parray的create语句,声明一个浮点数的指针x,按照犃类型的需要分配相应的物理存储器空间,即不分页内存中可以存储(3×2)×4个浮点数的空间,空间的起始地址存储在指针x中.第3行循环语句对x指向的数组进行初始化,$dim(犃)$代表类型犃的维度大小即24,$犃[i]$代表按照犃的根维度取第i个元素的偏移量.当然,由于犃为自然数组,$犃[i]$等于i.第4行代码打印x[$犃[5][2]$]的元素值,其中$犃[5][2]$代表对应犃类型的行和列维度(即犃_0维度和犃_1维度)值分别为5和2的元素的物理偏移值.第5行代码打印类型犃的犃_0维度的大小,即类型犃的行数值6.最后,第6行代码释放x指针指向的对应类型犃的物理存储空间.数组的嵌套维度构成维度的树结构.对于图1中定义的类型犃,其维度构成如图2所示的树结构.对于类型犃,可以将其视为一维、二维或三维数组.作为一维数组访问时,x[$犃[i]$]即为x[i];作为二维数组访问时,x[$犃[5][2]$]即为x[5×4+2];作为三维数组访问时,x[$犃[[i][j]][k]$]即为x[(i×2+j)×4+k]或x[i×8+j×4+k].根据维度的方括号嵌套结构,类型犃的根维度又分解为类型犃_0和类型犃_1,维度分别为[3][2]和[4];类型犃_0又进一步分解为犃_0_0和犃_0_1,维度分别为[3]和[2].在Parray中,维度的名称均遵循上述的命名原则.事实上,这些分解出来的部分类型亦可在程序中作为已有类型使用.例如,程序员可以使用语句1.#create犃_1(y)在不分页内存中分配4个浮点数大小的空间y.同样,$dim(犃_1)$可以用来得到类型犃_1的维度大小,即4.y[$犃_1[i]$]可以用来访问y指针指向的类型为犃_1数组的第i个元素.Parray中,自然数组类型的维度和C语言中的数组维度一样,遵循行优先和右侧维度变化最快的原则.图1中定义的类型犃对应的数组在内存中的物理元素分布如图3所示:同一行的元素在内存中连续.犃[[0][0]][0]犃[[0][0]][1]犃[[0][0]][2]犃[[0][0]][3]犃[[0][1]][0]犃[[0][1]][1]犃[[0][1]][2]犃[[0][1]][3]犃[[1][0]][0]犃[[1][0]][1]犃[[1][0]][2]犃[[1][0]][3]犃[[1][1]][0]犃[[1][1]][1]犃[[1][1]][2]犃[[1][1]][3]犃[[2][0]][0]犃[[2][0]][1]犃[[2][0]][2]犃[[2][0]][3]犃[[2][1]][0]犃[[2][1]][1]犃[[2][1]][2]犃[[2][1]][3]2.2人工数组在Parray中,数组的物理存储与逻辑视图在概念上进行分离:类型提供且仅仅提供数据的逻辑视图,与数据的物理存储空间无关;数组维度变换和转置通过人工数组实现.人工数组是指含有一个或多个人工维度的数组类型,人工维度来自于对已有数组类型维度的引用.人工数组的示例见下列代码(如图4).1.#parray{pinnedfloat[4][3]}犃2.#parray{pinnedfloat[#犃_1][#犃_0]}犅3.#create犃(x)4.x[$犃[3][2]$]=123;5.printf("arrayaccess:%d\n",x[$犅[2][3]$]);6.printf("numberofrows=%d\n",$dim(犅_0)$);7.#destroy犃(x)图4的代码中,第1行声明一个4行3列的自然数组类型犃,第2行声明一个人工数组类型犅,其两个维度为#犃_1和#犃_0.以#起始的维度代表该维度引用自其他数组类型的已有维度,并同时引用其维度数值和偏移函数.本例中,数组类型犅的两个维度将犃的两个维度进行了调转.第4行代码以类型犃的视图对数组第3行第2列的元素赋值123,第5行代码以类型犅的视图取得数组第2行第3列的元素,即为123.第6行代码以类型犅的视图打印数组的行数,即类型犅的第1个维度的维度值,即为3.2.3线程数组程数组.型犕犢犜犃,其维度为2×2.1.#parray{pthd[2][2]}犕犢犜犃在Parray中,使用parray命令同样可以声明线如下代码声明一个PthreadCPU线程数组类Page4如下代码声明一个CUDA线程数组类型犕犢犆犝犇犃犜犃,其维度为8×8×16×16.由于Parray编译器会为每一个被detour调用的CUDA线程数组生成相应的内核函数,因此在代码中跟在关键字cuda之后的worki和worko为该内核函数的形参.1.#parray{cuda(float2worki,float2worko)[[128/16][128/16]][[16][16]]}犕犢犆犝犇犃犜犃如下代码声明一个MPI进程数组类型犕犢犕犘犐犜犃,其维度为2.1.#parray{mpi[2]}犕犢犕犘犐犜犃2.4线程数组的调用在Parray中,和数据数组一样,同构的进程或线程也可以组成数组,各同构线程数组执行各自的代码.各同构线程数组的代码块,以类似于函数调用的静态嵌套的形式书写于相同的程序上下文中.程序的控制流由外层线程数组的代码块转移到内层线程数组的代码块,并在内层线程数组的代码块执行完毕之后再转移回外层线程数组的代码块.2.4.1主进程到线程数组的调用图5是一个简单的Parray的线程数组调用代码示例.其中,第1行mainhost说明该代码是一个在单机上运行的程序,并开始主进程运行的代码;第2行通过parray命令声明一个PthreadCPU线程数组类型犕犢犜犃,该数组的维度为2×2;第3行通过detour命令调用线程数组myta,使得其中的每一个线程执行第4行的打印语句,该打印语句打印各个线程对应的自己在该线程数组中的ID(即$tid$)和在线程数组中对应的0维度的ID(即$tid_0$);该detour调用执行完毕后,在第5行控制流程转回到mainhost对应的主进程.1.#mainhost{2.#parray{pthd[2][2]}犕犢犜犃3.#detour犕犢犜犃(){4.printf("myta%d%d\n",$tid$,$tid_0$);5.}6.}图5中代码的执行结果如图6所示.1.myta002.myta103.myta214.myta312.4.2CUDA线程数组的调用图7中的代码示例了mainhost主进程对CUDA线程数组的detour调用,该段代码使用CUDA线程数组在GPU上对128×128的正方形float2进行转置.在转置过程中,使用到了GPU共享存储器,算法基本思路和CUDASDK中的示例是一样的.在代码的第1行,声明在设备存储器上的数据类型犇犃犜犃,该类型将128×128的正方形数据做了进一步的维度划分,将其划分为8×8个16×16的小正方形;代码的第2行,声明需要被调用的CUDA线程数组类型犕犢犆犝犇犃犜犃,其维度为8×8×16×16,此外,还声明了该CUDA线程数组对应内核函数的形参worki和worko,用来向内核函数传递设备存储器的指针;代码的第3~4行,声明需要用到的GPU共享存储器的自然数组类型犃和人工数组类型犅,用来对共享存储器进行无存储体冲突的访问.1.#parray{dmemfloat2[[128/16][16]][[128/16][16]]}犇犃犜犃2.#parray{cuda(float2worki,float2worko)3.#parray{smemfloat2[16][17]}犃4.#parray{smemfloat2[#犃_0][16#犃_1]}犅5.6.#mainhost{7.…//初始化操作,分配GPU和内存等8.#detour犕犢犆犝犇犃犜犃(adri,adro){9.#create犃(tile)10.tile[$犅[$tid_1_0$][$tid_1_1$]$]=11.#sync{}12.worko[$犇犃犜犃[[$tid_0_1$][$tid_1_0$]]13.}14.…//释放操作,释放GPU和内存等15.}在代码的mainhost主进程中,首先第7行进行GPU初始化和存储空间分配的操作,之后第8行使用detour命令调用犕犢犆犝犇犃犜犃线程数组对adri指向的数据进行转置,结果数据放到adro指针的位置.在犕犢犆犝犇犃犜犃线程数组被调用执行的代码中,第9行先使用create命令分配需要使用的共享存储器空间,之后第10行各个GPU线程通过数据数组类型访问将设备存储器中的数据读入到共享存储器中,在第11行各个线程块进行同步之后,在第12行各个GPU线程再通过数据数组类型访问将转置后的共享存储器中的数据写回到设备存储器中.最后,对犕犢犆犝犇犃犜犃线程数组中的detour调用返回之后,mainhost主进程在第14行进行GPU和存储空间释放的操作.需要注意的是,在代码第2行声明CUDA线程Page5数组类型犕犢犆犝犇犃犜犃时,犕犢犆犝犇犃犜犃_0的维度8×8对应到girdDim.y和girdDim.x,犕犢犆犝犇犃犜犃_1的维度16×16对应到blockDim.y和blockDim.x;与此相对应,犕犢犆犝犇犃犜犃线程数组被detour调用的代码块中,$tid_0_0$、$tid_0_1$、$tid_1_0$和$tid_1_1$分别对应到CUDA内建变量blockIdx.y、blockIdx.x、threadIdx.y和threadIdx.x.3矩阵乘法代码演变3.1单CPU线程矩阵乘法单CPU线程矩阵乘法代码为典型的三重循环.使用Parray书写的单CPU线程矩阵乘法代码如图8所示.代码第1行声明数组类型犕,为存储在分页内存中的1024×1024个浮点数类型;代码第2~4行使用create命令在内存中创建3个数组类型犕的存储空间,起始地址分别存储在指针变量a、b和c中(指针变量a、b和c为已声明的全局变量);代码第5~11行为计算矩阵乘法的典型三重循环,将指针a处存储的数组类型犕的矩阵和指针b处存储的数组类型犕的矩阵做乘法,其结果存储在指针c处:循环变量i、循环数组类型犕的_0维度即$dim_0(犕)$=1024次,循环变量j、循环数组类型犕的_1维度即$dim_1(犕)$=1024次,对每一个结果矩阵中的元素做计算,即将a[$犕[i][k]$](解释为指针a处存储的视为数组类型犕的数组的i行k列的元素)和b[$犕[k][j]$](解释为指针b处存储的视为数组类型犕的数组的k行j列的元素)相乘,累加到变量Cvalue,而后将Cvalue写回到c[$犕[i][j]$](解释为指针c处存储的视为数组类型犕的数组的i行j列的元素);最后,代码第12~14行使用destroy命令释放指针变量a、b、c指向的数组类型犕的存储空间.1.#parray{pagedfloat[1024][1024]}犕2.#create犕(a)3.#create犕(b)4.#create犕(c)5.for(inti=0;i<$dim_0(犕)$;i++)6.for(intj=0;j<$dim_1(犕)$;j++){7.floatCvalue=0;8.for(intk=0;k<$dim_1(犕)$;k++)9.Cvalue+=a[$犕[i][k]$]b[$犕[k][j]$];10.c[$犕[i][j]$]=Cvalue;11.}12.#destroy犕(a)13.#destroy犕(b)14.#destroy犕(c)3.2多CPU线程矩阵乘法延续图8中代码的思路,可以使用Parray书写出多CPU线程的矩阵乘法代码.如图9所示,假设4个CPU线程计算目标矩阵犆,4个线程根据自己的下标不同分别计算犆中1/4方阵的数据.使用Parray书写的多CPU线程矩阵乘法代码如图10所示.代码第1行声明数组类型犕,根据图12所示,维度划分为(2×512)×(2×512);代码第2行声明CPU线程数组类型犕犢_犜犃,维度划分为2×2;代码第6行对线程数组犕犢_犜犃进行调用,4个线程分别完成各自的计算,每个线程需要计算$dim_0_1(犕)$×$dim_1_1(犕)$即512×512个元素,第7~8行的循环变量i和循环变量j负责每次循环计算一个元素,第9~16行即为计算一个元素的代码.1.#parray{pagedfloat[[2][512]][[2][512]]}犕2.#parray{pthd[2][2]}犕犢_犜犃3.#create犕(a)4.#create犕(b)5.#create犕(c)6.#detour犕犢_犜犃{7.for(inti=0;i<$dim_0_1(犕)$;i++)8.for(intj=0;j<$dim_1_1(犕)$;j++){9.floatpa=a+$犕[[$tid_0$][i]][0]$;10.floatpb=b+$犕[0][[$tid_1$][j]]$;11.floatCvalue=0;12.for(intk=0;k<$dim_1(犕)$;k++)13.Cvalue+=pa[$犕[0][k]$]14.pb[$犕[k][0]$];15.c[$犕[[$tid_0$][i]][[$tid_1$][j]]$]16.=Cvalue;17.}18.}19.#destroy犕(a)20.#destroy犕(b)21.#destroy犕(c)对于图9中圆点所示的元素的计算,应该由下标为(1,0)的线程完成,该元素在块内的下标为(i,j),因此,该元素为c[$犕[[1][i]][[0][j]]$],Page6其中,通过线程数组下标1和0对块进行寻址,通过块内数据元素下标i和j对元素进行偏移寻址,更一般的,矩阵犆中的元素即为c[$犕[[$tid_0$][i]][[$tid_1$][j]]$].对该元素的计算需要图12中pa指针指示的矩阵犃中的一行元素和pb指针指示的矩阵犅中的一列元素.图13中代码第9行计算pa指针的位置,使用线程数组下标$tid_0$和块内行下标i计算矩阵犃中行方向的偏移$犕[[$tid_0$][i]][0]$;同样,代码第10行计算pb指针的位置,使用线程数组下标$tid_1$和块内列下标j计算犅中列方向的偏移$犕[0][[$tid_1$][j]]$.第12~14行,使用循环将犃中一行的元素pa[$犕[0][k]$]和犅中一列的元素pb[$犕[k][0]$]分别相乘并累加,最后,将该元素的计算结果写回到c[$犕[[$tid_0$][i]][[$tid_1$][j]]$].3.3GPU线程矩阵乘法对图10上的代码稍加修改,可以得到GPU线程矩阵乘法代码.GPU线程矩阵乘法代码如图11所示,与图10代码的区别用下划线标出.第1行声明数组类型犕为GPU设备存储器类型,这样在第5~7行执行create指令时,创建的存储空间位于GPU设备存储器;第2行声明犕犢_犜犃为GPU线程数组类型,同时给出该GPU线程数组对应内核函数的形式参数列表,第8行在调用GPU线程数组犕犢_犜犃时,将实参列表传递给内核函数(此例中,形参和实参均为a、b和c);除此之外,图11中的代码和图10的代码完全相同.注意,图11代码中不包括对GPU设备存储器数据传输或初始化的内容.1.#parray{dmemfloat[[2][512]][[2][512]]}犕2.#parray{cuda(floata,floatb,floatc)3.[2][2]}犕犢_犜犃4.INIT_GPU(0);5.#create犕(a)6.#create犕(b)7.#create犕(c)8.#detour犕犢_犜犃(a,b,c){9.for(inti=0;i<$dim_0_1(犕)$;i++)10.for(intj=0;j<$dim_1_1(犕)$;j++){11.floatpa=a+$犕[[$tid_0$][i]][0]$;12.floatpb=b+$犕[0][[$tid_1$][j]]$;13.floatCvalue=0;14.for(intk=0;k<$dim_1(犕)$;k++)15.Cvalue+=pa[$犕[0][k]$]16.pb[$犕[k][0]$];17.c[$犕[[$tid_0$][i]][[$tid_1$][j]]$]18.=Cvalue;19.}20.}21.#destroy犕(a)22.#destroy犕(b)23.#destroy犕(c)3.4演变的GPU线程矩阵乘法实际上,图11中的代码使用了4个GPU线程计算矩阵乘法,这并不符合GPU体系结构计算的特点.理想的做法是使用1024×1024个GPU线程,每一个线程计算犆中的一个元素,得到图12所示代码.与图11代码的区别以粗斜体标出:第1行和第2行分别修正数组类型犕和线程数组犕犢_犜犃的维度大小,程序其他部分不变.1.#parray{dmemfloat[[1024][1]][[1024][1]]}犕2.#parray{cuda(floata,floatb,floatc)3.[1024][1024]}犕犢_犜犃4.INIT_GPU(0);5.#create犕(a)6.#create犕(b)7.#create犕(c)8.#detour犕犢_犜犃(a,b,c){9.for(inti=0;i<$dim_0_1(犕)$;i++)10.for(intj=0;j<$dim_1_1(犕)$;j++){11.12.13.14.15.16.17.c[$犕[[$tid_0$][i]][[$tid_1$][j]]$]18.19.}20.}21.#destroy犕(a)22.#destroy犕(b)23.#destroy犕(c)3.5化简的GPU线程矩阵乘法图12中的代码可以做进一步的简化.图12中代码第1行,大小为1的两个维度可以省略;由于每个GPU线程计算犆中的一个元素,图12中代码第9~10行的循环不再需要.简化后的代码如图13所示.1.#parray{dmemfloat[1024][1024]}犕2.#parray{cuda(floata,floatb,floatc)3.[1024][1024]}犕犢_犜犃4.INIT_GPU(0);5.#create犕(a)6.#create犕(b)7.#create犕(c)8.#detour犕犢_犜犃(a,b,c){9.floatpa=a+$犕[$tid_0$][0]$;10.floatpb=b+$犕[0][$tid_1$]$;11.floatCvalue=0;12.for(intk=0;k<$dim_1(犕)$;k++)13.Cvalue+=pa[$犕[0][k]$]14.pb[$犕[k][0]$];15.c[$犕[$tid_0$][$tid_1$]$]=Cvalue;16.}17.#destroy犕(a)18.#destroy犕(b)19.#destroy犕(c)Page7图14的代码为《CUDA编程手册》[7]中作为示例的矩阵乘法内核代码,和图13中Parray书写的代码具有同样的思路:每个GPU线程计算犆中的一个元素,第5行计算在矩阵犃中的访问偏移(对应图16代码的第9行),第6行计算在矩阵犅中的访问偏移(对应图16代码的第10行),第8~11行做累加计算并写出一个元素的计算结果(对应图13代码的第12~15行).1.__global__voidMatMulKernel2.(Matrix犃,Matrix犅,Matrix犆){3.//Eachthreadcomputesoneelementof犆4.//byaccumulatingresultsintoCvalue5.introw=blockIdx.yblockDim.y+threadIdx.y;6.intcol=blockIdx.xblockDim.x+threadIdx.x;7.floatCvalue=0;8.for(inte=0;e<犃.width;++e)9.Cvalue+=犃.elements[row犃.width+e]10.犅.elements[e犅.width+col];11.犆.elements[row犆.width+col]=Cvalue;12.}图14CUDA编程手册中的GPU线程矩阵乘法代码3.6不同数组存储方式的GPU线程矩阵乘法尽管图13和图14中的代码完成同样的功能,但是,只需对图13代码中的数组类型稍加修改,便可以使其工作于不同的数组物理存储方式.之前的代码均工作于行优先存储的数组,如图15所示,图中线条连接的元素顺序即为其在内存中实际存储的顺序.在图16所示的矩阵乘法中,正方形矩阵并非行优先存储.它的数组元素的存储方式按照图中线条连接的元素顺序在内存中连续存储.对图13中的代码稍加修改,得到图17所示的代码,可以工作于图16所示的数组物理存储方式.代码的区别以粗斜体标出:第1行和第2行修改了数组类型犕的声明,程序其他部分不变.1.#parray{dmemfloat[2][1024][512]}犃2.#parray{float[#犃_1][[#犃_0][#犃_2]]}犕3.#parray{cuda(floata,floatb,floatc)4.[1024][1024]}犕犢_犜犃5.INIT_GPU(0);6.#create犕(a)7.#create犕(b)8.#create犕(c)9.#detour犕犢_犜犃(a,b,c){10.floatpa=a+$犕[$tid_0$][0]$;11.floatpb=b+$犕[0][$tid_1$]$;12.floatCvalue=0;13.for(intk=0;k<$dim_1(犕)$;k++)14.Cvalue+=pa[$犕[0][k]$]15.pb[$犕[k][0]$];16.c[$犕[$tid_0$][$tid_1$]$]=Cvalue;17.}18.#destroy犕(a)19.#destroy犕(b)20.#destroy犕(c)图17不同数组存储方式的GPU线程矩阵乘法代码图17中代码第1行声明自然数组类型犃,其维度为2×1024×512,对应图16中数组的物理存储方式,即数组左右两边2个1024×512的数据,在内存中连续存储;第2行声明人工数组类型犕,犕的_0维度为#犃_1,即犃的_1维度,大小为1024,犕的_1维度为[#犃_0][#犃_2],即犃的_0维度和_2维度,大小为2×512.这样,通过人工数组类型犕,将物理存储为自然数组类型犃的数据,从逻辑上视为1024×1024的正方形数组.同理,可以修改图17中数组类型犕的声明,使之可以计算其他物理存储方式的数组,程序其他部分不变.对于图18(a)中示意的数组存储方式,只需将数组类型犕的声明修改为如下即可:1.#parray{dmemfloat[4][1024][256]}犃2.#parray{float[#犃_1][[#犃_0][#犃_2]]}犕对于图18(b)中示意的数组存储方式,只需将数组类型犕的声明修改为如下即可:1.#parray{dmemfloat[2][2][512][512]}犃2.#parray{float[[#犃_0][#犃_2]][[#犃_1][#犃_3]]}犕4GPU高性能矩阵乘法4.1算法描述在GPU上计算矩阵犃和矩阵犅的乘积得到矩Page8阵犆的计算可以按照下述分块的方式进行:每一个线程块负责计算矩阵犆的一个子块Csub,而该线程块中的每一个线程负责计算子块Csub中的若干个元素.Csub是两个长方形矩阵的乘积:矩阵犃的和Csub具有相同行数的长方形矩阵乘以矩阵犅的和Csub具有相同列数的长方形矩阵,如图19所示.由于GPU硬件存储的限制,这两个长方形的矩阵也被分割为相应的子块,称为Asub和Bsub,Csub为对应Asub和Bsub子块矩阵乘积的和.Csub的大小决定了每轮运算必须被加载到GPU芯片的数据量大小.如果Csub过小,则SGEMM会变成一个带宽受限的问题.假设Asub、Bsub和Csub的大小分别为m×k、k×n和m×n,而矩阵犃、犅和犆分别被分为M×K、K×N和M×N个这样的子块.计算一个Csub需要从设备存储器中取K个Asub和Bsub.由于在矩阵犆中有M×N个Csub,则整个计算需要从设备存储器中取M×m×N×n×K×k×(1/m+1/n)个浮点数.在文献[1]中对文献[8]中的实现进行了改进,该算法以行优先的方式设计.线程块的线程数目为128,被组织成4×32的格式.Csub的大小为16×256.每一个线程块负责计算一个Csub,而一个线程负责计算Csub中的两列.图20显示了每一个线程块的流程图.首先,线程块申请16×32的共享存储器空间用于存储矩阵犃的子块,每一个线程申请寄存器用于存储Csub中的16个float2,即32个float.在使用线程ID计算出访问矩阵犃、犅和犆的指针位置之后,进入一个循环.在每一轮循环中,一个线程块从设备存储器中读入16×32的Asub的数据到共享存储器中,之后,又一个内层的循环被执行:在每一轮内层循环中,一个线程从矩阵犅中读入一个float2,把这两个float和共享存储器中相应的数据做计算,将结果累加到Csub对应的寄存器中.最后,每个线程将其计算的两列Csub的数据写回设备存储器中.使用Parray实现的文献[1]中GPU高性能矩阵乘法算法的代码如图21所示.1.#subprogSGEMM(da,db,dc,type)2.#global{3.#parray{cuda(floata,floatb,floatc)4.[[N/16][N/256]][[4][32]]}CUDA5.#parray{dmemfloat6.[[N/16][[4][4]]#type_0][[N/32][32]#type_1]}犃7.#parray{dmemfloat8.[[N/32][32]#type_0][[N/256][[128][2]]#type_1]}犅9.#parray{dmemfloat[#犃_0][#犅_1]}犆10.#parray{dmemfloat[#犆_0_1][#犆_1_1_1]}犆011.#parray{smemfloat[32][17]}犛//avoidbankconflict12.#parray{smemfloat[32#犛_0][[4][4]#犛_1]}犛013.#parray{rmemfloat[16][2]}犚14.}15.#detourCUDA(da,db,dc){16.#create犛(s)17.#create犚(r)18.#pragmaunroll19.for(inti=0;i<$size(犚)$;i++)r[i]=0;20.a+=21.$犃[[$tid_0_0$][[0][$tid_1_0$]]][[0][$tid_1_1$]]$;22.b+=$犅[0][[$tid_0_1$][[$tid_1$][0]]]$;23.c+=$犆[[$tid_0_0$][0]][[$tid_0_1$][[$tid_1$][0]]]$;24.for(inti=0;i<$size(犃_1_0)$;i++){25.#copy犃_0_1_0(a+$犃_1_0[i]$)26.to犛0_1_0(s+$犛0[$tid_1_1$][[0][$tid_1_0$]]$)27.#sync_128.for(intm=0;m<$size(犃_1_1)$;m++){29.float2b2;30.floatb1=b+$犅_0[i][m]$;31.if($cnt(犅_1_1_1)$)b2=((float2)b1)[0];32.else{b2.x=b1[0];b2.y=b1[$犅_1_1_1[1]$];}33.#pragmaunroll34.for(intj=0;j<16;j++){35.r[$犚[j][0]$]+=b2.xs[$犛[m][j]$];36.r[$犚[j][1]$]+=b2.ys[$犛[m][j]$];}37.}38.#sync_139.}40.#copy犚(r)to犆0(c)41.}42.#end图21Parray实现的GPU高性能矩阵乘法代码Page94.2性能测试我们对上述代码在天河1A系统单节点上进行了性能测试(单FermiGPU、驱动程序版本为CUDA4.0),如图22所示.图中,ParraySGEMM为Parray代码工作于行优先物理存储方式数组时的性能,ParraySGEMMofMatrix1为Parray代码工作于图18(a)示意的物理存储方式数组时的性能,ParraySGEMMofMatrix2为Parray代码工作于图18(b)示意的物理存储方式数组时的性能,CUBLAS4.0为天河1A系统预装的CUBLAS数学库的性能.可以看出,Parray矩阵乘法代码工作于行优先和图18(a)示意的物理存储方式的数组时性能相当,这是由于在这两种存储方式下,对GPU设备存储器的访问都是以基本凝聚的方式进行的,同时,Parray矩阵乘法和CUBLAS4.0的实现具有相当的性能,这说明Parray编程接口本身并不引起性能的损耗.同时,Parray矩阵乘法代码可以工作于图18(b)示意的物理存储方式的数组,这对于CUBLAS4.0或直接使用CUDA书写的代码是无法做到的.5总结本文介绍了针对异构集群体系结构特点设计的编程接口Parray.Parray使用数组类型对数据的物理存储和逻辑结构进行分离,为多维数组维度逻辑操作提供了程序语言层面的支持;Parray使用统一的线程数组类型表示各种进程(线程)的创建以及它们之间的控制流转.本文通过矩阵乘法实例演示Parray程序设计的特点:该程序由一个单CPU线程程序演变为多CPU线程程序、再演变为GPU线程程序.由于使用Parray针对数据的逻辑结构进行编程,代码可以运行于不同的数组物理存储方式;由于对不同种类线程使用统一的数组表示方式,程序的各次演变仅通过数组类型的变化和代码的细微修改即可完成.本文介绍使用Parray实现的高性能GPU矩阵乘法,在天河1A单节点上的测试性能和CUBLAS4.0相当,同时也给出该代码工作于不同物理存储方式数组的测试性能.使用Parray实现的另一个典型代码为GPU集群上的FFT,该代码在天河1A上进行了全系统性能测试;由于篇幅限制,此工作将在其他论文中进行介绍.
