Page1跨数据中心的动态资源联合预留研究伍之昂1),2)罗军舟1)宋爱波1)东方1)1)(东南大学计算机科学与工程学院南京210096)2)(南京财经大学江苏省电子商务重点实验室南京210003)摘要如何有效保障服务质量(QualityofService,QoS)是网格计算和云计算等网络计算平台面临的核心问题之一,资源可获得性和性能的动态变化使得将多媒体网络的资源提前预留机制直接应用到网络计算平台面临巨大挑战.为此提出了一种跨数据中心的资源联合预留体系架构,引入虚拟资源容器以适应资源的动态性,提出了资源预留协商算法以增加预留请求与资源的匹配率.在东南大学校园云计算平台的实验结果表明,在资源动态变化的网络计算环境中,所提出的体系架构能显著提高预留请求的成功率,且具有更强的稳定性.关键词网络计算;数据中心;资源提前预留;虚拟资源容器1引言从较早的对等计算、网格计算[1],到近年来炙手可热的云计算[2]无不希望通过Web服务的方式向用户提供计算和存储资源.Web服务作为目前最流行的分布式计算方式之一,提供了一种与操作系统无关、与程序设计语言无关、与机器类型无关、与运行环境无关的开放平台,实现网络资源的协作与共享[3].网络计算平台集成的资源多以Web服务的形式向用户提供,许多科学计算任务往往需要大量的计算资源和存储资源协同工作来完成[4],有效地控制和保障这类科学计算任务的服务质量(QualityofService,QoS)成为网络计算中面临的重要议题之一.资源提前预留基于商定的QoS需求为用户分配资源,它源于多媒体网络中的Intserv体系架构的资源预留协议RSVP(ResourceReservationProtocol).由于网络计算环境下资源的可获得性和性能动态变化,未来预留时间段内资源的可获得性和性能难以得到保证,因此,将资源提前预留作为一种QoS的保障机制引入到网络计算环境下面临着挑战.GGF(GlobalGridForum)将资源提前预留定义为:用户与资源所有者经过协商,在指定时间段内获得的对资源的限制性访问权限①.实际上,提前预留可以看作是网络计算平台的一种承诺,在指定时间段内可以获得特定水平的服务.Foster等人[5]早期设计了针对资源预留和分配的通用体系架构,它通过定义各种QoS控制的API,简化了端到端QoS管理策略的设计和开发,但是,GARA由以计算为中心的计算网格提出,并不能完全套用于服务网格环境中.文献[6]提出了基于GlobusToolkit4开发的服务网格下的提前预留和资源联合分配系统架构,它对预留实体的属性做了详细的描述,将预留请求分为复合预留请求和简单预留请求,通过联合资源分配管理器将复合预留请求分解为多个简单预留请求,并将其分配到各个本地资源管理器进行分别处理.但是,大型网络计算平台大多由多个数据中心协同工作,因此,资源预留体系架构需要支持跨数据中心资源协作和共享.SNAP(ServiceNegotiationandAcquisitionProtocol)协议SLAs(ServiceLevelAgreements)的生命周期管理、资源预留协商以及资源和服务发现等方面的实现方案[7],SNAP定义了3类SLA:TSLA(TaskSLA)、RSLA(ResourceSLA)和BSLA(BindingSLA).TSLA将任务用一组子服务的工作流来描述,并定义了任务的QoS需求参数;RSLA用来描述资源的服务能力,将资源抽象为一组QoS参数的集合;BSLA将TSLA和RSLA相关联,描述了任务与特定资源的绑定.网络计算环境下资源的动态性表现在两个方面:(1)资源可能在未来时间段内产生不可预测的失效,以致资源的可获得性表现出动态性;(2)由于资源的本地负载动态波动、网络传输存在延迟,以致资源的服务性能表现出动态性.SNAP采用的直接将TSLA和RSLA静态的绑定方式使预留请求在提交时就与具体的物理资源关联,资源的动态性导致与预留请求相绑定的资源在预留请求开始执行时的可获得性和服务性能都无法得到保障,系统就可能拒绝该预留请求,从而极大地增大了拒绝率.作为补救方案的重协商机制则容易产生不必要的系统开销.以往的研究提出了多种资源预留的协商策略:如文献[8]提出的资源备份选择策略解决了在资源可获得性无法保障的情况下,如何通过选用备份资源来保证QoS的问题;文献[9]提出的QoS和冲突感知的多资源预留策略旨在减轻端到端的资源冲突,提高资源预留整体的成功率.近年来,云计算技术受到越来越多的关注,基于资源预留为应用动态提供资源以保证QoS仍然是云计算研究中的一个热点问题,比如,文献[10]提出一种云计算环境下资源动态提供策略;文献[11]研究了云工作流型应用在网络环境下的资源预留问题.尽管如此,这些策略缺乏对预留请求的QoS需求参数系统的表述和分类,并且如果整个预留请求中只要有一个QoS需求无法得到满足,整个预留请求就会被拒绝,于是一些非功能性的QoS参数也会决定预留请求的成功与否,这将导致误拒率的上升.Al-Ali等学者从两个角度将网络计算服务的QoS参数分为5类[12],从服务性能来看,QoS参数可分为accountingQoS、serviceQoS和servicesecurity;从保证机制来看,QoS参数可分为serviceQoS和provisionalQoS,分别对应于硬QoS和软QoS.硬QoS参数是服务的必备属性,其取值不能改变;软QoS参数是服务的辅助属性,其取值可以在一定的范围内变化.针对这些问题,本文提出了跨数据中心的资源①http://www.fz-juelich.de/zam/RD/coop/ggf/graap/graap-Page3联合预留体系架构,该架构利用虚拟资源容器将同类资源虚拟化,适应网络资源的动态变化,并对QoS偏差距离的计算方法做了改进,资源预留协商采用了基于QoS偏差距离的算法减少预留请求的误拒率.最后,我们将所提出的联合预留体系架构部署到东南大学校园云计算平台中,实验结果表明,该架构在资源可获得性和服务性能都动态变化的情况下,可显著地提高预留请求的成功率,且具有更强的稳定性.2资源提前预留的生命周期一次成功的提前预留应该包括以下属性:时间帧、预留类型、QoS需求参数、预留标识、资源标识.时间帧定义了本次预留的开始时间和结束时间;预留类型规定了预留请求是简单预留请求还是联合预留请求,简单预留请求只需一个数据中心的资源即可完成,联合预留请求则需多个数据中心的资源协同完成.QoS需求参数描述了用户希望达到的服务水平.一旦预留请求成功,系统分配给本次预留以全局唯一的预留标识.资源标识描述了预留执行时所绑定的资源.在提前预留的整个生命周期内,一般存在如下9个状态①:请求预留协商、拒绝、预定、预定重协商、撤消、活跃、终止、完成、活跃重协商.图1描述了提前预留的状态转换过程.首先,用户发出资源预留请求,与系统进行预留协商.如果协商成功,预留请求转为预定状态,否则转为拒绝状态.处于预定状态的预留请求一旦等待至预留开始时间,则系统将其与资源进行绑定,预留转为活跃状态.如果在预留开始之前,用户需要改变预留请求QoS参数,则预留转入预定重协商状态.如果重协商成功,则回到预定状态.当然,在预留开始时间前,用户或系统也可以取消已经预定的预留请求,预留转为撤销状态.用户或系统若在预留结束时间前终止处于活跃状态的预留,则进入终止状态预留如果占用资源至完成时间则进入完成状态.与预定重协商状态类似,用户或系统在活跃重协商状态可以动态改变处于活跃状态预留的QoS参数.3跨数据中心的资源动态联合预留体系架构大型网络计算平台通常包含多个数据中心,每个数据中心有其特定的资源及资源共享策略.对于复杂的资源预留请求,单个数据中心的资源往往不能满足其需求,这就需要分布在多个数据中心内的资源相互协作完成.本文分别提出跨数据中心的联合资源预留的整体架构和单个数据中心的提前预留架构,联合资源预留整体架构为数据中心提前预留架构的接入提供了接口.图2描述了联合资源预留的总体架构,用户代理提交联合预留请求到联合预留接口,联合预留接口与全局信息中心进行交互,确定所需资源的种类及其所在的数据中心,将联合资源预留请求分解多个为简单预留请求,然后分配给多个数据中心.分配到各个数据中心的简单预留请求由如图3所示的单个数据中心内提前预留系统架构进行处理,各个简单预留完成后,联合预留接口负责组合各个数据中心的执行结果,反馈给用户代理.根据请求资源种类、信息中心进行资源发现和复杂预留请求分解不属于本文的研究焦点,本文聚焦在单个数据中心内提前预留系统架构如何屏蔽资源的动态性及如何为简单预留请求选择合适的逻辑资源.图3描述了单个数据中心内提前预留系统架构,该系统架构引入了虚拟资源容器(VirtualResourceContainer,VRC),它聚合同一个数据中心中相同种类的逻辑资源,形成功能性的虚拟资源.当Page4图3单个数据中心的提前预留体系架构接收到联合预留架构中的简单资源预留请求时,由虚拟资源容器的提前预留协商接口根据可用虚拟资源列表与用户代理进行提前预留协商,由图1状态图可知,若协商成功,则进入预定状态,完成协定设置后,构建提前预留队列.VRC还需负责维护提前预留队列,调度和重协商预留请求等功能.请求的预留开始时间到达时,虚拟资源容器通过逻辑资源选择管理器选择当前最合适的逻辑资源与预留请求进行绑定,提前预留过程进入活跃状态.数据中心中也存在物理资源的动态加入和退出,比如服务器的启动与关闭.而且,资源的存储和计算性能亦随负载变化而体现出时变性.数据中心内的物理资源通过网络计算平台资源设备层的封装,形成对应的逻辑资源,用于描述物理资源在网络计算环境中的服务功能和能力.物理资源本地负载的变化和允许动态加入和退出的策略使得相对应的逻辑资源的服务能力在网络环境下具有动态波动性.SNAP中的提前预留策略通过协商过程直接将任务与逻辑资源在未来的一段时间内绑定,当发现预留资源在预定时间段内不能满足任务的资源需求时,系统需要进行重协商或取消该预留请求.逻辑的动态波动性将导致系统重协商次数的增多,易增大系统不必要的开销,并大幅度降低预留的成功率.虚拟资源容器聚合不同功能的底层逻辑资源,当预留请求的开始时间到达时,虚拟资源容器选择合适的逻辑资源实现动态绑定,第5节详细讨论了逻辑资源选择算法.在如图3所示的系统架构中,虚拟资源容器起着最关键的作用,虚拟资源容器的性能直接决定着数据中心跨虚拟组织联合预留体系架构的总体性能.文献[13]利用排队论证明了包含多台独立计算机的计算资源池能够在保持相同服务质量(任务的平均等待时间不变)的前提下提高服务能力(每台计算机接受任务的速率增大).文献[13]的证明方法同样可以用于论证虚拟资源容器能在保证服务质量的同时提高服务能力,其证明过程如下.对于某一类资源的虚拟资源容器,假设预留请求的到达过程遵循泊松过程,不同资源的服务时间假定为相互独立的指数分布随机变量,一个资源虚拟容器就可以看作是一个M/M/c排队系统.M/M/c排队系统的平均等待时间Wqc可以通过式(1)得出:其中,a为负载比率,C(c,a)为虚拟容器中所有资源都不空闲的概率.如式(2)和(3)所示:Page5C(c,a)=由式(3)易知,给定c,排队时间Wqc随λ1的增大而增大.而给定λ1,排队时间Wqc会随c的增大而减小,即在资源服务时间指数分布不变的情况下(即μ1保持不变),若要不增加请求的排队时间,资源数量c需随请求到达速率λ1的增加而增加.文献[14]利用排队论进一步解释了Wqc和c之间的关系,说明虚拟资源容器只需聚合适中数量的资源就足以使平均等待时间维持在用户可以忍受的范围之内,从而不造成资源的浪费.因此,虚拟资源容器的引入可以在不影响整个跨数据中心联合预留体系架构的性能的前提下,减少资源性能动态波动所导致的预留失败率,虚拟资源容器的引入不会成为整个系统的性能瓶颈.4支持联合预留体系架构的SNAP协议扩展跨数据中心联合预留体系架构依然采用SNAP进行用户和资源提供者之间的预留协商和对SLA的生命周期管理,我们对SNAP做了两方面的扩展以支持本文所提出的联合预留体系架构:对SLA状图4扩展的SLA状态转换图4.2SLA元语言的扩展SNAP协议定义了SLA的元语言,包括针对RSLA的资源描述语言、针对TSLA的任务描述语言以及针对BSLA的绑定描述语言,SNAP将这3类元语言统一用四元组表示:〈I,c,tdead,a〉.其中I是SLA的类型标识符,c是用户代理的标识符,tdead是SLA的最终期限,a统一表示了SLA描述元语言.具体地,〈I,c,tdead,〈r〉R〉,〈I,c,tdead,〈j〉T〉和〈I,c,tdead,〈j〉B〉分别表示了3类元语言,其中资源态进行扩展以支持虚拟资源容器,对SLA元语言进行扩展以支持区分硬、软两类QoS参数.4.1SLA状态的扩展SNAP协议通过制定SLA完成资源提供者与用户之间的QoS协商,SNAP包括3种SLA:TSLA、RSLA和BSLA.TSLA用来协商活动或任务的需求,TSLA通过用户的资源需求来表征任务;RSLA用来协商资源的性能和消费的权限,RSLA通过资源的抽象服务能力来表征资源;BSLA将TSLA与RSLA相绑定,RSLA所规定的资源属性必须满足TSLA所规定的任务需求.在本文所提出的体系架构中,预留请求并非直接与资源相绑定,而是与虚拟资源容器相绑定,因此SNAP中所定义的3类SLA不能描述任务与虚拟资源容器之间的绑定,需要扩展SNAP定义新型的SLA来描述这种绑定.我们定义VBSLA来描述任务与虚拟资源容器之间的绑定.图4描述了扩展的SLA状态转换图,其中,S0,S1,S3,S4是SNAP原有的状态,S2是扩展的状态.状态S0表示SLA未被创建或已经执行完毕而终止或者被取消;状态S1表示一些TSLA和RSLA已经被创建,但尚未绑定,状态S2表示TSLA已经与虚拟资源容器绑定形成VBSLA,即完成了提前预留的协商,进入预定状态;状态S3表示提前预留执行时间到达,TSLA与RSLA成功绑定形成BSLA;状态S4表示请求任务调度成功,开始执行,进入运行状态.描述元语言是任务描述元语言的子集,于是,只要扩展资源描述元语言就可以使SNAP的SLA描述元语言达到区分硬、软QoS参数的目的.资源描述元语言定义了多种描述资源的方式,其中的类型集合(TypedSet)用来描述有一组属性的资源,能够较好反映出资源的QoS属性.例如,[x1bytes,x2byte/s]disk描述了容量为x1bytes,转速为x2byte/s的硬盘;[xbyte/s]bandwidth描述了传输速度为xbyte/s的带宽.为了支持硬、软QoS参数Page6的区分,可以添加属性H或S来实现,如[x1bytes,x2byte/s,H]disk描述了硬盘要求是硬QoS参数[xbyte/s,S]bandwidth描述了带宽要求是软QoS参数.在时间帧[t0,t1]的预留请求的资源需求可以表示为一段时间内的资源需求序列,该资源序列r[t0,t1]可以表示为r[t0,t1]=[TS[t0,t1]其中,TSi为资源类型集合,可以表示为其他有关SLA描述元语言,及协定建立和管理操作均参考SNAP所定义的规范[7].5单个数据中心内的逻辑资源选择算法由于目前的大部分资源预留协商策略都严格按照SLA所规定的QoS参数来选择逻辑资源,使得很多非功能性的QoS参数对预留请求成功与否起决定性的作用,以致很多预留请求因为软QoS参数的不满足被拒绝或需要进行重协商,这导致了误拒率的上升和资源管理系统性能的下降.本文提出基于QoS偏差距离的逻辑资源选择算法,该算法被部署到逻辑资源选择管理器上.在算法提出之前,本文针对目前在QoS偏差距离方面计算方法的不完善做了改进,使QoS偏差距离更确切地描述出SLA中所规定的QoS需求与资源提供的QoS参数之间的偏差程度.5.1QoS偏差距离的定义与计算QoS偏差距离(QoSDeviationDistance,QDD)用于衡量用户QoS的要求与资源提供的QoS参数之间的偏差程度,文献[15]提出了QoS偏差距离的计算公式:其中,犙={q1,q2,…,qm}为TSLA中描述所的用户QoS需求向量,犚={r1,r2,…,rm}为RSLA中描述的资源QoS属性向量.式(6)说明QoS偏差距离越小,资源提供的服务越符合用户的需求.服务网格中的QoS参数有硬QoS参数和软QoS参数之分,对于软QoS参数,偏差距离越小说明资源服务越符合用户的需求.但是,对于硬QoS参数,只要用户的QoS参数得不到满足,不管资源属性与用户的QoS参数之间的偏差程度,该用户请求都要被拒绝.所以,文献[15]中QoS偏差距离的计算方法在缺乏对不同类QoS参数的区分,存在一定的不足.为此,本文对QoS偏差距离的计算做了改进.应用集合表示为A={S1,S2,…,Sm}.SNAP针对每个服务制定相应的TSLA,每个服务看作是一组QoS参数的集合,即Si={q1,q2,…,ql}.网络计算平台资源层被抽象为一组逻辑资源的集合R={R1,R2,…,Rn},针对每个逻辑资源SNAP制定RSLA,每个逻辑资源也被看作是一组QoS参数的集合,即Ri={r1,r2,…,rl}.假设l个QoS参数中有k个硬QoS参数,即有l-k个软QoS参数.于是,TSLA和RSLA分别可以重新表示为Si={q1,q2,…,qk,qk+1,…,ql},Ri={r1,r2,…,rk,rk+1,…,rl}.QoS参数可以分为积极度量和消极度量两类[16].消极度量指其值越高,质量越低的QoS参数,如响应时间.积极度量指指其值越高,质量越高的QoS参数,如CPU份额、声誉等.我们定义特征值cj表示第j个QoS参数是否被满足.cj=1表示该QoS参数被满足,否则cj=0.ε=其中,hi表示服务Si的所有硬QoS参数是否被满足,hi=1表示,cj=1,1jk,即k个硬QoS参数都能被满足;否则,hi=0,该资源将无法满足这样的服务请求,所以资源属性与用户请求QoS之间的偏差距离为无穷大.只有在服务请求所有硬QoS参数都被满足的前提下,利用偏差距离表示服务被满足的程度才有意义.5.2基于QoS偏差距离的逻辑资源选择算法跨数据中心的联合预留体系架构在接收到用户预留请求后,将复杂预留请求分解为简单预留请求,通过与信息中心的交互发现服务,并定位到某个数据中心,由单个数据中心的提前预留体系架构对简单资源预留请求进行处理,它通过逻辑资源选择管理器为预留队列中到达预留时间的简单预留请求选择当前时间最合适的逻辑资源,实现资源预留请求与逻辑资源的最终绑定.本文提出了基于QoS偏差距离的逻辑资源选择算法,试图选择与预留实体之间偏差距离尽量小逻辑资源来完成该预留请求,算法伪代码如下所示.Page7算法1.基于QoS偏差距离的逻辑资源选择算法.输入:AR_Queue={S1,S2,…,Sm},输出:Createareservationmatrix犚犲mnforeachservice1.WHILE(AR_Queueisnotnull)DO2.Si←AR_Queue.out();3.IF(tcur=tstart)THEN4.FORj←1TOn5.ComputingεijbyEq.(10);6.犙犱mn[i][j]=εij;7.ENDFOR8.Sortresourcesaccordingtoεijinnon-decreasing9.FORj←1TOn10.IF(∩k=1,2,…,i-1(tkj<tstart11.犚犲mn[i][j]=tstart12.break;13.ENDIF14.ENDFOR15.ENDIF根据资源提前预留的属性,预留请求可以被抽象为三元组〈tstart源容器的预留请求队列,虚拟资源容器为每个预留请求分配一个ID,tstart预留的时间段,犙i为预留请求的QoS需求向量,犙i包含了硬QoS和软QoS参数.一次成功的预留过程也可以抽象为三元组〈R_ID,AR_ID,LR_ID,T〉,R_ID为预留过程的标识,AR_ID为预留请求的ID,LR_ID为逻辑资源的ID,T为预留过程的时间帧,它包括预留的开始时间、结束时间及占用资源时间段.算法的输入是预留请求队列AR_Queue和逻辑资源集合LR_Set,输出是预留矩阵犚犲mn,它描述了预留请求与逻辑资源之间的授权关系.预留请求队列根据请求的开始时间排序,当预留实体到达开始时刻时,算法首先计算该请求与所有逻辑资源的QoS偏差距离,保存在矩阵犙犱mn中,然后算法选择QoS偏差距离尽量小、并处于空闲状态的逻辑资源来完成该预留请求.预留矩阵犚犲mn记录了预留过程的结束时间,它和预留请求向量一起记录了预留过程的时间帧向量.值得注意的是,当有一预留请求到达时,可能存在多个资源同时满足最小偏差距离,本文提出的逻辑资源选择算法将随机选择资源匹配预留请求,这可能导致后到的预留请求无法获得最优的资源,从而导致全局角度的非最优.为此,我们设计以下两种方案作为逻辑资源选择算法的补充:(1)预留请求批处理和优化,对给定时间点以前到达的预留请求批处理,根据偏差距离给出整体最优的资源分配方案;(2)重协商,设当前到达的预留请求Q2的QoS需求无法得到满足,而能满足Q2的资源R1已经预留给先到达的请求Q1,若比具有较低的QoS需求,且能找到资源R2满足Q1,则采用重协商方法重新为Q1和Q2分配合适资源.6实验结果本文实验在东南大学校园云计算平台SEUCloud(SoutheastUniversityCloud)环境下完成.SEUCloud是在学校多个独立的数据中心基础上,基于虚拟化技术以及相应开源资源管理平台OpenStack,为AMS-02实验海量数据的处理而搭建的一个统一云计算环境.AMS实验是国际空间站上唯一大型物理实验,是人类第一次在太空中精密地测量高能量带电原子和粒子的实验.其目的是为寻找由反物质所组成的宇宙和暗物质的来源以及测量宇宙线的来源[17].AMS-02将在国际空间站运行10年~15年,大量的原始数据(每年约50TB)和蒙特卡罗仿真(MonteCarlo,MC)数据(每年约200TB)需要在地面数据中心SOC(ScienceOperationsCenter)进行传输、存储、处理、计算和分析,SEUGrid是SOC的重要组成部分.由于MC仿真、原始数据重建以及物理分析等同类子任务之间数据依赖性差,是典型的参数扫描型应用,适合于在网络计算平台上进行并行计算.为了很好的保证诸如MC仿真此类大计算量、大数据量科学计算任务的服务质量,往往单一的云计算平台无法满足如此巨大的资源需求,此时就需要由多个云计算数据中心共同提供资源协作完成.目前整个SEUCloud环境中CPU总量达3640个,峰值速度达37.8万亿次,存储总量近540TB.本文提出的跨数据中心的联合预留体系架构被部署到SEUCloud环境中以保障用户的QoS需求,我们利用SEUCloud的部分资源进行单个数据中心的提前预留体系架构性能分析实验.6.1虚拟资源容器的性能分析我们首先测试虚拟资源容器的性能,为了模拟资源的动态性,我们通过随机产生不同规模的HPLPage8(HighPerformanceLinpack)测试作业来产生变化的本地负载,本地负载越大该资源在SEUCloud上表现出的服务能力就越差,当本地负载达到满负荷时,就视为该资源不可获得,相当于退出SEUCloud,并取消该时间段内已经绑定好的预留请求,且不再绑定任何新的预留请求.因此,总的工作负载就由一段时间内的HPL测试作业的规模和数量决定,工作负载通过式(8)计算:其中Ni为某HPL作业的问题规模,Linpack测试中规定2/3×N3作业的浮点运算次数,分母为CPU总的理论浮点运算次数.下面通过两个实验说明引入虚拟资源容器所带来的预留机制在性能上的改进,我们选用预留请求成功率作为比较目标.(1)实验1.考察了使用和未使用虚拟资源容器时系统工作负载的变化对预留成功率的影响.我们分别在使用和未使用虚拟资源容器两种情形下为MC仿真作业预留资源,预留请求的到达过程遵循Poisson过程,两个连续预留请求到达的间隔时间便服从指数分布,取平均到达速率λ=2.8.结果如图5所示:资源预留请求成功率随着工作负载的增大而减小,但是使用虚拟资源容器时预留请求成功率减小幅度较小,并始终优于未使用虚拟资源容器时的情况.当负载达到0.8以上,即负载偏重时,两种情形的预留请求成功率下降速度明显加快,但是使用虚拟资源容器时的下降速度明显小于未使用时的情况,可见虚拟资源容器增强了系统的稳定性.图5工作负载变化时的资源预留请求成功率对比图(2)实验2.比较了使用和未使用虚拟资源容器两种情形在同样的负载变化影响下的预留请求成功率,我们在24h内提交约2400个MC仿真作业的预留请求,即平均每30min提交50个,并对每30min内的预留请求成功率进行采样.结果如图6所示:使用虚拟资源容器时一天内的平均预留成功率可达到92%,而未使用虚拟资源容器时只有66%.由于资源的动态波动性,24h内的预留请求成功率也在一定范围内动态波动,使用虚拟资源容器的曲线仅在12.9%的范围内波动,未使用虚拟资源容器的曲线波动范围达到35.3%.以上两个实验结果表明,在资源可获得性和服务性能都动态变化情况下,虚拟资源容器的引入可以使联合资源预留体系架构显著地提高预留请求的成功率,并且具有更强的稳定性.6.2基于QoS偏差距离的逻辑资源选择算法性能分析SNAP协议根据3类SLA进行资源提前预留,只有当TSLA中所有的QoS参数全部被RSLA满足时,才将TSLA与RSLA绑定,并建立BSLA.在本文的跨虚拟组织联合预留架构中,虚拟资源容器首先建立VBSLA,VBSLA根据QoS偏差距离选择逻辑资源建立BSLA,算法优先选择QoS偏差距离尽量小的逻辑资源完成预留请求.我们在预留体系架构的逻辑资源选择管理器分别部署基于SLA和QDD的两种逻辑资源选择算法,在3种不同预留请求到达速率的场景下分别比较了预留请求成功率随着负载变化的性能,3种场景的平均到达速率分别为λ1=2.7,λ2=3,λ3=3.5.结果如图7所示:在预留请求到达速率最低的场景1中,两种算法的预留请求成功率差不多;随着预留请求到达速率的增大,两种算法的预留请求成功率的差距明显增大.接下来,我们观察预留请求成功率和资源利用率随着请求到达间隔时间变化的情况,设工作负载Page9图7资源预留请求成功率随系统负载变化比较图σ=0.3,请求到达间隔时间从500s~20s范围内逐步减小,图8和图9分别比较基于SLA和QDD的两种逻辑资源选择算法在请求成功率和资源利用率上的比较结果.可以看出,当请求到达间隔时间较长时,即单位时间内预留请求数量较少,两种算法性能接近;随着请求到达间隔时间的逐步减小,基于QDD的算法优势愈发明显,表现在具有较高的预留请求成功率和资源利用率.图8资源预留请求成功率随请求到达时间变化比较图图9资源利用率随请求到达时间变化比较图本文提出的基于QDD的算法明显高于SNAP协议中基于SLA的算法,其原因在于基于SLA的选择算法没有考虑QoS参数之间的差别,描述服务辅助属性的软QoS也可能决定预留请求的成功与否,导致了系统的误拒率偏高.而基于QDD的选择算法只拒绝硬QoS参数不能被满足的资源预留请求.另外,在资源负载变重,预留请求数量增多时,比如各个场景的重负载状态、及预留请求达到率较高的场景3,两种算法所导致的预留请求成功率的差距变大,可见基于QDD的逻辑资源选择算法在负载相对过重,资源相对紧张的情况下能表现出更强的优越性.7结束语本文提出了跨数据中心的资源联合预留体系架构,引入虚拟资源容器以适应资源的动态变化,并对QoS偏差距离的计算方法做了改进,资源预留协商采用了基于QoS偏差距离的策略以减少预留请求的误拒率.最后,在东南大学校园云计算平台上对单个数据中心的提前预留体系架构进行了性能分析,实验结果表明,该架构在可获得性和服务性能都动态变化情况下,可显著地提高预留请求的成功率,且具有更强的稳定性.
