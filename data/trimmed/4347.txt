Page1稀疏局部保持投影郑忠龙1)黄小巧1)贾竳1)杨杰2)1)(浙江师范大学计算机系浙江金华321004)2)(上海交通大学图像处理与模式识别研究所上海200040)摘要LASSO(LeastAbsoluteShrinkageandSelectionOperator)是1范数和2范数混合学习的一种理论框架,基于LASSO提出了局部保持投影的稀疏回归算法SpLPP及其广义的正则化形式RSpLPP,并从理论上证明了所提模型的收敛性及求解算法,给出了算法的复杂性分析.所提算法同时具有特征选择、降维的特性,在有监督学习、无监督学习两种任务情况下,都可以应用该算法.在人工数据集和真实数据集上进行的大量仿真实验,取得了较好的结果,证明了所提算法的有效性.关键词稀疏学习;局部保持投影;流行学习;正则化1引言维数灾难(CurseofDimensionality)是模式识别、机器学习等领域的难题之一.线性降维方法因其简单、有效而备受青睐.主成份分析PCA(PrincipalComponentAnalysis)是一种无监督线性降维方法,广泛应用于模式分类、可视化、数据压缩等[1].PCA的本质是提取主成分,即数据的主要散布方向.线性判别分析LDA(LinearDiscriminantAnalysis)是一种有监督线性降维方法,其本质是提取最佳判别方向上的低维映射[2].局部保持投影LPP(LocalityPreservingProjection)是拉普拉斯特征映射LE(LaplacianEigenMap)的线性逼近,当应用于人脸识别时,也被称为LaplacianFaces[3].LPP的本质是考虑数据集的流形结构,并且在嵌入空间中保留数Page2据之间的局部特性.在图像检索和人脸识别领域,LPP已经显示出了它的优越性.以上三种方法所得到的低维嵌入结果是所有原始特征的线性组合,因此,学习后的结果在直观意义上的解释较为困难(尤其是负系数).换句话说,降维后的子空间中的数据应是最富信息特征的线性组合.生理和心理方面的研究结果表明:人类大脑对物体的表征可能是基于稀疏性的[4].在信号处理和信息理论领域,稀疏表征已经取得了令人瞩目的新进展[5-10].Zou等人[11]提出了稀疏PCA,该方法是通过在传统的主成份分析上施加1约束而得到的.Wu等人在一个相似框架下提出了稀疏LDA[12-13].需要注意的是,稀疏LDA算法只适用于两类问题.近年来,稀疏回归分析得到了研究人员的广泛关注,如稀疏回归的优化算法[14]、稀疏回归模型的构建[15-16]以及稀疏回归的应用研究[17-19].本文通过对传统的LPP算法加入1约束,进而提出了基于LASSO(LeastAbsoluteShrinkageandSelectionOperator)回归框架的稀疏局部保持投影算法SpLPP(SparseLocalityPreservingProjection),并拓展了其正则化形式RSpLPP(RegularizedSpLPP).SpLPP把局部保持和稀疏性结合起来,同时将降维、特征选择综合在一种分析中.并且,SpLPP算法可应用于有监督和无监督两种学习模式.本文第2节简短回顾LPP算法;第3节详细描述SpLPP算法及其正则化形式RSpLPP;第4节给出多个实验结果以证明所提算法的有效性;最后,第5节对本文内容进行总结并对未来工作进行展望.2LPP简介给定原始数据集{x1,…,xn}∈[x1,…,xn],则犡是一个m×n大小的矩阵.令犛表示由所有成对样本点测度构成的相似矩阵,则LPP最佳投影可通过以下问题得到其中,犔=犇-犛是图拉普拉斯矩阵,对角阵犇ii=∑j犛ij反映了xi附近的局部样本密度.LPP算法中的对称相似度矩阵犛ij定义为犛ij=exp(-xi-xj其中,ε>0是局部邻域的半径.这里的犛ij实际上可看做是惩罚因子[20],即如果相邻的xi和xj被映射后的yi和yj相距很远,则LPP的目标函数会导致一个较大的惩罚因子.因此,目标函数的最小化保证如果xi和xj距离很近,那么映射后yi和yj距离也很近.此最优化问题最终可转化为广义特征值问题:投影矩阵狑由式(3)的最小特征值对应的特征向量组成.3稀疏局部保持投影算法SpLPP式(3)可改写为令犕犇=犡犇犡T且犕犔=犡犔犡T,则式(4)等价于:因而可以得出结论:投影矩阵狑由式(5)的最大特征值对应的特征向量组成.为了以下讨论的方便,引入n×m矩阵:则有需要指出的是,因为图拉普拉斯矩阵犔和对角阵犇都是对称和半正定的,所以矩阵犕犇和犕犔也是对称和半正定的.需要指出的是,式(6)中犔1犇12-犛12=(犇-犛)1的犔13.1LPP的回归分析在介绍SpLPP算法之前,首先在回归框架下重新讨论式(5)中的广义特征值问题.定理1.假定犕犔是正定的且它的Cholesky分解是犕犔=犌犔犌T角矩阵.令Φ={1,…,k}表示式(5)的前k个最大特征值λ1λ2…λk对应的特征向量.令犘m×k={p1,…,pk},犙m×k={q1,…,qk}.对于λ>0,令^犘和^犙是以下回归方程的解:min犘,犙∑ns.t.犘T犘=犐其中犉犇,i是式(6)中定义的犉犇的第i行.则^犙的所有k列与Φ所张成的子空间是相同的.证明.分别给定犘和犙,则定理1可以从以下3个步骤加以证明.如无特别说明,以下矩阵范数为Frobenius范数.(1)如果给定犘,则犙的求解是一个回归问题.Page3因为犘是列正交归一的,所以存在一个矩阵犘⊥使得[犘,犘⊥]是一个m×m的正交归一矩阵.式(8)的第1项可改写为犉犇犌-T犔-犉犇犙犘T2=犉犇犌-T犔[犘,犘⊥]-犉犇犙犘T[犘,犘⊥]2=犉犇犌-T犔犘-犉犇犙2+犉犇犌-T犔犘⊥=∑k那么,当犘给定后,可通过脊回归计算犙:min犙∑ki=1fj关于狇j的偏导数为=fj狇j{(犉犇犌-T犔pj-犉犇狇j)T(犉犇犌-T犔pj-犉犇狇j)+λ狇T=2(犉T犇犉犇狇j+λ犕犔狇j-犉T令fj或(2)如果给定犙,那么犘的求解是一个Procrustes问题[23].因为∑ni=1=tr{(犉犇犌-T犔-犉犇犙犘T)(犉犇犌-T犔-犉犇犙犘T)T}=tr{犉犇犌-T犔犌-1犔犉T2tr{犙T犉T约束条件为犘T犘=犐.故可得出结论:使式(8)最小化的犘等价于最大化如下问题:将式(12b)中的^犙代入式(14)可得到max犘tr{犘T犌-1犔犕犇(犕犇+λ犕犔)-1犕犇犌-T犔犘}(15)进一步,有犌-1犔犕犇(犕犇+λ犕犔)-1犕犇犌-T犔=犌-1犔犕犇犌-T犔(犌-1犔犕犇犌-T犔+λ犐)-1犌-1犔犕犇犌-T犔(16)令=犌-1犔犕犇犌-T犔=犈Λ犈T,其中犈={η1,…,ηm}由的m个特征值对应的特征向量构成,Λ是一个由特征值构成的m×m对角阵.从式(16)可以得出结论:犈的所有列也是矩阵犌-1犔犕犇(犕犇+λ犕犔)-1犕犇犌-T犔的特征值对应的特征向量.根据附录中的引理1,式(15)的最优解^犘满足其中犃是任意的m×m正交阵.可得^犙=犌-T犔(犌-1犔犕犇犌-T犔+λ犐)-1犌-1犔犕犇犌-T犔^犘i=1由于式(5)广义特征值问题的特征值对应的特征向量即是Φ=犌-T犔犈的所有列,故由式(18)可知:所张成的线性空间与Φ是相同的.证毕.事实上,在犘T犘=犐列正交的约束下,犘可以通过对式(14)进行SVD分解:令^犘=犝犞T即可[12].3.2稀疏约束信号的稀疏表征在信息论等相关领域已引起研究人员极大的兴趣[5,24-27].稀疏表征通过施加最小化1约束得到,可通过凸优化进行求解.如经典的LASSO算法,通过施加1约束,可得到稀疏回归系数[28].受此启发,定理1中^犙的LASSO估计可通过如下准则得到min犘,犙∑k约束条件为犘T犘=犐,不同的基函数狇j允许有不同的惩罚因子γj.为简化起见,可将所有的γj设为相同的值γ.λ和γ是非负常数.式(20)的凸优化问题是广义ridge回归与LASSO的组合,本文称之为稀疏局部保持投影(SparseLocalityPreservingProjection,SpLPP).LASSO算法可以看做是SpLPP算法的一个特例,即λ=0的情况.m>n时,选择λ>0,则SpLPP可包含回归模型中的所有变量,进而克服LASSO的局限性.SpLPP另一个优点是它的群组特性,即一旦高度相关的一组变量中的一个被选中,则整个组都会被选中.相反,LASSO仅仅选择群组变量中的一个.此外,式(20)中的二次项λ狇T与ElasticNet是不同的[29].对于有限图,犕犔近似于紧凑的黎曼流形上的LaplaceBeltrami算子.LaplaceBeltrami算子是黎曼测度和图的邻接关系的度量.LE算法是通过最优化如下问题计算最优投影方向[20]:其中,L是流形上的LaplaceBeltrami算子,因而最优的f是L的特征函数.假设f是线性的,即f=狑Tx,那么,上式的积分Page4可通过狑T犡犔犡T狑=狑T犕犔狑离散化逼近,这与式(20)的中间项具有相同的形式.SpLPP算法的本质是在LASSO约束下求解保持局部特性的最佳投影.式(20)可通过交替迭代犘和犙进行求解:(1)固定犙,当1约束和式(20)的中间二次项被忽略时,问题等价于式(14)或(15),故可得犌-1犔犕犇犙=犝犇犞T,进而可令^犘=犝犞T.(2)固定犘,令Yj=犉犇犌-T犔pj,则可通过如下k个独立的优化问题求解犙:min狇j结合Cholesky分解犕犔=犌犔犌T其中,^犢j=(犢T的LASSO形式,已有高效的实现算法[30-32].SpLPP算法详细的实现步骤如算法1所示.算法1.SpLPP算法.1.给定数据集犡m×n=[x1,…,xn],根据k近邻规则或ε规则构造其邻接图;2.根据式(2)选择相似度矩阵犛;3.根据式(6)和式(7)分别计算犕犇,犕犔,犉犇,犉犔;4.计算下三角矩阵犌犔使其满足Cholesky分解犕犔=犔;5.计算经典的LASSO问题犌犔犌T其中,^犢j=(犢T6.计算SVD问题,犌-1犔犕犇犙=犝犇犞T,并令^犘=犝犞T;7.重复步5和步6直到收敛.3.3正则化形式RSpLPP在小样本问题中,式(5)中的犕犔经常会是奇异的,所以SpLPP算法不能够直接应用.正则化技术是处理奇异性的一种有效方法[33-35].因此,当犕犔奇异时,可施加正则化约束^犕犔=犕犔+c0犐m,其中c0>0,犐m为大小为m×m的单位阵.通过与3.1节与3.2节类似的推导,可得出结论:广义特征值问题i=1的前k个最大的特征值对应的特征向量{q1,…,qk}可通过如下回归方程求解:min犘,犙∑ns.t.犘T犘=犐进一步,犙的LASSO解可通过求解如下最优化问题得到min犘,犙∑ks.t.犘T犘=犐{犉犇犌-T犔pj-犉犇狇ji=1RSpLPP算法如算法2所示.算法2.RSpLPP算法.1.给定数据集犡m×n=[x1,…,xn],根据k近邻规则或2.根据式(2)选择相似度矩阵犛;3.根据式(6)和式(7)分别计算犕犇,犕犔,犉犇,犉犔;4.计算下三角矩阵犌犔使其满足Cholesky分解(犕犔+ε规则构造其邻接图;c0犐m)=犌犔犌T5.计算经典的LASSO问题其中,^犢j=(犢T6.计算SVD问题,犌-1犔犕犇犙=犝犇犞T,并令^犘=犝犞T;7.重复步5和步6直到收敛.3.4SpLPP算法的复杂度分析根据算法1可以分析一下SpLPP算法的复杂度.从算法1中可以看出,SpLPP算法主要涉及如下几个基本操作:邻接图矩阵的构造、广义特征方程的求解(采用SVD求解)、矩阵的Cholesky分解和L1范数的求解.原始的输入矩阵犡∈犚m×n,以上4种基本操作的复杂度分别为Ο(n2)、Ο(m×n2)、Ο(m3)、Ο(m3).对于高维小样本问题,通常mn,因而,SpLPP算法的复杂度为Ο(m3).4实验结果通过两类实验来测试SpLPP算法的性能:一类实验测试其表征能力;另一类实验测试其分类能力.此外,与其他一些相关方法进行了比较,如PCA[1]、LDA[2]、LPP[3]、SPCA[11]和SDA[36].需要说明的是,SpLPP与LPP在构建邻接图时所采用的参数是相同的.目前,对于L1范数的求解主要有:基追踪算法[37](BasisPursuit,BP)、迭代再加权最小二乘算法[38](IterativelyRe-weightedLeastSquares,IRLS)、平滑0范数算法[39](SmoothL0norm,SL0)以及贪婪算法[40](如OMP、ROMP、SP、COSAMP等).鉴于BP方法所需观测点少,精度高,可以保证解的L1范数最小,因而本文采用的是BP算法.需要指出的是,本文部分理论研究内容在生物基因数据上亦取得了较好的结果[41].仿真环境为WinXP32位,Matlab2011(利用了L1magic工具包),DellT7500图形图像工作站.4.1基于表征的实验在两个数据集上测试SpLPP的表征能力:一个是4.1.1节中的合成数据集;另一个是4.1.2节中的Frey人脸数据集.4.1.1合成数据集所生成的三维数据集为Page5其中,标量a1,a2和a3分别均匀分布于[0,1],[5,10]和[10,20],b1,b2和b3分别均匀分布于区间[20,30],[30,40]和[40,50],c1,c2和c3是常数.每图13维合成数据集图2训练集在2维空间中的嵌入结果图3相应测试集在2维空间中的嵌入结果个fi(t)由200个点组成.所生产的一组训练集如图1(a)所示,独立生产的一组测试集如图1(b)所示.应用SpLPP算法,将训练集与测试集数据降维至2维,出于可视化的目的,2维嵌入结果如图2和图3所示.从嵌入结果看,SpLPP算法相比其他算法的优点在于同时保留了局部结构与判别能力.Page64.1.2Frey人脸库Frey人脸库包含1965幅人脸图像,这1965幅人脸图像取自一个小型视频的连序帧[42].每幅图像的大小是28×20像素,256灰度级.为测试不同算法的鲁棒性,在图像数据中加入15%的高斯白噪声.所有算法将人脸图像数据嵌入2维空间,有噪声和无噪声的嵌入结果如图4所示.因为是在无监督学习,所以这里只考虑了PCA,LPP和SpLPP.由图4可以看出,如果局部结构受到噪声的影响,那图4Frey人脸数据集的2维嵌入结果(样本在无噪声和有噪声情况下的拓扑关系)4.2基于分类的实验分类实验在两个真实的数据集上进行.4.2.1节中的虹膜数据集来自UCI,可从http://www.ics.uci.edu/mlearn/~MLRepository.html下载,由3个类别150个样本组成(每类50个样本).4.2.2节中的USPS数据集包括大小为16×16的0~9手写体数字的灰度图像,http://www.cs.toronto.edu/~roweis/data.html下载,每类数据包含1100个样本.么,LPP可能难以发现数据集的内蕴结构.由于SpLPP施加了1约束,它比原始的LPP更加鲁棒,加入噪声后,如图5所示,人脸图像仍近似被分为两部分:一部分是开嘴的人脸图像,另一部分是闭嘴的人脸图像.这是因为SpLPP在低维嵌入时保留了局部结构,所以强调了数据中的自然聚类.需要指出的是,PCA对于噪声也是鲁棒的,但是PCA所提取的是数据集的整体特征而难以保留局部拓扑结构.4.2.1UCI数据集随机选择每类中的一半样本作为训练集,另一半作为测试集.所有的算法将数据投影到2维空间.为简单起见,采用最近邻分类器(NearestNeighborClassifier,NNC).共计50次随机实验,平均实验结果如表1所示.算法正确率PCA95.11±1.28SDA96.30±0.87SPCA95.00±1.06LPP95.39±0.90LDA95.39±0.92SpLPP97.84±0.744.2.2USPS数据集在USPS数据集上的实验,采用文献[43]中的10-fold交叉验证方法.表2总结了由不同算法得到的平均正确率及对应的最佳维数.分类器仍采用NNC.算法维数正确率算法维数正确率PCA8381.87±2.13SDA987.08±1.57SPCA4185.13±1.77LPP6885.42±1.74LDA979.25±1.76SpLPP3789.19±1.40Page7如表2所示,在USPS数据集上,本文所提算法具有一定的优势.需要说明的是,LDA算法效果较差,本文认为主要是其可得到的嵌入维数不足导致的.在USPS数据集上的另一个实验是针对SpLPP算法,采用不同的分类器测试其分类性能.3种分类器:NNC,最小均值分类器(NearestCentroidClassifier,NCC)和支持向量机(SupportVectorMachine,SVM),图6所示为平均测试错分率.3个分类器的错分率趋势都是先下降后上升.SVM在有少量特征的情况下具有最佳的分类性能.5展望在回归框架下对LPP算法进行了重新分析,通过施加稀疏约束,提出了稀疏局部保持投影算法SpLPP.该算法将降维、特征选择统一到一种分析中,并可应用于无监督学习和有监督学习.进一步的工作是考虑SpLPP在半监督学习中的应用.在许多实际情况中,容易得到大量未标记的样本,如何利用这些未标记的样本去发现数据的流形结构,从而提高表征性能或分类准确率,将是一件很有意义的工作.
