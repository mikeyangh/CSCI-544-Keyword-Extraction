Page1一种面向众核架构的数据流编译框架魏海涛1)秦明康1)于俊清1),2)范东睿3)1)(华中科技大学计算机科学与技术学院武汉430074)2)(华中科技大学网络与计算中心武汉430074)3)(中国科学院计算技术研究所计算机体系结构国家重点实验室北京100190)摘要数据流编程模型将程序设计与媒体处理相结合,已大量应用到各个领域.众核处理器已经成为主流和工业标准,如何利用众核架构的特性来提高流应用执行性能已成为目前研究工作的一大难点.文中提出了一个高效的流编译框架来优化流应用的执行,该框架包含3个优化策略:设计一个最优的软件流水调度方法;提出一个高效的数据存储分配算法;并采用合理的众核间的映射策略,减小通信以及同步的开销.文中在Godson-T上实现了该编译器框架,实验结果表明,该方法比优化前有较大性能改进.关键词编译框架;数据流程序;众核处理器;软件流水;并行1引言近年来,众核架构已经成为处理器设计的主流.各个研究机构和芯片厂商推出了相应的处理器原型.如MIT的Raw[1]有16个核,中国科学院计算技术研究所的Godson-T[2]有64个核,Intel的SCC[3]有48个核.众核虽然具有高效的计算能力,但是也带来了复杂的存储和通信机制等问题,增加了编程的难度.数据流编程(DataFlowProgramming)模型将程序设计与多媒体处理相结合,在简化了编程的同时,为编译器在众核架构下的并行性优化提供了极大的可能性,受到了广泛的关注.软件流水调度(SoftwarePipeliningSchedule)充分地利用了数据流程序中存在的并行性,使不同执行阶段的指令可以在流水线中并发地执行,提高了程序的执行性能,并且可以通过降低每次迭代的时间来优化软件流水调度的执行性能.然而,处理器核、通信带宽和片上存储等系统资源的局限性会影响软件流水调度的性能,本文将针对这一问题,提出一种高效的优化方法.本文基于Godson-T众核架构提出了一种针对数据流程序的编译器框架.该框架包括3个组成部分:前端编译模块、优化模块和代码生成模块.我们选择DFBrook[4]为输入语言,通过该编译框架生成在Godson-T上可编译执行的软件流水代码.DF-Brook在语义上对Brook语言进行了数据流扩展,相对于传统语言具有更好的数据并行性和运算密集图1Godson-T的整体架构及每个小核的结构[2]为了确保L1cache与L2cache间的数据一致性,Godson-T采用了区域cache一致性协议;为了性,在图形图像处理,网络服务和其他媒体处理领域具有较强的适用性.本文的主要贡献有以下4点:(1)提出了一个有效的针对众核架构的数据流编译器框架;(2)基于软件流水调度,提出了混合分配SPM(Scratched-PadMemory)和主存的存储分配算法并对特定的数据拷贝操作进行消除,提高了数据访问速度;(3)提出了一种虚拟核向物理核映射的算法,进一步减小通信开销和同步开销;(4)在Godson-T上实现了该编译器框架并通过实验来验证该框架的有效性.本文第2节介绍同步数据流模型和Godson-T的体系结构;第3节讨论软件流水调度,混合存储优化和通信优化;第4节介绍实验方法和结果;第5节介绍相关工作;第6节对全文进行总结.2Godson-T体系架构与流编程模型2.1Godson-T体系架构Godson-T是中国科学院计算技术研究所正在研究开发的片上众核系统,它主要是面向大规模的并行处理的众核结构.它的提出是为了解决片上多/众核架构下数据在L2cache下访问延迟和存储容量利用率的问题.Godson-T的整体的架构由64个RISC计算节点构成,每个计算节点都有自己私有的计算与访存部件以及L1cache,所有计算单元共享L2cache,另外,计算节点都有作为自己协处理器存在的DataTransferAgent(DTA),它类似于DMA用来传输数据.同步管理的方便,Godson-T包含有专用的同步管理器.这些协议和部件优化了Godson-T的系统架构.Page3本文用Godson-T模拟器作为实验平台,在该平台上Godson-T的核上浮点加操作、整型加操作以及数据包经路由的延迟分别为4cycle、2cycle和2cycle.2.2流编程模型流编程模型提供了一种开发众核架构下并行性程序的可行方法.在本文中,我们重点关注基于同步数据流(SynchronousDataflow,SDF)的流编程模型,该模型中每个结点代表一个独立的计算单元(也称actor),拥有自己独立的指令与地址空间,每条边代表actor间的数据传输,通过通信管道实现.SDF编程模型将存在并行的actors和核间的数据通信暴露给编程人员,为编译器优化提供了新的机遇和挑战.actor的执行靠数据驱动,只要actor的输入端口有足够的数据消耗,它将不停地重复执行并产生数据到输出端口.actor每次执行消耗的数据量称为消耗率,同样地,actor每次执行产生的数据量称为产生率.在SDF模型中,actor有两种类型,有状态的和无状态的.一个有状态的actor根据上次执行的状态来决定执行之后的状态.一个无状态的actor不关心上次执行的状态,因此,无状态的actor允许我们复制一个无状态的actor.利用这个特性我们可以像之前工作[5]中讨论的那样,极大地扩大应用程序的并行度.3众核架构下的流编译框架本节介绍SCMC(StreamCompilationforMany-Core)的基本框架,其编译框架如图2所示.我们主要从计算能力,存储性能和网络通信等3个方面来进行性能的优化,目标是获得使流程序具有最大吞吐率的软件流水调度.SCMC编译框架的输入是以DFBrook语言编写的流程序,经过编译产生的源文件可以在Godson-T上运行.我们的优化过程包括3个阶段:软件流水调度阶段、存储访问优化阶段和通信优化阶段.所有优化过程是在源流程序经过前端解析之后生成SDF图进行的.优化过程的第1阶段是软件流水调度过程.根据各个actor的任务计算量,将所有的actor均衡地划分到众核上,然后根据划分的结果形成一个软件流水调度方案.在这个阶段我们假定每个核的存储容量为无限大且所处的网络环境是完全对等的.这样做的目的是为了平衡各个核上的计算量,同时将通信和计算重叠,让通信过程隐藏在计算之中,最大化软件流水的理论吞吐率.第2阶段为存储性能的优化,根据众核架构下的层次性存储结构,不仅要高效地分配存储资源,而且要消除冗余的数据拷贝操作.第3阶段为核间映射阶段,把初始划分后的任务映射到各个物理核上,在映射的过程中根据众核的拓扑结构和通信的模型进行优化,减小通信的开销.最后SCMC根据优化结果生成Godson-T上的源代码,该代码经编译器编译链接后生成在Godson-T上可运行的执行文件.3.1软件流水调度3.1.1任务划分任务划分的目的是为了使众核的负载均衡,核间的通信与计算更好的重叠,从而构造一条具有最大吞吐率的软件流水调度.在本节中,我们将任务划分问题规划化为一个整数线性规划问题(IntegerLinearProgramming,ILP).采用传统的流水线启动间隔(InitiationInterval,II)来衡量软件流水调度的性能.II被定义为相邻两次循环迭代(任务)进入流水线的时间间隔[6],所需要的最小启动II越小表示流水线的吞吐率越大.ILP目标就是使II的值最小,即使流水线的吞吐率最大.本实验中,我们假定每个处理核的存储容量是无限大,每两个核间的数据传输延时是相同的,并且所有的数据都分配在SPM上,而且所有核间的操作都通过DTA操作完成.对SDF图G=(V,E),其中,V表示图中的计算结点,E表示图中两个计算结点的通信边.用狉表示计算结点重复执行次数的向Page4量,对于任意v∈V,狉v表示稳定状态调度中结点v的重复执行的次数.PC代表众核架构下处理核的个数,用0,1,…,Pmax-1来表示.通过任务划分,确定每个actor执行时所在的核以及其在流水线中的执行阶段.本实验中,定义0-1变量av,p来表示结点v是否在处理核p上.av,p为1表示v在处理核p上,否则不在处理核p上.对处理核我们有如下限制.等式(1)保证每个actor必须分配到一个处理核上,且只能分配到一个处理核上.本实验没有考虑数据之间的依赖性,当存在数据依赖的结点分到两个不同的处理核上时,需要增加一个DTA操作以完成它们之间的数据传输.这里,我们规定由目的actor所在的处理核发起DTA请求.对每条边(u,v)∈E,我们定义0-1变量du,v,p.du,v,p=1表示v分配到处理核p上,u没有分配到处理核p上.这意味着u,v之间需要DTA传输,且该DTA传输由处理核p发起.du,v,pav,p-au,p烄du,v,pav,p烅烆du,v,p1-au,软件流水的启动间隔II是由流水线中各资源的处理时间决定,不小于各资源中最长处理时间[6],此处的资源为处理器核和DTA部件,因此我们的II由处理核上的最大计算时间和DTA上通信时间来决定.我们以work(v)代表actor结点v的计算时间,以comm(u,v)代表结点u和v之间的数据通信时间.不等式(3)保证每个小核上的计算时间之和不会大于指定的II,不等式(4)保证小核之间的通信时间之和不会大于指定的II.表达式(5)指定了我们的目标是最小化II.∑v∈V∑(u,v)∈E3.1.2阶段赋值通过上面的公式,我们给出了actors在PCs上重复执行以及如何进行DTA分配的信息,目的是实现负载均衡.但是,任务划分阶段只是完成了如何将各个actor划分到各个执行单元,其并没有考虑actor之间存在的数据依赖关系.因此,需要通过为actor和数据传输分配执行阶段,以满足actor间的依赖限制.我们定义stagev代表结点v的阶段号,stageu,v代表边(u,v)∈E中结点u和结点v之间DTA操作的阶段号.为了满足actor间的数据依赖关系,规定actor和数据传输的阶段号遵循以下条件:(1)对于任意有向边(u,v)∈E,源节点u必须在目的节点v之前执行,即stageustagev;(2)如果任务划分阶段将源节点u和目的节点v分配到不同的执行单元,则需要一个额外阶段来完成DTA操作,即stageu,v,同时该DTA两端的actoru和v的阶段和该DTA的阶段需要满足不等式stageu<stageu,v<stagev,也就是说,源节点u、数据传输操作DTA和目的节点v的阶段号必须至少差一个阶段,这样就在满足数据依赖的条件下,将计算操作和数据传输操作相分离,使处理单元和数据传输单元并行执行.算法1给出了阶段赋值的具体算法.首先,我们对流图G(V,E)进行拓扑排序.对于计算节点u,遍历u的所有父节点,如果父结点v与子节点u不在同一个处理核上,那么v和u的阶段差为2;如果父结点v和子节点u被划分到同一个处理单元,那么v和u的阶段号相同.对于处理单元间的数据传输DTA,如果其源节点u和目的节点v不在同一个处理单元上,那么该DTA的阶段号比父结点的阶段号大1;而如果有向边(u,v)∈E的源节点u和目的节点v被划分到同一个执行单元,此时不需要额外的DTA操作,也就没有DTA阶段号.算法1.StageAssignmentAlgorithm.输入:actorProcMap(actor:proc)输出:actorStageMap(actor:stage),DTAStageMap(edge:stage)1.topologOrder=topologTrav();2.forallactorvintopologOrderdo3.intmaxStageNum=0;intstageNum;4.forallactorsuwhichisaparentofvdo5.if(actorProcMap[u]!=actorProcMap[v])then6.stageNum=actorStageMap[u]+2;7.DTAStageMap[(u,v)]=actorStageMap[u]+1;8.else9.stageNum=actorStageMap[u];10.DTAStageMap[(u,v)]=Invalid;11.endif12.if(stageNummaxStageNum)then13.maxStageNum=stageNum;14.endif15.endfor16.actorStageMap[v]=maxStageNum;17.endforPage5如图3所示,任务经过划分和各阶段号的赋值之后,进行软件流水调度.在初始的填充阶段,当满足数据依赖时,每个核上的actor和DTA操作依次图3(a)任务划分和阶段赋值的例子,结点中的数字代表了计算时间.(b)例子(a)中的软件流水执行过程3.2存储访问优化为了在存储大小受限的情况下,对存储访问效率进行优化,本文采用了混合存储分配算法和拷贝消除策略.本节将对上述算法和策略进行介绍.3.2.1混合存储分配算法在软件流水线上,通过设置缓冲池(bufferpool)来实现不同actor间的数据通信,即数据传输.不同actor之间缓冲池中缓冲区的个数取决于actor间的阶段差和该对actor的划分情况,若这对actor划分在同一个核内则至少含有一组个数为两者阶段号差加1的缓冲区组(BufferGroup,BG),若这对actor划分在不同的核上,则需要2个BG.每个缓冲区的大小由编译器根据该对actor的计算速率在编译阶段时确定.例如,图4(a)中,actoru和actorv之间的阶段差为1,则其BG中含有2个缓冲区,actoru和v位于同一核中(图中方框代表一个核),故缓冲池中只含有一个BG.同理,图4(b)中,actoru和DTA的阶段差为1,则其BG中也含有2个缓冲区,而actoru和v位于不同的处理器核中,故其缓冲池中含有2个BG.在任务划分和阶段赋值过程中,我们假定所有的缓冲区都位于该actor的处理器核上的SPM上.不过,SPM的容量并不能确保容纳所有的缓冲区.开始启动执行.当流水线达到满阶段时,吞吐率达到最大值.进入最后的排空阶段时,所有actor和DTA操作会按顺序依次退出执行,直至结束.图4缓冲区分配的3种情况:(a)缓冲区分配在同一个核的SPM上;(b)缓冲区分配在不同的核的SPM上;(c)缓冲区分配在Cache(通过主存)上故为了确保流程序在机器上顺利运行,我们将实现一个有效的缓冲区分配机制.如第2节所述,Godson-T上的小核上的本地存储被配置成SPM和L1cache的混合.SPM由软件管理,load/store只需要一个cycle的延时,因此存在cachemiss和替换造成的开销.Licache为各个核私有,L2cache被所有小核共享,是主存Memory的局部拷贝.因此,本文将采用将缓冲区分配至SPM的策略,若有溢出,则Page6将其分配至2级缓存(L2Cache).图5解释了混合缓冲分配机制.图5(a)中含有处理器核P1和P2,P1上分配了BG0~BG6,P2上分配了BG7和BG11.其中的箭头代表核间数据传递,核内的数据传递箭头未显示.图5(b)代表了进行混合分配策略后的结果,由于SMP空间有限,处理器核上的SMP0和SMP1只能容纳部分BG,剩余的BG放置于L2cache中.这样不仅不会引入新的流水线阶段,而且还能消除内存拷贝中的冗余操作,极大的提高程序效率.图5(a)和(b)是SPM与L2-cache/Memory混合分配算法在分配前和分配后的状态,(c)是使用拷贝消除策略后L2-cache/Memory上的存储状态算法2所示为混合存储分配算法.初始阶段,将所有BG都分配到SPM上,并且设置BG都为未选中状态.根据已经划分好的结果以及每个任务的阶段值计算出每个处理核的存储消耗,进一步得出超出当前SPM限制的处理核.针对这些超出限制的处理核,根据SPM大小的限制,合理地标记出满足条件的BG,将满足条件的选中的BG在SPM上分配存储空间,对未标记的BG在L2cache/Memory上分配存储空间.同时,我们需要对DTA操作的阶段号进行合理的调整.算法2.HybridBufferAllocationAlgorithm.输入:procMap(processor:actor[]),1.forallEdgeeginedgeMapdo2.bg=findBufferGroup(eg)3.InitasNonVictimBufferGroup(bg);4.endfor{ComputememoryusageforeverySPMbasedon5.memUsage[processor]←6.(victimProc[],nonVictimProc[])←7.SortSpilledBufferDescending(victimProcs);8.forallProcessorpinvictimProcsdo9.BufferGroupBGs[]←AllBufferGroups(p);10.SortBySizeDescending(BGs);11.forallBufferGroupbginBGsdo12.if(memUsage[p]>SPM_SIZE)then13.if(!isVictim(bg))then14.markAsVictimBufferGroup(bg);15.updateMemUsage(p);16.endif17.else18.break;19.endif20.endfor21.endfor22.forallEdgeeginedgeMapdo23.bg←findBufferGroup(eg);24.if(!isVictim(bg))then25.allocateBufferOnSPM(bg);26.else27.allocateBufferOnMem(bg);28.endif29.updateDTA(eg);30.Endfor3.2.2拷贝消除在L2cache/Memory中,可能存在由BG分配算法分配的一对BG,这对BG在软件流水执行时,由于actor间存在着数据依赖,BG间的数据会不断地进行拷贝,从而引起缓存缺失率的上升.这会导致极大的系统开销和网络负载.图5(b)中的BG11与BG6就是这样的BG对.BG拷贝操作带来的性能影响太大,需要设法加以改进.这里考虑到L2cache的核间共享关系,则可以消除在L2cache上的BG对.图4(c)将图4(b)中的BG对合并,因此消除了在图4(b)中存在数据拷贝的BG对.同时,在软件流水的阶段划分中,actoru与actorv之间的阶段Page7数也由原来的3个阶段减少为现在的2个阶段.流水线阶段数的更新使得软件流水总阶段数降低,这会提高软件流水执行的效率.在拷贝消除操作完成之后,节点的阶段赋值的阶段数将会减少,这时需要重新计算BG的大小来满足软件流水的需求,这里引入了阶段重赋值算法来解决这一问题.这里用到的阶段重赋值算法称作混合缓存分配算法,如图6所示,该算法与算法1相比针对L2cache做了相关优化,如果actor边上的两个节点分配到了不同的核上,而它们的BG又同时分配到了L2cache上,则这两个节点间的阶段数之差设为1.同时,由于它们间的数据拷贝可以通过上述方法加以消除,则它们间的DTA将会标记为非法.若不属于以上的情况,则按照一般的方法进行处理.3.3通信优化在3.1节中,我们在虚拟处理核上将流程序划分成负载均衡的群组.在本节中,我们会在考虑通信开销的情况下,将虚拟核映射到Godson-T架构下的实际物理核上,以减小通信和同步开销.我们称这一过程为核间映射的通信优化.本实验使用模拟退火算法(SimulatedAnnealing,SA)来实现核间映射过程.首先,定义3个函数:扰动函数、概率接受函数和代价函数.其中,扰动函数的作用是从当前的映射状态变换到另一个新的映射状态,而在实际处理中,我们随机选择两个虚拟核对应的物理核.概率接受函数用于判断经过上述扰动函数产生的映射状态是否成立.代价函数用来计算每一种映射对应的通信开销和同步开销,并选择开销最小的状态进行映射.实验中,我们用(vs,vd)表示虚拟核vs与虚拟核vd之间存在数据通信量,用assign(C)代表虚拟核C映射到的物理核.route(ps,pd)代表物理核ps和pd在进行数据传输时经过的路径.Godson-T采用了静态X-Y路由策略,即数据先沿X轴传输,然后沿Y轴传输到达目的处理核.Cost(map)=∑all(vs,vd)wherepath=route(assign(vs),assgin(vd))(6)式(6)为给定映射下的代价函数.其中,Items包含在流水线满时vs上的actor与vd上的actor之间总的数据传输量.hop函数得到path这条路径中包含的处理核的个数.sync函数返回path这条路径上包含其他路径经过的处理核的个数,如果有多条路径同时经过同一个核,数据则会按序在该核上路由.权值w1和w2分别为通信开销和同步开销,本实验设置w1为2,因为在Godson-T上,一个字的数据经过一个处理核有2个cycle的延时.同步开销并没有理论值,经过在Godson-T上的实际测试,我们设置w2的值为20.4实验结果与分析本实验完成了Godson-T下的编译框架,实验的测试是在Godson-T的模拟器(Godson-TArchi-tectureSimulator,GAS)进行的.其中,GAS模拟了一个64核处理器的所有功能单元.我们从DFBrook程序集中选出测试程序集,并做了适当的扩展来增加优化的机会.首先,我们比较了优化前后的整体性能.在优化前只使用了第1阶段的软件流水调度而没有使用存储访问优化和通信优化,所有的数据都存放在L2cache中,也没有对冗余的数据拷贝操作进行消除,核间映射采用了X-Y寻径的方式.图6给出了优化前后的性能比较.实验使用了Godson-T中的所有64个核,每段程序的执行时间被标准化到0-1区间.对大部分的测试代码来说,SCMC比优化前的性能提高了70%,对于所有测试代码,平均性能可以提高58.3%.图6Godson-T上测试代码在优化前后的性能对比图7显示的是测试代码经过SCMC编译前后在不同核数处理器上执行时的加速比.从图中可以看出,当执行的核数小于32时,加速比与核数接近线性关系.选择16个核执行时加速比为13x,而对于多数测试代码,选择64个核执行时加速比为33x,这是由任务划分过程中各个核的负载不均衡造成的,对比图6,avermoti和MultiBlock测试程序Page8在存储和通信优化后性能有显著提高,而在64个处理核时加速比却没有显著变化,这是由于这2个测试程序任务的均衡性差,因此加速比没有较大提高.图7测试代码在Godson-T上达到的加速比SCMC编译框架的第2个优化阶段是存储访问优化,该阶段的目的是使本地存储空间的利用率尽可能的最大化,减小数据访问的时间.存储访问优化包含混合存储分配和拷贝消除两个阶段.图8给出了在16个处理核时,对3种存储访问策略性能的比较:(1)L2Cache/Memory.所有BGs都存放在L2cache上并且不使用拷贝消除策略;(2)L2cache与拷贝消除结合.BGs都放在L2cache上并且使用了拷贝消除策略;(3)SPM与拷贝消除.即混合存储分配和拷贝消除策略,也是本文用到的方法.从图中可以看出,程序利用方法(2)比方法(1)的执行性能平均提升31%,而对于测试用例avermoti,性能可提升50%.因为冗余数据的消除减小了冗余数据传输的时间,而且减小了缓冲区的利用个数,从而降低了L2cache上的cachemiss率.采用方法(3)进行存储访问,对比方法(1),程序的执行性能平均提升50%,对比方法(2),程序的执行性能平均提升27.5%.这是因为方法(3)采用了将一部分缓冲区放在SPM上,提高了数据访问效率.在实验中,我们发现L1cache的miss率很低,而且对当前测试程序的实际性能的影响很小.因此,我们主要对L2cache的cachemiss率进行了分析.图9给出了上面3种存储访问方法的L2cachemiss率.图中数据显示,采用拷贝消除,减小缓冲区的个数以及将部分缓冲区分配到SPM上的方法可以有效的减小cachemiss率.图93种存储优化方法cachemiss率的比较我们将基于缓冲区的混合存储分配策略与文献[7]中的单缓冲分配策略进行了比较.在文献[7]的方法中,每个BG中的单个缓冲区作为基本分配单元.当SPM溢出后,BG中溢出的缓冲区会分配到其他空闲的SPM中,如果没有空闲的SPM存在,那么溢出的缓冲区放在L2cache上.如图10所示,为混合存储分配方法和单缓冲分配方法的性能比较,实验数据表明,相对于单缓冲方法,采用混合存储分配方法的性能可以提高3%到55%.这是因为文献[7]的方法在缓冲区溢出的情况下,溢出的缓冲区部分与原缓冲区之间会产生额外的DTA操作,从而增加了通信开销,除此之外,还会增加软件流水的阶段数,也会导致软件流水开销的增加.我们的方法以BG为分配单元,不但没有引入额外的数据传输开销,而且消除了冗余的数据拷贝,从而降低了软件流水的阶段数.图10单缓冲分配策略与本文分配策略的性能对比Page9SCMC的最后一个优化阶段是通信优化的核间映射.核间映射的目的是为了将虚拟核映射到物理核上以减小Godson-T上的核间通信代价.本实验与X-Y寻径进行了比较.在X-Y寻径中,虚拟核按照先X轴再Y轴的顺序映射到物理核上.如图11所示,为测试代码采用两种方法的执行时间的比较.实验数据表明,相比X-Y寻径方法,本实验采用的核间映射的方法可以使程序的执行性能提升9%左右,其中avermoti的执行性能提高达到24%.该方法对带宽有限的系统会有更好的效果.5相关工作软件流水调度作为一种开发指令级并行的调度策略当前逐渐被用到了众核处理器上.软件流水最初是在开发硬件指令集流水线时提出来的[7].在原先的模型中,每个任务在开始执行后,会同步地在资源之间进行软件流水调度,并且这些资源可以重复利用;当一个任务与其他运行的任务不存在资源冲突时,该任务便可以开始执行.Kudlur等人采用软件流水的方法实现了StreamIt在众核处理器上的调度,该方法通过线性规划的方法将所有actor按计算量均衡地分配到众核上,并通过阶段赋值算法为每个actor确定阶段号,然后进行软件流水调度,这种方法的缺点是没有考虑数据的通信量相对比较大的情况[6].我们先前的工作将计算量和通信量同时进行了建模,在最小化II的情况下同时减小通信开销,但该方法并没有考虑存储消耗情况[4].Choi等人[8]在Kudlur的基础上提出了在资源受限情况下软件流水调度,该方法采用了保守估计的方法对存储进行的建模,丢失了部分解.很多关于SPM管理技术的研究专注于将数据以静态或动态的方式放在SPM上.Steinke等人[6]提出了一个静态的,理想的选择算法.这个算法选择有利的程序段和变量到SPM中来节省能耗.Uday-akumaran等人提出了一种postpass的静态方法,它会找到一组最优的频繁访问的指令块放入SPM中.Verma等人[9]提出了一个静态算法和一个ILP公式来减小指令读取时的能耗,主要针对包含了cache和SPM的嵌入式系统,同时也提出了一种cache-aware的SPM分配策略,通过在SPM上存储指令来达到节省能耗的目的.Avissar等人[10]提出了一个在SPM和外存间静态数据划分算法来提高性能.Francesco等人[11]提出了一个使用DMA来减少将数据动态拷贝到SPM上的开销的运行时机制,并提供了一组API接口.Kandemir等人[12-13]提出了一种基于循环转换的,为数组动态管理SPM的编译器技术.6总结本文提出了一个优化流应用执行效率的流编译框架———SCMC.它将编译优化过程分为了3个阶段:软件流水调度,存储访问优化和核间映射.除了开发流程序的并行性,SCMC同时也实现了高效的混合存储分配算法和拷贝消除策略,提高存储效率.并且针对众核架构上复杂的片上网络通信的问题,提出了核间映射策略来减小网络通信负载和同步开销.结果表明,SCMC能达到33x的加速比并比优化前获得平均58%左右的性能提升.
