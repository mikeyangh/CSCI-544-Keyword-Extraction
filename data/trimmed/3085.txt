Page1基于FPGA的细粒度并行CYK算法加速器设计与实现夏飞1)窦勇1)宋健2)雷国庆1)1)(国防科学技术大学计算机学院长沙410073)2)(中国人民解放军61785部队北京100075)摘要基于随机上下文无关文法(SCFG)理论模型进行RNA二级结构预测是目前采用计算方法研究RNA二级结构的一种重要途径.由于基于SCFG模型的标准结构预测算法(Coche-Younger-Kasami,CYK)巨大的时空复杂度,对CYK算法进行加速成为计算生物学领域一个极具挑战性的热点问题.CYK的并行性能受限于算法多维度、非一致性的数据依赖关系和较低的计算/通信比,现有的基于通用微处理器结构的大规模并行处理方案不能获得令人满意的加速效果,并且大规模并行计算机系统硬件设备的购置、使用、日常维护的成本高昂,其适用性受到诸多限制.文中在深入分析CYK算法计算特征的基础上,基于FPGA平台提出并实现了一种细粒度的并行CYK算法.设计采用了对三维动态规划矩阵“按区域分割”和“逐层按列并行处理”的计算策略实现了多个处理单元间的负载均衡;采用数据预取、滑动窗口和数据传递流水线实现处理单元间的数据重用,有效解决了计算和通信间的平衡问题;设计了一种类似脉动阵列(systolic-likearray)结构的主从多PE并行计算阵列,并在目前最大规模的FPGA芯片(XilinxXC5VLX330)上成功集成了16个处理单元(processingelements),实验结果表明作者提出的CYK算法加速器结构具备良好的可扩展性.当RNA序列长度为959bps,CM模型状态数为3145时,与运行在Intel双核E52002.5GHzCPU、2.0GB主存通用计算上的Infernal-1.0软件相比,可获得超过14倍的加速效果.配置一个FP-GA算法加速器的通用计算平台的综合处理性能与包含20个Intel-XeonCPU的PC集群相当,而硬件成本仅为后者的20%,系统功耗不到后者的10%.关键词生物信息学;RNA;二级结构预测;SCFG模型;并行CYK算法;FPGA;硬件加速器1引言随着人类基因组及多种模式生物测序项目的完成,生物信息学的研究已步入后基因组时代.序列比对和搜索已成为分子生物学领域最重要的基础性工作.目前在DNA和蛋白质序列分析领域已有许多经典的研究工具,如BLAST[1]、FASTA[2]、HM-MER[3]、ClustalW[4]等.而自20世纪80年代中期具有催化性质的RNA被发现以来,RNA所起的各种重要生物化学功能如蛋白质装配、mRNA剪接和编辑、RNA与蛋白质的相互作用以及RNA干扰等引起了人们的日益关注与重视.由于RNA的各种重要功能与其结构直接相关,并且RNA分子结构上的保守性大于序列的保守性,有时序列本身相似度很低的两个RNA分子有可能具有很相似的结构,所以仅仅采用传统的序列分析工具(如BLAST、FASTA等)无法满足对RNA结构特性研究的需求.另一方面,由于结构信息的加入大大增加了RNA序列分析的复杂性,关于RNA序列分析模型及二级结构预测方法,一直是近年来生物信息学研究的热点和难点问题.目前,最直接的RNA结构测定方法是采用X射线衍射和核磁共振(NMR),这种方法虽然结果精确可靠但是只有在有条件的实验室环境下才能进行,并且其过程非常耗时和昂贵.因此采用计算机和数学模型预测RNA序列二级结构的方法被广泛采用,成为近年来研究的热点.目前存在三类主要的RNA二级结构预测方法:基于热力学模型的Zuker最小自由能方法[5]、基于比较序列分析模型的多序列比对方法[6]和基于随机上下文无关文法(SCFG)的结构预测方法[7].其中,SCFG理论是目前最好的RNA二级结构建模方法,在RNA二级结构预测研究领域占有重要地位,目前基于SCFG理论模型的标准比对算法为Coche-Younger-Kasami,简称CYK算法.CYK算法用于实现单条序列与单个RNA家族的共变模型(covariancemodel,简称CM模型)进行比对,从而判断该RNA序列是否属于该家族并且进一步地得到该序列的二级结构.CYK算法是一种三维动态规划算法[8-10],根据矩阵填充方向不同,又可分为inside和outside两种算法,但本质上并无不同之处.CYK算法的时间复杂度为O(KL2+BL3),空间复杂度为O(KL2),其中L是RNA序列长度,B是分支状态个数,K是CM模型中的状态数.由于巨大的时空复杂度,标准CYK算法并不适合处理较大规模的RNA序列.以LSURNA序列为例,在单个Alpha处理器上,执行不带回溯的CYK/inside算法需要17391s[11](约4.8h),带回溯的CYK/inside算法则需要27798s(约7.72h),并且存储需求超过150GB[10],将人类全基因组中的RNA片段与目前Rfam数据库中的RNA二级结构模型进行比对需要300年[23],因此对CYK算法进行加速成为一个极具挑战性的问题.目前人们主要采用软件并行方案基于多处理器平台如Cluster系统对CYK算法实现加速.Hill等人于1991年提出了一种按照对角线方向并行的无回溯CYK/inside算法,在7个处理器上可以获得3.5倍的加速效果[12].2005年,中国科学院Tan等人提出了对三维动态规划矩阵采用层内并行处理策Page3略,对矩阵进行等面积的动态任务划分,在多个处理器上并行执行.但由于CYK算法中每个元素的计算量与其在矩阵中所处的位置相关,这种常规的按面积等分的任务划分策略会导致处理器负载不平衡,并且随着处理器个数的增加,计算过程中的通信开销随之增长.算法复杂的数据依赖关系导致处理器间的数据通信粒度小,通信次数频繁,因而并行处理的效率不高.在由8个节点(每个节点包括4个2.4GHzAMDOpteronCPU和8GB主存,一共32个处理器)构成的Cluster上执行长度为1542的LSURNA序列的比对只能获得19倍的加速效果[13],并行处理效率不到60%.新加坡南洋理工大学Liu等人提出在PC集群上按状态层方向流水处理的层间并行处理方案,在设计时根据元素位置与计算量之间的关系,在进行计算区域划分时采用了近似等负载的任务划分策略,即单个元素计算量较小的区域包含的列数较多,单个元素计算量较大的区域包含的列数较少,尽量让每个处理器负责计算的元素的计算量之和相等而不是元素个数相等.研究表明,新算法的并行处理效率显著提高(在包含20个Intel-Xeon2.0GHzCPU的多处理器平台和包含48个1.0GHzAlphaEV68处理器的Cluster上执行LSURNA序列的比对,可以达到16倍和36倍的加速效果[11]),但是大规模并行计算机系统硬件设备的购置、使用、日常维护的成本高昂,包含20个Intel-XeonCPU的PCCluster系统硬件成本超过20万元,系统功耗超过3kW,而由48个Alpha处理器构成的Cluster成本超过40万元,系统功耗超过6kW,只有少数机构才有实力采购使用,而非一般用户所能用得上.近年来,FPGA(FieldProgrammableGateArray)器件以其可编程特性、细粒度并行能力、丰富的计算资源、灵活的算法适应性、低硬件代价和高性能功耗比成为理想的可编程系统平台.以目前世界上的两大FPGA芯片制造商的高端产品为例,XilinxVirtex5系列的高端产品XC5VLX330片内集成了51840个Slice,总计331776个逻辑单元;288个BRAM,总计10368Kb存储容量;片内还集成了192个DSP模块,可达到105GMACS的处理能力.AlteraStratixIII系列的高端产品EP3SL340集成了135200个自适应逻辑模块(ALM),338000个等价逻辑单元(LE);1144个M9K存储器模块和48个M144K存储器模块,片内存储容量超过17208Kb;片内还集成了576个18×18乘法器模块,可以达到300GMACS的处理能力.同时现有的高档FPGA芯片还支持PCI-E、Ethernet、RocketIO等多种IO接口,而芯片硬件成本不足2万元,最大功耗不足30W.另一方面,由于生物信息学领域经典的动态规划算法往往具备良好的位级(bit-wise)并行性,CPU与FPGA相结合的生物信息学算法加速器成为了研究的热点.目前已有许多基于FPGA器件对RNA二级结构预测方法实现硬件加速的报道,例如2006年Tan等人在FPGA上实现了基于最小自由能模型的细粒度并行Zuker算法[14],2008年ArpithJacob实现了基于最大碱基互补配对模型的Nussinov算法[15,21],但是现有的研究仅限于在FPGA上对二维动态规划算法的并行化,并未涉及到更高维度的CYK三维动态规划算法.文献[16-17]基于FPGA器件实现了CFG模型的解析过程,但并未涉及CYK算法的计算过程.最近,Moscola等人提出了两种基于FPGA的细粒度CYK算法并行计算结构,该结构对小规模的RNA序列和CM模型可以获得大约24倍左右的加速比,但该结构是针对特定CM模型的,为特定CM模型定制计算逻辑将耗费大量FPGA逻辑资源,目前只能支持长度大约为100bps的RNA二级结构预测,这意味着Rfam8.0中只有5%的CM模型能够在Moscola的硬件平台上运行.本文针对无回溯的CYK/inside算法的全部计算过程,提出了一种基于FPGA平台的细粒度并行算法,采用“按区域分割”和“逐层按列并行处理”的计算策略对三维动态规划矩阵的计算实现了细粒度的并行化,使得多个处理单元间的负载尽可能均衡;并在此基础上设计和实现了一种类似脉动阵列(systolic-likearray)结构的主从多PE并行计算阵列.阵列由一个主处理单元和多个从处理单元串联构成,只有主处理单元具备外部存储器数据载入权限.设计通过重组单元计算顺序、CM模型预取、数据缓存、滑动窗口和数据传递流水线等策略实现处理单元间的数据重用,从而减少对片外存储带宽的需求,实现计算和通信之间的平衡.此外本文提出的硬件算法和电路结构与RNA序列和CM模型无关.我们在单个FPGA芯片上成功集成了16个PE,实验结果表明,与运行在Intel双核E5200CPU,2.0GB主存通用计算机平台上的RNA结构预测软件Infernal-1.0相比,可获得超过14倍的加速效果.配置一个FPGA算法加速器的计算平台的综合处理性能与包含20个Xeon2.0GHzCPU的Cluster相当,而硬件成本约为3.5万元,仅为后者的20%;系统功耗不超过200W,仅为后者的10%.Page42CYK算法2.1算法简介CYK算法用于实现单条RNA序列与RNA家族对应CM(CovarianceModel)模型之间的比对,以判定该序列是否属于该家族.CYK算法考虑到了结构信息,是一个典型的三维动态规划算法.算法的输入为一条长度为L的RNA序列x=x1x2…xL和一个CM模型.CM模型[18]是Eddy和Durbin提出的max(i-1midj)[M(i,mid,kl)+M(mid+1,j,kr)],S(k)=BIF图1CYK/inside算法三维动态规划矩阵的计算过程上述公式中的M(i,j,k)代表当前计算的元素,i,j,k是元素在三维矩阵中的坐标.S(k)表示当前状态的类型,其中BIF表示分支状态,其它状态如D,S,MP,ML,IL,MR,IR,E统称为非分支状态.C(k)表示当前状态的子状态集合,γ表示当前状态的某个子状态,ek和tk分别代表当前状态下的字符生成概率和子状态转移概率,变量d=j-i+1.无回溯的CYK/inside算法的计算过程是从三维矩阵最下层(每一层称为一个deck,它对应CM模型中的一个状态)边缘的元素开始,沿对角线按照波前(wavefront)顺序进行计算.每层元素的计算启动前首先判断CM模型中当前状态层的类型,根据当前状态类型选择公式中的一个分支执行.当下层一种进行RNA二级结构分析的概率模型,它利用了随机上下文无关文法(SCFG),从一组相关的RNA序列之间的多序列比对和一致结构中构建,以刻画RNA家族的结构和序列信息.CM模型由K个不同的状态、状态间的连接关系以及每一个状态对应的字符生成概率(ek)和状态转移概率(tk)组成,而状态信息中包括当前状态的类型、编号、父/子状态的数量以及编号.CYK算法的核心是不断地迭代计算一个截面为三角形的三维矩阵[19](如图1所示),迭代公式如下(1ij+1,0jL,1kK):deck所有元素计算完毕后再计算上一层的元素.计算过程由下至上逐层推进,直到CM模型的根节点(对应矩阵最上层的三角形)中的所有元素算完.此时元素M(1,L,1)(图3最上层三角矩阵顶点位置的单元)的值就是最终比对分值,它代表该序列与当前RNA家族的相似度.基于FPGA平台对CYK算法进行细粒度并行化同样面临一系列的挑战:(1)CYK算法多维度、非一致性的数据依赖关系(数据相关性跨越矩阵三维空间,并且数据相关距离随元素的位置变化)导致计算过程中任务划分困难,难以实现负载平衡;(2)程序空间局部性差(访存粒度小(以KB为单位),每次读写操作的数据量不等且不连续),导致访存调度困难;(3)FPGA片内存储资源不能满足矩阵犗(L3)的存储需求,导致片外存储器访问延时将成为提高并行处理效率的瓶颈.在CM模型中,三维矩阵的层数K大约是序列长度L的3倍,以长度为2898bps的LSURNA序列为例,对该序列进行比对需要计算的三维矩阵的层数为9023,每层的存储需求大约为32MB,而目前容量最大的FPGA器件片内存储容量不足2MB,因此有限的片内存储容量和相对较低的存储带宽(FPGA工作频率不到CPU的1/5)是对CYK算法实现硬件加速的瓶颈.2.2程序特征分析法计算过程具备以下4个特征.通过对CYK算法的进一步分析,可以发现算Page5特征1.三维矩阵中每个元素的计算量与当前状态的类型和所处的位置相关.根据CYK算法迭代公式,如果第k层为非分支层,则元素M(i,j,k)的计算量(即加法和比较操作的执行次数)与当前状态k的子状态数量有关.例如MP状态层元素M(i,j,k)的计算需要执行7次加法和6次比较运算,而其它非分支状态层每个元素的计算需要执行3~7次加法和比较运算,具体的计算量由CM模型中该层对应的子状态数量确定.而如果第k层为分支层,根据公式中分支层元素的数据依赖关系可知元素M(i,j,k)的计算量C(i,j,k)依赖于元素下标i和j的值.C(i,j,k)=j-i+2,状态层k中第j列元素M(,j,k)的计算量C(,j,k)等于本列所有元素的计算量之和:C(,j,k)=(j+1)(j+2)烄烅(j+1)s,s∈{3,4,5,6,7},烆状态层k中相邻两列元素的计算量之差为ΔC(j,j+1,k)=j+2,s,s∈{3,4,5,6,7},根据式(1),可以发现非分支层元素M(i,j,k)的计算量为常数s,而在分支层k中同列元素的计算量由下往上逐渐增加,同行元素的计算量由左往右逐渐增加,其中三角矩阵右上角元素M(1,L,k){图2RNA家族CM模型示例特征3.层内(Deck)元素的数据依赖关系取决于状态类型,既可能存在于同层元素内部也可能存在于不同层的元素之间,从而导致计算过程中存在大量细粒度不连续的存储访问.根据CYK算法的迭代公式,每一层元素在计算时都要根据当前状态的类型选择执行不同的分的计算量最大,它分别依赖于其左、右子状态层的第1行和第L列,因此采用常规的等面积的任务划分策略将导致负载不平衡.而根据式(2)和(3),可以发现非分支状态层中相邻两列元素的计算量之差为常数s,在分支层中相邻两列元素的计算量之差也很小,为O(L)量级,L为RNA序列的长度.为此我们采用了按矩阵列顺序进行循环分配的任务划分策略,根据处理单元(PE)的数目将状态层矩阵划分为若干区域,每个区域包含p列元素(p为PE的个数),在同一个区域中,每次为每个PE分配一列,按照列顺序依次进行计算.特征2.状态层(deck)之间的数据相关距离是不确定的,它由RNA序列家族CM模型中父子节点的连接关系决定.图2所示的RNA家族CM模型包含24个节点共81个状态[10].图中的弧线代表状态(也就是三维矩阵层与层)之间的数据依赖关系.从图中可以看出,每一个状态(层)的计算所依赖的数据层的数量(最少为0,最大为6)和位置都是不确定的,数据相关距离可能为0(即依赖于本层的数据),也可能为几十、几百乃至上千(图2中第10层的计算便依赖于第46层的元素).此外,每一个状态层的存储开销均为O(L2),有限的FPGA片内存储空间无法满足CYK算法的存储需求.针对该问题,我们采用了“按区域分割”和“逐层按列并行处理”的细粒度处理策略,将整个矩阵划分为多个区域,逐个区域、逐层计算,避免存储整个三角矩阵,将计算过程的存储需求由MB量级减少为KB量级,从而满足FPGA器件存储资源的限制.支.对于非分支(状态)层而言,每一个数据在计算时最多依赖6个子状态的数据(并且这6个数据位于不同状态层),最少需要1个子状态的数据.(1)若当前状态k为D或S:则元素M(i,j,k)的计算依赖于子状态对应Deck中处于相同位置的元素(i,j);Page6(2)若当前状态k为MP:则元素M(i,j,k)的计算依赖于子状态对应Deck中当前位置左下方的元素(i+1,j-1);(3)若当前状态k为ML或者IR:则元素M(i,j,k)的计算依赖于子状态对应Deck中当前位置下图3非分支状态层数据依赖关系示意图对分支(BIF)状态层k而言,每个元素依赖于其左子状态层的第i行和右子状态层的第j列元素.如图4所示,元素M(i,j,k)的值等于其左子状态同行元素M(i,,kleft)与右子状态同列元素M(,j,kright)之和的最大值.根据特征2,不管是何图4分支(状态)层数据依赖关系示意图特征4.分支状态是计算的瓶颈,可以使用数据重用策略减少对访存带宽的需求.CYK算法时间复杂度表达式O(KL2+BL3)中的KL2为非分支状态层的计算开销,BL3为分支状态层的计算开销,两者的比例为KL2/BL3=K/BL,一般来讲,K≈3L,并且当序列长度L大于1024时,B3,分支状态层的计算开销超过90%,并且该比例随着序列长度的增加显著增加,所以分支状态层是计算的性能瓶颈.根据算法特征3,分支状态的计算依赖于左右两个子状态上三角矩阵的行/列元方的元素(i+1,j);(4)若当前状态k为MR或者IR:则元素M(i,j,k)的计算依赖于子状态对应Deck中当前位置左侧的元素(i,j-1).种类型的状态,其子状态数据层与父状态数据层的距离是不确定的,并且该数据无法存储在FPGA片内,所以在计算时将导致大量不连续和跳跃性的片外存储访问.素,如果按照波前计算(wavefront)策略沿对角线方向推进,则需要反复从片外载入计算所需的行/列元素.为了减少片外存储访问开销,我们采用了以下几种数据重用策略.(1)右子状态列元素重用从图4可以看到,元素M(i,j,k)的计算依赖于其右子状态层第j列元素((c)子图用阴影填充的元素)M(,j,kright)).对一个PE而言,如果采用按列顺序进行计算的策略,当计算位置从M(i,j,k)移动至M(i-1,j,k)时,只需要载入一个新的元素,其余Page7(j-i)个列元素都可以重用,并且子状态层的计算在父状态层计算之前已经完成,如果能将本列元素缓存在片内则可避免片外存储访问.(2)左子状态行元素重用从图(b)可以看到,元素M(i,j,k)的计算依赖于其左子状态层中的同行元素M(i,,kleft).如果将BIF状态层第i行的p个元素(图(a)中用阴影填充的一行元素)分配到p个处理单元上同时执行,同时将载入的左子状态层第i行元素(图(b)中用阴影填充的一行元素)广播到所有的p个处理单元上,则可实现对载入的左子状态行元素的重用,从而减少片外存储访问开销.(3)非分支状态元素的重用根据图3所示的非分支状态元素的数据依赖关系,如果当前状态为MP、MR和IR,则第j列元素M(,j,k)的计算依赖于子状态层中相邻的第j-1列.如果每个处理单元每次负责一列元素的计算,并将计算结果存储在局部存储器中,又因为子状态层的计算在父状态层计算之前已经完成,则当前状态层第j列元素的计算所需的数据都在FPGA片内(要么在本地存储器中,要么在相邻的前一个处理单图5CYK算法加速器结构PCI-E接口控制器基于XilinxIPCore实现,负责加速器与主机间的数据通信,有效数据传输带宽可达1GB/s.DDRII存储控制器负责实现对片外DDRIIDRAM的存储访问.CYK算法逻辑包括PE元的局部存储器中),而不需要访问片外存储器.采用上述数据重用策略可以极大地减少计算过程中对片外子状态层数据访问的带宽需求.3CYK算法加速器3.1系统结构如图5所示,CYK算法加速器系统平台由一台个人计算机(HostPC)和一个可重构算法加速器组成,个人计算机作为系统的主机.系统主机负责与用户的交互,将用户输入的RNA序列和CM加载至算法加速器,并启动加速器;算法加速器完成无回溯的CYK/inside算法的全部计算过程,然后将比对得分报告给主机.加速器以高性能商用FPGA芯片(XC5VLX330)为基础,配置两个4GBDDRII大容量商用存储器,通过高带宽的PCI-E×8数据通道与主机相连,实现通用处理器与算法加速器的协同工作.可编程的FPGA芯片是算法加速器的核心,内部由PCI-E接口控制器、DDRII存储控制器和CYK算法逻辑三部分构成.阵列控制器、PE阵列、列同步和写回控制器模块等部分.其中PE阵列控制模块负责控制PE阵列的初始化、实现动态任务划分(对三维矩阵实施按列循环分配,并将其加载至PE阵列).PE阵列负责三维动Page8态规划矩阵的并行计算,它由一系列PE模块串联构成,其中第一个模块是主处理单元(MasterPE),其余模块为从处理单元(SlavePE).每一个PE模块都包括一个局部存储器(LocMem,采用FPGA片内分布式的存储资源实现,用于存储编码后的RNA序列)和一片计算结果缓存区,缓存区包含16个存储区域,采用FPGA片内BlockRAM实现,最多可以缓存16个子状态层对应位置的一列元素.PE间的数据传递寄存器组用于实现非分支状态的数据在PE阵列中的重用(根据图3(b)所示,MP、R状态层第j列元素的计算依赖于子状态层左侧j-1列的元素).由于每一列元素的计算量并不完全相等,列同步和写回控制器模块负责控制PE阵列的计算同步和将部分状态层数据写回DRAM(如果当前状态层为左S,需要将数据从PE的局部存储器(Loc-Mem)中写回至DRAM).3.2无回溯的细粒度并行CYK/inside算法标准CYK算法采用如图1所示的逐层依次计图6细粒度并行CYK/inside算法的任务划分与计算顺序算的策略,层内采用沿对角线按照波前(wavefront)顺序进行计算.根据CYK算法特征2,对当前状态层的计算而言,它所依赖的数据层的数量和位置都是不确定的,FPGA芯片的内部存储容量不能满足O(L2)的存储需求;而根据CYK算法特征1,采用常规的等面积任务划分策略将导致负载不平衡,所以我们采用了图6(a)所示的采用“按区域分割”和“逐层并行处理”的计算策略.首先将整个三维矩阵沿维度k按列垂直切分为若干区域(section),每一个区域仍是一个三维矩阵,它包含了每一个状态层的n列元素,n为PE阵列的规模;然后从k=K开始,依次计算每个区域内的所有状态层,直到k=1.区域内每一个状态层内的计算采用细粒度并行的方式,每个PE每次负责计算当前状态层中的一列元素,当本区域当前状态层中的所有元素计算完毕后,再将下一状态层分配给PE阵列.如图6(a)所示,我们以包含4个PE的处理阵列为例说明处理过程.图中的三维动态规划矩阵犕被划分为3个区域Page9(Section1,Section2,Section3),每个区域包含K层,每层包含4列元素.图中标示的数字和虚线箭头代表计算顺序.首先将Section1中的k=K层加载至PE阵列,然后计算第K-1层,当本区域最上层(k=1)算完后再将Section2的第K层加载至PE阵列,直到最后一个区域Section3的最上层(k=1)中的所有元素算完.对每一个状态层的计算而言,首先需要根据当前状态的类型选择执行哪个不同的分支.如图6(b)、(c)所示,图中填充阴影的4列元素表示当前状态层的计算区域,它们被同时分配至PE1~PE4上并行计算.图中标示五角星的单元表示PE阵列当前计算的元素所处的位置.如果是非分支状态层,则每一个PE的计算都按照图6(c)子图所示从当前列算法1.Fine-grainedParallelCYK/insideAlgorithmwithoutTraceback.Input:R:anRNAsequenceR=r1r2…rLoflengthL;Output:M[1,L,1]:optimalglobalalignmentscore;Variables:i,j,k:elementlocationindexinthethree-dimensionalDP-matrix;S(k):thetypeofcurrentstatek;C(k):thestatesthatkcantransitto;P(k):thesetofparentsofstatek;BEGIN:ParallelCYK/insideDatainput:S2:if(p=1)//it’stheMasterPELoadM[,jp,kright]oftherightchilddeckfromownlocalon-chipmemory);forallγ∈C(k)do{LoadM[,jp-1,γ]fromexternalDRAM};LoadM[,jp,kright]oftherightchilddeckfromownlocalon-chipmemory);Calculation:S4:if(S(k)=BIF)thenSynchronization:S5:PE_pwaitinguntilthelastPE_nprocessingfinished;Dataoutput:S6:if(S(k)isaleftsubstateofBIF),thensendM(,jp,k)toexternalDRAM;Sectionadvance:图7无回溯的细粒度并行CYK/inside算法的底部位置开始(即三角形的斜边),按照由下至上的顺序执行.根据式(1),处于同一对角线位置的元素计算量相等,所以在任意时刻,PE阵列当前计算的元素总是处于三角矩阵的同一条对角线上.如果是分支状态层,则每一个PE的计算都按照图6(b)所示,从当前列的顶部位置开始,按照由上至下的顺序执行.根据算法特征3,每个PE负责计算区域中的一列元素,当计算位置由上至下移动时,其右子状态的列元素可以重用.对整个PE阵列而言,由于都要使用左子状态的同一行元素,所以可将从DRAM中载入的第i行元素广播至所有处理单元实现行元素的重用.图7采用单程序多数据模型(SPMD,SingleProgramMultipleData)描述了细粒度并行CYK/Page10inside算法执行过程.算法的输入为一条长度为L的RNA序列x=x1…xL和一个包含K个状态的CM模型.算法的核心是两重For循环语句,循环体内部按照图6(a)所示的任务划分策略将当前状态层中一块包含n列元素的区域加载至PE阵列进行处理.在初始化阶段,每一个PE单元在本地存储器中选择一个空闲的存储区域存放将要计算出的列元素(语句S1),然后根据当前状态类型选择执行不同的分支.处理过程包括数据载入(datainput)、列元素计算(calculation)、列同步(synchronization)、数据输出(dataoutput)和新区域分配(sectionadvance)5个步骤.在数据载入阶段,不同类型的PE单元根据当前的状态采用不同的数据载入方式(S2):只有主处理单元(MasterPE,p=1)具备外部存储器访问权限,如果当前状态为BIF,则MasterPE从本地局部存储器中读取右子状态的列元素,从DRAM中读取左子状态的行元素并将其发送至所有PE单元(S21);如果当前状态为MR、IR或者MP,则MasterPE计算所需的数据都在片外存储器中,需要从片外存储器载入计算所需的所有子状态的第j-1列元素(S22);如果当前状态为其它非分支状态,则PE单元计算所需的数据都在本地存储器中,只需从局部存储器中载入所需的子状态第j列元素即可(S23).与主处理单元的区别在于,从处理单元(SlavePE)不发出数据载入请求,它根据当前的状态类型,选择从本地局部存储器中读取数据、从前一个PE局部存储器中读取数据或者等待接收主处理单元读回的数据(S3).PE单元的计算过程采用数据驱动的流水线模型,一旦计算所需的有效数据到达便启动该分支对应的计算流水线.尽管所有PE都使用标准CYK算法的迭代公式,但是需要根据不同的状态类型采用不同的计算顺序:如果是非分支状态层,则按照由下至上顺序计算;如果是分支状态层,则按照由上至下的顺序执行计算(S4).所有PE的计算结果都缓存在各自的局部存储器中.由于每一列元素的计算量不等,所以当前PE完成本列元素的计算后要进入列同步等待状态(PE在同步等待的同时执行写回操作),以便保持数据依赖关系的一致性(S5).如果当前状态是分支状态的左子状态,则所有PE都需要将当前的计算结果写回片外存储器(因为分支的左子状态数据在未来很长一段时间内都不会使用).PE阵列采用依次写回的策略,先算完先写回,尽量将写回开销隐藏在计算开销中.如果当前状态是MR、IR或者MP状态,则只有最后一个从处理单元需要将当前的计算结果写回片外存储器(该PE的计算结果位于当前计算区域的最右侧,下一个区域计算时将会被用到).除此以外,所有PE的计算结果都保存在局部存储器中,以便在后续的计算中使用或者通过数据传递寄存器供相邻的下一个PE使用(S6).当数据写回操作完成后,执行局部存储器数据替换操作:如果某一状态的所有父状态都已算完,则释放该状态占用的数据缓冲区(S7).当最后一个PE(PE[n])计算完成并将结果写回后(说明此时本区域的计算任务已完成),PE控制模块将本区域的下一个状态层加载至PE阵列(S8),然后所有PE同时开始执行新区域的计算.所有PE单元重复上述计算过程直到新指派的元素列号超出输入的RNA序列的长度(此时当前PE处于空闲状态,直到阵列中所有的PE计算完成).当三维矩阵最后一个状态层的计算完成后,PE阵列将最终的计算结果M(1,L,1)写回片外存储器,计算过程终止.3.3PE模块的设计PE模块的功能是按照CYK/inside递归公式实现对三维动态规划矩阵元素的计算.主要由PE控制模块(PEControlModule)、分值计算模块(ComputationModule)、RNA序列存储器(SeqMem)、CM模型存储器(CMmodelinfobuf)和计算结果局部缓存区(Deckbufs)5部分构成.如图8所示,PE控制模块主要包含一个控制状态机和数据IO通路.控制状态机负责实现PE模块的初始化,从片外DDRII存储器中载入RNA序列和CM模型的当前状态信息;启动PE模块的计算,控制数据载入、分值计算、阵列同步以及数据写回和替换.分值计算模块(图8中部的虚线框部分)由END、DSPLR和BIF3个子模块构成,用于实现对应状态分支的计算.3个子模块的内部结构大致相同,都由一个32位加法器和一个32位比较器构成,主要区别在于子模块数据输入来源不同.END状态不需要从外部载入数据,该状态在整个计算过程中被作为特例处理;D、S和L状态的计算只需要从本PE的局部数据缓存区载入数据,而在P、R状态下,主处理单元需要从外部DDRII存储器载入之前的Page11图8PE模块结构中间结果,从处理单元则从相邻的前一个PE载入数据;在分支状态下,BIF计算模块需要从外部DDRII存储器载入左子状态层(layerkleft)和从自身局部缓存区中载入右子状态层(layerkright)的数据.所有子模块的计算结果都先存入局部缓存区中,当本列元素计算完成后再根据当前状态的类型判断是否需要写回DRAM.RNA序列存储器(SeqMem)用于存储当前的RNA序列.系统初始化时,由PE控制模块将RNA序列从DRAM中取出,存入序列存储器.相对于FPGA的存储容量而言,CM模型的存储需求过大,不能将其完整地存储在片内,因此在计算过程中仅将当前状态层计算所需的信息从DRAM中读出缓存在FPGA片内.为了实现对概率矩阵元素的并行查询,我们为每个PE都设置了一个CM模型存储器,采用FPGA片内分布式存储资源实现.计算结果存储器(Deckbufs)由16个相对独立的存储区域(Deckbuf)组成,每一个存储区可以存储三角矩阵的一列,使用FPGA片内存储块实现(BlockRAM),用于存储本PE的计算结果.局部缓存区采用双端口设计,可以直接被本PE和相邻的下一个PE访问.假设当前PE的编号为p,则Deckbufs中存储的是当前section的第p列元素,但这16列元素都位于不同的状态层.每个存储区域都设置了两个寄存器,其中位置寄存器(deck_id)用于记录所存储的数据所处的状态层编号,父状态寄存器(p_cnt)记录还未计算出的本状态层对应的父状态数量(即本状态层元素还要用到的次数).当p_cnt=0时,则释放该存储区,并将其加入可用存储区集合.结果缓存区的设置与CM模型中BIF状态的个数和RNA家族解析树结构相关,16个缓存区已能够满足当前设计的需求.4实验与性能分析4.1并行效率分析硬件CYK/inside算法加速器的执行总时间(T)等于矩阵单元的计算时间(Tc)、同步等待和写回开销(Tm)、数据载入(TLB)以及列转换(sectionadvance)开销之和,其中列转换开销只有在当前区域所有元素计算完成后才会产生,并且每次列转换只需要改变PE的列下标,时间开销为O(1),可以忽略,而数据载入(TL)开销只有在分支状态下才会产生.假设p是处理阵列的规模,L是RNA序列的长度,B为分支状态数量,K为整个CM模型状态数量,参数s=[L/p].由于计算过程中的并行处理方式不同,我们将分支状态和非分支状态分开考虑.在非分支状态下,三角矩阵的计算开销(TC1)为TC1=(p+2p+…+sp)·Δt1,写回开销(TM1)为TM1=110(为平均一个单元的计算开销),Δt2=1(平均一个Page12单元的写回开销).因此,所有非分支状态层的时间开销(TNB)为TNB=(TC1+TM1)·(K-B).在分支状态下,三角矩阵的计算开销(TC2)为TC2=[12p]2·Δt3,写回开销(TM2)为TM2=L·(L+1)Δt4,数据载入开销(TLB)在PE阵列启动当前对角线元素的计算时产生,从masterPE发出片外存储器访问地址,到第一个元素进入PE阵列,每个区域需要发出(s-1)·p次访存请求(s为当前计算的区域编号),平均每次访存需要16个时钟周期,所以TLB=(p+2p+…+sp)·16.这里,Δt3=2(执行一次加法和一次比较操作),Δt4=16(元素按矩阵列顺序写回).因此,所有分支状态层的时间开销(TB)为TB=(TC2+TM2+TLB)·B.我们可以得出总的计算开销(TC)为硬件CYK/inside算法总执行时间(包括计算和写回,按时钟周期数计算)为TNB和TB之和:T=(TC1+TM1)·(K-B)+(TC2+TM2+TLB)·B根据式(4)和(5),CYK/inside算法加速器的并行执行效率(EC)为PE数量表1CYK算法加速器并行效率RNA序列SRPRNAB=4,L=301,K-B=923p=4s=18,Ec=91%s=29,Ec=94%s=76,Ec=93%s=95,Ec=93%s=387,Ec=95%93p=8s=9,Ec=87%s=15,Ec=92%s=38,Ec=91%s=48,Ec=90%s=194,Ec=93%91p=16s=5,Ec=83%s=8,Ec=90%s=19,Ec=88%s=24,Ec=86%s=97,Ec=89%87p=20s=4,Ec=81%s=6,Ec=89%s=16,Ec=86%s=19,Ec=83%s=78,Ec=87%85p=32s=3,Ec=78%s=4,Ec=85%s=10,Ec=82%s=12,Ec=78%s=49,Ec=82%81对相同规模的PE阵列和特定的CM模型而言,即p、K和B都不变,显然α的值将随着序列的长度L的增加而减小,所以并行效率EC增大.但在一般情况下,CM模型的状态总数K和分支状态数量B都会随序列长度的增加而增大,我们将K≈3·L代入式(7)并进行变换得到由于p不变,而B的值本身很小,LB,所以α的值随着L的增大而减小,从而导致并行效率Ec增大.实际上对不同规模的RNA序列而言,随着长EC=TCT=1将TC1,TC2,TM1,TM2和TLB分别代入,当Lp,KB时,可得到α的近似值:根据式(6)和(7),以SSURNA序列(L=1545,K=4789,B=30)为例,当p=16时,加速器的并行执行效率(Ec)接近90%,可见细粒度并行CYK算法具有良好的可扩展性.表1是在不同的阵列规模和输入条件下,根据式(6)计算得出的并行效率,从表中可以看出,随着RNA序列长度和处理器个数的增加,并行效率随之下降.我们可以从式(7)出发分析原因:对同一条RNA序列而言,序列的长度L、CM模型状态的数量K以及分支状态数量B的值都不变,处理器个数p的增加将导致α的值增大,从而导致并行效率EC降低.实际上处理器个数p的增加导致MasterPE的同步等待时间和数据载入开销增大,因为只有最后一个PE计算完成后才能给PE阵列分配新的计算任务;而另一方面PE数量增加缩短了计算时间,导致PE阵列计算部分占整个任务处理时间的比例下降.度L的增加,分支状态数量也随之增加,而根据算法特征4的分析,分支状态层的计算开销占整个开销的90%以上,计算开销部分增长的幅度(为O(L3)量级)远大于写回和数据载入开销(为O(L2)量级),从而在PE阵列规模一定的情况下,对较大规模的问题能够取得更好的加速效果.4.2实验环境我们在测试平台上实现了硬件CYK算法加速器.测试平台由一台通用计算机和一个算法加速器构成.主机配置为Intel双核E5200处理器,2.0GB主存.算法加速器硬件主要包括1片XilinxVirtex5系列FPGA芯片(XC5VLX330),两条总容量为Page134GB的DDRIISODIMM存储条(KingstonKVR667D2S5/2G),加速器通过PCI-E×8数据通道与主机相连.算法加速器支持动态重构,可在60ms内完成不同规模的CM模型间的快速切换,与配置时间为秒级的常规配置方法(如JTAG或并行Slect-MAP)相比,FPGA的配置效率提高了2~3个数量级.CYK/inside软件版本为Infernal-1.0,由美国华盛顿大学医学院SeanEddy实验室开发[22],分别在IntelE5200双核CPU、AMD9650四核以及IntelQ9400四核CPU3种不同平台上运行.4.3FPGA资源利用我们在目前最大规模的商用FPGA器件XilinxXC5VLX330平台上实现了硬件CYK算法加速器.从表2的资源使用情况可以看到,在XC5VLX330上能够实现最大规模为16-PE的处理阵列,片内逻阵列规模片内逻辑资源利用率BRAM存储利用率频率/MHz1-PE16438/207360(8%)25/288(9%)1928-PE50046/207360(24%)137/288(48%)16816-PE88458/207360(42%)265/288(92%)164表3不同平台下的软硬件算法执行时间(s)与加速比测试序列IntelE5200①0.251.005.031.08.601.00279.51.00798.61.00AMD9650②0.221.144.211.17.151.23227.21.23614.31.29IntelQ9400③0.211.194.201.26.371.35224.91.24605.71.32FPGA(8-PE)④0.064.400.796.41.207.1738.77.21111.27.25FPGA(16-PE)⑤0.0357.100.4710.70.6213.920.113.9055.914.30注:硬件环境:①IntelDual-CoreE52002.50GHzCPU,2.0GB内存;②AMDPhenom9650QuadCPU,2.3GHz,3.0GB内存;③IntelCore2Q9400QuadCPU,2.66GHz,3.0GB内存;④FPG加速(8-PE),160MHz,4.0GB内存;⑤FPGA加速器(16-PE),160MHz,4.0GB内存.(2)性能功耗比图9是CYK算法加速器与通用微处理器的性能/功耗对比.我们同样以IntelE5200双核微处理器为参照,3种通用微处理器的平均功耗约为65W~95W,而V5系列中最大规模的XC5VLX330芯片的功耗不超过20W,仅为CPU功耗的1/3~1/5.AMD和Intel四核平台的性能为E5200处理器的1.3倍,但CPU功耗大约是后者的1.5倍,所以就RNA二级结构预测应用而言,3种通用微处理器的性能功耗比数据(t=1000P/W)比较接近,而包含16个PE的FPGA算法加速器的性能功耗比是通用微处理器的30倍以上.此外,使用电流计(型号HIOKI3290)的测试结果表明,配置IntelE5200双核CPU,2.0GB内存的主机在运行Infernal-1.0程辑资源利用率为42%,而存储资源的利用率达到了92%,可见存储资源是系统实现的瓶颈.从时钟频率一栏可以看到,阵列规模从8-PE扩大为16-PE时,加速器的工作频率并没有出现显著的下降,这也从实验上进一步验证了系统的可扩展性.4.4与基于单CPU平台的软件CYK处理方案对比(1)计算时间与加速比我们测试了Infernal-1.0程序在3种不同的通用微处理器平台下的执行时间,并与硬件加速器进行了比较.硬件CYK算法执行时间包括加速器计算时间和数据输入输出开销,FPGA频率为160MHz.从表3可以看到,以Intel双核E5200处理器为参照,串行Infernal程序在3种计算平台上执行的性能比较接近(AMD和Intel四核平台的速度大约为E5200CPU的1.1~1.3倍),但是16-PE的硬件CYK算法加速器性能明显超过这3种通用微处理器.在算法加速器上执行RNaseP序列(长度为379,模型状态数为1176)的二级结构比对可获得接近14倍的加速比,而当测试序列长度为959bps,CM模型状态数为3145时,可获得14.3倍的加速效果.测试平台RnaseP时间加速比序时的平均功耗为150W,而配置一块FPGA加速卡的算法加速平台功耗不超过180W,仅增加了Page1420%,但获得了超过14倍的性能提升.4.5与基于多CPU(Cluster)平台的软件并行CYK处理方案对比(1)计算时间与加速比为了与相关工作[11,13]进行比较,我们选取了两组有代表性的测试序列(RNaseP和SRPRNA)和对应的CM模型进行对比测试.Liu和Tan等人分别在XeonCluster[11](由10个节点构成,每个节点包含2个Intel-Xeon2.0GHzCPU,1GB主存)和AMDCluster[13](由8个节点构成,每个节点包含4个2.4GHzAMDOpteronCPU和8GB主存)平台上使用标准C和MPI函数库实现了无回溯的并行CYK/inside算法.从图10可以看到,在这两条标准测试序列下,细粒度硬件CYK算法的加速效果都优于软件并行方案.以执行RNasePRNA序列的比对为例,在由8个Xeon处理器构成的Cluster上测得的加速比为5.1,按照并行效率换算,在16个Xeon处理器上最多可获得7倍的加速效果;在由8个AMD处理器构成的Cluster上测得的加速比为5.9,在16个AMD处理器上可获得10.5倍的加速效果.而使用包含8-PE的算法加速器可获得接近7倍的加速效果,使用包含16-PE的处理阵列可获得超过14倍的加速比.从图中还可以看到,软件并行方案的加速性能随着处理器个数的增加而下降,而使用算法加速器可获得接近线性的加速果,显示出细粒度硬件CYK算法良好的可扩展性.(2)性能价格比与性能功耗比图11是FPGA算法加速器与基于多CPU的Cluster平台的性能、价格和功耗对比.为了便于比较,所有指标都以包含16个处理器的XeonCluster为参照.从系统造价上看,3种Cluster系统的造价大约在10~16万元之间,而配置一个FPGA(XC5VLX330)算法加速器的通用计算平台的价格大约为3.5万元,仅为Cluster系统的20~30%.从系统功耗上看,目前单个微处理器的平均功耗为65W~95W[20],以平均功耗80W计算,由16个CPU构成的Intel-Xeon、AMD和AlphaCluster平台的功耗大概在2kW~3kW之间.而使用电流计的测试结果表明,包含16个PE的XC5VLX330芯片功耗不超过20W,算法加速器的平均功耗小于30W,整个加速器系统平台(包括主机和加速器)的总功耗不超过200W,仅为Cluster系统功耗的10%左右.从对CYK算法的整体加速效果来看,包含一个FPGA算法加速器的计算平台的性能与包含20个Intel-XeonCPU的PC集群相当,而从性能价格比(s=100P/C)和性能功耗比(t=100P/W)参数来看,基于FPGA平台的细粒度并行CYK算法加速器方案明显优于传统的基于通用微处理器的PCCluster解决方案.5结论基于随机上下文无关文法(SCFG)模型的CYK算法是一类重要的RNA二级结构预测方法,现有的基于大规模并行处理平台的CYK算法加速方案受限于计算通信比,随着问题规模的增长并行处理效率明显下降,此外大规模并行计算机系统硬件设备的购置、使用、日常维护的成本高昂,其适用性受到诸多限制.本文在深入分析CYK算法计算特征的基础上,基于FPGA平台提出并实现了一种细粒度的并行CYK算法.设计采用了对三维动态规划Page15矩阵按状态层方向进行区域分割、区域内逐层计算、状态层内按矩阵列顺序并行处理的任务划分策略,实现了处理单元间的负载平衡;采用数据预取、滑动窗口和数据传递流水线实现处理单元间的数据重用,采用数据驱动的计算流水线隐藏片外数据载入开销,达到每个时钟周期流出一个结果的效果,有效解决了计算和通信间的平衡问题;设计了一种类似脉动阵列(systolic-likearray)结构的主从多PE并行计算阵列,并在单片FPGA(XC5VLX330)上成功集成了16个PE,实验结果表明我们提出的CYK算法加速器结构具备良好的可扩展性.当RNA序列长度为959bps,CM模型状态数为3145时,与运行在Intel双核E5200通用微处理器上的Infernal-1.0软件相比,可获得超过14倍的加速效果.就对CYK算法的加速效果而言,配置一个FPGA算法加速器的通用计算平台的处理性能与包含20个Intel-XeonCPU的PC集群相当,而硬件成本仅为后者的20%,系统功耗不到后者的10%.
