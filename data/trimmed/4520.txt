Page1半监督学习方法刘建伟刘媛罗雄麟(中国石油大学(北京)自动化研究所北京102249)摘要半监督学习研究如何同时利用有类标签的样本和无类标签的样例改进学习性能,成为近年来机器学习领域的研究热点.鉴于半监督学习的理论意义和实际应用价值,系统综述了半监督学习方法.首先概述了半监督学习的相关概念,包括半监督学习的定义、半监督学习研究的发展历程、半监督学习方法依赖的假设以及半监督学习的分类,然后分别从分类、回归、聚类和降维这4个方面详述了半监督学习方法,接着从理论上对半监督学习进行了分析并给出半监督学习的误差界和样本复杂度,最后探讨了半监督学习领域未来的研究方向.关键词半监督学习;有类标签的样本;无类标签的样例;类标签;成对约束1引言半监督学习(Semi-SupervisedLearning,SSL)是机器学习(MachineLearning,ML)领域中的研究热点,已经被应用于解决实际问题,尤其是自然语言处理问题.SSL被研究了几十年,国内外涌现出大量Page2SSL研究的相关情况进行总结.鉴于SSL的理论意义和实际应用价值,本文系统综述SSL方法的研究进展,为进一步深入研究SSL理论和拓展其应用领域奠定一定的基础.本文第2节概述SSL的基本概念、研究历程、依赖的假设及分类;第3节到第6节分别介绍用于分类、回归、聚类、降维问题的SSL方法;第7节对SSL进行理论分析,综述SSL的抽样复杂性和误差界;第8节展望未来的研究方向;第9节对全文进行总结.2半监督学习概述ML有两种基本类型的学习任务:(1)监督学习(SupervisedLearning,SL)根据输入-输出样本对犔={(狓1,y1),…,(狓l,yl)}学习输入到输出的映射f:犡→犢,来预测测试样例的输出值.SL包括分类(Classification)和回归(Regression)两类任务,分类中的样例狓i∈犚m,类标签yi∈{c1,c2,…,cC},cj∈Ν;回归中的输入狓i∈犚m,输出yi∈犚.具有代表性的SL方法有线性判别分析(LinearDiscriminativeAnalysis,LDA)、偏最小二乘(PartialLeastSquare,PLS)、支持向量机(SupportVectorMachine,SVM)、K近邻(K-NearestNeighbor,KNN)、朴素贝叶斯(NaiveBayes)、逻辑斯蒂回归(LogisticRegression)、决策树(DecisionTree)和神经网络等.(2)无监督学习(UnsupervisedLearning,UL)利用无类标签的样例犝={狓1,…,狓n}所包含的信息学习其对应的类标签犢^标签信息把样例划分到不同的簇(Cluster)或找到高维输入数据的低维结构.UL包括聚类(Clustering)和降维(DimensionalityReduction)两类任务.具有代表性的UL方法有K均值(K-Means)、层次聚类(HierarchicalClustering)、主成分分析(PrincipalComponentAnalysis,PCA)、典型相关分析法(CanonicalCorrelationAnalysis,CCA)、等距特征映射(IsometricFeatureMapping,ISOMAP)、局部线性嵌入(LocallyLinearEmbedding,LLE)和局部保持投影(LocalityPreservingProjections,LPP)等.在许多ML的实际应用中,如网页分类、文本分类、基因序列比对、蛋白质功能预测、语音识别、自然语言处理、计算机视觉和基因生物学,很容易找到海量的无类标签的样例,但需要使用特殊设备或经过昂贵且用时非常长的实验过程进行人工标记才能得到有类标签的样本,由此产生了极少量的有类标签的样本和过剩的无类标签的样例[5].因此,人们尝试将大量的无类标签的样例加入到有限的有类标签的样本中一起训练来进行学习,期望能对学习性能起到改进的作用,由此产生了SSL[1-2],如图1所示.SSL避免了数据和资源的浪费,同时解决了SL的模型泛化能力不强和UL的模型不精确等问题.2.1半监督学习研究的发展历程SSL的研究历史可以追溯到20世纪70年代,这一时期,出现了自训练(Self-Training)、直推学习(TransductiveLearning)、生成式模型(GenerativeModel)等学习方法.Scudder[6]、Fralick[7]和Agrawala[8]提出的自训练方法是最早将无类标签的样例用于SL的方法.该方法是打包算法,在每一轮的训练过程中反复运用SL方法,将上一轮标记结果最优的样例和它的类标签一起加入到当前训练样本集中,用自己产生的结果再次训练自己.这种方法的优点是简单,缺点是学习性能依赖于其内部使用的SL方法,可能会导致错误的累积.直推学习的概念最先由Vapnik于1974年提出[1].与归纳学习(InductiveLearning)不同,直推学习只预测当前训练数据和测试数据中无类标签的样例的类标签,而不推断整个样本空间的广义决策规则.Cooper等人提出的生成式模型假设生成数据的概率密度函数为多项式分布模型,用有类标签的样本和无类标签的样例估计该模型中的参数[1].后来,Shahshahani和Landgrebe将这种每类单组分的场景拓展到每类多组分,Miller和Uyar进一步将其推广[1].这一时期,McLachlan等人研究用无类标签的样例估计费希尔线性判别(FisherLinearDiscriminative,FLD)规则的问题[1].对SSL的研究到了20世纪90年代变得更加狂热,新的理论的出现,以及自然语言处理、文本分类和计算机视觉中的新应用的发展,促进了SSL的发展,出现了协同训练(Co-Training)和转导支持向量机(TransductiveSupportVectorMachine,TSVM)等Page3新方法.Merz等人[9]在1992年提出了SSL这个术语,并首次将SSL用于分类问题.接着Shahshahani和Landgrebe[10]展开了对SSL的研究.协同训练方法由Blum和Mitchell[11]提出,基于不同的视图训练出两个不同的学习机,提高了训练样本的置信度.Vapnik和Sterin[12]提出了TSVM,用于估计类标签的线性预测函数.为了求解TSVM,Joachims[13]提出了SVMlight方法,DeBie和Cristianini[14]将TSVM放松为半定规划问题从而进行求解.许多研究学者研究将期望最大算法(ExpectationMaximum,EM)与高斯混合模型(GaussianMixtureModel,GMM)相结合的生成式SSL方法[15-16].Blum等人[17]提出了最小割法(Mincut),首次将图论应用于解决SSL问题.Zhu等人[18]提出的调和函数法(HarmonicFunction)将预测函数从离散形式扩展到连续形式.由Belkin等人[19]提出的流形正则化法(ManifoldRegularization)将流形学习的思想用于SSL场景.Klein等人[20]提出首个用于聚类的半监督距离度量学习方法,学习一种距离度量.研究人员通过理论研究和实验对SSL的学习性能进行了分析.Castelli和Cover[21]在服从高斯混合分布的无类标签的样例集中引入了一个新的有类标签的样本,通过理论分析证明了在无类标签的样例数量无限的情况下,可识别的混合模型的分类误差率以指数形式快速收敛到贝叶斯风险.Sinha和Belkin[22]从理论上研究了当模型不完善时使用无类标签的样例对学习性能产生的影响.Balcan和Blum[23]以及Singh等人[24]用概率近似正确(ProbablyApproximatelyCorrect,PAC)理论和大偏差界理论分析了基于判别方法的SSL方法的性能,给出了说明无类标签的样例何时帮助改进学习性能的相容性函数.Balcan等人[25]在理论上说明了在每个视图给定适当强的PAC学习机,仅依赖比充分冗余假设更弱的假设,也足以使协同训练迭代成功.Goldberg和Zhu[26]将基于图的SSL方法用于情绪分级问题,证明了无类标签的样例能够改进学习性能.Leskes说明当协同训练的不同学习机在相同的给定训练数据集上得到的结果一致时,训练结果的误差减小[27].在SSL成为一个热门研究领域之后,出现了许多利用无类标签的样例提高学习算法预测精度和加快速度的学习方法,因此出现了大量改进的SSL方法.Nigam等人[28]将EM和朴素贝叶斯结合,通过引入加权系数动态调整无类标签的样例的影响提高了分类准确率,建立每类中具有多个混合部分的模型,使贝叶斯偏差减小.Zhou和Goldman[29]提出了协同训练改进算法,不需要充分冗余的视图,而利用两个不同类型的分类器来完成学习.Zha等人[30]提出了一种解决多类标签问题的基于图的SSL方法.Zhou和Li[3]提出了基于差异的SSL方法,利用多个学习机之间的差异性来改进SSL性能,有效地降低了时间损耗,并且提高了学习机的泛化能力.Wu等人[31]引入一种密度敏感的距离度量,并结合基于图的方法,显著提高了算法的聚类性能.Xing等人[32]引入度量学习的思想进行聚类,并通过实验说明用成对约束的马氏距离度量能提高聚类的准确性.Yu等人[33]将类标签信息引入概率PCA模型处理多输出问题,具有较好的可扩展性.Hwa等人[34]将主动学习与SSL相结合,提出一种基于协同训练的主动半监督句法分析方法,实验结果显示该方法可以减少大量的人工标记量.Johnson和Zhang将基于频谱分解的无监督核与基于图的方法结合,提高了预测性能.Mallapragada等人[36]提出一种SSL的改进框架,提高了已有方法的分类准确性.Shin等人[37]提出解决反向边问题的方法,提高了学习性能.Shang等人[38]提出一种新的SSL方法———核归一正则化SSL方法(Semi-SupervisedLearningwithNuclearNormRegularization,SSL-NNR),能同时解决有类标签样本稀疏和具有附加无类标签样例成对约束的问题.Wang等人[39]提出双变量的基于图SSL方法,将二值类标签信息和连续分类函数同时用于优化学习问题.随着SSL技术的发展,SSL已用于解决实际问题.例如,Yarowsky不同的分类器对词义进行消歧,其中一个分类器利用文本中该词的上下文,另一个分类器基于该文本中其他地方出现的该词的意义;Riloff和Jones[40]同时考虑名词及该词出现的语境,实现了对地理位置名词的分类;Collins和Singer[41]同时利用实体的拼写和该实体出现的上下文,完成了对命名实体的分类;Yu等人[42]完成了对中文问题的分类;Li和Zhou[43]对三训方法进行了扩展,并将该方法用于乳腺癌诊断中的微钙化检测;Zhou等人[44]将协同训练用于图像检索;Goldberg和Zhu[26]利用基于图的方法解决了情绪分级问题;Chen等人[45]将标签传播法用于关系抽取;Camps-Valls等人[46]提出基于Page4图的混合核分类方法,并将其应用于解决超光谱图像问题;Cheng等人[47]提出一种基于半监督分类器的粒子群优化算法用于解决中文文本分类问题;Zhang等人[48]提出一种基于图的多样例学习方法用于各种视频领域研究;Carlson等人[49]将耦合SSL用于从网页提取类别和关系的信息;Guillaumin等人[50]将多模态SSL用于图像分类;He[51]将半监督子空间学习用于图像检索;Balcan等人[52]用基于图的SSL方法进行低质量摄像头图像中的身份识别;Wang等人[53]提出半监督散列方法用于处理大规模图像检索问题.2.2半监督学习依赖的假设SSL的成立依赖于模型假设,当模型假设正确时,无类标签的样例能够帮助改进学习性能[10].SSL依赖的假设有以下3个:(1)平滑假设(SmoothnessAssumption).位于稠密数据区域的两个距离很近的样例的类标签相似,也就是说,当两个样例被稠密数据区域中的边连接时,它们在很大的概率下有相同的类标签;相反地,当两个样例被稀疏数据区域分开时,它们的类标签趋于不同.(2)聚类假设(ClusterAssumption)[1,54].当两个样例位于同一聚类簇时,它们在很大的概率下有相同的类标签.这个假设的等价定义为低密度分离假设(LowSensitySeparationAssumption),即分类决策边界应该穿过稀疏数据区域,而避免将稠密数据区域的样例分到决策边界两侧.(3)流形假设(ManifoldAssumption)[4,55].将高维数据嵌入到低维流形中,当两个样例位于低维流形中的一个小局部邻域内时,它们具有相似的类标签.许多实验研究表明当SSL不满足这些假设或模型假设不正确时,无类标签的样例不仅不能对学习性能起到改进作用,反而会恶化学习性能,导致SSL的性能下降.但是还有一些实验表明,在一些特殊的情况下即使模型假设正确,无类标签的样例也有可能损害学习性能[55].例如,Shahshahani和Landgrebe[10]通过实验证明了如何利用无类标签的样例帮助减轻休斯现象(HughesPhenomenon)(休斯现象指在样例数量一定的前提条件下,分类精度随着特征维数的增加先增后降的现象),但是同时实验中也出现了无类标签的样例降低学习性能的情况.Baluja[56]用朴素贝叶斯分类器和树扩展朴素贝叶斯(TreeAugmentedNaveBayesian,TAN)分类器得到很好的分类结果,但是其中也存在无类标签的样例降低学习性能的情况.Balcan和Blum[57]提出容许函数使分类器能够很好的服从无类标签的样例的分布,但是这种方法同样会损害学习性能.2.3半监督学习的分类SSL按照统计学习理论的角度包括直推(Transductive)SSL[58]和归纳(Inductive)SSL两类模式.直推SSL只处理样本空间内给定的训练数据,利用训练数据中有类标签的样本和无类标签的样例进行训练,预测训练数据中无类标签的样例的类标签;归纳SSL处理整个样本空间中所有给定和未知的样例,同时利用训练数据中有类标签的样本和无类标签的样例,以及未知的测试样例一起进行训练,不仅预测训练数据中无类标签的样例的类标签,更主要的是预测未知的测试样例的类标签.从不同的学习场景看,SSL可分为4大类:(1)半监督分类(Semi-SupervisedClassifica-tion)[11,59].在无类标签的样例的帮助下训练有类标签的样本,获得比只用有类标签的样本训练得到的分类器性能更优的分类器,弥补有类标签的样本不足的缺陷,其中类标签yi取有限离散值yi∈{c1,c2,…,cC},cj∈Ν.(2)半监督回归(Semi-SupervisedRegression)[60-61].在无输出的输入的帮助下训练有输出的输入,获得比只用有输出的输入训练得到的回归器性能更好的回归器,其中输出yi取连续值yi∈犚.(3)半监督聚类(Semi-SupervisedClustering)[62-63].在有类标签的样本的信息帮助下获得比只用无类标签的样例得到的结果更好的簇,提高聚类方法的精度.(4)半监督降维(Semi-SupervisedDimensionalityReduction)[64].在有类标签的样本的信息帮助下找到高维输入数据的低维结构,同时保持原始高维数据和成对约束(Pair-WiseConstraints)的结构不变,即在高维空间中满足正约束(Must-LinkConstraints)的样例在低维空间中相距很近,在高维空间中满足负约束(Cannot-LinkConstraints)的样例在低维空间中距离很远.为便于更加清晰地介绍各种SSL方法,这里按照图2对各种SSL方法进行归类.Page53半监督分类方法,…,狓testt半监督分类问题是SSL中最常见的问题,其中有类标签的样本数量相比聚类问题多一些,引入大量的无类标签的样例犝={狓l+1,…,狓l+u}和犜={狓test1(狓l,yl)}不足的缺陷,改进监督分类方法的性能,训练得到分类性能更优的分类器,从而预测无类标签的样例的类标签.其中样例狓i∈犚m,类标签yi∈{c1,c2,…,cC},i=1,…,l,…,l+u,…,l+u+t,训练样例数量为ntrain=l+u,测试样例数量为ntest=t.主要的半监督分类方法有基于差异的方法(Disa-greement-BasedMethods)、生成式方法(GenerativeMethods)、判别式方法(DiscriminativeMethods)和基于图的方法(Graph-BasedMethods)等,下面分别对这几种方法进行描述与分析.3.1基于差异的方法ML中的数据有时可以用多种方式表示其特征.例如,在网页分类问题中,网页可以用每页出现的词描述,也可以用超链接描述;癌症诊断可以用CT、超声波或MRI等多种医学图像技术确定患者是否患有癌症.基于这些朴素的思想,产生了基于差异的方法.1998年,Blum和Mitchell[11]提出了协同训练方法.如图3所示,协同训练方法的基本训练过程为:在有类标签的样本的两个不同视图(View)上分别训练,得到两个不同的学习机,然后用这两个学习机预测无类标签的样例的类标签,每个学习机选择标记结果置信度最高的样例和它们的类标签加入另一个学习机的有类标签的样本集中.这个过程反复迭代进行,直到满足停止条件.这个方法需要满足两个假设条件:(1)视图充分冗余(SufficientandRedundant)假设,即给定足够数量的有类标签的样本,基于每个视图都能通过训练得到性能很好的学习机;(2)条件独立假设,即每个视图的类标签都条件独立于另一视图给定的类标签.许多研究人员通过理论分析和实验证明了基于差异的方法的有效性.Dasgupta等人[65]从理论上说明,当训练数据满足视图充分冗余假设时,基于差异的方法通过使基于不同视图的学习机在无类标签的样例上的一致性达到最大化,得到相同的分类预测结果,可以降低误分类率.Zhou等人[66]证明当训练数据满足视图充分冗余假设时,即使只给定一个有类标签的样本,也能有效地进行SSL.Wang和Zhou[67]进行了理论证明和实验验证,理论结果显示出,基于差异的方法并不是必须具备多个视图,为单视图类型的方法提供了理论支持.尽管基于差异的方法已经广泛应用于许多实际领域,如统计语法分析、名词短语识别等,但是在大多数实际问题中,训练数据往往不满足视图充分冗余假设.因此,研究人员开始致力于研究基于放松的视图充分冗余假设或不需要满足视图充分冗余假设的基于差异的方法.Nigam和Ghani[68]在不具有充分冗余视图的问题上对基于差异的方法的性能进行了研究,通过实验证明,将训练数据随机划分到两个视图后,基于差异的方法的误分类率明显降低.2000年,Goldman和Zhou[69]提出基于差异的改进方法,这个方法不需要训练数据满足视图充分冗余假设,而Page6是用两个不同的SL方法,将样本空间分到一组等价类中,通过交叉校验来确定如何对无类标签的样例进行标记.2002年,Abney签的样例的一致性最大化的贪婪算法,在命名实体分类基于差异的训练实验中产生好的学习效果.2003年,Clark等人[55]提出间接寻找无类标签的样例的最大一致性的朴素基于差异的训练过程.2004年,Zhou和Goldman[29]通过使用多个不同类型的学习机对之前提出的基于差异的训练改进方法进行了扩展,在一定程度上放宽了标准协同训练方法的假设条件,但是这个方法要求两个学习机所采用的学习方法能够将样本空间划分为等价类集合,而且训练过程耗时很大.为了解决这个问题,2005年,Zhou和Li[71]提出了三训方法(Tri-Training),用三个学习机分别进行训练,按投票选举的方式间接得到标记置信度,如果两个学习机对同一个无类标签的样例的预测结果相同,则认为该样例具有较高的标记置信度,将其与它的类标签加入到第三个学习机的训练数据集中.他们在UCI数据集和网页分类问题上进行实验,证明能够有效地利用无类标签的样例提高学习机性能.三训方法利用三个学习机来选择标记置信度,不仅有效地降低了时间耗费,而且能够利用集成学习提高学习机的泛化能力.但是当初始学习机性能较差时,在训练过程中将会引入噪声,导致预测精度下降.为此,2007年,Li和Zhou[43]对三训方法进行了扩展,提出可以更好发挥集成学习作用的Co-Forest方法,并将这个方法用于乳腺癌诊断中的微钙化检测,通过实验证明这个方法能够有效提高预测精度.基于差异的方法由于性能优越而得到了广泛的应用,由此出现了许多变形[72].Nigam和Ghani[68]提出协同EM方法,只用有类标签的样本初始化第一视图学习机,然后用这个学习机以概率方式标记所有无类标签的样例,第二视图学习机训练所有数据,将得到的新的样本提供给第一视图学习机进行再训练.这个过程反复迭代进行,直到学习机的预测结果收敛.Steedman等人[73]提出了一种基于差异训练的统计句法分析方法,用两个功能完整的不同统计句法分析机进行基于差异的训练,通过实验证明,基于差异的训练方法能够显著提高句法分析机的性能.Hwa等人[34]将主动学习与SSL相结合,提出一种基于差异训练的主动半监督句法分析方法,在学习过程中,一个学习机挑选并标记自己最确定的样本给另一个学习机,而另一个学习机则挑选自己最不确定的样本请用户标记后再提交给该学习机用于模型更新.他们的研究结果表明,该方法可以减少大约一半的人工标记量.Zhou等人[44]将基于差异的训练引入图像检索,提出了基于差异训练的主动半监督相关反馈方法.Wang和Zhou[74]将基于差异的方法和基于图的方法结合.Yan等人[75]提出一种概率SSL模型,用多个分类器进行学习,并通过实验证明了该方法的优越性能.3.2生成式方法生成式方法假定样例和类标签由某个或有一定结构关系的某组概率分布生成,已知类先验分布p(y)和类条件分布p(狓y),重复取样y~p(y)和狓~p(狓y),从这些分布中生成有类标签的样本犔和无类标签的样例犝.根据概率论公理得到后验分布p(y狓),找到使p(y狓)最大的类标签对狓进行标记[76-77].生成样例的模型有高斯模型、贝叶斯网络、S型信度网(SigmoidalBeliefNetworks)、GMM、多项混合模型(MultinomialMixtureModel,MMM)、隐马尔可夫模型(HiddenMarkovModel,HMM)和隐马尔可夫随机场模型(HiddenMarkovRandomField,HMRF)等.(1)高斯模型[10]中的样例服从高斯分布p(狓y)=N(狓μ,Σ)=(2π)D/2Σ1/2exp-式(1)中μ是均值,Σ是协方差阵.(2)贝叶斯网络[78]中的样例的概率分布如图4所示.(3)S型信度网[79]中的样例服从概率分布p(狓ipa(狓i))=式(2)中pa(狓i){狓1,狓2,…,狓i-1}表示狓i的父节Page7点,犑ij和hi是网络中的权值和偏差.(4)GMM[80]是多个高斯分布的混合分布模型,假定样例由多个模型加权混合生成∑i∑iπi=1,每个模型的分布服从式(1)的高斯分布.(5)MMM是多个多模态分布的混合分布模型,假定样例由多个模型加权混合生成∑i∑iπi=1,每个模型的分布服从多模态分布p(狓=(x·1,…,x·d)μ)=式(3)中μ是多个模态共同选择的概率向量,D是模态数.(6)HMM[81]用于建立样例序列的模型,指定状态间的转移概率矩阵按一定周期从一个状态转移到另一状态来形成序列,序列中每个样例由隐状态生成,其中状态条件分布可以是高斯混合分布或多模态混合分布.当前状态只依赖前一状态,并且输出只依赖当前状态.(7)HMRF[82]的每个模型都与之前的模型无关.定义两个随机场:隐随机场犡H和可观测的随机场犡.根据MRF的局部特性,当给定犡H和它的领域犡N,(犡H,犡)的联合概率分布为p(狓,狓H狓N)=p(狓狓H)p(狓H狓N).犡的边缘条件概率依赖于参数θ=(μ,Σ)和犡H的领域分布犡Np(狓狓N,θ)=∑l∈L式(4)中l∈犔为犡H的取值空间,p(狓;θl)为犡的条件概率分布.常用的生成式方法是朴素贝叶斯分类器,假设样例的各属性条件独立,对样例狓i={xi1,xi2,…,xim},i=1,…,n进行分类的过程实际就是利用朴素贝叶斯计算狓i的类标签yi∈{c1,c2,…,cC}的后验概率,然后将狓i标记为具有最高后验概率的类标签yi.其目标函数为yi=argmax也可以用逻辑斯蒂回归进行训练,预测样例的类标签式(6)中θ是调节的参数向量.常用极大似然估计(MaximumLikelihoodEstimation,MLE)或最大后验估计(MaximumAPosteriori,MAP)求解这个问题[83-84].混合模型或更复杂的生成式模型的目标函数非凸并且难于优化,无法用MLE或MAP解析地求解,这种情况下通常用直接梯度下降法或EM算法等迭代算法求解得到局部极大值.3.3判别式方法判别式方法利用最大间隔算法同时训练有类标签的样本和无类标签的样例学习决策边界,如图5所示,使其通过低密度数据区域,并且使学习得到的分类超平面到最近的样例的距离间隔最大[85-86].判别式方法包括LDA、广义判别分析法(Gen-eralizedDiscriminantAnalysis,GDA)、半监督支持向量机(Semi-SupervisedSupportVectorMachine,S3VM)、熵正则化法和KNN法等.LDA也叫做费希尔线性判别法(FisherLinearDiscriminativeAnalysis,FDA),最初由Fisher[87]于1936年提出,其基本思想是将样例投影到合适维数的低维空间中,使投影后的样例在新的子空间中有最大的类间距离和最小的类内距离,即样例按照类别能被分成许多簇.Baudat和Anouar[88]将LDA发展到多类问题,提出GDA.通过一个非线性映射,将样例映射到高维特征空间,在这个特征空间中运用FDA进行训练.LDA和GDA用于SSL时,有一部分样例的类标签的值是不知道的,目标是要求解类标签的值,由于yi∈{c1,c2,…,cC},因此这是一个混合整数规划问题.TSVM最初由Vapnik和Sterin[12]提出,用于估计类标签的线性预测函数f(狓)=狑T狓+b,由于它也可以用来估计未知的测试样例的类标签,实际得到的是整个样例空间上的决策边界,不是严格的直推方法,而是归纳半监督方法,因此被称为S3VMs.TSVM的目标函数为min狑,b,犢u12式(7)中V(y,f(狓))是损失函数,两个正则化参数C1和C2,是有类标签和无类标签的样例上损失的加Page8权值,分别用于权衡有类标签和无类标签的数据上的复杂度和实际误差.式(7)非凸并且难以找到全局最优解,可以用半定规划(Semi-DefiniteProgramming,SDP)、分支界定法(BranchandBround,BB)、确定性模拟退火算法(DeterministicAnnealing,DA)和同伦连续法等方法求解.熵正则化法[1,89-90]采用香农条件熵来度量类之间的重叠程度,其目标函数为KNN法[91]在所有样例中找到与测试样例距离最近的k个近邻样例,其中各类所占的数量为kj,j=c1,…,cC,用决策规则argmax择类标签对样例进行标记.3.4基于图的方法基于图的方法的实质是标签传播(LabelProp-agation),基于流形假设,根据样例之间的几何结构构造图(Graph),用图的结点(Vertice)表示样例,利用图上的邻接关系将类标签从有类标签的样本向无类标签的样例传播.如图6所示,基于图的方法的基本训练过程为:(1)选择合适的距离函数计算样例间的距离.可供选取的距离函数有欧氏距离、曼哈顿距离、切比雪夫距离、明氏距离、马氏距离和归一化欧氏距离等;(2)根据计算得到的距离选择合适的连接方式,将样例用边(Edge)连接,构造连接图.构造的连接图分为稠密图(DenseGraph)和稀疏图(SparseGraph),稠密图的典型代表是全连接图,如图7(a)所示,任意两个结点之间都有边连接;稀疏图如图7(b)所示,按照某种准则将距离最近的某几个结点连接,包括KNN图、ε近邻(ε-NearestNeighbor,εNN)图、正切权图和指数权图等;(3)用核函数(Kernel)给图的连接边赋权值(Weight),用权反映这个边所连接的两个结点之间的相似程度,当两个结点狓i和狓j距离很近时,连接这两个结点的边的权wij就很大,这两个样例有相同的类标签的概率就很大;反之,当两个结点狓i和狓j距离很远时,连接这两个结点的边的权wij就很小,这两个样例有相同类标签的概率就很小.常用的核函数有线性核、多项式核、高斯核、径向基核、双曲正切核、神经网络核、费希尔核和样条核等;(4)根据学习目标确定优化问题并求解.半监督分类问题的目标是找到使目标函数最小的类标签的预测函数f(狓),这个问题可以看作是一个由损失函数和正则化函数组成的复合目标函数的正则化风险最小化问题[92-94],因此基于图的方法解决半监督分类问题的目标函数一般表示为式(9)中损失函数V(y,f(狓))用来惩罚样例的预测类标签不等于给定类标签的情况,正则化函数Ω(f)用来保证预测函数的平滑性,使近邻点的预测类标签相同.根据具体的学习任务可以选择不同的损失函数和正则化函数,如损失函数可以选取平方误差函数、绝对值函数、对数函数、指数函数和铰链损失函数等.一般将损失函数和正则化函数限制在再复制核希尔伯特空间(ReproducingKernelHilbertSpace,RKHS)中,用核学习算法求解学习机[95].2001年,Blum和Chawla[17]提出第一个基于图的SSL方法———最小割法(Mincut),将已标记为正的样例看作源结点,已标记为负的样例看作目标结点,找到一组边,使移除这些边之后,源结点和目标结点之间没有连接,即图被分割成两个独立的云团,将Page9这组边称为图割(Cut).当图被分割后,将连接源结点的结点类标签标记为正,将连接目标结点的结点类标签标记为负.这个方法选择带无限权的平方损失作为损失函数V(y,f(狓))=·∑用割的大小表示正则化函数Ω(f)=犳TL犳,并且限制在任意有类标签的结点狓i上,预测类标签等于给定类标签f(狓i)=yi.随后,Blum等人[96]通过人工引入边权的随机噪声,指出基本最小割法的一些缺点,并对最小割法进行改进.求解最小割法的目标函数实际上就是求解函数f(狓)∈{+1,-1},如果忽略f(狓)上的整数约束,那么这个问题就变为凸二次优化问题,此时有近似形式的解.但是与此同时,在没有约束的情况下,无法得到唯一的确定解.为了解决这个问题,Zhu等人[97]提出比例割法(RatioCut),在正则化函数中引入分割后云团的势来使分割后的云团所包含结点的数量平衡,可以得到唯一的解.但是在许多实际情况中,边的权存在较大差异,此时这种比例割法不一定能得到较好的解.为此,Zhou等人[98]提出归一化割法(NormalizedCut),损失函数同时惩罚有类标签和无类标签的样例上预测类标签不同于给定类标签的情况,同时为了充分考虑到分割后云团中权的分布的平衡特性,在正则化函数中引入分割后云团的大小.2003年,Zhu等人[18]提出调和函数法(HarmonicFunction),也称为高斯随机场法(GaussRandomField),在图上建立预测函数的分布模型,从而将离散预测函数扩展为连续预测函数.与最小割法一样,这个方法也选择带无限权的平方损失作为损失函数,用割的大小表示正则化函数,区别只在于预测函数取连续值,而不是直接等于类标签值,充分考虑了样例的分类概率,解决了最小割法不能解决的问题.2004年,Belkin和Niyogi[99]提出拉普拉斯正则化法(LaplacianRegularization),假定训练数据分布在流形(Manifold)上,用离散频谱和近邻图的特征函数将学习问题嵌入到希尔伯特空间中.Zhou等人[100]用归一化图拉普拉斯算子阵作为正则化因子,提出了一种迭代的标签传播法.上面介绍的方法都是转导SSL方法,无法直接处理训练数据之外的数据,必须重新构造图进行重新计算,并且这些方法都限制有类标签的样本的预测类标签必须等于给定类标签,无法解决训练数据中有噪声的问题.2005年,Chen和Wang图拉普拉斯算子组合产生式混合模型和判别式正则化项来克服非归纳和适用性的限制.2006年,Belkin等人[19]将流形学习的思想用于SSL场景,提出一种归纳SSL方法———流形正则化法(ManifoldRegularization).这个方法并不限制有类标签的样本的预测类标签必须等于给定类标签,损失函数为V(y,f(狓))=犳-犢l中一个Ω1(f)=犳T犔犳用来控制预测函数的复杂性,用图拉普拉斯算子阵使f平滑变化,另一个Ω2(f)=犳2的作用是保持样本分布的内在流形结构,防止f在测试数据上发生急剧改变,波动太大.2008年,Goldberg等人[102]提出在线流形正则化结构,提高了流形正则化在大规模数据和实时问题中的适用性.Yan和Wang构,l1图的想法来源于每个数据可以通过训练数据的稀疏线性叠加进行重构,通过解决稀疏表示上的l1优化问题得到稀疏重构系数,用得到的系数推断出有向l1图的权值,以参数自由的方式同时得到l1图中的近邻结构和权值,并进行人脸识别和图像分类实验,实现结果显示出该方法的性能比传统的图方法更加优越.Liu等人[104]提出一种处理大型数据集的SSL方法,用图正则化执行学习过程.Dhillon等人[105]提出一种用于半监督结构化输出学习的新方法,用无类标签的样例上的放松标记来解决类标签空间的组合特性,并进一步用领域约束来指导学习.Breve等人[106]提出一种新的基于图的半监督分类模型,用粒子的组合随机贪婪行走结合竞争和合作机制,将类标签传播到整个网络.Zhang等人[48]提出快速基于图半监督多样例学习算法,用于搜索并训练小规模专家标记视频和大规模未标记视频得到模型.近年来,也出现了许多将基于图的方法与其他方法相结合的SSL方法.Sindhwani等人[107]将流形正则化嵌入到定义在整个输入空间的SSL核学习结构中,产生新的RKHS.Tang等人[108]提出基于敏感型结构的基于图的SSL方法.He等人[109]提出一种基于图的生成式SSL方法,利用图的传播来估计类条件概率,用线性回归估计类先验信息.Johnson和Zhang的SSL方法结合,提高了预测性能.Mallapragada等人[36]提出一种SSL的改进框架———SemiBoost,利用无类标签的样例提高已有的SL方法的分类准确性.Zha等人[30]提出用于多类标签SSL场景的新的基于图的学习结构.Zhang和Wang动构造最优图的线性近邻传播法,接着提出使这个方法能用于大型数据集的多层次方法.Page10影响SSL性能的除了SSL方法的性能之外,还有图本身的性能,图的构造的好坏甚至对学习性能起决定性作用.因此,有关图的学习问题也受到一些研究人员的关注与研究.Carreira-Perpinan和Zemel[111]提出构造用于学习的鲁棒图;Wang和[112]用局部线性嵌入的思想得到图的连接边Zhang的权;Hein和Maier[113]尝试移除噪声数据来得到更好的图;Shin等人[37]提出解决反向边问题的方法,即当图的连接边从无类标签的样例指向有类标签的样本时,调整这个连接边的权来减少不确定信息的传播,从而提高学习性能.基于图的方法性能优越,具有坚实的数学理论基础、计算速度快及准确性高等优点,已被用于解决一些实际问题.例如,Goldberg和Zhu[26]利用基于图的方法解决了情绪分级问题,Chen等人[45]将标签传播法用于关系抽取,Camps-Valls等人[46]提出基于图的混合核分类方法,并将其应用于解决超光谱图像问题.4半监督回归方法与半监督分类方法一样,半监督回归方法引入,…,狓testt大量的无输出的输入犝={狓l+1,…,狓l+u}和犜={狓test1(狓l,yl)}的不足,改进监督学习方法的性能,训练得到性能更优的回归器,从而预测输入的输出.其中输入狓i∈犚m,输出yi∈犚,i=1,…,l,…,l+u,…,l+u+t,训练输入数量为ntrain=l+u,测试输入数量为ntest=t.虽然在SL中,回归问题与分类问题近乎同等重要,但是对SSL方法的研究主要集中在半监督分类问题上,而对半监督回归问题的研究比较有限.产生这个现象的一个原因是半监督分类中的聚类假设对回归问题不一定成立,因此不能直接将大多数半监督分类方法用于回归问题.但值得庆幸的是,流形假设在回归问题中仍然成立,因此,利用特征空间中的局部平滑性的基于流形学习的半监督回归方法是可行的.主要的半监督回归方法有基于差异的方法和基于流形学习的方法等,下面分别对这几种方法进行描述与分析.4.1基于差异的方法Zhou和Li[114]最早用协同训练方法进行半监督回归,提出基于协同训练的半监督回归方法———COREG(Co-TrainingRegressors),用两个不同阶明氏距离的KNN回归器作为学习机,每个学习机挑选无输出的输入并进行回归预测,然后加入对方的训练集中供对方学习,最后的回归预测结果是两个学习机的预测平均值.这个方法不要求充分冗余视图,用不同的距离度量或近邻数寻找两个回归机之间的差异,而不需要两个视图的输入,适用于没有自然属性分割的回归问题.Brefeld等人[60]提出一种半监督最小二乘回归方法———coRLSR(co-RegularisedLeastSquaresRegression),将协同训练用于希尔伯特空间中的归一化风险最小化问题,并提出线性计算无输出的输入数量的半参数近似法.Ma和Wang监督回归模型,使用两个SVM回归模型进行协同训练,适用于解决缺少大量有输出的输入的情况,缓解了只使用单一回归模型造成的错误累加,提高了回归模型的泛化能力,同时由于SVM的优势,可用于解决小样本、非线性回归问题.4.2基于流形学习的方法Wang等人[116]提出半监督核回归法(Semi-SupervisedKernelRegression,SSKR),利用所有观察到的有输出的和无输出的输入,用加权因子调节无输出的输入的影响,并通过实验说明这个方法比传统的核回归和基于图的半监督回归方法更有效.Verbeek等人[117]将高斯场框架用于高维数据上的半监督回归.提出基于熵最小化和极大似然模型选择法的主动学习策略,并将广义LLE用于高斯场框架.Pozdnoukhov等人[118]将Belkin和Niyogi[99]提出的方法用于回归问题,通过实验得到这个方法仅适用于线性ε不敏感损失函数的回归问题.Yang等人[119]提出拉普拉斯正则化框架,导出基于一类广义损失函数的拉普拉斯半监督回归,能够利用数据所在流形的内在几何结构进行回归估计,并给出几种损失函数的拉普拉斯半监督回归方法.Navaratnam等人[120]说明如何利用来自边际分布的无输出的输入改进拟合,并用高斯过程隐变量模型学习共隐低维流形特征和参数空间的映射.Ji等人[121]提出用于半监督回归问题的算法,将从有类标签和无类标签的样例得到的积分算子的最初几个特征函数看做基函数,利用简单的线性回归过程学习预测函数.基于流形学习的半监督回归方法利用数据所在流形的内在几何结构进行回归,但是由于流形学习的复杂性,这种方法中的参数较多,并且没有一种指导性的选择参数的方法.Page115半监督聚类方法半监督聚类问题与分类和回归问题不同,在大量的无类标签的样例犝={狓l+1,…,狓l+u}中引入少量的有类标签的样本犔={(狓1,y1),…,(狓l,yl)},用有类标签的样本包含的监督信息指导算法,将样例犡={狓1,…,狓n}划分到c个簇犆1,…,犆c中,提高聚类的性能.其中样例狓i∈犚m,类标签yi∈{c1,c2,…,cC},i=1,…,l,…,l+u,训练样例数量为n=l+u,簇犆k的中心为犿k(k=1,…,c).SSL中可利用的监督信息除了类标签之外,还有成对约束.成对约束包括正约束和负约束,正约束指两个样例属于同一类,负约束指两个样例不属于同一类[122].将正约束集记为犕犔,负约束集记为犆犔.主要的半监督聚类方法有基于距离的方法和大间隔方法等,下面分别对这些方法进行描述与分析.5.1基于距离的方法基于距离的方法通过学习样例之间的距离,将距离很近的样例划分到同一簇,将距离很远的样例划分到不同簇.根据具体的学习方式,基于距离的方法可以分为基于距离度量的方法、基于约束的方法及非线性方法.5.1.1基于距离度量的方法基于距离度量的方法通过训练得到某种自适应距离度量,使其满足类标签或成对约束,然后用学习到的距离度量执行聚类.常用的距离度量有马氏距离[32]、改进的欧氏距离[20]和K-L离差(Kullbac-LeiblerDivergence)等.一种常用的基于距离度量的聚类方法是谱聚类(SpectralClustering)[123],用加权图的结点表示样例,连接结点的边的权表示两个样例之间的相似度,通过将图的结点分割到不同的部分得到簇.2002年,Klein等人提出第一个半监督距离度量学习聚类方法,根据被正约束影响的相似图中的最短路径学习一种距离度量[20].他们通过研究认为正约束在样例上具有二值传递关系,根据这种传递关系可以将正负约束进行传播以反映样例的空间分布信息.这个方法首先通过求最短路径施加正约束,得到度量矩阵,再利用完全链接层次聚类算法间接施加负约束.Xing等人[32]定义一种距离度量d(狓i,狓j)=d犃(狓i,狓j)=狓i-狓j犃=(狓i,狓j)T犃(狓i,狓j和狓j相似,则(狓i,狓j)∈犕犔;若狓i和狓j不相似,则(狓i,狓j)∈犆犔.目标函数为凸优化问题Ng等人[124]提出一种谱聚类算法,首先计算邻接矩阵犃∈犚n×n,若(狓i,狓j)∈犕犔,则Aij=Aji=1;若(狓i,狓j)∈犆犔,则Aij=Aji=0,且Aii=0,然后构造矩阵犔=犇-素为Dij=∑所对应的特征向量狊1,狊2,…,狊k,构造犛=[狊1,狊2,…,狊k]∈犚n×k.对犛的每一行进行单位化处理,得到矩阵犕,Mij=空间中的一个点,使用K均值法或其他方法进行聚类.若犕的第i行分配到cj类,则将狓i也分配到cj类.Kamvar等人[125]提出的谱聚类算法与Ng等人[125]提出的谱聚类算法基本相同,区别只在于他们对犔的构造方式进行了改进,犔=dmax为对角线元素的最大值.Basu等人[62]提出一种HMRF法,将距离度量与约束条件相结合来解决图像分割问题,充分利用了相邻模型之间的相关信息,克服了均值场算法对初始化条件要求非常苛刻的缺点.Ji和Xu[126]提出半监督归一化割谱聚类法(Semi-SupervisedSpectralClusteringwithNormalizedCuts,SS-SNC),利用监督信息用谱方法改变有成对约束信息的聚类距离度量.Zhang和Li[127]提出基于密度的约束扩展方法(Density-BasedConstraintExpansionMethod,DCE),将样例的结构信息引入聚类,在约束条件较少,不足以反映样例分布特点时,可以得到更好的聚类效果,扩展后的约束集可用于各种半监督聚类算法.Wu等人[31]提出密度敏感的半监督聚类法(Density-SensitiveSemi-SupervisedClustering,DS-SC),引入一种密度敏感的距离度量,能较好地反映聚类假设,并充分利用样例中复杂的内在结构信息,同时与基于图的SSL方法相结合,使算法在聚类性能上有了显著的提高.随后,Wu等人[128]又提出一种改进的密度敏感的半监督聚类法,得到一种改进的密度敏感的距离度量,可以有效地增大位于不同稠密区域的样例的距离,并缩小位于同一稠密区域内的样例的距离.Luo和Page12[129]提出双相似性度量半监督聚类法(DoubleWangSimilarityMeasureSemi-SupervisedClustering,DMSC),结合主空间和辅助空间来共同影响聚类过程,引入两个近邻度量函数,一个基于辅助空间,采取K-L离差,另一个基于主空间.这个方法不易陷入局部最优,受初始点影响小,可以提高聚类的有效性,但是必须找到合适的k值和下降函数.Bijral等人[130]以基于密度的距离估计为基础,提出一种用图上的最短路径进行计算的简单有效的方法,该方法适用于稠密的全连接图情况,能有效减少运行时间.5.1.2基于约束的方法在许多实际问题中,成对约束信息比类标签信息更普遍,如在语音识别、GPS导航和图像检索等问题中往往不知道样例的类标签,而知道两个样例是否属于同一类.基于约束的方法通过将约束条件加入目标函数或修改目标函数使其满足约束条件来进行训练,得到更合适的数据划分.2000年Wagstaff和Cardie[122]提出成对约束之后,许多研究人员对利用成对约束的聚类方法进行了研究.一种基本的半监督聚类方法是K均值法[131],其目标函数为式(11)中ni为第i个簇的样例数量,犿i=Wagstaff等人[35]将成对约束引入K均值聚类法中,提出约束K均值法(ConstrainedK-MeansClustering,COP-K-Means),调整簇的数量使其满足成对约束,并通过大量说明这个方法可以减少迭代次数和聚类的总运行时间,在给定较好的初始解的情况下能够提高聚类的准确性.Basu等人[132]在K均值法的基础上,引入由少量已标记的样本组成的seed集,采用EM算法进行优化,提出两种半监督K均值法———Seeded-K-Means和Constrained-K-Means.Klein等人[20]通过使两两近似的样例变形将成对约束引入完全连接聚类中.Shental等人[80]在EM算法中考虑成对约束,能够提高混合模型簇和类标签的相似度.Xing等人[32]将梯度下降法和迭代映射组合作为凸优化问题,利用成对约束学习用于K均值的马氏距离,引入学习度量的思想进行聚类,并通过实验用数据说明用成对约束的马氏距离度量能提高K均值聚类的准确性.Bar-Hillel等人[133]以及Chang和Yeung验.Basu等人[62]和Lange等人[135]将成对约束信息加入K均值聚类法中,在违反概率簇分配的约束上增加了惩罚项.Davidson和Ravi[136]提出一种K均值式算法,使约束向量量化误差最小,但是这种方法在每次迭代中并不一定满足所有约束.Cohn等人[137]和Jain等人[138]提出允许迭代地提供一些数据上的反馈的聚类方法.Bilenko等人[139]将基于约束的方法和距离度量学习相结合,为每个簇学习一个单独的距离度量.Gao等人[140]综合考虑有类标签的样本的背景信息与无类标签的样例的特征,将问题表示为有约束的优化问题,并提出两种学习方法解决硬聚类和模糊聚类问题.Tang等人[141]提出了一种改进的高维数据半监督聚类的方法,利用约束指导特征映射,而不是用距离度量.Lu等人[142]和Nelson等人[143]提出半监督聚类概率方法,通过贝叶斯先验信息将成对约束与聚类算法合并.Chen等人[144]提出用于文本聚类的半监督非负矩阵分解框架(Semi-SupervisedNonnegativeMatrixFactoriza-tion,SS-NMF),通过迭代算法执行文本间相似矩阵的对称三次因式分解,来推断出文本簇.SS-NMF可以看做是现有的半监督聚类方法的一般框架.Yin等人[145]提出一种基于成对约束的判别式半监督聚类分析法(DiscriminativeSemi-SupervisedClusteringAnalysiswithPairwiseConstraints,DSCA),首先利用正负约束产生投影矩阵,在投影空间中对样例聚类生成簇标号,然后利用LDA选择子空间,接着用基于成对约束的K均值法对子空间中的样例聚类.这个方法有效地利用了监督信息进行聚类,降低了基于约束的SSL聚类算法的计算复杂度.Xia[146]将Tuy的凹割平面方法用于半监督聚类,并给出了半监督聚类局部最优解的一些特性.Lu和Ip谱聚类的约束传播方法,将约束传播问题分解为一组独立的约束传播子问题,用基于KNN的SSL方法在平方时间内解决这些子问题,并在真实数据集上进行实验,结果说明了该方法的优越性.5.1.3非线性方法非线性方法通过核函数间接将样例映射到特征空间中,在原始空间的非线性边界的帮助下完成聚类[148].Kulis等人[149]将Basu等人[62]提出的方法扩展为基于核的半监督聚类,提出半监督核K均值法(Semi-SupervisedKernelK-Means,SS-KK),不是增加违反成对约束的惩罚项,用有奖励和惩罚约束Page13的加权核K均值法来变换聚类距离度量,执行向量形式或图形式的样例的半监督聚类.Yan等人[150]提出一种半监督聚类自适应核学习法(AdaptiveKernelLearningMethodforSemi-SupervisedClus-tering,ASSKKM),将Basu[62]提出的方法中的目标函数核化.Chang和Yeung找局部线性度量的方法.随后,Yeung和Chang指出之前提出的方法的目标函数有许多局部最优解,并且在训练过程中并不能很好的保持拓扑结构,提出两种基于核的度量学习方法.Tsang等人[152]提出核相关组分分析方法,利用合适的核函数将Bar-Hillel[133]的方法泛化到非线性问题.Chen等人[153]提出非线性自适应距离度量学习,首先用核函数将样例映射到高维空间,然后应用线性映射找到低维流形,最后在映射得到的低维空间中完成聚类.Chang等人[154]提出一种度量自适应方法,迭代地调整样例的位置,使相似的点距离越来越近,相异的点距离越来越远.但是这个方法缺乏显式变换映射,因此不能直接处理变换空间中新加入的样例.Xiang等人[155]提出用追踪比优化问题更合适作为的目标函数,并提出一个很好的启发式搜索方法用于解决这个问题.5.2大间隔方法最近,将大间隔方法用于聚类的研究成为SSL研究热点之一,已提出了MMC[155-156]、IterSVR[157]、GMMC[158]、CPMMC[159]、CPM3C[160]和MKC[161]等算法,并且与当前主流聚类算法如K均值聚类和归一化割谱聚类法相比有较大的优势.大间隔方法用于聚类时,求解原问题Γ2为s.t.yi·(〈狓i,狑〉+b)1-ξi式(12)中C>0,l0为待定的参数,式(12d)为类平衡约束,避免出现平凡解.Γ2中一部分样例狓i的类标签yi的值是不知道的(一般是全不知道),目标是不但求解出狑和b,还要求解类标签yi的值,由于式(12e),因此这是一个整数规划问题.以两类为例,由于约束yi∈{±1}y2题为非凸整数规划问题.Xu提出的MMC算法先求出聚类大间隔方法对偶问题式(13)中犓∈犚n×n为输入为[犓]i,j=k(狓i,狓j)的核矩阵,狔=(y1,…,yn)T,犲=(1,…,1)T,犃犅表示矩阵元素相乘运算.再次引入等式约束对偶变量珔犫和不等式约束对偶变量μ,υ,得min狔,δ,μ,υ,珔犫s.t.Xu提出用n×n实值正定矩阵犕0代替狔狔T∈{±1}n×n,此时平衡约束变为-l∑-犾犲犕犲犾犲,为避免正定约束中珔犫和狔出现共线性,放松使得珔犫=0,等于假定分类超平面通过原点.经过以上放松,问题变为以下半定规划问题式(15)的半定规划问题算法复杂性仍然很高,不适合实际应用,Zhang等人[162]提出把铰链损失换为拉普拉斯损失或平方损失,直接求解非凸优化问题的迭代SVR算法,为了解决MMC算法只能求解中等规模问题,Li等人[163]使用多类标签组合核学习方法,提出LG-MMC(Label-GeneratingMMC)算法.为解决聚类精度不稳定问题,Hu等人[164]提出引入成对约束,用约束凹凸过程(ConstrainedConcave-ConvexProcedure,CCCP)迭代求解一系列二次规划问题.针对犕∈犚n×n中要求解的参数数量是样例数量的二次方,无法处理大规模数据,分类超平面通过原点,不适合聚类类不平衡数据等问题,赵兵与张长水提出割平面算法,把原问题的n个约束条件依次加入目标函数求近似解,从而逐次逼近原问题的Page14解[160].Valizadegan[158]提出放松非凸问题为凸问题时并不增加求解变量,求解的问题与样例个数成线性关系,把核矩阵逆替换为归一化图拉普拉斯算子,从而具有无监督核学习能力,能够同时自动确定适当的核矩阵和簇成员关系的算法.Xu[157]和Zhao等人[161]分别给出了大间隔方法聚类的多类版本并扩展到多核情形.另外,Gieseke等人[165]提出在核空间中求解正则化最小二乘损失函数由表示理论知犳[犓]i,j=k(狓i,狓j)的核矩阵,Gieseke等人提出用进化算法求解以下问题min犮∈犚nZhang等人[166-167]提出多样例最大间隔聚类法(MaximumMarginMultipleInstanceClustering,M3IC),使用割平面方法和CCCP组合求解最优化问题.6半监督降维方法在许多实际问题中,如数字图像、金融时间序列、基因表达微序列等,常常会遇到高维数据,直接处理这些高维数据容易出现维数灾难(CurseofDimensionality)问题,这就需要对数据进行降维.降维通常被看做改进分类、回归、聚类等任务的工具,通过对数据进行降维处理,可以改进随后的训练过程的学习性能.半监督降维的目的是在大量的无类标签的样例犝={狓l+1,…,狓l+u}中引入少量的有类标签的样本犔={(狓1,y1),…,(狓l,yl)},利用监督信息找到高维数据犡={狓1,…,狓n}的低维结构表示犣={狕1,…,狕n},狕i∈犚d,d<m,i=1,…,n,n=l+u,与此同时保持数据的内在固有信息(IntrinsicInfor-mation),即保持原始数据及成对约束信息犕犔和犆犔的结构,也就是说,犕犔中的样例最终应该距离很近,犆犔中的样例最终应该互相远离.其中样例狓i∈犚m,样例的低维表示狕i∈犚d,d<m,类标签yi∈{c1,c2,…,cC},i=1,…,l,…,l+u,训练样例数量为n=l+u.当降维方法为线性时,训练过程为学习一个映射矩阵犠={狑1,…,狑d},狑j∈犚m,j=1,…,d,使得犣=犠T犡.当降维方法为非线性时,训练过程不需要学习这个映射矩阵犠,而是直接从原始数据中学习得到数据的低维表示犣.SSL中利用的监督信息既可以是样例的类标签,也可以是成对约束信息,还可以是其他形式的监督信息.主要的半监督降维方法有基于类标签的方法、基于成对约束的方法及其他方法等,下面分别对这几种方法进行描述与分析.6.1基于类标签的方法当前应用广泛的降维方法之一是LDA,寻找一个映射矩阵,使降维后的同类数据之间的距离尽量减小,不同类数据距离尽量增大.Baudat和Anouar[168]用核技巧将LDA扩展到非线性形式,提出广义判别分析法(GeneralizedDiscriminantAnalysis,GDA).Yan等人[169]和Sugiyama[170]将FDA扩展为间隔费希尔判别分析法(MarginFisherDiscriminativeAnalysis,MFA)和局部费希尔判别分析法(LocalFisherDiscriminativeAnalysis,LFDA).Costa和Hero[171]为了提取与分类任务相关的低维特征,将具有流形结构的样例的类标签引入低维数据嵌入结构中调整拉普拉斯法,提出非线性降维方法———分类约束降维法(ClassificationConstrainedDimen-sionalityReduction,CCDR),将每类中所有结点的中心点作为新的结点加入近邻图中,并用权为1的边连接同一类中的结点和它们的中心点.Yu等人[33]在概率PCA模型的基础上加入了类标签信息,提出半监督概率PCA模型(Semi-SupervisedProbabilisticPrincipalComponentAnalysis,S2PPCA)[93],将类标签信息引入映射过程中,能很好地利用所有的信息定义映射,并得到解决这个问题的有效的EM学习算法,而且从理论上分析了这个方法的性能.这个方法能用于处理多输出问题,不仅表现出优越的性能,而且具有较好的可扩展性.Chen等人[172]将LDA重写成最小平方的形式,通过加入拉普拉斯正则化项将该模型转化为最小平方正则化问题.Cai等人[173]在传统的LDA中引入流形正则化项,提出半监督判别分析法(Semi-SupervisedDiscriminativeAnalysis,SDA),在最大化类间离散度的同时可以保持数据的局部结构信息.与LDA一样,也可以将SDA的目标函数转化为一个广义特征分解问题.Zhang和Yeung半监督判别分析法(Semi-SupervisedDiscriminantAnalysis,SSDA),也是使用正则化项来保持数据的流形结构,但与SDA不同的是这个方法使用一种基于路径的鲁棒相似度量来构造近邻图,并用得到的相似性使不同类间的可分离性最大化,并提出用于半监督非线性降维的核化方法.这个方法能够利用Page15数据的全局结构,并且在定义近邻关系中对噪声具有鲁棒性.Song等人[175]提出半监督降维方法框架,在原始LDA中加入一个正则化项,这个正则化项以有类标签的样本和无类标签的样例提供的先验信息为基础,可以用图拉普拉斯构造,并且这个方法可以用核技巧进行核化.这个方法能够发现每类的子流形结构,并将判别子流形嵌入到低维全局坐标系中,能够很好地解决半监督归纳问题,并且PCA、LDA等及其核化方法可以看做这个统一框架下的特殊情况.Zhang和Yeung约束凹凸过程半监督判别分析法(Semi-SupervisedDiscriminantAnalysisAlgorithmConstrainedCon-cave-ConvexProcedure,SSDACCCP),利用无类标签的样例最大化LDA的优化准则,并用约束凹凸过程解决优化问题.这个方法克服了当有类标签的样本数量有限时,LDA受到严重限制的情况.随后提出SSDACCCP的变形M-SSDACCCP,依赖流形假设来利用无类标签的样例,综合了TSVM和基于图的半监督学习方法的特点.Sugiyama等人[177]将局部费希尔判别分析法与PCA相结合,提出半监督局部费希尔判别分析法(Semi-SupervisedLocalFisherDiscriminantAnalysis,SELF),在保留局部费希尔线性判别分析法优点的同时,可以保持无类标签的样例的全局结构,基于特征分解能够得到全局优化的解析解.Chatpatanasiri和Kijsirikul[178]从流形学习的角度提出广义的半监督降维框架,在该框架下,可以很容易地把传统的费希尔判别分析法扩展为半监督的形式,即使在每类的训练数据形成复杂非线性流形上的单独簇的情况下,基于这个框架的方法也能够找到很好的低维子空间.最近出现的基于谱分解的半监督框架可以看做是这个框架的特殊情况.还提出一个新的非线性化框架———核主成分分析(KernelPrincipalComponentAnalysis,KPCA)技巧框架,并将其用于半监督场景.6.2基于成对约束的方法Tang和Zhong维过程中,目标函数加上低维空间中满足负约束的点之间的距离,并减去所有满足正约束的点之间的距离和,然后使这个目标函数最大.这个方法能同时利用正负约束信息,但是没有考虑大量的无类标签的样例,并且构造的所有约束都占同等重要的地位,这可能会产生不好的结果.Hoi等人[180]提出利用成对约束进行度量学习的判别成分分析法(DiscriminativeComponentAnalysis,DCA),像LDA一样寻找一个映射矩阵,使降维后的满足正约束的数据之间的距离尽量减小,满足负约束的数据尽量远离,并提出核化的DCA法.Bar-Hillel等人[133]提出可以有效解决降维问题的约束费希尔线性判别法(ConstrainedFisherLinearDiscrimination,CFLD),利用成对约束信息对数据进行预处理,随后产生改进的分类和聚类,并说明需要多少成对约束信息修改数据表示才能得到好的学习结果,从而解决学习度量问题.CFLD是相关组分分析法(RelevantComponentAnalysis,RCA)的一个中间步骤.RCA搜索并删除数据在全局内不需要的变量,将用于数据表示的特征空间进行改变,用全局线性变换给相关维分配大的权,给不相关维分配小的权,是一种简单有效的学习马氏距离的方法.但是CFLD只能解决正约束问题.Zhang等人[64]提出半监督降维法(Semi-Super-visedDimensionalityReduction,SSDR),这个方法并不像CFLD那样利用约束信息构造散布矩阵,而是使数据满足约束来直接指导降维过程,同时能够像PCA一样保持数据的内部结构信息,在一些特定的拉普拉斯矩阵的本征问题上有近似解,可以从更直观有效的角度同时利用无类标签的样例和正负约束来指导降维过程.但是这个方法只能保持全局协方差结构,而没有保持数据的局部结构.Cevikalp等人[181]在局部保持映射法中引入约束信息,提出约束局部保持映射法(ConstrainedLocalityPreservingProjections,CLPP),首先构造数据的加权近邻图,然后利用约束信息修改近邻关系和加权阵,增大满足正约束的数据之间的权值,减小满足负约束的数据之间的权值,同时修改与满足约束的结点直接相连的结点的权值,对约束信息进行传播,最后通过解最小特征值得到损失函数对应的最优映射矩阵.不同于SSDR,CLPP在降维过程中保持数据的局部结构信息.但是,没有找到确定构造近邻图所需优化参数的可靠方法,同时通过实验发现,参数的一个小扰动就会产生截然不同的训练结果.Wei和Peng督降维法(NeighbourhoodPreservingbasedSemi-SupervisedDimensionalityReduction,NPSSDR).与CLPP不同,NPSSDR不需要构造数据的邻接矩Page16阵,而是通过添加正则化项的方法来实现,组合给定数据集的流形结构和近邻保持,然后最小化重构误差.NPSSDR在利用约束信息指导降维过程的同时,还能保持正负约束与嵌入低维子空间中的数据的局部结构,有近似形式的解,因此可以很容易的用于解决未知测试数据的情况.Peng和Zhang引入成对约束信息,提出半监督典型相关分析法(Semi-SupervisedCanonicalCorrelationAnalysis,Semi-CCA),同时利用无类标签的样例和成对约束信息指导降维过程,并验证了两者的相对重要性.Baghshah和Shouraki[184]将正负约束引入拓扑结构中,提出解决非线性变换的基于核的距离学习方法,将NPSSDR用于度量学习,并用二分搜索法来优化求解过程.大多数降维方法都是由目标函数驱动的,可能只有一部分满足或者甚至不满足性能需求.因此,Davidson[185]提出通过线性映射的图驱动约束降维法(Graph-DrivenConstrainedDimensionReductionviaLinearProjection,GCDR-LP),将编码成图领域的专业知识引入降维过程中,使其成为更普遍的特征值问题.这个方法引入了约束图,可以更加灵活地利用占不同重要性的约束建立模型,并强调了局部几何结构的重要性.6.3其他方法还有许多基于其他形式监督信息的降维方法.6.3.1基于流形嵌入的方法Ham等人[186]给出部分样例的嵌入结果作为监督信息,用核方法解释了流形上的降维方法,如ISOMAP、LLE、图拉普拉斯特征映射(GraphLaplacianEigenmap,GLE)等,这些方法都是利用局部近邻信息构造流形并将其全局映射到低维空间,并说明这些方法都可以被描述为特定结构的格莱姆矩阵上的核PAC法.Yang等人[187]将特定数据的流形坐标形式的先验信息用于降维过程,将LLE、ISOMAP、局部切空间排列法(LocalTangentSpaceAlignment,LTSA)等基本的非线性降维方法扩展到半监督的形式,并说明利用哪些先验信息能更好地提高解的性能.但是,获取数据的流形坐标比获得数据的成对约束要困难得多,因此基于流形嵌入的方法并不常用.6.3.2基于样例相关性的方法Memisevic和Hinton[188]提出的多关系嵌入法(MultipleRelationalEmbedding,MRE)可以综合利用多种相似性关系学习数据的低维嵌入,通过分别给基本隐空间的维数再加权选择不同的相似性表示,并用一些简单的例子证明这个方法的有效性.Yu和Tian[189]提出语义子空间映射法(SemanticSubspaceProjection,SSP),在嵌入到高维空间中的非线性图像子空间上建立图像模型,充分利用语义和图像之间的相似或相异信息,通过增量学习有效地结合相关反馈,并通过理论分析证明LDA可以表示为这个方法的特例.Liu等人[190]通过询问样例相关或不相关来对其进行标记,提出相关聚合映射法(RelevanceAggregationProjections,RAP),用半监督的方式学习有效的子空间映射.这个方法根据反馈给定样例之间的相关性或不相关性,生成一个子空间,在这个子空间中,相关的样例被聚合到一个点,不相关的样例被大的间隔分开[115].7半监督学习理论分析大量研究证实SSL能够利用无类标签的样例提高学习算法预测的准确性和预测算法的速度.因此,许多研究人员对SSL进行了理论分析.Castelli和Cover[76]说明在无类标签的样例数量无限的情况下,可识别的混合模型的误差率以指数收敛到贝叶斯风险.Ratsaby和Venkatesh[191]得到服从两个GMM分布的SSL的PAC结构的学习率.Lafferty和Wasserman[192]从极小极大理论提出半监督回归方法的理论分析,并说明这些方法在合适的假设下可以产生更好的性能.Ben-David等人[55]在生成式场景和诊断场景中对SSL模型和标准的监督PAC模型进行了比较,得到的结论是只有在满足类标签分布的强假设的条件下,SSL才能够提供合适的样本大小来保证其性能优于SL的性能,并提出当前SSL方法中普遍存在的一个问题,目前没有方法来实际验证样本-类标签之间的数据结构关系是否成立.Wang和Zhou在文献[74]中给出了充分必要性定理,为协同训练方法的成功提供了充分必要条件,该定理深刻揭示出,只要学习机之间存在显著差异并且学习机不太差时,就可以通过协同训练提高学习的性能,若希望用协同训练得到理想的结果,则必须将每个无类标签样例在某个联合假设空间中与有类标签样本相连.该结果被Mitchell等人认为是SSL领域的重要进展,并在ICML2010的报告上进行介绍.Urner等人[193]研究如何用无类标签的样例帮助构造更快速的分类器,Page17提出一种SSL算法架构,利用一组预定的快速分类器中的无类标签的样例来学习分类器,并分析在何种条件下无类标签的样例能够有效提高这种方法的性能.Darnstadt和Simon[194]分析了SSL的样本复杂度,说明的确存在巧妙的PAC学习机,利用有限数量的有类标签样本在不用关于基本领域分布的任何先验信息的情况下完成学习目标,从信息论的角度说明了完整的领域分布先验信息并不能有效减少完成学习目标所需要的有类标签样本的数量.Dillon等人[195]基于随机组合似然的拓展形式,定量分析了生成式SSL的渐近准确性,通过提供与不同标记方法相关的度量值的可替代架构,补充分析了分布自由的数据训练性能,解决了标记多少数据及以何种方式标记数据这些基本问题.与此同时,也有许多研究表明由于引入了无类标签的样例而造成学习算法性能下降,由此,许多研究人员对SSL方法进行改进.Balcan和Blum[57]提出同时利用有类标签和无类标签的样例的PAC模型,引入相容函数使满足假设的分类机能够很好的服从无类标签的样例的分布,通过得到的样本复杂度得出结论,在训练误差为零且相容性很高的条件下,只用很少的有类标签的样本就能得到好的假设.Nigam等人[28]在文本分类任务中,用具有固定结构和大量特征的朴素贝叶斯分类器讨论了无类标签的样例降低分类性能的情况,提出一些技术来抑制性能的降低,他们提出分类性能的降低可能是由于特征空间中的自然簇和真实类标签之间的失配.与其他SSL方法类似,研究发现利用无类标签的样例也会降低S3VMs的学习性能.为了解决这个问题,Li和Zhou[86]提出了可靠S3VMs(SafeSemi-SupervisedSupportVectorMachines,S4VM)方法,不根据目标值选择最优的低密度分离机,而是考虑所有低密度分离机,特别针对直推场景,优化最坏情况下的无类标签的样例的类标签分配来构造S4VMs.Li和Zhou[196]提出相比于利用所有无类标签的样例而造成S3VMs性能下降,应当选择更能起到帮助作用的无类标签样例,而避免使用高风险的无类标签样例,因此提出了S3VM-us(Semi-SupervisedSupportVectorMachinewithUnlabeledInstancesSelection)方法,用层次聚类来选择无类标签的样例,并用实验结果说明S3VM-us方法比现有的S3VMs在引起学习性能下降方面具有很大改进.有许多研究人员从理论上分析给出了SSL算法的误差界.Leskes[27]给出了协同训练的广义误差界,并提出当协同训练的不同学习机在给定相同的训练数据集上得到的结果一致,并且训练结果的误差较低时,广义误差界更紧.El-Yaniv和Pechyony的学习场景中,在没有假定来自未知分布的样本独立同分布的情况下,给出一致稳定学习算法的边界.Kaariainen[198]讨论不用类标签条件分布先验假设的SSL方法的广义误差界.Derbeko等人[199]明确地给出了直推学习的误差界,得到了一个在直推场景下构造误差界的方法,并将这种方法用于压缩和聚类场景得到误差界.假定p是依赖全部样例的类上的先验分布,给定δ∈(0,1),训练误差R(犡l)=li=1V(yi,f(狓i)),直推风险R(犡u)=∑∑f(狓i)).以至少1-δ的概率在全部样例上选择训练样本,得到直推场景的广义误差界为R(犡u)R(犡l)+2R(犡l)(l+u)(槡(2log1p当R(犡l)→0时,式(18)中的平方根消失,得到更快的学习率.可以根据式(18)使用训练样例犡l+u来选择先验分布p,不过该方法在实际应用中还没有得到广泛应用.对于来自固定数据集犡l+u的随机选择的大小为l的子样例,式(18)以至少1-δ的概率成立.利用多重先验分布p1,…,pk,扩展到PAC贝叶斯结构,以至少1-δ的概率在来自全部样例的随机选择的大小为l的子样例上选择训练样本,得到实际直推场景的紧致误差界为R(犡u)R(犡l)+2R(犡l)(l+u)(槡(2min1iklog相比用一个先验分布p,根据式(19)可以用k个先验分布p1,…,pk,并从k个相应的PAC贝叶斯边界中选择最优的那个.一个确定性算法对每个二分法最多只能确定一个假设h∈犎.用确定性学习算法基于大小为s的小Page18压缩集产生假设,以至少1-δ的概率对所有h∈犎成立,得到压缩算法的紧致误差界为R(犡u)R(犡l)+2R(犡l)(l+u)2e(l+u)(槡(2slog训练分类器时很容易计算得出式(20)的误差界,并且当压缩集非常小时,就可以得到紧致误差界.用聚类算法将训练样例犡l+u聚类到2,3,…,c个聚类簇中,其中cl,产生将犡l+u划分τ=2,3,…,c个聚类簇中的分割集合,以至少1-δ的概率对所有τ成立,得到聚类算法的误差界为R(犡u)R(犡l)+2R(犡l)(l+u)(槡(2τ+ln当聚类算法用少量的聚类簇得到训练样例的数据结构时,用式(21)可以得到紧致误差界.同时,有许多研究人员从理论上分析给出了SSL的样本复杂度.Gentile和Helmbold[200]研究在噪声场景中,固定分布的d个间隔的并集的样本复杂度下界为Ω2dlog(1/Δ)(Δ(1-2η)到目标的距离,η是错误类标签出现的概率.Ben-David等人[55]在生成式场景和诊断场景中,比较SSL模型和监督PAC模型的样本复杂度.置信度δ>0,准确度ε>0,在生成式场景中,SL的样本复杂度上界是ln(1/δ)ln(1/δ)2ε阶项,对绝对连续的无类标签数据分布,可以得到在生成式场景中SL的样本复杂度至多是SSL的2倍.在诊断场景中,Ben-David等人明确地构造噪声场景,并利用信息论方法,得到SSL在(0,1)均匀分布上学习阈值的样本复杂度为Θd个间隔的并集的样本复杂度为Θ该结果说明在诊断场景中,SSL在样本复杂度方面的改进不超过SL的常数倍.8未来研究方向经过大量研究人员的长期努力,SSL领域的研究已取得了一定发展,提出了不少SSL方法,同时已将SSL应用于许多实际领域.但目前这个领域的研究仍存在许多有待进一步解决的问题,我们认为未来的研究方向包括以下一些内容.8.1理论分析目前对SSL的理论分析还不够深入.在类标签错误或成对约束不正确时学习方法的性能如何改变,选择不同的正约束和负约束的比例会对降维的性能造成什么影响,除了通常采用的分类精度和运算速度之外,还有没有其他更合适的评价指标,对学习性能起到改进作用的是准确的最优化求解算法,还是使用的学习模型中的数据表示和学习方法,最优解对学习结果的影响有多大,未来还需要进一步探讨这些问题.8.2抗干扰性与可靠性当前大部分SSL利用的数据是无噪声干扰的数据,而且依赖的基本假设没有充分考虑噪声干扰下无类标签数据分布的不确定性以及复杂性,但是在实际应用中通常难以得到无噪声数据.未来需要研究如何根据实际问题选择合适的SSL方法,更好地利用无类标签的样例帮助提高学习的准确性和快速性,并减小大量无类标签数据引起的计算复杂性,可以考虑引入鲁棒统计理论解决该抗噪声干扰问题.此外,大量实验研究证明当模型假设正确时,无类标签的样例能够帮助改进学习性能;而在错误的模型假设上,SSL不仅不会对学习性能起到改进作用,甚至会产生错误,恶化学习性能.如何验证做出的模型假设是否正确,选择哪种SSL方法能够更合适地帮助提高学习性能,除了已有的假设之外,还可以在无类标签的样例上进行哪些假设,新的假设是否会产生新的算法,SSL能否有效用于大型的无类标签的数据,这些问题还有待未来研究.此外,导致SSL性能下降的原因除了模型假设不符合实际情况外,还有学习过程中标记无类标签的样例累积的噪声,是否还有其他原因使无类标签的样例造成学习性能的下降,也是未来需要进一步研究的问题.8.3训练样例与参数的选取通常训练数据是随机选取的,即有类标签的样例和无类标签的样例独立同分布,但是在实际应用Page19中,无类标签的样例可能来自与有类标签的样例分布不同或未知的场景,并且有可能带有噪声.未来的研究需要找到一个好的方法将SSL和主动学习相结合,选取有利于学习模型的训练样例,并确定SSL能够有效进行所需要的有类标签的样本数量的下界.此外,许多研究人员将SL和UL算法扩展用于SSL,但是许多这些算法是根据先验信息得到训练数据集的参数,并利用这些参数改进算法在SSL中的性能.目前都是人工选取一种SSL方法,并设定学习参数,保证SSL的性能优于SL和UL,但是当选取的SSL方法与学习任务不匹配或者参数的设定不合适时,会造成SSL的性能比SL或UL更差.如何自动根据学习任务选取合适的SSL方法并准确得到参数是未来SSL需要深入研究的内容,可以考虑用全贝叶斯学习理论解决.8.4优化求解从各种SSL算法的实现过程可以看出,SSL问题大多为非凸、非平滑问题,或整数规划和组合优化问题,存在多个局部最优解,例如求解SSL产生式方法目标函数的EM算法只能得到局部极大值.目前主要采用各种放松方法把目标函数近似转化为凸或连续最优化问题,不易得到全局最优解,算法的时空复杂性很高,问题的求解依赖于最优化理论的突破,未来需要研究新的算法求解全局最优解.8.5研究拓展SSL从产生以来,主要用于实验室中处理人工合成数据,未来的研究一方面需要讨论SSL可以显著提高哪些学习任务的性能,拓展SSL在现实领域的实际应用,另一方面需要制定出一个统一的令人信服的SSL方法的使用规程.此外,目前有许多的半监督分类方法,而对半监督回归问题的研究比较有限.未来有待继续研究半监督分类和半监督回归之间的关系,并提出其他半监督回归方法.9结束语SSL作为ML中的一个重要问题,能够同时利用大量的无类标签的样例和有限的有类标签的样本一起训练,近几十年来得到了越来越多的关注,许多SSL理论和方法得以发展.本文详细概述了SSL的相关基本概念和研究发展历程,分别从分类、回归、聚类及降维这四个方面全面总结了SSL的理论和方法,综述了SSL的理论分析、误差界和样本复杂度,并在最后从理论分析、抗干扰性与可靠性、训练样例与参数的选取、优化求解和研究拓展五个方面指出了SSL未来的研究方向.随着SSL理论与方法研究的深入,SSL将被更加广泛地应用在各个领域.
