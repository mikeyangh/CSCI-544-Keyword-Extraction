Page1基于线程的MPI通信加速器技术研究刘志强宋君强卢风顺赵娟(国防科学技术大学计算机学院长沙410073)摘要为了针对多核系统构建更高效的MPI支撑环境,文中提出了一种基于线程的MPI加速器,称作MPIActor.MPIActor是一种用于协助传统MPI库的透明中间件,用户可以在编译期选择是否在单线程MPI程序中采用该中间件.加入MPIActor后,每个节点内的MPI进程都被映射成同一进程中的多个线程,从而节点内的通信可通过轻量级的线程通信机制实现.作者给出了MPIActor的基本设计,详细阐述了其工作机制、通信体系结构及关键技术,并在真实系统上分别针对MVAPICH2和OpenMPI并行环境利用OSULATENCY基准测试进行了性能评测.实验结果表明在两种MPI环境上进行节点内8KB~4MB数据通信时MPIActor都能使通信性能平均提高一倍左右.关键词MPI软件结构;线程MPI;MPI加速器;MPIActor1引言多核技术,即在同一芯片内包含更多的核心,被认为是一种能够继续保持处理器性能发展遵循摩尔定律的有效手段.在过去的几年中,多核处理器的发展验证了该手段的有效性,其间处理器内的核心数目也基本保持每18个月左右增长一倍①②.可以预Page2见,在相当的一段时间内多核处理器的核心数目还会持续增长.在多核处理器高速发展的背景下,近年来,采用多核处理器的集群系统(简称多核集群)主导了高性能计算(HPC)领域的主流硬件平台.消息传递接口(MPI)是该领域并行程序开发的事实标准.基于MPI的并行程序通过MPI运行时库提供的消息传递接口函数进行通信;随着节点内核心数目的增多,节点内通信的性能将会对MPI程序的整体性能产生越来越重要的影响.例如当64个MPI进程平均分布在16个节点上进行HPL基准测试时,约有57%的通信发生在节点内[1].传统MPI基础软件(如MPICH2①、OpenMPI②和MVAPICH③)的节点内通信性能受限于低效的操作系统进程间通信.这一问题归因于传统MPI基础软件将每个MPI逻辑进程映射为一个操作系统进程;相应地MPI逻辑进程间通信只能通过高代价的操作系统进程间通信完成,由此带来的性能缺陷难以通过单纯地改进通信算法来克服.在20世纪90年代末大规模SMP系统发展的顶峰时期,一些工作[2-4]曾针对大规模SMP系统对线程MPI(ThreadedMPI)进行过研究.线程MPI即:在共享内存系统中,将MPI逻辑进程映射为同一操作系统进程中的多个线程.虽然这些早期的工作随着大规模SMP系统淡出市场没有得到延续,但这些工作的结果表明:线程MPI可以通过改进MPI程序的运行机制来提供高效的节点内通信.理论上线程MPI同样是一种可以用于改进多核系统上节点内通信性能的有效途径.但MPI多年来的发展和用户对MPI应用需求的提高使得线程MPI面临两方面的问题.首先,线程MPI难以支持MPI逻辑进程内的多线程.支持MPI逻辑进程内的多线程是MPI2.0标准中的一项重要内容④,它的缺失将会导致多线程应用程序(如基于MPI+OpenMP的应用程序)无法在线程MPI环境中正常运行.其次,传统的MPI基础软件历经数年的发展,规模已非常庞大,由于底层机制的改变,即使在已有代码的基础上开发一套完整的线程MPI基础软件也需要庞大的工作量.据我们所知,目前仍无成熟的线程MPI支撑环境供用户使用.为了实现高效的线程MPI支撑环境,本文提出了一种基于线程的MPI加速器技术,称作MPIActor.MPIActor是一种可协助传统MPI软件的透明中间件,它位于用户MPI程序和MPI运行时库之间并对用户MPI程序提供线程MPI支撑环境.传统的MPI支撑环境由单一MPI运行时库实现所有通信;引入MPIActor后,MPI支撑环境则由独立并低耦合的两部分运行时库分别完成节点内通信和节点间通信.MPIActor的贡献在于:(1)以低耦合的软件结构化解了线程MPI和MPI逻辑进程内多线程之间的矛盾.程序员可以根据需要在编译期选择是否加入MPIActor,编译时加入MPIActor得到的MPI可执行程序是线程MPI的,编译时不加入MPIActor得到的MPI可执行程序能够支持MPI逻辑进程内多线程.(2)实现了线程MPI支撑环境并充分利用了传统的MPI基础软件,降低了开发线程MPI的难度和工作量.在基于MPI+MPIActor的并行程序中,节点内通信由MPIActor完成,节点间通信由MPIActor转发到下层的MPI库完成.因此,MPIActor可以专注于实现高效的节点内通信,而对节点间通信只需实现简单的转发.(3)不依赖于特定的传统MPI基础软件提高节点内通信性能,即MPIActor即可用于MVAPICH2也可用于OpenMPI或其它符合MPI2.0标准的基础软件.我们目前已实现了MPIActor的基本框架和节点内点对点通信模块.本文在一个双路四核Nehalem系统[5]上利用OSULATENCY基准测试③分别在MVAPICH2和OpenMPI环境中对MPIActor进行了性能评测,实验结果表明MPIActor能够在不影响节点间通信的情况下有效地提高MPI运行环境的节点内通信性能.在两种MPI环境上传输8KB~4MB之间的数据时,MPIActor能够使节点内通信性能平均提高一倍左右.这些数据表明MPI加速器技术是一种对传统MPI基础软件有效的节点内通信性能优化技术.本文第2节介绍相关工作;第3节阐述MPI-Actor的工作机制;第4节介绍MPIActor的通信体系结构;第5节和第6节介绍实现MPIActor两个方面的关键技术:通信操作聚合和基于单次内存拷贝的节点内点对点通信;第7节在真实平台上进行①②③④Page3性能评测;最后一节对本文的工作进行总结并提出对未来工作的展望和设想.2相关工作MPI标准的制定计划诞生于1992年,并于1994年发布第一个版本.MPI的初衷是为分布内存系统上的消息传递并行编程模式提出统一的标准①,因此,在基于传统MPI基础软件的程序中,每个逻辑上的MPI进程在运行时都将被映射成一个操作系统进程.MPI具有直观的编程模式和良好的可扩展性,因此它迅速成为高性能计算领域并行程序设计的事实标准,并广泛地应用于共享内存系统,如大规模SMP系统.但在共享内存系统中,进程级并行相对于线程级并行的交互开销往往要大数倍.传统上基于进程的MPI难以发挥出共享内存系统的最佳性能,于是,早期也有多个工作关注于线程MPI[2-4].TOMPI[3]和MPI-Lite[4]研究在单台共享内存系统中以线程为单位运行MPI程序.TMPI[2]是公开发布的第一个可以用于共享、分布内存混合系统上的线程MPI.但随着2000年左右大规模SMP系统淡出市场,这几项线程MPI的工作都没有继续下去.近年来,采用多核处理器的集群系统(简称多核集群)主导了高性能计算领域的主流硬件平台②.随着多核处理器的发展,主流集群系统各节点内的计算核心数目不断增长,节点内通信性能的重要性不断增加,线程MPI又再次被关注.AzequiaMPI[6]是一项目前正在进行中的工作,它是西班牙“Ingenio2010”政府计划③中的一个子项目,其目标是建立一套完全支持MPI1.3标准的线程MPI基础软件.不同于TMPI和AzequiaMPI,MPIActor以透明中间件的方式基于传统MPI基础软件提供线程MPI环境,从而解耦了线程MPI与MPI进程内多线程之间的矛盾.引入MPIActor后,传统MPI基础软件的结构得到改进,节点内通信和节点间通信分别由低耦合的独立模块完成,即:传统MPI库完成节点间通信;MPIActor专注于实现节点内通信.3工作机制如图1所示,MPIActor通过编译期和运行期的相关支持使传统的用户MPI程序在线程MPI环境中执行,期间不需要用户专门为MPIActor对源代码做任何手工改动.我们在下文将基于MPI+MPIActor的应用程序简称为MPIActor程序,相应的并行作业称为MPIActor并行作业.本节的以下内容首先阐述MPIActor程序的运行期机制,其次介绍将MPIActor加入传统MPI程序的编译期机制.图1用户MPI程序在编译期被转换为基于MPI+MPIActor的可执行程序,运行期在线程MPI环境中执行3.1运行期机制传统MPI程序的运行机制如图2(a)所示,在此运行机制下,每个MPI逻辑进程在运行时都被映射为一个操作系统进程,不论节点内通信还是节点间通信都通过MPI运行时库完成并最终通过某种进程间通信机制实现.图2传统MPI程序与MPIActor程序的运行机制比较①②③Page4MPIActor程序的运行机制如图2(b)所示,在MPIActor的支持下,同节点的MPI逻辑进程被映射为同一进程(称容器进程)中的多个线程.这些线程由容器进程的主线程根据MArun传入的参数派生,其中MArun是用来提交MPIActor并行作业的脚本程序,它将用户命令参数分解,一部分传递给MPI作业提交器,另一部分传递给容器进程.MPIActor运行时库为MPI逻辑进程构建了以线程为单位执行的基础结构,它位于用户MPI程序和MPI运行时库之间,处理运行时用户MPI程序中的MPI调用,其中节点内通信由MPIActor运行时库单独完成,节点间通信由MPIActor转发到底层的MPI库完成.我们将这种由MPIActor协同标准MPI库处理MPI通信调用的方法称为通信操作聚合技术,它是MPIActor成为透明中间件的关键技术之一,将在本文的第5节中详细阐述.3.2编译期机制为了不给用户增加额外的工作量,MPIActor在改变MPI程序运行方式的同时保持了传统的MPI编程模式.但在传统的MPI编程模式中存在一个潜在假设:每个MPI逻辑进程都有独立的内存地址空间.传统基于进程的MPI恰好自然地满足这一假设,因此用户在编写MPI程序时往往习惯于使用全局变量实现MPI逻辑进程全局范围内的数据共享或者使用静态变量实现对数据的静态访问.不幸的是基于线程的MPI无法满足这一假设,若不改变用户的编程习惯,就需要对线程MPI提供编译期支持.在MPIActor的支撑软件中,MAcc(MPIActorCCompiler)为基于MPIActor的C语言程序提供编译期支持.它的工作原理如图3所示,由用户MPI程序源代码到MPIActor可执行程序需要经过3个阶段:预编译、编译和链接.在预编译阶段,MAcc将用户MPI程序代码转换为线程安全的程序代码,确保用户MPI程序在映射为同一进程中多个线程执行时彼此的数据空间无重叠.MAcc主要完成3项工作:(1)将程序的入口函数main改名为mpi_main;(2)将全局变量和静态变量改为线程局部存储(threadlocalstorage)[7]的变量;(3)将程序中调用的非线程安全的函数(如getopt)改为调用MPIActor运行时库中相应线程安全的函数.在编译和链接阶段,MAcc利用mpicc完成对预编译转换后代码的编译和链接工作.在编译阶段,MAcc指定mpi.h的路径到MPIActor提供的mpi.h.在链接阶段,MAcc添加MPIActor的运行时库.目前我们正在设计用于编译Fortran语言和C++语言程序的编译器,其工作步骤与MAcc类似,但需要注意的是Fortran语言目前还不支持线程局部存储,支持Fortran语言程序的预编译器要比用于C和C++语言的预编译器复杂得多.通过编译期的支持,用户可不对原程序代码作任何修改将基于传统MPI软件的程序代码编译成基于MPI+MPIActor的可执行程序.4通信体系结构MPIActor的通信体系结构是实现通信操作聚合和节点内通信的基础.如图4所示,在此通信体系结构中,N个MPI逻辑进程间通过N2个虚拟通道实现点对点通信,即任意一对MPI逻辑进程PA和PB对应两个独立的虚拟通道CAB和CBA.若PA和PB属于同一节点,则CAB和CBA为节点内通信虚拟通道(ON-hostVirtualChannel,ONVC),它是MPIActor实现节点内通信的物理基础结构,节点内通信操作在此基础上实施.图4中的每个灰色圆形都表示一个ONVC,ONVC是单向通信通道,它被接收端MPI逻辑进程创建并由发送端MPI逻辑进程持有引用指针,即CAB用于PA发送消息到Page5PB,CBA用于相反方向通信.另外,ONVC内包含3个用于实现节点内点对点通信的先入先出(FIFO)队列:接收请求队列、发送请求队列和anysource接收请求队列.节点内通信的细节部分将在本文第6节中详细讨论.若PA和PB属于不同节点,则CAB和CBA为节点间通信虚拟通道(OFf-hostVirtualChannel,OFVC).与ONVC不同,OFVC并不作为节点间通信的物理基础结构,仅缓存用于转发节点间通信所需的一些必要信息,包括远端节点rank(RemoteNodeRank,RNR)、发送标记基数(SendTagBase,STB)和接收标记基数(ReceiveTagBase,RTB).图4中的白色圆形表示OFVC,OFVC是双向通信通道,它由各个进程创建并独立持有,即CAB由PA持有并缓存与PB通信的相关信息,CBA由PB持有并存放与PA通信的相关信息.5通信操作聚合技术通信操作聚合是MPIActor整合自身运行时库与底层标准MPI库实现MPI通信语义的关键技术.其基本过程如图5所示,我们将每一次MPI通信接口调用作为一次通信请求,MPIActor程序发出的通信请求首先经由“通信请求分离”过程判断通信属于何种类型,再根据通信的类型将请求递交给相应的聚合过程处理.本节将详细讨论这一关键技术.5.1通信请求分离在多核集群系统上,MPI消息传递通信究竟发生在节点间还是在节点内是一个运行时特征,我们称此特征为通信位置特征,MPI标准中定义的通信接口语义并不区分类似的运行时特征.根据MPI-Actor的运行机制,MPI通信请求须根据其通信位置特征由不同的运行时库处理,因此MPIActor在重载MPI接口时需要在过程中辨别通信的类型,我们称这一过程为通信请求分离,简称CRS(Commu-nicationRequestSeparating),它是通信操作聚合过程中的第一个关键步骤.从CRS的输入角度看可将MPI通信请求分为两类,具体分类如表1所示.阻塞通信接口非阻塞通信接口第1类通信请求须根据接口参数进行CRS.MPIActor通过虚拟通道通信体系结构建立了MPI逻辑进程间的通信关系,在此基础上第1类通信请求通过输入参数获取虚拟通道,并根据通道的类型判断通信请求的通信位置特征.另一类通信请求根据通信请求对象中的通信位置特征进行通信请求分离,其中这些通信请求对象由第1类通信请求的非阻塞通信过程返回.经由CRS过程,MPI通信请求将会被传递给MPIActor的通信过程、标准MPI库的通信过程或MPI_ANY_SOURCE处理过程作进一步处理.5.2节点间通信请求的转发方法设r为输入MPIActor的通信请求,若r对应的通信发生在节点间,则r会被MPIActor转发到底层的标准MPI库处理,在此过程中MPI库需要知道与对方节点容器进程中的哪个线程进行消息交互,这是节点间通信请求转发需要解决的问题.本小节将以转发节点间MPI_Irecv请求为例详细讨论节点间通信请求转发的方法及规则.我们有以下定义.定义1(通信请求对象).通信请求对象是一个三元组CRO=〈S,D,T〉,其中:S={i|i∈N,0i<Pmax}∪{MPI_ANY_SOURCE}表示通信源MPI进程,其中Pmax为MPI辑进程个数.D={i|i∈N,0i<Pmax}表示通信目的MPIT={t|t∈N}∪{MPI_ANY_TAG}表示通信进程.标记.Page6定义2(接收请求).接收请求是一个二元组RR=〈S,T〉,表示从源MPI进程接收某标记的数据,其中S和T的定义同定义1.定义3(MPI进程的节点位置).称MPI进程所在的节点为节点位置:其中Proc,NodeN,npos(p)能够得到MPI进程p的节点位置.定义4(非阻塞接收).非阻塞接收以非阻塞方式处理接收请求并返回一个通信请求对象:MPI_Irecv(p,〈s,t〉)=〈p,s,t〉,其中p为当前MPI进程的进程序列号(rank).对非阻塞接收MPI_Irecv(p,〈s,t〉),在s不等于MPI_ANY_SOURCE的情况下,如果npos(p)不等于npos(s),则〈s,t〉需要被转换为〈s,t〉并通过底层MPI库中的MPI_Irecv(p,〈s,t〉)操作完成通信,其中:当t不等于MPI_ANY_TAG时:t=L(s)offsets+L(p)offsetr+t,Cps.rtb=soffsets+poffsetr+t,其中offsets和offsetr分别表示发送位置偏移和接收位置偏移,L(x)表示MPI进程x的本地进程号.可知MPIActor程序进行节点通信时源进程和目的进程通过底层MPI接口中的通信标记参数识别.另外,若Cps为p到s的通信虚拟通道,则有其中Cpsrtb为Cps中的接收标记基数.在实现节点间通信请求转发时可利用此基数转换通信标记.当t=MPI_ANY_TAG时,t=t,此类型的节点间通信将通过MPIActor内置的特殊过程处理.s=MPI_ANY_SOURCE情况下的通信请求处理方法将在5.3节讨论.5.3对MPI_ANY_SOURCE类型请求的处理方法MPI_ANY_SOURCE可作为MPI接收(Recv)或检查(Probe)接口中“数据源进程号”的参数值,表示任意数据源.当MPI程序调用接收或检查操作时的“数据源进程号”参数值为MPI_ANY_SOURCE时,算法无法通过输入参数的静态值确定其发出的通信请求对应节点间通信还是节点内通信,我们称此类通信请求为MPI_ANY_SOURCE类型的请求,或简称ASR(AnySourceRequests)类型的请求.ASR通过专门的处理过程协同MPIActor通信库和MPI通信库中的通信操作完成,涉及此类通信的MPI接口有MPI_Recv、MPI_Irecv、MPI_Iprobe、MPI_Probe和所有第2类通信请求接口.本小节的以下内容将以MPI_Iprobe为例讨论针对ASR的处理方法.MPI_Iprobe操作的定义为MPI_Iprobe(source,tag,comm,flag,status),其语义是在不实际接收输入消息的情况下检查是否收到comm通信域中源为source标记为tag的消息.当调用MPI_Iprobe“数据源进程号”参数的值为MPI_ANY_SOURCE时,通信请求由CRS传给ASR_MPI_Iprobe处理,其定义和算法如图6所示.ASR_MPI_Iprobe算法通过MPI_Iprobe检查并获取发送给所属容器进程的节点间通信消息,若收到则根据获取到的信息检查消息是否为发送给本MPI逻辑进程的消息;另一方面,ASR_MPI_Iprobe通过检查虚拟通道的发送队列检查是否有节点内通信.算法.ASR_MPI_Iprobe.输入:tag,comm;输出:flag,statusBEGIN/检查是否收到节点间消息/MPI_Iprobe(MPI_ANY_SOURCE,MPI_ANY_TAG,/检查所收到节点间消息的目的地是否为本进程/IF(tflag==1)andismymsg(ts,tag)flag=1;将ts转换到status;return;ELSE检查虚拟通道发送队列中是否已收到匹配的节点内消息ENDEND并返回结果;除ASRMPI_Irecv外,其余ASR通信请求对应的处理方法在基本原理上都与ASR_MPI_Iprobe算法相似.ASRMPI_Irecv较为特殊的原因是它的非阻塞数据接收机制,如何协同MPIActor通信库和MPI通信库有效实现这一机制是一个难点.ASRMPI_Irecv的基本原理是若通过MPI_Iprobe未发现匹配的请求则同时启动节点内MPI_Irecv通信和节点间MPI_Irecv通信,然后在接收到数据后再对其中一个选择性取消.为此我们在MPIActor的通信体系结构中为每一个MPI逻辑进程增加了一个any-source请求队列,并将其连入虚拟通道.ASRMPI_Irecv的实现方法我们将在后续工作中详细介绍.6基于单次内存拷贝的节点内点对点通信基于单次内存拷贝的节点内点对点通信是Page7MPIActor提高通信性能最重要的技术手段之一.在MPIActor程序的运行机制下,若点对点通信发生在节点内则源MPI进程中的发送缓冲区和目的MPI进程中的接收缓冲区都属于同一进程地址空间,进而节点内点对点通信可自然地通过单次内存拷贝实现.本节的以下内容主要讨论MPIActor中基于单次内存拷贝的点对点通信算法.另外,为了更清晰地说明MPIActor节点内通信的性能优势,我们将在最后一个小节详细介绍基于核心态内存映射的单次内存拷贝机制并与之进行比较.6.1节点内单次拷贝点对点通信算法MPIActor中的节点内点对点通信算法如图7所示,包括相互配合的发送算法和接收算法.接收和发送算法的基本结构类似,均包含两个步骤:第1个算法.Intra_MPI_Send.输入:buf,bsize,dest,tag,comm输出:bufBEGIN/步骤1/通过本进程号(rank)和dest从comm对应的通信域对象中得到通信虚拟通道CVC;mutex_lock(CVC.mutex);/加锁/狉犲狇=犆犞犆.狉犲犮狏狉犲狇_狇狌犲狌犲.犺犲犪犱;IFreq!=NULLWHILE(req!=NULLreq=req->next;END/步骤2/IFreq!=NULL犱犲狇狌犲狌犲(犆犞犆.狉犲犮狏狉犲狇_狇狌犲狌犲,狉犲狇);/将req出列/犿犲犿犮狆狔(狉犲狇->狉犲犮狏犫狌犳,犫狌犳,狉犲狇->犫狊犻狕犲);ELSE申请一个新的通信请求对象req,并初始化;狉犲狇->狊犲狀犱犫狌犳=犫狌犳;犲狀狇狌犲狌犲(犆犞犆.狊犲狀犱狉犲狇_狇狌犲狌犲,狉犲狇);/将req入列/ENDmutex_unlock(CVC.mutex);/解锁/END图7节点内点对点通信算法6.2进程间单次内存拷贝的点对点通信传统的MPI软件通常基于用户态共享内存实现节点内点对点通信,通信的基本原理是将源MPI进程中的消息拷贝到共享内存的缓冲区,再从共享内存缓冲区拷贝到目的MPI进程中的接收缓冲区,整个通信过程需要通过两次内存拷贝.近年来的一些工作(如MVAPICH2中的LiMiC[8]和OpenMPI中的KNEM[9])为传统的进程MPI提出了一种新的节点内点对点通信机制:基于核心态内存映射的通信机制(Kernel-BasedMemoryMap-pingCommunicationMechanism,KBMMCM),基步骤进行通信请求匹配,第2个步骤根据第1个步骤的运行时结果选择创建或处理通信请求.发送算法的第1个步骤遍历虚拟通道的接收请求队列并获取第1个匹配的通信请求对象req,此过程的时间复杂度为O(n).算法第2个步骤的两个分支分别对应成功获取匹配的通信请求对象和未获取通信请求对象的情况,如果成功获取匹配的通信请求对象req则将req从接收请求对列中移出并根据请求中的接收缓存地址和大小完成数据拷贝;如果未获取匹配的通信请求对象则申请一个新的通信请求对象并填充本次发送请求的信息,之后加入发送请求队列.两个步骤都在虚拟通道访问锁的保护下进行.接收算法与发送算法的结构相同,具体过程对偶.图7用黑体标记了两种算法的区别.算法.Intra_MPI_Recv.输入:count,src,tag,comm输出:bufBEGIN/步骤1/通过本进程号(rank)和src从comm对应的通信域对象中得到通信虚拟通道CVC;mutex_lock(CVC.mutex);/加锁/狉犲狇=犆犞犆.狊犲狀犱狉犲狇_狇狌犲狌犲.犺犲犪犱;IFreq!=NULLWHILE(req!=NULLreq=req->next;END/步骤2/IFreq!=NULL犱犲狇狌犲狌犲(犆犞犆.狊犲狀犱狉犲狇_狇狌犲狌犲,狉犲狇);/将req出列/犿犲犿犮狆狔(犫狌犳,狉犲狇->狊犲狀犱犫狌犳,犫狊犻狕犲);ELSE申请一个新的通信请求对象req,并初始化;狉犲狇->狉犲犮狏犫狌犳=犫狌犳;犲狀狇狌犲狌犲(犆犞犆.狉犲犮狏狉犲狇_狇狌犲狌犲,狉犲狇);/将req入列/ENDmutex_unlock(CVC.mutex);/解锁/END于该通信机制的通信过程仅需单次内存拷贝.KBMMCM的基本原理如图8(a)所示.KBMMCM需要在系统中建立一个内核模块,此模块可通过相应的KBMM驱动进行使用.基于KBMMCM的点对点通信过程主要通过6步完成:1.发送端将发送缓冲区的起始地址buff和缓冲区大小size发送给KBMM驱动.2.KBMM接收到buff和size后将发送进程的进程指针(current)、内存指针(current->mm)、buff占用的页大小等信息存在一个数据结构kbmm_s中返回给发送进程.Page8送给接收端.4.接收端将kbmm_s发送给KBMM驱动,KBMM驱动根据kbmm_s得到发送缓冲区的页列表pages_list.5.KBMM驱动通过kmap将pages_list映射到一个核3.发送进程将kbmm_s打包并通过MPI点对点通信发心态内存地址kbuff.6.将kbuff拷贝到接收缓冲区.此过程只需一次内存拷贝,但同时会产生较大的处理开销,在某些情况下处理开销甚至会超过减少一次内存拷贝带来的收益,造成性能下降,因此基于核心态内存映射是一种重量级的单次内存拷贝机制.与之相比MPIActor中的单次内存拷贝机制不需要进行繁琐的内存转换过程,是一种轻量级的通信机制.7实验与结果7.1实验环境与方法我们的实验环境是一个128个节点的双路四核Nehalem集群(本文实验只需要其中的两个节点),每个节点都包含两颗XeonE5540处理器和32GB内存,测试时处理器主频为2.53GHz,外频为1066MHz.操作系统采用RedhatLinuxEnterprise4.我们实验的性能测试工具采用OSUBench-mark中的OSU_LATENCY,其内部采用ping-pong测试方法,是一种用于测试MPI点对点通信性能的通用工具.通过OSU_LATENCY,我们的实验分为两个部分:第1部分的实验用于评测在MVAPICH2基础上MPIActor节点内通信的性能,对比以下MPI通信配置的性能:(1)基于用户级共享内存的MVAPICH21.4;(2)基于LiMiC的MVAPICH21.4;(3)MVAPICH21.4+MPIActor.另外,在这一部分实验中我们还将对比节点间通信的性能,目的是为了测试经MPIActor转发通信后性能是否会受到显著影响.第2部分的实验用于评测在OPENMPI基础上MPIActor节点内通信的性能,对比以下MPI通信配置的性能:(1)基于用户级共享内存的OPENMPI1.5a1;(2)基于KNEM的OPENMPI1.5a1;(3)OPENMPI1.5a1+MPIActor.对以上每一种配置我们都分别测试了处理器内通信和处理器间通信.7.2实验结果与分析将两部分实验结果综合,可以得出如图9~图11的实验结果.可以发现,MPIActor有效的提高了MVAPICH2和OpenMPI的节点内点对点通信的性能,同时对节点间通信性能几乎不造成影响.处理器内的通信性能评测结果如图9所示,相对于基于用户级共享内存的MVAPICH2,MPIAc-tor在传输大小为8KB~256KB的消息时通信性能提高了37%~114%;在传输256KB~2MB的消息时性能提高了68%~111%;在传输4MB~8MB的消息时性能提高了60%~12%.相对于OpenMPI,MPIActor在传输大小为8KB~256KB的消息时通信性能提高了48%~91%;在传输256KB~2MB字节的消息时性能提高了47%~106%;在传输4MB~8MB的消息时性能提高了56%~10%.另外,LiMiC在传输大小为16KB~256KB数据时性能提高了5%~35%,但在传输其余大小区间的消息时通信性能较使用用户态共享内存的MVAPI-CH2有明显下降;KNEM对处理器内点对点通信的性能没有任何提高,我们进行多次实验都得到了相似的结果.处理器间的通信性能评测结果如图10所示,相对于基于用户级共享内存的MVAPICH2,MPIAc-tor在传输大小为8KB~256KB的消息时通信性能提高了30%~144%;在传输256KB~2MB的消息时性能提高了74%~117%;在传输2MB~16MB的消息时性能提高了44%~64%.相对于OpenMPI,MPIActor在传输大小为8KB~256KB的消息时通信性能提高了46%~98%;在传输256KB~2MB的消息时性能提高了70%~72%;传输2MB~16MB的消息时性能提高了38%~70%.另外,LiMiC在传输32KB~4MB的数据时有Page937%~98%的性能提升,在传输更大的消息时性能有5%左右的提升.图9处理器内点对点通信延迟图10处理器间点对点通信延迟节点间通信的性能比较如图11所示,可以发现节点间通信经由MPIActor转发性能几乎不受影响.Page10图11节点间点对点通信延迟8总结和未来的工作MPI加速器是一种对传统MPI基础软件有效的通信性能优化技术.它以透明中间件的形式提供了一个线程MPI运行环境.采用这种形式,MPI-Actor一方面解耦了线程MPI与MPI进程内多线程之间的矛盾,另一方面简化了线程MPI软件的开发工作量.实验表明:(1)MPIActor能够有效地全面提高传统MPI软件的节点内通信性能,同时不影响节点间通信性能.(2)线程MPI的点对点通信比LiMiC和KNEM等基于核心级内存映射机制的点对点通信优化方法更高效.下一步我们将在MPIActor的基础上通过基于共享内存的算法进一步优化集合通信性能.
