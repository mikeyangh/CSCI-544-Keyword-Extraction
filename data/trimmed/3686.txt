Page1计算机系统与计算机网络中的动态优化:模型、求解与应用林闯1)万剑雄2)向旭东2)孟坤2)王元卓3)1)(清华大学计算机科学与技术系北京100084)2)(北京科技大学计算机与通信工程学院北京100083)3)(中国科学院计算技术研究所北京100190)摘要动态优化是计算机系统与计算机网络中进行资源分配与任务调度等方面研究所采用的主要理论工具之一.目前,国内外已开展大量研究,致力于深化动态优化的理论研究与工程应用.文中从模型、求解与应用3个角度,对马尔可夫决策过程动态优化理论模型进行了综述,并重点介绍了将动态优化理论与随机Petri网理论相结合的马尔可夫决策Petri网和随机博弈网模型,详细讨论了这些模型的建模方法、求解算法与一些应用实例.最后,对全文进行了总结,并对未来可能的研究方向进行了展望.关键词动态优化;马尔可夫决策过程;随机Petri网;马尔可夫决策Petri网;随机博弈网1引言随着计算机网络与计算机系统在国民生活各个领域应用的不断拓展,其承载的业务种类与数量也在不断增加.如何在复杂的应用环境中合理地分配系统资源并进行调度任务,以提高计算机系统与计算机网络的运行效率,降低运行成本,是一个亟待解决的问题.优化理论是学术界研究计算机系统与计算机网络中资源分配与任务调度问题普遍采用的方法之一.从时间这个维度进行分类,优化理论可分为静态优化与动态优化两种.其中,静态优化将系统看作为一个时不变系统,即将系统的资源需求量与资源保有量视为一个与时间无关的常量.但是,实际的系统往往都是随时间变化的,而且会受到各种外部随机事件的影响.静态优化模型忽略了未来可能的系统变化,也不能反映决策者当前行为对未来的影响,无法刻画系统随时间变化的特性.因此,本文着重研究动态优化理论在计算机系统与计算机网络中的应用.在动态优化理论中,系统的目标函数是系统收益关于时间的累积量.相对于静态优化理论,动态优化理论可以较好地对系统的时变性进行刻画,更好地反映系统当前决策对时间累积目标函数的影响.动态优化的基本理论模型是马尔可夫决策过程(MarkovDecisionProcess,MDP).MDP可以用来描述这样一类离散时间决策过程:系统t+1刻状态的转移,只依赖于t时刻的系统状态与决策者的行为,而与[0,t-1]时间段内的系统状态与决策者行为无关.MDP可以从执行时间、决策者观测能力、状态转移的确定关系、时间的连续性、状态转移/收益的确定性、是否具有附加限制条件以及决策目标数量等角度进行分类.通常情况下,对于计算机系统与计算机网络中的资源管理问题,由于资源的种类繁多,数量庞大,因而所建立的MDP模型通常会遇到“状态空间爆炸”问题,即MDP模型的状态空间随着问题规模指数级增长,这使得传统精确求解算法如值迭代与策略迭代等无法应用.因此,本文详细讨论了MDP模型的近似求解算法,将这些算法归为3类:贪心算法、基于状态聚合的算法以及基于近似动态规划(ApproximateDynamicProgramming,ADP)的算法.马尔可夫决策过程在实际应用中体现出一些不足,主要表现在:(1)模型不够直观.一方面,MDP中的各个模型要素都使用了严格的形式化定义,虽然具有较强的逻辑性与严密性,但是模型的直观性与可理解性却相对较低.另一方面,模型建立需要较强的数学背景,例如在推导系统状态转移概率时,往往需要建模者具有一定的随机数学基础,增加了模型建立的难度.(2)在一些复杂应用环境中,单纯的MDP模型难以精确刻画系统的特点.例如,在网络安全问题中,MDP难以描述网络拓扑与各个组件之间的逻辑关系.这些不足激励学者们进行了进一步的模型拓展研究,其中较有代表性的是马尔可夫决策Petri网(MarkovDecisionPetriNets,MDPN)与随机博弈网(StochasticGameNets,SGN).这些模型方法将动态优化理论与随机Petri网理论相结合,在一定程度上克服了上述缺点.随机Petri网模型语义明确,使用图形化表示方式,直观易懂.系统中各个组件之间的关系可以灵活地使用组件之间的连接弧与变迁可实施函数等方式表现.建模者可以将精力更多地放在研究目标系统与精确描述系统与决策者行为方面,而状态转移概率等其它较为复杂的模型元素则可以利用Petri网工具中集成的功能实现自动化推导.MDPN模型将MDP理论与随机Petri网理论相结合,可以体现出系统与决策者宏观层面上的行为交替.利用MDPN模型,可以方便地借助Petri网图形工具对系统进行建模,并对模型的可达图进行规约得到MDP模型.SGN模型是动态优化模型的进一步扩展,它将动态随机博弈与Petri网理论相结合,允许系统中存在多个决策者.每个决策者一般都有各自的目标函数,他们之间既可以是合作关系,也可以是竞争关系.在建立SGN模型时,可以先单独从各个决策者的角度出发,建立SGN子模型,再利用模型组合与化简技术,得到完整的SGN模型.求解SGN是一个寻求每个决策者均衡策略的问题,可归结为一个静态非线性规划问题.动态优化模型是当前计算机系统与计算机网络的资源分配与任务调度等问题中的研究热点,对降低系统维护成本、提高系统运行效率具有重要的意义.本文从建模、求解与应用等角度,论述了马尔可夫决策过程、马尔可夫决策Petri网以及随机博弈网等动态优化模型在计算机系统与计算机网络中的应用.Page32基于马尔可夫决策过程的动态优化模型2.1马尔可夫决策过程一个基本的马尔可夫决策过程包括以下要素:(1)状态集合S,描述系统的状态.(2)行为集合A,描述决策者在状态空间中可能的行为.通常行为集合会依赖于当前状态,即可将其记为A(s).(3)收益函数R(s,s,a),s,s∈S,a∈A,描述系统在决策者行为的影响下运行所产生的收益.(4)状态转移关系SM,描述系统状态在决策行为影响下的转移过程.马尔可夫决策过程的一个显著特征是无后效性,即系统在下一时刻的状态仅依赖于当前所处的状态与决策行为,而与系统的历史无关.根据SM的性质不同,MDP可以分为确定MDP与随机MDP两大类.对于确定MDP,在某个状态下的某个行为会导致唯一确定的状态转移,即SM:S×A→S,此时状态转移方程可记为s=SM(s,a);对于随机MDP,未来系统状态不仅取决于当前系统状态下决策者的行为,还受到外部随机变量W的影响,即SM:S×A×W→S,此时状态转移方程可记为s=SM(s,a,W(ω)),其中W(ω)为外部随机变量的一个实现样本.随机MDP的未来状态一般服从某种分布,该分布可记为P(s|s,a).本文主要研究随机MDP,下文中提到的马尔可夫决策过程,一般均指随机马尔可夫决策过程.定义R(s,a)=∑s∈SP(s|s,a)R(s,s,a)为状态s下采用行为a所产生的收益.在马尔可夫决策过程中,策略π定义为从状态集合S到行为集合A的一个映射.决策者根据策略π来得到当前所需的决策行为.一个典型的马尔可夫决策过程的执行流程如下:1.决策者观察当前所处的状态s.2.根据当前状态确定决策行为π(s).3.执行行为π(s),系统状态发生转换.4.重复1.MDP在系统演进过程中,会产生一个收益序列.为比较MDP中决策的优劣程度,引入了目标函数J.它将一个收益序列映射为一个单一的实数值.对于无限时间MDP来说,其设置一般有3种方法:(1)在无限时间MDP中截取一个足够长的有限时间MDP,则无限时间MDP的目标函数可近似地看作有限时间MDP的收益的和.(2)依照时间的推移对未来所得收益进行逐步折扣,保证对时间累加的总收益总是收敛的.这种方式更看重当前所得的收益.(3)平均收益在时间趋于无穷处的极限值.通过目标函数J,可以定义策略之间的偏序关系,这样就可以对策略的优劣进行比较了.MDP中另一个重要概念是值函数Vπ(s).Vπ(s)是从π×S到实数集的映射,其含义为在采用策略π的前提下,在状态s∈S下所得到的目标函数J的期望.无限时间MDP的值函数满足Bellman递推方程,即式(1):Vπ(st)=R(st,π(st))+α∑st+1∈S其中α为折扣因子.式(1)说明,给定策略π,则在状态s的值函数等于当前一步决策所得收益与下一时刻折扣后值函数期望的和.式(1)也可以写成如式(2)所示的向量形式,即2.2马尔可夫决策过程建模与分析时,可使用如下步骤:在利用马尔可夫决策过程对系统进行建模分析(1)明确系统运行目标.该步骤中需要确定MDP的收益函数R与目标函数J.一方面,不同系统的运行目标可能不同.另一方面,即使是对于同一系统,研究角度的差异也会导致不同的收益函数与目标函数.以计算机网络为例,较为常用的目标函数有①节点吞吐量[1-3];②能量消耗[4-6];③信道利用率[7-8];④延迟[9-10];⑤分组丢失率[11].对于随机MDP,通常使用带有期望形式(E)的有限马尔可夫决策过程:目标函数.一般期望目标函数具有如下形式:无限马尔可夫决策过程:Page4其中,st与at分别为t阶段系统所处状态与决策者采取的行为.式(4)与式(5)分别为无穷时间折扣情形与无穷时间平均情形下的目标函数.系统的运行目标通常是最大化或最小化上述目标函数J.(2)确定系统运行状态空间与决策者的行为空间.系统的状态空间与决策者的行为空间可能是离散可列的.例如,在认知无线电系统中,信道可以用两个离散的状态刻画,即{空闲,占用},用户的行为也可能是离散的,如{发送数据,监听信道}.状态空间与行为空间也可能是连续的.例如,在上例中,若用户的行为变为“以概率p发送数据”,则用户行为空间是连续的,且其取值范围为[0,1].(3)根据系统状态之间的动态转移关系建立Bellman递推方程.该步骤中要找到状态之间的转移关系.对于随机MDP来说,转移关系包括状态转移方程s=SM(s,a,W(ω))与转移概率P(s|s,a).有时状态转移概率无法精确得知,此时可以使用强化学习[12]的方法来求解马尔可夫决策过程.Bellman方程描述的是值函数V的递推关系,该方程在求解最优策略时发挥了重要作用.(4)根据所建立的Bellman递推方程,对模型进行求解,得到最优策略π.以最大化目标函数为例,求解过程中的关键步骤包括π(st)=argmaxat∈AR(st,at)+α∑st+1∈SV(st)=Rst,π(st())+α∑s∈S式(6)按照最大化策略,根据当前所得的值函数V求得在状态s下应该采取的策略π,而式(7)则根据π计算其所对应的值函数.式(6)、(7)实际上是一个迭代的过程.不同的求解算法,如值迭代、策略迭代等,均需要使用以上两个步骤,只是顺序不同.下面以一个接纳控制为例(图1),举例说明MDP的建模方法.外部任务到达后,首先缓存在接纳控制器的等待队列中.在每个时间槽的开始,接纳控制器将其等待队列中的任务按照某种策略或将任务丢弃,或将任务分配给服务器1~n中的一个.在这个系统中,决策者为接纳控制器,其t时刻的行为是向量狓t={xit},其中每个分量xit表示向服务器i分配的任务数.系统中的外部随机变量W包括两部分:①[t-1,t]内到达接纳控制器的任务数λt;②[t,t+1]内服务器i完成的任务数μit.系统在t时刻的状态可用{λt,q1t,…,qnt}表示.其中qit表示服务器i中的队列长度.假设若等待队列中的任务没有得到及时服务,则下一时刻这些任务会丢失.系统的状态转移方程可写为此时决策行为受到如下流守恒条件约束:若假设系统每个时间槽内的收益与完成的任务数成正比,与服务器中驻留的任务数成反比,则系统在[t,t+1]时间段内的收益函数可定义如下:其中r为每完成一个任务所得的收益,ci为在服务器i上每个时间槽内服务一个任务所需要的成本.该系统的无穷时间折扣MDP的目标函数为MDP的求解方法将在2.4小节进行详细讨论.根据不同的划分依据,可将马尔可夫决策过程2.3马尔可夫决策过程的分类进行分类,如表1所示.执行的时间决策者的观测能力完全可观测MDP,部分可观测MDP转移关系的确定性确定MDP,随机MDP时间的连续性转移概率/收益的确定性普通MDP,带有强化学习的MDP是否有附加限制条件不受限MDP,受限MDP(CMDP)目标的数量(1)按照系统的执行时间分类.现实中系统的运行时间都是有限的.对于有限时间马尔可夫决策过程,其目标函数可以简单地写成在系统运行期间内收益的和,如文献[3].当系统Page5(2)按照决策者的观测能力分类.一般情形下,决策者可以完全观测到系统的状态,并根据所观测到的状态进行决策.但是,有些时候决策者不能完全观测到系统状态.这时,需要利用部分可观测马尔可夫决策过程(PartiallyObservableMarkovDecisionProcess,POMDP)进行建模.Zhao等人[2-3]研究了认知无线电系统中次用户的信道监听与接入的问题.在该问题中,由于次用户监听可能发生错误,因此系统是一个部分可观测马尔可夫决策过程.POMDP求解相较于MDP来说较为复杂.因为决策者没有系统状态的精确信息,所以需要维护一个信任向量,用来描述系统当前位于各个状态的概率.信任向量随着系统的演进而不断更新.(3)按照转移关系的确定性分类.决策者在某个状态下所做的行为,有时会导致一个确定的结果,即以概率1转移到下一个状态,这称为确定马尔可夫决策过程.有时,决策者的行为会导致不确定的结果,这称为随机马尔可夫决策过程.(4)按照时间的连续性分类.现实中的一些问题是离散时间的,例如在库存管理问题中,仓库管理员一般每隔一个固定的时间间隔采购商品,更新库存.还有一些问题是连续时间的,典型问题如队列管理问题[14]与设备维护问题[15-16].在这些问题中,系统的状态转换间隔时间(如顾客到达的间隔时间与设备正常运转的时间等)服从指数分布,每次系统状态发生转换时都需要决策者进行决策.运行时间很大时,也可以近似地认为系统的运行时间是无限的.此时针对该系统建立的马尔可夫决策过程就是无限时间马尔可夫决策过程.无限时间马尔可夫决策过程通常采用折扣累积收益或平均收益作为其目标函数,即式(4)和(5).Haas等人[13]分别针对这两种目标函数研究了无线多媒体网络环境中的资源分配问题.折扣累积收益目标函数已经被广泛研究,理论成果较为完善.(5)按照转移概率/收益的确定性分类.在一些复杂系统中,系统的状态转移概率P(s|s,a)难以精确测量,收益函数R(s,a)也无法推导出显式的解析式.这时,就需要建立基于强化学习的MDP模型,采用跟踪实际系统运行过程或MonteCarlo模拟等方法,不断学习系统的未知特性.值得注意的是,基于强化学习的MDP也是一种MDP的近似求解方法,简化了Bellman方程中值函数期望的计算.(6)根据是否有附加限制条件分类.有时决策者行为会受到一些客观条件的影响,这时可将该问题归结为一个受限马尔可夫决策过程(ConstrainedMarkovDecisionProcess,CMDP)问题.以折扣情形为例,一个CMDP模型可表达为约束(9)中c(st,at)可视为阶段t所产生的资源消耗,C为客观资源总量限制.这类问题在计算机系统内有大量的应用,如Djonin等人[17]研究了在MIMO系统中,在数据延迟受限的情况下,最小化平均发射功率的问题.求解CMDP可以使用线性规划法[18]与拉格朗日法[19]等.(7)按照目标数量分类.很多动态优化问题只考虑一个目标函数,也就是常见的单目标优化问题.如果目标函数有多个,就需要用多目标优化建模.一般处理多目标问题的方式有3种:①将一部分目标函数转化为约束,进而转化为CMDP模型[20];②将各个目标加权平均,组合成一个整体目标[21];③求解帕雷托前沿(ParetoFrontier)[22-23].2.4马尔可夫决策过程的求解夫决策过程的求解.求解算法分类如图2所示.由于篇幅所限,本文主要讨论无穷折扣马尔可2.4.1精确求解算法折扣情形下的最优解满足运算符T定义如下:[T(犞)]s=maxa∈A(s)R(s,a)+α∑s∈S其中,[·]s为向量的第s个分量.满足式(10)的值函数即是最优值函数犞.可以看到,式(11)实际上是采取最大化策略的式(1)的变形.Page6(1)值迭代算法.值迭代算法实际上是近似算法,随着迭代过程的进行,该算法会不断逼近最优解.值迭代算法如算法1所示.算法1.值迭代算法.1.n=0,给定初值犞0=狏.2.根据迭代式犞n=T(犞n-1),计算第n次迭代的值函3.重复步2.可以证明,算法1在n→时收敛于最优值函数犞.此外,还可以在每一次迭代时估计出最优解的区间,即犞n+α其中,犲为全1向量,αn与βn定义如下:式(12)也可以作为值迭代算法运行结束的判定方法.例如,可事先指定一精度ε,使得成立时算法终止.(2)策略迭代算法.可以证明,当状态集合与行为集合有限时,策略迭代算法可以在有限迭代次数内获得最优解,且迭代次数上界为策略数,即∏s∈SA(s),其中|·|为集合内的元素个数.策略迭代算法如算法2所示.算法2首先确定一个初始策略π0,并直接根据式(2)求解得到该策略所对应的值函数.最后,再根据所得的值函数对策略进行更新.若更新前后的策略相同,则说明已经找到了最优策略,算法结束.算法2.策略迭代算法.1.n=0,给定初始策略π0.2.通过求解(犐-α犘πn)犞n=犚πn确定犞n.3.确定πn+1使其满足4.ifπn+1=πn,算法终止,设定最优策略π=πn.elsen=n+1,转到步2.此外,学者们还基于以上两种基本算法设计了一些变形算法,如修正的策略迭代(ModifiedPolicyIteration)等,此处不再赘述.2.4.2近似求解算法在一个实际系统中,资源种类与资源数量都极其庞大,导致所建立的MDP模型无法利用精确算法进行求解,原因在于:①需要为每个状态存储其值函数.在状态数较多时,现有的技术无法提供足够的存储空间;②在迭代过程中,计算值函数要遍历所有状态,会导致迭代一次所需时间较长,算法收敛速度太慢.基于这些考虑,人们开始寻找MDP的近似求解算法,使得在有限的时空复杂度范围内,得到可接受的次优解.(1)贪心算法贪心算法又称为近视策略(myopicpolicy),它可表示为:在时刻t,求解如下优化问题例如,在图1的接纳控制问题中,贪心算法为贪心算法是最简单的一类近似算法.它只关注系统当前的收益,而忽略当前决策对未来收益的影响.这种方法虽然未必是最优的,但是至少提供了一种动态优化问题的简单求解方案.贪心算法的最大优点在于,其求解过程没有算法1与算法2中的迭代过程,因而时间复杂度较低.此外,也不需要提供存储值函数的空间.在一些特殊的动态优化模型中,贪心策略就是最优策略.Karush与Dear[24]将一个学习过程利用POMDP建模,并证明了贪心策略在该类问题中的最优性.Krishnamurthy等人[25]研究了目标跟踪中的动态传感器调度问题,并给出了贪心策略是最优策略的一些充分条件.文献[26-28]分别从不同角度研究了机会频谱接入问题,并证明了贪心策略的最优性.然而,通常情况下贪心策略并非最优策略.如Ahmad等人[29]指出,在负相关系统转移的机会频谱接入问题中,若信道都是独立同分布的Gilbert-Elliot信道,当信道数量大于3时,贪心策略并不是最优策略.虽然如此,在很多应用中,贪心算法表现出较好的适应性[30-31].(2)基于状态聚合的算法精确求解算法应用的最大障碍是状态空间爆炸问题.因此,一种很直观的近似求解策略是将问题空间进行聚合化简,使得问题规模减少,便于精确算法求解.以图1中问题为例,假设接纳控制器的等待队列与所有服务器的服务队列最大容量均为100个任Page7务,则该问题MDP模型的状态空间共有100n+1个状态.此时,可设定如下状态聚合策略:为每个队列设定一个阈值,当队列长度低于该阈值时,则认为处于宏状态“低负载”,反之,则处于宏状态“高负载”.这样,每个队列的状态可以化简为2,整个系统的状态也缩小为2n+1.一种常用的MDP状态聚合方法来源于马尔可夫过程的近似求解理论,见算法3.假设有状态转换如图2所示的马尔可夫决策过程.如果存在一种对状态空间的划分,在每个划分内,任选一个状态,使得:①以该状态作为起始状态,则转移到该状态所属划分内状态的概率很大;②以该状态作为起始状态,则转移到不属于该状态所属划分内状态的概率很小,则这个MDP可以进行状态聚合化简.例如在图3中,实线转移概率比虚线转移概率大很多,则该模型可以利用图中所示方式进行聚合.Liu等人[32]利用该方法近似求解了分布式Web服务系统中的服务器选择问题.算法3.MDP状态空间化简算法.1.将状态空间S进行划分:{S1,S2,…,Sn}.2.fori=1ton3.将所有转到Si以外状态的概率都设置为0.4.将Si内的状态转移概率进行归一化处理.5.计算Si内状态的稳态概率分布,利用π=π犘,其中6.计算Si到Sj的转移概率Pij=∑k∈Si7.endfor值得注意的是,只有在上面两个假设条件都满足的时候,算法3才能得到精度较高的近似解.否则,误差会比较大.此外,这种近似方法还有一个缺点,即不能得到近似解与精确解之间的关系.为了克服这些问题,学者们又提出了其它解决方案,有界参数MDP(Bounded-ParametersMDP,BMDP)就是这些方案中较有影响力的方法之一.BMDP由Givan等人[33-34]提出,它是非精确状态转移概率MDP(MarkovDecisionProcesseswithImpreciselyKnownTransitionProbabilities,MDPIPs)模型的一种特殊情况.BMDP是一个4元组S,A,R,P{每个状态的收益函数R与状态转移概率P是一个区间,而不是一个点值.若一个MDPM,其状态、行为集与BMDPM的状态、行为集完全相同,且M的收益函数R与转移概率P都在M所规定的区间内,则称M∈M.在一个BMDPM中,给定一个决策策略π,则该策略所产生的值函数也是一个区间,称为区间值函数Va其中VπM中的一个MDPM的值函数.可根据实际工程应用背景,定义区间值函数的比较方法.例如对于策略a与b,可定义:①乐观最优Vb②悲观最优VbVa可以证明,存在M∈M,使得所有状态的值函数能同时达到最大或最小,并称这两个MDP分别为关于策略π的最大MDP与最小MDP.寻找最大或最小MDP的过程,相当于寻找关于值函数上界降序排列状态空间序列与值函数下界升序排列状态空间序列的序列最大MDP(OrderMaximizingMDP).具体来讲,一个状态空间序列O={s1,s2,…,sn}为状态空间中所有状态的一个排列顺序,则状态空间序列O的序列最大下标r与序列最大MDPMO可定义如下.定义1(序列最大下标与序列最大MDP)[34].对于某个状态s与决策行为a,其关于序列O的序列最大下标r为argmax1rn∑r-1相应的序列最大MDP是一个满足式(18)的MDPMO∈M利用BMDP可以对问题空间进行状态聚合.一个精确MDP经过聚合后,一般都可以归结为一个BMDP问题,可以利用区间迭代求解算法进行求解,即Page8IVIopt(V)(s)=maxa∈A(s)[minM∈M计算式(20)实际上可以看作为具有2个决策者的2步博弈过程.以乐观最优为例,在第1步中,决策者1与决策者2为合作配合关系,决策者1利用所定义的乐观最优比较运算符求得最大化区间值函数上界的策略π↑opt.在第2步中,决策者1与决策者2为对立竞争关系,决策者2求得策略π↑opt的最小MDP,并计算区间值函数的下界.该过程可用算法4描述.其中,Sort_Dec_Order与Sort_Inc_Order为排序函数,Order_Max_Ind利用式(17)求得对应的序列最大下标.这样,就可以在缩小的问题空间中,求得原问题具有边界的解.算法4.区间迭代算法.1.Oup=Sort_Dec_Order(V↑),2.foralls∈Sdo3.foralls∈Sdo4.rup=Order_Max_Ind(M,Oup,s,a).5.rdown=Order_Max_Ind(M,Odown,s,a).6.fori=1tondo7.根据式(18)、(19)计算Pup(sOdown(i)|s,a)与8.endfor9.endfor10.V↑=max犪A(s)R↑(s,a)+α∑s∈S11.if|a|=1anda={a}then12.V↓=R↓(s,a)+α∑s∈S13.π(s)=a.14.else15.π(s)=a.16.endif17.endfor(3)基于近似动态规划的算法近似动态规划(ApproximateDynamicPro-gramming,ADP)是一种解决大规模动态优化问题的现代近似求解方法.目前,关于近似动态规划的代表性专著,主要有3本[12,35-36],分别从人工智能、控制论以及运筹学的角度对近似动态规划进行了详细的论述.近似动态规划能有效解决马尔可夫决策过程中的状态空间爆炸问题.在ADP中,式(11)通常改写为V(st)=maxat∈A(st)R(st,at)+α·E{V(st+1)}(21)在式(21)中,状态空间爆炸问题表现为:(1)问题状态空间S太大,现有的技术无法提供足够的存储空间;(2)外部随机变量有时无法精确测量其分布,或即使分布已知,也会由于随机变量状态太多而导致其期望难于计算.在近似动态规划中,主要使用基于值函数近似(ValueFunctionApproximation)与后决策状态(Post-DecisionStateVariable)的前向动态规划方法来克服以上问题.令系统的状态转换方程为其中,W(ωt)是t时刻外部随机变量的一个样本,则基本的近似动态规划算法可表述为算法5.算法5.基本近似动态规划算法.1.初始化:2.fort=0toTdo3.求解4.利用下式对珚V(st)进行更新5.选定一个采样路径ωt.6.计算下一个状态7.endfor算法5首先初始化所有状态的值函数,并指定一个初始状态.然后,利用MonteCarlo方法对随机外部信息进行一次采样.算法的核心是步2~7,首先求解一步优化问题(步3),并利用所得出的v^t对值函数进行更新.其中ηt是步长.该算法与用于求解一般马尔可夫决策过程迭代算法的最根本区别,在于时间是顺序演进的,而不是倒序演进的.算法运行的过程,实际上是一个系统仿真的过程.以图1中接纳控制问题为例,其ADP求解算法可描述如下:1.设定每个状态值函数的初始值,选取起始状态,并令t=0.2.采集t时刻系统状态,根据当前的值函数,计算当前决策行为xt,并得到当前状态值函数的一个样本v^t(算法5步3).其中,E珚V(st+1)|s{}t可用MonteCarlo模拟的方法求得.3.根据值函数样本,更新当前状态的值函数(算法5步4).Page94.使用MonteCarlo方法得到外部随机变量的样本,即任务到达数λt与任务完成数μit.5.t←t+1,并根据式(8)得到t+1时刻的系统状态,重复步2.该算法其优点显而易见,在迭代过程中不需要枚举系统的所有状态来计算值函数,一定程度上规避了状态空间爆炸问题.但是,算法5中仍然存在不足.例如,该算法为每个状态均设立一个变量珚V(s)用以存储其值函数.当问题状态空间较大时,难以提供足够的存储空间.同时,该算法只更新所遍历到的状态的值函数,而未遍历到的状态的值函数却得不到更新.下面我们就算法5中的各个步骤展开论述,详细介绍近似动态规划算法克服状态空间爆炸问题的主要手段.①后决策状态后决策状态是决策者做完决策后、且外部随机信息到达前系统的状态.这样,式(22)就分为了两步:其中,sxt称为t时刻的后决策状态,st+1称为t+1时刻的前决策状态.后决策状态可以看作为前决策状态与决策行为的确定函数.以图1中接纳控制问题为例,{λt,q1t,…,qnt}为系统的前决策状态,而其后决策状态为{qx,1t,…,qx,nt},它们之间的状态转移如下后决策状态的值函数定义如下:注意到式(26)中,等式右边已经没有期望运算.即它是下一时刻前决策状态值函数的期望.此时,步3可以改写为v^t=maxat∈A(st)R(st,at)+α·珚V(SM,x(st,at))(26)②值函数近似在算法1与算法2中,值函数表现为一种“查表”形式(TableLookupForm),即算法需要维护一个值函数表,表项为每个状态s所对应的值函数V(s).这种方式使得值函数的存储与计算都较为困难.在ADP中,可以利用函数近似的方法,利用一些简单的函数形式拟合后决策状态的值函数.线性值函数近似是普遍使用的一种值函数近似方法.令为动态优化问题中的特征集,该特征集与问题结构本身有较大相关性.如在分布式库存管理问题中,特征集可以包括各地库存量、各地仓库在单位时间内到达的货物量、库存变化的方差以及这些特征的平方等[37].定义基函数(BasisFunction)f(sxt),f∈为关于后决策状态sxt中某一特征f数量关系的函数,即f(sxt)为从后决策状态集合到实数集合的映射,则后决策状态的值函数可以利用如下方式进行近似:此时算法5中步3可以进一步改写为v^t=maxat∈A(st){R(st,at)+α·∑f∈这样,估计值函数的过程,就转化为估计θf的过程,即θf随时间演进而不断更新,因此也可记作θf,t.一般情况下,特征集的空间远小于问题的状态空间.因此,值函数近似可以较好地解决状态空间爆炸的问题.③值函数样本的取得2.1小节提到,状态st的值函数V(st)为从状态st开始到时间趋于无穷时收益函数的累加.在策略π作用下,V(st)的一个无偏估计样本可以直观地写为式(29)可以用一个有限时间累计收益进行近似,即取一个足够大的T,使得αT-t→0,则式(29)还可改写为v^(st)=∑V(st)-αV(s)由于α∈(0,1)且V(s)有界,因而αV(s)→0,式(31)可近似地变换为v^(st)=V(st)+∑其中R(sτ,aπτ)-V(sτ)+αV(sτ+1)称为即时差分(TemporalDifference,TD)或Bellman误差(Bell-manError),表示当前值函数估计值与上次值函数估计值之间的差.在一些文献中,折扣因子α有时用λ表示,因此这种取得值函数样本的方法又叫做TD(λ).当折扣因子α=0时,又可以得到一种特殊的表示方式:式(33)称为TD(0).注意式(33)与带有后决策状Page10态变量的Bellman方程(26)极为相似.当π为式(26)中的最大化策略时,V(st+1)为式(26)中珚V(SM,x(st,at))的无偏样本.当利用形如式(27)所示的值函数近似方法时,ADP算法并不关注值函数本身,而着重考察值函数的导数θf.例如,在资源管理问题中,f(sxt)可以代表具有某一特性f的资源的数量,这时,θf的物理含义是该类资源的边际收益[38].θf的样本可以通过以下两种方法得到:(i)优化问题的对偶变量.一般资源管理问题都存在资源数量的约束,该约束所对应的对偶变量θ^f就是资源的影子价格,即θf的样本.(ii)数值微分.在状态st,根据式(33)可得v^(st).此时,可将状态st的f类资源的数量减1得到状态珓st,重新进行优化,得到v^(珓st),则数值微分可表示为④值函数更新方法随机梯度法是一种常用的值函数更新方法,可以通过逐步学习值函数样本v^,使珚V不断逼近真实值函数.随机梯度法的目标是即寻找最符合样本v^的值函数珚V.由于v^(s)是一个随机变量,因此该问题为一个随机优化问题,其求解算法与静态优化问题中的梯度法类似,称为随机梯度法.若在t时刻,系统位于状态s,对应的步长为ηt,则珚V(s)←珚V(s)-ηt(珚V(s)-v^(s))=(1-ηt)珚V(s)+ηtv^(s).注意到该式就是算法5中步4.若使用后决策状态的值函数,则优化目标(34)变为此时更新方法为珚V(sxt-1)←珚V(sxt-1)-ηt(珚V(sxt-1)-v^(st))若使用形如式(27)的后决策状态值函数近似策略,则随机梯度法的目标变为即寻找最接近实际值函数的后决策状态近似值函数珚V(sxt-1|θ).此时只需更新θ:θ←θ-ηt珚V(sxt-1|θ)-v^(st其中,由式(27)得θ珚V(sxt-1|θ)=此外,还有一些基于线性回归的值函数更新算法,如最小二乘即时差分(LeastSquaresTemporalDifferences,LSTD)与最小二乘策略估计(LeastSquaresPolicyEvaluation,LSPE)[35].这两种算法的主要区别在于,LSTD采集所有值函数样本后一次进行拟合,而LSPE为一种边采集值函数样本边拟合的递归算法.⑤状态聚合2.4.2节介绍了一些基于状态聚合的近似求解算法.事实上,ADP中也可以使用状态聚合.不失一般性,一个聚合状态sg的值函数V(sg)可定义为该聚合状态所包含状态的值函数的平均值,即状态聚合解决了状态空间爆炸问题,但是随之而来的问题是如何确定合理的状态聚合策略以获得较好的近似解.George等人[39]提出了一种多层状态聚合的思想,将状态的近似值函数定义为不同层次聚合值函数的加权平均.令G为聚合层次的集合,则其中sg为非聚合状态s在第g层聚合中所对应的聚合状态.wg可以通过跟踪各层聚合状态值函数的误差与方差等参数确定.这种方法在实际样本较少的问题中,显示出较强的适应性,可以加速算法的收敛速度.⑥步长步长一般可分为两类,一类是确定步长,如ηt=1/(t+1)或ηt=a/(t+a)等;另一类是随机步长,这类步长与每次取得的样本v^或θ^等相关,一般收敛速度较快.本文中以确定步长为例,简要介绍ADP值函数更新算法中的步长.为保证随机梯度法收敛,一般要求确定步长η满足如下条件:(i)ηt0;(ii)∑Page11.在算法5的步4中,由于值函数样本v^t与所估计的值函数珚V(st)单位相同,因而可以简单地取0ηt1,如令ηt=1/(t+1),t=0,1,….然而,在随机梯度法式(37)中,由于等式右边珚V(s|θt-1)-v(θt-1)与θ的单位不一定相同,η的取值还需仔细调整.Powell对步长进行了较为详细的介绍[36],有兴趣的读者可以参考.⑦探索(Exploration)与利用(Exploitation)问题算法5中采用前向动态规划法,且下一个状态st+1的选取都与当前状态st所做的决策at有关,这称为依照策略的学习方式(On-PolicyLearning).这种方法充分利用了前期估计得到的统计信息,会不断提高所遍历到的状态的值函数,而没有遍历过的状态的值函数的数值则相对较低.这很容易导致算法收敛于局部最优解而非全局最优解.针对这个问题,学者们又提出了不依照策略的学习方式(Off-PolicyLearning).但是,这种方式不能保证ADP算法收敛.因此,又提出了一些折中的方案,如Boltzmann探索[40]等,在算法前期,先利用Off-PolicyLearning遍历尽量多的状态,采集足够多的统计信息,而在算法后期,则使用On-PolicyLearning方法,加快收敛速度.3基于马尔可夫决策Petri网的动态优化模型Beccuti等人[41-42]于2007年提出了马尔可夫决策Petri网(MarkovDecisionPetriNets,MDPN),将MDP的思想融入了Petri网中,其目的是为了提供一种比MDP更高层的建模工具,从宏观的角度反映决策者行为与系统行为的交替,并从语义的角度严格定义两种行为的转换过程.3.1马尔可夫决策Petri网马尔可夫决策Petri网可分为两种子网:代表系统行为的随机子网(ProbabilisticSubnet)以及代表决策者行为的非确定子网(NondeterministicSubnet).这两种子网通过立即变迁NdtoPr与PrtoNd同步.随机子网的行为通过两类变迁Trunpr与Tstoppr来描述.Trunpr代表系统运行的中间过程,而Tstoppr代表系统当前阶段运行过程的终止.随机子网中的每个变迁都对应一个权值(weight),用来计算某个状态下系统可实施变迁的概率.此外,每个变迁还对应系统中一个触发该变迁的行为(act),包括组件集合的一个子集.在MDPN中,系统由多个组件构成.这些组件有些是可控的,有些是不可控的.非确定子网用两类变迁Tnd制行为,而Tndl代表组件级的控制行为.与随机子网中的变迁类似,非确定子网中的这两类变迁又可以细分为Trunnd定子网中的变迁还对应一个对象,用以说明该变迁对应行为的施加组件对象.定义2(马尔可夫决策Petri网)[42].一个马尔可夫决策Petri网是一个四元组MN={Comppr,Compnd,Npr,Nnd},其中Comppr是一个有限非空系统组件集合;CompprComppr∪{ids}是非空可控组件集合,其中ids代表整个系统;Npr由3部分构成:①一个带有优先级的Petri网P,Tpr,Ipr,Opr,Hpr,priopr,m{的权值weight:Tpr→;③一个对应的行为act:Tpr→2Comppr,其中Tpr=Trunpr∪Tstoppr;Nnd由两部分构成:①一个带有优先级的Petri网{P,Tnd,Ind,Ond,Hnd,priond,m0};②一个对应的对象obj:Tnd→Compnd,其中Tnd=Trunnd∪Tstopnd.此外,MDPN还需满足以下条件:①一个变迁不能既是非确定变迁又是随机变迁;②每个系统组件至少可以触发一个Tstoppr类型的变迁;③每个可控系统组件至少是一个Tstopnd类型变迁的对象.在MDPN中,收益分为两部分.第一部分是状态收益,即系统到达某个状态后得到的收益.第二部分是行为收益,定义为一连串决策行为所得到的收益.行为收益与行为序列的顺序无关.3.2马尔可夫决策Petri网的建模与分析当构建好决策者行为子模型与系统行为子模型后,需要加入一些附加的位置与变迁,将两个子模型连接起来.一个基本的MDPN模型如图4所示.位置i、RunprStoppr系统组件、整个系统以及决策者之间进行同步.对于每个组件i,都有一个Stoppr者采取了针对整个系统的全局性行为,则需插入位置Stopnd0与Runnd0.若采取的是针对某个系统组件的局部行为,则需插入位置Stopndi与Runndi.变迁NdtoPr与PrtoNd描述系统行为与决策者行为的交替进行.NdtoPr只有在Stopnd0与所有Page12Stopndi位置都有标记时才能实施,代表模型由决策态转移到系统运行态.而PrtoNd相反只有在i位置都有标记才能实施,代表由系统运行态Stoppr转移到决策态.3.3马尔可夫决策Petri网的求解RGMDPMDPN的求解过程可以分为如下4个步骤[41]:(1)由MDWN模型求得该模型的可达图RG可达状态集合(ReachabilitySet,RS)可分为两部分:非确定状态(RSnd)与随机状态(RSpr).在非确定状态中,只有Tnd类型的变迁是可实施的,而在随机状态中,只有Tpr类型的变迁是可实施的.(2)将可达图RG规约为非确定可达图RGnd在RG中,定义非确定子路径与随机子路径分别为RG中经过同样类型状态的最大路径.搜索所有非确定子路径,并将每个非确定子路径压缩为一个决策状态,代表所有可能的决策行为,得到非确定可达图RGnd.(3)将非确定可达图RGnd规约为MDP可达图搜索所有随机子路径,通过路径途中经过变迁的权值,计算各个路径的概率,并将每个随机子路径压缩为RG中的一条有向弧,代表宏观的系统状态转移,得到MDP可达图RGMDP.(4)计算对应MDP的转移概率转移概率矩阵为其中,犘(pr,pr)为RG中从一个随机状态转移到另一个随机状态、且途中没有非确定状态的概率,犘(pr,nd)为从一个随机状态转移到非确定状态的概率.转移矩阵犘可用式(42)进行计算:犘=(5)根据Bellman方程计算MDP中的最优策略可根据算法1或算法2求得MDP中的最优策略,也就是MDWN模型中的最优控制策略.3.4应用与扩展本小节中将以一个可修复系统为例[41],对MDWN模型的各个要素进行说明,其模型如图5所示.左半部分为随机子网,描述一个既可能正常工作(变迁Workfine)、又可能失效(变迁FailProc)的系统组件.右半部分为非确定子网,描述决策者的行为,包括分配资源以维修失效组件(变迁AssignRes)与Page13不分配资源(变迁NoAssignRes).在随机子网中,Tstoppr类型的变迁有WorkFine、Fail、Wait、EndReq,Trunpr类型的变迁仅有一个,即Resume.非确定子网中所有的变迁均为Tstopnd类型.在MDPN模型中,在位置、标记以及变迁中增加颜色的概念后,可进一步得到马尔可夫良构Petri网(MarkovDecisionWell-formedNets,MDWN)模型.这种模型可以较好地处理具有对称属性的系统,有效地缩小问题空间.MDPN与MDWN的模型与算法都已经集成在GreatSPN工具中[43-45].进行仿真时,在每个系统终结状态可利用式(26)进行决策,并得到一个值函数样本.注意行为at可能是一个行为序列.此时,可利用式(35),在值函数样本的基础上,更新其上一时刻决策终结状态的值函数.这样,就将ADP中的前向动态规划算法集成到了MDWN中.目前,针对MDPN与MDWN模型的应用研究已经逐步开展.文献[46]分别利用MDPN/MDWN研究了高质量视频处理中的资源管理问题.文献[47]研究了无线传感器网络中,对象移动跟踪的最优能源管理问题.文献[48-49]研究了一类非确定可维修故障树(NondeterministicRepairableFaultTrees,NdRFT)模型与MDWN模型转换的方法,并将MDWN模型作为求解NdRFT模型最优策略的方法.3.5MDPN与ADP的结合3.3小节中的MDPN求解方法,将MDPN规约为MDP,然后再利用精确求解算法进行求解.这种方式使得MDPN的求解仍然存在“状态空间爆炸”问题.为此,我们将MDPN与ADP结合,利用ADP中MonteCarlo仿真的方法,解决MDPN的近似求解问题.在结合ADP方法的MDPN中,不需要通过Petri模型得到完全的可达图RG,也不需要通过式(42)计算状态之间的转移概率.相反的,在模拟系统与决策者行为的同时,不断地更新可达状态集RS.由于MDP中只关注决策与系统运行的最终状态,因此只需记录位置Stopndi中全部都有标记的状态(称为决策终结状态),或者位置Stoppr有标记时的状态(称为系统终结状态).若该状态在RS中不存在,才将该状态加入RS中.若新加入RS中的状态为决策终结状态,则为其关联一个后决策值函数并设定其初始值,其功能与式(25)类似.4基于随机博弈网的动态优化模型上述MDP、MDPN以及MDWN模型,都只能描述具有集中式控制设施的系统,即系统内只有一个决策者.在现实生活中,还存在着大量具有多个决策者的系统.上述模型在处理这类问题时,只能从各个决策者的角度分别建模,而将其他决策者视为不可控外部随机事件,无法体现出决策者之间的联系.文献[50]于2008年首次提出了随机博弈网(StochasticGameNets,SGN),将动态随机博弈与随机Petri网结合,能够对具有多个决策者的系统进行建模分析.动态随机博弈可以看作是马尔可夫决策过程的扩展,可包含多个决策者并能体现出他们之间的复杂关系,包括:(1)竞争关系.即每个决策者只关心最大化自己的收益;(2)合作关系.即所有决策者作为一个群体关心的是总收益.将动态随机博弈与随机Petri网相结合,有助于系统的细粒度建模与简化求解.4.1随机博弈网定义3(随机博弈网)[51].一个随机博弈网是一个9元组:SGN={N,P,T,F,π,λ,R,U,M0},其中:N={1,2,…,n}是决策者(博弈局中人)的P是有限的位置集合;T=T1∪T2∪…∪Tn是有限变迁的集合,其中π:T→[0,1]是决策者选择某个变迁的概率;FI∪O是弧的集合,其中I(P×T),O(T×P),且有P∩T=,P∪T≠.记x的前集合为·x={y|(y,x)∈F},x的后集合为x·={y|(x,y)∈F};R:T→(R1,R2,…,RN)为决策者采用某个变迁所对应行为所得的收益函数,其中Ri∈(-,+),i∈N;λ={λ1,λ2,…,λW}为变迁的实施速率,其中W是变迁的个数;U是决策者的总收益函数;M0是起始状态,代表所有决策者的最初状态.在该定义中,P是博弈的状态,在某个位置p∈P中有标记意味着所有决策者都在该状态中.位置p中的标记s对应一个收益向量Tk是第k∈N个决策者的行为;集合;Page14p(s)为决策者k在状态p中所得的收益.当其中hk变迁t实施,所有决策者都会得到收益其中Ri(t)为决策者i所得的收益.若标记经过变迁t到达位置p,则收益都会累加在标记的收益向量犺p(s)中.在SGN中,当系统运行至状态p时,决策者k的策略可定义为其中,π(tk显然,对于所有状态p,都有进一步,参照博弈论中纳什均衡的概念,可以定义SGN中的均衡策略π=(π1,π2,…,πn)满足Uk(π1,…,πk-1,πk,πk+1,…,πn)Uk(π1,…,πk-1,πk,πk+1,…,πn),k∈[1,2,…,n]其中πk是除πk外所有其它可能的策略.均衡策略的含义在于,某个决策者在其他决策者都不偏离均衡策略的情况下,采用非均衡策略不会取得比采用均衡策略更高的收益.换句话说,该决策者没有偏离均衡决策的动机.值得注意的是,在MDP中一般都使用确定行为(称为纯策略)作为最优解(一些例外的情况如探索/利用问题中会采取一些不确定行为来主动学习值函数).而在SGN中,一般采用在行为空间的概率分布(混合策略)作为均衡解,因为在多人决策问题中,在纯策略意义下一般不存在均衡解,而在混合策略意义下一定存在均衡解.4.2随机博弈网的建模与分析构建一个SGN模型一般分为4个步骤[52]:(1)建立每个决策者的子SGN模型.在实际系统中,识别出SGN对应的要素,包括①变迁.变迁代表决策者的行为.注意行为集合中也可能包括空行为,即决策者不采取任何行为.②收益.对于每个变迁t,赋予其一个收益函数R,其每个分量Ri代表决策者i在该行为结束后所得的收益.③位置集合P.每个位置p代表系统的一个状态.(2)描述纳什均衡条件.对于竞争博弈,每个决策者的目标是最大化自己的收益;对于合作博弈,每个决策者的目标是最大化所有决策者的收益的总和.对于有限时间SGN,可仿照MDP中式(3)定义决策者i的总收益:其中N是时间的长度,Rπn是阶段n使用策略π时所得的收益.注意Uπi与所有决策者的策略都相关,而并非只与i自己的策略相关.对于只有两个决策者的系统,均衡策略π={π1,π2}满足Uπ1,π21且Uπ1,π2Uπ1,π2(3)求解纳什均衡策略.一般情形下求解均衡策略难度较大.本文仅对只有两个决策者的特殊情况进行讨论,此时,系统求解问题可以化归为一个静态非线性规划问题,详细请参见4.3小节.(4)合并子模型,建立全局SGN模型.将子模型中含义相同的位置合并,可将所有子模型进行组合,得到全局SGN模型.4.3随机博弈网的求解文献[52-54]给出了二人动态博弈的纳什均衡求解方法,该方法基于文献[55],将二人动态博弈问题化归为一个静态非线性规划(NonLinearPro-gramming,NLP)问题:minU1,U2,π1,π2s.t.:R1(pi)π2(pi)+α犜(pi,U1)π2(pi)1TU1(pi),(π1(pi))TR2(pi)+α(π1(pi))T犜(pi,U2)1TU2(pi),其中,值向量为犜(p,U)={[犘(p1|p,t1,t2),…,犘(p|P||p,t1,t2)]TUk}且有k∈{1,2},i∈{1,…,|P|},pi∈犘,t1∈犜1,t2∈犜2.该非线性规划的最小全局解,就是SGN中的纳什均衡解.4.4随机博弈网的模型化简与合并当利用SGN对实际问题建模时,通常会遇到的一个问题是决策者的行为复杂,导致所建立的SGN模型难于求解分析.文献[56]针对这一问题,给出了一些SGN模型化简的方法,例如在图6左半部分所示的模型,可以等价地化简为右半部分的简单模型.4.2小节中提到,在构建SGN全局模型时,需要进行子模型合并.文献[57]讨论了在利用SGN对网络攻防进行建模时,子模型合并的方法,将决策者之间的关系分为两类:禁止类型与结束类型,相应的组合方法如图7所示.Page15一方面,当防御与攻击行为都可以实施时,若防御先实施,则可禁止攻击行为的实施(图7(a)).另一方面,防御行为的实施,也可以使得整个攻击过程结束(图7(b)).4.5应用与扩展文献[57]在SGN的基础上进行延伸,进一步提出了针对网络安全攻防的攻击-防御随机博弈网(Attack-DefenseStochasticGameNets,ADSGN),准确地刻画了网络攻击者与防御者之间的零和竞争博弈关系.本小节中以企业网中的安全攻防问题为例,说明SGN的建模方法.在一个典型企业网中,从攻击者与网络管理员的观点来看,网拓扑结构可抽象为图8.攻击者可进行一些攻击行为,如扫描网络脆弱性、攻击数据库、破译服务器密码等.网络管理员可以进行一些相应的防御措施,例如利用入侵检测系统进行扫描、阻止攻击者IP进入系统、移除嗅探器等.攻击者SGN子模型与防御者SGN子模型分别如图9、图10所示.这两个子模型从不同决策者的角度刻画了决策者在每个决策时间可能采取的攻防行为.应用4.4小节中的模型化简与合并技术,可将图9、图10中的SGN子模型合并为图11所示的SGN完整模型.Page16图11SGN完整模型目前,关于SGN的应用研究绝大多数都集中在网络安全方面,如文献[58]研究了利用网络连接关系与脆弱性信息等输入数据生成SGN模型的方法,文献[52-53,57]研究了企业网中的安全问题,文献[56,59]研究了电子商务中的若干安全问题,文献[54]研究了电子邮件蠕虫病毒的传播问题.另外,在无线网络领域,也有一些初步的研究成果,如文献[60]研究了无线网络中共享信道竞争的性能评价问题.总之,SGN是一个正在发展与完善中的研究领域,在理论与应用方面均具有较为广阔的前景.5结论与展望本文对动态优化在计算机系统与计算机网络中的建模、求解与应用进行了综述.相较于静态优化,动态优化可以精确地刻画系统的时变性.本文主要讨论了3种理论模型,即马尔可夫决策过程模型、马尔可夫决策Petri网模型以及随机博弈网模型,对这些模型的建模方法、求解算法、与应用实例进行了较为深入的研究.计算机系统与计算机网络中的资源种类复杂,数量众多.面对这种复杂的应用环境,如何合理地运用动态优化理论对系统进行建模,并采取适当的求解算法进行(近似)求解具有极大挑战性.在本文最后以以下几点为例,列举一些未来可能的研究方向:(1)马尔可夫决策过程的近似求解算法.众所周知,目前还不存在适用于所有MDP近似求解的统一“万能药”算法.很多看似合适的算法得出的近似解往往质量较差,在某些环境下甚至会出现算法不收敛的情况.近似解的质量在很大程度上还取决于算法设计者对领域专业知识的理解程度与算法设计经验,Powell甚至将ADP近似值函数中的特征函数选取称为一种“艺术(art)”[36].对于近似求解算法的应用范围、解的质量以及收敛性等一系列问题,还需要进一步深入研究.(2)马尔可夫决策Petri网与随机博弈网等模型的近似求解问题.一方面,在MDPN/MDWN与SGN模型中,虽然存在一些对模型进行化简的方法(如4.4节),但是这些方法往往局限于对某些特定模型结构的化简,还无法处理更为复杂的模型.另一方面,这些模型均采用精确求解算法,这使得利用这两种模型对大规模系统进行建模分析时求解较为困Page17难,大大限制了其应用范围.在3.5节中我们对MDWN中结合ADP算法的方式进行了一些初步的探索,但还不够深入.后续工作还应对这些模型的近似求解算法进行研究.(3)随机博弈网的应用研究拓展.目前随机博弈网模型方法主要应用于网络安全分析中,而就随机博弈网的模型特点来说,它可以适用于模型分析具有多个独立决策者参与的计算机系统应用,如无线网络、对等网络(P2P)以及社交网络等.进一步的研究工作将针对这些应用的特点,研究有针对性随机博弈网的建模与分析方法,拓展随机博弈网的应用领域.
