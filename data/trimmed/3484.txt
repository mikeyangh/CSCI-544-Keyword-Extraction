Page1海量数据分析的One-size-fits-allOLAP技术张延松1),2)焦敏1),3)王占伟1),3)王珊1),3)周?1),3)1)(数据工程与知识工程教育部重点实验室(中国人民大学)北京100872)2)(中国人民大学中国调查与数据中心北京100872)3)(中国人民大学信息学院北京100872)摘要传统的OLAP被迅速膨胀的海量数据推动进入了大规模数据分析时代,其主要特点是存储密度大,计算强度大,需要大规模并行存储和处理能力.无论是传统的并行数据库技术还是热点的MapReduce技术都不得不面对海量数据在大规模并行处理环境下的性能和并行处理效率的问题.以星型模型上复杂多表连接为基础的OLAP算法的复杂度和并行处理过程中的数据网络传输代价都成为制约性能的重要因素.通过深入分析OLAP存储模型和查询负载特征,提出了对OLAP查询中最基础的SPJGA-OLAP子集在存储、查询处理、数据分布、网络传输和分布式缓存等方面面向海量数据大规模并行处理框架的优化策略和实现技术.通过对TPC-H和SSB两个工业界和学术界公认的测试标准的分析,评估了技术的可行性.提出了以内存predicate-vectorDDTA-JOIN算法为核心的并行内存OLAP架构,以维表上规范化的谓词向量操作替代了多样的连接执行计划,实现以一种查询处理模型同时满足集中式处理和大规模并行OLAP处理的需求,充分利用现代计算机的硬件优势,最小化网络传输和OLAP查询处理代价.实验中分析了在1TB和100TB数据集中数据分布策略的存储代价和传输代价,通过并行OLAP代价模型和实际数据的实验测试验证了技术的可行性和并行处理效率.关键词OLAP;海量数据分析处理;谓词向量;星型模型1引言OLAP是一种多维数据分析处理模型,基于关系数据库的OLAP(RelationalOLAP,ROLAP)是一种面向分析型负载的读密集型查询处理.OLAP以星型模型和雪花型模型为存储模型,一般由一个事实表和多个维表组成,OLAP的基本功能是切片、切块、上卷、下钻、旋转等操作,即在事实表与维表连接的基础上进行不同粒度的分组聚集计算.在海量数据处理时代,TB级甚至PB级的数据需要大规模并行计算网络的支持,巨大的存储、连接、传输和聚集归并等代价使SQL引擎不堪重负.SQL引擎以传统的事务型处理为基础(OLTP),相对于OLAP负载以数据计算为中心的查询处理模式显得过于复杂,一方面复杂的事务和并发机制增加了冗余的代码代价,另一方面面向大数据集的复杂多表连接操作缺乏强有力的技术支持.以传统的并行事务处理为基础的并行数据库技术在扩展性方面受分布式事务控制机制的制约而缺乏良好的可扩展性,当前新兴的分析型数据库(如Vertica、ParAccel、Greenplum等)虽然面向分析型数据处理的特征优化了存储、查询处理和并行计算等技术,但其查询处理技术仍然带有OLTP查询处理引擎的影子,是一种由通用SQL引擎面向OLAP负载的特殊优化技术.MapReduce是一种大规模并行计算模型,它良好的扩展性使其成为海量数据大规模OLAP处理的候选技术方案,但MapReduce在解决多表连接问题时低下的性能使其难以适应复杂模型的OLAP处理.因此,问题的关键是,无论是并行数据库技术还是MapReduce技术都没有根据OLAP的本质特征来创建订制式的并行存储和处理框架,优化工作难以进一步深入.图1显示了SQL与OLAP的包含关系.SQL可以看作是查询处理技术的全集,包括事务处理和分析型处理,TPC-C和TPC-E是典型的OLTP负载.OLAP相当于SQL集合中面向分析型处理的子集,以TPC-H为代表,查询负载以批量更新和读密集型复杂查询为特征,包含了复杂的子查询嵌套结构.SPJGA-OLAP是本文提出的OLAP基本操作集,以OLAP中最基础的S:选择,P:投影,J:连接,G:分组,A:聚集为主,面向OLAP模型标准的切片、切块、上卷、下钻、旋转等操作,排除了子查询等复杂操作.SPJGA-OLAP是通用OLAP的核心功能子集.本文的研究以OLAP核心的SPJGA-OLAP操作集上的优化为中心,提出了面向星型模型特点的以维表为中心的分布式存储模型,将事实表对维表的数据依赖规范化为bitmap过滤器,通过分布式维表列存储缓存策略和分组编码谓词向量技术最小化OLAP处理时的网络传输代价.本文的贡献主要体现在以下几个方面:(1)将OLAP最核心的操作集SPJGA-OLAP分离出SQL集合,从而使优化的目标局限于具有最大并行处理潜质的标准多表连接分组聚集计算上,简化了大规模并行计算模型的复杂度;(2)提出了以维表为中心的海量数据分布式存储策略,以最低的负载均衡和数据更新同步代价服务于操作型BI需求;(3)将OLAP对应的SQL操作分解为过滤器、分组器、聚集器,连接谓词根据模式建立内部key-address映射,将复杂的SQL简化为简单的谓词表达式和属性输入参数,支持OLAP向非SQL查询Page3处理引擎的迁移和与各种SQL引擎的融合;(4)谓词向量技术将多表连接的数据依赖规范化为各个维表上的bitmap过滤器,最小化并行处理时数据依赖所产生的网络传输代价;(5)分布式缓存机制充分利用处理节点的内存容量来优化网络传输代价,减少同步更新代价.本文首先在第2节分析OLAP模型特征和相关研究的技术路线和成果;在第3节中给出SPJGA-OLAP模型的描述和实现技术;在第4节中设计并行SPJGA-OLAP代价模型和实验,并分析实验结果;最后给出论文的结论并讨论了进一步的工作.2OLAP模型分析和相关工作2.1TPC-H和SSB模型分析OLAP计算模型的复杂度取决于数据模型的特征.图2中显示了工业界和学术界普通采用的TPC-H和SSB标准.TPC-H是一个双事实表结构,图2TPC-H和SSB模型TPC-H在模式上形成雪花状结构,因此查询计划中连接操作较多.对于集中式处理模型,雪花状结构能够最小化存储代价,但对于并行计算模型,雪花状结构增加了大量的节点间数据复制或传输代价,LINEITEM表与OREDER表以及其它维表上的大量连接操作在执行代价和数据分布代价上都较大.因此并行计算环境下模式设计与集中式环境下的模式设计有所不同,考虑的首要问题是复制与传输代PARTSUPP和LINEITEM都是事实表,以组合键(PARTKEY,SUPPKEY)连接,OREDER表可以看作是LINEITEM事实表的辅助表,LINEITEM表以(L_ORDERKEY,L_LINENUMBER)为主键,因此OREDER表与LINEITEM表的连接通常采用索引连接.在TPC-H的22个标准测试查询中,查询计划树中的主要执行部分是事实表与多个维表连接的查询子树.考虑到并行计算环境下的数据分布,由于OREDER表与LINEITEM表是14的对应关系,通用的规则是将OREDER表与LINEITEM表按L_ORDERKEY进行Hash分布以减少并行连接时节点间的数据传输代价,提高节点的并行处理能力.但LINEITEM表无法同时满足与OREDER表和PARTSUPP表进行Hash分布的需求,TPC-H中只有Q9涉及LINEITEM表与PARTSUPP表的连接操作,而LINEITEM表与OREDER表的连接数量较多,因此数据分布策略只考虑OREDER与LINEITEM表.价而不是存储代价,即需要采用物化或非规范化思想将复杂的雪花状模型简化星型模型,将聚集计算属性尽量归并到事实表中,而维表只保留基本的选择和分组属性,将OLAP查询计划规范化为维表上的过滤→与事实表连接→分组聚集计算模式的简单操作.只有简单的存储模型和简单的计算模型才能最大化大规模并行处理的收益.图2中的SSB标准[1]是TPC-H标准的星型化Page4模型,目前被学术界所广泛采用.它将模式清晰地分解为四个维表和一个事实表,消除了TPC-H中LIENITEM与ORDER表的巨大连接代价,消除了雪花状模型带来的复杂查询执行计划,从而使其更加适合于大规模并行计算环境下的简单数据分布.两种模型的差异还体现在维表数据量上,以SF=1000(ScaleFactor=1000,对应1TB的测试数据集)为例,TPC-H中5个维表的数据总量为50188825KB,而SSB的4个维表数据总量为4062216KB,所占的比例分别为约5%和4.维表不同的数据量决定着在大规模并行计算环境下采用什么样的数据分布与数据传输策略以及各种策略的执行效率.我们将在后面的部分继续讨论针对模式特点进行的优化工作.2.2相关工作在关系操作中,连接操作依赖于两个不同的数据集,本文将维表定义为事实表连接依赖数据集(joindependencydataset),连接依赖数据集可以是整个维表、维表上的选择和投影子集、维表属性列或在维表上生成的Hash表.当处理节点获得连接依赖数据集后,各节点即可执行并行查询处理.因此并行数据库优化工作的核心问题是优化连接依赖数据集的复制和传输效率[2].例如,在TPC-H中可以将LINEITEM表与OREDER表按L_ORDERKEY和ORDERKEY进行Hash分布,保证两表在处理节点上的并行连接性能.当SSB中节点数量较少时,较小的维表可以采用全复制的方式复制到每个处理节点上以支持完全并行的查询处理,其代价是冗余复制的空间代价和维表更新时较高的同步代价.主流的数据库,如TeraData、Greenplum、ParAccel等一般采用on-the-fly传播的方式在并行查询执行过程中动态分布连接依赖数据集.当维表上的选择率较低且维表数据量较小时,网络传输的效率较高(Gpbs网络的有效传输效率高于单磁盘的数据传输效率).但OLAP负载与OLTP负载不同之处在于,OLTP查询中选择率一般较低,以点查询为主,而OLAP中查询选择率很高,以范围查询为主,在SSB测试查询的4个维表上,选择率最大值分布在1/5~6/7,因此网络传输的数据量依然较大.针对SSB特征的OLAP查询负载,很多研究[3-7]采用最简单的维表全复制策略,包括并行数据库、DB-cluster以及MapReduce模型上的研究,通过减化数据分布模型的方式简化并行计算模型,从而减少并行计算时高昂的网络传输代价.否则,由于事实表是多外键结构,与任何一个维表的连接操作都需要将两个表按特定的连接属性在节点中重新Hash分布后并行处理,网络传输代价非常高昂.文献[8]分析了当前OLAP的新趋势,其中操作型BI(operationalBI)的需求与传统OLAP中只读型数据处理的假设相冲突,因此传统OLAP中的物化策略、预处理策略、维表层次编码策略等失去了假设的基础,全复制策略也面临着巨大的同步更新代价.当前解决操作型BI的主要技术路线是双事实表,如SAP[8]和Vertica[9]都采用了双事实表技术来同时提供分析型和操作型处理.但从实际应用特点来看,典型的电子商务企业,如Amazon、淘宝、阿里巴巴等,更新不仅仅体现在不断追加的交易数据,而且包括不断更新的维表数据,而维表数据的变化直接影响OLAP的执行结果.双事实表技术只能解决数据迁移过程中的操作型问题.2.3海量OLAP时代模型设计原则海量OLAP意味着巨大的数据存储、访问、计算、传输和同步代价,而且需要具有良好的可扩展性支持.大规模并行计算的核心是简单的可并行计算模型和简单高效的数据分布模型.数据仓库的基本特征是按主题组织数据,也就是说一个数据仓库的数据模型在逻辑上就是一张表,简单的数据模型能够支持MapReduce这样的大规模可扩展并行计算框架.为了支持简单并行计算,我们需要维护数据模型的单一性,即以事实表为数据存储和并行处理的中心,从模式设计上缩减维表的规模,避免庞大的连接表或维表所产生的数据分布与并行连接代价.也就是说模式设计应该从TPC-H的以业务逻辑为中心向SSB的以分析逻辑为中心的设计原则转移.从并行计算设计上,OLAP处理集中在易于并行计算的SPJGA-OLAP子集,对于TPC-H中复杂的迭代子查询处理,我们的原则是将其中SPJGA操作子树并行化,查询树中其它难以并行化的部分交给SQL引擎来处理,即我们的研究重点集中在并行化收益最大的SPJGA操作部分,不做通用的并行SQL查询优化.3SPJGA-OLAP模型研究3.1存储模型本文的研究以SSB模型为基础.SSB是以事实表为中心的星型模型,我们提出了反转星型模型的并行存储模型,即以维表集中存储为中心,以事实表Page5水平分片为外围处理节点.反转星型模型的优点是简单,维表集中存储能够消除操作型BI所面临的实时更新所产生的数据复本同步代价,整个存储模型简化为以事实表为中心的分布式单表存储结构,易于数据分布和保持负载均衡,通过分布式缓存策略图3反转星型存储模型3.2OLAP查询功能分解SQL具有复杂的语法结构,在SPJGA-OLAP中,SQL语法可以简化为三类对象:过滤器、分组器和聚集器.图4显示了SSB对应的SQL命令和功能分解,group-by子句中的c_nation是查询的分组器,用于构建group-by操作的Hash表;where子句中的谓词一部分是连接谓词,与模式中事实表与维表之间的主外键引用参照完整性约束条件相对应,维表上的谓词表达式起到过滤器的作用,在SSB的SQL命令中只包括维表属性上的直接谓词;SELECT子句中的SUM(…)为聚集器,用于描述事实表度量字段上的聚集计算.因此,对于SPJGA-OLAP,复杂的SQL被转换为几个标准的输入参数接口,我们可以将SPJGA-OLAP定义为NoSQL模式的API,在算法设计和执行层面上独立于SQL引擎,避免基于传统的事务型查询处理引擎的设计在OLAP处理时的效率损利用各处理节点内存来加速连接和分组操作.图3所示的存储模型是并行计算框架内的逻辑存储模型,在实际应用中需要与具体的物理存储模型相结合,如在各个处理节点内采用列存储模型[10-11]、压缩等存储优化技术.失,同时也可以通过标准的SQL转换接口嵌入传统的SQL引擎中,作为SPJGA-OLAP类查询任务的并行处理加速器.通过查询功能的分解,维表只起到过滤器和提供分组器的作用.我们将过滤器优化为bitmap,即用一位来表示维表对应的记录是否满足该维表上所有谓词条件.一方面我们将事实表与维表的连接操作简化为事实表按外键属性与bitmap进行匹配,缩减了连接依赖数据集的大小,另一方面,通过维表主键与bitmap数组下标的直接映射(维表主键一般为自然序列),事实表与bitmap的连接简化为事实表根据外键值直接访问对应下标的bit位.3.3谓词向量并行DDTA-OLAP算法在反转星型存储模型和OLAP查询分解的基础上,并行集群上的OLAP处理被分解为4个阶段:(1)查询改写.将SQL查询改写为在每个维表上的谓词操作,为每个维表生成唯一的谓词向量(predicate-vector),谓词向量表示为与维表记录数量等长的bitmap,每一位置0或1,表示该维表记录是否满足维表上所有的谓词.(2)谓词向量广播.通过广播的模式将中心节点生成的谓词向量传播到各处理节点的内存缓冲区中,为并行OLAP处理做数据准备.采用广播方式一方面降低网络传输延迟,另一方面在节点规模扩大时保持网络传输延迟的稳定性.(3)并行OLAP处理.每个处理节点拥有自己独立的事实表数据分片,获得了维表谓词向量后即可独立地完成连接操作.图5显示了基于谓词向量Page6的连接操作过程.我们在前期研究[12]中提出了DDTA-JOIN算法来执行OLAP的多表连接操作,其基本原理是将维表主键顺序化使其与内存维属性列数组的下标直接映射,从而使事实表中的维属性外键值可以直接映射到内存维属性列数组的下标,图5基于谓词向量的OLAP连接操作图5表示一个标准的OLAP查询由SQL改写为对指定维表的谓词操作,并将查询结果存储为内存bitmap形式;在扫描事实表的过程中按事实表中对应的各个维属性外键值直接映射各个谓词向量bitmap指定的数据位,并根据多个位进行与操作的结果来判断当前记录是否满足连接条件,满足连接条件的记录再从内存维属性列中按照地址直接映射抽取分组属性值,传递给分组聚集器进行聚集计算.谓词向量优化技术最小化了连接依赖数据集,减少网络传输代价.(4)聚集结果集归并.在TPC-H和SSB标准中,聚集函数为可分布式聚集函数(SUM,COUNT)和代数可分布式聚集函数(AVERAGE),因此各个并行处理节点在各自事实表分片上的聚集计算结果具有可归并性,聚集计算可以下推到并行处理节点内执行.如果聚集函数是不可分布式聚集计算函数,如MEDIAN、RANK、PERCENTILE等,则必须将连接结果集汇集到中心节点后由中心节点完成最终的聚集计算任务.OLAP查询的结果集以分组聚集计算结果为展示形式,结果集的大小取决于各分组属性的集势(cardinality,即不重复值的数量),通常情况下远远小于连接的记录数量.如SSB的13个测试查询中,分组聚集结果集最多只有800条记录,远远小于查询中满足条件的连接记录数量.在并行处理规模较小时,我们采用集中式Hash归并算法来处理并行OLAP结果集,当并行处理规模较大时,我们采用迭代归并树算法来处理聚集归并问题(迭代归并树算法用于解决大规模集群中查从而将复杂的多表连接操作优化为简单的按事实表外键值进行内存按地址访问操作.我们将维表属性列进一步优化为整个维表对应一个bitmap,事实表通过直接访问内存谓词向量bitmap完成在维表上的过滤操作.询结果子集的聚集归并问题,与Reduce功能类似,但采用类似于B+-Tree的结构,优化归并过程中的网络连接数量和网络传输代价,在本文中不做过多讨论).通过对OLAP并行查询算法的优化,我们将各种SPJGA-OLAP负载规范化为统一的并行OLAP处理模型,实现one-size-fits-all模式的查询执行计划.3.4维属性列分布式缓存策略谓词向量将维表简化为bitmap,但OLAP查询中的分组属性和维表之间的谓词属性如果被压缩到谓词向量中,需要将对应属性值编码并替代谓词向量中的位编码,从而增加谓词向量的宽度.如图6所示,在customer维表上有选择谓词“c_region=‘AMERICA’”和分组属性c_nation,则维表需要提供两种类型的数据,一是谓词向量,用于标识哪条维表记录满足选择谓词条件,二是分组属性,将满足选择条件的分组属性提供给连接操作.在此我们考虑两种谓词向量策略:(1)谓词向量与分组属性组合编码策略.图6显示了该策略的原理.我们将customer维表上的选择改写为“SELECTCASEWHENc_region=‘AMERICA’c_nationELSE0FROMcustomer”,则可以得到以分组列属性值替代过滤结果的value-vector,如图6中间部分所示,然后将value-vector中的数据编码,根据变元数量分配适当的编码宽度,然后用编码代替value-vector中的原始值,形成紧凑的key-vector,如图6中最后一个Page7框图中的部分.通过这种编码向量的方式,每个处理节点可以如图5所示的操作步骤同时完成连接过滤和分组聚集操作,本地结果集以编码作为聚集结果的分类值,最终结果在中心节点进行全局归并后再实现将分组编码通过分组编码字典表的还原过程.图6谓词向量与分组属性组合编码(2)维属性列分布式缓存策略.当维表较小时且行数较少时,key-vector(如char-vector,可表示28个不同变元)数据量较小,网络传输代价差异不大.而且我们在实验中观测到,按下标地址直接访问bitmap-vector时比直接访问char-vector需要更多的CPU周期来解析位操作,因此当维表较小时,分组编码谓词向量(char-vector或短数据类型vector)能够获得较为理想的总体性能.当维表数量较大时,bitmap-vector与char-vector数量相差8倍以上,基于bitmap的谓词向量能够更少地消耗处理节点的内存并降低网络传输延迟.在这种策略下,需要将OLAP查询所需要的维表分组属性和维表间的谓词属性增量地缓存到处理节点的内存中.如图3所示,随着查询的执行,将维表分组属性通过广播方式缓存到各处理节点的内存中形成内存维属性列,支持基于接收的谓词向量执行的DDTA-JOIN操作.各处理节点的内存相当于分布式缓存,通过中心节点的广播同步更新,缓存的维属性列在缓冲空间不足时可以根据访问频率实行LRU替换算法,缓存的维属性列也可以物化到磁盘上.在SSB中,分组属性相对比较集中,在40个维表名大小/KB行数实际行数PART24420265SF200000200000000PART188523200000(1+log2SF)2193157表名大小/KBSUPPLIER1414902SF1000010000000SUPPLIER877957CUSTOMER24353654SF150000150000000CUSTOMER2995508NATION325DATE228REGION1维表总数据量50188825维表总行数谓词向量大小图71TBSSB数据集维表数据特征分析这种策略增加了维表上的谓词生成代价,增加了谓词向量的宽度和网络传输的数据量,但对处理节点的存储要求最低.缺点是编码谓词向量是查询的私有数据,分组属性在每次查询中都要先过滤再编码,没有重用性.属性中只涉及到7个属性,分组属性一般为低势集数据列,可以应用字典表轻量内存压缩算法[13]大幅度降低存储的空间代价,提高网络传输和分布式缓存的效率.维表列分布式缓存策略将连接操作数据依赖集分为两个部分,一是连接过滤器,二是分组器,连接过滤器的内容随查询内容的变化而各不相同,属于查询的私有数据集,而分组属性具有共享性,即多个查询可能共享一个较小的分组属性集.对于单个查询,在维表谓词操作基础上选择的分组属性子集编码具有最小的网络传输和分布式缓存代价,但在大量并发查询负载下,完整分组属性列的分布式缓存机制能够有效地降低系统整体的传输和缓存代价.在实验中我们具体分析分布式缓存在SSB中的存储代价.4并行SPJGA-OLAP性能分析与实验4.1网络传输与分布式存储代价分析我们用TPC-H和SSB的数据生成器生成1TB数据集(SF=1000),图7中统计了数据集的数据特征.我们看到在TPC-H中维表数据量约占5%,而SSB中维表数据比重为4;当采用谓词向量技术Page8时,TPC-H中维表谓词向量总数据量为43MB(维表每一行映射为一位,谓词向量大小(B)=总行数/8-bit),SSB的谓词向量总数据量为4MB.因此在两个测试标准中,并行连接操作数据依赖集的大小分别为43MB和4MB,在当前千兆网的支持下(Gbps网卡理论传输速度为125MB/s,实际测试大约在80MB/s的水平),谓词向量的网络传输代价为毫秒级(54ms和5ms),我们采用网络广播模式从中心节点向各个查询处理节点同步谓词向量,因此网络传输代价在网络规模扩展时也能够保持相对稳定.通过对数据的实际分析,我们获得了在大数据集环境中并行星型多表连接操作数据依赖集的大小和网络传输代价.处理节点上的多表连接操作被优化为按内存bitmap谓词向量直接访问对应位的操作,在确定事实表记录是否满足输出条件后在本地缓存的维属性分组列中抽取分组属性值完成其后的分组聚集操作.在SSB中,3个较大的维表上的选择率分别为PART:1/1,000~2/5,SUPPLIER:1/125~1/5,CUSTOMER:1/125~1/5.当对维表列采用轻量字典表压缩,采用16位压缩编码(能够表示216个变元)时,3个表的维属性列所占的存储空间分别为4MB,4MB和60MB,可以实现维属性列内存数组化,从而支持通过事实表维属性值向维属性列数组下标的直接映射和数据访问.在TPC-H中,维属性列的可能数据量将分别达到PART:400MB,SUPPLIER:20MB,CUSTOMER:300MB,分组属性的总数据量可能达到GB级.当维属性上的选择率较低时,可以对分组维属性按谓词向量先进行过滤操作,然后把满足条件的维属性值组织成内存Hash表,实现事实表记录向分组维属性列的Hash连接,但当并发查询较多时,大量的Hash表会占据较多的内存资源,因此在大并发负载和选择率波动较大的情况下,分组维属性列适合于内存直接访问策略.当数据集大小为100TB时,对于SSB数据集,谓词向量总量约为382MB,维表列大小分别是:PART:7MB,SUPPLIER:400MB,CUSTOMER:6GB,总大小为6.8GB,即100TB数据集上实现内存OLAP处理只需要6.8GB的内存,能够被大多数的计算硬件配置所满足,实现内存DDTA-OLAP查询处理.对于TPC-H数据集,谓词向量总量约为4.3GB,维表列大小分别是:PART:40GB,SUPPLIER:2GB,CUSTOMER:30GB,总大小为76.3GB,超出大多中低端服务器的硬件配置,TPC-H查询中一个维表上经常使用多个分组属性,进一步增加了内存开销.因此,SSB数据集比TPC-H更加适合于并行内存OLAP处理.对于100TB数据集的TPC-H查询,可以将谓词向量内存化,维表分组属性列采用磁盘存储模式,在完成事实表与谓词向量的DDTA-JOIN后与相应的维表分组列进行基于磁盘的连接操作.图8显示了两种磁盘维属性分组列的连接策略.左图对应选择率较大的情况,缓存中的事实表记录根据谓词向量找到满足全部过滤条件的记录后,根据谓词向量的数组下标(事实表维属性值)直接访问磁盘中列存储的分组维属性值(列存储以定长字段存储数据,可以将维属性键值直接映射为文件中的偏移地址来直接访问对应的记录),如果采用随机访问性能更好的SSD硬盘则将进一步提高磁盘按位置访问性能.右图对应选择率较小情况下的分组属性访问策略.先通过内存中的谓词向量对磁盘分组属性列进行过滤,将满足条件的分组属性存储到内存Hash表中,以维属性主键(列存储中数据的偏移地址)作为Hashkey,当事实表记录根据谓词向量找到满足全部过滤条件的输出记录后,根据事实表维属性外键值与对应的维属性分组Hash表进行Hash匹配,找到对应的分组属性值,组合成输出记录,传递给后面的分组聚集器完成其后的操作.通过对大数据集的分析,我们可以保证在较大的数据规模下(1TB或者100TB,1PB的SSB数据集需要约3.8GB的内存存储谓词向量,基本能够被当前硬件配置所满足),并行OLAP所需要的谓词向量和分组维属性列能够控制在几个GB的规模,与当前服务器通常采用几十个GB内存的硬件配置相比,能够保证算法的内存处理特性.4.2谓词向量并行DDTA-OLAP模型代价分析如图3所示,谓词向量并行DDTA-OLAP分为4个执行阶段:谓词向量创建,谓词向量广播,谓词向量DDTA-OLAP处理,查询结果集聚集归并.并行OLAP查询的代价表示为Ttotal=TGenPredVec+TBroaPredVec+TDDTA-OLAP+TResMerg,其中:TGenPredVec:表示生成谓词向量的代价,由维表行TBroaPredVec:由谓词向量大小决定;TDDTA-OLAP:由事实表分片数据量、分组维属性数量、谓词向量选择率等因素决定;TResMerg:由节点数量和结果集大小决定.我们用8台PC机进行并行DDTA-OLAP实数、谓词表达式CPU计算代价等因素决定;4.3实验平台和实验设计Page9图8对较大的磁盘维属性分组列的访问验测试,每台计算机的配置为Intel?CoreTM2DuoCPUE7500@2.93GHz,2GB内存,80GB硬盘,操作系统为Ubuntu2.6.35-22.我们使用OpenMPIv1.4.3作为并行处理框架并实现网络广播模式的数据发布.一个节点被用作中心节点,集中存储全部维表数据,7个节点作为并行查询处理节点,每个节点存储本地的事实表分片.我们通过C++开发了谓词向量DDTA-OLAP实验系统,在实验中我们使用的数据集大小为7×32GB=224GB,每个处理节点分配32GB的事实表水平分片,实验采用分段测试的方法,即在DDTA-OLAP并行SSB查询处理过程中设置计时器分别统计每个处理阶段的时间.谓词向量采用分组压缩编码谓词向量和bitmap谓词向量+分布式分组属性缓存两种策略,分别测试了两种策略下谓词向量的生成和广播时间.由于SSB测试的结果集较小且集势稳定,我们采用基于MPI的内存Hash归并算法进行全局聚集计算归并.4.4性能测试与分析图9显示了在并行DDTA-OLAP查询处理过程中各个处理阶段的时间代价.由于谓词向量较小,创建代价大约是几百毫秒,广播传输代价大约为几千毫秒,与基于磁盘I/O代价的本地DDTA-OLAP相比所占比例非常低.采用谓词向量技术时,默认为分组属性已增量式缓存于各处理节点,因此只需要传输bitmap结构的谓词向量,网络传输代价最小,创建谓词向量、广播谓词向量和查询结果集归并Page103个串行负载代价占并行查询处理总代价的比例为1.3%.分组压缩编码谓词向量技术不需要在处理节点缓存分组属性,谓词向量采用多位编码,同时代表过滤信息和分组语义,传输代价略有提高,但串行负载在并行查询处理负载中所占的比例为2%.图10中显示了串行负载比例最大的分组压缩编码谓词向量技术,其并行处理的加速比接近于理想的线性加速比性能.实验结果证明在磁盘OLAP中利用内存优化处理技术能够以极低的代价来保证复杂多表连接操作的线性处理性能和线性并行加速性能.同时,广播谓词向量的机制也保证了集群规模扩展时网络传输代价的稳定性.并行加速比是衡量算法可并行性的重要指标,在实际的系统中,不同的数据库产品采用不同的查询优化策略,如列存储、索引、压缩等技术,因此绝对查询处理时间上的对比往往并不公平.DDTA-OLAP是在查询处理层上进行的优化,不依赖于具体的底层存储技术,同样可以采用列存储来提高I/O性能,进而提高整体OLAP查询处理性能.在高可扩展的并行OLAP处理框架下,进一步提高OLAP性能的关键是提高处理节点的性能,如采用列存储、多核并行、高性能存储设备等软/硬件技术.图3所示的并行处理框架没有硬件和数据分布的限制,因此对于快速增长的数据具有良好的可扩展性,广播和谓词向量机制保持了系统规模扩大时查询处理框架在软件结构上的稳定性.传统的并行数据库性能依赖于数据分布策略、并行连接操作时的数据重分布或数据迁移策略、查询处理引擎性能等多种因素.为达到图10所示理想的线性加速比性能,一般采用对较小的维表全复制的策略,从而实现数据并行和查询处理并行,但在大规模并行计算集群中产生巨大的冗余存储代价和高昂的同步更新代价.分析型数据库集群(如Teradata、ParAccel等)一般使用特殊设计的高性能硬件来控制并行查询处理时的数据迁移代价,一方面造成硬件成本的迅速提高,另一方面由硬件的依赖性产生可扩展性瓶颈问题.本文提出的谓词向量广播技术将并行查询时的数据分布控制在非常小的常量时间范围之内,在保证存储效率和同步更新性能的同时与高代价的全复制并行查询处理机制保证相同的并行加速比性能.同时,我们在后续的研究中采用列存储技术存储巨大的事实表,能够进一步提高并行OLAP的处理性能,同时保证良好的并行加速比性能和进一步提高整体查询处理性能.5结束语大数据集上的OLAP处理必然由昂贵的高端服务器处理走向廉价的集群处理模式.集群并行OLAP处理主要的瓶颈在于OLAP算法的可并行性,并行OLAP处理时的datamotion(数据迁移)代价、本地处理时的I/O代价、多表连接操作时的查询处理代价.本文提出了one-size-fits-all模式的并行OLAP处理,利用现代计算机的大内存容量,将连接依赖数据集通过列存储、压缩、谓词向量等技术最小化其内存存储空间代价,使其能够满足内存直接访问,从而使不同的SPJGA类OLAP查询能够规范化到统一的DDTA-OLAP处理模型中,最小化OLAP查询处理时连接数据依赖数据集的datamotion(数据迁移)代价和基于多表连接的OLAP处理代价,使OLAP算法的并行度得以提升.本文研究的重点是大数据集在集群环境下的并行处理框架,充分利用大容量内存简化OLAP算法,并实现内存OLAP处理,从而使并行OLAP算法具有可扩展性.在此框架下,可以进一步结合列存储、内存查询处理、多核并行处理等技术进一步提高本地OLAP查询处理的性能,提高整体的并行OLAP查询处理性能.SPJGA类并行OLAP查询作为通用OLAP中最基础、代价最大的处理子任务,其良好的并行处理性能是提高通用OLAP并行处理性能的关键.SPJGA-OLAP可以作为其它并行数据库系统中的并行处理模块来加速并行数据库的多表连接查询子任务的性能.我们将在未来的工作中将SPJGA-OLAP嵌入到其它并行数据库系统中(如PostgreSQL-XC)来提高其并行OLAP处理性能.Page11
