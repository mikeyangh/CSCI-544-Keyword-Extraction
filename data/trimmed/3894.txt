Page1云数据管理系统能耗基准测试与分析宋杰1)李甜甜1)朱志良1)鲍玉斌2)于戈2)1)(东北大学软件学院沈阳110819)2)(东北大学信息科学与工程学院沈阳110819)摘要云数据管理系统是一种新兴的数据管理系统.为了研究云数据管理系统的能耗优化,实现“绿色计算”,首先要定义能耗的度量模型和基准测试方法,分析系统的能耗特点.目前云数据管理系统的基准测试主要集中在性能方面,对能耗方面的评估和优化工作很少;对测量仪器、测试手段、测试用例以及能耗基本规律的研究存在空白.文中提出了一种能耗的度量模型和数学表达;定义了一组数据装载、查询和分析用例来测试云数据管理系统的能耗;设计了系统能耗的测量方法;分析了若干云数据管理系统在执行数据装载、读取、查询、聚集和连接等操作时的能耗特征,提出了通过降低“等待能耗”而进行云数据管理系统的能耗优化.大量实验数据证明,尽管云计算被认为是一种绿色计算,但文中测试的云数据管理系统在能耗方面差异较大,需要对部分系统进行进一步的优化.关键词云数据管理系统;能耗;基准测试;MapReduce;大数据;云计算;绿色计算1引言云数据库(CloudbasedDatabase)[1]是一种基于云技术的数据库系统,是云计算中极具潜力的数据存储和管理模式.而云数据管理系统(CloudDataManagementSystem)通过营造“面向服务的数据管理和提供(ServiceOrientedDataManagingandProvisioning)”环境,蕴含着前所未有的数据交付能力.云数据管理系统成本低廉,有着良好的并行性和伸缩性,支持海量数据管理和大规模集群系统,支持服务化的访问和按需使用、按需付费[2].云数据管理系统是一种新兴的数据管理系统.目前已经存在很多云数据管理系统,其中开源系统包括HBase、Cassandra、Hive、MongoDB、HadoopDB、GridSQL、Hypertable、CouchDB、Dynomite、Voldemort、Elastras等,商业系统则有亚马逊的SimpleDB、微软的AzureSQL、谷歌的BigTable、雅虎的PNUTS和EMC的GreenPlum.上述云数据管理系统实现技术各不相同,但较传统数据管理系统最显著的优点是节省开销.HBase宣传自身适用于那些“一年在Oracle数据库上的花费超过一个小国家国民生产总值的企业”①.云数据管理系统确实能在保证服务质量的同时有效地降低成本.从数据服务角度,它提供“数据即服务”(DataasaService)的使用模型和“按需使用,按需付费”的付费方式,用户不必购买昂贵的软硬件资源;从数据管理角度,虚拟化技术和分布式的数据管理方式,有利于合理分布数据和调度任务,使负载更为集中(WorkloadConcentration),减少空闲的IT资源.因此,可以认为云数据管理系统是一种追求节能,更为“绿色”的数据管理模式.尽管云数据管理被认为是一种绿色的数据管理模式,但其本身并没有提供成熟的解决方案来评价和降低能耗,仍需要能耗优化方法来切实地实现“绿色计算”.研究云数据管理系统的能耗优化,首先是要定义能耗的度量模型和基准测试方法,分析各种云数据管理系统的能耗特点.就我们所知,目前云数据管理系统的基准测试主要集中在性能方面,对能耗方面的评估和优化工作很少;对测量仪器、测试手段、测试用例以及能耗基本规律的研究存在空白.为更好地降低能耗,本文将从以下几个方面开展研究:首先,本文设计了一组用于测试云数据管理系统能耗的数据操作和分析用例,并定义了能耗度量模型.本文未采用经典的数据库测试用例TPC-H②,这是由于云数据管理系统的特殊性和不成熟性,TPC-H的诸多查询过于复杂而难以支持.本文沿用文献[3]中的测试用例,该用例被广泛地运用在基于MapReduce的数据分析的基准测试上[4-5],本研究在云数据管理系统上实现该用例并略加改进.其次,本文提出了一种降低能耗的新思路.传统的集群系统能耗分析方法主要评估“空闲能耗”,即关闭空闲的计算机以减少空闲能耗,“空闲能耗”越多,整体能耗提升空间越大.而本文针对云数据管理系统的特点,提出减少“等待能耗”以节能.节点因等待作业调度、I/O操作或其它节点的运算结果而处于暂时的“空闲”,等待时间中产生的能耗为“等待能耗”.与“空闲能耗”不同,某节点产生“等待能耗”时,该节点任务尚未执行结束,无法通过关闭该节点来节能,但我们可以通过减少节点等待而降低“等待能耗”.“等待能耗”越少,系统能耗越优化.最后,本文对主流的云数据管理系统HBase、Cassandra、Hive、MongoDB、HadoopDB和GridSQL在不同运行条件下执行数据装载、读取、查询、聚集和连接等操作时的基准能耗进行测量[6],总结其能耗规律.上述数据管理系统的实现方式各不相同,但都广泛的运用在云计算的各个应用中.实验证明,尽管云计算被认为是一种绿色计算,但上述云数据管理系统在能耗方面差异很大,其能耗有很大的优化空间.对云数据管理系统进行能耗基准测试有着重要的意义:(1)分析各种云数据管理系统的能耗特征,总结影响能耗的因素,提出能耗优化的基本方法;(2)通过测试和比较各云数据管理系统之间的能耗,可指导系统选择和低能耗系统的设计和研发;(3)通过总结云数据管理系统能耗规律,可优化基于该系统的低能耗应用软件;(4)使能耗作为一种服务质量要素,云数据服务提供商可以准确计算能源成本,服务使用者也准确地按需使用,按需付费.本文第2节简要介绍研究背景,包括云数据管理系统和MapReduce;第3节介绍基准测试方面的相关工作;第4节定义本文提出的基准测试用例和能耗度量度模型;第5节定义了本文提出的“等待能耗”,介绍了能耗的分析方法、测量方法和测试环境;第6节分析和比较了HBase、Cassandra、Hive、MongoDB、HadoopDB和GridSQL在相同测试用例下的能耗,提出能耗优化方法;最后,在第7节对本文的研究进行总结并提出进一步的工作.①②Page32研究背景目前学术界尚没有给出云数据库的确切的定义.通常认为云数据库是基于CAP理论[7-8]、BASE理论[9]和数据库Sharding①②技术而发展起来的新型数据库,其与现有的关系型数据库存在较大差别,大部分云数据库采用非关系型的、更为松散的数据模型[10].云数据库的出现是为了解决关系数据库伸缩性差、读写慢、成本高、支撑容量有限等问题;其设计针对“快速响应请求”、“支撑海量的数据和流量”、“大规模集群”、“低运营成本”等需求[11].大部分云数据管理系统(如HBase、Hive和Cassandra)采用键值(Key-Value)数据模型存储和管理数据,也有的沿用传统的关系模型(如Hadoop-DB和GridSQL).键值数据模型是一种稀疏的、分布的、持久化的、多维有序映射表(Map)[12],其特点是简单而灵活.还有一些云数据管理系统基于文档数据模型(JSON或XML文档等),如MongoDB、BaseX等.云数据管理系统大多通过Shared-Nothing体系结构和MapReduce编程模型实现大规模的分布式计算.MapReduce[13]是用于可扩展并行数据处理的编程模型,该模型有多种实现,如谷歌MapRe-duce、ApacheHadoop③、Map-Reduce-Merge[14]、多核和多处理器系统的MapReduce[15]等.MapReduce被广泛地运用于海量数据的分析和处理中.一个MapReduce程序仅包含两个函数,即Map和Reduce,它们定义了用户处理“键值对数据”的逻辑.程序的输入数据集位于分布式文件系统或数据库系统中,采用“迁移运算而非迁移数据”的方式,MapReduce程序被下载到每个数据节点并执行,输出结果仍保存在分布式文件系统或数据库系统中.Map和Reduce函数的输入和输出都是用户定义格式的键值对形式,其中Map函数输入〈KeyInM,ValueIn函数的输入为〈KeyInR〉,其中KeyOutValueOutR是保存ValueOutValueInMapReduce将作业(Job)分解为任务(Task)并在每个节点上并行地执行.MapReduce程序首先将输入数据分割成M份(通常M大于节点个数),作为M个Map实例的输入.Map函数从一份输入中读取每一条记录,对其进行必要的过滤和转换,然后输出〈KeyOut果被Hash函数按KeyOut每个组被写入到处理节点的本地磁盘中.所有Map函数终止时,M个Map实例把M份输入文件映射成M×R个中间文件.由于所有Map函数的分割函数都一样,因此相同键Hash值(设为j)的Map函数输出结果被存在文件Fij(1iM,1jR)中.MapReduce的第二阶段执行R个Reduce实例,R通常是节点的数量.每个Reduce实例Rj输入文件为Fij(1iM).这些文件从各个节点通过网络传输汇聚到执行节点.Reduce函数输入Fij的键M和对应的一组ValueOutKeyOut数据库系统输出若干〈KeyOut录.所有的从Map阶段产生的具有相同Hash值的M,ValueOut〈KeyOut例处理.输入数据集以集合的形式存在于分布式文件或数据库系统中的一个或多个分区中.MapReduce的调度器决定执行多少个Map实例,以及如何把它们分配给可用的节点;同样,调度器还必须决定运行Reduce实例的数量和执行位置(一些MapReduce实现,如Hadoop早期版本让用户指定Map和Reduce的数目).MapReduce中央控制器协调每个节点上的运算,一旦最终结果以新数据的形式写入分布式文件或数据库系统中,MapRedcue程序执行完毕.本文将具体考察五种云数据管理系统在能耗上的差异.其中,HBase是一个分布式的、面向列(Column-Oriented)的开源数据库,构建于Hadoop分布式文件系统(HDFS)之上,支持MapReduce编程模型,用于管理需随机访问、实时读写的海量数据;Hive基于分布式文件系统,而HadoopDB基于关系型数据库PostgreSQL;两者都支持SQL和MapReduce编程模型,使SQL按MapReduce的方式执行,兼顾效率和水平伸缩性(Scaleout);Cassandra是一个环状的、面向列的云数据管理系统,基于P2P技术的去中心化(Decentralized)存储实现高并行性和伸缩性;MongoDB是一个分布式的文档数据库,通过索引机制支持高效的查询;GridSQL是一种并行化关系型数据管理系统,是EnterpriseDB的开源实现,基于开源数据库Post-greSQL,采取非共享(SharedNothing)的体系结构并行地处理数据请求,是高性能并行数据库产品.①②③Page43相关工作基准测试是指“运行一系列程序或操作以获得待测对象性能”的过程.应用程序的基准测试可分为三类[16]:基于向量的(Vector-based)、基于路径的(Trace-based)和混合的(Hybrid)方法.基于向量的方法把系统的性能抽象为多个向量,对每个向量进行基准测试,最后得出整体系统性能,如文献[16]中提及的使用基于向量的基准测试方法测试Java虚拟机性能;基于路径的基准测试方法则着重考虑测试用例而非待测系统,首先定义可用于进行基准测试的测试用例(路径),然后定义影响用例负载的因素,如数据量、并发量等,以及生成不同负载的用例的方法.TPC测试就是基于路径的基准测试.混合方法则结合了两种方法.本研究提出的云数据管理系统能耗基准测试是基于路径的基准测试.到目前为止,我们没有发现对云数据库或其它数据库的能耗基准测试的报道,大部分基准测试针对的是数据库性能.其中TPC-H[3]是一个决策支持系统的基准测试集,它由行业相关的数据组成,包括8张表和22个面向业务的查询.TPC-H的测试结果还可以通过QphH@Size和Price/QphH来衡量,许多数据库集群都使用两者比较性能[3].TPC-H使用了海量的数据集和极其复杂的查询,这些查询对于云数据管理系统来说过于复杂,难以实现(如目前HBase和Cassandra尚不提供显式的表连接和分组聚集操作).有一些基准测试则是定义在其它基准测试基础上.如STMBench7[17]是一个用来评估软件事务性内存(SoftwareTransactionalMemory,STM)的基准,STMBench7是以面向对象数据库系统的基准测试———OO7基准测试[18]为基础.由于云数据管理系统的特殊性,我们无法参照现有关系数据库的基准测试集,但我们参考了部分现有的MapReduce基准测试研究[3-5].目前存在一些针对云数据管理系统性能的基准测试.文献[12]评估了不支持结构化查询语言的数据管理系统的性能,如谷歌BigTable.雅虎公司研发的YCSB(YahooCloudServingBenchmark)[19]框架提供一组由数据插入、读取、更新和扫描等云数据管理服务而组合的测试用例,可以用来衡量云数据管理系统的性能,YCSB使用的测试用例仅涉及最基本的数据操作,对于数据管理操作来说过于简单.文献[3]在HadoopHDFS上基于MapReduce实现了传统数据库的增加、查询、聚合和连接操作,并且比较了这些操作的Hadoop实现和并行数据库实现的性能.此外,云数据管理系统多基于MapReduce机制并行地执行数据查询作业,虽然MapReduce编程模型已得到广泛运用,对MapReduce的性能衡量和基准测试工作仍较为有限.除前文提到的云数据管理系统基准测试中直接或间接地测试了MapReduce性能以外,一些研究设计了如计数、排序、Grep、矩阵乘法和聚合算法等面向CPU密集型、I/O密集型、(Map阶段)CPU和(Reduce阶段)I/O密集型、交互型的MapReduce用例[13,15,20].本研究更多地延续了文献[3]的设计,但本文提出的基准测试与上述工作有明显差别.首先,我们提出的是能耗基准测试而非性能基准测试;其次,我们并非针对MapReduce模型或文件系统,而是数据管理系统;同时为更好地衡量能耗,我们对部分测试用例进行了扩展,对用例的云数据管理系统实现进行了优化.本文的目标不是研究能耗的优化方法,而是研究能耗的基准测试方法,但也为能耗优化方法提供参考.文献[21]从网络全局角度研究网络的能耗模型和算法,对不同的数据传递模式的能耗进行建模并研究其能耗优化方法.文献[22]从能耗测量、能耗建模、能耗管理实现机制、能耗管理优化算法四个方面对虚拟化云计算平台能耗管理的最新研究成果进行了介绍,从中可以看出很多现有研究都是从“负载集中”和“开关算法”的角度来考虑能耗优化[23-24],因此多关注“空闲能耗”,让空闲的节点休眠[25]以节省能耗.而本文在实验数据的基础上,分析了云数据管理系统的能耗规律,并提出通过“等待能耗”来评价系统的能耗特性,指出即使节点没有完全空闲,也会因等待调度、I/O操作等其它资源而产生等待能耗,降低整体能效.“等待能耗”过多是造成部分云数据管理系统能耗高的原因.从研究对象角度部分现有研究分析了MapReduce执行环境的各种静态参数对其能耗的影响[26-28],但没有针对数据库操作,如查询、聚合和连接运算加以分析.就云数据管理系统而言,尚未见其软件层面能耗分析和比较之类的研究报道.4能耗基准测试本节将介绍基准测试用例的数据模型、用例的定义和基于MapReduce的实现算法.4.1数据模型本文设计的基准测试用例基于三个简单的表结构.Grep查询作业来自文献[13],是MapReduce程序中最具代表性的应用例子.在Grep作业中,系统Page5必须从一个数据集合中扫描出一个包含三个字母的样式,数据集合由大量长度为100字节的记录组成.每条记录中前10个字节是一个唯一的标识,后面90字节是随机的值.样式匹配查找在后90字节中进行,平均每10000条记录中才能查找到一次.因此本文设计了Grep表来支持这种查询,表结构如图1所示.本文还设计了另外两个数据库表结构(图1),用来保存一个Web网站的日志信息,其中UserVisits表的destURL字段参照了Rankings表的pageURL字段,由于大多数云数据库不支持外键连接,因此我们没有建立表Rankings和UserVisits之间的连接.我们采用随机算法生成数据(服从平均分布),表1列出了上述数据库表中每个字段的数据生成规则.其中Grep和UserVisits的数据量(条)为1亿(约30GB);Rankings表的数据量是UserVisits表的十分之一;数据复制因子为3.表字段生成规则(随机数服从平均分布)GrepfieldRankingsUserVisits4.2测试用例本小节描述基准测试用例,基准测试用例包括数据的装载、查询和分析用例,每个用例都有两个参数来限定其执行环境和负载,即节点数量和数据量.本节还介绍用例在云数据管理系统上的实现方法.部分基于Key-Value数据模型的云数据管理系统尚不支持标准SQL,也不显式地支持分组、聚集和表连接等较为复杂的关系操作,因此需要通过编程实现.本节将着重描述测试用例的HBase实现方式以及MapReduce算法,其它非关系数据管理系统与此类似.4.2.1数据装载(Loading)数据装载测试用例定义为“以不同线程数目并行地对Grep、Rankings和UserVisits表加载数据”.HBase装载数据的方式有3种:(1)直接使用HBase提供的API逐条数据插入;(2)利用MapReduce程序,从现有分布式文件系统(如HDFS)中的文件导入至HBase;(3)把数据写入HFile格式文件,然后由HBase自动从HFile中加载数据.本用例由于测试数据随机生成,因此无法利用MapReduce从文件系统中加载数据.文献[3]在研究HDFS数据加载时采用先生成数据文件然后加载至HDFS的方式,文献[3]的性能数据假设数据文件已经存在,忽略了生成文件的时间.本研究中,我们实现了三种数据加载方式,发现第一种效率最高,因为后两种方法都需要生成临时的数据文件,I/O读写将是一个很大的开销,因此我们采用直接装载的方式.4.2.2Grep运算(Grep)模糊查询用例执行如下SQL描述的功能:SELECTFROMGrepWHEREfieldLIKE‘%XYZ%’为满足上述查询的命中率为万分之一,我们定义Grep表的field字段每条记录的每个字符都从95个不同的字符(ASCII码32~126,键盘字符)中随机选择,field字段长度为90字符,因此每条记录包含88个长度为3的子串.长度为3的字符串是“XYZ”(“XYZ”可以用任意三个键盘字符替换)的概率为953分之一,因此上述查询的命中率约为953/88分之一,基本满足要求.此外,基准测试的查询条件需要动态变化,每个请求的查询条件三个字符也是从ASCII码32~126中随机选取的,避免了多次测试或并发请求环境下同一查询反复执行,且保证了查询命中率基本一致.HBase能够很好地完成基于MapReduce的查Page6询功能.每个节点的Map实例遍历数据集,对每条数据进行字符串匹配检测,输出满足条件的结果.由于不存在进一步运算,因此不需要Reduce函数,Map方法的输出即为程序最终的输出.4.2.3查询运算(Selection)范围查询用例执行如下SQL描述的功能:SELECTpageURLFROMRankingsWHEREpageRank>10范围查询是一个轻量级的过滤器,它从Rankings表中找出pageRank高于用户定义的阈值的pageURL.测试用例同样使用动态的查询条件,将其设为9.5~10.5之间的一个随机小数,由于pageRank是一个0~100之间的一个随机数,因此查询命中率近似为90%.4.2.4聚合运算(Aggregation)聚合运算用例执行如下SQL描述的功能:SELECTsourceIP,SUM(adRevenue)FROMUserVisitsGROUPBYsourceIP;聚合运算从UserVisits表中计算出每个sourceIP所产生的adRevenue(广告收入)之和.设计该测试用例是为了度量针对单个数据表的并行分析能耗.由于有65536种sourceIP(参见表1),查询返回65536条分组记录.HBase提供的API不能自动地支持分组和求和操作.我们通过一个MapReduce程序完成该用例.Map方法输入每条UserVisits记录,输出以sourceIP为键,adRevenue为值的中间结果.Reduce方法将每个sourceIP关联的adRevenue累加在一起,并输出以sourceIP为键,sumAdRevenue为值的结果.算法1.聚合运算测试用例的MapReduce实现.1.Map(KeyIn2.Write(row.sourceIPasKeyOut3.}4.Reduce(KeyIn5.Doublesum=0.0;6.FOREACHadRevenueINadRevenueArray;7.sum+=adRevenue;8.ENDFOR9.Write(ipasKeyOut10.}4.2.5连接运算(Join)连接运算用例是数据库常用操作的集合,包括一次范围查询、一次表连接和一次分组聚集.测试用例执行如下SQL描述的功能:SELECTINTOResultssourceIP,AVG(pageRank)asvgPageRank,SUM(adRevenue)astotalRevenueFROMRankingsASR,UserVisitsASUVWHERER.pageURL=UV.destURLANDUV.visitDateBETWEENDate(‘2005-01-01’)ANDDate(‘2006-01-01’)GROUPBYUV.sourceIP;Join作业包含了四个子任务,在两个数据集上执行复杂的计算.第一个子任务在UserVisits表中查找特定时间范围内的sourceIP.我们在实验中使用了随机的一年间隔作为查询范围,在UserVisits表中visitDate字段为2000年~2010年间的随机时间,因此,选中的数据占UserVisits总数据量的10%;第二个子任务连接UserVisits和Rankings表;第三个子任务按照sourceIP对Rankings表的pageRank分组求平均,对UserVisits的adRevenue字段分组求和;最后一个子任务把按sourceIP的统计结果写入Results表.上述任务最突出的是表连接操作,HBase没有提供连接查询的API,也不支持在稀疏表上创建外键,我们将用MapReduce实现这一过程.使用MapReduce完成表连接的方法有很多种,如MapSideJoin,ReduceSideJoin,SemiJoin,DistributedHashJoin和BloomFilterbasedJoin等[29-32].其中ReduceSideJoin是一种最简单的连接方式,Map函数同时读取主表和从表,并为每条数据打一个标签(Tag)以区别该数据的来源;Reduce函数获取Key相同的来自主表和从表文件的数据并进行连接(笛卡尔乘积).文献[3]在HDFS上使用三次MapReduce完成该用例,相比而言,我们在HBase上优化为两次MapReduce.这两个阶段分别是连接阶段和聚集阶段,前一阶段的Reduce函数输出是后一阶段Map函数的输入.其中聚集阶段同聚合运算测试用例(算法1)类似,仅需要考虑区别输入数据哪些是pageRank列数据,哪些是adRevenue列数据,对不同的列聚集算法不同(pageRank为求平均数,adRevenue为求和),且Reduce函数输出到Results表中.我们着重阐述连接阶段的MapReduce操作.具体算法如算法2所述.算法2.连接运算测试用例的连接阶段MapReduce实现.1.Map(KeyIn2.IF(rowisRanking)3.Write(row.pageURLasKeyOutPage74.ENDIF5.IF(rowisUserVisits)6.IF(row.visitDataebetweenbegin_dateand7.pair=newPair(row.sourceIP,8.Write(row.destURLasKeyOut9.ENDIF10.ENDIF11.}12.Reduce(KeyIn13.SetipSet;14.SetpageRankSet;15.FOREACHelementINarrays16.IF(elementisUserVisits)17.ipSet.add(element.sourceIP);18.Write(element.sourceIPasKeyOut19.ENDIF20.ELSEIF(elementisRanking)21.pageRankSet.add(elementaspageRank)22.ENDELSE23.ENDFOR24.FOREACHipINipSet25.FOREACHpageRankINpageRankSet26.Write(ipasKeyOut27.ENDFOR28.ENDFOR29.}连接阶段的Map函数输入Rankings表的所有数据和UserVisits表的所有数据,Map函数对每一条数据进行判断,如果是UserVisits记录,则对其按visitDate进行过滤;如果在给定日期范围内,则输出以destURL为键,sourceIP和adRevenue组成的二元组为值的中间结果.如果是Rankings记录,则直接输出以pageURL为键,以pageRank为值的中间结果.连接阶段的Reduce函数输入以URL为键,由(sourceIP,adRevenue)二元组或pageRank组成的集合为值的数据.Reduce函数对其进行约简,输出以sourceIP为键,adRevenue或pageRank为值的结果.4.3能耗模型对云数据管理系统的能耗进行基准测试和分析比较,首先要定义能耗的度量模型.能耗的度量对象归根结底包括三部分:计算机、网络和空调等辅助设备.关于网络的能耗模型,已有很多相关研究,如文献[33]提出的一种基于随机模型的绿色评价框架.本文仅考虑计算机能耗,网络设备和空调等辅助设备的能耗本文暂不涉及.基于前文定义的基准测试用例,我们采用基准能耗来衡量云数据管理系统针对上述用例的能耗.每次数据库查询作业可以看作一个事务(Transaction),基准能耗是特定环境下云数据管理系统完成一个事务时的能耗.定义1.基准能耗.基准能耗是云数据管理系统在特定执行环境下(如节点数量、数据量、数据复制份数等),平均完成某个基准测试用例消耗的能量.云数据管理系统中的每个计算机节点都有额定的功率,但文献[34]证实了计算机功率是动态变化的,与计算机的繁忙程度有关,当计算机空闲时,功率较低,反之亦然.因此不能简单地使用用例执行时间与额定功率的乘积来计算基准能耗.对于云数据管理系统,设存在N个节点,每个记为ci(1iN),对于ci节点功率为pi(t)(单位为瓦特),在T时间内完成某同一基准测试用例的M次请求,则基准能耗的计算方法为5能耗的分析和测量本节介绍能耗分析方法、测量方法和实验环境.5.1能耗分析方法基准能耗能够清晰地度量云数据管理系统的能耗,但粒度过大,过于笼统.为确定云数据管理系统的能耗规律和能耗优化的目标,我们对基准能耗做进一步的分析.目前能耗优化主要有两种思路,一是开发更加节能的硬件设备,二是关闭空闲设备.而本文则结合云数据管理系统的特点,提出能耗优化的第三个思路:减少“等待能耗”.定义2.等待能耗.在集群环境中,计算机节点(或计算机某组件)因等待其它资源而处于“被动空闲”的时间段内消耗的能量称为等待能耗.例如,当集群中某个节点在等其它节点的运算结果时,节点产生等待能耗,该节点并非真正意义上空闲,通常无法关闭该节点以节能,因此我们应该尽量减少节点对资源等的等待,以减少等待能耗.同理,对于一个计算机内部的各个组成部分,如CPU运算也会因等待I/O操作而阻塞,产生等待能耗.等待能耗的定义和分析符合云数据管理系统的Page8特点.首先,云数据管理系统是一种分布式的集群系统,作业在各个节点间调度执行,若作业调度策略不合理,会导致部分节点等待作业而空闲;其次,MapReduce模型把作业分解成任务(如Map任务和Reduce任务)并部署到每个节点并行的执行,由于数据分布特点和任务执行的特点,很可能会因任务结束时间不同步而造成部分节点等待(如Reduce任务需要等待所有Map任务执行结束);最后,云数据管理系统执行的大部分是I/O密集型运算,对于每个节点,CPU的执行速度要远大于本地I/O和网络I/O的读写速度,CPU易因等待I/O操作而产生等待能耗.计算机的能耗主要包括CPU能耗、硬盘能耗、内存能耗、显卡能耗、网卡能耗、主板能耗以及其它附属设备的能耗(对于云数据管理系统的每个节点,我们不考虑显示器,键盘鼠标等人机接口设备的能耗),其中以CPU能耗为主,占节点能耗的70%.此外,节点在“等待调度”、“等待其它节点”或“等待I/O操作”时其CPU也处于低使用率的等待状态.因此,考虑CPU等待能耗等同于考虑节点等待能耗.我们定义如表2所述的若干特征属性来观察各系统在各测试用例执行时的CPU等待能耗特征.其中CPU使用率、磁盘I/O读写速率、内存的使用率以及网络通信量分别代表了CPU、硬盘、内存以及网卡繁忙程度.特征属性单位CbusyDr/sByte硬盘每秒读入数据量Dw/sByte硬盘每秒写入数据量MusedNin/sByte网卡每秒接收的数据量Nout/sByte网卡每秒发送的数据量5.2能耗测量方法测量E(T)的值的方法有很多种.所有计算机都有额定功率,但实际功率却是动态变化的,因此很难找到pi(t)的数学表达,并按照式(1)计算能耗E(T).我们使用功率计(PowerMeter)每隔Δt时间对节点进行一次实时功率的采集,根据积分的定义:E(T)=∑N当Δt足够小时,式(2)的值即为E(T)的值.但大部分计算机设备或操作系统都未提供实时功率的测量接口,测量pi(tj)存在一定难度,需为云内每个计算机节点安装功率计,并且控制他们之间的通信(时钟同步,数据汇总).我们选用的功率测量设备可以通过USB数据线与远程计算机通信,每隔1.5s~3s(受限于采集设备)采集一次实时功率,采集间隔不等.此外,除按式(2)的方式计算,还可以通过电力监测仪直接测量T时间经过电缆线的电量即为整个云计算系统的整体能耗值,但此时要考虑仪器的最大功率测量范围.主流操作系统都提供了对系统资源进行监控的接口,因此特征属性的测量均不需要特殊的仪器,也不依赖于特定的云数据管理系统种类.我们在云系统中每个节点上部署监控代理(软件),设置代理在T时间内每隔Δt时间对节点进行一次特征属性值的采样,云计算中分布式的编程环境可以很容易地同步这些监控代理并汇总数据.5.3实验环境我们使用12台高档微机搭建了一个集群环境.HBase、Hive、HadoopDB、MongoDB和GridSQL都为主从结构,其中1台节点作为主节点,11台节点作为从节点.而Cassandra为环状结构,12台节点均为数据节点.实验环境细节如表3所述.节点操作系统CentOS5.6,Linux2.6.18内核云数据管理系统编程环境JavaSE6功率计特征属性频率Δt1s采集1次数据(Δt=1s)测量单位并发性数据量Page96基准能耗比较与分析我们逐一考察了HBase、Cassandra、Hive、MongoDB、HadoopDB和GridSQL的能耗特性,表4对比了上述几个云数据管理系统的特点.名称数据存储数据模型编程模型支持SQLHBase分布式文件系统Key-ValueMapReduce否Cassandra本地文件系统Key-ValueMapReduce否Hive分布式文件系统关系型MapReduce是MongoDB本地文件系统Key-ValueMapReduce否HadoopDB关系型数据库关系型MapReduce是GridSQL关系型数据库关系型并行算法是从实现技术考虑,HBase和Cassandra较为相似,Hive和MongoDB较为相似,HadoopDB和GridSQL较为相似.6.1基准能耗比较本节比较HBase、Cassandra、Hive、MongoDB、HadoopDB和GridSQL的基准能耗.为了表述简单,Loading用例采用“20并发线程下对Grep表装载100万记录”.HadoopDB的数据装载较为复杂,需要手工完成,而Hive的数据装载实为把数据文件纳入分布式文件系统HDFS,因此我们没有比较这两种系统的装载用例能耗.每种系统执行不同基准测试用例的能耗和效率对比如图2(对数坐标轴)所示.图2云数据管理系统在不同测试用例下能耗和执行时间的比较图2(a)表明HBase和Cassandra的装载基准能耗要明显低于GridSQL,在并发装载的情况下优势更为明显.GridSQL是关系型数据库,数据类型丰富,数据约束多,需要对数据进行大量约束检测和格式转换,HBase和Cassandra则有着简单的表结构和单一的数据格式,对每条数据标记时间戳后写入文件中,后两者装载数据更为简单,需要的运算更少,能耗更低.此外HBase和Cassandra数据存在复制而GridSQL没有复制机制,因此,HBase和Cassandra写入的数据量大于GridSQL,前两者实际数据写入速度更快.MongoDB采用两段式数据装载,第一阶段将数据写入节点,这个阶段的能耗很低且效率很高;第二阶段是节点间点自动分发并平衡数据的过程,这个阶段是在后台运行并持续较长的时间,其开始和结束时间无法测量.图2数据为第一阶段的测量值,因此能耗很低,效率很高.对于Grep和Selection两种查询用例,GridSQL耗能很低,MongoDB、HadoopDB和Hive次之,而HBase和Cassandra能耗很高;对于Aggregation和Join两种分析型用例,HBase和Cassandra的能耗要远远高于Hive、HadoopDB和GridSQL.对于Aggregation这种单表分组聚合操作,大部分系统的基准能耗和查询用例的能耗规律一致.MongoDB的Aggregation用例性能要优于Join用例,这是因为MongoDB的Group关键字不支持大数据量运算,因此Aggregation通过MapRe-duce实现,其“内存映射机制”不能很好的发挥,且需要对全部数据进行读取,磁盘I/O增加.对于多表连接操作,由于各系统的实现方法不同,Hadoop-DB的能耗最低,性能最优.综合上述数据,基于关系型数据库和MapRedcue并行编程模型的HadoopDB在本测试环境中能耗最低.我们可以通过云数据管理系统的性能差异来解释上述现象.性能直接决定用例的执行时间,一般情况,执行时间越长则能耗自然越高,然而也存在特例,如实验用计算机的有功功率在50W~100W之间浮动,若该计算机以最小功率运行2s和以最大功率运行1s,能耗相同.因此,我们不能简单地认为执行时间长的用例能耗一定高.但在本例中,执行时间对能耗起到决定因素.分析数据可发现,各个系统“平均功率”(能耗除以执行时间)基本一致,说明功率的浮动被平均化.MapReduce的性能问题已经备受关注[3-6],本研究并不进一步分析系统性能因素,而从另外一个角度分析能耗差异.以HBase和HadoopDB为例,在同样的实验环境和执行用例下,经推理,造成HBase能耗高于HadoopDB的原因有两个:一是算法复杂度高,完成相同的用例,HBase需要执行更多的运算;其二是HBase在运算过程中等待能耗Page10多,能源效率不高.本研究并非比较各个云数据管理系统的实现算法细节,因此我们将针对第二个原因做进一步分析.6.2等待能耗分析本节分析各个云数据管理系统的等待能耗(定义2).如5.1节所述,我们着重研究CPU的等待能耗.部署在各个节点上的监控软件每隔一秒钟测量Cbusy(CPU使用率)、Dr/s(硬盘读速率)、Dw/s(硬盘写速率)、Nin/s(网卡读速率)、Nout/s(网卡写速率)和Mused(内存使用率)一次.实验记录了6种系统执行5个基准用例时12个节点的所有特征属性值.记录图3CPU使用率、本地I/O和网络I/O的比较由图3(a-1),(b-1),(c-1),(d-1)可以看出,各种云数据管理系统的CPU使用率很低,执行查询类用例(Grep和Selection)时CPU使用率低于33%,执行分析类型用例(Aggregation和Join)CPU使用率低于51%左右.CPU使用率低但每个节点的运算并没有执行完毕,因此CPU并非处于一种空闲繁多无法逐一展示,本节将列举Grep,Selection,Aggregation和Join用例的部分统计数据和实时数据.首先,所有系统在执行所有用例时Mused值均非常高(不低于90%),说明内存被充分的使用,这也符合I/O密集型运算的特点,大量内存被用作数据缓存和本地(网络)I/O写的缓冲区.为便于观察各系统的CPU、本地I/O和网络I/O特征,我们统计了所有节点在执行每测试用例时的CPU使用率、硬盘读写数据总量和网络数据传输总量,并比较它们之间的关系,如图3所示.状态,而是一种等待状态,产生等待能耗.进一步实验分析知,造成CPU等待的原因主要有以下3点.(1)CPU等待作业调度本研究没有扩展现有云数据管理系统的作业调度算法,而是通过作业的请求方式来影响调度算法.图3是在顺序的作业请求条件下测得,因此各系统Page11将采用顺序作业调度方式,节点会因等待作业调度而产生等待能耗.若采用并发请求的方式,则各系统的作业调度方式将会对应调整.实验证明,采用并发作业请求时,节点CPU使用率会随并发程度的增加而逐渐增加,等待能耗减少,并最后达到一个稳定值.然而,对于不同的系统,“并发请求调度”对降低等待能耗的贡献不同.HBase、Cassandra、Hive的CPU使用率随并发量变化很小,而MongoDB、HadoopDB和GridSQL则有明显的改观.我们测试了各系统执行并发Grep作业的性能,以HBase和GridSQL为例.对于GridSQL,当并发请求为60时,GridSQL的CPU使用率升为80%;对于HBase,并发请求为60时,使用率升为35%,使用率依旧很低.我们对HBase和GridSQL的主节点(仅一个)和从节点(任选一个)在Grep执行时间内的CPU使用率进行了直方图统计(用0~100%的10个等宽区间加以划分),图4中y轴为每个区间出现的使用率频数占总采样数的比例.图5则给出了实时CPU使用率的变化曲线.图4并发请求下HBase和GridSQL的主节点和从图4可直观地看出:GridSQL和HBase的主节点CPU使用率不高,而GridSQL的从节点大部分时间处于CPU高使用率的状态,CPU等待明显减少,说明图3(a-1)中GridSQL的CPU使用率低是因为“等待作业调度”.而HBase的从节点恰好相反,尽管“并发调度”使CPU使用率较图3(a-1)有所提高,但依然很低,CPU还在等待其它资源,如节点同步、本地I/O或网络I/O.从图5可更明显地看出,GridSQL的从节点CPU一直处于高使用率的运算状态直到用例结束,而HBase的从节点CPU使用率则忽高忽低,处于阶段性的“空闲”状态.对比GridSQL的CPU使用率特征,HBase的查询和调度等算法存在很大的优化空间.(2)CPU等待本地I/O或网络I/O操作导致HBase和Cassandra的CPU等待的原因之一是“等待I/O操作”.从图3可以看出,MongoDB、HadoopDB和GridSQL执行测试用例时本地I/O和网络I/O最少,Hive次之,而非关系型的HBase和Cassandra执行测试用例时本地I/O和网络I/O则很高.尤其是Join用例(图3(d-3)),HadoopDB和GridSQL采用了优化的表连接算法[35-36],网络数据迁移很小,而HBase则有5GB的网络数据读写,因此执行效率较前两者慢了上百倍(参见图2(b)).图3(a-3)中Cassandra执行Grep用例时网络I/O稍高,这是因为Cassandra是去中心化的,而MapRedcue框架需要有一个JobTracker和多个TaskTracker,因此Cassandra中总会有一个节点被选为JobTracker,而该节点保存的数据就会传输给其它TaskTracker处理.从图2可以看出MongoDB的Aggregation用例能耗高,而图3(d-2)可以看出能耗高的原因是本地磁盘访问量大,因为用例通过MapReduce实现而非使用MongoDB提供的查询语言,“内存映射机制”不能很好地发挥,且需要对全部数据进行读取,磁盘I/O增加.这进一步证明了空闲能耗的产生是因为“CPU等待本地I/O或网络I/O操作”.一方面,CPU处理数据的速度一般大于硬盘或网卡读写数据的速度,因此,在内存受限的环境下,大量的本地I/O和网络I/O会使CPU因等待I/O操作而空闲;I/O越多,CPU等待时间越长,等待能耗越大.另一方面,系统能耗很大程度上取决于效率,I/O密集型运算的运算效率主要取决于I/O效率,在集群环境中网络I/O明显要慢于本地I/O.因此,无论是基准能耗还是CPU等待能耗,都很大程Page12度上取决于网络读写的数据量大小.(3)CPU等待其他节点任务完毕导致HBase和Cassandra的CPU等待的另一个原因是“等待其它节点执行完毕”,因节点间执行任务不同步而导致部分节点等待,产生等待能耗.由于CPU使用率、本地I/O和网络I/O都能够表征节点的繁忙程度,而其中以CPU使用率最具有代表性.本文仅给出CPU使用率实时曲线.为简化作图,我们仅选取了主节点和3个从节点的CPU使用率实时数据.图6是3种云数据管理系统执行Grep用例时的4个节点的实时CPU使用率曲线(Selection,Aggregation用例与此类似).图中可以明显地看出,对于HBase(Cassandra类似),从节点的并行性较差,有些节点较早完成运算,有些则较晚,节点之间相互等待;而Hive和HadoopDB(GridSQL类似)的各个从节点并行性好,CPU使用率同时达到峰值和谷值,节点间无明显相互等待的现象.由此可见,提高节点间并行性会减少CPU等待,进而降低能耗.图6Grep用例下HBase,Hive,HadoopDB的主节点和图7展示了Join用例执行时HBase、Hive和GridSQL的主节点和从节点实时CPU使用率曲线.总体上可以看出Hive和GridSQL的并行性要优于HBase,前两者的从节点CPU使用率曲线基本吻合.Join用例明显地分为两个阶段,即“表连接”阶段和“分组聚集”阶段.对于HBase,两个阶段是通过两个独立的MapReduce完成的,“表连接”阶段采用ReduceSideJoin,大量数据在节点间迁移,因此CPU使用率不高;“分组聚集”的Map阶段数据在本地计算完成,而Reduce阶段又存在数据的迁移,因此对应CPU较繁忙和较空闲两个阶段.Hive的“表连接”阶段采用的是LookupJoin的方式,需要反复查找“连接键”是否存在,因此节点在执行本地查找时较为繁忙,在等待其它节点查找结果时较为空闲,曲线峰谷交替;而“分组聚集”阶段由于对“sourceIP”进行了分组,数据均为本地读写,网络I/O少,节点CPU工作稳定.GridSQL的“表连接”阶段采用DistributedHashJoin,连接开始阶段数据需要重新分区和迁移,网络I/O密集,CPU使用率较小,“分组聚集”阶段则主要是在本地操作,CPU工作稳定.图7Join用例下HBase,Hive,GridSQL的主节点和Join用例的能耗或是执行性能主要取决于各系统的“表连接”实现算法,很多数据管理系统会根据数据的特征来对“表连接”进行优化,本文不过多地讨论这些算法.总体上,有效地减少网络I/O可以Page13提高Join用例的执行性能,降低能耗.6.3优化方法本实验对6种云数据管理系统的能耗和性能做了分析和比较.可以看出,以HBase为代表的非关系型云数据管理系统的基准能耗要高于以Hadoop-DB为代表的关系型云数据管理系统.实验证明六种系统在执行基准测试时CPU多为等待状态,造成CPU等待的原因有三:其一是节点等待作业调度,可以通过增加作业请求的并发性得到改善;其二是节点运算存在瓶颈,CPU在等待网络I/O操作;其三是节点间因任务执行不同步而相互等待.因此,从方法层面,我们可从资源分配策略、数据布局策略和作业执行策略3个角度来优化能耗:(1)通过能耗优化的资源分配和任务调度策略,让每个任务占有的各种资源间比例合理,在最大化使用节点资源的同时避免资源间等待而产生的瓶颈;(2)通过合理的数据布局策略,提高节点间并行性,减少因任务同步而造成的节点等待;(3)通过对作业执行策略I/O代价的评估,动态地根据上下文选择I/O代价最小的执行策略,并优化现有执行策略的I/O代价.同样,也可以采用一些技术手段优化能耗.我们以HBase为例对CPU等待现象的成因做了如下推理并提出优化方法:(1)HBase不提供索引机制,数据查询和分析需要扫描整个数据集,I/O操作多,算法复杂度高,引入索引机制可以减少I/O操作;(2)数据节点(从节点)存在I/O瓶颈,MapReduce编程模型尽管独立于存储系统,但在本实验中是依赖HBase实现的,其本质是分布式文件系统HDFS,基准用例都需要通过HDFS的I/O接口扫描数据,接口读写速度较慢导致CPU等待,可以针对数据访问接口进行性能优化;(3)MapReduce的中间结果以文件方式输出,这又增加了算法的I/O操作,因此可以优化MapReduce任务执行算法,减少中间结果的输出,减少I/O操作;(4)使用高速网络或固态硬盘来降低I/O操作的代价.上述优化方法根据本实验分析结果以及HBase和MapRedcue的特性推理得出,进一步验证这些优化方法是我们下一步的工作.7结论和进一步工作本文提出了一种面向云数据管理系统的能耗模型、基准测试用例和测量方法.基于此,分析了典型的云数据管理系统HBase、Cassandra、Hive、MongoDB、HadoopDB和GridSQL的能耗和性能特征,针对云数据管理系统的特点提出减少等待能耗以实现节能的方法,并以HBase为例指出具体实现办法.本文通过实验得出如下结论:(1)尽管都采用SharedNothing体系结构和MapReduce编程模型,但以HBase为例的非关系型云数据管理系统执行数据查询和分析基准测试用例的能耗高于以HadoopDB为例的关系型云数据管理系统,但前者执行数据装载基准测试用例的能耗低于后者.能耗的高低主要取决于测试用例的执行时间.(2)几种云数据管理系统在执行基准测试用例时CPU等待时间较多,产生等待能耗.(3)产生等待能耗的原因是作业调度不合理,节点在等待作业调度;CPU等待本地I/O和网络I/O操作;以及节点之间任务执行不同步而相互等待.(4)从方法层面,可通过合理的资源分配和任务调度策略来提高资源使用率;或合理的数据布局策略提高节点间并行性;或作业执行策略I/O代价的评估和动态的选择等方法优化能耗.(5)从技术层面,可以通过增加索引机制、优化I/O性能、优化数据存储格式、选择高效存储来保存中间结果、优化网络传输、优化节点间的调度算法等技术来减少CPU等待,降低能耗.本文定义的能耗模型、基准测试用例、测试方法和得出的相关结论都有助于测试和比较各云数据管理系统的能耗,研究能耗优化的基本方法,指导云数据管理系统选择和低能耗云数据管理系统的设计和研发.本研究还处于初步阶段,我们将依据本文给出的优化思路,进一步研究云数据管理系统的能耗优化方法.同时,可以建立基于QoS的能耗模型,能耗作为一种服务质量要素,可以使云数据服务提供商准确地计算能源成本,服务使用者也准确地按需使用,按需付费.
