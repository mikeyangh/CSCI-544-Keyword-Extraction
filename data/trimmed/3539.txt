Page1基于FPGA的高精度科学计算加速器研究雷元武窦勇郭松(国防科学技术大学计算机学院长沙410073)摘要探索了FPGA平台加速高精度科学计算应用的能力和灵活性.首先,研究科学计算中最常用的操作———向量内积,提出基于定点操作的精确向量内积算法.以IEEE754-2008标准的四精度(QuadruplePrecision)浮点算术为例,在FPGA平台上设计了一个基于全展开方法的全流水四精度浮点乘累加单元(QPMAC):提出两级存储策略精确存储乘累加和;采用保留进位累加策略减少定点加法器位宽、简化进位处理、优化关键路径;引入累加和划分策略,实现流水吞吐率.最后,在XC5VLX330FPGA芯片上设计一个LU分解和MGS-QR分解加速器原型来验证QPMAC的性能.实验结果表明,与运行在Intel四核处理器上的基于OpenMP的并行算法相比,集成4个QP-MAC单元的加速器能获得42倍到97倍的性能提升,并且能获得更高结果精度和更低能量消耗.关键词四精度浮点算术;LU分解;MGS-QR分解;FPGA;硬件加速器;E量级计算1引言大部分科学计算采用双精度浮点算术实现.由于浮点数据是对现实世界中精确数据的近似表示,运算过程中存在舍入误差,这种舍入误差的累积将导致计算结果不精确、不可靠、甚至不正确.预测到E量级(ExaScale:百万万亿次/秒)[1]计算时代,LU分解的计算结果仅有3位有效[2],这表明双精度浮点算术不能满足未来科学计算的精度要求.高精度计算是解决精度问题最直接、有效、可靠的方法,已经广泛应用于各个领域,如天气或气候模拟、超新星模拟、计算几何、数值理论等[3].鉴于高精度计算在提高应用结果精度、数值算法稳定性和结果再现性等方面的优势,IEEE754-2008浮点算术标准中增加了四精度(QuadruplePrecision)浮点格式[4]来支持高精度计算.目前,四精度浮点算术通常使用软件模拟实现,如IntelFortran[5]、GMP、MP-FR[6]、QD(Quad-Double)[7]等函数库.软件方法的最大缺点是计算性能低,相对于双精度浮点算术,四精度浮点算术的性能降低了一个数量级,Linpack测试时间为双精度浮点算术的36倍[3,8].有学者设计专用硬件逻辑来支持四精度浮点算术,克服软件方法的性能障碍.IBMS/390G5处理器[9]在硬件上支持四精度浮点基本操作.Akkas等人[10]设计了双模式的四精度加法、乘法和除法单元,支持一个四精度浮点操作或者两个并行双精度浮点操作.使用可重构平台加速科学应用已经成为一种主流趋势,在高精度科学应用中具有较高的性能和良好的可扩展性[2].可重构计算通过定制逻辑满足应用对计算精度的需求,以达到最佳性能.可重构计算的底层技术是FPGA(现场可编程门阵列)技术.目前,FPGA向更大密度、更高性能和更低功耗发展,XilinxVirtex-6FPGA具有741K个逻辑单元,37.4Mbit嵌入式存储器和2016个DSP48E乘法器.文献[2]在FPGA平台上设计了Double-Double(DD)和QD类型的科学应用加速器,相对于通用CPU,能够取得30倍以上的加速比.本文在文献[2]的基础上,以IEEE754-2008标准的四精度浮点算术为例,在FPGA平台上设计了一个基于全展开方法的全流水四精度浮点乘累加单元(QPMAC),采用无误差的定点操作代替浮点操作以获得精确的结果,并提出两级存储策略、保留进位累加策略、累加和划分策略,来提高QPMAC性能,实现流水吞吐率.最后在FPGA芯片上设计LU分解和MGS-QR(ModifiedGram-SchmidtQR)分解加速器原型来验证QPMAC的正确性及性能.2背景介绍2.1浮点算术一个浮点算术系统可以用四元组R(b,r,e1,e2)表示,其中b表示基(2或10);r表示尾数的位数,即精度;e1为最小指数,e2为最大指数.表1列举了部分浮点算术系统.格式四精度[4]R{2,113,-16382,16383}128≈10-34DD[7]≈R{2,106,-1022,1023}128≈10-32QD[7]≈R{2,212,-1022,1023}256≈10-64QD函数库采用多个标准的浮点数据和来表示一个高精度数据.这类高精度算术充分利用当今处理器提供的高性能浮点处理能力,计算速度较快,但是这种实现方式的精度是确定的,数据的表示范围较小,仅为双精度浮点表示范围.2.2浮点算术中的精度损失浮点算术的精度损失来源于截断误差和基本操作的舍入误差.截断误差是将精确值转换为确定格式浮点数据时产生的误差.基本浮点算术操作中,减法对精度损失影响最大,两个非常相近的数据之间的减法操作存在“巨量消失”现象[11],计算结果的相对误差变大,不精确位向前传播多位.文献[2]以LU分解为例,通过大量随机测试,建立精度损失与基本操作数量的关系模型.LU分解结果的不精确位数(m)是以矩阵规模(n)自然对数的方式增加(m=3.11×ln(n)-1.499),预测在即将到来的E量级计算时代,结果的平均精度仅有3位有效(十进制).这不能满足科学计算的精度要求,因此,我们必须构建更高精度的算术单元.2.3高精度向量内积的相关研究向量内积(∑n一,也是BLAS函数库中的一个基本子程序,几乎在所有科学和工程应用中出现.在矩阵类运算、Krylov子空间等应用中,对向量内积的计算精度要求较高[12].但是,通用处理器没有专用的硬件来实现向量内积,只能通过基本浮点操作模拟实现.而基Page3本浮点操作的舍入误差可能导致内积结果不精确.为了提高计算精度,Higham[13]引入了重排序方法和补偿累加方法.重排序方法首先按升序或降序对累加数据进行排序,然后对有序数组进行累加,这种方法增加排序开销;补偿累加方法记录和累加每次舍入的信息,最后将其补偿到最终结果中,这种方法需要额外开销完成舍入信息的累加.Rump等人[14]提出了精确累加算法,其关键是使用一种无误差划分技术.以上方法都是使用软件方法来提高计算精度,不能有效避免累加过程中的“巨量消失”现象,而且软件方法的速度较慢.还有学者提出高精度向量内积单元的设计方法.Kulisch[15]建议将精确内积单元集成到通用处理器中,作为“第5个基本浮点算术”.He等人[16]提出了累加数据分组方法,每组数据移位对齐后再进行定点累加,这种方法在每组计算中需要移位对齐、规格化处理,增加处理开销.Kulisch[17]和Knofel[18]设计了一个标量内积单元(SPU)协处理器实现32位和64位浮点精确向量内积.我们设计了DD类型和QD类型的高精度乘累加单元(HPMAC)[2].从表1可知,相对于DD和QD类型,IEEE754-2008标准的四精度浮点格式的数据范围扩大了16倍,这意味着QPMAC设计中用于快速进位处理的标志位宽度也需要扩展16倍.这将给全流水QPMAC设计带来挑战———如何快速定位进位终止因子(CarryTerminateFactor)和确定进位传递因子(CarrySkipFactor).本文在文献[2]的基础上对基于全展开的全流水高精度乘累加器的设计及FPGA加速高精度科学应用的探讨进行完善:首先,采用保留进位累加策略来替换快速进位策略,以简化进位处理,支持更大数据范围的浮点格式,并以IEEE754-2008标准的四精度浮点为例进行验证;其次,增加MGS-QR分解应用来说明QPMAC的性能及FPGA加速高精度科学应用的能力;最后,使用性能较高的任意精度函数库(MP-FR)和针对Intel处理器进行性能优化的IntelFor-tran函数库来对比基于QPMAC单元的FPGA加速器的性能.为了公平对比多核CPU和FPGA平台的性能,我们采用OpenMP技术对LU分解和MGS-QR分解应用进行并行化,充分开发多核CPU的性能.3高精度浮点乘累加单元的FPGA实现高精度浮点乘累加操作首先进行无精度损失的定点乘法和加法得到精确定点乘累加和,然后对其进行规格化.该操作仅在规格化过程中进行一次舍入操作,最多引入一位误差.3.1精确向量内积算法假定sign(x)(或SX),exp(x)(或EX)和mant(x)(或MX)分别表示浮点数据x的符号、指数和尾数;犃=(Ai)和犅=(Bi)(i=1,2,…,n)表示两个长度为n的向量,其中x,Ai,Bi∈(2,r,e1,e2).向量内积为狊=∑n度为2r位,指数范围为[2e1,2e2],即P∈(2,2r,2e1,2e2).因此,乘积P所有位的信息可以无损失地存储到一个长度为L=|2e1|+2r+2e2的寄存器中.图1描述了精确向量内积算法.首先,读取浮点数据Ai和Bi,并将其分解为符号位、指数位、尾数位;然后,执行定点尾数乘法得到宽度M=2r的乘积product;再将product累加到寄存器sum中;最后,规格化L位的累加和sum为标准格式数据.Algorithm:ExactDotProductInput:Ai,Bi(i=1,…,n)Output:ResultInitialize:sum[L:1]=0;1.fori=1tondo2.Load(Ai,Bi);3.s=sign(Ai)sign(Bi);exp=exp(Ai)+exp(Bi);4.new_sum[2exp-1:1]=sum[2exp-1:1];5.if(s==0)then6.{carry,new_sum[2exp+M-1:2exp]}=sum[2exp+M-1:7.new_sum[L:2exp+M]=sum[L:2exp+M]+carry;8.else9.{carry,new_sum[2exp+M-1:2exp]}=sum[2exp+M-1:10.new_sum[L:2exp+M]=sum[L:2exp+M]-carry;11.endif12.sum[L:1]=new_sum[L:1];13.endfor14.Result=normalize(sum[L:1])算法的步4~11将尾数乘积product累加到L位累加和sum中,通常,需要一个L位的定点加法器,这会造成较大的延时.然而,我们可以根据乘积指数exp,将sum分为3个部分,定点累加过程只需一个M位加法器.累加和sum的小数点位于|2e1|+2r,product对应于sum的2exp到2exp+M-1的位置.累加和sum低位部分保持不变(算法的第4行).仅需要一个M位的定点加法完成product对应位置的累加(算法的第6行或第9行).高位部分的操作取决于M位加法的进位信息(算法的第7行或第10行).Page43.2四精度浮点乘累加单元设计如图2左边所示,四精度浮点乘累加单元(QP-MAC)主要由113×113定点尾数乘法器,尾数乘积累加模块和规格化模块组成.图2QPMAC结构框图3.2.1两级存储策略L位定点累加和寄存器组织成一个两级存储结构.如图2所示,第1级称为存储体(SumMem或Bank),第2级称为子存储体(Sub-bank).两级存储体的容量为L=|2e1|+2r+2e2=65756位.将L位存储体组织成一个多子存储体的结构,这能提供多个端口,快速访问与尾数乘积product对齐的数据.我们为子存储体中每个项设置一个全0标志位(ZF或Zero_Flag)和一个进位标志位(CF或Carry_Flag).当该项的值不为0时,置ZF为1,否则置为0.全0标志位的作用:(1)初始化存储体,即在累加操作前,设置存储体中对应的ZF为0,而不需要对存储体进行初始化;(2)规格化时快速确定L位sum中首1位置.通过ZF可以快速定位首个非全0的字,这样将L位的首1查找转换为B位(子存储体的端口宽度)的首1查找.进位标志位将在下面进行介绍.我们根据B来组织两级存储结构.通过选择合适的端口宽度以均衡加法长度、选择器数量、标志位的长度等硬件开销.设:B为子存储体的端口宽度,为2的幂;M为定点尾数乘积product的位宽;NS为每个存储体中子存储体的个数,通常为2使用XilinxVirtex-5系列FPGA芯片中的DSP48E(18×25)单元来构建113×113定点尾数乘法模块.该模块采用6级流水结构,包括35个DSP48E单元和部分积加法树,乘积结果为226位.的幂,这样子存储体的访问地址为乘积指数exp的低位;WC为进位标志位的宽度,后面分析得WC=3;WA为定点加法组中加法器宽度,为B+WC;NA为定点加法组中WA位加法器的个数;NM为两级存储体结构中B位选择器的个数;LF为Zero_Flag标志位的长度,则有:NA=M/B+1,NS=2logNA(NS+NA)+NA,LF=L/B.表2可由上述公式推导出来,可以看出情况2(B=128)为较优的折中选择,即每个存储体由4个子存储体组成,子存储体的端口宽度为128位,每个周期能够访问4个连续的128位数据.情况BNSNAWANMLF164866762102821284413136514325643259312573.2.2保留进位累加策略本文采用保留进位累加策略实现定点尾数乘积的累加(下面简称为乘积累加),每次累加计算选择对应字的进位标志位参与计算,并根据计算结果更新进位标志位.Page5Algorithm:ProductAccumulationbasedonCarry-SaveSchemeInput:Producti,expi[15:0](i=1,2,…,n)Output:C[515:0],CF[515:0],Zero_Flag[515:0]Initialize:ZF[515:0]=0;1.fori=1tondo2.{Pi[2],Pi[1],Pi[0]}=Producti<<expi[6:0];3.Setk=expi[15:7].LoadC[k]~C[k+3]and4.if(ZF[k]==0)then{CF[k],C[k]}=0;5.{CF[k],C[k]}=C[k]+Pi[0];if(ZF[k+1]==0)then{CF[k+1],C[k+1]}=0;if(ZF[k+2]==0)then{CF[k+2],C[k+2]}=0;if(ZF[k+3]==0)then{CF[k+3],C[k+3]}=0;{CF[k+1],C[k+1]}=C[k+1]+Pi[1]+CF[k];{CF[k+2],C[k+2]}=C[k+2]+Pi[2]+CF[k+1];{CF[k+3],C[k+3]}={CF[k+3],C[k+3]}+CF[k+2];6.StoreC[k]~C[k+3]andCF[k-1]~CF[k+3]toSumMem;7.UpdataZF[k]~ZF[k+3]accordingtoC[k]~C[k+3]and8.endfor图3基于进位保留策略的定点尾数乘积的累加算法如图2和如图3所示,基于保留进位策略的乘积累加过程如下:步1(RdC):对齐sum和product,本文采用两级对齐策略:步1.1,乘积指数的高9位作为sum的访问地址(假定k=exp[15:7]),读出与product对齐的连续4个128位的字C[k+3:k]及相应的进位标志CF[k+3:k],同时根据符号位将product转换为补码表示.步1.2,然后根据乘积指数的低7位对product进行移位,使之与C[k+3:k]对齐,得到P[2:0];步2(CalC):使用4个128位的定点加法器完成定点尾数乘积的累加,使用1个3位的进位加法完成进位处理,如图2所示.步3(WrC):将累加结果C[k+3:k]和更新后的进位标志位CF[k+3:k]写回到sum的相应位置.步4(UpF):根据C[k+3:k]和CF[k+3:k]更新相应的全0标志.定理1.采用上述保留进位累加策略,累加次数小于2127次时,每个进位标志的数值不会超过3.证明.假定W[i]={CF[i],C[i]},采用归纳法证明第j次累加后,W[i]<2129+2×j,其中0i515.(1)初始化时,W[i]=0,0i515.(2)第j+1次累加时,假定W[k]、W[k+1]、W[k+2]、W[k+3]参与累加计算,根据归纳假设有:W[k]=P[0]+C[k]<2129+2×(j+1);W[k+1]=P[1]+C[k+1]+CF[k]<2129+W[k+2]=P[2]+C[k+2]+CF[k+1]<W[k+3]=W[k+3]+CF[k+2]<2129+2×2×(j+1);2129+2×(j+1);(j+1).其余字和对应的进位标志位保持不变,这可保证第j+1次累加结果满足W[i]<2129+2×(j+1),0i515.若进位标志数值超过3,则2129+2×(j+1)>3×2128,即j>2127-1.因此每个进位标志位仅用三位来表示,高位表示进位符号、低两位表示进位数值.位累加策略的优势在于:相对于快速进位处理方法[2,17-18],这种保留进(1)将文献[2]中的384位加法转化为4个并行执行的三输入128位加法,减少延时.同样采用先行进位设计方法,将三输入128位加法分成4段,13个32位的加法操作并发执行,最后根据各段的进位选择最终结果.这种设计方法将三输入128位加法延时从6.48ns降低到4.35ns,且小于384位加法延时(6.8ns)[2].(2)简化进位处理逻辑.快速进位方法中传递因子和终止因子处理过程:首先,根据乘积指数exp对all_one_flag标志位[2]进行左移操作;然后,进行数1操作确定移位后的标志位寄存器中最低的0位,对应的字为终止因子;最后,更新传递因子和终止因子,并进行右移操作,完成all_one_flag标志位更新操作.从DD类型扩展到四精度时,all_one_flag标志位宽度由36位增大到516位,导致快速进位过程中的移位操作和数1操作的延时达到11.0ns和10.1ns,导致HPMAC单元的频率降低.保留进位累加策略采用进位标志位替换传递因子、终止因子的确定及进位与终止因子加法逻辑,从而提高QPMAC单元的频率和性能.3.2.3累加和划分策略乘积累加需要进行累加和寄存器访问、定点加法计算、标志位更新等操作.整个过程延时较大,需要使用流水线技术来提高运行频率.根据保留进位累加策略,将乘积累加模块设计成四级流水线结构,分别为RdC、CalC、WrC和UpF.同时,本文提出累加和划分策略来实现全流水乘积累加模块,使QPMAC获得流水吞吐率.该策略将累加和sum分为4个部分,分别存储到存储体SumMem[0]~SumMem[3]中.多存储体流水执行的时空图如图4所示,每个存储体4个周期完成一次乘积累加操作,每个时钟周期各个存储体处于不同流水阶段.假如product在周期j进入乘积累加模块,则将其累加到存储体SumMem[j%4]中,其中“%4”表示模4操作.总之,累加和两级存储策略中,第1级由4个存Page6储体(SumMem[0~3])组成,这使QPMAC单元能够获得流水吞吐率;第2级由4个子存储体(Sub_bank[0~3])组成一个存储体,这能够提供多个访问端口,每个周期可以访问多个数据,减少数据访问周期.3.2.4QPMAC实现四精度浮点基本操作四精度浮点加法、乘法及乘加融合(FMA)操作可以视为内积运算的特殊情况.浮点加法可视为向量长度为2的内积,且向量犅的元素均为1;浮点乘法可视为向量长度为1的内积;FMA操作(r=a×b+c),可视为向量长度为2的内积,且向量犃={a,c},向量犅={b,1}.4基于QPMAC的应用本节以DoolittleLU分解和MGS-QR分解应用为例来说明QPMAC的性能及高精度科学计算FPGA加速器的设计方法.加速器结构如图5所示.4.1基于QPMAC的LU分解LU分解是Linpack测试基准中的核心算法.如下所示,DoolittleLU分解方法[13]由向量内积和除法操作组成,它将非奇异矩阵犃分解为下三角矩阵犔和上三角解矩阵犝,计算复杂度为O(2n3/3).fork=1tonforj=ktonendforfori=k+1tonendforendfor图6描述了基于QPMAC细粒度流水LU分解算法.由两个并行算法组成,分别为Master算法和Slave算法,它们采用基于消息传递接口(MPI)的SPMD模式来描述,其中,N表示矩阵犃的规模,P表示执行Slave算法的处理单元(PE)个数.如图5(a)所示,LU分解加速器主要由接口控制模块、主PE和一个计算PE阵列组成.其中,接口控制模块通过PCIE通路与主机进行数据和命令交互,同时还控制两个DDR2存储条的访问;主PE执行Master算法,计算PE执行Slave算法,主PE和计算PE阵列共同完成LU分解的计算任务.主PE和计算PE阵列均运行N/P次.每次运行主PE从DDR2存储器中读取数据,发送到计算PE1,执行发送原语5和6;当所有计算PE完成计算后,主PE执行写回原语9和10,将犔矩阵的P列和犝矩阵的P行写回到DDR2存储器.计算PE的结构如图5(b)所示,主要包括两个RAM、两个FIFO、一个QPMAC单元、一个四精度浮点除法模块和控制逻辑.两个RAM(RAM_TL和RAM_TU)存储计算结果中犔矩阵的一列和犝矩阵的一行;两个FIFO(FIFO_L和FIFO_U)用于缓冲计算PE之间的数据;四精度浮点除法模块采用基4-SRT除法算法[19]实现.QPMAC单元能够独立计算原语Cal_U,由QPMAC和除法模块共同计算原语Cal_L.注意到:图6算法A中原语Cal_L(Line9)执行之前,向量犜犔已经存储在RAM_TL中,因此,Cal_L的计算是由向量犜犇的数据所驱动,接收原语(Line8)、Cal_L原语(Line9)和发送原语(Line11)能够并行执行.同理,接收原语(Line12)、Cal_U原语(Line13)和发送原语(Line15)能够并行执行.这种计算与数据通信相重叠的方式可以有效掩盖数据传Page7递延时,数据以流水方式在PE之间传递,这最大限度发挥了QPMAC单元的流水吞吐率优势,达到每AlgorithmA:PipelinedLUdecompositionalgorithmMasteralgorithm:1.ifpid=0then2.fork=1toNbyP3.M=SetLength(k);Setk=k+pid;4.doinparallel5.Send(1,A[k:N,1:(k+P)]);6.Send(1,A[1:(k+P),k:N]);7.enddo8.forpid=1toP9.Store(A[k:N,(k-1)],pid,TL[k:N]);10.Store(A[(k-1),(k-1):N],pid,TU[(k-1):N]);11.endfor12.endfor13.endifStore(X,pid,Y):storeelementsofanarrayYofPEnumberedpidintoXSend(pid,X):sendelementsofanarrayXtoPEnumberedpid;Rcv(pid,X):receiveelementsofanarrayXfromPEnumberedpid;犆犪犾_犝(X[1:n-1],Y[1:n],u):calculateelementsofU犆犪犾_犔(X[1:n-1],Y[1:n],a,l):calculateelementsofLAlgorithmB:PipelinedMGS-QRdecompositionalgorithmMasteralgorithm:1.ifpid=0then2.fork=1toNbyP3.M=Setlength(k);3.Send(1,A[k:N,1:N]);4.Store(A[k+p:N,1:N],P,FIFO_A);5.forpid=1toP6.Store(R[(k+pid),(k+pid):N],pid,TR[(k+pid):N]);7.Store(Q[(k+pid),1:N],pid,TQ[1:N]);8.endfor9.endfor10.endif犆犪犾_犚(X[1:n],Y[1:n],u):executethedotproductuofvector犡and犢Cal_Q(X[1:n],R,Y[1:n]):executetheoperationY[i]=X[i]/RCal_A(X[1:n],R,Y[1:n]):executetheoperationX[i]=X[i]-RY[i],wherei=1,2,…,nSlavealgorithm:1.if1pidPthen2.while(TRUE)3.WaitSetLength(M);SetM=M+pid-1;4.Rcv(pid-1,TL[1:M+P-1]);5.Rcv(pid-1,TU[1:M+P-1]);6.Cal_U(TL[1:M-1],TU[1:M],TU[M])7.fori=M+pidtoN8.Rcv(pid-1,TD[1:M+P-1]);9.Cal_L(TL[1:M-1],TD[1:M],TU[M],TL[i]);10.TD[M]=TL[i];11.Send(pid+1,TD[1:M+P-1]);12.Rcv(pid-1,TD[1:M+P-1]);13.Cal_U(TU[1:M-1],TD[1:M],TU[i]);14.TD[M]=TU[i];15.Send(pid+1,TD[1:M+P-1]);16.endfor17.endwhile18.endifSlavealgorithm:1.if1pidPthen2.while(TRUE)3.WaitSetLength(M);4.Rcv(pid-1,TA[1:N]);5.Cal_R(TA[1:N],TA[1:N],Temp_R);6.TR[M+pid]=sqrt(Temp_R);7.Cal_Q(TA[1:N],TR[M+pid],TQ[1:N]);8.fori=M+pid+1toN9.10.11.12.13.endfor14.endwhile15.endif图6细粒度流水LU分解算法和MGS-QR分解算法4.2基于QPMAC的MGS-QR分解MGS-QR分解算法[13]如下所示,该算法将非奇异矩阵犃分解为正交矩阵犙和上三角矩阵犚,计算复杂度为O(n3).fork=1toNR[k,k]=sqrt(∑i=1…NA[k,i]·A[i,k]);Q[k,]=A[k,]/R[k,k];forj=k+1toNR[k,j]=∑i=1…NQ[k,i]·A[i,j];A[j,]=A[j,]-R[k,j]·Q[k,];endfor▲=1,2,…,Nendfor基于QPMAC流水MGS-QR分解算法及加速个周期完成一次四精度乘累加操作.ParallelDoParallelDoParallelDoParallelDoParallelDoParallelDo器结构与LU分解算法相似,如图5、6所示.Master算法的主要区别是从DDR2存储器中读出的数据和传递到计算PE中的数据不同.LU分解中,每个计算PE是对原始矩阵的数据进行操作;而MGS-QR分解中,每个计算PE需要更新整个矩阵,下一个计算PE的数据是当前计算PE的更新结果,Master算法和Slave算法每执行一次,都需要将最后计算PE的更新矩阵写回到DDR2存储器中.如图5(c)所示,MGS-QR分解加速器中的计算PE主要由两个RAM、一个FIFO、一个QPMAC单元、一个四精度浮点除法模块和一个四精度浮点开方模块.RAM_A用于存储来自上一个PE的更新Page8数据,RAM_Q用于存储计算结果中犙矩阵的一行;FIFO_A用于缓冲计算PE之间的数据;四精度开方模块采用Non-Restoring开方算法[20]实现.与LU分解相似,QPMAC单元能够独立计算原语Cal_R和Cal_A,采用数据传递和计算重叠的方法来隐藏通信延时,提高QPMAC单元的利用率.但与LU分解不同的是,图6算法B中Slave算法的第11、12行必须在第9、10行完成后才能执行,这就使得数据接收、计算和发送不能并行执行.只能通过数据接收与计算重叠和数据发送与计算重叠方式来隐藏数据通信延时.5实验结果我们在FPGA开发板上验证上述设计.开发板上包含一块FPGA芯片、2×2GBDDR2存储条.DDR2控表3FPGA资源使用表基本运算部件LU分解加速器4PE68957(33%)143087(68%)134(46%)140(72%)160.81021.93MGS-QR2PE39223(18%)75242(36%)76(26%)70(36%)184.26417.04分解加速器4PE67808(32%)142635(68%)134(46%)140(72%)160.30819.96从表3中可以看出,DSP48E资源已经成为四精度科学计算加速器设计的瓶颈.因为DSP48E用于完成定点尾数乘法,它的需求量是以尾数宽度的平方速度增长.在QDHPMAC(256位)中DSP48E的消耗达到了50%[2].在全流水QPMAC设计中,使用FPGA片内分布式RAM实现两级存储体结构中的子存储体.这使FPGA的布局布线更加灵活,克服FPGA片内块RAM位置确定,结构固定的不足,同时这也能提高FPGA片内存储器资源的利用率.但是,分布式RAM占用FPGA的逻辑资源.因此,QPMAC中SliceLUT消耗达到13%.由于在QPMAC设计中采用保留进位累加策略来克服数据范围扩大带来的挑战,综合频率相对于DDHPMAC(170.7MHz)[2]提高了12%.LU和MGS-QR分解加速器的综合频率都超过160MHz.而且,计算阵列规模的扩大并没有导致加速器频率出现明显下降,这表明LU和MGS-QR分解加速器结构具有良好的可扩展.对于4PE的LU或MGS-QR分解加速器来说,FPGA的峰值性能达到制器的运行频率为200MHz,峰值带宽可达6.4GB/s.FPGA芯片为XilinxVirtex5XC5VLX330-1FF1760,它包含207360个LUT、192个DSP48E、10368Kb片内存储器.主机与FPGA通过PCIEx8通道进行数据和命令的交互,带宽为900MB/s.我们使用性能较高的任意精度函数库MPFR函数库[6](MPFR3.0.0,精度设为113位)和针对Intel处理器进行性能优化的IntelFortran函数库(简称Intel函数库)[5],来对比性能和结果精度.软件平台主要包括四核处理器(2.33GHzIntelCore2QuadQ8200)、4GBDDR3存储条.FPGA平台的运行时间包含从主机向开发板发送初始数据和主机从开发板接收结果数据的时间.5.1FPGA资源LU分解和MGS-QR分解加速器的综合结果.表3描述了基本运算部件及不同PE个数的01280MFLOPS(128-bitFloating-pointOperationsPerSecond).下面对于向量内积、LU及MGS-QR分解应用,FPGA平台均运行于133MHz.表3中的功耗是通过ISE11.3中的XPowerAnalyzer工具估计得到的,其中20%~30%的功耗用于IObuffer.4PE的LU分解加速器的功耗为21.93W,它仅为4核处理器IntelCore2QuadQ8200功耗(95W)的1/4.5.2向量内积应用的精度和性能图7比较了FPGA平台和软件平台实现不同长度向量内积运算的精度.在文献[2]图14(d)中定义的数据集Set1~Set6上测试内积运算,选取随机Page9测试中最坏情况来说明.从图7可以看出,软件平台的内积结果精度会随着向量长度的增加而降低.而QPMAC计算过程中使用精确的定点运算,不会产生误差,因此能够获得精确的结果.两种方法的精度差别达到12位.图8比较了软件平台(MPFR、Intel函数库)和FPGA平台实现向量内积的性能.MPFR和Intel函数库的吞吐率分别为6MFLOPS和21MFLOPS,而QPMAC单元采用全流水设计,能够取得流水吞吐率,假定初始数据全部存储于DDR2存储条上,QPMAC单元以133MHz运行能取得266MFLOPS的性能,加速比分别达到45.9和12.8.我们通过IntelVTune性能分析工具来分析CPU和FPGA平台的性能.如表4所示,任意精度函数库MPFR实现1M的向量内积需要1126M条指令,这是因为MPFR函数库中,任意精度浮点操作使用整数操作模拟实现,所需的指令数较多,而且还有部分指令用于函数调用等.Intel函数库是针对IEEE标准的四精度浮点算术进行专门设计,相对MPFR来说,其指令数可以减少一半以上,而且Intel函数库针对Intel处理器的特点(如SSE)进行性能优化,其CPI更低,因此,Intel函数库的性能比MPFR高2.6倍.FPGA平台能够针对不同的计算精度设计相应的逻辑单元,所需指令数较少,而且QPMAC单元采用流水设计,因此,FPGA平台能够以133MHz的运行频率取得12倍以上的加速比.表4不同平台下长度为1M的向量内积的性能分析MPFR(113bits)N.I.CPIT/ms1126M0.7344503M0.4495.82M7.512注:表中:N.I.表示指令数,CPI表示ClockPerInstruction,T表示时间,S表示相对于Intel函数库的加速比.5.3LU分解应用的精度和性能如表5所示,基于QPMAC的LU分解的计算精度比相同精度的软件实现更精确.这是因为软件实现向量长度n的内积时,需要引入2n-1次舍入误差,而QPMAC单元仅在规格化时才进行一次舍入操作.对于规模为4096的LU分解,结果精度提高了5.4位.LUFPGA100.198.897.595.794.1MGS-FPGA103.6102.3100.998.897.1QRCPU100.899.697.193.791.2图9比较了FPGA平台和软件平台上不同规模LU分解的性能.在Intel四核处理器上实现串行LU分解算法和基于OpenMP的并行LU分解算法.对于MPFR和Intel函数库,并行算法的执行速度分别为串行算法的2.8倍和2.65倍.从图9(a)可知,对于给定矩阵规模,LU分解加速器的吞吐率随着PE个数成线性增长,这是因为随着PE个数的增加,Master和Slaver算法的运行次数线性减少,而每次运行的时间开销基本保持不变.对于确定PE个数,LU分解加速器的吞吐率随着矩阵规模的增大略有提高,这是因为计算PE阵列的启动和流水排空开销固定不变,而数据以流水方式在计算PE之间传递,随着矩阵规模的增大,启动和流水排空开销在总时间的比重减小.当矩阵规模为4096时,FPGA平台上4PE的LU分解加速器的吞吐率接近1000MFLOPS,而MPFR和Intel函数库的吞吐率分别为10.1MFLOPS和22.7MFLOPS,加速比可以达到97倍和43倍.图9LU分解的性能比较,其中图(a)中M、I、S、O分别表示MPFR、IntelFortran、串行算法、基于OpenMP的并行算法,柱状图表示不同实现方式的吞吐率,从左至右依次为:串行MPER、并行MPER、串行InterFortran、并行InterFortran及集成(1PE、2PE、3PE、4PE)的FPGA加速器Page105.4MGS-QR分解应用的精度和性能与LU分解应用相似,基于QPMAC单元的MGS-QR分解的计算精度比相同精度的软件实现更精确.如表5所示,对于矩阵规模为4096的MGS-QR分解,结果精度提高了5.9位.图10比较了不同平台不同规模MGS-QR分解的性能.在Intel四核处理器上,基于OpenMP的MGS-QR并行分解算法的执行速度约为串行算法的1.9倍和1.8倍.与LU分解相似,对于给定矩阵规模,MGS-QR分解加速器的吞吐率随PE个数线性增长.但是对于确定PE个数,MGS-QR分解加速器的吞吐率不再随着矩阵规模增大而提高,这是因为MGS-QR分解算法中矩阵更新过程的数据接收、计算和发送不能同时执行,数据不能以完全流水的方式在计算PE阵列中流动,这导致不同矩阵规模,计算PE阵列的启动和流水排空开销在总时间的比重保持不变.对应矩阵规模为4096的MGS-QR分解,4PEFPGA实现的吞吐率达到918MFLOPS,而MPFR和Intel函数库的吞吐率分别为12.5M和21.7MFLOPS,加速比达到73倍和42倍.图10MGS-QR分解的性能比较,子图中M、I、S、O的定5.5PCI-E性能分析由于主机与FPGA通过PCIEx8通道进行数据交互,FPGA平台的性能受到PCIE带宽的限制.在向量内积应用中,QPMAC的计算性能为266.6MFLOPS,匹配的IO带宽为4.3GB/s,而实际通信带宽仅为900MB/s,这导致FPGA平台的计算性能下降4.8倍,相对于Intel函数库的加速比仅为2.7.在LU和MGS-QR分解应用中,由于计算复杂度与通信复杂度的比为O(n),通信开销在总时间中的比重是随着矩阵规模增大而减小,如图11所示.因此,使用FPGA平台加速高精度科学应用时,需要尽量复用数据,提高计算复杂度与通信复杂度的比值,减少IO带宽的需求,从而提高加速性能.图11LU和MGS-QR分解中PCIE通信开销相对于4PE6结论本文定制了一个四精度浮点乘累加单元(QP-MAC),采用精确定点乘法和加法操作代替浮点操作,仅在最后规格化时进行一次舍入操作.这样能够获得精确的向量内积结果,避免减法操作的“巨量相消”现象.在QPMAC设计中,提出两级存储策略来存储累加和的数据,同时引入保留进位累加策略和累加和划分策略,来最小化关键路径长度,简化进位处理,保证流水吞吐率.最后,为了验证QPMAC性能,在XC5VLX330芯片上设计一个LU分解和MGS-QR分解加速器原型,集成4个QPMAC单元,与Intel四核处理器相比获得42到97倍的性能提升.
