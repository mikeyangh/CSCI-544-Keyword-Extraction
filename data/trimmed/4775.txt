Page1分布式存储中的纠删码容错技术研究王意洁许方亮裴晓强(国防科学技术大学并行与分布处理国家重点实验室长沙410073)(国防科学技术大学计算机学院长沙410073)摘要大数据规模上体量大和增长速度快的特点对存储系统的性能和可扩展性提出了严峻挑战.使用普通商用服务器构建的分布式存储系统服务能力强、成本低廉且极易扩展,在大数据的存储管理中得到了极为广泛的应用.分布式存储系统庞大的节点数量导致节点失效情况频发,必须采用一定的容错技术来保证数据可靠性.常用的容错技术主要包括多副本技术和纠删码技术两种.与多副本容错技术相比,纠删码容错技术能够以低得多的存储开销提供相同甚至更高的数据可靠性.随着近年来数据规模的爆炸式增长,纠删码容错技术受到了业界的广泛关注.该文综述了分布式存储中纠删码容错技术的研究现状.首先,介绍了纠删码容错技术的基本原理和概念,指出了纠删码容错技术在大规模分布式存储中面临的主要技术挑战;然后,从编码实现、纠删码设计、数据修复和数据更新等方面阐述了分布式存储中纠删码容错技术的研究进展,重点研究分析了各项关键技术的特点和局限性,并依据主要评价指标对现有纠删码的编码性能和修复性能进行了对比和分析;最后,基于最新研究动态指出了分布式存储中纠删容错技术未来的研究方向,包括同步编码实现技术、低冗余再生码设计和数据失效预测技术等.关键词分布式存储;纠删码;编码实现;数据修复;数据更新1引言进入大数据时代,数据在规模方面的显著特点是体量庞大和增长迅速[1-2].例如,欧洲中等范围天气预报中心在2015年11月存储的原始数据达到87PB,并且每月增长约3PB①,年化增长率为41%;根据2016年3月的数据,社交网站Facebook上用户分享的图片在过去6年间增长了20倍②,平均每年增长65%;从2012年到2016年3月,云存储平台Dropbox上的数据量从40PB增长到了500PB③,年复合增长率高达88%.规模庞大并且仍然在不断迅速增长的数据,对构建良好的存储系统提出了重大挑战.系统既要有极高的数据存取性能,也要有良好的可扩展性,可以在不影响系统正常运行的前提下动态地增加系统规模并获得相应的性能提升.庞大的数据规模对系统的经济成本也提出了严格要求.传统基于RAID(RedundantArrayofIndependentDisks)[3]的单点存储系统或基于SAN(StorageAreaNetwork)[4]的网络存储系统等都无法同时满足大数据存储在性能、可扩展性和经济成本等方面的要求.构建于大量廉价商用硬件之上的分布式存储系统,不仅可以通过并行访问提供极高的数据存取性能,也可以通过增加存储节点增大规模并提升性能,并且成本低廉.因此,分布式存储系统在大数据的存储管理中得到了极为广泛的应用.例如,谷歌的文件系统GFS(GoogleFileSystem)[5]和Hadoop中被广泛采用的存储系统HDFS(HadoopDistributedStorageSystem)[6]就是该类存储系统的典型代表.数据容错是大规模分布式存储中一项不可或缺的关键技术.由于数据量极为庞大,该类存储系统往往包含几千甚至几万个存储节点[5,7-8].例如,百度公司单个集群的节点数量在2014年就超过了10000台[9].庞大的节点数量使得节点失效成为常态[5,7,10-11].近年来一些大型系统中的统计数据表明,平均每天都会有1%~2%的节点发生失效[8,12].因此,采用一定的数据容错技术,从而保证在部分存储节点失效的情况下数据仍然能够被正常地访问就显得尤为重要.目前,常见的数据容错技术有两种:一种是多副本容错技术[13],通过复制进行容错;另一种是纠删码容错技术[14],通过编码进行容错.与多副本容错技术相比,纠删码容错技术可以在显著降低存储空间消耗的同时提供相同甚至高得多的数据容错能力[15-16].例如,与被广泛采用的三副本相比,(14,10)-RS(Reed-Solomon)纠删码[17]既可将存储空间消耗降低53%,也可将容错能力提高一倍.随着大数据时代数据规模的爆炸式增长,容错能力强且存储成本低的纠删码容错技术受到了广泛关注,成为了存储领域的一个研究热点.目前,国内外关于纠删码容错技术研究的综述文献较少,主要是关于纠删码在RAID存储系统中的应用[18-19].但是与RAID存储系统相比,大规模分布式存储系统具有不同的特点,其中纠删码容错技术面临的技术挑战也不尽相同.除了纠删码设计,近年来也涌现出了很多从编码实现、数据修复和数据更新等其它方面优化纠删码容错技术的研究工作.本文将介绍大规模分布式存储中纠删码容错技术面临的主要技术挑战,以及围绕这些技术挑战的最新研究进展,并指出大规模分布式存储中纠删码容错技术的未来研究方向.本文第2节介绍纠删码容错技术的基本原理并明确常用的相关概念;第3节介绍纠删码容错技术在大规模分布式存储中面临的主要技术挑战;第4①②③infrastructure/Page3节介绍应对编码实现挑战的研究进展;第5节和第6节分别介绍从纠删码设计和数据修复技术两个方面应对数据修复挑战的研究进展;第7节介绍应对数据更新挑战的研究进展;第8节介绍有关纠删码的其它研究内容;第9节展望未来的研究方向;最后,第10节总结全文.2纠删码的基本原理和概念通常,纠删码[15-16]可以用三元组(n,k,k)来表示,其中n>kk.一个(n,k,k)-纠删码将大小为M的数据对象O划分成k个大小均为M/k的数据块o1,o2,…,ok,然后通过相应的编码算法对这些数据块进行运算,得到n个编码块c1,c2,…,cn,并保证使用这n个编码块中的任意k个都能解码恢复出原始数据对象O.2.1纠删码的基本原理目前,存储系统采用的纠删码都是线性的,本文也只关注线性纠删码.如式(1)所示,在线性纠删码中,每个编码块ci都可表示成所有数据块的线性组合.其中gij∈Fq,Fq是包含q个元素的伽罗瓦域(GaloisField).(gi1gi2…gik)×与编码过程相同,在线性纠删码中,失效块的修复也是通过对可用块进行线性组合来完成.可用块的选择以及相应组合系数的计算由具体的纠删码设计决定.例如,对于经典的RS码[17],修复任何一个块都需要下载k个块;对于近来提出的改进型纠删码,如LRC(LocallyRepairableCode)[20]等,在多数失效情况下进行修复需要的块数目都小于k,只在多块失效等少数情况下才会大于k.2.2纠删码的相关概念目前,对存储系统中纠删码容错技术的相关概念尚无一致的定义.为了便于本文的描述与理解,现对本文常用的相关概念明确如下:(1)数据块(DataBlock).用户存储到系统中的原始数据对象被系统划分后产生的块.(2)编码块(CodedBlock).数据块经过编码算法运算后产生的所有块.(3)条带(Stripe).独立地与一个纠删码算法相关的所有编码块所构成的集合.例如,一个(n,k,k)-纠删码的条带包含n个编码块.(4)系统码(SystematicCode).如果一个纠删码产生的编码块包含条带内的所有数据块,即满足{o1,o2,…,ok}{c1,c2,…,cn},则称该纠删码为系统码.对于系统码,在条带内没有编码块失效时,原始数据对象可以直接读取,而无需解码.这个性质对存储系统至关重要.因此,用于存储系统的纠删码几乎全都是系统码.在没有特殊说明的情况下,后文所讨论的纠删码都指系统码.(5)精确码(ExactCode).如果一个纠删码进行数据修复时所恢复出的编码块与丢失的编码块在内容上完全相同,则称这一纠删码为精确码.任何线性精确码都可转化为系统码[21].因此,用于存储系统的纠删码也几乎全部都是精确码.在没有特殊说明的情况下,后文所讨论的纠删码都指精确码.(6)校验块(ParityBlock).系统码产生的所有编码块中除数据块以外的部分称为校验块.在不需要区分数据块和校验块时,后文统一用编码块或块来指代.(7)容错能力(FaultTolerance).一个条带可以容忍的最大任意块失效数目.假设一个纠删码的容错能力为t,则此纠删码能且只能在任意不多于t个块失效的情况下,恢复出原始数据.一个(n,k,k)-纠删码的容错能力为t=n-k.(8)MDS码(MaximumDistanceSeparableCode).如果一个(n,k,k)-纠删码满足k=k,则称此纠删码满足MDS性质,也称该纠删码为MDS码.MDS码可以用更简单的二元组(n,k)来表示.MDS码在相同的容错能力下拥有最小的存储空间开销.(9)数据修复(DataRepair).当一个条带中的一个或多个编码块因节点或磁盘失效等原因而丢失时,利用条带内剩余的编码块恢复出丢失的编码块的过程称为数据修复.3主要挑战采用纠删码进行数据容错不仅可以极大地降低系统的存储空间消耗,甚至同时还可以提供更高的数据可靠性.早期纠删码主要应用在基于RAID[3]的存储系统中.由于以前芯片的运算能力有限,早期关于纠删Page4码的研究主要关注其计算效率问题.传统纠删码的复杂性主要来源于编解码过程中大量伽罗瓦域上的运算.为此,涌现出了众多只需进行异或运算的纠删码,极大降低了计算的复杂度.该类纠删码包括由RS码[17]优化而来的柯西RS码[22],用于RAID的EVENODD码[23]、RDP(Row-DiagonalParity)码[24]、X-Code[25]、Liberation码[26]、Liber8Tion码[27]、STAR码[28]、F-Code[29]、P-Code[30]、T-Code[31]和一般化的X-Code[32]等.纠删码容错技术在大规模分布式存储中面临的主要挑战已不再是其较高的运算复杂度,而是其较高的网络资源消耗,以及实现上的复杂性.一方面,随着CPU运算能力的飞速发展,现在普通商用服务器的运算能力对于大部分存储系统已经严重过剩.例如,百度云盘的云端存储系统Atlas甚至由于普通处理器利用率过低而采用了ARM处理器[33].另一方面,与RAID的集中式操作不同,分布式系统的分布特性使得纠删码容错的相关操作需要多个节点相互协作,不可避免地带来大量的数据传输,占用较多的网络资源.一直以来,网络都是分布式系统中的稀缺资源,往往是整个系统性能的瓶颈所在.此外,分布式系统的特性也增加了实现的难度.具体来说,大规模分布式存储系统中纠删码容错技术面临的挑战主要表现在编码实现、数据修复和数据更新等3个方面.3.1编码实现编码实现是指根据给定的纠删码对数据块进行运算、得到校验块并将数据块和相应的校验块分散到不同存储节点上的过程.其主要挑战在于如何在编码实现完全完成之前保证数据的可靠性.一方面,由于存储系统一般以大小固定的块为基本管理单位,数据产生只有累积到一定量才能启动编码实现;另一方面,由于编码实现的复杂性,其编码过程需要花费一定的时间.因此,在将数据块和校验块分散到相应的存储节点之前都有可能发生数据失效,造成数据丢失.此外,编码过程中数据块的读取和校验块的分发也会产生大量的数据传输,占用网络资源.总之,编码实现不仅要考虑数据可靠性还要尽量降低数据传输量.3.2数据修复数据修复的挑战主要在于如何降低数据读取量和数据传输量.与多副本容错技术只需重新拷贝一个副本不同,采用纠删码容错的分布式存储系统进行数据修复时,需要从多个存储节点下载数据并对这些数据进行编解码运算.对于大部分纠删码来说,这些运算是如式(2)所示的线性组合.一个编码块失效,需要使用多个其它编码块进行修复,过程中需要读取并传输大量的数据.这不仅会影响数据修复的速度而且会对数据访问效率产生明显的影响.3.3数据更新数据更新的挑战主要在于如何降低数据读写量和数据传输量.在基于多副本容错的数据更新中,仅需用新的数据依次覆盖各个副本中的原数据即可.在基于纠删码容错的数据更新中,需要先将数据块中校验块中更新部分的原数据读取出来,重新计算相应的校验数据,然后再写入.在纠删码容错中一个数据块关联的校验块较多,需要更新的编码块也更多,这必将产生较大的数据读写量和数据传输量,直接影响到数据更新即覆盖写操作的性能.此外,更新过程中节点也可能失效,造成部分数据已更新而其余部分未更新,从而导致数据不一致.因此,保证数据的一致性也是纠删码容错技术中数据更新的主要挑战之一.4编码实现技术研究在采用纠删码容错的分布存储系统中,需要对新写入的数据进行编码,为了保证数据可用性,通常先利用复制技术保存新数据,等到编码完成后再将多余的副本删除.当前,采用这种方式的分布存储系统包括Facebook公司的HDFS-RAID[34]、微软的云存储系统WAS(WindowsAzureStorage)[35]、百度云盘的存储系统Atlas[33]和阿里的TFS(TaobaoFileSystem)①等.具体的编码实现方法包括集中式编码实现方法和分布式编码实现方法.4.1集中式编码实现方法目前,实际系统中大多采用集中式编码实现方法.如图1所示,在集中式编码实现方法中,一个条带的所有编码工作都由一个编码节点来单独完成.编码节点从存储节点下载条带中的所有数据块,编码计算校验块,然后再把校验块发送到其它存储节点上.在具体实现中,可以从存储数据块的节点中选择一个作为编码节点,以便减少传输数据量,降低网络资源开销.①http://tfs.taobao.org/Page5HDFS-RAID[34]、Atlas[33]、WAS[35]和TFS等分布式存储系统均采用集中式编码实现方法.其中,HDFS-RAID和TFS在编码实现时,编码节点从多个数据节点下载数据块进行编码;Atlas和WAS在编码实现时,将存储在一个数据节点上的大数据块划分成较小的块进行编码,然后把划分的数据块和编码产生的校验块分发到不同的数据节点上.Atlas和WAS在编码之前不需要下载数据,但是其在编码完成后需要发送大量的数据.总体而言,Atlas和WAS的数据读取量和传输量与HDFS-RAID和TFS相等.但是,Atlas和WAS的数据写入量远高于HDFS-RAID和TFS.以常见的(14,10)-RS码为例,Atlas和WAS的数据写入量是HDFS-RAID和TFS的3.25倍.集中式编码实现方法的优点是简单易于实现,其缺点是存在较为严重的性能瓶颈.编码节点负责编码计算和分发编码块,易产生计算瓶颈和网络传输瓶颈,从而影响编码实现效率.4.2分布式编码实现方法为了解决集中式编码实现方法的性能瓶颈问题,Pamies-Juarez等人[36]针对RapidRaid码的特点,提出了基于流水线的分布式编码实现方法.RapidRaid码是Pamies-Juarez等人设计的一种非精确码.该方法将RapidRaid码的编码计算分解为可以在不同节点上并行执行的子任务,将编码实现的网络传输负载和计算负载均衡分布到多个节点上,采用流水线方式进行编码计算.如图2所示,该编码方法将参与编码实现的节点构成一条流水线,节点i接收前一个节点i-1发送的数据xi-1,i,将xi-1,i和自己所存储的数据块oi进行组合,分别生成自己最终需要存储的编码块ci和供节点i+1编码使用的数据xi,i+1,并将xi,i+1发送出去.针对RapidRaid码的分布式编码实现方法解决了集中式编码的负载不均衡问题,但是,该方法也存在许多不足之处:(1)该方法只适用于Pamies-Juarez等人提出的RapidRAID码[36],通用性较差;图2RapidRAID码的分布式编码实现方法[36](2)RapidRAID既不是系统码,在大多数情况下也不是MDS码,数据读取性能较差且空间利用率不高;(3)与集中式编码实现方法相比,该方法没有减少编码实现过程中的计算量或传输的数据量,并且将读取的数据量提高了1倍.为了减少编码实现过程中的数据传输量,Pamies-Juarez等人[37]进一步提出了一种新的分布式编码实现方法.该方法采用流水线思想,只有最终存储校验块的节点参与编码实现,且流水线可能需要循环多次;需要特殊的数据块放置策略将多余的数据块均匀放置到最终存储校验块的节点上,以使参与编码实现的节点上有多个数据块.文献[37]中的方法虽然在一定程度上降低了编码实现过程中传输的数据量,但是它需要根据要产生的校验块数目和数据块的分布情况来构造相应的纠删码.此外,该方法需要参与编码实现的节点上有较多的数据块.否则,流水线需要循环多次才能达到较高的可靠性,从而增加了编码实现过程中的计算量.例如,为了获得(10,6)-MDS码,当参与编码实现的4个节点上有12个数据块时,流水线只需循环1次;而当参与编码实现的4个节点上只有6个数据块时,流水线则需要循环2次.循环2次时,该方法虽然可将网络传输量减少约22%,但是会增加约79%的乘法运算和60%的加法运算.4.3总结总体来说,相对于集中式编码实现方法,现有分布式编码实现方法可以提升编码实现的性能,但提升效果较为有限.一方面,集中式编码虽然具有一定的性能瓶颈,但是实际上其性能瓶颈并没有那么严重.因为这里的集中仅指一个条带的编码工作,不同条带的编码仍然可以由不同的节点并行地完成,在Page6并行度较高时,各个节点负载相差得并不多.另一方面,现有分布式编码实现方法一般对纠删码或数据放置等有严格要求,并且数据传输方面的提升往往以更多的数据读取量或计算量为代价.5低修复开销纠删码研究修复开销是由纠删码的特性决定的.因此,设计新型纠删码是从根本上降低修复开销的重要途径.根据不同的编码结构,低修复开销纠删码可以分为分组码和再生码两大类.5.1分组码传统MDS码(如RS码[17])数据修复开销大的根本原因是其条带内的每个校验块都与所有数据块相关,导致任何一个块失效都需要下载其它k个块才能修复.因此,如果将一个条带内的数据块分组,利用组内的数据块产生局部校验块,就可以降低数据修复时需要下载的数据量.以该思想为基础构造的纠删码称为分组码.根据不同的分组方法,分组码可进一步分为层次分组码和交叉分组码两类.5.1.1层次分组码层次分组码的分组呈现明显的层次.首先将一个条带内的数据块分成少数几个较大且互不重叠的组,然后每个组再进一步分成几个若干互不重叠的子组.以此类推,可以根据需要划分多个层次.最后,每组各产生一个只与组内数据块相关的局部校验块,整个条带再产生若干与条带内所有数据块相关的全局校验块.这样大部分数据失效就可以在组内进行修复,从而降低成本.最基本的层次分组码是两层分组码.图3显示了微软云存储系统WAS[35]中采用的LRC码[20]的一个实例LRC(8,2,2).LRC(8,2,2)的一个条带包含8个数据块,平均分成两个组,每组各产生1个局部校验块,整个条带再产生2个全局校验块.可见,当组内只有1个块失效时,只需使用组内的其它4个块即可进行修复.只有在一个组内有多个块失效时,才需要利用组外的块来修复.考虑到超过98%的失效情况下,条带内都只有一个块失效[38],LRC可以有效降低数据修复的成本.在组内有多个块失效时,两层分组码仍然需要在全局范围内进行修复.为此,可以对数据块进行更多层次的分组,高层的组包含若干低层较小的组.Pyramid码[39]就是根据该思想构造的多层次分组码.随着分组层次的增多,多层分组码的平均修复开销会降低,但其存储空间开销会增大.因此,多层分组码可以在修复开销与存储空间开销之间灵活地进行权衡.除了LRC码[20]和Pyramid码[39],层次分组码还有Facebook采用的LRCs码[11]和EXPyramid码[40].与LRC码相比,LRCs码的主要不同之处是其对所有全局校验块再产生一个局部校验块,降低了全局校验块的修复开销.EXPyramid码将两层Pyramid码的数据块改为阵列结构,在横向和纵向都产生局部校验块,进一步降低了数据修复的成本.5.1.2交叉分组码交叉分组码与层次分组码的不同之处有两点:(1)交叉分组码中各组包含的数据块数目基本相同,且各组包含的数据块有一部分相同,而层次分组码不同层级的分组大小差别很大,每个高层的分组会包含若干较低层次的组;(2)交叉分组码一般不存在全局校验块.图4是Miyamae等人[41]提出的交叉分组码SHEC(ShingledErasureCode)的一个实例SHEC(10,6,5).SHEC的基本思想是使各个分组像屋顶的瓦片一样相互重叠,从而在多个块失效时可以利用较少的组进行局部修复.图4中的SHEC(10,6,5)将10个数据块分为相互重叠的6个组,每组包含5个数据块,除了第3组和第4组,其它任意两组之间重叠3个数据块,每组的校验块均通过组内的5个数据块运算产生.具体的编码过程如式(3)所示:p1=α1,1o1+α1,2o2+α1,3o3+α1,4o4+α1,5o5烄p2=α2,1o3+α2,2o4+α2,3o5+α2,4o6+α2,5o7p3=α3,1o5+α3,2o6+α3,3o7+α3,4o8+α3,5o9烅p4=α4,1o6+α4,2o7+α4,3o8+α4,4o9+α4,5o10p5=α5,1o8+α5,2o9+α5,3o10+α5,4o1+α5,5o2烆p6=α6,1o10+α6,2o1+α6,3o2+α6,4o3+α6,5o在此实例中,单个块的失效修复只需下载5个块;两个块的失效修复只需下载6或7个块.例如,当图2中的o6和o9失效时,根据式(3)中的第3和第4个等式,使用o5,o7,o8,o10,p3和p4这6个块即可修复.可见,SHEC避免了下载数据量随着失效块数增加而急剧增长的情况.Page7图4SHEC(10,6,5)的构造和修复示意图[41]交叉分组码还有WEAVER码[42]、GRID码[43]和Hover码[44]等.WEAVER码先限定所有数据块的出度(一个数据块关联的校验块个数)和校验块的入度(一个校验块关联的数据块个数),然后通过搜索的方法找出最优的组合和放置方法,使容错性达到最高.WEAVER码在固定数据修复开销前提下可构造出容错能力高达12的实例.但是WEAVER码的冗余度较高,空间利用率始终不超高50%.如果想要同时维持较低的修复开销和较高的容错能力,甚至需要3倍或4倍的存储空间开销.GRID码和Hover码都是纵横相结合的阵列码,可以达到较高的容错能力,同时也具有较高的存储空间利用率.5.2再生码再生码(RegeneratingCodes)[45-46]是一种基于网络编码思想[47]设计的纠删码,由Dimakis等人[48]在2007年首次提出.其基本思想是通过适当增加冗余并且使新生节点从尽量多的节点下载数据来降低修复需要下载的总数据量.再生码具有两个明显的特点:(1)再生码的数据块和校验块都包含相同数量的子块,编码与修复时以子块为基本单位,子块之间的关系也更为复杂;(2)再生码在进行数据修复时,新生节点需要从尽量多的节点来下载数据.再生码一般用三元组[n,k,d]来表示.[n,k,d]-再生码的一个条带包含n个编码块,可以容忍任意n-k个块失效,进行数据修复时新生节点可以连接d个存活节点下载数据,其中kdn-1.另外,再生码还有3个常用的辅助参数α,β和B,分别表示单个编码块包含的子块个数,连接到d个节点进行数据修复时从单个节点下载的子块个数和一个条带包含的所有数据子块个数.图5显示了一个简单的[4,2,3]-再生码及其数据修复过程.在该再生码中,原数据对象被分割为4个大小相等的数据子块o1,1,o1,2,o2,1和o2,2,每两个数据子块合成一个数据块,一个校验块也包含两个校验子块.校验子块完全通过异或运算产生,校验子块和数据子块之间的关系如图5所示.当第2个校验块失效时,先将子块o2,1和o2,2在其节点内相异或得到o2,1+o2,2,然后o2,1+o2,2与o1,2+o2,2相异或恢复出失效的o1,2+o2,1,o1,1与o1,2+o2,2相异或恢复出失效的o1,1+o1,2+o2,2.在一般的(4,2)-纠删码中,新生节点需要从2个节点下载相当于4个子块的数据.而在图3的再生码中,新生节点虽然连接了3个节点,却只需下载o1,1,o2,1+o2,2和o1,2+o2,2这3个子块,将下载量减少了25%.实际上,Dimakis等人[48]证明,如果新生节点能够从n-1个节点下载数据,再生码可以将总体数据下载量降低高达84%.图5一个[4,2,3]-再生码及其数据修复过程再生码的研究主要关注MBR码(MinimumBandwidthRegeneratingCodes)和MSR码(Mini-mumStorageRegeneratingCodes).MBR码具有最低的数据修复带宽,MSR码具有最低的存储开销.再生码同样分为精确码和非精确码,本文只关注精确再生码.Dimakis等人[48]提出了再生码的概念并证明了再生码修复带宽的下界,但是没有证明达到这个下界的再生码是否存在,也没有给出构造这种再生码的具体方法.2009年,[n,2,n-1]的精确MSR码被证明是存在的[49].同年,[n,k,n-1]的精确MBR码被构造出来[50].2010年,[n=d+1,k,d2k-1]的精确MSR码被构造出来.2011年,Rashmi等人[21]利用矩阵乘的方法构造出了[n,k,d]的精确MBR码和[n,k,d2k-2]的精确MSR码,并证明不存在d<2k-2的精确MSR码.至此,所有存在的MBR码和MSR码都可以用统一的方法被构造出来.再生码可以极大地减少修复时的传输数据量,但是需要读取的数据量却更大.在数据修复过程中,参与修复的节点需要把自己存储的所有数据都读取出来进行组合.由于MBR码需要存储的数据量更Page8大,所以修复时需要读取的数据量比传统纠删码多,这不仅增加了系统的磁盘负载,也限制了修复的速率.虽然MSR码存储的数据量和传统MDS码相等,但是修复时需要从多于k个节点下载数据,所以其读取的数据量也比较多.针对上述问题,研究者设计了RBT(Repair-by-Transfer)MBR码[51-52].RBTMBR码在数据修复时只传输数据而不进行任何数学运算,使需要读取的数据量和需要传输的数据量相同.该类再生码最先由Shah等人[51]提出.其构造方法是:先用任何一种(n(n-1)/2,B)-MDS码对条带中的B个数据子块进行编码,产生n(n-1)/2个编码子块.然后,将产生的n(n-1)/2个编码子块分别对应于一个包含n个顶点的全连接图的n(n-1)/2条边,该图的每个顶点分别对应一个存储节点,如图6(a)所示,每个节点存储与其对应的顶点相连的所有边上的编码子块,如图6(b)所示.从图6(b)可以看出,当任何一个节点失效时,只需从其余节点分别读取并传输一个编码子块就可以恢复出失效数据,不需要进行任何数学计算.图7包含4个数据磁盘的RDP码的编码结构与数据修复示意图Wang等人[55]和Xu等人[56]分别将Xiang等人[54]的思想引入到EVENODD码[23]和X-Code码[25]的修复中,通过同时使用多个校验磁盘来降低针对上述构造方法存在计算复杂度较高的缺点,Lin等人[52]提出了一种新的构造方法.此构造方法也采用Rashmi等人[21]提出的矩阵乘框架,可将编码运算需要的伽罗瓦域大小从n(n-1)/2减小为n,将编码运算的复杂度从Ο(n4)降低到Ο(n3).近来,Rashmi等人将RBTMBR码的思想引入到MSR码中,提出了一种近似Repair-by-Transfer的MSR码,称为PM-RBT(Product-MatrixRBT)码[53].由于MSR码的冗余度较低,PM-MSR码只能减少部分数据修复时需要读取的数据量.此外,大部分阵列码在结构上也具有再生码的特点,可以通过从尽量多的节点下载数据来降低数据修复开销.此发现最早由Xiang等人[54]提出,用于优化RDP码[24]修复时下载的数据量.图7(a)显示了包含4个数据磁盘的RDP码[24]的编码实例,其中磁盘0、磁盘1、磁盘2和磁盘3为数据磁盘,磁盘4和磁盘5为校验磁盘.现假设磁盘0失效.传统的修复方法是使用磁盘1、磁盘2、磁盘3和磁盘4上的数据进行修复,即分别使用d1,i,d2,i,d3,i和p0,i修复d0,i,i=0,1,…,3.这需要访问4个磁盘并下载16个子块.Xiang等人指出,访问5个磁盘可以减少总下载数据量.如图7(b)所示,可以先用d1,0,d2,0,d3,0,p0,0和d1,1,d2,1,d3,1,p0,1分别修复出d0,0和d0,1,然后使用d2,0,d1,1,p0,3,p1,2和d3,0,d2,1,d1,2,p1,3分别修复出d0,2和d0,3.虽然访问了5个磁盘,但是只需下载d1,0,d2,0,d3,0,p0,0,d1,1,d2,1,d3,1,p0,1,p0,3,d1,2和p1,3等共计12个块即可完成修复,将总数据下载量减少了25%.这些纠删码的数据修复开销.Khan等人[38]进一步将此问题一般化,使其适用于任何基于异或运算的纠删码.基于异或运算纠删码的每个块均有多个大Page9小相等的子块构成,校验子块通过某些数据子块相异或而产生.由于数据子块与校验子块之间的关系较为复杂,在一个块失效时,其中的每个子块都可能有多种修复方法.Khan等人[38]提出了一种通用的搜方法,来查找出使总下载数据量最小的修复方法.基于此思想,Khan等人[38]还提出了容错能力分别为2和3的RotatedRS码.Khan等人[38]用他们提出的搜索算法分析了多种基于异或运算纠删码的性能.结果表明,对于常见的基于异或运算纠删码,上述方法可将总数据下载量减少20%左右.然而,Khan等人[38]提出的搜索算法复杂度极高,为了选出最好的组合不得不穷尽所有的选择.为了解决这一问题,Khan等人[57]和Zhu等人[58]分别提出了近似最优的搜索方法.基于异或运算的纠删码和RotatedRS码的容错能力都比较有限,上述思想对这些纠删码的优化效果也较为有限.近来,Rashmi等人[12,59-60]提出了一种称为Hitchhiker的MDS码.Hitchhiker码可基于任何MDS码进行构造.Hitchhiker码的一个条带由两个普通MDS码条带构成.其基本编码思想是:每个子条带分别先用普通的MDS码进行编图8Hitchhiker码的基本编码框架和编码实例[59](n=14,k=10)相比于基于异或运算的纠删码和RotatedRS码,Hitchhiker码能够提供任意的容错能力,并且其可将数据修复所需下载的数据量降低高达45%.5.3总结5.3.1数据编码性能对比影响纠删码数据编码性能的因素有编码算法的时间复杂度以及编码过程中需要读取、传输和写入的数据量.纠删码的编码运算主要是有限域上的加法和乘法运算,其中较为费时的是乘法运算.所以,乘法运算的数量可以用来表征编码算法的复杂度.此外,编码,然后对其中一个子条带中的数据计算一些检验子块,并把这些校验子块与另一个子条带中的子块相合并,如图8(a)所示.图8(a)中的f1和g1等表示计算校验的函数,其中f1,f2,f3和f4表示采用的普通MDS计算校验的函数.例如,f1(犪)表示子条带1的第一个校验子块.容易证明,不论g1,g2,…,g14是什么样的函数,Hitchhiker码都具有MDS性质.但若要降低数据修复开销,则需要对这些校验函数进行特殊的设计.Rashmi等人[59]给出了3种设计.图8(b)显示了其中一种,称为Hitchhiker-XOR码.Hitchhiker-XOR码的额外校验数据全部通过异或运算产生.现假设数据块1失效,即子块a1和b1失效,数据修复分为3步.第1步,使用b2,b3,…,b10和f1(犫)等10个子块修复出子块b1;第2步,使用b1和b2,…,b10等10个块计算出f2(犫);第3步,使用f2(犫),f2(犫)+a1+a2+a3,a2和a3等10个子块修复出子块a1.整个过程中总共需要读取b2,b3,…,b10,f2(犫)+a1+a2+a3,a2和a3等13个子块.普通的MDS码则需要下载相当于20个子块的数据.Hitchhiker-XOR码将数据下载量降低了35%.码使用的有限域的大小也对运算时间有重大影响.随着有限域的增大,乘法运算的复杂度成指数级增长.此外,对于较小的有限域,如8位256个元素的有限域,可以将所有可能的乘法运算结果保存在内存中,用查表的方法加快乘法运算速度.目前对于常见的参数,上述各类纠删码中较优秀者的编码运算基本可以在8位有限域上完成.在现代分布式存储系统中,真正决定纠删码编码性能的是需要读取、传输和写入的数据量.随着计算机运算能力的飞速增长,编码运算的速度已远远超过数据的读取、传输和写入速度.影响数据读取、Page10传输和写入量的主要因素是编码前数据的分布情况和采用的编码实现方法.此外,纠删码的数据冗余度也对运算量、数据传输量和写入量有重大影响.冗余度越高,意味着有更多的校验数据需要产生、发送出去并写入到磁盘中.表1从存储空间利用率以及编码单位数据平均需要的乘法运算量、平均数据读取量、平均数据传输表1几种典型纠删码的数据编码性能对比RS(14,10)LRCs(10,2,4)SHEC(10,6,5)62.50(9,5,8)-MSR(14,10,13)-MBR46.70(14,10)-Hitchhiker-XOR71.41从表1可以看出,在运算复杂性方面,RS码与LRCs码和Hitchhiker码这些以RS码为基础的纠删码基本相同.交叉分组码SHEC(10,6,5)完全的运算复杂度较低,但其容错能力也较低.(9,5,8)-MSR码和(14,10,13)-MBR码具有最高的运算复杂度.除了较为复杂外,再生码较高的运算复杂度也与其更高的数据冗余度有重大关系.在集中式编码实现方法下,除了数据读取量,数据传输量和写入量完全取决于纠删码的冗余度.因此,冗余度最低的RS(14,10)码和(14,10)-Hitchhiker-XOR码表现最好,冗余度最高的(14,10,13)-MBR码表现最差.5.3.2数据修复性能对比通常情况下,采用修复一个失效块平均所需下载的数据量来衡量纠删码的数据修复开销.将修复一个块平均所需下载的数据量与块大小之比称为单块修复代价,以Rs表示.单块修复代价在一定程度上反映了纠删码数据修复的开销,但无法反映整个系统数据修复的总体开销.不同冗余度的纠删码具有不同的存储开销.在原始数据量相同的情况下,系统采用不同开销的纠删码,其实际存储的数据量是不同的,从而导致数据修复的总体开销不同.假设系统存储的原始数据量为V,数据失效速率为λ,纠删码的存储空间利用率为η,则可计算出系统单位时间内的总体修复带宽F为从式(4)可以看出,在原始数据量和数据失效速率相同的情况下,纠删码的总体修复带宽正比于其单块修复代价Rs,反比于其存储空间利用率η.因量和平均数据写入量等方面,对比了几种典型纠删码在集中式编码实现方法下的编码性能.除了其中的(9,5,8)-MSR码和(14,10,13)-MBR码两个再生码以外,其它纠删码均已被应用在了实际的大型分布式存储系统中.表1中,SHEC(10,6,5)的容错能力为3,其它纠删码的容错能力均为4.111111此,可以用Rs与η之比来表示纠删码的总体修复代价.以Rc表示纠删码的总体修复代价,则有表2从存储空间利用率、单块修复代价和总体修复代价等方面对比了表1中几种典型纠删码的数据修复性能.作为对比,表2中加入了常见的三副本技术.表2几种典型纠删码与多副本技术的数据修复性能对比LRCs(10,2,4)62.503.756.00SHEC(10,6,5)62.505.008.00分组码(9,5,8)-MSR55.602.003.60(14,10,13)-MBR46.701.002.14(14,10)-Hitchhiker-XOR71.417.6410.70从表2可以看出,传统MDS码的存储空间利用率最高,但是其数据修复开销也最大,甚至高于其它种类纠删码数倍.相比于传统MDS码,分组码能够以较少的额外存储空间开销为代价,显著降低数据修复的成本.分组码也较容易实现,这也是其在大型存储系统中得到应用的重要原因之一.再生码可以极为有效地降低数据修复开销,但是再生码的存储空间利用率明显低于其它类别纠删码,其存储空间利用率最高也只能达到50%左右.所以,再生码不适合于对存储成本要求较高的大规模存储系统,而适合于对带宽成本极其敏感的系统.例如,可以将再生码用在数据中心级的数据容错中,因为数据中心之间的网络带宽极其昂贵.Page115.3.3总结分布式存储系统对纠删码的要求主要在容错能力、存储空间开销和数据修复开销3个方面.但这3个方面是相互排斥的,提升其中一个方面必然会牺牲其它方面.分布式存储系统中,纠删码的设计本质上都是在容错能力、存储空间开销和数据修复开销之间进行权衡.由于容错能力是分布式容错系统的基本要求,现有研究基本都是在保持容错能力的前提下,在存储空间开销和数据修复开销之间进行权衡.分组码就是此思路的典型代表.相比于分组码,再生码在理论上取得了重大突破,证明并达到了一定存储空间开销下修复时数据下载量的下界.不过早期再生码在数据下载量方面的减少是以增加数据读取量为代价的.RBTMBR码的提出,使MBR码在数据下载量和读取量都达到了最优.PM-RBT码的提出,缓解了MSR码在数据下载量方面的问题.6数据修复技术研究除了从纠删码本身入手降低数据修复的代价之外,从数据修复的具体过程入手,通过优化修复时的数据读取和数据传输过程也可以进一步提高数据修复的效率.传统的数据修复方法通常采用星型的数据传输方式,所有提供节点直接将数据发送给新生节点,所有参与修复的节点构成一个以新生节点为中心的星型结构.星型数据修复方法简单直观,但是其存在严重的性能瓶颈.现有的数据修复技术大部分都基于树型数据修复方法.系统会先构建覆盖所有参与修复的节点且以新生节点为根的修复树.在修复过程中,叶节点先将自己的数据乘以相应的系数后将其向上传输给自己的父节点,内部节点收取其所有子节点发送的数据并将这些数据和自己的数据进行一定的组合,然后再将组合结果传输给自己的父节点,以此类推,直至最终到达修复树的根节点.根节点将收到的所有数据进行组合后就可恢复出失效数据.图9(a)显示了树型数据修复方法的数据传输结构.其中,失效编码块c5可用编码块c1,c2,c3和c4来修复,其关系如式(6)所示.作为对比,图9(b)显示了传统星型数据修复方法中的数据传输结构.根据修复树构造方法的不同,现有数据修复技术可以分为两大类:一是带宽感知的数据修复技术,图9树型修复方法和星型修复方法的数据传输结构依据网络带宽来构建修复树;二是拓扑感知的数据修复技术,依据网络拓扑来构建修复树.6.1带宽感知的数据修复技术带宽感知的数据修复技术最早由Li等人[61]提出.其主要考虑到大规模分布式系统往往是异构的,即节点的性能不尽相同,节点之间的网络带宽也不同.此类方法试图根据网络带宽来构造修复树,通过尽量利用网络中的高可用带宽达到提高数据传输速度、缩短修复时间的目的.Li等人证明,采用以节点间可用网络带宽作为边权重的最大生成树作为修复树,可使数据修复时间达到最小.图10显示了传统星型数据修复方法和文献[61]中带宽感知的树型数据修复方法的对比情况.其中,原数据对象大小为M,分为3个数据块,通过编码产生个1校验块,3个数据块和1个校验块分别存储在节点2、节点3、节点4和节点5等4个节点上,各块大小均为M/3.假设节点4上的块失效,用节点2、节点3和节点5上的3个可用块进行修复,节点1为新生节点.图10(a)显示了各节点之间的可用网络带宽.图10(b)显示了采用星型数据修复方法时的数据传输情况.在此方法中,所有3个提供节点(节点2、节点3和节点5)都直接向节点1传输数据.星型修复方法的修复时间取决于可用节点与新生节点之间的瓶颈带宽,即节点1和节点3之间的带宽.每个可用节点与新生节点之间传输的数据量均为M/3.可以计算出,传统星型数据修复方法的修复时间为(M/3)/20=M/60.图10(c)显示了带宽感知的树型数据修复方法的数据传输过程,图10(d)显示了对应的修复树.根据Li等人的证明,该方法的修复时间取决于修复树中的最小带宽.此实例中的最小带宽为40.该方法中,节点之间传输的数据量也为M/3,可以算出其修复时间为(M/3)/40=M/120.可见,相比于星型数据修复方法,带宽感知的数据修复方法将修复时间缩短了一半.Page12图10星型修复方法和带宽感知的树型数据修复方法的对比Li等人[62]又将充分利用可用带宽的思想引入到再生码中,提出了针对再生码的树型数据修复方法RCTREE.为了更加有效地利用系统中的可用带宽,加速多节点同时失效情况下的数据修复,Sun等人[63]扩展了文献[61]中的数据修复方法,提出了一种带宽感知的并行数据修复方法TPR(Tree-struc-turedParallelRegeneration).当多个节点同时失效时,TPR方法会以各新生节点为根,分别构建多个修复树,并行地对失效节点进行修复.6.2拓扑感知的数据修复技术拓扑感知的数据修复技术的基本思想是通过构造与物理拓扑相符的修复树,以减少数据修复时在网络拓扑的高层链路上传输的数据量.目前,最常见的网络拓扑仍然为多层的树型结构[64],由下到上依次为由机架交换机(Top-of-Rack,TOR)组成的边界层(EdgeLayer),由聚合交换机组成的数据聚合层(AggregationLayer)和由核心交换机和路由器组成的核心层(CoreLayer).树形网络的突出问题是高层的带宽往往非常紧张,目前部署的网络中边界层的总带宽仍然为核心层的4~10倍[64-65].近来有关数据中心网络负载的研究[8,64,66]均表明,核心层链路的利用率是最高的.因此,如果能够有效减少核心层的带宽消耗,将极大提高系统整体的性能.针对此问题,Zeng等人[67]和Zhang等人[68]提出了拓扑感知的数据修复技术,以降低数据修复占用的核心网络带宽.这种数据修复技术的基本思想是,将距离较近的编码块(如处于同一个机柜中的编码块)先就近组合然后再发送到更远的节点进行进一步的组合,直至最终汇入新生节点,这样就可以逐步减少在网络拓扑高层中传输的数据量,降低核心带宽消耗,从而提高数据修复效率,并降低数据修复对整个系统性能造成的不良影响.图11显示了该修复方法的一个实例,其中存储校验块o1+o2+o3+o4的节点失效.修复过程中,交换机S1先将数据块o1,o2和o4进行组合,产生o1+o2+o4并将其发送给交换机S2.交换机S2再将o1+o2+o4和o3组合从而恢复出失效的校验块o1+o2+o3+o4.整个过程中,交换机S1和S2之间只传输了1个块.在传统的星型修复方法中,交换机S1和S2之间则需要传输3个块.可见,拓扑感知的树型数据修复方法能够有效降低网络拓扑中高层的数据传输量.6.3总结带宽感知的数据修复技术虽然在理论上非常吸引人,但是存在难以克服的缺点:(1)分布式系统中节点间的带宽是实时动态变化的,测试成本高且难以获得精确的结果;(2)该类技术只是将数据传输导向到较快的链路,并没有降低数据修复的负载,所以不能有效提升总体的数据修复效率.此外,很多研究工作涉及的网络模型也与实际网络不符.相对而言,拓扑感知的数据修复技术更加具有可操作性.但是,目前这类方法也存在比较明显的缺点.需要由交换机完成修复过程中的数据合并,交换机需要支持数据运算,也需要设计专门的底层通讯协议.这些问题限制了拓扑感知的数据修复技术在实际系统中的应用.7数据更新技术研究纠删码中一个数据块关联着较多的校验块,导Page13致数据更新需要同时更新较多的块,不仅需要大量的数据传输和写入,也使保持数据的一致性面临挑战.依据更新方式,可将现有纠删码容错技术中的数据更新方法分为3种:替换式更新方法、追加式更新方法和混合式更新方法.7.1替换式更新方法替换式更新方法[69-70]直接用新的数据覆盖原有数据,完成对数据块和校验块的更新后才返回更新成功.图12显示了一个替换式更新方法的实例.其中,节点1和节点2存储数据块,节点3存储校验块,a,b,c和d分别表示4个原始数据块,a+c和b+d分别表示2个校验块,a,b和c分别表示对原数据块a,b和c中的一部分进行更新的新数据子块.可以看出,替换式更新方法直接更新了所有的数图13追加式更新方法的数据更新实例相比于替换式更新方法,追加式更新方法避免了数据更新过程中对原校验数据的读取,追加式的写入也将随机写转化为了顺序写,这些都有利于提高数据更新的效率.但是,追加式更新方法存在存储空间消耗较多和数据访问性能较差等不足:(1)追加式更新方法需要同时保存原数据和新数据与原数据的差值,这必然会消耗更多的存储空间.当数据更新频繁发生时,该问题会更加严重;(2)经过更新后,原来数据块中连续的数据将随机地分布在日志中,导致无法顺序读取数据,明显影响了数据访问的效率.此外,访问校验块需要先合并数据,这影响了数据修复的性能[71-73].7.3混合式更新方法混合式更新方法[74-75]是替换式更新方法和追加式更新方法的结合.具体来讲,混合式更新方法采用替换式更新方法对数据块进行实时更新,同时采据块和校验块.替换式更新方法能够保证数据块与校验块实时更新,对提高系统的性能有重要作用.数据块的实时更新能够保证应用访问到的数据始终是最新的,有利于提高应用运行的效率和准确性.校验块的实时更新能够保证访问的校验数据始终是最新的,有利于提高数据修复的性能.但是,替换式更新方法存在数据读写量大和数据传输量大等不足,对数据更新效率产生显著影响.7.2追加式更新方法追加式更新方法先将新数据以日志的形式追加在原始数据块之后,将新数据与原数据的差值以日志的形式追加在校验块之后;一段时间之后或需要访问新的数据块或校验块时再将追加的数据与原数据合并.图13显示了一个追加式更新方法的实例.更新过程中,先读取被更新的数据,并计算新数据a,b和c与被更新部分数据的差值Δa,Δb和Δc,然后将新数据a和c追加到节点1上,将新数据b追加到节点2上,将新数据a,b和c与被更新部分数据的差值Δa,Δb和Δc追加到校验节点上.GFS[5]和Azure[35]采用的是追加式更新方法.用追加式更新方法对校验块进行延迟更新.图14显示了一个混合式更新方法的实例.更新过程中,先读取被更新的数据,并分别计算新数据a,b和c与被更新部分数据的差值Δa,Δb和Δc,然后用新数据a,b和c分别覆盖被更新的部分数据,并将新数据a,b和c与被更新部分数据的差值Δa,Δb和Δc追加到校验节点上.相比于追加式更新方法,混合式更新方法对数据块的实时更新使得对数据块的读取可以顺序进行,提高了数据读取效率,也一定程度上降低了存储空间的消耗.但是,混合式更新方法对数据块的写操作是随机的,影响了数据更新的效率.在不考虑数据合并的情况下,混合式更新方法的数据读写量和传输量与追加式更新方法完全相等.此外,由于在访问校验块时仍然需要先合并数据,混合式更新方法也同样存在数据修复性能差等不足.Page14图14混合式更新方法的更新过程示意图传统的混合式更新方法[76-77]由于没有为追加的数据差值预留空间,追加的数据差值实际上是随机分布在磁盘上的,导致合并数据时产生大量的随机读操作.针对该问题,Chan等人[78]提出了一种预留空间的混合式更新方法PLR(ParityLoggingwithReservedSpace).PLR在每个校验块后预留一定的空间,使追加的数据可以连续地分布在磁盘上.这样合并数据时就无需多次访问磁盘,只需在连续的空间读取一次即可,既减轻了磁盘的负载也提高了数据修复的效率.PLR的不足之处是预留空间导致增加了存储空间的消耗.为此,Chan等人[78]设计了一种负载感知的预留空间动态调整方法.对于以追加的方式处理更新的系统,需要建立有效的索引信息以便快速定位原数据的更新数据.由于数据更新的位置和大小是随机的,一般把数据划分成大小固定的数据片(如文件系统的chunk)作为基本单位来管理更新信息.在磁盘阵列中常用的索引结构是哈希日志列表[75,79].如图15所示,对于每个被更新的数据片,都会存在一个与之对应的列表来保存其每次更新的信息.图15中,LBA(LogicalBlockAddress)表示每次更新追加的数据的位置,Hash_pre和Hash_next分表表示前一次更新和下一次更新的索引信息的指针.因此,通过Hash_pre和Hash_next可以快速遍历一个数据片的所有更新记录.日志列表适用于数据量较小的系统.在大规模分布式存储系统中一般使用键/值存储系统来保存追加数据的索引信息.例如,PLR[78]使用基于MongoDB①的键/值存储系统.图15保存追加数据索引信息的日志列表结构另外,Pei等人[80]提出使用树型结构传输更新数据,通过感知机架位置,减少数据更新过程中机架间的数据传输量,提高更新效率.7.4总结表3显示了上述3种更新方法的对比情况.其中,读取量、写入量和传输量分别表示采用(14,10)-RS码时更新单位数据平均产生的数据读取量、数据写入量和网络传输量.RL和RH分别表示追加式更新方法和混合式更新方法因数据合并产生的数据读取量,WL和WH则分别表示这两种方法因数据合并产生的数据写入量.由于混合更新方法不需要合并数据块,在合并频率相同的情况下,混合式更新方法的数据读写量优于追加式更新方法,即有RL>RH和WL>WH.从表3可以看出,替换式更新方法对数据块和校验块进行实时更新的特点,提高了数据访问和修复的效率,但是该方法的数据读取量非常大,明显降低了更新效率.追加式更新方法通过追加的方式显著降低了数据读取量,提高了数据更新的效率,但增加了存储空间消耗,并降低了数据读取效率.混合式更新方法对数据块进行实时更新,对校验块进行追加更新,比追加式更新方法具有更高的数据访问性能和更低的存储空间消耗.表33种数据更新方法的对比(犚犔>犚犎,犠犔>犠犎)更新方法读取量写入量传输量读操作写操作替换式554连续随机追加式1+RL5+WL4随机顺序混合式1+RH5+WH4连续随机+顺序8其它研究8.1带扇区容错的纠删码带扇区容错的纠删码是针对磁盘扇区出错和SSD存储单元易磨损,且这些错误很难发现的问题提出的.整个磁盘或节点的失效非常容易被探测到,但是存储介质基本单元的失效只有通过全盘扫描才能发现.庞大的数据量使得无法频繁进行这样的扫描,从而使得系统中存在大量的“隐含”失效.当系统①http://www.mongodb.orgPage15探测到磁盘或节点失效时,就有可能因“隐含”失效过多而导致某些数据无法修复.图16显示了一种“隐含”失效导致(6,4)-RS码无法修复的情况.其中整个磁盘0失效,但是因为磁盘3和磁盘4的相同位置均存在“隐含”的失效扇区,导致扇区d0,0无法修复,同时磁盘3和磁盘4上的失效扇区d3,0和p0,0也无法修复,造成数据永久丢失.图16扇区失效导致(6,4)-RS码无法修复情况示意图应对“隐含”失效的简单方法是采用容错能力更强的纠删码.但是这种方法的存储成本极高,每一个额外扇区的容错能力都要求增加一个额外的冗余磁盘.针对上述问题,Plank等人[81-82]提出了SD(Sector-Disk)码.SD码能够容忍任意m个磁盘和任意s个扇区同时失效.如图17所示,除了m个校验磁盘外,SD码任意s个扇区的容错能力只需额外增加s个扇区的校验数据,存储成本极低.SD码对失效扇区的分布没有任何要求.但是SD码的m和s不能任意取值,只有在m和s的取值满足一定条件的情况下,SD码才能被构造出来.并且,SD码的构造需要进行穷尽搜索.更严重的问题是,SD码往往需要在很大的有限域上进行构造,极大地增加了编码和数据修复的复杂度.PMDS(Partial-MDS)码[83]也是带扇区容错的MDS码,是SD码的子集.针对SD码存在的问题,Li等人[84]提出了STAIR码.STAIR码同样只需s个扇区的额外校验数据即可提供s个扇区的容错能力.与SD码能容忍任意s个扇区失效不同,STAIR码限制了包含失效扇区的磁盘个数和每个磁盘包含失效扇区的最大个数.但是,STAIR码对任意的m和s均存在,并且STAIR码可以使用统一易行的方法正向构造出来.此外,STAIR码的构造也不需要在很大的有限域上进行,编码和修复的复杂度更低.然而,上述几种带扇区容错纠删码的数据更新成本极高.这些纠删码中校验磁盘上的每个扇区和每个专门的校验扇区都与所有的数据扇区相关.这使得更新任意一个数据扇区都要同步更新所有的校验磁盘的所有扇区和所有校验扇区,需要传输大量的数据并进行大量的运算.8.2混合使用多种纠删码不同纠删码具有不同的存储空间开销和数据修复开销.一般情况下,存储空间开销越高的纠删码其数据修复开销越低;反之亦然.混合使用多种纠删码可以同时发挥多种纠删码的优势.Xia等人[85]提出通过跟踪数据的访问频率来动态调整纠删码的参数,以达到既降低存储空间开销又提高失效数据访问速度的目的.有些纠删码存储空间利用率较高,但数据修复速度很慢;有些纠删码存储空间利用率较低,但是数据修复速度较快.我们知道,对系统中数据的访问往往是不均衡的.有些数据很“热”,经常被访问;有些数据则很“冷”,很少被访问.数据的“冷”和“热”也是动态变化的.当数据较“热”的时候使用存储空间开销较大但数据修复较快的纠删码;当数据较“冷”的时候则使用数据修复较慢但存储空间开销较低的纠删码.这样整体上既能保证较快的失效数据访问速度也能保持较低存储空间开销.但是,如果使用完全不同种类的纠删码,每次动态转换都相当于完全重新进行了一次编码,成本极高.较好的方法是在同一种纠删码的不同参数之间转换,这样可以减少转换需要重新产生的校验数据以及数据的读取和传输.例如,可以采用如LRC码[20]这样的分组码,通过调整分组的大小来调整数据修复的速度和存储空间开销.调整的过程中,全局校验块完全不需要重新产生.图18显示了LRC(12,6,2)码和LRC(12,2,2)码的相互转化过程.从LRC(12,2,2)码转化为LRC(12,6,2)码只需将6个局部校验块合并为2个即可;从LRC(12,6,2)Page16码转化为LRC(12,2,2)码也只需读取2/3的数据块和2个局部校验块.图18LRC(12,6,2)码和LRC(12,2,2)码的相互转化除了多种纠删码混合使用,多副本也经常和纠删码一起使用.最常见的使用方式是先用多副本保存新产生的数据,过一段时间后再转换为纠删码.这种方式不仅简化了编码实现,也提高了数据的访问速度,因为新产生的数据往往是被频繁访问的“热”数据.HDFS-RAID[34]、微软的云存储系统WAS[35]、百度云盘的存储系统Atlas[33]和阿里巴巴的TFS等系统都使用了这种方式.另外一种常见的混合使用方式是,用多副本保存那些被频繁访问、对性能有重大影响且规模小的数据,用纠删码保存规模庞大的数据.例如,Zhang等人[86]在提出在内存键/值存储系统中使用多副本保存规模小且极为分散的元数据和“键”,用纠删码保存规模较大的“值”.TFS文件系统也使用了这种方式.此外,TFS文件系统还使用多副本与纠删码相结合的方式来处理数据更新.TFS文件系统直接将新的数据追加到数据块和校验块之后.更新时不用读取原数据,读取新数据时也不用合并数据.8.3磁盘失效预测近年来,出现的应对数据修复挑战的最新思路是磁盘失效预测.通过预测磁盘的失效,可以提前备份即将失效磁盘上的数据,从而将数据修复转化为复制,不仅可有效降低数据修复开销也可提高数据访问速度.EMC公司的Ma等人[87]分析了5年来约一百万个磁盘的追踪数据,发现磁盘的失效概率与磁盘中再分配扇区的个数有着明显的正相关关系.鉴于此,Ma等人[87]提出了完全基于磁盘再分配扇区个数的磁盘失效预测方法RAIDShield,将再分配扇区数目达到一定阈值的磁盘作为即将失效的磁盘.大规模长时间的运行表明,RAIDShield可以有效地预测磁盘失效,极大减少过多磁盘同时失效导致整个RAID失效的概率.9研究展望关于大规模分布式存储中纠删码容错技术,未来值得进一步研究的方向主要包括以下几个方面.(1)同步编码实现技术现有的编码实现方法都是异步编码,利用多副本技术来保证编码之前的数据可靠性.在编码之前,数据的多个副本同时提供服务,可以获得更高的访问吞吐率,这对那些需要对数据进行大量分析计算的存储系统具有较大意义.但是,对于无需进行大量运算的系统,比如向用户提供文件存储服务的个人云盘、提供视频流或视频分享的视频存储系统等,由于访问瓶颈存在于互联网,异步编码中的多个副本无法有效提高用户的访问速度,并存在较为严重的资源浪费.因此,有必要研究同步编码实现技术,在数据存储时就立刻进行编码,避免复制数据占用大量磁盘和网络资源.(2)低冗余再生码设计再生码能够显著降低数据修复开销,但是其存储开销较大.现有的关于再生码的研究工作都是在一定数据修复开销的前提下,大致估算存储空间消耗,然后再构造具体编码.但是以存储空间利用率为前提,研究再生码数据修复开销的下界,并进一步构造具体的再生码,也是极具挑战的研究工作.从这个思路出发,就有可能设计出兼顾存储开销和数据修复开销的再生码.(3)数据失效预测技术较高的数据修复开销一直是纠删码在大数据存储中面临的一个重大挑战.设计新的纠删码和数据修复技术可以在一定程度上解决这些问题,但是无法根本解决问题.Ma等人[87]对磁盘失效预测的研究为解决这个挑战指出了一个新的有效思路.如果能够预测节点或磁盘的失效,就可以提前复制其中的数据,避免失效后的数据修复.根据我们的初步估算,即使失效预测的准确率只有50%,也可将数据修复开销降低40%.10总结构建于商用廉价服务器之上的分布式存储系统具有服务能力强、扩展性好和成本低廉等优点,被广泛应用于大数据的存储管理.分布式存储系统中节Page17点失效的常态化使得数据容错不可或缺.近年来,纠删码作为一种容错能力强且存储开销小的数据容错技术,受到了业界的广泛关注.本文首先指出了纠删码容错技术在分布式存储中面临的主要技术挑战;然后分类综述了分布式存储中纠删码容错技术的相关研究进展,重点分析了各种方法的优缺点;最后,本文基于最新研究动态对今后的研究方向进行了展望.
