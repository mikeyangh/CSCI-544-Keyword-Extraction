Page1计算模式的统一研究沈绪榜孙璐(西安微电子技术研究所西安710065)摘要现在的支持TLP、DLP和OLP3种计算模式的系统芯片/阵列芯片是不统一的,没有自然反映时空计算的概念和数学语言的特点,带来了计算机应用、设计和制造的多样性与复杂性.因此,该文从时空计算的概念和数学语言的特点出发,提出了计算模式的编程语言和体系结构的统一研究.关键词计算模式;时空计算;阵列数据;阵列语句1引言芯片技术问世之后,计算机的设计工作越来越多地转移到了芯片上.1971年Intel公司的TedHoff(特德·霍夫)工程师发明了世界上第一颗微处理器芯片Intel4004[1].微处理器将计算机的ISA(InstructionSetArchitecture)设计工作转移到了指甲大小的芯片上,这是继晶体管之后计算机小型化的又一个新的转折点.1987年人们提出了系统芯片(SystemonChip,SoC)的概念,要将计算机的系统设计工作也转移到小小的芯片上来,从单处理器的计算模式发展出了TLP(ThreadLevelParallel,线程级并行)、DLP(DataLevelParallel,数据级并行)以及OLP(OperationLevelParallel,操作级并行)3种计算模式.这几种计算模式的芯片实现是不统一的,如图1所示.按照牛顿物理学,人们可感知的3维空间中的事物是随1维的时间而演变的[2];而按照爱因斯坦的狭义相对论,时间和空间并不是有所分别的两件东西,两者合成4维的时空(Space-time)时间是第4个维度.狭义相对论是4维时空的数学框架,和牛顿物理学本质上是相同的.因此,可以认为人们可感知的客观世界的一切事物是在4维时空中演变的.计算机是用来对事物演变的数据完成时空计算的,计算模式的实现应该自然反映时空计算的概念.数学家发明的计算机以描述客观世界的数学语言为基础.数学是无穷(Infinite)的科学,它研究数和形,计Page2算机就是以无穷的存储空间(图灵抽象机的无穷长的带和冯·诺依曼计算机的输入输出体系结构)和无止境提高计算能力(也就是提高处理器的计算频率,以及增加处理器的复杂度)来支持数学上的“无穷”.数和形的研究不仅有细粒度的标量计算,还有操作粗粒度和数据粗粒度的阵列计算.正如美国科学院调查委员会所下的结论:“高科技本质上是数学技术”.本文从时空计算的概念和数学语言的特点出发,提出了计算模式实现的统一研究,使计算机同时具有细粒度标量计算和粗粒度阵列计算的能力.第2节是计算模式的实现分析;第3节研究了计算模式的实现统一;最后是结束语.2计算模式的实现分析按照冯·诺依曼体系结构的Flynn分类[3],图1(a)的单处理器计算机以单指令流单数据流的SISD(SingleInstructionSingleData)体系结构为基础,主要用来实现细粒度的标量数据操作,以下就简称为标量计算机(ScalarComputer).描述事物演变的数据以点为单位,存放在1维存储器中.标量计算机是“从点开始,从点到线,从线到面”再“分层处理”完成事物演变数据的时空计算,是一种(时间)1维的计算机.数据粗粒度的计算和操作粗粒度的计算在标量计算机上,是依次对相邻点的数据、相邻线的数据以及相邻面的数据进行顺序计算来完成的.正如文献[4]中所说的,因为顺序计算有一个通用的计算模型———冯·诺依曼体系结构,所以顺序计算没有多核计算的那些缺点,是一种确定性的计算模式.标量计算机的编程语言和机器语言都以标量数据为基础,分别用来描述和实现标量操作,它们的操作类型和数据类型基本上一一对应,软件和硬件之间无间隙(Gap),程序设计是一种确定而可预测的过程,能从算法解决问题的方式中直觉地产生出来.这种确定性的计算模式促进了软件工业的繁荣,已被成功使用了60多年[4].按照摩尔预言,计算机随微电子技术制造能力的提高而演变.现在的微电子制造技术已经可以研制2维的实现粗粒度计算的处理器.例如,Intel公司80个处理元的TeraScaleProcessor计划的系统芯片[5],如图2所示.处理器中处理元之间的互连关系,有时间1维访问存储器的互连,还有空间2维访问相邻处理元的东南西北(NEWS)互连,如图2(b)所示.这种处理器可以对阵列数据(ArrayData)中的各点同时进行计算,即可以用来实现数据粗粒度的计算和操作粗粒度的计算,以下就简称为阵列处理器(ArrayProcessor).从数学语言的细粒度的标量计算和粗粒度的阵列计算特点来看处理器发展的必然结果是阵列处理器.在1987年休斯公司研制的3维计算机的影响下,1997年西安微电子技术研究所研制了有64个定点16位字长的处理元(核)的系统芯片(与图2中一样,处理元之间采用了NEWS互连技术);并采用64个系统芯片,研制了有4096个处理元的阵列处理器.在此基础上作者编著了《MPP嵌入式计算机技术》一书[6].现在思考起来有两个很不够的方面:(1)没有像物理学家那样,明确4维时空计算(Space-TimeComputing)的概念;(2)没有像数学语言那样,明确数据粗粒度计算和操作粗粒度计算的特点.前述3种计算模式的实现,没有自然反映时Page3空计算的概念和数学语言的特点,遭遇了计算扩展限制问题和能源使用问题.2.1TLP计算模式的分析从图1(b)TLP计算模式的概念图可以看出,每个线程都以标量计算机的计算模式为基础,在每个标量处理器核(CPU)上同时执行以提高计算能力的.这实际上是一种仅执行标量计算的多核芯片,是一种单指令流多线程流的SIMT(SingleInstructionMultipleThread)执行.核之间没有直接的互连关系,通过共享存储器通信,没有自然反映应用场景的时空计算概念.将大型实验的模拟和仿真等高性能计算任务分成多个线程安排在每个核上完成计算,虽然线程可以同时开始执行计算,但不一定能同时完成计算.其中,存在如何同步(同时完成)的问题;由于线程之间可能存在数据相关,不能独立地执行,有互斥(数据依赖)的问题.正如文献[4]中所说,多核程序设计是一种内在不确定性的过程(NondeterministicProcess).现在通过编译器的编译尽可能地使核的线程相互独立互不干扰,并通过操作系统的调度尽量使线程之间充分协调.不难看出,同步和互斥问题将随着线程个数的上升越来越严重,限制了可扩展性(Scalability).允许不确定性又是多核程序实现高性能与可扩展(Scalable)的关键,即允许多核程序的不确定性行为是可控的.但是,只有在必要时才采用.因为多核程序设计要求彻底了解多核计算机的所有细节,例如核的数目、主存储器的布局以及Cache存储器的层次等,才能使程序的逻辑结构与计算机的物理结构相匹配.有效的匹配才能提高性能,实现所希望的可扩展性.为了编写安全有效的多核应用程序,需要有复杂的开发和调试工具以及新的程序设计技巧和不同的解题思考方式.因为多核程序源代码中所看到的并不是优化后所得到的,不可能从源代码来检查.例如,像无序执行的转移预测和指令重排这样的主动优化过程,经常是很难检测的竞争条件的主要原因.多核程序检测的复杂性,是由程序的多线程执行问题的隐藏性和不确定性时序引起的,寻找不可预测的临时故障就像在草堆中寻找一根针一样困难.标量计算机的程序员不用处理逻辑结构和基本物理核之间的关系、线程的通信和同步、TLP环境中的性能测试、负载不平衡的来源,以及程序中的数据和控制依赖性、死锁、矛盾和竞争条件等繁琐的事情.多核程序设计应该像标量计算机的顺序程序设计一样简单直观,具有确定性.2.2DLP计算模式的分析按照冯·诺依曼体系结构的Flynn分类,DLP计算模式应该以单指令流多数据流的SIMD体系结构为基础,从标量数据的操作上升到阵列数据的操作,以实现数据粗粒度的高效计算.它主要用在视觉传感器的图像阵列数据处理任务中,有7种应用场景,即有点操作的计算、局部邻域操作的计算、递归邻域操作的计算、几何操作的计算、整体操作的计算、统计操作的计算和目标操作的计算[7].早在1987年,为了实现航空航天图像处理计算机的小型化,美国休斯公司就研制了32×32个处理元的系统芯片[6],构成了一个有1024个处理元的阵列计算机,能高效完成图像阵列数据的7种处理任务的计算.例如,假定两个3×3的阵列数据及其矩阵乘法结果的3×3的阵列数据(如图3(a)所示)存储在分开的3个寄存器阵列中.在阵列计算机中,第1次计算过程(如图3(b)所示)首先进行几何变换,完成阵列数据(被乘矩阵)第1列数据[147]T的列播送,产生1个新的阵列数据(被乘矩阵),然后完成阵列数据(乘矩阵)的第1行数据[ABC]的行播送,产生一个新的阵列数据(乘矩阵).再对这两个新的阵列数据完成MATLAB语言中所说的阵列乘法运算,就得到了乘积中第1项的部分结果.其中,行Page4播送和列播送以及点对点的阵列乘法运算,直接在阵列处理器上完成,反映了时空计算的概念和数学语言(几何变换)的特点.类似的,第2次计算(过程如图3(c)所示)首先完成被乘矩阵的第2列数据的列播送,产生一个新的被乘矩阵,然后完成乘矩阵第2行数据的行播送,产生一个新的乘矩阵,再对这两个新矩阵和上一次的运算结果完成DLP的阵列乘加运算,得到乘积中第1项和第2项之和的部分结果.以此类推,如图3(d)所示,完成第3次DLP计算,就得到了3×3的矩阵乘法结果.不难看出,参照3×3矩阵乘法的例子,采用MATLAB语1A+2D+3G1B+2E+3H1C+2F+3K4A+5D+6G4B+5E+6H4C+5F+6K7A+8D+9G7B+8E+9H7C+8F+9图3休斯公司阵列计算机上3×3矩阵乘法的计算过程floata[n][n],b[n][n],c[n][n],d[n][n],e[n][n];inti,n;i=1;c[n][n]=(broadcast)0;FOR(i<=n){d[n][n]=(broadcast)a[:,i];//播送第i列e[n][n]=(broadcast)b[i,:];//播送第i行c[n][n]=c[n][n]+d[n][n]e[n][n];//阵列乘加操作i=i+1;}实际应用表明,现在的DLP计算模式的GPU系统芯片(例如索尼、东芝和IBM公司联合开发的总线互连的Cell芯片),比图1(b)中的多核芯片的效率高得多[8],已应用在超级计算机中.特别是GPGPU系统芯片,效率能比仅执行标量计算的多核芯片提高100倍以上,同时能耗更小[9].因此,2010年11月TOP500强的前10台最高性能的计算机中,有8台都采用了GPGPU加速器[10].针对未来应用的Nvidia公司的Echelon系统芯片,是按2017年的10nm工艺、峰值性能言中的阵列数据表示方法,就可以设计出c[n][n]=a[n][n].b[n][n]矩阵乘法的计算程序(如图4所示).从矩阵乘法的计算程序不难看出,DLP计算模式由阵列计算机实现,阵列数据粗粒度操作的编程模型,不仅与标量数据细粒度操作的编程模型一样(是一种确定而可以预测的过程),而且由于数据粗粒度的阵列操作能将标量数据的矩阵乘法计算的3次循环减少到1次循环,编程更简单;计算时间只与矩阵的阶数n成比例,计算效率高.而标量数据程序的矩阵乘法计算时间与n3成比例,计算效率低.烌烄=烆烎C16Tflops/s、存储带宽1.6terabytes/s、芯片功耗小于150W来设计的[10].Nvidia公司认为到那时,GPU不再是CPU的外带加速器,相反地,CPU与GPU将集成在统一存储器体系结构的同一芯片上.因此,它和DLP计算模式的阵列计算机实现相比,有两点不同:(1)Echelon系统芯片是一种通用的细粒度并行计算系统,它采用NoC(NetworkonChip)互连,和Cell系统芯片一样,也是DLP计算模式的一种SIMT执行,没有自然反映时空计算的概念和数学语言的特点.在图像阵列数据处理的7种计算任务中,除了点操作的计算任务之外,其它操作的计算任务都存在同步和互斥的问题.例如,在Echelon系统芯片上实现n×n矩阵乘法时,因为不能像在阵列计算机上实现那样,直接完成几何变换的播送操作,以及对整个阵列数据点对点的阵列乘法操作,不能获得O(n)的计算效率,而且程序设计复杂;(2)在Echelon系统芯片上,有256个TOC(Throughput-OptimizedCores)和8个LOC(Latency-OptimizedCores),分别用来完成GPU和CPU的Page5功能.由于系统芯片TOC和LOC的数目比例固定,不随不同的计算任务而变化,因此会出现比例分配(ExactBalance)不平衡问题.正如文献[10]中所说的“TheexactbalancebetweenLOCsandTOCsisanopenresearchquestion”.在阵列计算机上,阵列处理器的处理元之间的互连直接反映时空计算的概念,用阵列处理器就能统一完成所有计算模式的计算,没有Echelon系统芯片中TOC(GPU)与LOC(CPU)之间的比例分配问题.2.3OLP计算模式的分析1985年,XILINX公司推出了全球第一款FPGA产品XC2064[11],该产品由8×8个可重构逻辑块(ConfigurableLogicBlock,CLB)组成,用户可以根据算法的需要通过逻辑设计定义可重构逻辑块的功能,再将这些逻辑块连接起来,实现OLP计算模式的操作粗粒度计算.XILINX公司将这种芯片称为现场可编程门阵列(Field-ProgrammableGateArray,FPGA)芯片.FPGA芯片通过阵列结构对单数据流/多数据流完成时空计算,如图1(d)所示.与完成操作粗粒度的ASIC阵列芯片的电路设计相比,FPGA阵列芯片有可反复使用、流片次数少、研发周期短、能够高效实现多种算法计算的特点,被广泛应用于仿真计算和嵌入式计算中.因此,随着半导体工艺的发展,从最初的不超过1000门的2μm工艺的芯片,到2006年达到了千万门的65nm工艺的芯片,现在28nm工艺的FPGA阵列芯片已经作为一种应用平台,集成处理器、存储器、IP核等用于高性能计算[12].实际应用表明,FPGA阵列芯片的计算效率,比图1(b)中的多核众核的CPU系统芯片的效率高得多[13].在此,以四阶滤波器的公式y(n)=a3x(n-3)+a2x(n-2)+a1x(n-1)+a0x(n)为例,来说明OLP计算模式实现操作粗粒度计算的优越性.领域专家实现了滤波器的优化算法之一(如图5所示),四阶滤波器可由8个处理元来完成.上面的4个处理元完成乘法操作,下面的4个处理元完成加法和寄存缓冲操作.滤波器计算的过程是:乘法操作执行的结果yi(i=0,1,2,3)作为下面4个加法器的操作数.4个加法操作按流水线方式完成,即前一个加法操作执行的结果经寄存器r1缓冲后作为后一个加法操作的操作数.因此,最后寄存器r1的输出就正好是所求的结果y(n).不难看出,由于FPGA阵列芯片的大小是可扩展的,不论滤波器的阶数n有多大,OLP计算模式的滤波器计算都只需用一次乘法操作和一次加法操作的时间完成,计算效率很高.如果用图1(b)中TLP计算模式的CPU系统芯片实现,不仅程序设计复杂,而且是绝对达不到FPGA的计算效率的.OLP计算模式采用ASIC/FPGA阵列芯片实现,自然反映了操作粗粒度时空计算的概念.但是,算法要通过电路设计/逻辑设计映射到ASIC/FPGA阵列芯片上.OLP计算模式的硬件设计语言的电路设计/逻辑设计,比TLP和DLP计算模式的软件设计语言的程序设计的抽象层次低,灵活性差,始终被限制着ASIC/FPGA阵列芯片的应用范围.3计算模式的实现统一综上所述,现在的TLP、DLP和OLP3种计算模式的系统芯片/阵列芯片是不统一的.因此,由2种甚至3种芯片组成的超级计算机是异构的.为了解决编程的不统一性问题,早在2008年,Apple公司就向Khronos集团提出基于C语言的OpenCL(OpenComputingLanguage,开放运算语言)标准,设计人员可以通过所熟悉的C语言,开发跨平台代码(从CPU到GPU,现在已扩展到了FPGA[14]).Xilinx公司在2011年3月推出的Zynq-7000EPP可扩展处理平台,是一种在FPGA阵列芯片上集处理器、DSP、存储器、总线、IP核等为一体的“AllProgrammable”平台[15].虽然OpenCL标准和“AllProgrammable”平台都提供了统一的编程环境,满足了用软件设计语言实现CPU、GPU、DSP、FPGA等的编程要求.但是不可避免地带来了编译器和编译方法的复杂性以及编译效率低、存取开销和时间开销大等众多复杂的软件开发问题.因此,本文从时空计算的概念和数学语言的特点出发,提出了计算模式实现的统一研究.统一研究最基本的挑战是计算机的可编程性和能效,研究的主要内容是编程语言和体系结构Page6的统一.编程语言的统一首先是用阵列处理器实现ASIC/FPGA阵列芯片的功能,将OLP计算模式的硬件设计语言统一为软件设计语言,然后是标量计算和阵列计算语言的统一.计算机的体系结构(Architecture)一词是1960年代初期由IBMSystem/360系列机的设计者们提出的[16].体系结构的统一首先是如何定义外体系结构(exo-Architec-ture)的计算机功能,即如何定义软件与具体机器之间接口的指令集合,其次是内体系结构(endo-Architecture)的计算机内部组织方面的设计.3.1编程语言的统一因为指令流包括了“操作流”和地址码控制的“数据流”,所以能用来完成阵列处理器的处理元之间的互连(如图2所示),通过指令就可以对阵列处理器处理元之间的数据进行空间访问.以多指令流单数据流MISD(MultipleInstructionSingleData)和多指令流多数据流MIMD(MultipleInstructionMultipleData)的体系结构,设计相应的阵列语句(ArrayStatement)/阵列指令(ArrayInstruction),就可以用软件程序语言,在阵列计算机上实现OLP计算模式的操作粗粒度计算.允许领域专家不需要了解具体芯片的复杂性,采用算法级的强有力的抽象,从数学公式就能得到FPGA阵列芯片的操作粗粒度计算过程.例如,从滤波器公式到图5中所示FPGA阵列芯片的计算过程,可以用阵列语句表示出来(如图6(a)所示).图6(a)中第1行的4条语句表示图5中第1行的常量a3,a2,a1,a0,分别与4个乘法处理元北面输入的数据x(n)相乘,结果传向南面的4个加法处理元;第2行的4条语句,表示图5中第2行的加法处理元西面和北面输入的数据,将作为加法处理元的操作数,相加结果给寄存器r1后,再将r1的值传送给该寄存器东边的加法处理元,最后东边的寄存器的输出就是计算的结果.在阵列计算机中,组成阵列语句的标量语句可以一一对应地替换成标量指令,而成为阵列指令,即阵列语句可以用阵列指令表示(如图6(b)所示).不难看出,通过阵列计算机,采用阵列语句的程序设计语言实现OLP(OperationLevelParallel)计算模式的操作粗粒度计算时,将具有FPGA阵列芯片硬件描述语言同样的计算高效性.为叙述简单起见,我们将通过阵列数据和阵列语句实现粗粒度计算的语言叫做阵列语言(ArrayLanguage),简称A语言.图6滤波器的多操作计算的阵列语句/指令表示从数据、操作、语句和顺序描述4个方面可以看出,从标量操作语言上升到阵列操作语言的统一设计,是能在标量操作语言(例如C语言和MATLAB语言)的基础上完成的.四是顺序描述,包括阵列数据和阵列语句的顺序描述.阵列数据的顺序描述可以与MATLAB或C语言的顺序描述相同并统一.由于阵列语句是由标量语句组成的,其操作符与标量操作的操作符相首先,数据要对应于数学语言中的标量数据和矩阵数据.C语言中,除了标量数据之外,还有叫做数组(DataArray,数据阵列)的数据,数组是由同一类型的标量数据元素组成的有序集合.因此,数组的数据类型就是其标量数据元素的数据类型.也就是说,数组的数据类型表示与标量数据的类型表示是相同且统一的.C语言中的数组实际上就是MATLAB语言中的ArrayData(阵列数据),只是名称不同而已.在MATLAB语言中,标量数据和向量数据作为阵列数据的特例表示[17].阵列语言A的阵列数据采用了与MATLAB语言同样的表示方法.其次是操作,阵列数据的操作是对标量数据元素的操作.因此,操作符的类型及表示符号均可以与C语言中的相同并统一.虽然阵列处理器早已问世,但C语言中没有定义对数组的操作,只能通过标量操作完成对数组的计算.在MATLAB语言中,与数学中的矩阵加减法操作相对应,定义了加减乘除的阵列操作,正好用来描述阵列处理器上的阵列指令.阵列语言A定义了MATLAB语言中那样的阵列操作,补充了支持实现几何变换的播送(Broadcast)等3个操作符.三是语句,阵列语言包括控制语句和阵列语句.由于控制语句是时间一维的,从图4中矩阵乘法的程序中就可以看出,阵列语言A的控制语句可以与标量操作语言(例如C语言)的控制语句相同并统一.由于阵列语句由不同标量语句元素(StatementElement)组成,标量语句的设计可以与C语言的标量语句的设计相同并统一.Page7同,对阵列语句的顺序描述可以像C语言的DataArray那样,采用先行后列,并从第一行开始顺序描述的程序设计方法.例如,图6(a)中的阵列语句就可以用标量语句顺序描述出来,如图7所示.这样一来,阵列语言A与现在的并行编程语言不同,不需要使用Parallel等术语来说明并行性.与标量操作语言一样,保留了顺序程序设计的简单性.floatfilter[2][4]={s[1,1]:ds=a3dn;//y3=a3x(n)s[1,2]:ds=a2dn;s[1,3]:ds=a1dn;s[1,4]:ds=a0dn;;s[2,1]:rl=dw+dn,de=rl;//de=r1=0+y3s[2,2]:rl=dw+dn,de=rl;//de=r1=y3+y2s[2,3]:rl=dw+dn,de=rl;//de=r1=y3+y2+y1s[2,4]:rl=dw+dn,de=rl;;//de=r1=y3+y2+y1+y0}语句位置阵列语句中的标量语句的顺序表示说明综上所述,因为当标量处理器上升到阵列处理器时,需要以时空计算的概念和数学语言的特点为基础,将细粒度的标量操作语言上升到粗粒度的阵列操作语言.实际上,阵列语言A与标量操作语言是统一的,基本上就是在标量操作语言基础上(例如C语言),增加了阵列数据的操作,以及支持几何变换的操作.从应用上不仅具有C语言和MATLAB语言的功能,并如图4中所示,具有直接编写粗粒度计算程序的简单性.3.2体系结构的统一采用阵列处理器就可以研制时空3维(时间1维+空间2维)的阵列计算机(ArrayComputer),能“从面开始,分层处理”事物演变的数据,使采用数据Cache或指令Cache实现的数据和程序的局部性统一,直接地转移到阵列处理器支持的DLP计算模式的数据粗粒度的高效计算和OLP计算模式的操作粗粒度的高效计算上.阵列计算机将由单指令存储器、阵列处理器和阵列数据/指令存储器(ArrayData/InstructionMemory)组成,如图8所示,可用来实现所有计算模式的粗粒度时空计算.如3.1节所述,阵列处理器的机器语言(外体系结构)与阵列语言A的操作类型和数据类型基本上是一一对应的,是统一的.阵列语言程序中顺序表示的阵列数据和阵列语句、机器语言的阵列数据和阵列指令的阵列表示,也只是通过映射器将其存放到阵列数据/指令存储器单元中时完成的,没有复杂的所谓串行到并行那样的编译过程.阵列数据和阵列指令是分别在操作指令和调用指令的控制下执行的.从时间上来看,操作指令/调用指令一条接一条地顺序执行;从空间上来看,阵列数据/指令存储器中的阵列数据/阵列指令在单指令的控制下,完成数据/操作粗粒度计算,自然反映了时空计算的概念.1980年代,人们形成了系统芯片的概念,将计算机系统的设计也转移到芯片设计上来,拓宽了计算机体系结构的设计思路.从1966年Flynn对冯·诺依曼体系结构的4种分类中可以看出,SISD是支持细粒度的标量计算的,SIMD是支持数据粗粒度的阵列计算的,MISD和MIMD是支持操作粗粒度的阵列计算的.体系结构的统一以冯·诺依曼体系结构的Flynn分类为基础,使前述3种计算模式能在一种体系结构的系统芯片上实现,而不是分别在CPU、GPU和ASIC/FPGA3种芯片上实现.DLP计算模式的系统芯片体系结构主要体现在系统芯片中的处理器核及其互连的设计上.因为系统芯片中的处理器核往往比标量处理器简单,可叫做瘦核(Thin-Core).一种极端的情况是叫做位核(Bit-core)的1位处理器核.例如最早的MPP(MassivelyParallelProcessors)计算机,就是叫做CM-1的连接机(ConnectionMachine).它由4096个芯片组成,每个芯片上有16个位核,以人脑结构为模型设计[18].人脑模拟和人工智能是计算机的重要研究内容.为验证一种治疗大脑疾病的药物,要进行大量的动物实验和人类实验,至少需要15年.现在用浮点64位的超级计算机来模拟人类大脑[19],可以解决实验时间问题.在人工智能的研究方面,打败顶尖人类选手,获得Jeopardy!(危险!)智力竞答节目冠军的也是超级计算机,即IBM研制的Watson(沃森).处理器核的字长,就像处理器一样,是随应用领域的需要而定的,可以是16位、32位或64位.不仅如此,对于无理数π的近似计算,2011年7月日本东京大学的金田康正和高桥大介用超大型计算Page8机运算37小时将π计算到了515.369亿位,这大概是迄今为止的最高近似计算纪录.在本世纪内,π的近似计算位数有望突破1000亿位.所以,计算机还有无限字长的计算能力.处理器核之间的互连是多种多样的.一种极端复杂的互连情况,就是以人脑结构为模型的CM-1连接机.它采用了12D超立方体的互连网络,将65536个位核连接起来,任何两个位核之间的通信需要12次或少于12次的转接.这种互连设计,与现在的DLP计算模式的系统芯片的互连设计,都没有自然反映时空计算的概念.DLP计算模式的程序设计是一种不确定的过程.而阵列处理器中处理元之间的互连关系,以及与阵列存储器之间的互连关系,是自然反映应用场景的时空计算概念,按照粗粒度计算的编程确定性设计的.为了解决应用场景的多样性问题,在标量(单处理器)计算机中,可以有User、SP(SubProgram)、IRQ(InterruptReQuest)以及Supervisor等4种管理模式.DLP计算模式的系统芯片采用阵列处理器的体系结构实现.类似的,阵列计算机也可以随应用的不同,通过管理模式的不同,将阵列处理器分成小的阵列,以实现现在的GPU系统芯片的SIMT计算.ASIC/FPGA阵列芯片的应用实践证明,OLP计算模式的操作粗粒度的阵列计算是提高时空计算效率的有效途径.根据算法特点,在ASIC/FPGA阵列芯片中,每个处理单元通过电路设计/逻辑设计生成多操作的互连关系.由于阵列处理器中的处理器核是相互邻接的,而不是像现有的处理器中那样,指令的地址码只对寄存器/存储器中的数据进行访问.阵列处理器中的指令具有邻接寻址模式,指令中操作数的地址码不仅可以对寄存器/存储器中的数据进行访问,也可以对相邻处理器核中的数据进行访问,实现了FPGA阵列芯片中多操作形成的运算单元之间数据的流动.因此,可以用阵列处理器芯片等效地替代ASIC/FPGA阵列芯片,实现OLP计算模式的阵列芯片体系结构的统一.从数学语言中可以看出,DLP和OLP是两种互补的计算模式.为了解决TLP计算模式CPU系统芯片体系结构不统一的问题,可以将TLP计算模式的线程改为DLP或OLP计算模式的线程,也就是将以SISD体系结构为基础的数据细粒度计算改为以SIMD体系结构为基础的数据粗粒度计算或以MI(SD+MD)体系结构为基础的操作粗粒度计算.这样一来,TLP计算模式的线程就可以在操作指令或调用指令的控制下,统一在阵列计算机上执行,而且不会有多核SIMT执行的同步和互斥问题,也不会有多种结构的处理芯片的比例分配和数据传输问题,以及由此引起的编程不确定性和复杂性问题.这使TLP与DLP和OLP计算模式的程序设计变为一种确定而可预测的过程.4结束语综上所述,计算模式的统一研究,就是开发全新的编程语言和计算架构,解决现有计算架构遭遇的计算扩展限制问题和能源使用问题.分别由CPU和GPU系统芯片以及ASIC/FPGA阵列芯片支持的TLP、DLP和OLP计算模式的编程语言和计算架构是不统一的,带来了多核程序的同步和互斥问题,以及多种结构的芯片比例分配和数据传输问题,有碍于解决可扩展性限制问题和能耗问题.本文从时空计算的概念和数学语言的特点出发,提出以阵列数据和阵列语句为基础的阵列语言,它保留了标量操作语言的特点,继承了顺序程序设计的习惯,统一了现有3种计算模式的编程语言.以处理元及其邻接互连为基础的阵列处理器统一了现有3种计算模式的计算架构,实现了时空计算的概念.以阵列数据和阵列指令为基础的阵列计算机解决了传统计算架构面临的计算扩展限制问题.如前言中所述,不断提高计算能力是计算机用来支持数学上的“无穷”的途径之一.由十几万枚芯片组成的千万亿次超级计算机的功耗就已达到2MW左右,机房面积比庞然大物的电子管计算机的机房面积还大10倍(约700平米).有专家认为,2017年可能实现的Eflops超级计算机的核心处理器的数量大概在1000万到1亿枚之间.正如文献[10]中所说,若将Eflops超级计算机的功耗限制为20MW的话,每个浮点运算的能耗只能为20pJ(2×10-11焦耳).而现在的CPU芯片能耗较高,例如IntelWestmere每个浮点运算的能耗为1.7nJ(1.7×10-9焦耳);FermiGPU芯片的每个浮点运算的能耗大约为225pJ.与20pJ的能耗相比,现在的CPU和GPU浮点运算的能耗分别为其85倍和11倍,能耗问题突出.计算机的能耗是由芯片的能耗和芯片之间互连线的能耗组成的.有人预测2023年~2062年之间,Page9新型芯片和纳米技术将使超级计算机的体积缩小到一块方糖那么大,没有各种电缆,也不需要散热[20].为了实现航空航天图像处理计算机的小型化,早在1987年,休斯公司就开发了圆片级的硅直通(ThroughSiliconVia,TSV)技术.现在,IBM公司针对超级计算机的能源使用问题,也研发了TSV技术,使芯片之间的距离只有几微米.而粗粒度的阵列计算机的规则性适合TSV技术,通过消除平面封装的mm级长线互连,实现μm级垂直堆叠,可以使阵列处理器和阵列存储器之间的距离减少至少1000倍,这能加速数据的传输并减少传输功耗.致谢专家对本文的修改提出了宝贵意见,在此表示感谢!
