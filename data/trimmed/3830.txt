Page1性能非对称多核处理器上的自适应调度聂鹏程1)段振华1)田聪1)杨孟飞2)1)(西安电子科技大学计算理论与技术研究所西安710071)2)(中国空间技术研究院北京100094)摘要现有的性能非对称多核调度算法要么不能充分利用其体系结构而吞吐量低,要么能充分利用其体系结构但扩展性差.有些算法即使考虑了扩展性,但也局限于CPU核数目,没有考虑到任务数方面的扩展性.为了解决这些问题,作者提出了一个自适应调度算法(称为AS4AMS).在任务的每一次调度中,AS4AMS首先通过分析任务运行时的平均停驻时间得出任务的计算需求,然后根据这些需求以及各CPU核的负载情况将任务分配到合适的CPU核上运行.另外,该算法任务结束前,会不断重复上述过程以适应任务需求的不断变化.实验结果表明:与现有方法相比,所提出的方法扩展性更好并且吞吐量也更大.关键词多核处理器;性能非对称;操作系统;调度1引言当前,微体系结构的发展已经从过去的单纯提升CPU的速度转变为提升CPU的并行程度[1].与单核处理器相比,采用多核设计的处理器扩展性强,性能高且散热少能耗低[2-3].现在多核体系结构已成为CPU发展的主流.著名的处理器设计制造厂商AMD、ARM、Fujitsu、IBM、Intel和Sun的主流产品全是采用多核体系结构,这些产品涵盖了从嵌入Page2式、PC到服务器的所有计算领域.随着多核CPU的运用和发展,最近体系结构研究者发现,采用性能非对称设计的多核处理器具有更高的能效比[4-5].这里需要注意的是,性能非对称多核处理器只是非对称多核处理器中的一种[6].根据核的差异类型,非对称多核处理器可以分为性能非对称多核处理器和功能非对称多核处理器.功能非对称是指多核处理器中不同类型核能处理的任务不同(指令集不同).现有的这类处理器当中比较典型的架构是少数用于控制的通用核搭配多个专用计算核.IBM的CellBE、LSI的AXXIA网络处理器以及TI的搭配ARM核与DSP的SOC都属于功能非对称多核处理器.在功能非对称多核中,通常同一任务不能在不同类型的核上执行.性能非对称则指多核处理器中的核的功能及能够处理的任务相同,但核的性能(处理速度)不尽相同.由于每个核的功能相同,所以同一任务可以在性能非对称多核处理器的各个核间来回迁移并执行.我们接下来讨论的调度算法针对的就是这种性能非对称多核处理器.性能非对称多核处理器的高能效比是由被调度任务(在本文中对任务、线程和进程不做区分)的特点决定的,不同的任务通常有不同的计算需求,即便是同一任务的计算需求也会随着执行不断变化.性能非对称多核处理器(也称为性能异构多核处理器)用性能高的核(简称快核)执行计算密集型任务,用性能低的核(简称慢核)执行存储密集型任务.因此,与全部采用高性能核的对称处理器相比,性能非对称多核处理器具有更低的能耗;与全部采用低性能核的对称多核处理器相比,性能非对称多核处理器具有更高的吞吐量,进一步实现了高性能与低能耗的结合.但性能非对称多核处理器的这种能效比建立在操作系统的准确调度之上.也就是说,任务所分配的CPU核的计算能力应该是与该任务的计算需求相匹配的.在性能非对称多核系统中,如果调度器将计算密集任务分配到性能低的核上,将存储密集型任务分配到性能高的核上,那么性能高的核将大部分时间处于等待状态,而性能低的核却忙得不可开交,这样极大地降低了系统的吞吐性能.2算法的提出自从性能非对称多核处理器在2003年由Kumar等人[5]正式提出以来,有不少面向性能非对称多核体系结构的调度算法被提出[7-12].Li等人[7]在Linux系统上实现了一个性能非对称调度器(称为AMPS).该调度器确保每个核上的负载与各自的性能成正比,并且保证性能高的核的利用率不低于性能低的核.AMPS简单易于实现,具有很好的扩展性.但是AMPS在调度时并没有考虑任务的计算特性以及处理器的性能非对称性,所以AMPS有可能将存储密集型任务分配到快核,造成系统效率降低.Kumar等人[8]与Becchi等人[9]分别提出两个相似的在线跟踪方法.它们首先将待调度线程在各种CPU核类型上试运行一段时间以跟踪线程的运行情况,并计算这些线程在快核上相对在慢核上的性能加速比.然后,将具有最高加速比的那些线程真正分配到快核上执行.只要线程未结束,这个试运行然后再分配的过程会不断重复.尽管Kumar等人[8]与Becchi等人[9]的方法相对那些没有考虑CPU性能非对称性的调度更准确,但它们的调度开销非常大,扩展性特别差.这是由于它们需要在每种CPU核上试运行以获取加速比,所以很难适应CPU核种类的增长.另外,它们总是假设任务是同时到达的,并且加速比的比较是建立在全局同步的基础之上的.也就是说,必须只有在所有的线程在各种CPU核上试执行一遍后才能比较出加速比最大的线程,这就意味着当系统线程数目多的时候,调度算法的开销将非常大.最后,任务在试运行阶段的来回迁移也容易造成负载不均衡.Shelepov等人[10]提出一个离线分析的方法(称为HASS).在运行之前,HASS先离线分析程序的体系结构属性,并将这些属性嵌入到程序二进制文件当中.调度程序读取这些属性后,将任务分配到合适的核上执行,不需要在真正执行时动态地在各种CPU核上试运行.与之前的在线动态方法相比,HASS确实简单易于实现,有较好的扩展性,但是它获取的属性是根据一个给定输入运行分析所得到的平均值,不适用于任务计算需求变化频繁的任务.另外,离线分析时给定的输入的选取也并非易事,因为这个输入必须要有一定的代表性才能使程序表现出来的计算特性反映其它输入的情况.所以,HASS难以推广及用于实际系统当中.在我们前面的工作中,结合已有在线方法和离线方法的优点也提出了一个性能非对称多核调度算法[11](称为ESHMP).ESHMP也是一种在线试运行的方法,与之前的在线方法不同的是,它使用单条指令的平均停驻时间(AverageStallTimePerInstruction,ASTPI)衡量每个线程的计算需求,而不是加速比.一个线程的ASTPI是指该线程中的Page3单条指令在访存操作上的平均花费,在当前每个CPU核具有相同存储体系结构的处理器中,同一线程在同一处理器中的不同核上具有相同的ASTPI值.如果一个线程的ASTPI值越小,则该线程在读写存储操作上的花费越小,加速比也就越大.所以,ESHMP只需要将待调度线程在一个CPU核类型上执行就可以得出该线程的计算需求,计算复杂度与具体平台上的CPU核类型数目无关,具有较好的扩展性.另外,ESHMP吸取以往在线调度周期跟踪的优点,也考虑了任务计算需求的阶段性变化.因此,ESHMP相对HASS能适应多种计算特性的任务,包括计算需求频繁变化的任务.虽然ESHMP继承了前述算法的优点,具有更好的性能,但是ESHMP也没有考虑在任务数方面的扩展性,需要全局同步比较任务的平均停驻时间.为了解决上述问题,本文提出一个自适应性能非对称多核调度(AdaptiveSchedulingforAsym-metricMulticoreSystems,AS4AMS).AS4AMS采用局部队列,根据系统的负载并结合任务的计算需求,将待调度任务分配到合适的核上执行,没有任务同时到达的限制也无需任务同步并比较.实验表明:与现有方法相比,AS4AMS具有更好的扩展性.3AS4AMS调度算法AS4AMS算法由计算特性分析和任务调度两部分组成.每个任务在执行过程中每一次调度都经历计算需求分析和调度这两个阶段.另外,AS4AMS还定期进行负载平衡处理,这样可以避免因负载不均衡而影响CPU利用率.3.1任务计算特性分析一般来说,与在性能低的核上的性能相比,基本上所有的任务在性能高的核上都能获得性能提升.但不同的任务获得的提升程度很可能不尽相同,即使是同一任务在不同时刻也会有不同的计算需求.因此,为了提高系统吞吐量,在所有待调度任务中,仅将那些在性能高的核上具有较高性能提升的任务分配到性能高的核上执行.然而,任务计算需求或特性的度量非常关键,它直接关系到性能非对称多核调度算法的性能.这里,用单条指令的平均停驻时间ASTPI衡量任务的计算特性.ASTPI值越大,线程的停驻时间越长,在快核上的性能提升程度越小;反之,提升程度越大.并且,同一线程在不同CPU核上的ASTPI相同,因为每个核都有相同的存储体系结构(见图1).因此,任务只要在任一核上试运行就可以获得其计算特性.下面,给出ASTPI的计算方法.通常,一个任务的完成时间CT(CompletionTime)可以划分成执行时间ET(ExecutionTime)和停驻时间ST(StallTime)两部分.ET是任务在得到数据后在核上真正执行的时间,等于指令数Ninstr与任务所在核A的最大执行速度MIPSA(MaximumInstructionsperSecond,核A不需要访存时的速度)之比,即Ninstr/MIPSA.CT可以用指令数Ninstr除以该任务所在核A上的实际执行速度IPSA(InstructionsperSecond)计算出,即Ninstr/IPSA.ST是由于任务访存等事件造成的CPU核停驻等待时间,可以通过CT减去ET求得,即Ninstr/IPSA-Ninstr/MIPSA.综上,ASTPI=ST/Ninstr=1/IPSA-1/MIPSA.其中,核A的最大执行速度MIPSA是核A可以提供的最大速度,可以由公式MIPSA=MIPCA×FrequencyA(在这里MIPCA是核A每个周期完成的最大任务数,而FrequencyA是核A的处理频率)求出;而IPSA是任务在核A上的实际执行速度,通过访问CPU的硬件计数器(HardwarePerformanceCounter,一种用于监视任务执行的寄存器)获得已经执行完成的指令数,再除以计算时间得出.当任务到达系统,调度器对任务的计算特性一无所知.调度器将任务分配到负载最少的核上执行一个时间段,并跟踪任务的执行情况计算出其ASTPI值.并且,调度器会跟踪任务的每一次执行过程,更新任务的计算特性.在每个CPU核上,调度器除了按优先级维护所有的任务,还按ASTPI大小建立任务的相互关系.此外,还记录当前核中ASTPI值最小的任务和ASTPI值最大的任务.Page43.2任务调度当任务的计算特性信息(即ASTPI值)获取到以后,首先比较该任务与所在核其它任务的计算特性关系,然后决定是否迁移、向更快还是更慢的核迁移;最后,如果需要迁移,则根据当前核的负载与目标核的负载关系采用不同的迁移策略.具体流程如算法1所示.算法1.任务调度.定义:t是待调度任务,astpi是它的一个字段,表示当前的ASTPI值.Max与Min是CPU核的两个字段,分别记录核上任务中最大与最小的ASTPI值.tMax和tMin则是与Max以及Min对应的任务.load是CPU核的字段,保存核的负载信息.ccur表示当前核.c表示目标核.faster_core与slower_core是两个函数,如果当前核类型存在多个核,则返回下一个同类型核;否则,返回分别比当前核更快或者更慢的核.1.if(ccur->Min<t->astpiandt->astpi<2.return3.endif4.if(t==ccur->tMin)then5.for(c=faster_core(ccur);c;c=faster_core(c))do6.if(c->Min<t->astpiandt->astpi<7.if(ccur->load==c->load)then8.t与c->tMax互相迁移到对方的核上执行9.elseif(ccur->load>c->load)then10.将t迁移到核c上执行11.endif12.break13.endif14.endfor15.elseif(t==ccur->tMax)then16.for(c=slower_core(ccur);c;c=slower_core(c))do17.if(c->Min<t->astpiandt->astpi<18.if(ccur->load==c->load)then19.t与c->tMin互相迁移到对方的核上执行20.elseif(ccur->load>c->load)then21.将t迁移到核c上执行22.endif23.break24.endif25.endfor26.endif从算法1中可以看出,只有在待调度任务的计算特性在当前核中是最大或最小时,才考虑将其从当前核迁移到其它核上.如果待调度任务是当前核上计算特性最高的任务(即ASTPI值最小),则考虑将该任务向更快的核迁移(如果当前核类型有多个核时,则会先考虑下一同类型核).从当前核出发找到一个性能不低于当前核的核c,任务t在核c上所有的任务当中的计算特性不是最大也不是最小.然后比较当前核与目标核的负载.如果当前核与目标核的负载相同,则将任务t与目标核上性能最小(即ASTPI最大)的任务交换核执行.如果目标核的负载比当前核的负载小,则直接迁移到目标核上执行.如果待调度任务是当前核中计算特性最低的任务(即ASTPI最大),则考虑将该任务向更慢的核迁移(如果当前核类型有多个核时,则会先考虑下一同类型核).从当前核出发找到一个性能不高于当前核的核c,任务t在核c上所有的任务当中的计算特性不是最大也不是最小.然后比较当前核与目标核的负载.如果当前核与目标核的负载相同,则将任务t与目标核上性能最大(即ASTPI最小)的任务交换核执行.如果目标核的负载比当前核的负载小,则直接迁移到目标核上执行.从上看出,AS4AMS在调度过程中不断调整任务,使得当前核中性能最高的任务比更快核中性能最低的任务的性能低,并使得当前核中性能最低的任务比更慢核中性能最高的任务性能高.也就是说,AS4AMS使得快核中的任务与慢核中的任务相比,具有更高的计算特性.另外,AS4AMS调度使整个系统负载朝更平衡的方向发展.如果目标核的负载高于当前核的负载,则待调度任务不迁移,继续在当前核执行.最后值得注意的是:每次迁移或是交换操作都包含更新相关核上任务的ASTPI关系、ASTPI最小与最大的任务.3.3负载均衡虽然AS4AMS在任务到达时以及任务时间片结束进行下一次调度时都保证系统处于平衡状态或向平衡方向发展,但是任务的结束或挂起等事件都会导致系统负载不均衡.因此,为了减少负载均衡操作的开销,AS4AMS的负载均衡采取基于事件驱动的机制,而传统调度器基本上都是基于时间驱动的,这也是本文的创新点之一.也就是说,只有当任务状态发生变化并影响负载时才考虑进行负载均衡操作.并且,只有在所在核的负载处于空闲状态或变化超过一定阈值时(在我们的实验中取10%),才平衡负载.另外,与面向对称多核处理器的负载平衡不同Page5的是,性能非对称多核处理器的平衡不是简单地保证每个核上的任务数相同或相差不超过1,而是要保证每个核上的相对负载相同或不超过1.这里的相对负载是指每个核以最慢核为单位相对自己性能的负载.相对负载的计算公式如下:其中,Loadrel表示相对负载,Nthreads表示任务数,Freqsmest和Freqcur分别表示最小核与当前核的频率.我们在算法1和算法2中用到的负载都是相对负载.算法2.负载均衡.定义:load和nthreads是CPU核的字段,分别用来保存核的负载信息和线程数目.ccur表示当前核.c表示目标核.no_threads用来存储需要迁移的线程数目.perf是CPU核上表示核的性能的字段.is_faster是用来比较CPU核性能的函数,如果第1个参数对应的核比第2个快,则返回真.1.找出当前系统负载最大的核,用c表示该核2.if(c->load-ccur->load<=1)then3.return4.endif5.no_threads=c->nthreds+ccur->nthreads6.no_threads=no_threads/(1+c->perf/ccur->7.no_threads=ccur->nthreads8.if(is_faster(ccur,c))then9.将no_threads个高性能线程从c迁移到核ccur上10.else11.将no_threads个低性能线程从c迁移到核ccur上12.endif例如,一个系统包含两个核A和B(频率分别为1GHz和2GHz),并有6个线程处于可运行状态.如果A和B上都各有3个线程,则该系统负载就处于不均衡状态,因为它们相对负载分别为3和1.5(A与B的相对负载超出1);而当A上有2个线程,且B上有4个线程时,该系统负载处于平衡状态,因为此时A与B的相对负载相同.算法2给出了AS4AMS的负载均衡算法.首先,找出当前负载最大的核,并比较该核与当前核的相对负载是否超过1.如果小于1,则无需均衡,算法退出.否则根据当前核是否比目标核c快还是慢,决定将目标核上no_threads个性能最高的还是最低的线程迁移到当前核上.其中,no_threads的计算过程如下:1.计算出两个核上的总线程数(算法2第5行).2.计算出当前核应该分配的线程数,也就是总的线程数除以核数目.注意,这里核的数目是以当前核为单位的,所以是1+c->perf/ccur->perf(算法2第6行).迁移的线程数(算法2第7行).3.应分配的线程数减去已有的线程数得到实际需要最后,需要说明的是在本项研究中,我们总是假定任务迁移到各个核的代价相同,这可能让算法在多CPU和多节点系统中有些限制.但相信随着单CPU中集成的核数目越来越多,现在的做法仍然具有重要意义.4实验评估4.1实验方法由于现在还没有商用的性能非对称多核平台,我们通过使用DVFS(DynamicVoltage/FrequencyScaling,动态电压频率调节)技术,修改CPU核的频率和禁用CPU核构造出一个性能非对称多核平台.实验用到的硬件平台是HPZ800工作站,配备有IntelXeonX55502.66GHz4核对称处理器.在实验中,我们用到3种配置(见表1).这3种配置包含不同的核数目,用于评估算法在CPU核数目方面的扩展性.配置名称配置11个2.66GHz核;1个1.6GHz核配置21个2.66GHz核;1个2.26GHz核;1个1.6GHz核配置31个2.66GHz核;1个2.26GHz核;1个1.86GHz核;为了使测试尽可能接近真实的工作负载,我们采用国际标准SPECCPU2006程序组中的基准测试程序构成测试用的工作负载(见表2).选取的程序包括:单阶段任务(Single-Phased,SP)以及多阶段任务(Multiple-Phased,MP).单阶段任务是指任务特性在运行过程中不发生变化的任务,要么为计算密集型任务(CPU-Intensive,CI),要么为存储密集型任务(Memory-Intensive,MI).与之相反,多阶段任务是指任务计算特性在计算过程中会发生变化.我们对表2中的负载按其负载构成的对应关系进行命名,如3MP-1SP表示右侧构成该负载的基准程序中前3个是多阶段任务,最后1个是单阶段任务.而4MP_A和4MP_B则分别表示两个负载中的4个基准程序都是多阶段任务.在表2中的前5个负载为单阶段负载,后5个负载为多阶段负载.另外,我们对每个负载中最长的基准程序加粗表示(每个负载的完成时间由最长基准程序决定).最后,按负载完成时间的升序关系将负载显示在表2当中.Page6负载名称3MP-1SPbwaves,leslie3d,astar,namd3CI-1MIcalculix,povray,hmmer,mcf4CI2CI-2MInamd,sjeng,soplex,omnetpp4MIGemsFDTD,soplex,milc,omnetpp1MP-3SPastar,sjeng,milc,mcf4MP_Ah264ref,libquantum,leslie3d,astar4MP_Bh264ref,dealII,bwaves,astar1CI-3MIgamess,GemsFDTD,milc,mcf2MP-2SPleslie3d,astar,namd,gamess我们在Linux2.6.21内核上实现和对比了AS4AMS调度算法、ESHMP调度算法、IPC-Driven调度算法和HASS调度算法.在实验过程中,对于表2中的每个工作负载,我们运行负载中的所有实例直到运行最长的基准程序实例完成3遍为止.除了比较不同特性的任务,我们对同一负载还选取不同的规模测试算法的扩展性(1倍实例、2倍实例和3倍实例),即对同一任务同时执行多个实例(1个、2个和3个).我们记录并比较同一负载在同一系统配置下,分别在AS4AMS调度器、ESHMP调度器以及IPC-Driven调度器和HASS调度器下的平均完成时间.通过IPC-Driven下的完成时间减去ES-HMP下的完成时间,再除以IPC-Driven下的完成时间得到ESHMP相对IPC-Driven的性能提升程度.用相同的方法也可得到AS4AM相对IPC-Driv-en和HASS的性能提升程度.4.2实验结果4.2.1核扩展性分析图2给出了在核数目不同但负载规模相同(1倍实例)时AS4AMS与ESHMP相对IPC-Driven的性能提升.因为,在调度算法和负载相同的情况下,任务的完成时间完全由硬件配置决定.所以,图2中的结果反映了不同核数目对这3个调度算法的影响.首先,可以看出在各种核数目的情况下,AS4AMS与ESHMP相对IPC-Driven都获得了性能提升.并且,我们还发现对于同一负载,核心数目越多,AS4AMS和ESHMP相对IPC-Driven的性能提升程度越大.在配置1时,AS4AMS和ESHMP相对IPC-Driven的性能提升最大分别达到13.9%和8.7%.在配置2时,AS4AMS和ESHMP相对IPC-Driven的性能提升最大分别达到26.9%和20%.这是因为,IPC-Driven采用加速比作为任务的性能衡量指标,需要在不同核类型上试运行.这种试运行导致任务在不同核间的迁移,带来额外的开销.而AS4AMS和ESHMP采用平均停驻时间作为性能衡量指标,只需要在一个核上运行就可以得到任务的计算特性,与配置中的核类型数目无关.其次,我们发现在各种核数目的情况下,AS4AMS相对ESHMP也获得了性能提升.而且,对于相同的负载,AS4AMS相对ESHMP的性能提升也随着核心数目增长而增长.如对于负载4MP_B,在2个核的情况时,AS4AMS相对ESHMP的性能提升为5.2%,但在4个核的情况时,AS4AMS相对ESHMP的性能提升达到8.6%.这主要是由于ESHMP采用的全局队列,各个核对全局队列的访问方式是串行的,因此核心数目越多,访问任务队列Page7的延迟越大.而AS4AMS采用局部队列,没有任务同步的限制也没有全局队列的瓶颈.4.2.2任务扩展性分析图3给出了在核数目相同(4个核)但负载规模不同时AS4AMS与ESHMP相对IPC-Driven的性能提升.从图3中可以看出,在不同负载规模时,AS4AMS与ESHMP相对IPC-Driven都获得了性能提升.而且,在相同配置下,实例数越多,AS4AMS与ESHMP相对IPC-Driven获得的性能提升越大.如在1倍实例时,对于负载4MI,AS4AMS与ESHMP相对IPC-Driven获得的性能提升分别达到28.6%和21.9%.而在2倍实例时,对于负载4MI,AS4AMS与ESHMP相对IPC-Driven获得的性能提升则分别达到39.1%和24.1%.与不同配置时的情况类似,这是由于IPC-Driven为了计算任务的计算特性需要在不同核类图3AS4AMS与ESHMP在不同负载规模时型上执行造成的.实例数越多,IPC-Driven这种获取计算特性方式造成的开销就会越大.现在我们来看不同负载规模时,AS4AMS相对ESHMP的提升.当表1中工作负载只有1倍实例时,从图3(a)中可以看出,由于没有任务同步的限制也没有全局队列的瓶颈,AS4AMS相对ESHMP获得了性能提升.正如我们所预计的,负载的执行时间越长,则AS4AMS的提升越大.最大达到8.9%(负载1CI-3MI),最小也有4.9%(负载3CI-1MI).这是因为AS4AMS与ESHMP都周期性地检测任务的阶段变化,负载的完成时间越长则调度的次数会越多,因而AS4AMS相对ESHMP的加速也就会累积得越多.当系统中每个工作负载有2倍实例时(每个核运行2个任务),从图3(b)中可以看出,对于同一负载,运行2倍实例与运行1倍实例相比,AS4AMS相对ESHMP的提升程度更大(相差5.4%~9.3%).原因是ESHMP采用全局调度,需要调度的实例越多,则每次调度需要同步比较的任务也就越多,因此开销也会更大.同样地,在图3(c)中可以看出,AS4AMS在3倍实例时的提升比在2倍实例时的提升大(相差4.9%~8.9%).但是,我们发现对于同一负载,AS4AMS在3倍实例相对2倍实例的提升比2倍实例相对1倍实例的提升小,如4MI3倍实例相对2倍实例的提升是6.5%,而2倍实例相对1倍实例的提升是8.3%.也就是说,AS4AMS对ESHMP的加速并不是随实例数增长而线性增长的.通过跟踪分析任务的调度发现,这是因为AS4AMS采用局部调度,每个任务从进入系统后的初始分配核到最佳分配核还有个自适应过程,并且这个过程也会随着任务数增长而增长.不过,与ES-HMP每次全局同步时CPU核空闲的性能损耗相比,AS4AMS的开销依然小很多,所以AS4AMS能提高系统的吞吐量.另外,我们还可以看出负载完成时间越长AS4AMS加速越大的结论也适用于负载规模为2倍实例和3倍实例的情况.可以看出,负载是计算密集型、存储密集型还是多阶段任务对于AS4AMS相对ESHMP的提升程度没有关系.这主要由于这两个算法都采用相同任务特性获取方法和相同的任务特性衡量指标ASTPI,所以二者的差别取决于各自调度阶段采用的方法,而各自调度阶段的开销与任务特性都没有关系.4.2.3迁移和负载均衡开销分析为了衡量AS4AMS与ESHMP的算法开销,Page8我们将这两个算法同HASS进行了比较.HASS采用离线分析的方法获取任务的计算特性,调度时根据离线分析得到的计算特性将各个任务分配到对应的核上执行,并且各个任务在各自分配的核上运行直到结束,不发生迁移.因此,相对AS4AMS与ESHMP,HASS没有运行时获取任务计算特性的开销和迁移以及负载平衡的开销.所以,同HASS进行比较可以很好地评估AS4AMS和ESHMP.图4给出了在4个核和1倍实例时AS4AMS与ESHMP相对HASS的性能提升.可以看到,对于前5个单阶段负载,ESHMP相对HASS的性能不但没有提升,反而出现了下降(不超过2%).然而,AS4AMS相对HASS的性能提升没有像ESHMP那样出现下降,而是达到了4.8%~8.3%的增长.跟踪任务的执行我们发现,单阶段任务的计算特性虽然整体上为计算密集型或存储密集型,但实际上也存在计算特性的变化,只不过变化相对较小.AS4AMS动态跟踪任务的计算特性变化,只有任务的计算特性变化到一定程度才会重新调整迁移.其中跟踪任务的计算特性属于对寄存器的计算操作.另外,我们的算法主要针对单CPU的多核系统,而现在主流的多核CPU的各个核心之间都共享最后一级缓存,核间迁移造成的开销很小.因此,AS4AMS利用了任务的阶段变化提高了CPU利用率,相对HASS获得了性能提升.但是,ESHMP由于全局同步的开销过大而无法充分利用任务的阶段变化,相对HASS出现了性能下降.而HASS采用离线分析得到整个运行阶段的任务特性,没有考虑到任务的计算特性是不断变化的,不能根据任务特性变化作出调整达到充分利用CPU的目的.对于多阶段任务,AS4AMS与ESHMP相对HASS都获得了性能提升.其中,ESHMP相对HASS提升了4.4%~13.9%,AS4AMS相对HASS提升了12.4%~22.5%.与单阶段任务不同的是,多阶段任务的阶段性变化更大、更频繁.因此,AS4AMS与ESHMP都能更好地利用任务的阶段性变化,使任务每个阶段都能分配到最适合的核上执行,更好地利用了CPU.并且,动态调整任务得到的性能提升相对调度开销要小,所以AS4AMS与ESHMP都获得了提升.5结束语和下一步工作性能非对称多核体系结构由于其相对对称多核体系结构具有更好的能效比,受到越来越多的关注.但是,性能非对称多核体系结构的这种高能效比优势只有通过合理的任务调度才能实现.本文提出一个自适应的性能非对称调度算法AS4AMS.AS4AMS调度算法在任务计算特性分析阶段使用平均停驻时间(ASTPI)作为性能指标,ASTPI与其它性能指标相比需要的计算开销更小也更准确;在任务调度阶段使用局部队列利用局部信息进行调度,避免了全局调度要求任务同步的瓶颈,也没有要求任务同时到达的假设.因此,AS4AMS具有更好的扩展性,不仅能适应核数目的增长,而且能更好地适应负载规模的增长(包括任务数量的增长和任务完成时间的增长).在本项研究中,我们并没有考虑负载均衡时任务迁移到不同核的代价差异.下一步工作中,我们将会对这一问题进行研究,让我们的调度算法适应更多的体系结构.
