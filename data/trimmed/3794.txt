Page1基于输出不一致测度的极限学习机集成的基因表达数据分类陆慧娟1),2)安春霖2)马小平1)郑恩辉3)杨小兵2)1)(中国矿业大学信息与电气工程学院江苏徐州221008)2)(中国计量学院信息工程学院杭州310018)3)(中国计量学院机电工程学院杭州310018)摘要选择性集成学习已经成为分析基因表达数据、获取生物学信息的有力工具.为了更好地挖掘基因表达数据,利用极限学习机的集成,克服单个ELM用于数据分类时性能欠稳定的缺点,文中提出了一种基于输出不一致测度的ELM相异性集成算法(D-D-ELM).算法首先以输出不一致测度为标准对多个ELM模型进行相异性判断,其次根据ELM的平均分类精度剔除掉相应的模型,最后对筛选后的分类模型用多数投票法进行集成.算法被运用到Breast、Leukemia、Colon、Heart基因表达数据上,并通过理论和实验得到验证.实验结果的统计学分析表明D-D-ELM能够以更少的模型数量达到较稳定的分类精度.关键词极限学习机;基因表达数据;集成算法;输出不一致测度;分类1引言近年来,由于集成学习(EnsembleLearning)在提高学习系统的泛化能力方面具有显著的优势,对其理论和算法的研究一直是机器学习领域的一个热点[1].国际机器学习界的权威学者Dietterich[2]曾在《AIMagazine》杂志上将集成学习列为机器学习领域的四大研究方向之首.在机器学习领域,最早的集成学习方法是BayesianAveraging.在此之后,集成学习的研究才逐渐引起了人们的关注.1990年,Schapire[3]提出了Boosting算法的雏形.但是这个算法存在着一个重大的缺陷,就是必须知道学习算法正确率的下限,而在实际中很难做到.1995年,Freund和Schapire[4]通过进一步研究,提出了AdaBoost算法,该算法不再要求事先知道泛化下界,可以非常容易地应用到实际的问题中去.1996年,Breiman[5]提出了与Boosting相似的技术———Bagging,进一步促进了集成学习的发展.基因表达数据[6]是通过DNA微阵列杂交试验获得的,经过预处理之后的数据通常是一个矩阵形式.由于基因表达数据具有维数高、样本少、类分布不平衡的特点,而对分类真正有用的基因只有很少的一部分,所以在对样本进行分类之前,需要进行基因选择,即选择出对分类起重要作用的那部分基因,从而降低数据的维数,达到更好的分类效果.对基因表达数据的分类,在肿瘤诊断上就是要区分肿瘤的类型以及区分正常组织与病变组织.随着基因芯片技术的出现,肿瘤分类已进入了分子分类阶段.Golub等人[7]首次利用基因芯片对急性白血病进行研究,发现了急性淋巴细胞性白血病(AcuteLymphoblasticLeukemia,ALL)下的两个亚型T2CellALL和B2CellALL.在分类方法研究上,Furey等人[8]利用支持向量机(SupportVectorMachine,SVM)、Khan等人[9]和陆慧娟等人[10]利用人工神经网络、Berrar等人[11]利用概率神经网络、陆慧娟等人[12]利用压缩感知技术实现基因表达数据的分类.2006年,Huang等人[13]提出了一种新的单隐层前馈神经网络(Single-hiddenLayerFeedforwardNeuralNetworks,SLFNs)学习算法,即ELM(ExtremeLearningMachine).理论上可提供良好的泛化能力和极快的学习速度.然而ELM也存在固有缺陷.研究表明,虽然ELM在大部分情况下可以获得良好的性能,但隐含层初始参数(连接权值、偏置值、节点个数)对ELM的分类精度仍存在很大影响,不恰当的参数会导致比较差的分类结果,并且单个极限学习机的学习性能具有不稳定性.2006年,Huang等人[14]提出的增量极限学习机(Incre-mentalExtremeLearningMachine,I-ELM),逐一增加隐层的节点,且在加入节点时保持当前隐层节点输出权值保持不变.为了提高收敛率,2007年,Huang等人[15]提出了凸增量极限学习机(ConvexI-ELM,CI-ELM),CI-ELM在加入新节点后,根据凸规划方法重新计算节点的输出权值.2008年,Huang等人[16]又提出强化的增量极限学习机(EnhancedI-ELM,EI-ELM),EI-ELM能产生更紧凑的网络结构,收敛率更高,学习速度更快.2009年,Feng等人[17]提出了一种误差最小极限学习机(ErrorMinimizedELM,EM-ELM),EM-ELM可以逐个或逐组增加隐层节点.与I-ELM不同的是,当新节点加入后,EM-ELM的输出权值要重新计算.在极限学习机集成方面,Lan等人[18]提出了在线连续极限学习机集成算法(EOS-ELM),它比OS-ELM要更加稳定且精度也更高,并且提高了分类器泛化性能.2011年,Cao等人[19]提出了V-ELM(VotingbasedExtremeLearningMachine)的方法.V-ELM利用k个简单的ELM得到输出结果,再通过集成方法求每个样本的后验概率,接下来根据后验概率计算样本类别.这种方法有效地解决了单个ELM学习的不稳定性,并且由于采用集成的方法,提高了ELM的泛化性能.同时,极限学习机的分类应用范围越来越广,Huang等人[20]将其用于分类优化,Pan等人[21]将其用于彩色图像分类,Zheng等人[22]将其用于文档分类,但是极限学习机相异性集成算法用于基因表达数据的分类尚无人涉及.本文利用多个ELM进行分类器并联集成,提出一种全新的基于输出不一致测度的相异性集成算法(D-D-ELM).算法已被运用到Breast(乳腺癌)、Leukemia(白血病)、Colon(结肠癌)、Heart(心脏病)等基因表达数据上,并通过理论和实验得到验证.实验结果显示分类效果明显改善.2极限学习机传统的神经网络学习算法(如BP算法)需要人为设置大量的网络训练参数,并且很容易产生局部Page3最优解.ELM只需要设置网络的隐层节点个数,在算法执行过程中不需要调整网络的输入权值以及隐元的偏置,并且产生唯一的最优解,因此具有学习速度快且泛化性能好的优点.文献[13]给定N个不同样本的矩阵(狓i,狔i),狓i=[xi1,xi2,…,xin]T∈Rn,狔i=[yi1,yi2,…,yim]T∈Rm,i=1,2,…,N,隐层结点的数目为L,激活函数为g(x),则ELM的模型可表示为其中,j=1,2,…,N,狑i=[wi1,wi2,…,win]T表示输入结点与第i个隐层结点的连接权重向量,βi=[βi1,βi2,…,βim]T表示第i个隐层结点与输出结点的连接权重向量,bi表示第i个隐层结点的偏移值.式(1)表示的N个等式用矩阵表示为其中熿犎=g(狑1·狓N+b1)…g(狑L·狓N+bL燀熿βTβ=燀βT犎为神经网络的隐层输出矩阵.由Moore-Penrose广义逆定理,通过奇异值分解求得犎,则3相异性集成相异性是集成系统中一个非常重要的研究课题.为了使得集成有实际意义,所使用的极限学习机之间必须存在一定的差异性,否则每个极限学习机的输出结果都一样,那么集成就失去了意义,只是徒增实验时间和空间的复杂度而已.因此在多个极限学习机集成系统的算法设计上,通常都想设计出相异性大、泛化能力强的极限学习机个体,因为这是构建多极限学习机集成系统的关键[23].但是,用什么样的标准去衡量极限学习机之间的相异度,相异度的大小对实际系统的影响效果如何,仍然是一个未解决的问题.然而,在现实生活中,不可能设计出各方面条件都满足的完美极限学习机,只能期待从某个角度出发,可以得到更好的效果.正如对某个样本来说,如果有部分极限学习机的判别结果是错误的,而其它的极限学习机的判别结果是正确的,通过相异性集成,虽然增加了集成的复杂度,但仍然有很大的概率能够得到正确的结果.而如果使用的极限学习机不具备相异性,则一旦结果判别错误,那就意味着最终对该样本的判别是错误的.由此可见,相异性集成的重要性.一种有效的相异性度量方法能够对建立集成系统起重要的指导作用.为了定性、定量地估计分类器之间的相异性,目前已经提出了各种各样的方法.大多数的度量方法都是根据分类器间的输出标签来进行度量的.下面主要介绍输出不一致测度.设样本的总数为M,极限学习机的个数为N,第i个极限学习机对第j个样本的输出值为fij3.1输出不一致测度输出不一致测度是根据分类器的输出标签进行度量的.对分类器fn和fm,设其输出结果为0或1.用Dif(fnk,fmk)表示两个分类器输出的差异,当这两个分类器对第k个样本的输出相同时,Dif(fnk,fmk)=0,否则等于1.此测度可由下式进行计算:其中,Diversitn,m与分类器fn和fm之间的相异度成正比.以上测度是基于分类器输出结果的相异性来衡量的.为衡量整个集成系统中的相异性,采用这种测度时,必须对所有分类器的输出结果都进行计算,最后可以采用求和来估算整体的相异性.除了基于分类器结果的测度外,研究者还提出了很多从整体上同时考虑所有分类器的测度,如基于熵的测度、Kohav-Wolpert变量和困难度测度等.这类基于全局的相异性测度能够直接对整个集成系统的相异性进行计算.目前从不同角度提出的相异性测度非常多,但哪种测度在什么情况下最适用,各种测度之间有怎样的关系,都还是未解决的问题.如果没有从理论上分析各种相异性测度在不同情况下获得成功或失败的原因,就无法真正理解相异性并将它用于集成系统的构造[25].3.2基于输出不一致测度的极限学习机集成算法采用基于输出不一致测度的极限学习机相异性集成的思想是从比较每个极限学习机的输出结果入手,如果第i个和第j个极限学习机对第k个样本的判别结果fik和fjk相同,则Dif(fik,fjk)=0(i=1,2,…,N;j=1,2,…,N;k=1,2,…,M),否则Page4Dif(fik,fjk)=1,用Diversiti,j=∑N表示第i个极限学习机与第j个极限学习机的相异性,可得到一个输出不一致性矩阵:犇犻狏犲狉狊犻狋=Diversit1,1…Diversit1,j…Diversit1,N熿Diversiti,1…Diversiti,j…Diversiti,NDiversitN,1…DiversitN,j…DiversitN,燀显然,犇犻狏犲狉狊犻狋是个对角线为0的对称矩阵.用selecti表示第i个极限学习机与其它所有极限学习机的相异性,其中计算分类器的平均分类精度p-,当0<p-0.5时,剔除掉select最小的极限学习机;当0.5<p-<1时,剔除掉select最大的极限学习机.3.3理论分析假设有N个极限学习机,M个样本,不妨设样本为两类样本(可递推至多类).采用输出不一致测度,记ξnm,k=Dif(fnk,fmk),ξnm,k可能的取值为0,1.ξnm,k=0,即(fnk,fmk)=(0,0),(1,1),ξnm,k=1,即(fnk,fmk)=(0,1),(1,0),则两个极限学习机之间的相异度为diversitnm=∑M学习机与其它所有极限学习机之间的相异度记为ηn,期望Eηn=∑N习机与其它极限学习机的相异度越大.设极限学习机fn和fm的分类精度分别为pn,pm,0<pn,pm<1.当样本k的真实类标为1且fn和fm对该样本的分类结果为ξnm,k=0时,有两种可能:(1)(fnk,fmk)=(0,0),此时的概率为p1=(1-pn)(1-pm);(2)(fnk,fmk)=(1,1),此时的概率为p2=pn·pm,则p1(ξnm,k=0)=p1+p2.当分类结果为ξnm,k=1时,也有两种可能:(1)(fnk,fmk)=(0,1),此时的概率为p3=(2)(fnk,fmk)=(1,0),此时的概率为p4=当样本k的真实类标为0且fn和fm对该样本(1-pn)pm;pn(1-pm),则p1(ξnm,k=1)=p3+p4.的分类结果为ξnm,k=0时,有两种可能:(1)(fnk,fmk)=(0,0),此时的概率为p1=pn·pm=p2;(2)(fnk,fmk)=(1,1),此时的概率为p2=(1-pn)(1-pm)=p1,则p0(ξnm,k=0)=p1+p2.当分类结果为ξnm,k=1时,也有两种可能:(1)(fnk,fmk)=(0,1),此时的概率为p3=pn(1-pm)=p4;(2)(fnk,fmk)=(1,0),此时的概率为p4=(1-pn)pm=p3,则p0(ξnm,k=1)=p3+p4.设全部样本中属于类1的样本有X个,属于类0的样本有M-X个.因为每个样本之间都是相互独立的,所以有p1(ξnm,1=0)=…=p1(ξnm,M=0)=p1(ξnm,k=0),p1(ξnm,1=1)=…=p1(ξnm,M=1)=p1(ξnm,k=1),p0(ξnm,1=0)=…=p0(ξnm,M=0)=p0(ξnm,k=0),p0(ξnm,1=1)=…=p0(ξnm,M=1)=p0(ξnm,k=1),则ηnm的数学期望为Eηnm=∑X=(p1(ξnm,k=0)·0+p1(ξnm,k=1)·1)X+=p1(ξnm,k=1)X+p0(ξnm,k=1)(M-X)=(p3+p4)·X+(p3+p4)·(M-X)=(p3+p4)·M=((1-pn)pm+pn(1-pm))·M=(pn+pm-2pnpm)·Mηn的数学期望为Eηn=E∑N由式(8)可得,当0<p-增,即分类器的分类精度越高则与其它分类器的相异度越大,所以此时剔除相异度最小的分类器能够提高整个集成系统的分类精度;当0.5<p-Eηn随pn单调递减,此时剔除相异度最大的分类器能够提高整个集成系统的分类精度.根据p-Page5剔除掉相应的分类器,可以得到以下结论:其中,pd-ELM,c为剔除后集成系统的分类精度,pELM,c为不剔除时的分类精度.即相异性集成的分类精度要比全部集成的高.虽然在剔除的过程中多了比较的工作,时间复杂度变为O(n2),但是在对时间要求不是非常高的情况下,精度的提高还是相当可观的.4实验结果及分析在理论分析的基础上,本节给出实验过程及结果:选取4组基因表达数据集进行测试,其中Breast、Colon、Heart为两类数据,Leukemia为多类数据.例如结肠癌数据集Colon包含了62个样本,每个样本含有2000条基因,其中22个样本为正常样本(Positive),40个样本为结肠癌样本(Negative),在此,根据文献[7]将前30个样本作为训练样本,后32个样本作为测试样本.对表1数据集进行测试,并采用经典的Bagging算法和Boosting算法进行对比试验来验证理论的分析结果.数据集样本总数基因数Breast9724481relapseLeukemia727129Colon622000NegativeHearth2703510Negative表2实验结果BreastLeukemiaColonHeart4.3实验分析根据表2中的实验数据,可得图1~图4,对全4.1实验步骤实验步骤:(1)分别用N个ELM对训练样本进行训练;(2)记录第i个极限学习机的分类精度pi和对第j个样本的判断结果fij;(3)如果第i个ELM和第j个ELM对第k个样本的判断结果不同,则Dif(fik,fjk)=1;否则Dif(fik,fjk)=0;集成;限学习机的相异度;(4)用Diversiti,j=∑N(5)计算selecti=∑N(6)当0<p-0.5时,剔除掉select最小的极限学习机;当0.5<p-<1时,剔除掉select最大的极限学习机;(7)通过多数投票法将剩余的极限学习机进行(8)使用集成后的分类器对测试样本进行测试;(9)对以上整个过程多次测试求平均值.4.2实验设置和实验结果为了获得更好的分类结果,先对这4个实验数据进行特征选择,去除冗余数据对实验结果的影响.输入权值和隐层结点的阈值随机选择,对应Breast、Leukemia、Colon、Heart这4个数据集的极限学习机的隐层结点的数目分别设为3、30、3、5.分别对4个数据集采用Bagging集成、Boosting集成和基于输出不一致测度的相异性集成(D-D-ELM).为了避免极限学习机稳定性不强所造成的误差,对不同个数的极限学习机进行集成时重复试验30次,求平均值,实验结果如表2.部数据进行F检验,因为在每个数据集上分别对1个,2个,…,30个ELM进行训练,所以自由度为Page629,对全部数据进行统计分析的结果如表3.在显著性水平为0.05时,通过查F分布表可得F0.05(29,30)=1.85,F0.05(29,24)=1.9,而计算所得F值均大于1.9,所以D-D-ELM算法是显著的.可以得出如下结论:基于输出不一致测度的相异性集成需要更少的极限学习机个数即可达到相同的分类精度,并且这种相异性集成能尽快地趋于稳定.可见,在使用少量极限学习机进行集成时,基于输出不一致测度的相异性集成的效果还是要优于其它几种经典集成方法的.数据集BreastLeukemiaColonHeart5总结与展望目前,生物学已不再仅仅是基于实验和观察的科学,理论和计算将发挥越来越大的作用[26].计算机科学、生物学与生物信息学等的交叉研究将进一步推动生物学的发展.不同组织类型的肿瘤,其发生、发展可能由某些共同的基因表达异常所致.基于基因表达数据,研究数据中蕴含的肿瘤与正常样本的分类算法,对揭示肿瘤区别于正常组织样本异常表达的基因,及这些基因如何相互作用从而导致样本出现相应的表型,具有十分重要的意义.一方面这样可以为生物医学研究人员提供可解释的样本分类知识,从而进行有针对的生物实验,以发现新的肿瘤相关基因及其表达模式;同时利用这些分类算法,也可直接用来进行样本类别的判断,从而为肿瘤的基因诊断和治疗提供参考.由于基因表达数据具有的基因数远远大于样本数的特点,本文提出了处理这类数据的一种通用的分类方法[27]:首先以输出不一致测度为标准对极限学习机进行剔除,然后根据多数投票法对剩余的极限学习机进行集成,最后用集成的分类器对基因表Page7达数据进行分类.本文通过对输出不一致测度进行理论分析得出其理论上的有效性,然后通过对基因表达数据的分类实验,得出其应用中的有效性—基于输出不一致测度的极限学习机集成算法,能够用更少的分类器得到较高的分类精度.本文只是以一种测度为标准对极限学习机进行集成,在接下来的工作中可以对多种测度进行分析与比较,进一步研究不同测度对分类精度的影响,更大程度地提高集成的效率和泛化性能.致谢中国矿业大学信息与电气工程学院王雪松教授和中国计量学院信息工程学院潘晨教授提供了宝贵的建议,籍此向他们表示深深的敬意谢忱!
