Page1选择性集成学习算法综述张春霞张讲社(西安交通大学理学院西安710049)(西安交通大学机械制造系统工程国家重点实验室西安710049)摘要集成学习因其能显著提高一个学习系统的泛化能力而得到了机器学习界的广泛关注,但随着基学习机数目的增多,集成学习机的预测速度明显下降,其所需的存储空间也迅速增加.选择性集成学习的主要目的是进一步改善集成学习机的预测效果,提高集成学习机的预测速度,并降低其存储需求.该文对现有的选择性集成学习算法进行了详细综述,按照算法采用的选择策略对其进行了分类,并分析了各种算法的主要特点,最后对选择性集成学习在将来的可能研究方向进行了探讨.关键词选择性集成学习;基学习机;集成学习机;多样性;泛化能力1引言模式分类与回归问题是机器学习和模式识别等研究领域中常遇到的最基本任务之一,该任务的主要目标是利用实际数据构建一个具有较强泛化能力Page2Dietterich[1]曾在《AIMagazine》杂志上将集成学习列为机器学习领域的四大研究方向之首.集成学习使用多个学习机来解决同一问题,它通过调用一些简单的分类算法,以获得多个不同的基学习机,然后采用某种方式将这些学习机组合成一个集成学习机.随着集成学习研究队伍的不断壮大,集成学习技术得到了快速发展,各种集成学习算法目前也正在被广泛应用于生物、工程、医学、计算机视觉和图像处理等研究领域[2-4].一般地,一个集成学习机的构建分为两步:基学习机的生成和基学习机的合并,现有的许多集成学习算法主要是在这两方面存在差异.在构建集成学习机时,有效地产生泛化能力强、差异大的基学习机是关键,即基学习机的准确性和它们之间的多样性是两个重要因素.目前,常见的用于生成基学习机的方法可以粗略地分为两大类:一类是将不同类型的学习算法应用于同一数据集上,这种方法得到的基学习机通常被称为是异质类型的(heterogeneous);另一类是将同一学习算法应用于不同的训练集(可基于原有的训练数据集进行随机抽样等方法得到),这种方法得到的基学习机被称为是同质类型的(homogeneous).对于生成同质类型基学习机的方法,基于它们获取不同训练集所采用的技术,又可以分为对训练集重抽样(如Bagging[5]、Boosting[6])、操纵输入变量(如随机子空间方法[7]、旋转森林[8])、操纵输出目标(如误差校正输出编码集成方法(Errorcorrectingoutputcodeensemble)[9])、注入随机性(如RandomForest[10]).在生成多个基学习机之后,一个很自然的问题是以何种方式将它们进行合并才能得到具有最强泛化能力的集成学习机?对此,研究者们也提出了很多解决办法,徐雷等人[11]针对模式分类问题,根据基分类器提供的信息水平将现有的合并准则分成了三大类:抽象水平、秩水平和置信值水平.抽象水平是指基分类器的输出是类标签,秩水平假定基分类器的输出是根据分类器的预测效果好坏而对它们赋予的秩序列,而置信值水平则假定每个基分类器的输出是一个概率分布.在置信值水平类的合并准则中,根据是否需要估计额外的参数,它们又可以分为固定的合并准则(fixedrules)和可训练的合并准则(trainablerules).固定的合并准则是基于基分类器的输出对基分类器直接进行合并,常用的有最大值、最小值、中位数、乘积、均值和多数投票(加权或不加权)准则;而可训练的合并准则是将多个基分类器的输出作为新的特征再构建一个更高水平的分类器,常见的方法有神经网络、决策树、支持向量机、Bayes准则、行为知识空间、Dempster-Shafer理论等.值得注意的是,可训练的合并准则需要额外的数据集估计其中的参数.但在实际问题中,一般只有一个数据集可用,因此该数据集必须被用来同时训练基分类器和合并准则.在这种情况下,有三种方法可以达到上述目的:重复使用策略(reusingstrategy)、随机划分策略(validationstrategy)和层叠泛化策略(stackedgeneralization),这三种策略的详细介绍和各自的优缺点可参考文献[12-13].在集成学习的研究初期,大多数方法都是先生成多个基学习机,然后将它们全部用于构建集成学习机.尽管采用这些方法得到的集成学习机的预测效果显著优于单个基学习机,但它们存在一些缺点:与基学习机相比,其预测速度明显下降,且随着基学习机数目的增多,它们所需的存储空间也急剧增多,这对于在线学习更是一个严重问题.因此,人们开始考虑:使用少量的基学习机是否可以达到更好的性能?2002年,周志华等人[14]首先提出了“选择性集成”的概念,肯定地回答了上述问题,并在国内外集成学习界引起了强烈反响.理论分析和试验研究表明,从已有的基学习机中将作用不大和性能不好的基学习机剔除,只挑选一些基学习机用于构建集成则可以得到更好的预测效果.随后,选择性集成学习引起了研究者们的关注并提出了一些有效的算法.这里,值得强调的是,选择性集成学习是在假定已生成多个基学习机的基础上,基于某种选择策略只从中选择一部分用于构建最终的集成.换句话说,选择性集成在选择的过程中,不会再生成新的基学习机,这与用训练数据生成一个Boosting集成的过程中,直接抛弃精度较差的基学习机是完全不同的.为了给选择性集成学习的初学者提供入门指导,并为新算法的设计提供参考,本文将对现有的选择性集成学习算法进行较为全面的综述,阐述近年来对选择性集成学习进行研究的主要内容和特点,通过分析现有算法的特征,将其分类,并对它们在未来的研究方向进行探讨.据我们所知,尽管文献[15]曾对选择性集成学习算法进行了简短的综述,但该文未对一些代表性的选择性集成学习方法进行详细说明,且该文献出现的时间较早,而近两年来在集成学习领域中已经又涌现出了很多有效的选择性集成学习算法.因此,本文的研究是文献[15]工作的进一步完善Page3和补充.本文第2节对用于解决分类问题的选择性集成学习算法进行详细综述,并分析几类算法的特点;第3节讨论选择性集成学习算法在回归预测中的应用;第4节是总结与展望,主要探讨选择性集成学习在未来可能的研究方向.2选择性集成学习算法假定tr={(狓i,yi)}Ntr由于分类问题和回归问题具有不同的特点,需要使用不同的方法进行解决,而集成学习最早是在解决分类问题的过程中提出的.因此,我们在本节中先对现有的关于分类问题的选择性集成学习算法进行讨论,下一节中再探讨基于回归问题的相关算法.在此,我们引入一些记号以方便后文的叙述.训练个体(狓i,yi),其输入变量狓i=(xi1,xi2,…,xip)∈Rp,输出变量yi∈Φ={ω1,ω2,…,ωc},c为类的个数.同时,令val、ts分别表示容量为Nval和Nts的验证集和检验集.在实际应用中,如果只给定了一个数据集,则可以通过随机划分的方法得到相应的训练集、验证集和检验集.此外,我们用={C1,C2,…,CT}表示基于tr训练的基分类器集合,={C2,…,CC中选取出来的基分类器集合.选择性集成学习的主要思想是基于某种衡量准则,从已有的基分类器中选择一些用于构建集成分类器,以加快分类器的预测速度、降低其存储空间需求并进一步提高分类精度.理论上,最优的基分类器子集可以通过枚举法(exhaustiveenumeration)得到.对的每个子集器的预测效果,选择具有最小泛化误差的集成分类器所对应的子集即可,即=Argmin们需要对∑T目T较大时,其计算量极大.因而,枚举法在实际中并不可行.鉴于上述原因,选择性集成学习在近10年中得到了广泛的研究,我们在下面的图1中给出了选择性集成学习算法的基本框架.对于现有的选择性集成学习算法,它们主要在评测方法的选择上存在差异.于是,这些算法大致可以分为以下几类:聚类、排序、选择、优化和其它方法.下面我们对这几种方法的主要思想和特点进行详细分析.择的基分类器个数S,评测方法输入:训练集tr,验证集val,基分类算法,基分类器个数T,选输出:选择的基分类器集合={C1,C2,…,CS}训练过程:初始化:令基分类器集合=.Fort=1,2,…,T基于训练集tr,采用某种技术(如Bootstrap随机抽样方法)获取新的训练集应用基分类算法于EndFor(得到初始的基分类器集合={C1,C2,…,CT})选择过程:在验证集val上对每个基分类器Ct(t=1,2,…,T)进行测试,得其输出Ot.利用评测方法基于Ot(t=1,2,…,T)对中每个元素进行评测,并从中选择出S个基分类器C1,C2,…,CS.2.1基于聚类的方法这类方法[16-21]的主要过程如图2所示.输入:验证集val,基分类器集合={C1,C2,…,CT},聚类算法输出:选择的基分类器集合={C1,C2,…,CS}选择过程:利用每个基分类器Ct(t=1,2,…,T)对验证集val中的个体进行预测,得到矩阵犗=(oij)T×Nval,其元素oij对应于第i个基分类器Ci对第j个个体狓j的预测结果,即oij=Ci(狓j)(i=1,2,…,T;j=1,2,…,Nval).视矩阵犗的每行狅i=(oi1,oi2,…,oiNval)为新的特征空间中的一个观测,犗为包含T个观测的数据集,将聚类算法应用于犗,找到具有类似预测结果的基分类器子集1,2,…,S(即聚类算法最终形成的类).对每个基分类器子集进行修剪,选择出具有代表性的基分类器C1,C2,…,CS.在上述过程中,有3个关键问题需要解决:如何衡量两个基分类器(子集)预测结果(如狅i和狅j,i≠j)之间的相似性?聚类算法选用何种算法?如何确定基分类器子集的个数S?类器Cs和Ct之间的距离为在文献[16]中,Giacinto和Roli定义两个基分d(Cs,Ct)=1-Prob(Csfails,Ctfails),其中,Prob(Csfails,Ctfails)为基于验证集val估计的两个基分类器同时分类错误的概率.同时,他们定义两个基分类器子集i和j之间的距离为d(i,j)=maxCs∈i,Ct∈j然后,采用层次凝聚的聚类算法(hierarchicalagglomerativeclustering)来找到具有相似预测结果的基分类器子集.至于S的确定,则是在聚类算法的每一步,从每个类中挑选出到其它类的平均距离最大的基分类器,采用简单投票的方式将其组合成集成分类器,并基于验证集衡量其分类精度.最后,Page4选择具有最高精度的集成分类器对应的基分类器子集,即={CLazarevic和Obradovic[17]则基于犗中观测的欧氏距离,应用K-均值聚类算法将基分类器进行分组,并利用基分类器子集的准确性和多样性对它们进行修剪.此外,文献[18-21]采用其它一些聚类技术如基于确定性退火(deterministicannealing)的软聚类算法、谱系聚类法、核聚类算法等对多分类器的选择问题进行了研究.2.2基于排序的方法通过对基分类器进行排序来达到修剪集成分类器的目的[22-29]是比较直观的选择性集成学习方法,它们的步骤大致可以分为两步:基于某种衡量标准(如准确性)对基分类器排序,采用合适的停止准则(如事先指定选取的基分类器个数)选取一定数量的基分类器.在下面的图3中,我们列出了这类算法的主要步骤.输入:验证集val,评测标准(如准确性、预测误差等),基分类输出:选择的基分类器集合={C1,C2,…,CS}排序过程:Fort=1,2,…,T器集合={C1,C2,…,CT}在验证集val上,依据评测标准对每个基分类器Ci(i=1,2,…,T)进行评价并排序.根据事先指定的基分类器个数S或评测方法自身确定的S,从选出性能较好的基分类器C1,C2,…,CS.EndFor在文献[26]中,Martínez-Muoz和Surez提出基于Boosting主要思想来对Bagging集成分类器进行修剪的算法,并给出了两种确定参数S的取值方法:(1)直接令S≈40%×T,因为作者之前进行试验的一些结果[23]表明该准则效果较好;(2)第一个Boosting停止点准则,即在选择到某个基分类s的加权训练误差εs>0.5时,停止选择过程.器CMartínez-Muoz和Surez采用16个UCI实际数据集和2个人工数据集进行试验,结果表明上述两种停止准则性能都较好.值得指出的是,文献[26]中选择基分类器的过程是基于训练集tr(同时也用于基分类器的训练)进行的,在实际应用中,为了避免出现过拟合现象,上述选择过程一般要基于验证集val进行.此外,Bryll等人[22]先采用绕封(wrapper)的特征选择方法确定特征子集,基于该特征子集训练基分类器,根据其分类准确性对所有的特征子集进行排序,最后选用得到秩数较高的一些子集所训练的基学习机来构建集成分类器.Martínez-Muoz和Surez[24]定义了一个参照向量(referencevector)和一个对应于Bagging集成中每个基分类器的标号向量(signaturevector),利用相应的标号向量偏离参照向量的程度对基分类器进行排序,通过及早停止合并过程而达到修剪Bagging集成的目的.Croux等人[25]则采用Out-of-bag样本[30]估计基于Bagging技术所生成的每个基分类器的泛化误差,并对其排序,通过预先设置的阈值将泛化误差较大的基分类器剔除.Martínez-Muoz等人[27]则对通过排序修剪Bagging集成分类器的技术作了详细的分析和研究,并指出可用于对基学习机排序的指标大致包括误差的减小量、κ统计量[31]、互补性、边缘距离等.Rokach[28]提出了CAP(Collective-Agreement-basedPruning)选择性集成学习算法,对每个基分类器子集,CAP基于其中每个成员的预测能力和它们之间的冗余性(用对称的不确定性或κ统计量来衡量)对这些子集进行排序,并选择冗余性小且平均预测精度高的基分类器子集.张春霞和张讲社[29]也采用Boosting的思想提出了对Double-Bagging集成分类器[32]进行修剪的技术,并取得了较好的效果.2.3基于选择的方法依据某种选择标准,只选择部分基分类器来参与集成学习是最直观的选择性集成学习方法,现有的多数相关算法[31,33-57]都属于此类.前述的基于排序的方法与此类方法密切相关,从广义上讲,基于排序的方法也属于选择类的方法.在选择类方法中,按照它们是否采用统一模型对检验集中的所有个体进行预测,又可以分为静态选择法(staticselectionmethod)和动态选择法(dynamicselectionmethod).2.3.1静态选择法该类方法[31,33-46]的主要特点是基于已有的基分类器,从中选择一部分构建集成分类器,并用其对所有的检验个体进行预测.各种方法之间的区别是采用不同的度量标准来选择基分类器.静态选择性集成学习算法的基本框架如图4所示.输入:验证集val,评测标准,基分类器集合={C1,C2,…,输出:选择的基分类器集合={C1,C2,…,CS}选择过程:初始化:令=.Fors=1,2,…,S在验证集val上,依据评测标准从中选择效果最好的基分类器Cs.将Cs加入集合中,并将其从中删除,即令=∪Cs,=\Cs.CT},选择的基分类器个数SEndForPage5在文献[31]中,Margineantu和Dietterich提出基于Kappa-Error图来修剪AdaBoost集成分类器,以保证选择的基分类器的准确性和它们之间的多样性,进而使它们组成的集成分类器的预测性能与原来的相比没有明显的下降;Tamon和Xiang[33]进一步改进了该方法,经理论分析,他们发现Boosting集成的修剪问题甚至用逼近的方法都难以解决,于是提出了基于边缘(margin)来选取最优基分类器子集的启发式方法.Caruana等人[34]针对多种不同的学习算法如神经网络、支撑向量机(SVM)、决策树、Bagging和Boosting分类树集成等,通过将这些算法采用不同的参数生成了大量基分类器(2000个),并利用前向逐步选择的方法来最大化选择性集成分类器在验证集上的分类效果.Banfield等人[35]基于基分类器的准确性设计了顺序后向选择方法(sequentialback-wardselection)、AID(AccuracyInDiversity)修剪算法和协同(concurrency)修剪算法.Demir和Alpaydin[36]基于效用理论(utilitytheory),提出了一种对损失敏感且期望效用最大化的选择性集成学习算法,试验结果表明:该方法在实际中通过达到准确性和检验损失之间的折中,可以成功地选择一些基分类器使得它们组成的集成分类器具有较强的泛化能力.Aksela和Laaksonen[37]定义了指数误差数(ExponentialErrorCount,EEC)用于度量基分类器误差之间的多样性,通过最小化EEC来选择最优的基分类器子集,在手写字符数据集上进行试验的结果表明该方法具有较好的效果.Rokach等人[38]针对异质类型基分类器的选择进行了研究,通过定义PEM量(potentialextractmeasure)提出了一种选择性投票(selectivevoting)方法.Shahjahan和Murase[39]则提出了修剪相互协作神经网络集成的算法PNNE(PruningNNen-semble).对每个NN,他们引入了隐节点的一个协作函数来支持衰减过程,并基于负相关学习(nega-tivelearning)增加各个NN间的多样性,进而将过拟合的NN从中剔除.Hu等人[40]提出了FS-PP-EROS(ForwardSearch-PostPruning-Ensemblemul-tipleROughsubspaces)算法,该算法首先基于粗糙集理论对数据的特征进行约简(roughset-basedattributereduction),得到一些特征子集,然后基于每个特征子集训练一个基分类器,并采用精度驱动的前向搜索和修剪的策略选择基分类器.唐耀华等人[43]利用ξα误差估计法度量个体SVM泛化性,并基于负相关学习引入差异性,通过递归删除法选择一组泛化能力优良、差异性大的SVM参与集成学习.针对半朴素贝叶斯分类器(seminaiveBayesianclassifier)族中的超1-依赖贝叶斯分类器(Super-Parent-One-DependenceEstimator,SPODE)在集成学习过程中应该进行选择还是加权合并的问题,Yang等人[41]采用58个数据集对16种方法进行了大规模的研究,为该类方法在实际中的应用提供了有效的指导.Folino等人[42]提出基于分格遗传规划(cellulargeneticprogramming)的方法来构建Boosting集成分类器,并采用遗传规划中的结构多样性(structuraldiversity)度量(主要有两个树之间的距离、一个树到空树的距离和κ统计量)来对集成分类器进行修剪.Meynet和Thiran[44]在信息论框架下建立了基分类器的准确性和多样性之间的联系,并建议基于信息理论得分(InformationTheoreticScore,ITS)来修剪集成分类器,相比于使用多样性来选择最优基分类器子集的方法,ITS算法更具优势.Ting和Witten[45]则提出用交叉确认法选择一个最好性能的基分类器来对检验数据进行预测.对于商业中的破产预测(bankruptcyprediction)二分类问题,Hung和Chen[46]建议基于破产和不破产的期望概率从决策树、后向传播神经网络和SVM三个分类器中进行选择,以汲取它们的优点并克服其缺点.经实际验证,这种方法比其它的加权或投票方法的预测效果要好.2.3.2动态选择法这类方法[47-57]的主要特点是对检验集ts中的每个个体狓,从已有的基分类器C1,C2,…,CT中动态挑选合适的一部分对其进行预测,每个个体选用的基分类器子集一般是不同的.图5给出了动态选择性集成学习算法的主要步骤.输入:验证集val,检验数据集ts={狓i}Nts输出:检验数据的类标签集合Φ={yi}Nts选择过程:Fori=1,2,…,Nts针对检验样本点狓i,在验证集val上寻找它的k个最近邻样本点;根据评测标准评价每个基分类器Ct(t=1,2,…,T)在k个最近邻样本点上的性能;选取一个或多个较好的基分类器对狓i进行预测,得到估计的类标签yi.类器集合={C1,C2,…,CT},邻域点个数kEndForWoods等人[47]提出了一种基于局部精度的动态选择法DCS-LA(DynamicClassifierSelection-Page6LocalAccuracy),通过在5个实际数据集上将DCS-LA的分类精度和ROC(ReceiverOperatingCharacteristic)曲线下的面积与其它几种合并准则(行为知识空间BKS、基于秩的方法和改进的秩方法)进行比较,发现DCS-LA总能改进具有最高预测精度的单个分类器的性能.Giacinto和Roli[48]根据多分类器的行为提出了DCS-MCB算法(DynamicClassifierSelection-MultipleClassifierBehavior),它与上述的DCS-LA算法存在两点差别.在DCS-MCB中,k是随着检验个体的不同而发生变化的.另外,只有当一个基分类器的LA明显高于其它分类器时,DCS-MCB才选择单个分类器对狓预测;否则,它采用简单多数投票的方式构建集成分类器来对狓分类.与DCS-LA密切相关的还有Kuncheva[50]提出的DCS-DT算法(DynamicClassifierSelection-DecisionTemplates),该方法的思想与DCS-MCB类似,但它采用成对的t检验(pairedt-test)方法来判别一个基分类器是否在LA方面具有显著优势,如果各个基分类器LA之间的差别不显著,则基于决策表矩阵(decisiontemplatematrix)来构建集成分类器对狓预测.Didaci和Giacinto[51]发现DCS-LA方法的性能不仅依赖于邻域形状和大小的选择,同时还与训练个体的局部密度分布(localdensitydistribution)有关.于是,他们提出了一种自适应选取邻域以更好估计LA的方法,试验结果表明通过适当调整一些额外参数,可以进一步改进DCS-LA的预测效果.Canuto等人在文献[52]中较全面地研究了异质类型基分类器的数目T的变化对上述几种DCS方法(DCS-LA,DCS-MCB和DCS-DT)性能的影响,并得出结论:DCS-DT在多数情况下效果较好.此外,Fan等人[49]针对集成学习在对损失敏感(cost-sensitive)的大规模数据集上的应用,提出了动态策略(dynamicscheduling)与基于收益的贪婪方法(benefit-basedgreedypruning)相结合的集成分类器修剪算法,该方法可以在不损失预测精度的前提下,剔除约90%的基分类器.Ko等人[53]基于Oracle的概念提出了4种动态选择基分类器子集的方法(KNORA-ELIMINATE、KNORA-UNION、KNORA-ELIMINATE-W、KNORA-UNION-W).它们的特点是对检验个体狓,基于验证集val确定狓的k个邻域点,挑选出那些对k个邻域点分类正确的基分类器,并根据简单投票或加权投票的方式合并被选出的基分类器来预测狓的类标签.DosSantos等人[54]提出了优化和动态选择相结合的两阶段选择法,目的是对检验集tr中的每个个体都搜索到最优的基分类器子集.优化阶段主要是为了产生具有高精度的集成分类器总体,优化目标是集成分类器的分类误差和多样性;而动态选择阶段则为每个检验个体,挑选出具有最高置信度的集成分类器,用于度量置信度的指标可以是模糊性(ambiguity)、边缘(margin)和相对于最近类的力量(strengthrelativetotheclosestclass).Cavalin等人[56]则从文献[54]提出的方法DSA(DosSantosetal’sApproach)出发,通过利用验证集val提供的上下文信息(contextualinformation)和基分类器提供的证据(evidence),并引入切换策略(switchmechanism)以处理类得票相等(tie-breaking)和大边缘决策(large-margindecision)问题,提出了一种新的动态选择分类器集成的方法DSAc.考虑到实际数据中通常含有噪声,Xiao等人[57]在对噪声具有较强免疫性的启发式数据挖掘方法GMDH(GroupMethodofDataHandling)的基础上,根据基分类器的准确性和多样性提出了动态选择集成分类器的GDES-AD算法(GMDH-basedDynamicclassifierEnsembleSelectionaccordingtoAccuracyandDiversity).采用误差的偏差-方差分解对集成分类器的研究结果表明,GDES-AD对噪声的强免疫能力主要在于它在降低分类误差的偏差方面更具优势.此外,Hernndez-Lobato等人[55]在贝叶斯理论框架下,探讨了通过将一个随机学习算法应用于一个给定的训练集上所生成的独立的同质类型基分类器的选择问题,提出了基于个体的修剪方法(Instance-Basedpruning,IB).Hernndez-Lobato等人经理论分析,得出结论:对需要预测的个体狓,若基于逐一添加基分类器和简单多数投票的合并方式来构建集成分类器,当对狓预测的精度达到事先给定的置信水平且对狓预测的类标签不再发生变化时,即可停止基分类器的添加.基于一些分类问题的基准数据集,Hernndez-Lobato等人采用Bagging和RandomForest技术生成基分类器,验证了理论分析的正确性和IB算法的有效性,并指出需要选择的基分类器个数S是与要预测的个体狓密切相关的.当基分类器对狓的预测结果基本一致时,只需少数几个来构建集成分类器即可达到较好效果;对于其它情形,尤其是狓接近分类边界时,则需较多的基分类器.Page72.4基于优化的方法这类方法[14,58-69]的主要思想是在基分类器的合并过程中对它们赋予权重,通过稀疏性约束或设置阈值,借助于优化算法来选择最优的基分类器子集.图6列出了基于优化的选择性集成学习算法的一般步骤.输入:验证集val,基分类器集合={C1,C2,…,CT},评测标准输出:选择的基分类器集合={C1,C2,…,CS}选择过程:对每个基分类器Ct(t=1,2,…,T)赋予初始权重ωt;根据评测标准,选择合适的目标函数f(如遗传算法中的适应基于验证集val,利用算法优化目标函数f,得到最优权重向量选择满足条件ω>λ的元素所对应的基分类器C1,C2,…,CS.值函数等);ω=(ω1,ω2,…,ωT);,优化算法,阈值λ图6基于优化方法的选择性集成学习算法框架在基于优化方法的选择性集成学习算法中,文献[14,58-60]均采用遗传算法(GeneticAlgorithm,GA)来优化每个基分类器被赋予的权重,而它们之间的主要区别在于采用了不同的遗传算法编码方法(二进制或实值).在2002年,周志华等人[14]首次从理论上证明了选择性集成学习的有效性,并提出了基于实值编码遗传算法的选择性集成学习方法GASEN(GeneticAlgorithmbasedSelectiveENsemble).在试验过程中,他们将算法中的阈值λ设定为0.05,研究了上述策略修剪Bagging和Boosting神经网络集成的效果,并得出结论:在所探讨的数据集上,GASEN方法都可以用较少的神经网络得到更好的泛化效果.同时,他们还指出通过调整遗传算法中的适应值函数、编码方法、遗传算子以及阈值λ,GASEN算法的性能都可以得到进一步改进.王丽丽和苏德富[61]针对已有的选择性集成学习方法计算复杂性高、效率低的特点,提出将具有快速收敛性质的群体智能算法用于多分类器的选择.Zhang等人[62]将多分类器的选择问题看成是二次整数规划问题(quadraticintegerprogramming),并给出了一种半定规划方法SDP(Semi-DefinitePro-gramming),试验结果表明该方法可以比其它启发式方法更好地逼近基分类器的最优子集.Chen等人[63]则从概率推理的角度出发提出了一种新的选择性集成学习算法,为了使每个基分类器的权重非负且尽量稀疏(即多数基分类器得到的权重为0),算法采用左截断的高斯分布(left-truncatedGaussiandistribution)对每个基分类器的初始权重赋予先验知识,并基于期望扩散(ExpectationPropagation,EP)算法来估计权重的后验分布.该算法的优点是在EP算法的训练过程中,不需要额外的计算即可获得LOO(Leave-One-Out)误差,结合贝叶斯证据(Bayesianevidence)也可以实现模型选择.为了避免标准的线性最小二乘回归合并准则(standardlinearleastsquaresregression)出现过拟合现象,Reid和Grudic[66]提出将基于层叠泛化策略(stackedgeneralization)的正则化线性模型用于基分类器的合并,模型中的未知参数可以用岭回归(ridgeregression)、Lasso回归(Lassoregression)和弹性网回归(elasticnetregression)等方法估计得到.Li和Zhou[65]在正则化的理论框架下,基于Hinge损失函数(对于Φ={-1,+1}的二分类问题,其定义为l(yi,C(狓i))=max(0,1-yiC(狓i)))和图的拉普拉斯正则子(graphLaplacianregularizer)将基分类器的选择问题归结为具有稀疏解的二次规划问题,并通过相关的优化算法(文中用的是MOSEK软件)进行求解.算法除了能用少数的基学习机达到较强的泛化能力之外,还可以充分利用无标签的数据(unlabeleddata)来进一步提高分类系统的性能.Zhang和Zhou[69]也基于正则化理论,采用Hinge损失函数和1正则项提出了一种基于线性规划的方法,优化的目标是最小化集成分类器的训练误差同时控制权向量,以使得最终得到的权向量尽量稀疏.DosSantos等人[64]针对决策树和K-NN(KNearestNeighbors)作为基学习算法,采用Bagging和随机子空间技术训练的基分类器,提出以分类误差和多样性为优化目标,并基于单目标和多目标的遗传算法(multi-objectivegeneticalgorithm)进行优化求解.试验结果表明,多目标优化的遗传算法除了能找到近似最优的基分类器子集之外,还可以控制过拟合现象的产生.此外,文献[64]中以多样性为单目标优化函数的研究还有助于探索基分类器之间的多样性与集成分类器性能之间的关系.杨晓霜和汪源源[68]则提出了一种基于Moore-Penrose逆矩阵的新型选择性集成学习算法PISEN(Pseudo-InversematrixbasedSelectiveENsemble),它基于矩阵的伪逆理论对每个基学习机的权值进行优化,并通过预先设定的阈值来选择最后的基学习机子集,算法的优点是简单、易于实现且效率较高.基于8个UCI实际数据集,作者将PISEN算法(阈值设为0.1)与文献[14]提出的GASEN算法(采用该文中的参数设置)进行了比较,发现二者的泛化误Page8差相当,但前者的计算效率要远远高于后者.此外,王磊[67]则基于约束投影矩阵法首先训练多个SVM,然后采用遗传优化和最小化偏离度误差的技术来对SVM进行组合.2.5其它方法除了上面提到的几类选择性集成学习算法,还有一些算法不能归入其中,我们在此对其作一简单介绍.Prodromidis和Stolfo[70]基于基分类器的预测结果构建一个决策树,并对树进行修剪,如果某个基分类器的预测结果不在被修剪后的树中,则将它从构建集成分类器的过程中剔除.Tsoumakas等人[71]建议采用多重统计检验的方法(如Turkey检验、Hsu检验、Scott和Knott检验等)来选择最优的基分类器集合,使得其中每个基分类器的预测精度基本相当,且集合中至少有一个基分类器的预测性能显著优于未被选择的任何一个基分类器.Partalas等人[72]则首次将增强学习(reinforcementlearning)的概念引入选择性集成学习中,提出了一种新的有效算法.盛高斌[73]针对小数据量聚类的有标记样本问题,提出了一种基于半监督回归的选择性集成学习算法SSRES(Semi-SupervisedRegressionEn-sembleSelection).Zhang和Chau[74]提出了基于多子群粒子群优化算法(Multi-Sub-SwarmParticleSwarmOptimi-zation,MSSPSO)的多层次修剪模型,在每一层修剪过程中,模型假定每个基分类器都产生一个明智的输出(oracleoutput),并把基分类器的选择看成是多模态的优化问题,基于前一层基分类器的输出采用MSSPSO算法进行求解,最终选择出包含重要信息的基分类器.Soto等人[75]针对二分类问题,将前述的对基分类器进行排序的方法与基于个体(Instance-Based,IB)的动态修剪技术相结合,提出了一种两层的修剪算法.算法首先采用排序(如基于Boosting的主要思想[26])的方法选出约20%的基分类器并设定置信值α,在对某个个体狓预测时,通过简单多数投票的方式逐一合并选出的基分类器,直至集成分类器预测的某个类的概率大于α.该算法的优点是能明显提高集成分类器的预测精度、降低存储需求(只需存储约20%的基学习机)、加快预测过程(基学习机的数目减少且IB修剪技术加快了对每个个体的预测).3基于回归问题的选择性集成学习算法解决回归问题的多数选择性集成学习算法都是经过对用于分类问题的相关算法进行推广得到的,如文献[3,14,29,76-81]等.若将这些算法按照其特点进行分类,则也可以类似地分为基于聚类、排序、优化和选择的方法.其中,Partalas等人[3]针对选择性集成学习在水质量预测问题中的应用,提出基于学习机的均方误差根(root-mean-squared-error)采用前向选择(forwardselection)和后向删除(back-wardelimination)的策略贪婪地搜索最优的基学习机子集.周志华等人[14,78]建议采用实值编码的遗传算法和设置阈值的方法来对基学习机进行选择.Hernndez-Lobato等人[77]基于基学习机之间的互补性对它们进行排序,并采用贪婪算法从中选取约20%的基学习机.与分类问题相比,对基于回归问题的选择性集成学习算法进行的研究相对较少,究其原因,大概是由以下因素造成的.选择性集成学习的目的是在不降低甚至进一步提高原集成学习机预测精度的前提下,尽可能减少参与集成学习的基学习机的数目,以大大降低其对存储空间的需求并加快预测速度.然而,在回归问题中,采用某种策略对基学习机进行选择之后,被剔除的基学习机往往较少,而在预测精度的改进方面效果也不太明显.4总结与展望本文对机器学习领域中一些具有代表性的选择性集成学习算法作了一个比较全面的综述,并分析了几类方法的主要特点.选择性集成学习的提出极大地丰富了集成学习的相关理论,并为其它相关领域的研究提供了新技术和新思路,研究前景广阔.近年来虽然仍有很多研究人员致力于该方面的研究,发表了很多研究成果,但目前的方法多是针对某个具体任务而言的,其中尚有一些问题需要在未来研究中得到突破:(1)若干关键参数的确定.例如,在基于聚类技术、排序和选择的方法中,如何确定最终保留的基学习机的数目S?在基于优化的方法中,如何设置权重的阈值以剔除某些效果较差的基学习机?针对这些参数,目前采用的大多是启发式的方法.如果选取不合适,则会大大影响预测效果.如何根据具体问题,自适应地选取这些关键参数将是一个值得研究的内容.(2)选择基学习机的衡量指标的选定.基学习机的准确性和它们之间的多样性在集成学习机的构建过程中起着至关重要的作用,只有当两者达到一Page9个较好的折中,集成学习机才能具有较强的泛化能力.然而,多样性在实际中难以衡量,且多样性和准确性与集成学习机预测性能之间的有效联系也较难建立.在选择性集成学习算法的设计中,如何选取合适的准则或度量标准将准确性和多样性因素充分考虑在内,也是一个需要解决的关键问题.(3)选择性集成学习算法的应用研究.目前,一些选择性集成学习方法已经应用在疾病诊断、人脸识别、图像挖掘等方面.研究该类方法更广泛的实际应用以及现有的机器学习方法在选择性集成学习中的应用都将有十分重要的意义.
