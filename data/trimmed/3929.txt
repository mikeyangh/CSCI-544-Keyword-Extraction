Page1一种双层条件随机场的场景解析方法李艳丽周忠吴威(北京航空航天大学虚拟现实技术与系统国家重点实验室北京100191)摘要现有的场景解析方法主要依赖于先验模型,由于先验模型难于全面表示物体的各种细节部分,使得场景解析后的物体不够精细.针对这个问题,该文引入了局部颜色模型,提出了一种结合先验和局部颜色模型的双层条件随机场的场景解析方法.首先以超像素为结点构建一个条件随机场,根据颜色、梯度、纹理和几何等外观特征训练出的先验模型粗略解析场景,进而提取场景中每个物体的局部颜色模型;然后构建一个以像素点为结点的条件随机场,通过EM(Expectation-Maximization)迭代法更新物体的局部颜色模型来指导优化场景解析.实验结果表明,相比于以往单纯利用先验模型的场景解析方法,该方法能有效地保持场景细节、提高解析精度.关键词场景解析;先验模型;局部颜色模型;条件随机场;EM迭代1引言场景解析是识别和分割图像内各个物体的技术,例如从街景图像中分解出天空、道路、行人和车场景解析法往往建立在马尔可夫随机场下以保证邻居结点的标识一致性.Lafferty等人[1]在2001年提出的条件随机场解决了其他马尔可夫模型难以Page2避免的标识偏置问题.2004年,He等人[2]首次实现了条件随机场下的场景解析,此后研究人员提出了各种基于像素点或超像素的条件随机场下的场景解析法.然而,以像素点为计算单元的方法需要获取大量特征,存在计算效率低的问题[2-3];以超像素为计算单元的方法受超像素的影响无法完整保留物体边缘[4-5].为了更精细地解析场景,最近几年研究人员提出了多层超像素下的场景解析方法[6-9]以及结合像素点和超像素的高阶条件随机场下的场景解析方法[10-12].前者将场景解析描述为多层超像素的识别和组合问题.由于多层超像素可获取更多分割粒度的聚类块,此类方法有效地避免了单层超像素的欠分割问题.后者在二阶条件随机场上增加了一个由像素点和超像素搭建的高阶项,从而将其扩展为高阶条件随机场.由于此类方法中的高阶项是对像素点的软聚类,相比于基于超像素的方法对像素点的硬聚类,解析出的物体边缘更精细.上述方法都是依赖先验信息来分离场景中的物体,即从大量训练数据统计出的先验模型指导场景解析.然而,先验模型只反映了物体间的共性特征,难以全面表达物体内的细节特性.我们注意到在图像分割领域还存在交互式的前景分割算法[13-15].交互式分割方法根据物体颜色具有内敛性的特点,依赖局部颜色模型来分离前景和背景.局部颜色模型代表了图像中物体内特有的属性,跟先验模型具有互补性.与上述方法不同,本文从另一个角度来提高场景解析精度,即将局部颜色模型引入到场景解析中,提出结合先验和局部颜色模型的场景解析方法.考虑到计算效率,首先以超像素为计算单元获取先验模型来粗略地解析场景,然后以像素为计算单元获取局部颜色模型进一步精细地解析场景.本文的主要贡献点在于提出了一种结合像素点和超像素的双层条件随机场,该条件随机场利用先验模型和局部颜色模型的互补性来保留场景细节.将该方法与基于像素点的方法[3]、基于超像素的方法[5]以及结合像素点和超像素的方法[10]进行了实验比较,结果表明本文方法引入局部颜色模型后使得场景解析结果更精细.本文第2节介绍场景解析方面的相关工作;第3节描述引入局部颜色模型的双层条件随机场;第4节给出双层条件随机场下的场景解析;第5节为实验结果和比较讨论;最后进行总结.2相关工作场景解析问题早在20世纪70年代就已提出[16],直到近年随着底层算法的成熟才成为计算机视觉的研究热点.现有的场景解析方法基本上可分为基于像素点的方法、基于超像素的方法以及结合像素点和超像素的方法.基于像素点的方法以像素点为计算单元.He等人[2]最早提出了条件随机场下基于像素点的场景解析法,首先由神经网络训练像素点的颜色特征获取先验模型,然后求解一个条件随机场下的全局能量函数完成场景解析.由于像素点的局部特征不包涵物体的全局统计信息,此类方法的准确性较差.另外,以像素点为计算结点的计算量大,此类方法的效率也偏低.基于超像素的方法以超像素为计算单元,其中的超像素是根据图像底层颜色信息聚类的像素块.Yang等人[4]最早提出了基于超像素的条件随机场下的场景解析方法.相比于基于像素点的方法,基于超像素的方法计算结点少,因此效率较高.然而每个超像素算法都存在欠分割或过度分割问题.欠分割无法完整保留物体边缘,而过度分割提取不到有价值的全局特征,此类方法解析出的场景比较粗糙.为了完整的保留物体轮廓,Caroline等人[6]、Stephen等人[7]、Kumar等人[8]和Cheny等人[9]提出了多层超像素下的场景解析方法.其中,Caroline等人[6]用超像素算法将图像分割为18层,以超像素的交叉区域为计算单元在一个条件随机场下解析场景;Stephen等人[7]获取了3层超像素空间,将场景解析描述为分割块选取和能量优化两个问题,用梯度下降法迭代优化;Kumar等人[8]则将场景解析描述为超像素选取的整形规划问题;类似的,Cheny等人[9]提出了一种增加上下文约束的整形规划式的场景解析方法.此类方法能精细解析场景主要建立在两个假设基础上:(1)算法能选取最佳分割块组合;(2)该组合内的分割块边缘能完整表示物体轮廓.然而这两种假设不一定总成立.为了避免超像素的欠分割问题,近年研究人员提出了结合像素点和超像素的高阶条件随机场下的场景解析方法[10-12].Kohli等人[10]最早把高阶条件随机场引入到场景解析中,通过将每个分割块内的像素点聚类成一个高阶项,求解一个以像素点为计算单元的高阶能量来解析场景.基于该工作,Kohli等人[11]又提出了一种泛化的高阶条件随机场,任何一种高阶条件随机场下的场景解析方法都可看成其参数调整后的特例.在此基础上,他们又提出增加共存性约束的高阶条件随机场下的场景解析方法[12].相比于基于超像素的方法对像素点的硬聚类,此类方法对像素点进行了软聚类,因此使得物体边缘更Page3平滑,解析的场景更精细.本文方法也属于结合像素点和超像素的方法,跟Kohili法[10-12]的不同在于:(1)Kohili法建立在一个高阶条件随机场下,本文方法建立在一个双层的二阶条件随机场下;(2)Kohili法的贡献点在于引入了像素点和超像素之间的高阶项以避免超像素导致的欠分割问题,本文贡献点在于引入了局部颜色模型以避免先验模型导致的统计特性偏差问题.3引入局部颜色模型的双层条件随机场3.1场景解析问题的数学描述给定一幅图像I,场景解析是对其中的像素点或超像素X={i}做自动标识L={li}的问题.其中,li代表了标识类别,如天空、道路、树木、行人和车辆等等.由于图像中存在空间马尔可夫性,在以像素点或超像素为结点构建的图结构G=〈X,Y〉(Y为相邻结点组成的边集合)中,现有方法一般将结点标识定义为一个二阶条件随机场下的能量函数[2-5]:E(L)=∑i∈X其中,(·)为数据项,该项根据结点特征计算单结点的标识误差,从而使得结点赋值最佳标识;φ(·,·)为平滑项,该项根据结点特征差计算相邻结点的标识误差,从而使得相邻结点赋值一致性的标识;θ和θφ是数据项和平滑项中的参数;λ为权重,用来平衡数据项和平滑项的比重,是经过实验分析得到视觉满意结果下选定的经验值.场景解析结果为最小能量下的标识:3.2双层条件随机场的建立基于上小节描述的单层条件随机场,我们设计了一个两层条件随机场的方法来解析场景.首先以超像素为结点构建一个条件随机场,在先验模型约束下初步解析场景;然后以像素点为结点再构建一个条件随机场,在先验和局部颜色模型联合约束下对场景再次解析.考虑到计算效率,初次场景解析以超像素为计算结点.图1为一个建立在超像素上的条件随机场示意图,其中的边连接了两个相邻的超像素.由于特定物体往往出现在特定场景下,本阶段从两个角度对场景解析,首先根据图像的全局特征识别出场景类别,即判断图像是属于街景还是室内场景等,然后在场景识别基础上根据分割块的外观特征解析出场景中的物体类别,即判断每个分割块是属于天空还是道路等等.相应的,所依赖的先验模型包括全局场景先验模型和局部分割块先验模型.两者都是以监督学习方式从先验数据训练获取的,在整个场景解析过程中保持不变.在全局场景和局部分割块先验模型约束下的条件随机场能量函数定义为E(Ls,S)=∑i∈Xs其中,XS为超像素结点集合,YS为边集合,S为场景标识,Ls={ls验模型约束下的数据项,s(·)为局部分割块先验模型约束下的数据项,φs(·,·)为相邻分割块间的平滑项,θs割块先验模型和平滑项中的参数,λk和λs为权重.场景解析结果为最小能量下的联合标识:由于超像素导致的欠分割以及物体外观多样性,该解析结果往往比较粗糙.在此基础上,我们引入局部颜色模型,以像素点为计算结点对场景进一步精细解析.图2为一个建立在像素点上的条件随机场的示意图.其中,条件随机场的结点为像素点,边连接了四连通的邻居结点.局部颜色模型是根据当前解析结果统计出来的,在解析过程中可迭代更新,其初始值来源于初次解析结果.在先验和局部颜色模型联合约束下的条件随机场的能量函数为Page4E(Lp)=∑i∈Xp其中,Xp为像素结点集合,Yp为边集合,Lp={lp像素点标识;s(·)是根据初次解析结果得到的先验数据项,p(·)为局部颜色模型约束下的数据项,φp(·,·)为相邻结点间的平滑项;λp和λc为权重.最终的场景解析结果为该条件随机场下最小能量对应的标识:4基于双层条件随机场的场景解析我们在上一节引入了局部颜色模型,建立了双层条件随机场,在此基础上设计一个具体的三步骤场景解析法.首先,根据全局场景先验模型识别场景类别;其次,以超像素为计算结点依赖先验模型完成场景的初次解析;最后,提取物体的局部颜色模型,在以像素点为计算结点、局部颜色模型约束下的条件随机场中用EM迭代法[17]优化解析.4.1场景全局语义识别全局场景先验模型结合了3种全局特征:Gist特征[18]、颜色直方图和图像的缩放图.其中,Gist特征是一种梯度直方图描述子,通过将原图切分为4×4个小图,统计每个小图3通道(RGB空间)上20维梯度直方图获取一个960维描述子;颜色直方图是一种全局颜色直方图描述子,统计原图像3通道(RGB空间)上8维颜色直方图获取一个24维描述子;缩放图是原图16×16尺寸3通道(RGB空间)的小图,一共768维.因此从每幅图像可提取一个1752维的全局特征描述子.在训练阶段,提取图像的全局特征描述子用随机森林分类器[19]进行训练,将获取的分类器模型(模型参数为θs段,同样提取图像的全局特征,通过全局场景先验模型预测属于每类场景的概率p(S|I,θs中的场景数据项为为了验证该步骤的有效性,我们对MSRCv2数据集进行了测试.该数据集包括20类场景,每类场景大约有30幅图像,一共是591幅图像.我们从每类中随机选取6幅图像,一共120幅图像作为测试数据,其它471幅用作训练数据来获取分类器模型.假设图像I的前a个最大可能标识下的场景标识集合为ST(a,I),则对于测试集ISet,定义其场景识别率为前a个最大可能标识中存在正确标识的概率,即其中,lb(I)为图像I的真实场景标识,δ(·)为diracdelta函数,δ(true)=1,δ(false)=0,Num(ISet)为训练数据中的图像个数.表1为上述MSRCv2数据集的场景识别率.表1全局场景先验模型下MSRCv2数据集的识别率场景识别数a12从表1中可以看出,该方法能高概率地识别出场景类别,前a=4个最大可能标识下的识别率高达100%,证实了由随机森林分类器结合三类全局特征训练出的全局场景先验模型可有效识别场景类别.在实验中,我们选出前a=4个最大可能标识为候选场景.4.2场景的初次解析局部超像素先验模型建立在Graph-Based超像素法[20]基础上,并结合了4类外观特征,即颜色、梯度、纹理和几何特征.其中,颜色特征包括HSV颜色空间的均值(3维)、HSV颜色空间的亮度分量直方图(5维)和饱和度分量直方图(5维),一共是13维颜色描述子;梯度特征建立在稠密SIFT特征[21]上,首先均匀采样出图像集的SIFT特征,然后用K-Means[22]将SIFT特征聚类成100个中心,并根据聚类中心量化SIFT特征得到SIFT量化子,最后统计分割块中SIFT量化子的直方图得到一个100维梯度描述子;纹理特征是由48个49×49维纹理滤波子①对图像滤波生成的一个48维纹理描述子;几何特征包括:分割块的中心点(2维)、水平方向的最高和最低位置(2维)、垂直方向的最高和最低位置(2维)、分割块包围盒的长宽比(1维),一共是7维几何描述子.因此从每个分割块中可提取一个168维的外观特征描述子.由于我们在场景识别阶段已经对场景分类,下面对每类场景独立训练以避免不同场景中外观相似物体出现混淆.由随机森林分类器[19]对先验数据的分割块特征进行训练,以训练出的分类器模型为分割块外观先验模型,其中的模型参数为θsk=1,…,K,K为从训练数据中获知的场景标识数.定义式(2)的分割块数据项为s(ls①http://www.robots.ox.ac.uk/~vgg/research/texclass/Page5定义式(2)的平滑项为i,ls其中,p(ls块标识概率.φs(ls其中,δ(·)为diracdelta函数,w(ls的边缘权重,我们同样以监督学习方式训练出的平滑项先验模型计算该值.平滑项先验模型所依赖的边缘特征为相邻超像素外观特征差的绝对值,边缘标识为根据标准解析图计算的δ(·)值.由随机森林分类器[19]结合边缘特征和边缘标识进行训练,获取K组二值分类器模型作为平滑项先验模型(模型参数为θs将式(2)中的权重经验值设定为λk=0.8,λs=1.2,最后由最大流/最小割算法[23]优化求解式(2)完成场景的初次解析.图3展示了MSRCv2中某个场景(图3(a))的初次解析结果.其中,图3(b)为原图的超像素图,图3(c)、(d)分别为牛和草的标识概率图(黑色为低概率,白色为高概率),图3(e)为初次解析结果,图3(f)为标准解析图.φ={θsw(ls4.3场景的再次解析从图3(e)可以看出,基于先验模型的场景解析虽然可以粗略识别和分割出物体,但无法保留场景中的一些细节如牛腿、牛尾巴等,主要原因是超像素算法导致的欠分割和物体外观的多样性.借鉴于GrabCut[14],我们引入局部颜色模型对场景进一步精细解析.与GrabCut不同的是:(1)GrabCut的输入为交互式的前景包围盒或前景/背景笔划,本文方法输入为初次解析结果,因此是全自动的;(2)GrabCut仅分割前景和背景两个物体,本文方法将其扩展为多物体的分割;(3)GrabCut在分割过程中仅利用了局部颜色模型,而本文方法结合了先验模型.在再次解析阶段用EM迭代法[17]不断更新局部颜色模型参数来指导场景解析,实验表明经过2或3次迭代后场景解析结果趋于稳定,因此将EM迭代次数的经验值设定为3.M-步骤.用于更新局部颜色模型的参数.用RGB空间中的颜色高斯混合模型表示局部颜色模型,其参数为θpn=1,…,N.其中,N为场景中的物体数目,M为颜色高斯混合模型的簇数(考虑到场景物体一般有5部分以下不同颜色的部件组成,将M值固定为5),μnm为每簇颜色的均值,Σnm为颜色协方差,wnm为相应的权重.在初次解析后我们根据解析结果估计出场景中的物体数目N.在迭代解析过程中,首先根据当前解析结果将同类物体的像素颜色值汇总,然后用K-Means聚类法[22]将每类物体颜色聚为M簇,最后统计每个聚类簇的均值、方差和权重来更新模型.E-步骤.用于优化解析结果.在3.2节式(3)描述了该阶段条件随机场下的能量函数.其中的数据项包括先验模型数据项s(lp据项p(lp初次解析结果,即将初次解析过程中计算出的超像素先验概率传递到块内的各个像素点上.局部颜色模型数据项定义为p(lpp,m(lp其中,G(·)为正态分布.平滑项定义为φp(lp其中,d(i,j)=‖Ii-Ij‖,Ii为像素点i的颜色,β为图像内所有相邻像素颜色差的数学期望值,δ(·)为diracdelta函数,将权重λc、λp的经验值设为2.0、50.最后,由最大流/最小割算法[23]求解式(3)对场景再次解析.图4为图3(a)的再次解析结果.其中,图4(a)、(b)分别为牛和草坪的标识概率图(黑色为低概率,白色为高概率),图4(c)为再次解析结果.从图4中可以看出,相比初次解析结果(图3(e)),Page6再次解析结果更精细,完整地保留了边缘细节,如牛尾巴、牛腿处等.算法1.本场景解析法的算法流程.输入:训练图像及其场景标识和标准解析图,测试图像输出:测试图像的解析图训练过程:步骤1.训练出全局场景先验模型①提取训练图像的全局特征和场景标识;②由随机森林分类器训练获取全局场景先验模型.步骤2.训练出局部分割块先验模型③提取分割块的外观特征和分割块标识;④由随机森林对不同场景标识的测试图像独立训练,获取局部分割块以及平滑项先验模型.解析过程:步骤1.计算场景标识提取测试图像的全局特征,计算场景标识概率,选前4个最大可能标识为候选场景.步骤2.场景的初次解析提取分割块的外观特征,计算式(2)中的数据项和平滑项,由最小割算法求解式(2)实现场景的初次解析.步骤3.用EM法迭代解析场景M-步骤.由当前解析结果更新局部颜色模型;E-步骤.根据先验和局部颜色模型计算式(3)中的数据项,用最小割法求解式(3)对场景再次解析.5实验结果和讨论我们用两组公共的数据集MSRCv1和MSRCv2①对本文方法进行测试.MSRCv1是由13种物体(建筑物、草、树、牛、马、羊、天空、山、飞机、水、人脸、汽车、自行车)组成的240幅图像,MSRCv2是由23种物体(建筑物、草、树、牛、马、羊、天空、山、飞机、水、人脸、汽车、自行车、花、路牌、鸟、书、椅子、道路、猫、狗、人体、船)组成的591幅图像.这两组图像集均有标准解析图,每幅图像大小为320×213左右.在训练阶段,我们根据场景标识将MSRCv1分为8类,将MSRCv2分为20类.从每类场景中随机选取20%的图像为测试数据,剩余图像为训练数据.本文方法在两层条件随机场下解析场景,图5为部分图像的初次和再次解析结果.如图5所示,虽然初次解析阶段可以粗略地提取物体,然而由于超像素算法导致的欠分割使得物体边缘轮廓比较粗糙.经过再次解析后,场景被分割的更精细,物体的边缘细节得以完整保留.将本文方法与近年的Texton[3]、STAIR[5]和ALE[10]方法进行了比较.这3个方法都是利用先验模型建立在条件随机场下的场景解析法.其中,Texton[3]建立在像素点基础上,用JointBoosting分类器对纹理、颜色、位置等特征进行监督学习;STAIR[5]建立在超像素基础上,用DiscreteBoosting分类器对纹理、形状、位置等特征进行监督学习;ALE[10]结合了像素点和超像素,用随机森林分类器对纹理、梯度等特征进行监督学习,然后在一个高阶条件随机场下解析场景.本文方法和STAIR[5]、ALE[10]的部分结果如图6①http://research.microsoft.com/en-us/projects/objectclass-Page7图6本文方法和STAIR[5]、ALE[10]的解析结果比较Page8所示.从图6比较效果上看,STAIR[5]仅在超像素级别上进行解析,超像素算法存在的欠分割问题使得大部分物体的边界轮廓比较粗糙.ALE[10]通过对像素点的软聚类使得物体边缘比较平滑,然而仍然受超像素和先验模型的影响,解析精度不够高.相比于ALE[10],本文引入局部颜色模型在双层条件随机场下的方法能更精细地解析场景,较好保留边缘细节,例如车轴、牛腿、鸭嘴等.我们用相同的训练和测试数据在一个2.99GHzCPU、2.0GB内存的台式机上量化比较了这4个方法,得到平均像素级、物体级的F-Measure值如表2、表3所示,表中加粗数字为该组的最优值.其中,F-Measure=2×Pre×Rec/(Pre+Rec),Pre为表3本文方法和Texton[3]、STAIR[5]、ALE[10]的物体级分割精度比较方法Texton23264765713867427337823463084173305642042.8STAIR50716769828188528957899482609659756257763771.0ALE53716686938689758389939671879563616783734077.1本文方法73727189948273838782929088869483848694906683.7然后提取图像中每个物体的局部颜色模型,在以像素点为结点条件随机场下再次精细解析场景.实验表明,相比于以往单纯基于先验模型的场景解析方法,本文方法能更精细地解析场景,保留了物体的边缘细节.从计算效率上分析,无论解析时间还是训练时间,Texton[3]的效率都是最低的,主要原因是该方法直接在像素级层次解析场景,需要提取每个像素点的外观特征用于监督学习,因此计算量比较大.相比而言,STAIR[5]、ALE[10]和本文方法初次解析都是建立在超像素级别上,因此训练和解析的时间较低.其次,相比STAIR[5]和本文方法,ALE[10]的训练和解析效率比较低,这是因为ALE[10]共构建了6层的超像素层,多层超像素涉及结点过多,计算量偏大,而本文方法初次解析和STAIR都建立在单层超像素场景解析的基础上.相比于STAIR[5],本文方法为了保留场景细节,引入局部颜色模型对场景的再次解析增加了计算量,因此解析效率稍低于STAIR[5].然而本文方法训练效率高于STAIR[5],我们通过补充实验找出其原因是我们采用的随机森林分类器比STAIR[5]的Boosting分类器计算效率更高.6总结针对目前场景解析方法中存在的解析不精细问题,本文引入了局部颜色模型,提出了双层条件随机场的场景解析方法.首先根据先验模型在以超像素为结点的条件随机场下粗略解析出场景中的物体;正确解析点在解析结果图所占的比例,Rec为正确解析点在标准解析图所占的比例.如表2、表3所示,Texton[3]的精度最低,STAIR[5]和ALE[10]的精度相当,本文方法的精度明显优于以上三者.表2本文方法和Texton[3]、STAIR[5]、ALE[10]的方法的性能Texton45.36.80169.0043.4014.70169.00STAIR72.60.836.8061.301.906.80ALE71.32.8043.0061.704.9043.00本文方法81.80.2413.3080.000.5313.30精度/%本文方法还存在两点局限性:(1)局部颜色模型是基于颜色信息来提取物体,因此本文方法主要适用于物体间颜色差异较大的场景解析;(2)本文方法初次解析属于监督学习的方式,要求训练数据包括像素级的标准解析图,而手动获取标准解析图的工作量大,因此该方法的数据规模较小.为扩展应用范围,可进一步考虑如何提取带有颜色、纹理、梯度和深度等信息的局部外观模型,结合先验模型和局部外观模型以弱监督方式来精细解析场景.此外,目前视频场景解析方法中也存在解析不精细的问题,还可以考虑如何结合先验模型和局部外观模型来提高视频场景解析的精度.
