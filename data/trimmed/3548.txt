Page1一种新的不平衡数据学习算法PCBoost李雄飞1)李军1),2)董元方1),3)屈成伟1)1)(吉林大学符号计算与知识工程教育部重点实验室长春130012)2)(长春理工大学应用数学系长春130022)3)(长春理工大学经济管理学院长春130022)摘要现实世界中广泛存在不平衡数据,其分类问题是机器学习研究中的一个热点.多数传统分类算法假定类分布平衡或误分类代价均衡,在处理不平衡数据时,效果不够理想.文中提出一种不平衡数据分类算法-PCBoost.算法以信息增益率为分裂准则构建决策树,作为弱分类器.在每次迭代初始,利用数据合成方法添加合成的少数类样例,平衡训练信息;在子分类器形成后,修正“扰动”,删除未被正确分类的合成样例.文中讨论了数据合成方法,给出了训练误差界的理论分析,并分析了集成学习参数的选择.实验结果表明,PCBoost算法具有处理不平衡数据分类问题的优势.关键词数据挖掘;不平衡数据;集成学习;提升;扰动1引言近年来,不平衡数据学习问题(ImbalancedDataLearning,IDL)得到了学术界、工业界和政府基金机构的广泛关注,有与之相关的重要研讨会、会议和专刊,包括AAAI不平衡数据学习研讨会(AAAI’00)[1]、ICML的不平衡数据学习研讨会(ICML’03)[2]以及2004年ACMSIGKDD的专题通讯[3]等.如果数据集类别分布不均匀,使得其中某个类别占支配地位,则称其为不平衡数据.作为一种特殊的分类学习,不平衡数据学习问题关注的重点是:在数据未被充分表达或严重类分布不平衡情况下,学习算法的性能.多数现有的学习算法假定或期望类分布平衡或误分类代价相等,因此,当处理复杂的不平衡数据集时,这些算法不能有效地表现数据的分布特征,分类结果不能令人满意.不平衡数据广泛存在于各种领域,如医疗诊断、雷达图像检测、诈骗检测、电信设备故障预测等[4].鉴于不平衡数据学习的重要现实意义,研究者对该问题进行了大量研究,提出的主要解决方案包括数据层面的方案和算法层面的方案,其主要目标是提高少数类的分类精度.本文融合数据采样和boosting技术,提出一种不平衡数据学习算法———PCBoost(PerturbationCorrectionBoosting).2相关工作多类别数据的不同类之间存在不平衡情形,由于篇幅关系,本文只考虑两类别不平衡学习问题.两类别情况下,通常称少数类为正类,多数类为负类.类别标签的取值分别为{-1,+1}.数据采样、代价敏感学习(cost-sensitivelearn-ing)、boosting技术、核方法、主动学习(activelearn-ing)以及单类别学习等方法,是处理不平衡数据的常见策略[4].数据采样技术通过添加少数类样例(过采样,oversampling)或移出多数类样例(欠采样,under-sampling)的方式平衡数据的类分布.两者各有优缺点,由于欠采样删除部分训练样例,故其主要缺点是引起信息丢失,而其优点是通过削减训练数据集可以降低训练模型的时间.过采样的主要缺点是若简单地复制原始数据,可能导致过拟合.与采用不同策略平衡类分布的采样方法不同,代价敏感学习关注错分样例的代价[5].研究表明:代价敏感学习和不平衡数据学习之间存在很强的联系,代价敏感学习的相关理论和算法可以用来解决不平衡数据的学习问题[6].应用核方法解决不平衡数据学习问题时,主要有3种策略:将支持向量机(SupportVectorMachine,SVM)与boosting结合、核更新方法和偏差惩罚方法.其主要思想是调整由于数据不平衡导致的偏斜的类边界[7].Ertekin等人[8]提出一种基于SVM的主动学习方法,该方法选择最富信息的“未见过”的训练样例,也即,离当前分类超平面最近的样例,重新训练SVM.间隔(margin)中的数据不平衡比率,低于整个数据集的不平衡比率,因此,该方法可以避免搜索全部数据.但是,搜索最富信息样例的过程计算量很大.作为集成学习方法的boosting技术用于提高分类性能[9].无论数据是否不平衡,都可以通过boosting迭代创建集成模型,提升弱分类器的性能.将boosting用于不平衡学习问题的优势在于:(1)数据空间重采样自动降低探测最优类分布和代表样例的额外学习代价;(2)通过组合多个分类器避免模型过拟合;(3)降低特定学习算法的偏倚.将boosting算法应用于不平衡数据的文献有两类:一类是可以直接应用到大多数分类器的学习算法.例如,AdaCost[10]、CSB1、CSB2[11]、RareBoost[12]和BABoost[13]等,此类算法主要讨论代价敏感学习和boosting技术的结合[14];另一类是将数据合成方法和boosting技术结合的算法,如SMOTEBoost和DataBoost-IM等.Song等人[13]提出一种改进的不平衡数据分类算法BABoost(BalancedAdaBoost),通过分别计算每个类上的错误率和调整参数λ给错分的少数类样例设置更高的权值,实验结果表明BABoost显著降低了少数类的预测误差,同时对多数类预测误差影响不大.SMOTEBoost[15]算法将Adaboost.M2与算法SMOTE[16]整合,在每次迭代中引入合成样例,使每个子分类器更多关注少数类.由于各子分类器建立在不同的数据样本上,集成分类器具有更宽泛的和良性定义的少数类边界.RUSBoost[17]是SMOTEBoost的变形,应用随机欠采样从多数类中随机移出样例,与SMOTE-Boost相比,算法具有简单易于实现、训练时间短等性能优势.DataBoost-IM算法将数据合成技术融合到Page3AdaBoostM1,在获得少数类较高的预测精度的同时,并未牺牲多数类的预测精度[18].在boosting迭代时识别多数类和少数类的困难样例,然后根据困难样例分别生成多数类和少数类的合成样例.Data-Boost-IM依据类间学习样例的比率生成合成样例.最后,根据新加入合成样例,更新权值分布.SMOTEBoost采用SMOTE算法通过“插值”的方式添加合成样例,造成合成样例只分布在原始样例的连线上,不能很好反映样例的分布.与之相比,随机采样方式能更好地模拟数据的真实分布,而且,SMOTE算法相对于随机采样过于复杂.RUS-Boost算法虽然克服了过采样引起信息丢失的问题,但其缺点是没有最大限度地“拓展”少数类边界.DataBoost-IM算法同时生成两类样例,而且没有及时“修正”错误添加的合成样例,其迭代过程将面临过多的训练数据.本文提出一种融合数据采样和boosting技术的不平衡数据分类算法———PCBoost(PerturbationCorrectionBoosting).在每次迭代初始,利用随机采样方式的数据合成方法,添加合成的少数类样例,平衡训练信息,并及时进行“扰动修正”(PerturbationCorrection),删除错分的合成样例.PCBoost发挥boosting技术提升分类器性能的作用,有效“拓展”少数类边界.理论和实验结果表明,PCBoost在处理不平衡数据时性能更优.本文第3节描述PCBoost算法,并给出数据合成方法和对训练权值更新的讨论;第4节讨论训练误差的界,子分类器集成的权重选择以及合成样例连续型属性的处理;第5节给出UCI数据上的实验结果;第6节给出结论和进一步研究展望.3不平衡数据挖掘算法PCBoostPCBoost算法包括3个阶段:首先对原始数据集的每个样例设置相同的初始权值;其次,调用数据合成方法,生成m个合成样例平衡少数类训练信息,合成样例添加后需要规范化权值;第3阶段调用弱学习算法,形成子分类器,并通过权值更新过程使得下一次迭代时,被当前子分类器错分的“困难样例”能够得到更多关注,同时修正扰动数据,消除“错误”添加的合成样例对集成学习的影响.第2阶段和第3阶段重复执行,直到达到迭代次数T.最后,将子分类器集成.3.1数据合成方法数据合成的目的是通过添加合成样例,平衡少数类的训练信息.以O表示所有原始样例集,Omin表示所有少数类样例集.在每一次迭代开始时,将少数类Omin中的样例视为“种子”,利用数据合成方法生成合成样例.以S表示所有合成样例集,St表示第t次迭代时生成的合成样例集,其数目与原始少数类样例数目相同,也即|St|=|Omin|=m.合成样例的类别标记与少数类相同.生成合成样例时,基于如下方法生成属性值:对于离散型属性,首先获得在该属性下少数类样例的属性值分布,然后,数据合成算法根据该分布随机选择一个属性值.设属性a为离散型属性,在该属性下少数类样例的属性值域为{a1,a2,…,am},每个属性值的出现频率分别为p1,p2,…,pm,设e(a)表示合成样例e的属性a的属性值,首先利用随机数发生器给出一个(0,1)上的均匀分布随机数ξ,然后根据下式选取合成样例的属性值:e(a)=由于ξ是均匀分布于区间[0,1]的,因此合成数据的属性取值的概率分布是对真实分布的近似.对于连续型属性,数据合成方法根据该属性分布的均值和方差,随机生成属性值.设属性a为连续型属性,其取值的均值和方差分别为μ和σ2,在生成合成数据时,利用随机数发生器给出服从正态分布N(μ,σ2)的随机数ξ,表示合成样例的属性值e(a).如果有诸多因素起作用,而每一种因素都不起主导作用时,根据中心极限定理可知,符合这种特点的随机变量近似服从正态分布.另外,正态分布是最常见的连续型随机变量分布,生产生活中的许多现象服从或近似服从正态分布.因此,PCBoost采用正态分布生成合成样例连续属性值.采用正态分布效果不理想时,可以按4.3节给出的方法进行处理.在生成合成样例时,采用独立的方式生成合成样例属性值,所产生的数据分布与真实分布有偏差.PCBoost算法的步6利用式(3)更新样例权值,将错分的合成样例权值设为0,也即删除错分的合成样例,从而,可以减少不符合真实分布的合成样例对分类效果的影响.按照上述方法生成每个属性的取值后,即可得到一个合成少数类样例.数据合成过程结束后,将St中的合成样例添加到S中,并以O∪S作为训练数据集训练第t个子分类器,由于逐次添加了若干合成样例,因此,数据分布逐渐趋于平衡.Page43.2训练数据权值更新第t次迭代过程中,有两次训练数据权值更新,分别是合成样例添加之后和子分类器形成之后.假设第t次迭代添加合成样例之后的训练数据集的基数|O∪S|=nt,由于|St|=|Omin|=m,因此对于合成样例集St中的每个合成样例,设定其权值为1,为规范化权值,|O∪S|的原有样例权值应调整为nt原来权值的nt-mDnewt(i)=其中,Doldt和Dnewt分别表示第t次迭代时,合成样例加入前及加入后的样例权值.定理1.Dnewt(i)是规范化的.由式(2),易知定理1成立.第t次迭代训练结束时,得到子分类器ht:x→{-1,1},(t=1,…,T),ht(x)给出样例x所属类别.根据子分类器ht的分类情况,更新样例权值,减少正确分类样例权值,增加错分原始样例的权值,以便在下一次迭代时,“困难”样例得到更多关注.同时,被子分类器ht错分的合成样例,相当于扰动数据.因此,需将错分的合成样例权值设为0,使得在下次迭代前,这些样例已经删除,从而起到扰动修正的作用.子分类器ht形成后的权值更新公式如下:Dt+1(i)=其中Zt为规范化因子:Zt=∑xi∈O∪S-StPCBoost算法输入.样例集合{(x1,y1),(x2,y2),…,(xn,yn)},yi∈初始化.对于i,令D0(i)=1步骤.fort=1,…,T:1.调用数据合成方法,生成m个合成样例,以平衡少2.添加合成样例,利用式(2)更新训练数据集权值.原3.在O∪S上用弱学习算法WeakLearn,获得子分类器数类训练信息;始样例集O与合成样例集S形成训练数据集O∪S;ht:x→{-1,1};4.计算子分类器ht的误差:εt(见4.2节式(6));5.令αt=16.利用式(3)更新样例权值输出.H(x)=sign∑T其中,符号函数sign定义为4训练误差的界与参数选择4.1训练误差的界Schapire和Singer给出了AdaBoost算法训练误差的界,并讨论了参数αt的选择[19].定理2给出本文PCBoost算法训练误差的界.定理2.最终分类器H的训练误差的界为n{xi:xi∈O∧H(xi)≠yi}∏T1证明.消去权值更新规则的递归,得如果xi∈O,则若H(xi)≠yi,则yiH(xi)0,从而,因此有其中I为示性函数.由式(4)、式(5)可知训练误差的界为1n{xi:xi∈O∧H(xi)≠yi}定理2给出了训练误差的界与迭代中的规范化因子Zt的关系.4.2参数α狋的选择由定理2可知,可以通过最小化每次迭代的ZtPage5来最小化误差界,从而降低训练误差.注意,当xi∈S∧yi≠ht(xi)时,Dt(i)=0,从而另外,注意yi∈{-1,1},ht(xi)∈{-1,1},因此Zt=∑i=∑xi∈Oyi=ht(xi)dZtdαt=∑xi∈Oyi=ht(xi)yi=ht(xi)令dZtdαt=0,可得αt的形式如下:αt=1其中显然,由式(6)的形式可知,εt是第t次迭代所得子分类器的训练误差.基于上述讨论,在PCBoost迭代算法中,为最小化误差界,参数αt应取为4.3合成样例连续型属性的分布对于连续型属性,采用正态分布生成属性值,效果不够理想时,可以按如下方法处理:对常见连续型分布(均匀分布、正态分布、拉普拉斯分布、韦布尔分布、指数分布、对数正态分布等)做皮尔逊χ2检验,利用分布函数的拟合优度检验,确定属性值的分布类型;然后,依据具体的分布类型给出合成样例的连续属性值.假设Fk(x;θ1,θ2,…,θlk),k=1,…,K是待检验的K个连续型分布函数,Fk含有lk个未知参数.a是连续型属性.对于第k个待检验的分布Fk,首先,用未知参数的极大似然估计量θ^1,θ^2,…,θ^lk代替θ1,θ2,…,θlk,使得Fk(x;θ1,θ2,…,θlk)不含未知参数,然后,按照以下步骤进行皮尔逊χ2检验:(1)对所讨论连续型属性a计算属性值的频数分布;(2)根据分布Fk(x;θ1,θ2,…,θlk)计算理论频数;(3)建立皮尔逊χ2统计量,在给定的显著性水平α之下,进行显著性检验.通过调整显著性水平α,使得仅有一种分布假设被接受.在合成样例时,利用该分布假设生成连续属性值.数据合成方法根据该分布的参数,利用随机数发生器给出服从相应分布的随机数ξ,表示合成样例的属性值e(a).5实验分析5.1数据集为评估算法的性能,选择10组具有不同实际应用背景的UCI数据.对于含有多个类别的数据,合并某些类别或只取两个类别.表1是用于实验的数据信息,包括数据集大小、少数类样例的比例、属性个数等.其中Glass的headlamps类作为少数类,合并其它类别作为多数类.Vowel的hed类作为少数类,其它类合并作为多数类.Vehicle的van类作为少数类,其它类合并作为多数类.取Segment的grass类作为少数类,其它类合并作为多数类.取Satimage的dampgreysoil类为少数类,其它类合并作为多数类.取Abalone数据的第18类为少数类,第9类为多数类.数据集样例数目少数类多数类类分布属性Sonar208971110.47:0.5360/0Monk2169641050.37:0.630/6Ionosphere3511262250.35:0.6534/0Breast-W6992414580.34:0.669/0Vehicle8461996470.23:0.7718/0Segment231033019800.14:0.8619/0Glass214291850.13:0.879/0Satimage643562658090.097:0.90333/0Vowel990909000.09:0.9110/3Abalone731426890.06:0.947/1一般认为当少数类与多数类的类分布比例低于12时,数据集具有不平衡特征.表1所示数据集具有不同不平衡比例.其中,Sonar数据集基本是平衡数据集,选取该数据集的目的是验证PCBoost算Page6法对一般数据集的有效性.5.2评价度量在评价分类性能和指导分类器建模时,评估度量起着至关重要的作用.机器学习领域对于不平衡数据分类的常用评价标准包括ROC曲线、AUC以及基于混淆矩阵的若干度量,如查全率(recall)、查准率(precision)、F-measure和G-mean等.在两类别情形下,将训练样例少,但具有高识别重要性的少数类视为正类,多数类视为负类.经过分类过程后,训练样例可以分为混淆矩阵中所表示的4种情况,如表2所示.实际正类TruePositives(TP)FalseNegatives(FN)实际负类FalsePositives(FP)TrueNegatives(TN)利用混淆矩阵,可以派生出几个度量:真实正类率:真实负类率:正类预测值:如果只考虑正类的性能,真实正类率TPrate和正类预测值PPvalue是重要的度量.在信息检索领域,将真实正类率TPrate定义为查全率recall,表示检索到的相关对象占实际正类的比例.将正类预测值PPvalue定义为查准率precision,表示相关对象占检索出的所有对象的比例.F-measure是查全率和查准率的调和均值,其取值接近两数的较小者,因此,较大F-measure值表示recall和precision都较大:F-measure=(1+β2)recall×precision图1不同算法的F-measure比较图2不同算法的G-mean比较其中β用于调节precision和recall的相对重要度,通常取为1.如果同时关注两个类的性能,也即,希望TPrate和TNrate都取较大值,可以使用G-mean度量学习算法在两个类上的平均性能:本文采用F-measure和G-mean作为评价度量.5.3实验结果使用基于信息增益率为分裂属性的剪枝决策树作为PCBoost弱学习算法,对每个数据集实施10-折交叉验证.在PCBoost算法逐次添加合成样例过程中,少数类边界得到有效拓展,使得分类边界更接近于多数类.随着合成样例的添加,TP、FP将增大,TN、FN将减小.根据TPrate、TNrate、PPrate的定义可以看出,这将导致TPrate(即recall)增大,TNrate降低,但PPrate(即precision)的变化取决于TP、FP变化的程度,当TP增加的比例大于FP增加的比例时,PPrate将增大.根据F-measure的定义,其取值接近于recall和precision的较小者,因此,如果算法有效地获得较大的F-measure,则说明算法的recall和precision都取较大值,也即,算法的少数类查全率和查准率都较高.同理,根据G-mean的定义,如果算法有效地获得较大的G-mean,则说明算法的TPrate和TNrate都取较大值,也即算法在两个类别上的精度都较高.表3给出在10个不同的不平衡数据上PCBoost与C4.5、AdaBoostM1、SMOTEBoost和Data-Boost-IM的比较结果.图1及图2分别给出不同数据集上各算法的F-measure和G-mean的比较.Page7数据集方法F-measureG-meanGlassSMOTEBoost84.0(N=100)91.1(N=100)SatimageSMOTEBoost65.3(N=300)76.0(N=300)VowelSMOTEBoost97.3(N=100)98.7(N=100)AbaloneSMOTEBoost39.0(N=300)58.1(N=300)SegmentSMOTEBoost95.4(N=300)97.2(N=300)SonarSMOTEBoost78.6(N=100)79.3(N=100)Monk2SMOTEBoost54.3(N=300)58.8(N=300)IonosphereSMOTEBoost90.2(N=100)92.0(N=100)Breast-WSMOTEBoost94.5(N=500)96.2(N=500)VehicleSMOTEBoost92.4(N=100)95.3(N=100)可以看出在Glass、Abalone、Segment、Breast-W、Sonar上,PCBoost的两个度量指标均优于其它算法;在Monk2、Ionosphere、Vehicle、Satimage上PC-Boost的一个度量指标优于其它算法,并且另一度量指标也是良好的.在Vowel上的度量指标略低于DataBoost-IM,其F-measure与SMOTE-Boost相当.Vowel数据集上G-mean较低的原因,主要是由于随着合成样例的添加,TNrate降低程度大于TPrate增大程度造成的.虽然Bartlett和Traskin证明了AdaBoost算法具有一致性[20],但是,Schapire指出AdaBoost算法的收敛性仍然是一个值得研究的问题[21].对于每一个数据集,在调用PCBoost算法时,为了选取能够取得最佳预测性能的分类器,需要确定恰当的迭代次数.实验中选取若干不同的迭代次数,然后根据在验证集上的结果优劣,选出能够获得最佳分类性能的迭代次数.与AdaBoost算法相比,PCBoost不仅改变样例权值,而且增加新的样例,这更有利于分类边界的确定.实验结果表明,PCBoost算法能够更快地收敛.6结论本文提出一种新的不平衡数据分类算法—PCBoost,算法融合了数据合成采样技术和boosting技术,逐步渐进地增加合成的少数类样例,平衡训练信息,并及时删除误分的合成样例,通过扰动修正,避免了不恰当的人工合成样例对集成分类器的影响.同时,从理论上分析了训练误差的界,并讨论了子分类器集成权重的选择依据.通过UCI数据集实验,以F-measure和G-mean为度量对算法进行评价,与决策树算法、标准AdaBoost算法以及两种基于采样和boosting融合的算法进行比较,实验结果表明PCBoost算法具有处理不平衡数据的优势.考虑属性间相关性的数据合成方法对PCBoost算法效率的作用,是值得深入研究的课题.进一步研究将从实验上研究不同数据合成方法对PCBoost算法的影响以及从理论上分析PCBoost算法误差界与AdaBoost算法误差界的关系.致谢审稿专家提出了宝贵建议.在此表示感谢!
