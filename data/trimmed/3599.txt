Page1一类神经网络逼近全实轴上函数:稠密性、复杂性与构造性算法曹飞龙1)李振彩1)赵建伟1)吕科2)1)(中国计量学院信息与数学科学系杭州310018)2)(中国科学院研究生院北京100049)摘要在已有的神经网络逼近研究中,目标函数通常定义在有限区间(或紧集)上.而实际问题中,目标函数往往是定义在全实轴(或无界集)上.文中针对此问题,研究了全实轴上的连续函数的插值神经网络逼近问题.首先,利用构造性方法证明了神经网络逼近的稠密性定理,即可逼近性.其次,以函数的连续模为度量尺度,估计了插值神经网络逼近目标函数的速度.最后,利用数值算例进行仿真实验.文中的工作扩展了神经网络逼近的研究内容,给出了全实轴上连续函数的神经网络逼近的构造性算法,并揭示了网络逼近速度与网络拓扑结构之间的关系.关键词神经网络;全实轴;逼近;速度;连续模1引言1.1神经网络逼近问题为Sigmoidal函数,如果它满足下列条件:全实轴(-,+)上定义的有界函数σ被称Sigmoidal函数是一类简单的常用函数,在神经网络的研究中常常作为隐层神经元的激活函数(或传递函数).众所周知,对连续函数或可积函数而言,前向神经网络是一个万能逼近器,它可以训练、学习连续函数或可积函数达到任意精度.因此,前向神经网络已被广泛应用于系统辩识与控制、模式识别、信号处理等领域.前向神经网络逼近的本质是以非线性参数展开来表示与近似一元或多元函数,与之相关的基本问题有稠密性、复杂性和算法等问题.对于稠密性问题,在20世纪80年代末与90年代初,许多学者进行了深入、系统的研究(典型的结果可参见文献[1-8]).1989年,Cybenko首先证明了如下的稠密性定理:令I=[0,1]d表示d维欧式空间中的单位立方体,如果φ是连续的Sigmoidal函数,则三层前向神经网络全体在C(I)(I上的连续函数全体)中稠密[1].此后,出现了一些关于稠密性定理的不同的证明方法.其中,一些学者对稠密性定理成立的激活函数φ的假设条件进行了一系列的研究.例如,1991年,Hornik在文献[3]中证明了当φ是一个非常数的有界连续函数时,稠密性定理成立;Mhaskar和Micchelli在文献[4]中证明了只要对连续函数在无穷远处的幅度做适当的限制,那么任意非多项式函数都可以充当激活函数;陈天平等人对这一问题做了深入细致的研究[6-8],得到了更广泛的研究结果:稠密性定理成立的充分必要条件是激活函数φ不是多项式函数.该结果为激活函数在更大范围内的选取提供了理论依据.近年来,神经网络逼近的复杂性问题引起了人们的广泛兴趣.神经网络逼近的复杂性问题是指人工神经网络的拓扑结构(如隐层神经元个数、权值的大小)与网络逼近能力、逼近速度之间的关系.从应用的角度看,人们更加关心该问题.近年来,该问题引起了国内外学者的广泛关注[9-23],人们从不同角度揭示了由不同种类激活函数构成的前向网络隐层拓扑结构与逼近速度之间的关系,较多的工作研究了为达到规定的逼近精度所需的隐层神经元最少个数问题.我们在文献[14-15]中还研究了神经网络逼近速度的下界估计,进一步为神经网络的构造提供了理论依据.1.2神经网络插值问题设(X,ρ)是一个距离空间,ρ是X上的距离,S={x0,x1,…,xn}是(X,ρ)中的n+1个互异的点(X中的插值节点),{yi}n为一插值样本.若f:X→(-,+)且满足f(xi)=yi(i=0,1,…,n),那么我们称f为插值样本(1)的一个插值泛函.如果存在前向神经网络Ne(x)满足下列条件:则说Ne(x)是样本(1)的一个精确插值网络.如果对于任意正数ε,存在前向神经网络Na(x),使得则称Na(x)是关于样本(1)的一个近似插值网络.神经网络插值问题一直是神经网络理论与应用的研究热点之一,备受国内外学者的关注.Shrivatava与Dasgupta给出了当激活函数φ为Sigmoidal函数11+e-x时,精确插值网络存在性的代数证明[24].Ito与Saito证明了当激活函数φ是非减的Sigmoidal函数时,精确插值网络是存在的[25].然而,具体求解精确插值网络的计算量是比较大的.于是,人们转向寻求近似插值网络的研究.Sontag在文献[26]中曾讨论过这个问题.2006年,Llanas与Sainz给出了当激活函数为非减的Sigmoidal函数时,三层前向精确插值网络存在性的代数证明[27].谢庭藩与曹飞龙对于一般的Sigmoidal激活函数和d维欧氏空间中的插值样本,分别构造了精确插值和近似插值的单隐层前向神经网络,估计了它们对连续函数的逼近误差,指出了神经网络插值与一般代数多项式插值的本质差异,并提出了如下的关于多元插值神经网络与目标函数的偏差问题:当样本中的yi取作f(xi)时,怎么刻画相应的多元精确插值神经网络与目标函数的偏差[28]?文献[29]在R2上讨论了多元精确插值神经网络与连续函数的偏差问题,举例说明尽管这样的多元精确插值神经网络存在,但并非一致逼近定义在紧集上的连续函数,从而揭示了多元插值神经网络与一元插值神经网络之间的本质差异.1.3一类四层前向神经网络在实际应用中,经常会遇到一般距离空间中的神经网络插值与逼近问题.文献[30]与[31]对于距Page3离空间(X,ρ)中的节点S={x0,x1,…,xn},A>0,令激活函数为φj(x,A)=e-Aρ(x,xj)作关于φj(x,A)(j=0,1,…,n)的线性组合则g(x)可以理解为一个4层前向神经网络的模型.其中,第1层为输入层,输入为x(x∈X);第2层为一个预处理层,将x变为ρ(x,xj),计算输入向量与插值结点之间的值,且作为第3层的输入;第3层具有n+1个神经元,第j个神经元的激活函数为φj(x,A);第4层为输出层,输出量为g(x).事实上,Sigmoidal函数φ(x)=1用作前向神经网络隐层的激活函数,它是一个典型的Logistic模型函数,在生物学、人口学等中有着非常重要的应用[32-33].自然地,函数可以被认为是该Logistic模型函数的一个多类推广(见文献[34]之10.6节),也可以被用作分类问题中的多类情形的回归模型.虽然式(2)所定义的函数φj(x,A)不是Sigmoidal型函数,但它具有一些比较好的性质.例如,0<φj(x,A)1(j=0,1,2,…,n)且∑n另一方面,由φj(x,A)的结构性质可以看出激活函数包含插值样本的信息,网络第2层可以被看作处理层,同时也作为第3层的输入层,这便于网络插值的研究.这也是选择φj(x,A)作为激活函数的一个主要原因.文献[31]采用不同于文献[27]的方法,在距离空间中构造形如式(3)的精确插值与近似插值网络,并用之逼近距离空间的连续泛函.文献[30]首先给出了精确插值神经网络的一个新的构造性证明,其次引进一类近似插值神经网络并估计它与精确插值神经网络的偏差,最后利用函数连续模作为度量尺度,估计近似插值网络逼近定义在紧集上连续函数的速度.文献[35]利用了泛函分析的方法研究该类神经网络在距离空间中的神经网络插值与逼近问题.首先引进一类较φj(x,A)更为广泛的激活函数,用比较简洁的方法讨论距离空间中插值神经网络的存在性,然后给出插值神经网络逼近连续函数的误差估计.1.4本文研究问题已有的关于神经网络逼近与插值的研究,被逼近的目标函数均是定义在某一紧集上的连续函数.而在许多实际问题中,网络的输入往往不是有界的.因此,自然地要问:如果目标函数定义在全实轴上,那么能否用形如式(3)的神经网络逼近?如果回答是肯定的话,则如何进一步估计其逼近误差?这正是本文将要回答的问题.我们将在第2节中研究第一个问题,当目标函数满足一定条件(即在无穷远处存在极限)时,给予肯定的回答;在第3节中我们将利用函数连续模为度量尺度,建立逼近速度的估计,给出逼近的复杂性定理;在第4节中,我们将给出数值算例进一步验证所获得的理论结果;最后一节扼要说明本文的主要结果与意义.2稠密性定理本节给出用形如式(3)的插值神经网络逼近全实轴上连续函数的稠密性定理,即证明这种逼近的可能性.因为有限区间上的连续函数具有一些较好的性质,如有界性、区间端点的左右极限存在等.而当目标函数的定义域从有限区间变为全实轴时,函数的性质将发生本质的变化,上述性质不再存在.所以,我们对目标函数作一适当的限制,即假定目标函数在无穷远处的极限存在,则我们有以下定理.定理1.假设函数f在全实轴(-,+)上连续,且满足limx→+f(x)=B1,limx→-f(x)=B2,其中B1、B2为常数,则存在形如式(3)的神经网络g,使得对任意ε>0,成立,其中‖f‖=supx∈(-,+)|f(x)|.所以对任意的ε>0,存在整数M,N>0,使得证明.因为limx→+f(x)=B1,limx→-f(x)=B2,且从而Page4且并且根据目标函数f的假设,不难证明f在(-,+)上是一致连续的.因此,当|x-x|<2将[-M,M]进行MN等分,取xi=-M+2i/N,fi=f(xi),i=0,1,…,MN,定义形如式(3)的插值神经网络为了证明定理1,我们取A=MN2+)分为如下3部分:(M,+)、(-,-M)以及[-M,M].现在,分3种情况进行讨论.第1种情况:当x∈(M,+)时,我们有|f(x)-g(x)|=∑MN∑MN|f(x)-fj|exp(-A|x-xj|)=∑MN-1|f(x)-f(M)|exp(-A|x-M|)|f(x)-fj)|exp(-A|x-xj|)j=0j=02‖f‖∑MN-1=2‖f‖∑MN-1|f(x)-f(M)|2‖f‖∑MN-1=2MN‖f‖exp(-MN)+ε<ε.|f(x)-g(x)|=∑MN第2种情况:当x∈(-,-M)时,我们有第3种情况:当x∈[-M,M]时,存在j0∈j=0j=0j=0|f(x)-fj|exp(-A|x-xj|)∑MN|f(x)-fj|exp(-A|x-xj|)=∑MN|f(x)-f(-M)|exp(-A|x+M|)j=1|f(x)-fj|exp(-A|x-xj|)|f(x)-fj)|exp(-A|x-xj|)2‖f‖∑MN<2‖f‖∑MN=2MN‖f‖exp(-MN)+ε<ε.{0,1,2,…,MN-1}使得x∈[xj0,xj0+1],则有|f(x)-g(x)|=∑MN∑MN=∑j0-1∑MN∑j=j0,j0+12‖f‖∑j0-12‖f‖∑MN|f(x)-f(xj0)|+|f(x)-f(xj0+1)|<2j0‖f‖exp(-MN)+2(MN-j0-1)‖f‖exp(-MN)+ε2MN‖f‖exp(-MN)+ε<ε.x∈(-,+),成立.j=j0+2综合上述3种情况,对任意的ε>0,对所有的Page53复杂性定理模[36]定义为对于(-,+)上定义的连续函数f,其连续ω(f,δ)=sup|x-y|δ,x,y∈(-,+)|f(x)-f(y)|.如果ω(f,δ)Cδα,0<α1,则称f属于Lipschitz-α函数类,并记f∈LipCα,其中C是与f、α有关的常数.连续模是逼近论中普遍采用的刻画逼近精度和函数光滑性的度量,它除了具有单调性(关于δ单调递减趋于0)、半可加性ω(f,δ1+δ2)ω(f,δ1)+ω(f,δ2)等性质外,还具有性质以及对于0<αβ1,具有关系LipCαLipCβ.此外,如果f的导数有界,即|f(x)|C,则必有f∈LipC1.下面,我们以连续模为尺度,估计网络逼近速度,即网络的逼近复杂性定理.定理2.假设函数f在(-,+)上连续,且满足limx→f(x)=B1,limx→-f(x)=B2,其中B1、B2为常数,则对任意r>0,存在与函数f和r有关的常数M>0,可以构造形如式(3)的神经网络其中xj=-M+2Mj/n,fj=f(xj),j=0,1,…,n,A=n22M,使得‖g(x)-f(x)‖2n‖f‖exp(-n)+ωf,2M()n+2r.证明.因为limx→+f(x)=B1,limx→-f(x)=B2,所以对任意的r>0,存在M>0,使得且因此,|f(x)-f(M)||f(x)-B1|+|f(M)-B1|<2r,且将[-M,M]进行n等分,取xi=-M+2Mi/n,fi=f(xi),i=0,1,…,n,定义插值神经网络第1种情况:当x∈(M,+)时,我们有类似于定理1的做法,取A=n2分为3部分:(M,+)、(-,-M)以及[-M,M].然后,我们分3种情况进行估计.|f(x)-g(x)|=∑n∑n|f(x)-fj|exp(-A|x-xj|)=∑n-1|f(x)-f(M)|exp(-A|x-M|)|f(x)-fj|exp(-A|x-xj|)j=0j=02‖f‖∑n-1=2‖f‖∑n-1<2‖f‖∑n-1=2n‖f‖exp(-n)+2r.j=0j=0第2种情况:当x∈(-,-M)时,我们类似有|f(x)-g(x)|2n‖f‖exp(-n)+2r.第3种情况:当x∈[-M,M]时,存在j0∈|f(x)-fj|exp(-A|x-xj|)|f(x)-fj|exp(-A|x-xj|){0,1,2,…,n-1},使得x∈[xj0,xj0+1],则有|f(x)-g(x)|=∑n∑n=∑j0-1∑n∑j=j0,j0+12j0‖f‖exp(-n)+2(n-j0-1)‖f‖exp(-n)+ωf,2M()n2n‖f‖exp(-n)+ωf,2M()n.j=j0+2Page6综上所述,我们有‖f(x)-g(x)‖2n‖f‖exp(-n)+ωf,2M()n+2r.推论1.在定理2的条件下,如果f∈LipCα,0<α1,则‖g(x)-f(x)‖2n‖f‖exp(-n)+C2M()n4数值算例取目标函数f(x)=x显然,limx→+f(x)=limx→-f(x)=0,‖f‖=1/2,且f∈Lip11.取g的构造如式(4),则根据推论3,有‖g(x)-f(x)‖nexp(-n)+2M()n由构造性算法,对于r=0.1,r=0.01可分别计算出M的值,从而得到网络g.取不同的神经元个数,利用Matlab得到图1~图16.由图1~图7、图8~图16可以看出:对于给定的r>0,神经元数目n越大逼近效果越好;比较组图1~图7与组图8~图16,可以发现:为了使逼近达到一定效果,如果r较小,则神经元数目n就必须越大.如果r较大,则神经元数目n就相对不必很大.这说明:目标函数在无穷远处的渐近状态直接影响网络的逼近效果.Page7Page85结论本文研究了一类插值神经网络逼近定义在全实轴上的连续函数的问题.在目标函数满足一定条件(在无穷远点极限存在)下,得到了网络逼近的稠密性定理与复杂性定理.特别地,本文给出了网络的构造性算法与逼近算法,并给出了数值例子进一步验证所获得的理论结果.本文研究结果表明:(1)只要目标函数在无穷远点处作适当限制,我们可以构造出形如式(3)的插值神经网络逼近定义在全实轴上的目标函数;(2)函数连续模以及神经元个数可以作为度量刻画逼近误差,以此揭示逼近速度与网络拓扑结构之间的关系,从而获得逼近复杂性定理;(3)所构造网络的逼近速度不仅与神经元数目和目标函数的光滑性有关,而且也与目标函数在无穷远处收敛速度有关(两者之间相互制约).可以看出,在目标函数给定的条件下,对于某给定的r>0,神经元个数n越大逼近效果越好;对于不同的r,为了使网络逼近达到所需的某一速度,如果r越小,则M就必然增大,从而所需的神经元个数n就必须越大.反之,如果神经元个数n增大,则M就必须增大,从而r可以变小.即越靠近无穷远处,拟合所需的神经元越多,这也是全实轴上神经网络逼近与有限区间上神经网络逼近的本质区别所在.
