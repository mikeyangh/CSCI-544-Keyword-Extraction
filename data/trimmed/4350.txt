Page1面向多核集群的数据流程序层次流水线并行优化方法于俊清1),2)张维维1)陈文斌1)涂浩2)何云峰1)1)(华中科技大学计算机科学与技术学院武汉430074)2)(华中科技大学网络与计算中心武汉430074)摘要数据流编程语言是一种面向领域的编程语言,它能够将计算与通信分离,暴露应用程序的并行性.多核集群中计算、存储和通信等底层资源的复杂性对数据流程序的性能提出了新的挑战.针对数据流程序在多核集群上执行存在资源利用低和扩展性差等问题,利用同步数据流图作为中间表示,文中提出并实现了面向多核集群的层次性流水线并行优化方法.方法包含任务划分与调度、层次流水线调度和数据局部性优化,经过编译优化后生成基于MPI的可并行执行的目标代码.其中任务划分与调度是利用程序中数据和任务并行性将任务映射到计算核上,实现负载均衡和低通信同步开销;层次性流水线调度是利用程序中的并行性构造低延迟流水线调度;数据局部性优化是针对数据访问存在的Cache伪共享做面向存储的优化.实验以X86架构多核处理器组成的集群为平台,选取媒体处理领域的典型应用算法作为测试程序,对层次流水线优化进行实验分析.实验结果表明了优化方法的有效性.关键词多核集群;数据流编程;编译;流水线;COStream1引言在多核时代,C++、C#和Python等由于能够大大提高编程效率正逐渐取代以C和Fortran为代表的传统的编程语言,但它们主要对应单指令流和传统集中式内存管理模式,无法很好地适合多核甚至分布式环境.尽管当前存在许多高层的并行编程系统如Erlang①和X10②等,但它们并没有将可编程性、可移植性和可扩展性很好地集合起来.在高性能计算领域如OpenMP③和MPI④(MessagePassingInterface)提供了显式访问底层系统资源的接口,但依然要求编程人员必须熟悉系统底层并行结构.设计并行程序时需要根据系统底层结构进行精心的任务划分、数据通信和同步设计,程序性能受制于编程人员对并行系统的理解,极大地增加了编程人员的负担.为此,数据流编程语言作为面向领域编程语言被提出来.与面向并行系统的并行编程模型不同,编程人员无需对并行系统底层结构有很深入的了解,只需按照面向领域应用问题本身数据流特性进行编程.目前,支持流应用的计算平台主要有片内多处理机Raw[1]和Godson-T[2]、SIMD流处理器Imagine[3]和Merrimac[4]以及CELL[5]等.主要流语言有StreamIt[6]、Brook⑤和Cg[7]等,这些流编程语言一般是针对特定系统平台开发,能够充分利用特定平台底层硬件资源.但是用这些流语言编写的应用在可移植、可扩展性以及跨平台使用上存在一定局限性.当前数据流程序编译并行优化研究的硬件平台一般集中在片上存储结构的多核处理器领域,但随着各种定制的多核集群系统逐渐成为并行计算平台的主流,如何在这种分布式的并行系统中使数据流程序能够高效运行是当前急需解决的问题.多核集群系统符合数据流编程模型的特性即计算和通信分离,数据流编程模型的提出是为了暴露程序中存在的并行性,通过合理使用目标体系结构提高程序执行性能.如何利用多核集群系统的特点以及数据流应用程序本身的局部性和并行性,挖掘程序潜在的并行性是当前关心的问题.结合大规模数据计算等计算密集型应用,以实际应用为驱动,开展面向多核集群平台的编程语言、编程模型及相关编译支持技术的研究,具有重要的理论研究和实际应用价值.本文基于COStream[8]数据流编程语言针对多核集群系统,在编译时分析和利用程序中存在的任务、数据和流水线并行性,提出层次流水线并行优化方法.主要贡献有以下4点:(1)基于多核集群平台设计并实现了层次性任务划分算法;(2)基于多核集群平台设计了数据流程序层次性流水线调度;(3)基于多核集群平台实现了数据流程序层次流水线并行优化的目标代码生成框架;(4)在多核集群平台下实现了层次流水线并行优化框架,以数字媒体领域的典型应用作为测试程序,通过实验验证了优化方法的有效性.本文第2节基于多核集群的数据流程序层次流水线编译优化框架;第3节讨论COStream在多核集群下的基于MPI的代码生成;第4节描述在多核集群环境下实验方法和结果;第5节阐述相关工作;第6节对全文进行总结.2数据流程序层次流水线编译优化2.1COStream数据流编程语言COStream[8]是一种面向多核并行体系结构的高性能层次性流编程模型,采用同步数据流模型作为执行模型,采用有向无环图(DirectedAcyclicGraph,DAG)描述应用处理过程,图中节点表示计算,边表示数据依赖.COStream语言主要有stream、operator和composite三个语法单元.数据流(Stream)①②③④⑤Page3作为通信载体连接数据流图中的计算单元,是对数据流图中通信边的抽象;SDF中最基本的组成单元是actor,在COStream中由operator文法结构表示,专门用来处理stream中的数据;composite相对于operator属于高层次复合结构,代表一个由一个或多个operator组成的可重用的子数据流图结构,是对SDF图中可复用子图的抽象.2.2层次流水线编译优化框架多核集群系统在并行性上至少存在节点内核间并行性和节点间并行性,在局部性(Locality)上至少存在处理器高速缓存(Cache)上的局部性和单节点主存(Memory)上的局部性.数据流程序在多核集群下的执行要充分利用系统的多级并行性,保证程序在执行过程中有良好的数据局部性.层次流水线并行优化是为了构造低延迟高吞吐量的流水线调度,主要从计算负载、网络通信和存储局部性等方面考虑优化策略.图1所示为层次流水线编译优化框图,输入为数据流编程语言编写的数据流程序,输出为多核集群上可并行执行的目标代码.数据流程序经过编译前端处理后生成SDF图作为中间表示,然后进行层次流水线并行优化,层次流水线并行优化的具体步骤如下:(1)任务划分与调度.该阶段要确定SDF中actor被调度到集群的哪个节点的哪个计算核上.该阶段分成两个步骤:首先是进程级任务划分,该步骤要综合考虑集群节点上的负载均衡和节点间的通信情况使延迟最小化,完成actor与节点间的映射;其次,对于各个进程内部做线程级任务划分,完成actor与核之间的映射,由于针对的多核集群的单节点是共享存储架构的,在该架构下核间的通信对程序执行的效率的影响相对较小,该步骤主要考虑核间计算的负载均衡.(2)层次性流水线构造.流水线一般分同步流水线和异步流水线两种方式,流水线是开发数据流程序并行性的一种有效方法.将数据流程序看作一个迭代循环,同步流水线能够实现不同迭代中的actor的并行执行,但性能会被计算单元间通信和同步开销所影响;相反,异步流水线将计算单元的计算与通信分离,通过非阻塞通信能够将计算与通信重叠提升程序执行效率.结合同步与异步流水线的特点,在多核集群中节点内部采用同步流水线,节点间采用异步流水线调度方式.(3)数据局部性优化.该阶段主要针对多核集群系统做局部性优化,局部性主要包括时间局部性(TemporalLocality)和空间局部性(SpatialLocality).根据数据流程序在多核处理器上的执行情况基于Cache做数据局部性优化.2.3数据流程序层次性任务划分对集群下不同层次的任务划分描述如下:进程级任务划分要求保证在节点间负载均衡的前提下最小化节点间通信开销,划分结果间不出现环路;线程级任务划分则要求保证节点间负载均衡的前提下最小化同步开销,且尽可能保证数据局部性.介绍具体划分算法之前先对SDF图进行形式化描述.SDF图G=(V,E)中的V代表数据流程序中的operators即SDF图中的actor,|V|表示图G中顶点的个数;E代表operator间的通信通道即SDF图中的边,每一个通信边连接一个生产者actor和消费者actor,图G(V,E)是一个有向无环图,E={(vi,vj)|vi,vj∈V,且vi,vj邻接}.Group划分策略是针对进程级划分提出的,线程级划分主要采用复制分裂算法.复制分裂算法在文献[9]中有详细描述,本节主要针对数据流图的进程级任务划分介绍Group任务划分算法.Group任务划分引入group结构,group表示由SDF图中一个或多个actor组成的集合,是V的子集.初始时将SDF图的每一个actor作为一个group对待,group间依赖关系与actor间的是一致的.用G(GV,C)表示group间的依赖关系,|GV|表示group图的group数目,C表示group间通信通道.Group任务划分主要有4个步骤组成即对SDF图预处理构建初始group图、group的粗粒度、对粗化后group依赖图进行初始划分以及对初始化分结果进行细粒度Page4调整.Group划分主要报告如下4个阶段:(1)预处理阶段预处理针对COStream程序中operator多输入多输出而设计.单个operator有多个输入输出边时,执行期间会出现在同一时间要与集群中其他多个节点进行通信的情况,将增大该operator所在线程的通信延迟.预处理将多个group融合成一个,降低了group内单个actor与其他group中actor通信边的数目.预处理过程的输入是经过周期性调度的SDF图和预处理后得到的group图中最少应该有的group的数目virtualGroupNum.virtualGroupNum是为图2SDF图预处理的示例(2)Group粗粒度阶段Group粗粒度是对预处理后的group图进行粗化处理,将多个相邻的group融合成一个,在这个过程中要综合考虑group的负载以及将相邻一对group融合时对于降低通信开销产生的收益.为了避免在最终划分结果中出现环,在粗粒度时要避免group图中出现环路.一对group融合产生的收益称为粗化收益,计算公式如式(1)所示:gain=comm(SrcGroup,SnkGroup)其中,workload(srcGroup)与workload(snkGroup)表示srcGroup和snkGroup各自的负载,comm(srcGroup,snkGroup)表示srcGroup与snkGroup之间的通信开销,通信开销包括数据发送和数据接了防止过度融合,对group数量设置的阈值.图2描述了对一个SDF图预处理的示例.图2(b)中的B和K与其上端或下端节点融合后得到的新的group与其他group中actor通信边的数目明显减少,O没有与L和P组合在一起的原因是融合前后通信边数目没有变化.图2(a)表示最初的SDF图,图(b)描述了预处理的过程,其中被虚线圈在一起的表示在预处理阶段可以被融合,图(c)描述了经过预处理后形成的group图,通过比较发现图(c)的通信边数目明显少于图(a),且图(c)的结构也相对较为简单.收两个方面,通过该公式刻画了单位通信量减少获得的计算负载收益.粗粒度阶段采用贪心思想,首先计算所有相邻group的粗化收益将结果保存在一个优先队列中,从优先队列中选择收益最大的一对group做融合,如果融合后形成的新的group的负载不大于划分后负载理论平均值且融合后group图中不会出现环路,那么该次融合是有效的,将经过有效融合掉的group从group图中删除,融合得到的新的group插入到图中更新group间依赖关系,根据新的group更新优先队列中的收益,反复迭代上述过程.算法的终止条件是任何一对group间融合都不会产生正收益或者group图中group的数目小于阈值.算法1描述了Group划分粗粒度算法.图3(a)所示描述了图2预处理过后group图粗粒度的结果.Page5算法1.Group任务划分粗粒度算法.输入:经过初始化过后group间的依赖图G:(GV,C),输出:经过粗化后group间的依赖图G:(GV,C)//计算理论平均最佳负载averageWorkload=Weight(groupGraph)/groupCount=Count(groupGraph);//构造以收益为权值的优先队列priorityQueue=ConstructPriorityQueue(groupGraph);WHILE(groupCount>virtualGroupNum){maxGain=GetMaxGein(priorityQueue);IF(maxGain<=0)break;pair〈srcGroup,snkGroup〉maxGainChannel=IF(!Isfused(srcGroup,snkGroup)‖Weight(srcGroup)+priorityQueue.delete(maxGainChannel);newGroup=Fused(srcGroup,snkGroup);G.update(srcGroup,snkGroup,newGroup);priorityQueue.update(maxGainChannel,newGroup);--groupCount;}//ENDWHILE图3图2对应的group图粗化和初始划分的结果(3)初始划分阶段初始化分初步决定粗化后group图中group与集群节点之间的映射.经过预处理和粗粒度后group图的规模已经大幅度降低,group间通信边数量也相对较少.初始划分是使得各个划分负载均衡且尽可能保证划分间通信最小.在初始划分过程中同样要避免group间出现通信环路,初始划分采用预防死锁的策略,在划分开始就避免在划分结果中出现环路.粗粒度后group图是一个DAG图,对于DAG图拓扑排序能够利用图中节点间的偏序关系得一个拓扑序列.初始划分时根据group拓扑序列逐个考察group图中group节点,确定每个group具体的划分编号.算法2描述了初始划分算法.图3(b)图描述了图3(a)在集群节点数目为3的情况下的各个group对应的划分编号,初始化分完成后就基本确定了actor与集群节点之间的映射.算法2.Group任务划分初始划分算法.输入:经过粗化过后group间的依赖图G:(GV,C),输出:经过初始划分后group间的依赖图G:(GV,C)averageWorkload=Weight(groupGraph)/groupTopoLogicSort[]=intcurPartitionNo=0;//当前的划分编号WHILE(依次遍历groupTopoLogicSort中的group){curPartitionWorkload=Weight(group)+IF(Weight(curPartitionNo)<averageWorkload}//ENDIFELSEIF(curPartitionWorkload<averageWorkload//根据group间的通信情况确定该group是否能够localCommData=GetCommData(curPartitionNo,groupCommData=GetCommData(group);groupChannelCount=GetChannelCount(group);IF(localCommData>groupCommData/}//ENDELSE-IFELSE}//ENDWHILE根据initPartitionMap融合在同一个划分中的group,并更新groupGraph(4)细粒度调整阶段初始划分基本确定了group内的actor与集群Page6节点之间的映射,细粒度调整是将划分的边界actor,即与其他集群节点上的actor存在通信的actor,根据通信情况做进一步调优,降低节点通信开销.细粒度调整是以边界actor作为调整对象,对一个边界actor而言,该actor所在的划分集合称为源划分(srcPartition),与该actor有依赖关系的actor所在的划分成为目标划分(objPartition),一个actor只有一个srcPartition,而可能存在多个objPartition,actor与srcPartition中的其他actor的通信量为internalData,actor与第i个objPartition中的actor的通信量为externalData[i],在细粒度调整时维护一个优先队列,其权值是externalData[i]—internalData.在调整过程中选择权值最大的进行处理,一个actor能否被移动到一个objPartition要从如下两个因素考虑:首先,不会在划分中引入环路;其次,不会破坏整个划分间的负载均衡.一个actor调整完过后要根据调整过后的结果更新优先队列,对于调整过的actor不会再被作为调整对象.图4描述了细粒度调整的结果,其中(a)图是完成任务划分过后的SDF图,(b)图是对J节点进行了调整,从划分Ⅰ移动到Ⅱ后的结果.2.4数据流程序层次性流水线调度多核集群环境下,进程级划分在将子任务分图5数据流程序在集群上异步流水线执行示意图图5描述了多核集群环境下数据流程序异步流水线执行.图中共有3台多核机器分别对应编译器经过进程级任务划分将数据流程序分为3个子任务Ⅰ、Ⅱ和Ⅲ.机器内actor的执行与机器内部并行架构和调度方式有关,在共享存储多核平台上节点内部采用同步流水线调度.节点间异步流水线为了摊销单位数据量在传输中的开销,数据流程序在节点图4图2对应的SDF图初始划分和细粒度调整的结果配到节点上的同时也确定了子任务间的依赖关系.异步流水线调度没有全局同步时钟,子任务执行满足数据驱动的特性,子任务间的执行符合生产者消费者模式.图5描述了图2(a)对应的数据流程序在由3台机器组成的集群上的执行示意图,图中只为说明数据流程序在节点间的流水线执行情况,暂不考虑具体任务在节点内计算核上的执行.间采用块通信方式,生产者将通信块填满时触发消息传递机制,消费者在收到消息后开始执行.以图5中的Ⅰ和Ⅱ为例,当actorC执行一段时间后actorC和actorF间的通信缓冲区被填满C发送数据到F,F收到C产生的数据后F开始执行,同时C可以继续执行生成新的数据.通过异步流水线执行方式保证数据流程序在集群上的执行.Page7多核集群平台下同步流水线主要针对节点内部多核间,同步流水线没有像异步流水线一样采用数据驱动的方式执行,为了保证数据流程序的正确执行,节点内actor的执行顺序需要预先确定.线程级划分确定了actor与核间的映射,同步流水线从时间上确定actor执行顺序.在同步流水线调度中,流水线的启动间隔(InitiationInterval,II)是指相邻两次循环迭代进入流水线的时间间隔,II越小意味着吞吐率越大.线程级任务划分以负载均衡为目标就是最小化II.在软件流水线调度中通过阶段赋值算法确定哪个actor被分配到流水线的哪个执行阶段.为了维护SDF中计算节点数据依赖的正确性,对于当前多核机器内部的actorv和actoru,存在有向边(u,v),则v的阶段值不小于u的阶段值.算法3描述了根据线程级划分结果利用阶段赋值算法构造同步流水线调度的过程.对一台机器中的每个actor,如果其父actor在另一个机器上,该父actor的执行阶段不会影响当前机器中的actor;如果其父actor与当前actor在同一台机器上并且在同一个计算核上,那么父actor与子actor能够被分配在同一个执行阶段;如果父actor与子actor在同一台机器上但不在同一个计算核上,结合actor间数据依赖关系,子actor的阶段号会比父actor的阶段号大1.算法3.层次性流水线调度节点内阶段赋值算法.输入:一个节点内部actor与核之间的映射输出:actor与执行阶段号之间的映射actorTopoSort=TopoSort(actorCoreMap);WHILE(依次遍历actorTopoSort中的actor){intactorMaxStage=0;intactorStage=0;parentActors=actor.GetParentActor();WHILE(依次遍历parentActors中的parentActor){}//ENDWHILE算法3描述了根据线程级划分结果利用阶段赋值算法构造同步流水线调度的过程.对一台机器中的每个actor,如果其父actor在另一个机器上,该父actor的执行阶段不会影响当前机器中的actor;如果其父actor与当前actor在同一台机器上并且在同一个计算核上,那么父actor与子actor能够被分配在同一个执行阶段;如果父actor与子actor在同一台机器上但不在同一个计算核上,结合actor间数据依赖关系,子actor的阶段号会比父actor的阶段号大1.3基于COStream的数据流程序目标代码生成3.1层次性流水线代码生成根据COStream程序在多核集群下的执行方式和代码结构,COStream面向集群后端的目标代码主要包括如下几个部分:(1)actor间通信边代码.SDF图中有向边表示数据的流动方向,边上端结点生产数据,下端结点消耗数据.COStream基于SDF图的这种特点采用生产消费者模型对缓冲区进行管理.在多核节点内部,生产者和消费者使用共享内存作为通信方式,在实现上开辟供生产者和消费者共同访问的循环缓存区,生产者将数据写入缓冲区尾部,消费者从缓冲区头部访问数据.在节点之间,生产者和消费者采用消息传递的方式进行通信,在实现上生产者所在的节点上开辟发送缓冲区,在消费者所在的节点上开辟接收缓冲区,当生产者将发送缓冲区填满时,生产者发送消息,当消费者收到消息时,消费者从消费者缓冲区中取数据开始执行.(2)actor对应类定义代码.SDF中每个计算节点对应一个actor类定义,这类的成员函数主要包含:一个用于初始化该actor访问的缓冲区构造函数;一个初态调度函数以及一个稳态调度函数,其中初态调度函数和稳态调度函数是根据SDF周期性调度结果确定的;actor对应的operator中的成员方法;用于数据传输的方法.(3)同步流水线循环和控制代码.根据节点内同步流水线调度结果生成该节点内所有actor执行Page8的控制代码和计算核上线程间同步的控制代码.COStream根据线程级任务划分决定一个线程内的actor,通过同步流水线调度确定actor间的执行顺序.进程内部的actor间通信采用共享内存机制消除数据拷贝,利用流水线执行阶段间的同步,保证在同一个进程内有数据依赖的actor间的数据能够及时得到满足.(4)进程管理代码.进程管理代码主要包括根据划分结果为多核集群上的节点创建进程、在进程内部创建多个线程、为单节点内线程间通信开辟空间和控制执行进程上的计算任务等.3.2COStream多核节点内部基于Cache的通信和同步的设计与优化集群节点内同步流水线各执行阶段的同步采用lock-free[9-10]方式实现,lock-free抛开锁的概念,执行效率高.由于多线程共享缓存数据,Cache是以CacheLine为存储单位的,当多个线程修改互相独立的变量且这些变量共享同一个CacheLine上时,存在伪共享(FalseSharing),就会影响线程的性能.线程间同步的伪共享利用缓存行填充机制能够有效的消除,这种方式采用空间换时间的思想,但是在同一个集群节点内部actor间通信也会存在伪共享,如图6(a)所示生产消费者链中如果P、C在不同的核上并行执行时,当P和C访问的空间在同一个CacheLine上时也会发生伪共享,如图6(b).在复杂的数据流图中会存在多条核间通信边,如果依然采用缓存行填充机制,必然会造成大量空间被浪费,降低存储利用率且产生较高通信延迟.为了消除通信缓冲区的伪共享且尽可能提高Cache的利用率,COStream提出了稳态扩展技术.稳态扩展技术是指在编译阶段通过增大SDF图中actor稳态执行次数,使actor在一次稳态执行产生数据量至少大于一个CacheLine的空间,消除通信边的伪共享,如图6(c)所示为消除伪共享后Cache的使用情况.稳态扩展技术主要从如下几个角度对Cache的利用进行了优化:(1)提高了指令的局部性;(2)消除核间通信边的伪共享;(3)提高核内actor数据局部性.尽管增大actor稳态执行次数能够增大Cache的利用率,但如果稳态执行次数过大可能会造成Cache溢出,并且执行次数过多也会增加程序执行延迟.算法4描述了对数据流程序actor稳态扩展因子的确定,算法采用贪心思想:首先计算SDF稳态执行一次所有输出边消除伪共享后相关的actor应该扩展的系数,然后在所有扩展系数中找能够使所有actor扩展后在执行时都不会使L1数据Cache溢出的最大系数作为最终扩展系数.为了使Cache更好的发挥效用,在查找扩展系数时并不一定要使所有的actor执行都不会发生数据Cache溢出,根据“90/10原则”允许有10%的actor执行时发生溢出L1数据Cache,但不会使L2/L3Cache溢出,这样也能取得较好的性能.算法4.稳态扩展因子计算算法.输入:SDF图G(V,E)及其稳态调度S,层次划分的结输出:扩展因子f创建一个大小为|S|的数组mf[];WHILE(遍历P中的CP){Page9}//ENDWHILESortByValueDescending(mf);//对mf按值递增排序遍历mf找出让所有的actor在执行时都不会出现L1数据Cache溢出的值lf;f=lf;3.3COStream集群节点间通信模型设计集群节点间采用异步流水线调度方式,通信模块设计目的是简化代码生成屏蔽通信细节,为目标代码的执行提供运行时支持.COStream面向集群后端的节点间通信库是使用MPI实现的,利用MPI中的非阻塞通信机制使计算与通信尽可能重叠.在COStream中对于生产者缓冲区双缓存机制,保证在通信时生产者的计算可以与通信重叠执行;对于消费者采用可变长的循环缓冲区,由于COStream执行存在初态和稳态状态,在两种执行状态下消息长度并没有必然联系,变长循环缓冲区能够有效的消除在消息长度不确定的情况下接收时将消息存放在不连续空间上的开销.图7描述了两个actorP和C被划分在不同的机器上执行时节点间通信的过程,Buffer1和Buffer2标识actorP的双缓存,Buffer表示actorP的接收缓存,tail和head分别表示Buffer的头指针和尾指针,MaxSize和ValidSize分别表示缓冲区有效空间大小和最大空间大小.图7(a)中actorP运行填满一个缓冲区,将数据发送给actorC,设发送的消息的长度为length,当Buffer的尾指针tail+lengthMaxSize,消息直接被存放在接收者缓存中且ValidSize=MaxSize.但是如果出现tail+length>MaxSize的情况时,由于缓冲区尾部剩余的连续空间不够容纳一条消息,那么Buffer的ValidSize被修改,同时置tail为0,当head—taillength时才接收消息,在开始接收数据前缓冲区的使用如图7(b)所示.为了减少接收消息的等待时间,在实际实现时通常Buffer的长度大于Buffer1长度的二倍.通过双缓存和变长循环缓冲区使得计算能够尽可能与通信重叠.4实验结果与分析4.1实验平台与测试方法实验平台由4台X86-64架构的通用多核服务器作为服务计算节点,采用局域网组成小型集群系统.每个计算节点配有2颗4核的2.40GHzIntelXeonE5620CPU,最大支持内存48GB,1个Gigabit网卡.节点上Linux操作系统内核版本为2.6.18,gcc版本为4.1.2,编译优化选项采用-O2,MPI版本是mpich2-1.4.1p1.为了对层次流水线编译优化进行全面的性能评估和分析,实验选取了9个数字媒体领域典型算法对其进行扩展作为测试程序,用COStream数据流编程语言实现.各个测试程序的功能、规模和结构描述如表1所示.测试程序名BeamFormer滤波器程序228286ChannelVocoder频道话路编码器497640DCTFFTFilterbank多速率信号处理滤波9801553SperpentVocoder比特率减低声码90011794.2并行优化系统实验与性能评价(1)层次性任务划分算法性能评价图8给出了进程级和线程级均用多层K路图划分算法[11](MultilevelK-wayPartitioning,MKP)实现的任务划分算法与层次任务划分算法的一个性能比较,实验在4个节点组成的集群下进行的.在实验中测试程序BeamFormer、ChannelVector、Serpent和Vocoder在采用MKP时划分结果出现环路,实际执行时出现死锁,无法统计执行时间;所有测试程序在层次性划分算法下均能够正确执行.从图8可以看出,对于MKP和层次性划分均能正常执行,程序中DCT采用MKP划分结果的执行时间是采用层次性划分的1.8倍,分析发现由于MKPPage10划分是以负载均衡为目标没有充分考虑节点通信延迟,造成MKP划分尽管各个节点有相对均衡的负载但节点间的通信数据量比采用层次性划分要大;对tde层次性划分比MKP要差,原因是层次性划分考虑了通信开销,程序在进程级划分后划分间的负载没有MKP好.平均来看层次性划分比MKP更能够满足数据流程序在多核集群下的执行.图8层次性任务划分算法与MKP划分算法对比(2)加速比图9给出了测试程序经过COStream编译后在单节点多核上执行加速比的示意图.从图中可以看出,测试程序执行加速比随核数目的增多而增大,基本呈现线性增长趋势.图10描述了测试程序分别在1个、2个、3个和4个节点的集群规模上加速比示意图.在图中测试程序执行加速比均呈现一个线性增长的趋势.测试程序在2个节点的情况下加速比基本能够达到1.5x以上,在4个计算节点的情况下tde达到3.5x且有3个测试程序的加速比在3x以上,加速效果比较理想;但对于FilterBank、Serpent和Vocoder等程序在4个节点上时,加速效果并不是十分理想.通过对分析发现由于SDF图本身结构复杂,进程级任务划分后进程间通信复杂,使得计算与通信不能够重叠,造成大的通信开销,影响加速效果.通过对测试程序在多核节点内部和多核集群上的执行情况分析发现影响加速效果主要有下面4个因素:(1)负载均衡.由于线程间采用同步流水线调度,如果计算核上的任务负载不均衡,即使不考虑同步、通信等因素,程序执行受负载最大的那个计算线程的影响,程序执行效果也不可能达到理想要求.对于集群节点间,负载均衡能够使各个节点生产和消耗数据速率相匹配,最小化数据等待延迟;(2)局部性(locality).局部性主要针对线程级任务划分,在线程级划分后如果子图的局部性越差,节点内核间通信边数目就越多,程序运行时核间就需要频繁地进行数据通信,影响数据局部性,增大数据访问延迟;(3)通信开销.通信开销主要针对集群节点间,尽管为了降低节点间数据传输开销采用了非阻塞通信机制和双缓存缓冲区管理策略,但进程级任务划分后如果数据传输时间大于计算单元产生能够填满一个缓冲区数据所需的计算时间,计算不能够与通信重叠,计算单元因为不足而陷入等待.另外,如果节点间通信边的数目过多,在节点间通信的软件层面需要切换网络连接,也会产生较大通信延迟;(4)数据流程序自身特性,主要是SDF图中通信边数、actor的负载以及actor生产和消耗数据速率等因素.在图10中Vocoder由于自身actor负载分布不均匀导致加速效果不理想;FilterBank由于actor间通信复杂加速效果也不太理想;而tde能够达到3.5x的加速比是由于图中各个actor的负载均衡且actor间通信边的数目较少,经过进程级和线程级划分能够得到具有良好负载均衡和低通信的划分结果,有较高加速比.(3)扩展性为了分析层次性流水线优化的扩展性,对部分测试程序的规模进行修改,图11描述了测试程序的不同规模在多核集群下的执行情况.从图中可以看出,一般当程序规模较小时,多节点集群环境并不能取得很好的性能收益.原因是节点间的通信不能被计算隐藏,在这种情况下单节点多核是较理想的选择,但随着程序规模的增大集群的优势也就越明显.从图11可以看出SDF图的规模变大,部分测试程Page11序的加速比能够达到3x以上,在图中FFT和DES随着测试程序规模的变化,加速比基本呈现一个线性增长趋势;tde和BeamFormer加速比并没有随着程序规模的变化,是由于在规模变大时tde的SDF图中actor间的通信量变大而BeamFormer的SDF图11多核集群环境下测试程序不同规模下执行加速比(4)基于Cache伪共享优化的效率为了提高节点内部程序的执行效率,针对Cache存在的伪共享进行了稳态扩展优化,图12描述了Cache优化前后程序在单节点8个核上执行性能的比较.测试程序中有3个程序执行了稳态扩展,而其他测试程序由于actor稳态执行生产的数据量超出了一个CacheLine大小不会发生伪共享.实验中BeamFormer优化后的性能提升达到60%,原因是该程序自身的通信比较复杂使得划分完成后核间的通信边过多.对经过稳态扩展的程序进行分析发现影响Cache优化性能提升的主要因素是核间通信边的数目.核间通信边越多,Cache伪共享对性能的影响越大,消除伪共享带来的性能提升越大,但在现代图中通信边增多,导致在实际执行时通信不能被计算覆盖,影响加速效果.总的来说,由于程序的规模变大,使得编译有更大的优化空间,从而能够取得较好的效果.通过图11说明了层次流水线并行优化具有较好的扩展性.的处理器结构中由于存在L2Cache甚至L3Cache,在一定程度上降低了Cache伪共享对程序执行性能的影响.图12单节点内8个核下测试程序在Cache优化前后执行Page125相关工作数据流编程模型是针对数据流应用大规模密集型计算特点和多核处理器而设计的一种编程模型[12].针对多/众核平台数据流程序的编译优化的研究成为当前研究的热点.麻省理工学院的Gordon等人[9,13]针对StreamIt在Raw下利用分裂和融合技术挖掘stateless类型节点存在的任务和数据并行性,利用软件流水线调度技术挖掘stateful类型节点的流水线并行性;Sermulins等人[14]研究StreamIt在带有Cache的处理器下的执行情况,提出了采用执行缩放技术、基于Cache的融合技术和标量替换技术相结合的方式提高指令和数据的局部性,改善程序执行性能;密歇根大学的Kudlur等人[15]针对流程序中stateless节点存在的并行性提出了SGMS调度方法(StreamGraphModuloScheduling),但是该方法在任务划分时只对计算资源进行形式化建模,没有考虑通信和存储资源;斯坦福大学的Park等人[16]针对SGMS提出了Team调度算法(TeamScheduling),Team调度灵活的控制缓冲区大小,适用于局部存储受限的多核机器;华中科技大学Wei等人[17]在资源受限的多核架构上采用整数线性规划理论对计算、通信、存储等资源和流水线执行阶段进行建模,但该方法并没有考虑分布式环境对流程序执行的影响.针对上述问题,本文基于多/众核架构提出一种层次流水线并行优化方法,并结合底层系统结构做优化,提高数据流程序在多核集群下的执行性能.6总结本文设计并实现了一个基于SDF图的数据流程序层次流水线并行编译优化框架,通过该框架有效提高了数据流程序在多核集群下的执行性能.该并行编译优化框架包含3个模块:层次性任务划分模块、层次流水线调度模块和与底层结构相关优化模块,最后生成基于MPI目标代码.该编译优化框架使得数据流程序在多核集群下能够取得较好的执行效果,但当前调度主要针对同构集群,在异构集群平台下如何开展数据流程序的并行编译优化,是数据流程序需要进一步研究的问题.
