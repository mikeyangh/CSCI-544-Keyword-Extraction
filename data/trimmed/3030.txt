Page1基于联合意义度量的Top-犓图模式挖掘刘勇高宏李建中(哈尔滨工业大学计算机科学与技术学院哈尔滨150001)摘要提出了一个新的研究问题:如何挖掘Top-K图模式,联合起来使某个意义度量最大化.利用信息论的概念,给出了两个具体问题的定义MES和MIGS,并证明它们是NP-难.提出了两个高效算法Greedy-TopK和Clus-ter-TopK.Greedy-TopK先产生频繁子图,然后按增量贪心方式选择K个图模式.Cluster-TopK先挖掘频繁子图的一个代表模式集合,然后从代表模式中按增量贪心方式选择K个图模式.当意义度量满足submodular性质时,Greedy-TopK能提供近似比保证.Cluster-TopK没有近似比保证,但比Greedy-TopK更高效.实验结果显示,在结果可用性方面,文中提出的Top-K挖掘优于传统的Top-K挖掘.Cluster-TopK比Greedy-TopK快至少一个数量级.而且,在质量和可用性方面,Cluster-TopK的挖掘结果非常类似于Greedy-TopK的挖掘结果.关键词图挖掘;图数据库;频繁子图;代表模式;联合熵;信息增益1引言作为一种通用的数据结构,图可以用来表示数据对象之间的各种复杂关系.例如:图可以表示化合物的分子结构、蛋白质交互网络、社会网络、Web结构图等.很多与图有关的应用都需要利用图模式来管理、查询和分析图数据.例如:图查询[1]可以利用Page2图模式建立有效的索引,图分类[2]可以利用图模式建立有效的分类模型.因此,从图数据库中发现有用的图模式已成为数据挖掘领域一项重要的研究课题.目前,与图模式挖掘有关的绝大部分研究主要集中在如何高效地挖掘频繁子图[3-8]以及频繁子图的各种简洁表示(频繁闭图模式[9]和频繁最大图模式[10-11]).然而,很少有研究考虑如何根据用户给定的意义度量来挖掘图模式.本质上,频繁子图挖掘所使用的支持度恰恰是一种特殊的意义度量.然而,在不同的应用中,用户很可能需要不同的意义度量.给定一个意义度量,传统Top-K挖掘方法对所有可能的图模式根据度量值的大小排序,输出前K个图模式.然而,传统Top-K挖掘并不考虑图模式之间的相关性,输出的Top-K模式可能非常相似.第5节的图5显示了信息增益作为意义度量时,传统Top-K挖掘方法从一个图数据库中挖掘的Top-5图模式.这5个图模式在结构上非常相似.如果用户得到其中一个图模式,就会对其它图模式失去了兴趣,因为在实际应用中用户希望得到的是一个多样化的图模式集合.为了克服传统Top-K挖掘方法的缺点,本文研究基于联合意义度量的Top-K图模式挖掘方法.联合意义度量的作用域是图模式集合而不是图模式,因此充分考虑了图模式之间的相关性.给定一个联合意义度量,本文要研究的问题就是挖掘Top-K图模式,联合起来使该意义度量最大化.本文的目标是设计一个适用于任何意义度量的通用的Top-K挖掘算法.本文首先讨论了适用于图模式集合的意义度量,并利用信息论中的概念(联合熵和信息增益)给出了两个具体问题的定义MES和MIGS,然后证明了它们是NP-难问题.为了高效地挖掘基于联合意义度量的Top-K图模式,本文给出了两个算法Greedy-TopK和Cluster-TopK.Greedy-TopK先产生频繁图模式(或频繁闭图模式),然后按增量贪心方式选择K个图模式.本文证明了如果用户给定的意义度量满足submodular性质,Greedy-TopK能提供近似比保证.然而,当频繁图模式(或频繁闭图模式)数量很大时,Greedy-TopK效率低,可扩展性差.为此,本文又提出了另一个更高效的算法Cluster-TopK.Cluster-TopK先从图数据库中挖掘所有频繁图模式的一个代表模式集合,然后从代表模式中按增量贪心方式选择K个图模式.Cluster-TopK最大的优点是无需产生频繁图模式(或闭图模式)就能快速地挖掘一个代表模式集合.而且,因为代表模式的数量比闭图模式的数量少很多,所以Cluster-TopK的贪心选择也比Greedy-TopK的贪心选择快很多.此外,本文还从理论上严格证明了Cluster-TopK产生的解和Greedy-TopK产生的解非常接近.实验结果表明本文提出的Top-K挖掘在结果质量和可用性方面要远远优于传统Top-K挖掘.Cluster-TopK和Greedy-TopK在结果质量和可用性方面非常接近.然而,Cluster-TopK能比Greedy-TopK快1~2个数量级.本文第2节介绍预备知识;第3节给出问题定义和NP-难证明;第4节给出完整的算法描述和算法分析;第5节通过实验证明算法的有效性以及挖掘结果的可用性;第6节介绍相关工作;第7节总结全文.2预备知识本节介绍图挖掘和信息论的一些基本概念.定义1(标号图).标号图G定义为四元组G=(V,E,Σ,l),其中,V是顶点集合,EV×V是边集合,Σ是标号集合,l:V∪E→Σ是一个函数,用来对顶点和边分配标号.定义2(子图同构).给定两个图G=(V,E,Σ,l)和G=(V,E,Σ,l),一个从G到G的子图同构是一个单射函数f:V→V,满足:(1)u∈V,l(u)=l(f(u));(2)(u,v)∈E,(f(u),f(v))∈E并且l((u,v))=l((f(u),f(v))).单射函数f也称为G在G中的一个嵌入.如果存在一个从G到G的子图同构,则G称为G的子图,G称为G的超图,记为GG.如果GG且G≠G,则G称为G的真子图,G称为G的真超图,记为GG.子图同构测试已被证明是一个NP-完全问题[12].如果GG,也称G包含G.给定一个图数据库D={G1,G2,…,Gn}和一个图模式p,p在D中的支持集定义为D中包含p的图集合,记为Dsupp(p)={Gi|pGi,Gi∈D}.|Dsupp(p)|称为p在D中的支持度,记为supp(p;D).|Dsupp(p)|/|D|称为p在D中的相对支持度.支持度度量具有反单调性质:如果p1p2,则supp(p1;D)supp(p2;D).对于用户给定的一个最小支持度阈值min_sup,如果supp(p;D)min_sup,称p在D中是频繁的.D中所有频繁图模式集合记为FS={p|supp(p;Page3D)min_sup}.本文中,min_sup既可以表示绝对最小支持度阈值,又可以表示相对最小支持度阈值.如果在D中p的任何真超图与p支持度都不相同,则称p是D中的闭图模式.D中所有频繁闭图模式集合记为CS={p|p∈FS,p∈FS使得pp并且supp(p;D)=supp(p;D)}.上下文明确时,可用supp(p)代替supp(p;D).因为本文后面介绍的意义度量将使用信息论的一些概念[13],这里先回顾一下有关的基本定义.定义3(熵).随机变量x的熵定义为其中dom(x)是x的定义域,p(vx)是x等于vx时的概率.定义4(条件熵).在给定随机变量x的条件下,随机变量y的条件熵定义为H(yx)=-∑vx∈dom(x)∑vy∈dom(y)p(vx,vy)log(p(vyvx)).定义5(联合熵).随机变量x和y的联合熵定义为H(x,y)=-∑vx∈dom(x)∑vy∈dom(y)p(vx,vy)log(p(vx,vy)).3问题定义本节首先讨论用于图模式集合的意义度量,然后给出两个具体问题的定义MES和MIGS,最后证明它们是NP-难问题.文献中已有大量的用于单一模式(包括项集模式,序列模式和图模式)的意义度量.例如:在统计学领域,x2检验和Pearson相关可以用来度量模式的统计意义.在数据挖掘和机器学习领域,信息增益和交叉熵可以用来度量模式是否适合作为分类特征.文献[14]总结了21个常用的意义度量.尽管文献中给出的大部分意义度量都能直接用来度量单一图模式的意义,然而它们中的绝大部分并不能用来量化一个图模式集合的意义.本文算法需要用户提供一个意义度量M使得M能量化一个图模式集合的意义.信息论中的联合熵和信息增益可以用来量化一个图模式集合的意义.在给出具体的问题定义之前,我们先给出本文要研究的一般问题.假设S是给定的图模式集合(例如:S可以表示某个图数据库中所有频繁图模式集合),T是S的子集合,M(T)是T的意义度量.本文要解决的问题就是发现S的一个大小为K的子集合T使得M(T)被最大化,即如果所有候选的图模式给定的话,上面定义的问题显然是一个组合优化问题.本文的目标是设计求解上述问题的通用算法,不受任何具体意义度量的限制.为了方便描述算法,我们利用信息论中的联合熵和信息增益给出下面两个具体的问题定义.定义6(基于熵的意义度量最大化).给定图数据库D,D的一个图模式集合S(或者最小支持度min_sup)和一个整数K,最大化基于熵的意义度量问题(MES)就是从S(或者D的所有频繁图模式集合)中发现一个大小为K的子集合T使得H(T)被最大化,其中H(T)是T中随机变量(图模式)的联合熵.定义7(基于信息增益的意义度量最大化).给定图数据库D,D的一个图模式集合S(或者最小支持度min_sup)和一个整数K.假设D中每个图都有一个类标记,设C是表示类标记的随机变量.最大化基于信息增益的意义度量问题(MIGS)就是从S(或者D的所有频繁图模式集合)中发现一个大小为K的子集合T使得IG(T)=H(C)-H(C|T)被最大化,其中H(C)是C的无条件熵,H(C|T)是T中随机变量(图模式)给定的条件下C的条件熵.基于熵的意义度量经常用来在无监督的环境下度量不确定性,而基于信息增益的意义度量经常用来在有监督的环境下选择强分类特征.在上面的形式化定义中,我们是把每个图模式p看成了一个随机变量vp,如果数据库中的某个图G含有p,则在G上vp等于1,否则vp等于0.通常,当某个具体的意义度量给定时,式(1)定义的问题是NP-难问题.下面,我们证明MIGS问题(定义7)是NP-难的.类似地,也可以证明MES问题(定义6)是NP-难的.定理1.最大化基于信息增益的意义度量问题(MIGS)是NP-难的.证明.通过把最大覆盖问题规约到MIGS问题,我们来证明该定理.设MC=(E,S,K)是最大覆盖问题的任一实例,其中E={e1,e2,…,en}是元素集合,S={S1,S2,…,SL}是集合族.求解MC要求从S中选择K个集合,使得这K个集合覆盖的元素数量最大化.我们可以在多项式时间内把MC转化成MIGS问题的一Page4个实例.(1)把E中每个元素看成一个边的标号,那么S中的某个集合Si={ei1,ei2,…,eit}可以转换成图1所示的图模式.该图模式用P(Si)表示.P(Si)中所有结点的标号都相同,但是边的标号各不相同.以相同的方式,我们可以把S中的所有集合都转换成对应的图模式.所有这些图模式构成的集合用PS表示.图1对应Si={ei1,ei2,…,eit}的图模式P(Si)(2)我们首先为E中的每个元素构造一个图.如果一个元素e能被{Si1,Si2,…,Sim}中的集合覆盖,我们为e构造一个图2所示的图,其中P(Si)是对应集合Si的图模式,连接各个图模式P(Si)的边标号都相同.显然,每个元素都将对应一个图,我们把所有这样的图标记为正类,放入数据库DB.此时,|DB|=n.(3)我们然后随意构造n个图使得这n个图中不含有PS中的任何图模式.我们把这n个图标记为负类,放入数据库DB.此时,|DB|=2n.给定上面构造的图数据库DB,DB中的一个图模式集合PS以及正整数K,我们得到了MIGS问题的一个实例IMIG.DB中正类图个数等于负类图个数,因此H(C)=1,其中C是表示类标记的随机变量.设T是PS中任意一个大小为K的子集合.因为T中每个图模式对应S中的一个集合,因此从T可得到S的一个大小为K的子集合S.设被S覆盖的元素数是x.对T中的任何图模式p,DB中的任何负类图都不含有p.根据条件熵定义,H(C|T)=-(n-x)+n(n-x)+n+nlogn-x(-n-x因此,IG(T)=H(C)-H(C|T)=1+n-x2n-x+1logn-x12logn2n-x.可以证明f(x)的一阶导数f(x)大于0,f(x)是一个单调递增函数.当x的值增大时,IG(T)也增大.因此,IMIG的最优解能被用来导出MC的一个最优解.4挖掘Top-犓图模式MES和MIGS都是NP-难问题,因此需要设计近似算法或启发式算法求解它们.本文提出了两个高效算法Greedy-TopK和Cluster-TopK来求解它们.Greedy-TopK从频繁图模式集合中贪心选择Top-K模式,具有近似比保证.Cluster-TopK从图数据库中挖掘代表图模式集合,然后再从中贪心选择Top-K模式.尽管Cluster-TopK没有近似比保证,Cluster-TopK却具有极高的挖掘效率.4.1Greedy-Top犓算法4.1.1算法设计Greedy-TopK算法采用著名的贪心策略,从所有频繁图模式集合中增量地选择K个图模式.为了应用贪心策略,我们定义了一个收益函数b.Greedy-TopK每次选择使收益函数b最大的图模式.假定T是已经选择的图模式集合.对MES和MIGS问题,图模式p的收益函数b定义如下b(p)=H(T,p)-H(T),对MES根据式(2)给出的贪心规则,我们设计了一个贪心算法Greedy-TopK,如算法1所示.初始化时,使用某个图挖掘算法(例如:gSpan[6])挖掘所有频繁图模式集合F,同时置结果集T为空.然后算法选择最有意义的图模式p放入T中.此后,算法进入一个循环过程,每次从剩余的图模式集合F-T中选择一个使收益函数b最大的图模式p,放入T中.当结果集T中图模式个数等于K时,退出循环,算法结束.算法1.Greedy-TopK.输入:图数据库D;最小支持度阈值min_sup;意义度量输出:使M最大化的K个图模式1.根据min_sup,挖掘D中所有频繁图模式F;Page52.从F中选择一个图模式p使M(p)最大化;3.T={p};4.While(|T|<K)do5.从F-T中选择图模式p使得b(p)最大化;6.T=T∪{p};7.输出T.下面,我们分析算法Greedy-TopK的近似比.当意义度量M满足submodular性质(见下面的定义)时,贪心算法将给出近似解[15].定义8(submodular性质).设M是定义在一个集合S上的度量函数.如果对任何TTS和任何p∈S,都有M(T∪{p})-M(T)M(T∪{p})-M(T),则称M是一个submodular的集合函数.仔细分析本文给出的两个意义度量,我们发现基于熵的意义度量满足submodular性质,因此算法Greedy-TopK对MES问题能给出如下的近似比.定理2.设T是算法Greedy-TopK选择的K个图模式集合,T是使联合熵最大化的K个图模式集合.那么,H(T)文献[15]给出了完整的证明过程.我们发现基于信息增益的意义度量并不满足submodular性质.因此,对MIGS问题,算法Greedy-TopK不能给出像定理2那样的近似比.然而,我们注意到,对MIGS问题,无条件熵H(C)是任何模式集合信息增益的上界.因此,算法Greedy-TopK对MIGS问题能给出如下的联机近似比.定理3.设T是算法Greedy-TopK选择的K个图模式集合.那么,算法Greedy-TopK对MIGS问题给出的联机近似比至多是H(C)/IG(T).4.1.2裁剪技术算法Greedy-TopK的第1步需要遍历图模式搜索空间,挖掘所有频繁图模式.本小节研究如何根据给定的意义度量设计有效的裁剪技术,将其集成到图模式挖掘算法的框架中裁剪图模式搜索空间.所有频繁子图挖掘算法都利用了支持度的反单调性质来裁剪搜索空间.即一个图模式p的支持度是p的所有超图支持度的上界.然而,通常情况下,给定的意义度量并不具有反单调性质.因此,我们不能像利用支持度那样简单地利用给定的意义度量.幸运的是,给定一个图模式p的意义度量值M(p),我们可以导出p的所有超图意义度量值的上界,从而根据这个上界来裁剪图模式搜索空间.我们仍然采用Greedy-TopK算法的框架来增量地选择Top-K图模式.因此,我们分两种情况介绍裁剪技术:当结果集为空时,选择第1个图模式;当某些图模式已经在结果集中,选择下一个图模式.(1)选择第1个图模式.下面讨论结果集为空时如何设计有效的裁剪技术.请注意,我们在4.1.1.1节和4.1.1.2节给出的裁剪技术可以应用到任何频繁子图挖掘算法的框架中.因此,我们未给出具体的挖掘算法.定理4给出了基于熵的裁剪条件.定理5给出了基于信息增益的裁剪条件.定理4.设D是给定的图数据库,q是p的任意超图.如果supp(p;D)1/2,则H(q)H(p).定理5.给定图数据库D=PD+ND,其中PD和ND分别是正类和反类图集合.设q是p的任意超图,PD中有x个图含有p,ND有y个图含有p.那么,IG(q)max其中,H=-|PD|(2)选择下一个图模式.结果集不空时如何设计有效的裁剪技术.在给出具体的裁剪技术之前,我们先定义一个新的概念———等价类.定义9(等价类).设D是图数据库,G是D中的任意图,T是已经选择的图模式集合.G相对于T的等价类定义为{G|G∈D,p∈T,I(pG)=I(pG)},其中I(·)是指示函数.根据上面等价类的定义,一个图模式集合T可以将数据库D划分成若干个等价类(块)集合DT={Bi|1il},并且D=∪1ilBi.利用等价类的定义,我们可以在结果集非空的情况下导出有效的裁剪条件.定理6给出了基于熵的裁剪条件.定理7给出了基于信息增益的裁剪条件.定理6.设D是给定的图数据库,T是已经选择的图模式集合,DT={Bi|1il}是用T划分D得到的等价类集合,q是p的任意超图(pq),并且pT,qT.那么,Page6H(T∪{q})∑Bi∈DT其中HBi(p)(=-xi|D)|,xi是Bi中含有p的图个数.log|Bi|-xi定理7.给定图数据库D=PD+ND,其中PD和ND分别是正类和反类图集合.T是已经选择的图模式集合,DT={Bi|1il}是用T划分D得到的等价类集合,q是p的任意超图(pq),并且pT,qT,那么,IG(T∪{q})H+∑Bi∈DT其中,H=-|PD|αBi=mi-xi|D|logmi-xiβBi=ni-yi|D|logni-yi(γBi=xi|D|logxixi+yi+yilogmi-xi|Bi|-(xi+yi)+ni-yimi和ni分别是Bi中正类图个数和负类图个数,xi和yi分别是Bi中含有p的正类图个数和含有p的负类图个数.4.2Cluster-Top犓算法Greedy-TopK算法需要先产生所有的频繁图模式,在频繁图模式数量较少的情况下,Greedy-TopK算法具有较高的效率.然而,在实际应用中,当数据库中图较稠密或者用户给定的支持度阈值较低时,频繁子图挖掘算法通常产生大量的甚至指数级的频繁子图.这使得Greedy-TopK算法不能在合理的时间内完成任务,限制了该算法的可用性.即使使用4.1.1节中的裁剪技术,Greedy-TopK也需要遍历图模式空间中的大量图模式.为此,本节提出了另一个更高效的算法Cluster-TopK.Cluster-TopK算法的基本思想是将所有频繁图模式聚类成若干个簇,选择每个簇的中心作为代表模式构成一个候选集合T,然后再使用贪心策略从T中增量地选择K个图模式.Cluster-TopK算法的优点是不需要先产生所有的频繁图模式,就能从图数据库中快速地挖掘代表模式得到候选集,具体内容见4.2.2节.假设C={p1,p2,…,pn}是由若干个图模式构成的一个簇,pi是该簇中心.因为pi将作为代表模式被选择,pi应具有这样的特点:Pj∈C,PjPi,并且Pj和Pi在数据库图中应经常一起出现,即Pj和Pi有类似的支持度.下面,我们给出这种类型簇的两个形式化定义δ-簇和Δ-簇.定义10(δ-覆盖).设δ(0δ1)是用户给定的一个参数,p和q是任意两个图模式.如果满足qp,1-supp(p)定义11(δ-簇).设δ(0δ1)是用户给定的一个参数,C={p1,p2,…,pn}是一个图模式集合.如果C中存在图模式pi满足pj∈C,pj被piδ-覆盖,则称C是一个δ-簇,称pi为该δ-簇的代表模式(中心点).定义12(Δ-覆盖).设Δ(Δ>0的整数)是用户给定的一个参数,p和q是任意两个图模式.如果满足qp,supp(q)-supp(p)Δ,则称q被pΔ-覆盖.定义13(Δ-簇).设Δ(Δ>0的整数)是用户给定的一个参数,C={p1,p2,…,pn}是一个图模式集合.如果C中存在图模式pi满足pj∈C,pj被piΔ-覆盖,则称C是一个Δ-簇,称pi为该Δ-簇的代表模式(中心点).Cluster-TopK算法就是根据用户给定的参数δ(或Δ),将频繁图模式集合划分成若干个δ-簇(或Δ-簇),选择每个δ-簇(或Δ-簇)的代表模式构成候选集,然后再从中贪心选择K个图模式.4.2.1节分析了Cluster-TopK算法采用这种聚类策略的优点.4.2.2节研究了如何在不产生所有频繁图模式的情况下快速地挖掘每个δ-簇(或Δ-簇)的代表模式.4.2.3节给出了具体的算法实现.4.2.1Cluster-TopK聚类策略的优点本小节讨论Cluster-TopK算法为什么采用这种先聚类后贪心的策略.设N是给定图数据库D的大小,即|D|=N.定义函数f(n)=nnN).下面的引理和定理将利用该函数.引理1.设D是给定的图数据库,|D|=N,P1和P2是两个图模式,d是P1和P2之间的海明距离(即Page7d=∑Gi∈D函数).那么,0H(P1,P2)-H(P1)d(f(1)+f(N-1)).证明见文献[16]中的Proposition6.3.根据引理1,我们容易得到下面的推论1.推论1.设D是给定的图数据库,|D|=N,F={P1,P2,…,Pn}是任意图模式集合,Pn+1是任意一个图模式,如果Pn和Pn+1之间的海明距离是d,则H(P1,…,Pn-1,Pn)-H(P1,…,Pn-1,Pn+1)d(f(1)+f(N-1)).证明.H(P1,…,Pn-1,Pn)-H(P1,…,Pn-1,Pn+1)H(P1,…,Pn,Pn+1)-H(P1,…,Pn-1,Pn+1)=H(Pn|P1,…,Pn-1,Pn+1)H(Pn|Pn+1)=H(Pn,Pn+1)-H(Pn+1).因为Pn和Pn+1之间的海明距离是d,再根据引理1,可得H(P1,P2,…,Pn)-H(P1,…,Pn-1,Pn+1)H(Pn,Pn+1)-H(Pn+1)d(f(1)+f(N-1)).因为频繁图模式的数量通常很大,直接对频繁图模式进行贪心选择,时间复杂性太高.对频繁图模式先聚类,只提取每个类的代表模式(中心点)构成候选集合,可以将频繁模式的数量降低2~3个数量级(见后面的实验结果).然后对候选集合进行贪心选择,可使算法具有极高的效率.而且,我们可以证明代表模式集合中的最优解和频繁图模式集合中的最优解差别很小.定理8针对MES问题,分析了最优解之间的差异.定理9针对MIGS问题,分析了最优解之间的差异.定理8和9给出了利用Δ-簇的分析结果.类似地,我们也容易给出利用δ-簇的分析结果.以MES问题为例,我们分析具体的差异.假设定理8中的N=10000,K=10,Δ=5,则有H(T)-H(S)ΔK(f(1)+f(N-1))≈0.073.这是理论上的最大可能差异,而实验中的结果显示:从聚类结果中选择的解和从频繁图模式集合中选择的解差别微乎其微.定理8.设D是给定的图数据库,|D|=N,F={P1,P2,…,Pn}是D中频繁图模式集合,F被划分成m个Δ-簇的集合CS={C1,C2,…,Cm},其中每个Ci(1im)都是一个Δ-簇,并且F=C1∪C2∪…∪Cm.CS中每个Δ-簇的代表模式被选择构成了一个集合RS={R1,R2,…,Rm},其中Ri(1im)是Ci的代表模式.再假设T是F中的一个大小为K的子集合,并且使H(T)最大化.S是RS的一个大小为K的子集合,并且使H(S)最大化.那么,H(T)-H(S)ΔK(f(1)+f(N-1)).证明.设T={Q1,Q2,…,QK}.因为F被划分成m个Δ-簇的集合CS,RS={R1,R2,…,Rm}是CS中Δ-簇的代表模式集合,并且TF,根据Δ-簇定义,对任意Qi∈T,都存在RS中的一个代表模式R使得R能Δ-覆盖Qi.不失一般性,假定Q1被R1Δ-覆盖.根据Δ-覆盖定义,可知Q1R1,supp(Q1)-supp(R1)Δ.因此,Q1与R1之间的海明距离d=supp(Q1)-supp(R1)Δ.我们试图用R1替换Q1.令T=T-{Q1}∪{R1}.根据推论1,可得H(T)-H(T)d(f(1)+f(N-1))Δ(f(1)+f(N-1)).以此类推,如果T中每个图模式都被RS中对应的代表模式所替换,可得到一个新的集合T,并且H(T)-H(T)ΔK(f(1)+f(N-1)).显然,T是RS的一个大小为K的子集合.因为S是RS的一个大小为K的子集合,并且使H(S)最大化.因此,H(T)-H(S)H(T)-H(T)ΔK(f(1)+f(N-1)).定理9.设D是给定的图数据库,|D|=N,F={P1,P2,…,Pn}是D中频繁图模式集合,F被划分成m个Δ-簇的集合CS={C1,C2,…,Cm},其中每个Ci(1im)都是一个Δ-簇,并且F=C1∪C2∪…∪Cm.CS中每个Δ-簇的代表模式被选择构成了一个集合RS={R1,R2,…,Rm},其中Ri(1im)是Ci的代表模式.再假设T是F中的一个大小为K的子集合,并且使IG(T)最大化.S是RS的一个大小为K的子集合,并且使IG(S)最大化.那么,IG(T)-IG(S)2ΔK(f(1)+f(N-1)).证明.假设C是表示数据库D中图类别的随机变量.类似于定理8中的证明过程,可得|H(T)-H(S)|ΔK(f(1)+f(N-1)),|H(C,T)-H(C,S)|ΔK(f(1)+f(N-1)).因为IG(T)-IG(S)=(H(C)-H(C|T))-(H(C)-H(C|S))=H(C|S)-H(C|T)=(H(C,S)-H(S))-(H(C,T)-H(T))=(H(T)-H(S))-(H(C,T)-H(C,S)),因此IG(T)-IG(S)2ΔK(f(1)+f(N-1)).4.2.2Cluster-TopK算法的关键技术如果先得到完整的频繁图模式集合F,再使用传统的聚类算法(例如k-means算法)对F进行聚类,时间复杂性将是O(|F|2),这显然是不可行的.为了使Cluster-TopK算法高效可扩展,Cluster-Page8TopK算法要实现如下两个目标:(1)只扫描频繁子图挖掘算法输出的图模式一遍而能得到一个代表模式集合;(2)裁剪图模式空间中不产生(或极少产生)代表模式的那些分枝.下面两个小节分别介绍实现这两个目标的关键技术.4.2.2.1产生代表模式图3显示了图模式空间中的一个分枝,每个节点代表一个频繁子图.大部分频繁子图挖掘算法(gSpan[6],FFSM[7]等)都采用深度优先方式访问该空间中的每个节点.采用深度优先方式将会访问每个节点两次:(1)第1次是从父亲节点到当前节点的访问,例如在图3中从节点A到节点C的访问.(2)第2次是在完成了所有后裔的访问后再次回到当前节点,例如在图3中访问完节点E、F和G之后第2次访问节点C.目前的频繁子图挖掘算法是在第1次访问某个节点时输出它.因此,对图3中的节点输出顺序是…,A,B,C,E,F,G,D,….为方便描述,我们将δ-覆盖或Δ-覆盖都简称为覆盖.我们的目标就是产生一个代表模式集合能覆盖所有的频繁图模式.对一个给定的图模式,例如图3中的节点A,根据定义,能覆盖节点A的图模式必然是节点A的超图.因此,图3中的节点P,Q和A的后裔都有可能覆盖节点A.然而,我们在文献[17]中证明了,如果采用gSpan[6]的枚举框架,在一个节点第2次访问之后所有能覆盖该节点的图模式都已经被输出.例如:采用gSpan的枚举框架,节点Q就不能覆盖节点A.因此,我们调整图模式空间中节点的输出顺序:只有当一个节点第2次被访问时,我们才输出它.这样一来就可以保证当一个图模式P输出时,所有能覆盖P的图模式都已经被输出,便于我们为P选择对应的代表模式.对图3中的节点,调整之后的输出顺序将是…,B,E,F,G,C,D,A,….采用上面调整之后的输出顺序,我们依此处理每个输出的图模式.假定RS表示已经产生的部分代表模式集合.如果当前输出的图模式P能被RS中的某个代表模式覆盖,我们继续处理下一个输出的图模式.如果P不能被RS中的任何代表模式覆盖,我们创建一个新的能覆盖P的代表模式.为了使最终产生的代表模式数量尽可能少,在创建新的能覆盖P的代表模式时,我们采用了贪心策略,选择P的后裔中能覆盖P的最大图模式.因为这样选择的图模式有更大的可能性来覆盖以后输出的图模式.假设需要为图3中的节点A创新一个新的代表模式并且已知节点B,C,F,D都能覆盖A,根据贪心策略,我们应创建一个新的代表模式F,因为节点F大于节点B,C,D.4.2.2.2裁剪不产生代表模式的分枝如果图模式搜索空间中的某个分枝不产生新的代表模式(或者极少产生新的代表模式),完全遍历该分枝会使算法的性能急剧下降.本小节研究如何有效地裁剪这样的分枝.图4显示了整个图模式空间中以图模式g为根的一棵子树.通过向图模式g中增加一条新边,g可以被扩展成一系列新的图模式g◇e1,g◇e2,…,g◇en.以g◇e1为根的分枝含有g◇e1的超图.以g◇e2为根的分枝含有g◇e2的超图,但不含有g◇e1的超图.同样地,以g◇ei为根的分枝含有g◇ei的超图,但不含有任何g◇ej(j<i)的超图.对以g◇e2为根的分枝中任何图模式g◇e2◇x,很可能存在g◇e2◇x的超图g◇e1◇e2◇x,并且g◇e1◇e2◇x出现在g◇e1为根的分枝里.如果在图数据库中,g和g◇e1经常一起出现,g◇e2◇x和g◇e1◇e2◇x有很大的可能性也经常一起出现.这也就意味着g◇e2◇x和g◇e1◇e2◇x在支持度上非常接近.假设以g◇e1为根的分枝已经被遍历.那么,在遍历以g◇e2为根的分枝之前,已经产生了一个代表模式R能覆盖g◇e1◇e2◇x.因为R能覆盖g◇e1◇e2◇x,根据覆盖定义,R是g◇e1◇e2◇x的超图.如果g和g◇e1经常一起出现,则很可能g◇e2◇x和g◇e1◇e2◇x也经常一起出现,即,supp(g◇Page9e1)≈supp(g),supp(g◇e1◇e2◇x)≈supp(g◇e2◇x).那么,supp(g◇e2◇x)-supp(R)≈supp(g◇e1◇e2◇x)-supp(R).根据覆盖定义(δ-覆盖或者Δ-覆盖),R具有很大的可能性覆盖g◇e2◇x.因此,如果g和g◇e1经常一起出现,以g◇e2为根的分枝不产生(或者很少产生)新的代表模式,我们可以略过该分枝的遍历,以提高算法的效率.类似地,如果存在一条边ei使得g和g◇ei经常一起出现,我们就可以略过那些以g◇ej(j<i)为根的分枝,因为这些分枝几乎不产生新的代表模式.现在需要解决的一个问题是怎样度量g和g◇e是否经常一起出现?为此,我们根据g和g◇e的支持度定义了它们之间的一个距离函数Dsupp(g,g◇e)=1-supp(g◇e)的距离阈值参数ε(0<ε<1).如果Dsupp(g,g◇e)<ε,就可以认为g和g◇e经常一起出现.另一种更精确的方法是考虑g和g◇e在数据库中的嵌入数(所有子图同构次数之和).设embeeding(g)表示g在数据库中的嵌入次数.根据g和g◇e的嵌入数可以定义它们之间的另一个距离函数Demb(g,g◇e)=1-embedding(g◇e)embedding(g).如果Demb(g,g◇e)<ε,就可以认为g和g◇e经常一起出现.请注意,尽管使用距离阈值参数ε可以略过很多搜索分枝,我们也不可避免地会丢失一些代表模式.ε值越大,丢失的数量越多.然而,在实验中,我们发现使用一个很小的ε值(例如0.01),就可以得到98%以上的代表模式,同时使算法的性能提高近2个数量级.下面分析丢失少量代表模式对挖掘结果质量的影响.假设一个代表模式R丢失了,R覆盖的某个频繁图模式P能有另一个代表模式R1所覆盖,R1没有丢失.因为R能覆盖P,说明R和P在结构和支持度上很接近.同样,因为R1能覆盖P,说明R1和P在结构和支持度上很接近.因此,R和R1在结构和支持度上也会很接近.R对意义度量的贡献能由R1近似代替.因为我们的目标是选择K个图模式联合起来使某一意义度量最大化,丢失的少量代表模式对意义度量的贡献能由其它的代表模式近似地代替.因此,挖掘结果质量不受什么影响.实验中,我们发现不同的ε值对挖掘结果质量影响非常小,而ε值对改进算法效率却起着巨大的作用.4.2.3Cluster-TopK算法描述本节将4.2.2节中的关键技术集成到gSpan[6]的DFS编码搜索框架中,给出完整算法Cluster-TopK(见算法2).在gSpan中,每个频繁子图都对应一个最小DFS编码(边的序列).DFS编码搜索框架采用深度优先搜索方法挖掘频繁子图,只在最小DFS编码上进行最右扩展.因为gSpan是经典的频繁子图挖掘算法,故省略了它的细节描述.DFS编码、最右扩展等具体概念请见文献[6].算法2.Cluster-TopK.输入:图数据库D;最小支持度阈值min_sup;意义度输出:使M最大化的K个图模式1.扫描D得到所有频繁边;2.删除D中不频繁的边和结点;3.S1={所有频繁边的最小DFS编码};//S1中的DFS编码按DFS字典顺序排序4.GS=;//全局栈5.RS=;//存放代表模式的全局数据结构6.ForS1中的每个DFS编码s7.s.min_distance=1;//初始化最大值8.CallMiningReprePatterns(s,NULL,D,min_sup,δ(Δ),ε);地选择K个图模式使M最大化.9.类似于算法1中的步2)~步7),从RS中贪心增量算法Cluster-TopK如下工作:初始化时,先扫描数据库,得到频繁边集合,然后删除数据库中不频繁的边和结点.在这之后,对每个1-边频繁子图,算法调用子过程MiningReprePatterns进行深度优先搜索,发现所有代表模式.在得到所有代表模式的集合RS之后,类似于算法1,从RS中贪心增量地选择K个图模式.子过程(MiningReprePatterns).输入:DFS编码s;s的父亲编码p;图数据库D;最小支输出:代表模式集合RS1.Ifp!=NULLp.min_distance<ε,Then2.子过程结束;3.Ifs≠min(s),Then4.子过程结束;5.p.min_distance=min(p.min_distance,Dsupp(p,s));6.把s的最后一条边放入全局堆栈GS;7.对GS中的每个入口(频繁图模式)Q8.根据贪心策略,如果s大于Q的候选代表模式Page109.扫描D一次,发现s的所有频繁最右扩展孩子;10.对s的每个频繁最右扩展孩子s◇re11.(s◇re).min_distance=1;//初始化最大值12.CallMiningReprePatterns(s◇re,s,D,min_sup,13.IfGS[top].covered=False,Then14.在RS中找一个代表模式R能覆盖s;15.IfRS中不存在这样的代表模式R,Then16.用s的候选代表模式创建一个新的代表17.对GS中的每个入口(频繁图模式)Q18.IfQ能被Rnew覆盖,Then19.GS[Q].covered=True;20.对GS中的每个入口(频繁图模式)Q21.IfQ能被R覆盖,Then22.GS[Q].covered=True;23.弹出GS的栈顶GS[top].在子过程MiningReprePatterns的第1行,我们测试以当前模式s为根的分枝是否能被裁剪.我们使用p.min_distance表示p和它的已经被遍历的孩子之间的最小距离(Dsupp或Demb).如果p.min_distance<ε,说明存在p的一个已经被遍历的孩子c,c和p经常一起出现.根据4.2.2.2节描述的思想,因为当前代表模式集合RS能覆盖以c为根分枝中的所有频繁图模式,RS也就具有很大的可能性可以覆盖以s为根分枝中的所有频繁图模式.因此,我们可以跳过以s为根的分枝(裁剪该子树).在第3行,s≠min(s)判断s是否是当前模式的最小DFS编码.如果不是,可以跳过以s为根的分枝[6].在第5行,我们根据p和s的之间的距离Dsupp(p,s)(或Demb(p,s))更新p.min_distance.在第6行,我们将当前模式s的DFS编码中的最后一条边放入全局栈GS.GS跟踪图模式空间中从根节点到当前节点(当前模式s)的所有图模式.GS中的每个入口对应一个图模式Q,含有如下信息:(1)Q的DFS编码中的最后一条边;(2)Q的支持度;(3)覆盖标记covered;(4)Q的候选代表模式Q.R.在第7行,算法扫描GS中的每个图模式Q,测试当前模式s是否能覆盖Q.如果s能覆盖Q并且s大于Q的候选代表模式Q.R,根据4.2.2.1节中的贪心策略,用s替换Q.R.第9行,算法扫描数据库一次,发现当前模式s的所有频繁最右扩展孩子.第10行,对s的每个频繁最右扩展孩子s◇re,(s◇re).min_distance被初始化为最大值1,算法递归调用子过程Minin-gReprePatterns继续深度优先搜索.随着对s◇re的孩子的遍历,(s◇re).min_distance的值被逐渐减小.当(s◇re).min_distance小于距离阈值ε时,根据4.2.2.2节中的思想,s◇re的没被遍历的孩子分枝就可以被裁剪掉.第13行,在遍历当前模式s的所有后裔之后,算法测试s是否已经被覆盖.如果s没被覆盖,算法在第14行扫描代表模式集合RS,试图发现一个代表模式R能覆盖s.如果这样的代表R不存在,算法用s的候选代表模式创建一个新的代表模式Rnew,把Rnew放入RS中(第16行),并且算法扫描GS中的每个图模式Q(第17行),判断Rnew是否能覆盖Q(第18行).如果Rnew能覆盖Q,则标记Q被覆盖(第19行).如果在RS中发现了某个代表模式R能覆盖s,也扫描GS中的每个图模式Q(第20行),判断R是否能覆盖Q(第21行).如果R能覆盖Q,则标记Q被覆盖(第22行).5实验结果及分析我们进行了大量的实验来考察算法的挖掘结果质量、执行效率、可扩展性以及不同参数对算法性能和结果质量的影响.5.1实验环境我们从一个真实的化合物集合中导出了若干个图集合.该化合物集合可从下面的网址获得:http://dtp.nci.nih.gov/docs/3d_database/structural_information/structural_data.html.该化合物集合是用来测试化合物对艾滋病病毒(AIDS)的抑制作用,含有大约44000个化合物.根据实验结果每个化合物都被分为下面三类中的一类:CA(confirmedactive)、CM(confirmedmoderately)和CI(con-firmedinactive).其中,CA含有422个化合物,CM含有1081个化合物,CI含有剩余的化合物.我们根据类别,分别导出了3个图集合CA、CM和CI.其中,CI图集合是从所有CI化合物中随机选择5000个化合物.本文算法使用C++语言实现,用带有-O3优化选项的g++编译.用于实验的计算机具有PIV3.0GHzCPU和1GB内存,运行RedHatLinux8.0操作系统.Cluster-TopK算法需要两个额外的参数:聚类质量参数δ和距离阈值ε.在下面的实验中,若无特别说明,δ取0.1,ε取0.01.参数δ和ε对算法的影响在5.4节描述.Page115.2比较挖掘结果的质量本节比较算法Greedy-TopK和Cluster-TopK之间的挖掘结果质量,下节比较算法Greedy-TopK和Cluster-TopK之间的效率.表1显示了在不同的数据集合上,固定最小支持度min_sup等于10%,变化不同的K值,算法Greedy-TopK和Cluster-TopK选择的K个图模式而得到的联合熵大小(MES问题).表2显示了在相同条件下,算法选择的K个图模式而得到的信息增益大小(MIGS问题).表1比较Greedy-Top犓和Cluster-Top犓计算的联合熵(犿犻狀_狊狌狆=10%)K54.53327106.56251157.31281207.67565表2比较Greedy-Top犓和Cluster-Top犓计算的信息增益(犿犻狀_狊狌狆=10%)K对CA(参照CM)的信息增益50.1930260.187017100.3169930.304763150.4715300.453959200.5848030.557849为了进一步验证不同算法的结果质量,我们使用算法Greedy-TopK和Cluster-TopK最大化信息增益产生的Top-K模式进行分类实验,比较分类性能.我们构造两个分类任务:(1)对CA类和CM类中的化合物进行分类;(2)对CA类和CI类中的化合物进行分类.带有缺省参数的LIBSVM①被用作分类模型.分类准确率使用5次交叉验证进行评价.ROC曲线下的面积(AUC)被用来度量分类性能.AUC越大,表示分类性能越好.此外,我们也抽取了传统的Top-K模式(根据信息增益对图所有模式进行排序,从中选择前K个信息增益最大的图模式)进行比较.Trad-TopK表表3不同Top-犓模式构造的分类器的分类性能(AUC)(犿犻狀_狊狌狆=10%)K5101520最后,我们从结构上比较一下不同算法产生的Top-K模式.图5、6和7分别显示了算法Trad-TopK、Greedy-TopK和Cluster-TopK从CA(参照CM)数据集上挖掘的Top-5模式.可以看出,Trad-可以看出,算法Greedy-TopK和Cluster-TopK挖掘出的Top-K模式在质量上非常接近.尽管Clus-ter-TopK算法没有理论上的近似比保证,而它输出的意义度量值(联合熵和信息增益)与Greedy-TopK算法相比最多相差1%.有时侯,Cluster-TopK算法得到的结果还稍微优于Greedy-TopK算法.我们也将最小支持度的取值在5%~30%之间变化,用以比较结果差异.结果显示,Greedy-TopK和Cluster-TopK给出的Top-K模式在质量上仍然非常接近.示抽取传统Top-K模式的算法.我们使用算法Trad-TopK、Greedy-TopK和Cluster-TopK分别从正类和反类中抽取K个图模式,建立分类模型.表3显示了K变化时,不同的Top-K模式构造的分类器的分类性能(AUC).可以看出,Greedy-TopK和Cluster-TopK产生的Top-K模式在分类性能上远远优于传统的Top-K模式.因为传统的Top-K模式只考虑优化单一模式的意义度量,而没有考虑一个模式集合中所有模式的联合意义.此外,Greedy-TopK和Cluster-TopK产生的Top-K模式在分类性能方面也非常接近,再次说明Greedy-TopK和Cluster-TopK的挖掘结果质量差异很小.TopK挖掘的Top-5图模式在结构上有很大重叠,因为它只考虑单一模式的意义,使得挖掘出来的图①ChangC,LinC.LIBSVM:AlibraryforsupportvectorPage12模式存在很大的相关性.Greedy-TopK和Cluster-TopK考虑了模式的联合意义,挖掘的Top-5图模图5Trad-TopK算法产生的Top-5模式(MIGS,min_sup=10%)图6Greedy-TopK算法产生的Top-5模式(MIGS,min_sup=10%)图7Cluster-TopK算法产生的Top-5模式(MIGS,min_sup=10%)5.3比较算法的效率本节比较算法Greedy-TopK和Cluster-TopK的执行效率.在Greedy-TopK的第1步,既可以使用gSpan[6]挖掘频繁图模式,又可以使用CloseG-raph[9]挖掘频繁闭图模式.我们形成了Greedy-TopK的两个版本,gSpan+Greedy-TopK和图8支持度的变化对算法执行时间的影响(MES,K=10)图9支持度的变化对算法执行时间的影响(MIGS,K=10)式在结构上彼此之间有很大的差异.在实际应用中,这恰恰是用户想要挖掘的模式类型.CloseGraph+Greedy-TopK.注意:在与Cluster-TopK比较时,Greedy-TopK使用了4.1.2节中的裁剪技术以获得最高的执行效率.图8显示了当最小支持度变化时,求解MES问题不同算法所用的时间.图9显示了当最小支持度变化时,求解MIGS问题不同算法所用的时间.如果一Page13个算法不能在一个小时内完成,我们就终止算法.因此,图中gSpan+Greedy-TopK的结果是不完整的.从图8和图9可以看出,在执行效率方面,算法Cluster-TopK极大地优于Greedy-TopK.例如:在CA数据集合上,Cluster-TopK比gSpan+Greedy-TopK快2个数量级,比CloseGrpah+Greedy-TopK快1个数量级.而且,支持度越低,Cluster-TopK和Greedy-TopK的执行效率差异越大.Cluster-TopK的高效率来源于两个方面:(1)在挖掘代表模式时,因为Cluster-TopK裁剪掉了很多不产生(或少产生)代表模式的分枝,与闭图模式挖掘算法CloseGraph相比,Cluster-TopK遍历了图模式空间中更少的结点,获得了更高的执行效率.(2)在贪心选择时,因为Cluster-TopK产生的代表图模式数量比CloseGraph产生的闭图模式少很多,因此5.4不同参数对算法的影响本节评价算法Cluster-TopK中所使用的两个参数:聚类质量参数δ和距离阈值ε对算法的挖掘结果和运行时间的影响.由于聚类质量参数δ(相对值)和Δ(绝对值)具有完全相同的作用,我们只给出对δ的评价结果.我们先评价聚类质量参数δ对算法的影响,固定距离阈值ε=0.01,变化δ值.Cluster-TopK算法的运行时间由两部分构成:聚类时间和贪心选择时间.我们使用TCluster,TGreedy和TTotal分别表示算法的聚类时间、贪心选择时间和总的时间.Rep表示Cluster-TopK聚类之后产生的代表模式数量,M表图10支持度变化时,算法Greedy-TopK的不同变体所需的执行时间(CA数据集,K=10)表4聚类质量参数δ对算法Cluster-Top犓的挖掘结果和运行时间的影响δRep0.059766.574835.9111.6617.570.2974555.9245.5151.430.17606.592195.909.1415.040.3031555.9134.8540.760.156316.491195.897.7813.70.3014895.9029.5435.440.25516.49125.917.0412.950.3012885.8926.5532.440.254896.44645.906.3312.230.2849385.9123.8329.740.34536.459055.925.8511.770.2910655.9021.8127.71Cluster-TopK贪心选择的执行效率也比Greedy-TopK贪心选择的执行效率快很多.下面评价算法Greedy-TopK中使用的裁剪技术(见4.1.2节)的有效性.为此,我们形成了Greedy-TopK算法的4个变体.Greedy-TopK-1表示不使用裁剪技术的gSpan+Greedy-TopK,Greedy-TopK-2表示使用裁剪技术的gSpan+Greedy-TopK,Greedy-TopK-3表示不使用裁剪技术的CloseGraph+Greedy-TopK,Greedy-TopK-4表示使用裁剪技术的CloseGraph+Greedy-TopK.图10显示了当最小支持度变化时,算法Greedy-TopK的不同变体所需的执行时间.可以看出,4.1.2节的裁剪技术能有效地改善算法Greedy-TopK的效率.支持度越低,改善越明显.示最后输出的度量值.表4显示了当δ值变化时,算法Cluster-TopK的结果质量和运行时间.可以看出,当δ增加时,聚类时间TCluster不受影响.然而,随着δ值的增加,输出的代表模式数量逐渐减少,因此,贪心选择时间TGreedy逐渐减小,总的时间也逐渐减小.一个有趣的现象是δ值对最后度量值M的影响微乎其微.在4.2.1.1节,我们分析了Cluster-TopK的结果与Greedy-TopK的结果在理论上的最大可能差异.而实际中的差异比这种理论上的最大差异少的多得多.这再次说明了算法Cluster-TopK采用的聚类策略非常适用于本文的研究问题.Page14下面评价距离阈值ε对算法的影响,固定聚类质量参数δ=0.1,变化ε的值.表5显示了当ε值变化时,算法Cluster-TopK的结果质量和运行时间.可以看出,当ε增加时,聚类时间TCluster明显减少.这是因为大的ε值使得图模式空间中更多的分枝被裁剪掉,从而使得聚类时间TCluster被大大缩减.当ε增加时,由于输出的代表模式数量也在逐渐减少,因此贪心选择时间TGreedy和总的时间也逐渐减小.与聚类质量参数δ相比,距离阈值ε对结果质量的影表5距离阈值ε对算法Cluster-Top犓的挖掘结果和运行时间的影响(min_sup=5%)代表模式ε0.017606.592195.99.3115.210.3031555.934.7140.610.027316.592195.369.0114.370.3031555.3633.7839.140.036976.592194.498.1412.630.3031554.4931.235.690.046756.592193.937.9811.910.3031553.9330.1334.060.056446.592193.137.0810.210.3031553.1227.0530.170.066336.592192.936.959.880.3031552.9326.7129.640.076266.592192.90.086166.592192.716.789.490.3031552.7126.1528.860.096126.592192.596.729.310.3005212.5925.8928.480.105746.592192.446.458.890.3005212.4424.5326.970.154656.598961.835.146.970.2920421.8319.6321.460.203556.522531.314.235.540.2847861.3116.3717.696相关工作很多频繁子图挖掘算法已经被提出,大致可分为两类.第1类算法(例如AGM[3]和FSG[4])根据Apriori性质采用逐级搜索策略来枚举所有频繁子图.第2类算法(例如Mofa[5]、gSpan[6]、FFSM[7]和GASTON[8])采用深度优先搜索策略来枚举所有频繁子图.通常第2类算法比第1类算法有更好的内存利用率,因此具有更高的挖掘效率.挖掘频繁子图经常会产生指数级数量的图模式.为解决频繁子图挖掘时图模式数量“爆炸”问题,研究人员已经提出了两类主要方法:(1)挖掘频繁闭图模式[9];(2)挖掘频繁最大图模式[10-11].第1类方法仍然会输出大量的图模式.第2类方法会丢失一些重要的图模式.最近,我们提出了一个折衷方法[17],从图数据库中挖掘代表模式集合.其它与图模式挖掘有关的研究还包括图模式并行挖掘算法[18]、挖掘频繁树模式[19]以及挖掘图产生器[20]等.上面这些与图模式有关的挖掘方法除了支持度,都没有考虑其它的意义度量.最近,文献[21]提响更小.例如:当ε从0.01~0.1变化时,最后的度量值几乎不发生变化.在前面的实验中我们只使用了小的ε值0.01,就能使算法Cluster-TopK比CloseGraph+Greedy-TopK快1个数量级别.如果使用更大的ε值(例如:0.1)能使算法Cluster-TopK更快,而且对结果质量几乎没有影响.这恰恰说明了算法Cluster-TopK所采用的裁剪策略能在保证结果质量的前提下,极大地改善算法的效率.出了一个框架,根据用户给定的意义度量,挖掘意义度量值最大的一个图模式.然而,该框架只能输出一个图模式.在实际应用中,用户经常需要多个图模式来管理和分析图数据.传统的Top-K方法虽然可以输出多个图模式,但没有考虑模式之间的相关性,会输出结构上类似的图模式,无法满足用户对图模式集合多样化的需求.与上述工作不同,本文研究如何挖掘Top-K图模式,联合起来使用户给定的意义度量最大化.此外,本文提出的Cluster-TopK算法使用了一个新的挖掘代表图模式的算法.文献[17]中挖掘代表图模式最有效的算法RP-GD需要枚举所有频繁闭图模式,因此RP-GD挖掘效率明显低于CloseGraph算法[9].本文中挖掘代表图模式的新算法比CloseGraph快一个数量级,因此也能比RP-GD快一个数量级.而且,文献[17]挖掘代表图模式的目的只是用来近似概括频繁图模式,本文挖掘代表图模式的目的则是用来选择使联合意义度量最大的Top-K图模式.7结论本文提出了基于联合意义度量的Top-K图模Page15式挖掘问题,并给出两个高效算法Greedy-TopK和Cluster-TopK.实验结果显示本文提出的Top-K挖掘优于传统的Top-K挖掘.Cluster-TopK比Greedy-TopK快至少一个数量级.而且,Cluster-TopK的挖掘结果非常类似于Greedy-TopK的挖掘结果.本文提出的算法是一种通用算法,也适用于其它的联合意义度量和模式类型(例如:项集模式、序列模式、树模式等).
