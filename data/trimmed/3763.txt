Page1一种基于高斯隐变量模型的分类算法王秀美高新波李洁(西安电子科技大学电子工程学院西安710071)摘要高斯过程隐变量模型是近年来新兴的无监督降维方法,它可以找到高维数据的低维流形结构.但是由于高斯过程隐变量模型是无监督的概率降维方法,所以当数据集中的样本有类别标记信息时,高斯过程隐变量模型不能利用这些监督信息,实现分类的任务.为了使高斯过程隐变量模型可以处理分类任务,文中提出了一种监督的高斯过程隐变量模型分类模型.通过最大化后验似然的方法确定观测数据在隐空间的坐标,同时可以完成分类任务.实验结果证明了该模型可以有效地用于分类.关键词高斯过程;监督学习;降维;线性判别分析1引言“维数灾难”是模式识别和机器学习的常见问题,在人脸识别、动态跟踪、语音识别等实际问题中,高维数据之间往往存在着大量的冗余,如何消除冗余,寻找数据间的内在联系,从而减少数据处理过程中的计算量,是当前降维技术研究的重点.经典的降维方法多为线性方法,即寻找高维数据的某个线性子空间,将高维数据投影到这个子空间中,再进行识别或分析,大大降低计算量.主成分分析(PrincipleComponentAnalysis,PCA)[1-2]和线Page2性判别分析(LinearDiscriminantAnalysis,LDA)[3]是最具代表性的两种线性子空间分析方法.PCA是一种无监督的降维方法,它的主要思想是用较少的综合变量来代替原来较多的变量,同时要求这几个综合变量互不相关,并尽可能多地表示原来数据的能量.它是将多指标化为少数几个综合指标的一种统计分析方法;LDA是目前常用的有监督的线性降维方法,它利用已知的监督信息进行数据降维,降维过程最大化样本类间散度,同时最小化类内散度.由PCA衍生并推广出的概率主成分分析(ProbabilisticPCA,PPCA)[4]是一种从高斯隐变量模型扩展出来的概率方法,通过对高维数据的联合似然最大化,求得投影方向.由于PCA、PPCA和LDA都是线性降维方法,虽然易于实现,但是对一些结构复杂的数据却无能为力,这是由其本身的线性特性造成的.核主成分分析(KernelPCA,KPCA)[5]和核判别分析(KernelLDA,KDA)[6-7]分别是对PCA和LDA的非线性推广.由于核方法是非线性,多数是不可逆的,即只给出了如何从观测空间到低维隐空间的映射,但对于如何建立从低维到高维数据的映射,没有给出解决方法.高斯过程隐变量模型(GP-LVM)是一种非线性降维技术[8-9],是PPCA的对偶形式.GP-LVM克服了线性降维方法的局限,并且利用高斯过程[10]建立了从低维隐空间到高维空间的映射关系.通过最大化观测数据的联合密度,优化出高维数据在隐变量空间中的坐标位置.然而,GP-LVM所用到的高斯过程映射是在样本的每一维上单独进行,是无监督的,当数据中有监督信息,如类别标记可以利用时,GP-LVM不能有效地利用类标,根据样本的类别结构信息,从而建立有效的分类模型.为了利用观测数据的监督信息,文献[11]中提出了一种利用类标信息的方法,用LDA的能量函数作为隐空间变量的先验知识,最大化隐变量的后验知识,得到降维后的样本.此方法虽然可以利用监督信息,使得降维后样本在低维空间保持样本类间散度大、类内散度小的性质,但是它没有从根本上解决如何利用类别信息进行分类的问题.本文提出了一种监督的GP-LVM(SGP-LVM)分类模型,该模型基于隐变量模型(Latentvariablemodels)建立,利用了隐变量空间条件独立(Conditionalindependent)的性质[12].与文献[11]提出的方法不同,SGP-LVM既考虑观测样本的性质,也考虑已知的类标信息,分别建立了从隐空间到观测样本和类别信息集合的映射,前者达到了降维的目的,后者则实现了分类的任务.在本文的后半部分,在各种数据上的实验结果证明了本文方法的有效性.本文第2节简单介绍PPCA和GP-LVM的相关工作;第3节给出如何利用隐变量过程的条件独立性质建立监督的GP-LVM分类算法,包括如何进行分类和如何确定超参数和降维后隐变量的位置;第4节给出实验结果以证明分类算法的有效性;最后进行总结与讨论.2预备知识2.1概率主成分分析PPCA是PCA方法的概率扩展[2],它建立了一种从低维空间(隐变量空间)到高维空间的线性映射,若犡=[狓1,狓2,…,狓N]表示需要降维的高维数据,犣=[狕1,狕2,…,狕N]表示降维后的低维数据,并假设噪声ηn∈RD符合独立高斯分布,ηn~N(0,β-1犐),β-1为噪声方差.从低维空间到观测空间的映射可以表示为其中线性映射由犠∈RD×d确定,则观测数据点的似然概率为假设隐变量空间中的点是独立同分布的:即通过对隐变量空间中的点积分可以推导出边缘似然,P(狓n|犠,β)=∫p(狓n|狕n,犠,β-1犐)p(狕n)d狕n和观测数据的联合似然,用最大化似然的方法得到投影矩阵犠.求似然最大化的方法很多,其中EM[13]算法是比较常用的一种.EM算法是求参数极大似然估计的一种简单实用的学习算法,对应于本文中提到的PPCA,它包括两个步骤:E-step.计算数据的对数似然函数期望.似然函数为Page3若记对数似然函数为lnp(犡;犠),对数似然函数可以表示为则E-step求lnp(犡;犠)的期望值E{lnp(犡;犠)}.E{lnp(犡;犠)}={2lnβ+1D-∑Nn=11〈狕n〉T犠T(狓n-μ)+1β在上式中μ表示观测变量犡的均值并且有其中犕=犠T犠+β犐.M-step.关于投影矩阵犠最大化E{lnp(犡;犠)},即关于E{lnp(犡;犠)}对犠求导,得到最优的犠值,珦犠=∑N通过交替使用E-step和M-step直至收敛,收敛性判断可以用相邻两次的迭代得到的E{lnp(犡;犠)}的差值判断.E{lnp(犡;犠)}t+1-E{lnp(犡;犠)}tε,则E{lnp(犡;犠)}达到一个极值点,从而得到投影矩阵犠.详细步骤请参看文献[4].2.2高斯过程隐变量模型条件独立是隐变量模型的重要性质,即在隐变量给定的时候,观测变量的各维之间是独立的[12].如图1所示,给出隐变量狕时,观测变量狓各维之间条件独立.GP-LVM正是基于这个性质建立了从隐变量到观测变量的映射关系.GP-LVM的模型是基于图1建立,从所有的隐变量到观测变量的每一维建立映射,给每个映射一个高斯过程先验,则建立了一个非线性隐变量模型.这与PPCA不同,PPCA是假设各个样本独立同分布(i.i.d),假设降维后的样本服从高斯分布而建立模型.假设各维度上的映射fd独立同分布,服从高斯过程,则观测数据d的似然可表示为p(狓:,d|犣,β)=∫p(狓:,d|狕n,fd,β-1犐)p(fd)dfd由于各维之间独立,则观测数据的似然可以表示为数据各维度似然的乘积P(犡|犣,β)=∏D上式可以表示为矩阵的形式:P(犡|犣,β)=1其中,犓是协方差函数矩阵,或者称为核函数矩阵.若用线性的核函数,则可以定义为犓=犡犡T+β-1犐.若选用非线性核函数则可以定义为k(狕i,狕j)=θrbfexp-γk(狕i,狕j)为矩阵犓的第i行第j列对应的元素;δij为Kroneckerdelta函数.这时,从隐空间到高维空间的映射是一个非线性映射的高斯过程,而且对于观测数据,每一维是独立的.联合似然式(9)可进一步简化为P(犡|犣,β)=13监督的GP-LVM分类算法GP-LVM算法可以有效地找到高维数据(观测数据)在低维空间中上的流形,但是由于它是一种无监督的机器学习方法,所以当我们的观测数据中有类标信息时,它不能利用这些类标信息,实现分类目标.针对这个问题,我们将监督信息加入GP-LVM算法中,建立一种有监督的GP-LVM分类模型.这种有监督的模型是建立在隐变量模型的条件独立性质之上的,我们参考文献[14-15]中的思路建立分类模型.下面,我们将给出监督的GP-LVM模型,我们Page4用犔=[犾1,…,犾N]T表示输出矩阵,其中犾i∈RC,或者表示分类中观测数据的类标,例如二值分类时,犾i∈{+1,-1}.我们用(犡,犢)表示观测数据,则可以建立以下映射关系:式(18)中,函数f1表示从隐空间(低维空间)犣到高维空间犡的映射关系.若f1为高斯过程,则可以用Θ1表示映射的超参数.同样,函数f2表示从犣到犔的映射,映射的超参数用Θ2表示.模型中的噪声分别为ε狓和ε犔,假设为零均值的高斯分布,即ε狓~N(0,σ2狓犐),ε犔~N(0,σ2上面建立了含噪声的概率模型,接下来推导观测数据(犡,犔)的似然p(犡,犔|犣).由于在隐变量模型中,观测数据(犡,犢)的各维之间独立,即p(狓,犾|狕)=p(狓|狕)p(犾|狕),所以可以得到观测数据的联合似然为此时,我们利用最大似然原则就可以优化得到隐变量犣的值.先对式(15)求对数可以得到犔lnp(犡,犔|犣)=lnp(犡|犣)+lnp(犔|犣)(21)可以看到等式第1项为lnp(犡|犣)=-DN即为模型GP-LVM中的似然,是属于原来无监督的部分.式(16)右边的第2项为监督项,如果我们给映射f2高斯过程先验,则当犾i∈RC时,似然可以表示成lnp(犔|犣)=-N因此,用尺度共轭梯度方法[16]最大化观测数据的联合似然,可以得到隐变量在低维空间中的位置和映射的参数.首先用对数似然对隐变量犣求梯度,犣=lnp(犡|犣)犔其中,根据选用的核函数形式不同,犓同的值.例如若选用线性的核函数,则犓其次用对数似然对参数θ求梯度对隐变量犣和参数θ交替优化直至收敛.上面给出的是处理分类的情况,其推导是在给出类标犔=[犾1,…,犾N]T∈RN×C的基础上得出的.若给出的监督信息是回归的输出值,则可以直接将回归值替代类标用于给出的模型,该模型可以直接用于回归.4实验结果与分析本文算法在很多数据上进行了实验,以验证算法的有效性.实验分为两部分:第1部分验证了本算法作为分类器的有效性,第2部分验证了本算法作为回归算法时的有效性.第1部分又分为两组:第1组给出了本算法对USPS手写数字数据降维的结果,第2组实验给出了本算法在人脸数据上的识别结果.4.1数据降到2D空间的结果对比实验选取了USPS数据库中的手写数字(HandwrittenDigits)[8]识别的“3”和“5”,选取准则是这两个数字相对其它数字较难识别.USPS的手写数字数据是由0~9的10个数字组成,即数据共分为10类,其中每一类数据由大概500个样本组成,每个样本的维数是256.实验是在数字“3”和“5”上进行,对每类样本选取400个,共有800个样本,分别用GP-LVM算法和监督的GP-LVM算法降到2维,图2给出两种方法降维的结果,其中“3”的样本降维后用十字表示,“5”的样本降维后用圆圈表示.图2表示我们将USPS数据用GP-LVM和SGP-LVM降至2维的结果,来比较证明我们提出算法的有效性.从左边的图2(a)看出,图像的上方圆点较集中的地方有很多“+”也在其中,这表明GP-LVM的结果不能有效地将两类数据分开,两种Page5图2将USPS数据降为2维空间结果显示类型存在交叠.图2(b)表示的是SGP-LVM降维的结果,从图中可以看出,该算法不仅可以将两类数据有效分开,而且类间距较大,各自集中的区域很少有不同类型的点出现,这表示当用SGP-LVM将该类数据降至2维时,可以有效地将两类数据分开.4.2SGP-LVM、GP-LVM和LDA分类结果比较在本小节中我们对SGP-LVM、GP-LVM和LDA的分类效果进行验证,在下面的实验中,我们选用YaleB(TheextendedYaleFaceDatabaseB)和ORL(OlivettiResearchLaboratory)人脸数据[15].SGP-LVM可以对数据直接分类,而对于GP-LVM和LDA这两种方法,我们先用它们降维,然后用最紧邻(1-NN)进行分类.对于LDA,我们对数据先用PCA处理,再用LDA+KNN分类,这样做可以避免LDA的欠采样问题.YaleB数据包含38个人的2414幅人脸图像,每个人有64幅接近正面的不同光照下的人脸图像.每幅图片被裁减成32×32大小,即每个样本有1024维的特征来刻画.在实验中,对每个人随机选择30幅作为训练图像,余下的38幅图用作测试.ORL数据库是由40个人的人脸图像组成,每个人有10幅图像,分别在不同时刻、光照、表情(睁眼或者闭眼,微笑或者不笑)和不同面部细节(戴眼镜或者不戴眼镜)等情况下得到的.所有图像都是32×32像素大小.在分类实验中,我们随机选取每个人的5幅图像用作训练,剩余5幅用作测试.基于上面两组数据的实验结果如表1所示,最左边的列中的d值表示样本降维后的维数,余下3列,从左至右分别表示LDA、GP-LVM和SGP-LVM在不同维数上的分类结果.对YaleB数据,我们分别降至5、15、30和37维比较分类结果;对ORL数据,则降维至5、10、15和30维来比较3个方法的分类结果.YaleBd=50.5205+0.00850.6813+0.01900.5373+0.0240d=150.2332+0.01140.5788+0.03260.4831+0.0113d=300.1495+0.00640.1341+0.00650.1218+0.0525d=370.1272+0.00980.1221+0.00900.1165+0.0667ORLLDA+KNNGP-LVM+KNNSGP-LVMd=50.3500+0.01840.3540+0.04290.3440+0.0363d=100.1950+0.01370.1740+0.02770.1770+0.0332d=150.1720+0.02310.1570+0.02710.1453+0.0266d=300.1250+0.02620.1240+0.03230.1174+0.0469从以上两组实验结果可以看出,SGP-LVM和GP-LVM能得到比LDA较好的分类结果.对于第1组数据YaleB,虽然在维数降至d=5和d=15时LDA得到的分类效果比SGP-LVM和GP-LVM都要好,但随着维数的增加,SGP-LVM的分类效果明显得到提高.特别是对ORL数据,与LDA和GP-LVM相比较,SGP-LVM的分类效果更加明显.尤其当数据降维的维数高于15以后,SGP-LVM就能得到非常好的分类结果.这主要是因为相比于LDA和GP-LVM,SGP-LVM非线性和监督信息的利用取得了很好的效果.GP-LVM虽然可以对高维数据降维得到低维流形结构,但是这种降维流形不能区别类别信息.SGP-LVM正是对其进行了改进,使得GP-LVM这种有效的降维方法可以用于分类,以上的实验也证明了本文方法的有效性.4.3回归数据处理结果前两组实验验证了本文提出的算法在处理分类数据时的有效性,下面的实验则说明该算法在处理回归问题时同样可以取得较好的效果,实验数据Housing和ConcreteCompressiveStrength来自UCI数据库.Housing是由506个样本组成,每个样本有13个特征来描述,即样本的维数为13.第2个Page6ConcreteCompressiveStrength数据包含1030个样本,特征维数为8.这两个数据都要回归到1维输出上,输出数据与输入的高维特征间有很强的非线性.利用本文提出的算法可以找到高维特征潜在的流形结构,然后建立低维的流形到输出值之间的关系.在实验中我们也比较了经典非线性回归模型(高斯过程回归,GPR)[17].由于本文提出的方法是先试图消除次要或者无关因素对数据的干扰,找到数据潜在的流形结构,所以需要先高维数据降维,然后从低维空间再映射到图3本文算法与GPR回归性能比较5结论本文提出了一种基于高斯过程隐变量模型的非线性分类方法,该方法利用先验的类别信息,得到隐变量的后验概率.然后通过后验概率,得到高维数据点在隐空间的位置,同时优化出模型中映射的超参数的值,建立分类模型.同时该方法仍然保持了高斯过程隐变量模型的降维优势,即可以有效地找到高维数据的低维流形结构.实验结果证明了本文方法的有效性.由于我们利用的类别信息得到隐变量的后验,所以这种方法具有开放性,我们可以利用其它的监督信息替代这里的类别信息,例如回归模型的输出值;而若我们已知一些半监督的信息,那么该方法可以很容易扩展到半监督模型.
