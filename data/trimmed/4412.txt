Page1云存储中的数据完整性证明研究及进展谭霜贾焰韩伟红(国防科学技术大学计算机学院长沙410073)摘要随着云存储模式的出现,越来越多的用户选择将应用和数据移植到云中,但他们在本地可能并没有保存任何数据副本,无法确保存储在云中的数据是完整的.如何确保云存储环境下用户数据的完整性,成为近来学术界研究的一个热点.数据完整性证明(ProvableDataIntegrity,PDI)被认为是解决这一问题的重要手段,该文对此进行了综述.首先,给出了数据完整性证明机制的协议框架,分析了云存储环境下数据完整性证明所具备的特征;其次,对各种数据完整性证明机制加以分类,在此分类基础上,介绍了各种典型的数据完整性验证机制并进行了对比;最后,指出了云存储中数据完整性验证面临的挑战及发展趋势.关键词云存储;数据完整性证明;数据持有性证明;数据可恢复性证明;大数据;云计算1引言云计算是继对等计算、网格计算、效用计算、分布式计算后又一新型的计算模式,其一出现便成为Page2上.另外,强大的计算和存储能力也使得用户更愿意依托云来处理各种复杂的任务.当用户选择将大量应用和数据部署到云计算平台中时,云计算系统也相应地变为云存储系统.云存储系统可以使用户以较低廉的价格获得海量的存储能力,但高度集中的计算资源使云存储面临着严竣的安全挑战.据Gartner2009年的调查结果显示,因担心云数据隐私被侵犯,70%受访企业的CEO拒绝大规模的采用云计算的计算模式①.而在最近几年里,各大云运营商各自暴漏的安全存储问题,引起了人们的广大关注与担忧.如2011年3月,谷歌Gmail邮箱出现故障,而这一故障造成大约15万用户的数据丢失②.2012年8月,国内云提供商盛大云因机房一台物理服务器磁盘发生故障,导致客户的部分数据丢失③.由此可见,云中的数据安全存储已经阻碍了云计算在IT领域得到大规模的使用[3-4].当用户采用云存储模式后,其存储在云中的数据可能遭到其他用户或云计算提供商的窥视、修改或损坏.通常,数据的机密性可通过数据加密、匿名机制等机制来确保[5-7].但当采用云存储后,用户可能并没有在本地保存任何数据副本,从而无法保证云中的数据是完整的.用户将数据存储到云中时,可能面临以下三种损坏数据的行为:(1)软件失效或硬件损坏导致数据丢失,这种失效属于概率性事件;(2)存储在云中的数据可能遭到其他用户的恶意损坏,文献[8]以AmazonEC2存储服务为例,指出恶意的用户可以对云中同一宿主机上的其他虚拟机发起攻击,损坏其他用户的数据;(3)云服务提供商可能并没有遵守服务等级协议(SLAs),其为了经济利益,擅自删除一些用户不常访问的数据,或采取离线存储方式.总而言之,在现有的云体系框架下,云中数据的完整性无法得到有效的确保[9].数据完整性证明机制能及时地识别云中损坏数据的行为[10].本文通过对现有的多种数据完整性检测机制进行归类分析,探讨了适合云存储的完整性验证机制,并对云存储领域的数据完整性验证机制的发展趋势进行了展望.2云存储环境下数据完整性证明本节将对云存储环境下的数据模型和数据完整性验证机制的基本框架进行简单介绍.2.1数据存储模型云存储环境下数据完整性证明所采用数据存储模型如图1所示,由用户(Customer)、云服务提供商(CloudServerProvider,CSP)及可信第三方审计(TrustedPartyAuditor,TPA)组成[9].其中,用户(Customer)可以自定义地定制云存储服务;云服务提供商(CloudServerProvider,CSP)为用户提供存储或计算服务,具有强的计算能力和存储能力;第三方审计(ThirdPartyAuditor,TPA),具有用户所没有的审计经验和能力,可以替代用户对云中存储的数据执行审计任务,减轻用户在验证阶段的计算负担.由于接入云的设备受计算资源的限制,用户不可能将大量的时间和精力花费在对远程节点的数据完整性检测上.通常,云用户将完整性验证任务移交给经验丰富的第三方来完成.采用第三方验证时,验证方只需要掌握少量的公开信息即可完成完整性验证任务.2.2构建框架定义1.数据完整性证明机制由Setup和Challenge两个阶段组成,通过采用抽样的策略对存储在云中的数据文件发起完整性验证.具体实现由6个多项式时间内算法组成,如下所示.(1)密钥生成算法:KeyGen(1k)→(pk,sk).由用户在本地执行.k为安全参数,返回一个匹配的公钥、私钥对(pk,sk).(2)数据块标签生成算法:TagBlock(sk,F)→Φ.TagBlock(·)算法由用户执行,为每个文件生成同态签名标签集合Φ,作为认证的元数据.该算法输①②③Page3入参数包括私钥sk和数据文件F,返回认证的元数据Φ.(3)证据生成算法:GenProof(pk,F,Φ,chal)→P.该算法由服务器运行,生成完整性证据P.输入参数包括公钥pk、文件F、挑战请求chal和认证元数据集合Φ.返回该次请求的完整性证据P.(4)证据检测算法:CheckProof(pk,chal,P)→(“true”,“false”).由用户或可信第三方TPA运行,对服务器返回的证据P进行判断.输入参数为公钥pk,挑战请求chal及P返回验证成功或失败.当存储在云中文件需要支持动态操作时,还需要以下两个算法支持:(5)更新执行算法:ExecUpdate(F,Φ,Update)→(F,Φ,Vupdate).算法由服务器运行,将文件F作为输入,相应标签Φ及数据请求操作Update,输出一个更新文件F和更新标签集合Φ,及相对应地更新证据Vupdate.(6)更新验证算法:VerifyUpdate(pk,Update,Pupdate)→(“true”,“false”).该算法由用户执行,返回更新操作成功或失败.数据完整性验证机制在具体实施过程中可以分为两个阶段组成:Setup阶段和Challenge阶段:Setup阶段:初始化阶段.首先,用户运行Key-Gen(·)生成密钥对(pk,sk);然后,对存储的文件进行分块F=(m1,m2,…,mn);之后,运行TagBlock(·)为文件中每一个数据块生成同态标签集合Φ={σi}1in;最后,将数据文件F和签名集合Φ同时存入云中,删除的本地{F,Φ}.Challenge阶段:验证请求阶段.用户或TPA作为验证者,周期性的发起完整性验证.从文件F分块索引集合[1,n]中随机挑取c个块索引{s1,s2,…,sc},并且为每一个索引si选取一个随机数vi,将两者组合一起生成挑战请求chal={i,vi}s1isc发送给服务器.图2数据完整性证明机制框架分类数据持有性证明PDP机制最先运用于网格计算和P2P网络中.用户为了获取更强存储能力,选择将数据备份到远程节点上.Deswarte等人[11]最先服务器作为证明者,根据存储在其服务器上的数据文件{F,Φ},调用GenProof(·)生成完整性证据P,返回给验证者.验证者接受证据后,执行CheckProof(·)验证证据是否正确.2.3云存储环境下数据完整性证明所具有的特性与P2P网络、网格计算等分布式网络相比,云存储环境下数据完整性验证机制具有以下几个显著特点:(1)支持动态操作.为了满足云中的应用,需要完整性验证机制支持动态操作,而传统的数据完整性证明机制已预先为每一个数据文件生成不可伪造的数据签名标签集合,当数据进行更新时需要重新生成大部分签名标签,使得计算代价和通信开销较大;(2)公开认证,为了缓解用户在存储和计算上的压力,云环境下的数据完整性验证机制最好能支持公开认证,允许任意的第三方来替代用户来完成数据完整性验证;(3)本地无备份认证,用户在本地并不需要存储数据副本;(4)无状态认证,验证过程无需保存任何验证状态;(5)确保用户隐私,用户采用公开验证应能确保用户的数据隐私.3数据完整性证明验证机制框架分类数据完整性验证机制根据是否对数据文件采用了容错预处理分为数据持有性证明PDP机制(ProvableDataPossession,PDP)和数据可恢复证明POR机制(ProofsofRetrievability,POR).如图2所示,本文按不同的关注点进行了分类,PDP机制能快速判断远程节点上数据是否损坏,更多的注重效率.POR机制不仅能识别数据是否已损坏,且能恢复已损坏的数据.两种机制有着不同的应用需求,PDP机制主要用于检测大数据文件的完整性,而POR机制则用于重要数据的完整性确保,如压缩文件的压缩表等.对于这类应用,尽管只损坏很小一部分数据,但却造成了整个数据文件失效.考虑利用HMAC哈希函数来实现远程数据的完整性验证,数据存储到远程节点之前,预先计算数据的MAC值,并将其保存在本地.验证时,用户从远程Page4节点上取回数据,并计算此时的MAC值,比对验证者手中的MAC值来判断远程节点上的数据是否是完整性的.由于需要取回整个数据文件,该机制需要较大计算代价和通信开销,无法满足大规模的应用.为了支持任意次的完整性检测,Deswarte等人后来考虑利用RSA签名的同态特性来构造PDP机制,但该机制需要将整个文件用一个大数表示,导致高昂的计算代价.之后,Sebé等人[12]对该算法进行了改进,提出了利用分块的方法来减轻计算代价,但采用确定性的验证策略,同样无法验证大的数据文件.随后,Ateniese等人[10]提出了采用概率性的策略来完成完整性验证,且利用RSA签名机制的同态特性,将证据聚集成一个小的值,大大降低了协议的通信开销.随后,Curtmola等人[13]考虑了多副本情况下数据完整性的验证,实现了一种满足分布式存储环境的MR-PDP机制,但不能支持动态的数据操作.随着云存储模式的出现,Erway等人[14]发现,用户可能需要对存储在远程节点上的数据文件进行动态操作,如插入,修改,删除等,而传统的PDP机制已不能满足这一要求,他们考虑引入动态数据结构来组织数据块集合,实现了支持块级全动态操作.之后,Wang等人[9]实现了另外一种支持全动态操作的PDP机制,该机制考虑采用Merkle哈希树来确保数据块在位置上的正确性,而数据块值则通过BLS签名机制来确保.为了减轻用户的负担,该机制还考虑引入独立的第三方TPA来代替用户验证云中数据的完整性.但采用这种方式存在泄漏隐私的风险.针对这一缺陷,Wang等人[15]提出了另一种保护隐私的数据完整性验证机制,该机制通过随机掩码技术,有效地隐藏了云服务器返回证据中的数据信息,使得TPA无法探知数据内容.尽管PDP机制能识别远程节点上的数据是否已损坏,但却无法保证数据是否是可用的.Juels等人[16]最先考虑如何恢复已损坏的数据,提出了一种基于哨兵(sentinel)的POR机制,该机制不仅能判断远程节点上的数据是否已损坏,并且可以恢复一定程度的数据失效.但该机制不支持公开验证,且只能进行有限次验证.之后,Shacham等人[17]吸收Ateniese等人关于同态验证标签(HomomorphicVerifiableTags,HVTs)的思想,考虑运用BLS短消息签名机制来构造同态验证标签,减少了验证阶段的通信开销.该方法被证明在任何强威胁模型中是安全的.由于在初始化阶段引入数据容错预处理,使得动态块级操作在POR机制中变得难以实现.Wang等人[18]提出利用纠错码的线性特征来支持部分动态操作,但无法支持插入操作.Chen等人[19]对Wang的机制进行优化,考虑采用CauchyReed-Solomon线性编码来进行数据预处理,该方法有效地提高了恢复错误的效率,但更新操作需要云服务器重新生成所有的辅助容错信息,导致计算代价较高.综上所述,设计支持全动态操作的POR机制仍是一个开放性的问题.4数据完整性证明验证机制根据是否对原数据采用容错预处理技术,数据完整性证明机制可以分为数据持有性证明PDP机制和数据可恢复证明POR机制,下面将对这两类验证机制进行分析对比.4.1数据持有性证明犘犇犘机制现有的PDP机制包括:基于MAC认证码的PDP机制、基于RSA签名的PDP机制、基于BLS签名的PDP机制、支持动态操作的PDP机制、支持多副本的PDP机制及保护隐私的PDP机制等.4.1.1基于MAC认证码的PDP机制基于MAC(MessageAuthenticationCode,MAC)认证码的PDP机制(MAC-PDPI),利用消息认证码MAC值作为验证元数据,对远程服务器上存储的数据进行完整性验证[11].最为简单的一种方式是,用户首先利用调用函数MACsk(·)为数据文件F生成验证元数据集合MAC={maci}1in,然后将其和文件一起存储到远程节点上,最后将密钥信息sk和文件信息(包括文件名字等)发送给验证者(TPA);验证时,TPA将需要验证的数据块的块索引发送给远程服务器,远程服务器返回指定的数据块和与其相对应的MAC值作为响应.TPA利用私钥计算数据块的MAC值,通过比对返回证据中的MAC值判断文件F是否完整.每次验证请求时,该方法需要远程服务器返回F中的部分数据内容,不但造成了数据文件的隐私泄露,而且需要大的通信开销和计算代价.针对这一问题,Shah等人[20]提出了另一种基于MAC值的完整性检测机制(MAC-PDPII).在用户外包数据之前,用户选取s个随机消息认证码密钥{ski}1is,然后用户对整个文件调用函数MACski(F)求出s个MAC值,并将验证元数据{ski,MACi}1is发送给TPA;验证时,TPA向服务器发出一个挑战请求{ski}i←R[1,s],远程服务器计算文件的Page5MAC值,并将其返回给TPA.TPA比对存储在本地的MAC值后,判断数据文件是否是完整的.基于MAC认证码可行的PDP机制存在以下固有缺陷:(1)只能进行有限次验证;(2)验证者必须保存大量辅助验证信息,如密钥信息、验证元数据等;(3)无法支持动态更新操作,只能用于静态的数据完整性验证.4.1.2基于RSA签名的PDP机制针对基于MAC认证码的PDP机制存在的缺陷,Deswarte等人[11]考虑利用RSA签名机制的同态特性(am)r=arm=(ar)m(modN)来构造完整性检测机制,这里m为待验证的数据文件,a为用户选择的随机数,r为每次请求生成的随机数(RSA-PDPI).很明显,该机制能进行无限多次完整性验证,同时有效地确保了数据的内容不被验证者察知,但协议需要将整个数据文件F用一个整数d来表示,用于之后的运算.因此,对较大的数据文件来说,计算代价过高.该算法的具体实现如图3所示.Setup阶段:(1)选择两个长度为1024位的素数p和q,计算RSA模数N=p·q(2)将文件F用整数d表示,计算验证元数据:M=admodN,a←RZN(3)将F存储到远程服务器上Challenge阶段:(1)验证者随机选择r←RZN,生成验证请求A=armodN,并发(2)证明者计算证据B=AdmodN,发送响应集合B给证明者.(3)证明者计算C=MrmodN,并判断C=?B.送A给远程服务器.图3Deswarte等人的基于RSA签名PDP机制Filho等人[21]对Deswarte等人提出的协议进行了修改,利用欧拉函数(N)的性质来优化验证元数据的生成过程(RSA-PDPII).定义一个哈希函数h(d)=dmod(N),利用该哈希函数将大的数据文件压缩成小的哈希值后,再用于运算,减轻了计算代价.为了辅助完成完整性验证,还需定义另一个函数H:H(d)=rdmodN,该函数具有同态特性,即H(d+d)=H(d)H(d).由于(N)是群(Z/nZ)的阶,据此可以构建函数h与函数H的对应关系,即H:H(d)=rdmodN=rdmod(N)=rh(d)modN.利用这一性质可以设计基于RSA签名的PDP机制,具体实现如图4所示.相比前一机制而言,该PDP机制的有效地减少了验证元数据的存储代价和通信开销.但该机制同样不适合大规模的数据存储.为了减少认证元数据的大小,Sebé等人[12]用分块的思想改进Filho的工作,从一定程度上减轻了p·q元数据集的大小.Setup阶段:(1)选择两个长度为1024位的素数p和q,计算RSA模数N=(2)计算欧拉函数(N)=(p-1)(q-1)(3)将文件F用整数d表示,计算验证元数据h(d)=dmod(N)(4)将F存储到远程服务器上Challenge阶段:(1)验证者选择r←RZN,并发送r给证明者(2)证明者计算证据R=rdmodN,并将其返回给验证者(3)证明者计算R=rh(d)modN,并判断R=?RAteniese等人[10,22]最先对数据持有性证明PDP机制进行了形式化建模,并考虑采用抽样的策略完成完整性验证,有效地减少了计算代价和通信开销(S-PDPIII,E-PDPIII).该机制的创新性主要在于以下3点:(1)考虑先对文件F进行分块,然后对每个数据块分别计算元认证数据,降低了生成验证元数据的计算代价;(2)提出了同态认证标签的概念,降低了通信开销.同态认证标签(Homomor-phicVerifiableTags,HVTs)是一种无法伪造的认证元数据,并且可以将多个数据块的元数据聚集成一个值,解决了基于RSA签名PDP机制中的证据大小与数据块的数目呈线性增长的问题;(3)提出了采用抽样的策略对远程节点进行完整性检测.对于存储在远程节点上的数据,若想得到95%以上的损坏识别率,验证者只需从数据分块集合中随机抽取300个以上的数据块用于验证[23].如图5所示,Ateniese实例化了两种基于RSA签名的PDP机制,S-PDP机制和E-PDP机制.S-PDP机制在指数知识假定(KEA-r)下是安全的.E-PDP机制是一种弱化的数据持有性证明机制.但随后的研究表明,E-PDP机制并不是安全的[17].从上可知,Ateniese等人的研究有效地减轻了用户和服务器端的计算代价,其提出的PDP形式化框架能运用于云存储环境下的数据完整性检测.但是,为了支持外包数据的完整性检测,基于RSA的PDP机制需要为文件的每个分块都生成一个认证元数据(其大小为128B),计算代价和通信开销仍然较大.另外,该机制专门针对静态数据存储,并不支持动态数据块更新,无法满足动态的环境.4.1.3基于BLS签名的PDP机制BLS签名机制是由Boneh等人[24]提出的一种短消息签名机制,相比目前最常用的两种签名机制RSA和DSA而言,在同等安全条件下(模数的位数为1024bits),BLS签名机制具有更短的签名位数,Page6p·qS-PDP验证机制选取公共参数:f是伪随机函数,π是伪随机置换函数,H为密码学哈希函数Setup阶段:(1)选择两个长度为1024位的素数p和q,计算RSA模数N=(2)选取g作为QRN生成元,QRN是模N的二次剩余集(3)生成公私密钥对pk=(N,g,e),sk=(d,v),满足以下关系:e·d≡1mod(p-1)(q-1)(4)选取唯一的文件标识v←R{0,1}κ,对文件F进行分块:F={b1,…,bn},生成认证元数据集合:Φ={σi}1in,这里σi=(h(v‖i)·gmi)d(5)将F和认证元数据Φ存入远程服务器Challenge阶段:(1)验证者对c个数据块发出挑战请求chal:随机选择2个密钥k1←R{0,1}κ,k2←R{0,1}κ,选取随机数s←R(Z/NZ),计算gs=gsmodN.令chal=(c,k1,k2,gs),并将其发送给证明者.(2)证明者接收到请求chal后,首先计算ij=πk1(j),aj=fk2(j,1jc);之后,计算证据T=∏gajmj;最后,计算证据ρ=H(ga1mi1送给证明者.(3)验证者接收到证据{T,ρ}后,首先计算τ=Te;然后,对于1jc,计算ij=πk1(j),aj=fk2(j),τ=τ算ρ=H(τs)modN,判断ρ=ρE-PDP机制:在Challenge阶段做以下替换:第2步证据生成部分替换为T=∏H(gmi1第3步τ=τ图5Ateniese等人的基于RSA签名PDP机制大约为160bits(RSA签名为1024bits,DSA签名为320bits).另外,BLS签名机制具有同态特性,可以将多个签名聚集成一个签名.这两点好的特性使得基于BLS签名的PDP机制可以获得更少的存储代价和更低通信开销来实现.基于BLS签名的PDP机制是一种公开验证机制,用户可以将繁琐的数据审计任务交由第三方TPA来完成,满足了云存储的轻量级设计要求,具体实现如图6所示.另外,相比其他机制而言,该机制具有更小的存储开销和通信开销.4.1.4支持动态操作的PDP机制对于存储在云中的数据,用户可能随时需要更新,而之前提到的PDP机制已无法满足这种需求.因为在计算数据块的验证元数据时,数据块的索引作为一部分信息加入了计算,如σi=(H(v|i)umi)sk,倘若插入一个新的数据块,将导致该数据块后的其他数据块的验证元数据都需重新生成,导致计算代价过高而无法实现.为了解决这一问题,Wang等人[10]、Erway等人[14]考虑采用动态数据结构确保选取公共参数:e:G×G→GT为双线性映射,g为G的生成元,H:{0,1}→G为BLS哈希函数Setup阶段:(1)随机选取私钥α←Zp,计算相对应地公私v=gα.私密为sk=(α)公钥为pk=(v)(2)选取唯一的文件标识v←R{0,1}κ,随机选取辅助变量u←RG,对文件F进行分块:F={b1,…,bn},生成认证元数据集合Φ={σi}1in,这里σi=(h(v‖i)·umi)α(3)将F和认证元数据Φ存入远程服务器Challenge阶段:(1)验证者从块索引[1,n]中随机选取c个块索引,并对每个块索引i选取一个相应地随机数vi←RZp/2组成挑战请求chal={i,vi}s1isc,并将其发送给证明者.(2)证明者接收到请求chal后,首先计算σ=∏i)viuvimi;之后,计算μ=∑{σ,μ}作为证据返还给证明者.(3)验证者接收到证据{σ,μ}后,根据以下等式判断外包数据是否完整:e(σ,g)=?(e∏数据块在位置上的正确性,而数据块标签确保数据块在数值上是正确的.前3节已对如何确保数据块值的完整性进行了描述,因此在本小节中,将更多的关注如何支持动态操作.4.1.4.1支持部分动态的PDP机制Ateniese等人[25]最先考虑如何支持动态操作,通过对文献[10]提出的PDP机制做简单的修改,使其支持了部分动态数据操作(DPDP).但该机制仅能支持数据更新、数据删除和数据追加等操作,而无法支持数据插入.另外,执行删除操作时采用标记Dblock来替代之前的数据块位置,造成了一定程度的存储空间浪费.4.1.4.2基于跳表的PDP机制为了解决不能支持插入操作这一问题,Erway等人[14]考虑引入动态数据结构来支持全动态操作(S-DPDP).在初始化阶段,该机制首先对数据文件F分块,F={m1,…,mn};然后为每个数据块mi生成数据块标签,τi=gmimodN;之后,利用数据块标签生成根节点的哈希值Mc,并将其保存在验证者手中.在挑战请求阶段,验证者随机生成挑战请求chal={i,vi}s1isc,并将其发送给服务器.服务器接收到挑战请求后,首先计算M=∑sc指定数据块的认证路径{Π(i)}s1isc;最后,将{M,{Πi,τi}s1isc}作为证据返回给验证者.验证者先利Page7用公式计算T=∏sc是否相等,若相等,表示数据内容是否是完整的,否则,认为数据文件已被修改;之后,验证者根据返回认证路径{Π(i)}s1isc、标签信息{τi}s1isc及存储在本地认证元数据Mc判断数据块在位置上是正确的.这种机制是唯一一种不需要生成私钥的PDP机制,且数据块标签可以在挑战阶段由服务器临时生成.举例说明:假定认证跳表数据结构如图7所示,节点中的数值为节点的rank值,左上角的节点为跳表的起始节点w7,当前节点用v表示,目标节点为v3,w表示当前节点的右边节点,z表示当前节点的图7基于跳表的认证数据结构图8计算经过当前节点v可访问底层节点的范围(其中,w表示当前节点v右边的节点,z代表当前节点v之下的节点)动态更新操作时,用户首先发出更新请求chal={update,mi,τi},服务器更新过程分两个阶段进行:第1阶段,解析请求中的更新操作update倘若是删除操作(D),则直接删除所在的数据块,倘若是修改操作(M),则更新指定的数据块内容和块签名标签,倘若是插入操作(I),则在指定的位置插入数据块和块标签;第2阶段是,辅助用户更新跳表的根哈希值,服务器返回每个指定节点的认证路径,用户利用认证路径更新根节点的哈希值.基于跳表的全动态PDP机制,是第1种完全支持动态操作的PDP机制,但其存在认证路径过长,每次认证过程中需要大量的辅助信息支持,计算代价和通信开销较大等问题.4.1.4.3基于MerkleTree的PDP机制Wang等人[10]提出了另一种支持动态操作的下方节点.根据节点上的rank值,可以为每个节点界定一个经过该节点可访问底层节点的范围[low(v),high(v)].例如根节点w7的low(w7)和high(w7)的值分别为1,12.根据当前节点的访问范围,可以计算两个后继节点w,z的访问范围,即[low(w),high(w)],[low(z),high(z)],计算方法如图8所示.如果指定节点的块索引i∈[low(w),high(w)],跟随rgt(w),并设v=w,否则跟随dwn(w),即v=z,直到找到目标节点i.将寻找节点的路径称为访问路径,而与其相反的路径称之为认证路径.在图7中,目标节点的查找路径是(w7,w6,w5,w3,v5,v4,v3),认证路径为(v3,v4,v5,w3,w4,w5,w6,w7).PDP机制,即基于Merkle-Tree的PDP机制,该机制通过Merkle-tree结构来确保数据块在位置上的正确性,利用基于BLS-签名的PDP机制来确保数据块内容上的完整性(M-DPDP).与Eeway等人提出的基于跳表的PDP机制不同的是,数据块标签并没有参与动态结构中根节点哈希值的计算.初始化阶段分为两步进行:第1步与图6中的初始化步骤大致相同,除了块标签计算,为了支持动态操作,需要用σi=(H(mi)umi)α替换σi=(H(name‖i)umi)α;第2步,构造Merkle认证哈希树,并计算根哈希值,并将其存储到TPA中作为验证元数据.验证过程与图6的验证过程相比,需要做以下修改:(1)在返回证据{μ,σ}的同时,需要返回挑战请求中块索引所对应的辅助认证路径信息{H(mi),Ωi}s1isc;(2)验证证据时,利用辅助认证路径信息计算根节点的哈希值.只有根节点相同时才进行后续步骤;(3)验证公式需要更改为e(σ,g)=?(e∏scH(mi)vi·uμ,)v.Merkle认证哈希树的数据结构如图9所示,i=s1Page8Me叶节点值为数据块的哈希值,对叶节点的访问采用深度优先的方式进行,如图中黑虚线所示.云服务器为了证明用户的数据是完整的,首先需要构造一条认证路径(如图中虚线所示),及其辅助认证信息(AuxiliaryAuthenticationInformation,AAI)组成证据,返回给TPA.TPA根据认证路径和辅助认证信息重新计算根节点的哈希值,比对本地存储的根节点哈希值来判断数据在位置上是否是完整的.例如,倘若TPA发出的挑战请求中选取块索引为图9Merkle哈希树结构很明显,采用Merkle认证哈希树可以确保数据节点在位置上的完整性.动态更新操作时,在更新节点的同时,需返回辅助认证信息给用户,用户重新生成根节点的哈希值.之后,更新存储在TPA中的根哈希值.相比跳表数据结构,Merkle认证哈希树具有更简单的数据结构.4.1.5保护数据隐私的PDP机制采用第三方对存储在云中的数据进行完整性验证时,有可能泄露用户的数据隐私信息.通过n次挑战请求后,不怀好意的第三方有可能获取用户存储在云中的文件内容和元验证数据信息.例如,假定用户委托第三方对其存储在云中的文件F进行完整性检测,第三方重复地对{ii,…,ic}位置上的数据进行完整性检测,每次挑战请求为发送chalj={i,vji},s1isc,1jc给云服务器,经过c次挑战请求后,可以得到以下方程组:只要上述方程组中的系数行列式不为0,则可通过高斯消去法计算求得(mi1,…,mic)的值.同理,第三方也可以获得块索引相对应的签名标签(σi1,…,σic).目前大部分基于BLS签名的PDP机制采用公开验证时,将有泄露用户数据隐私的风险.Wang等人[15,26]建议用随机掩码技术来解决这一问题(PP-{2,7},云服务器需在返回证据{μ,σ}的同时,返回x2认证路径{h(x2),hc,ha}、辅助认证路径Ω={h(x1),hd}和x7的认证路径{x7,hf,hb}、辅助认证路径Ω={h(x8),he}给TPA,TPA重新计算Merkle哈希树的根节点哈希值,即根据hc=h(h(x1)‖h(x2)),ha=h(hc‖hd),hf=h(h(x7)‖h(x8)),hb=h(he‖hf),最后计算出根节点哈希值hR=h(ha‖hb),对比存储在本地的根哈希值来判断数据文件是否是完整的.PDP).该方法的核心思想是,基于BLS签名的PDP证据生成过程中,使用到了线性组合μ=∑sc计算证据参数μ,从而导致了数据隐私的泄露.因此,通过引入两个参数r,γ来隐藏μ值,其中r←RZp,γ=h(R)γ,R=e(u,v)r为该次审计特征.计算μ的公式变为μ=γ∑scS-PDP验证机制选取公共参数:e:G×G→GT为双线性映射,g为G的生成元,H:{0,1}→G为BLS哈希函数,h:GT→Zp将GT中的元素均匀的映射到Zp.Setup阶段:与图6中Setup阶段一致Challenge阶段:做以下替换:第2步证据参数的计算替换为μ=r+γ∑第3步证据检测替换为验证者计算R=h(R),判断下式是否成立Wang等人的保护隐私的PDP机制能防止云存储中采用第三方在审计时泄露隐私的风险,减轻了用户的审计负担.Page94.1.6支持多副本的PDP机制采用冗余备份的方式来存储重要的大文件数据,可以提高数据文件的可靠性(MR-PDP)[13,27].采用冗余备份的方式存储数据文件时,远程服务器可能并没有按照用户要求的备份数来存储数据.由于存储在远程服务器中数据副本完全一致,存储服务提供商可能只存储一份或几份数据原文件,而对外宣称按用户要求存储了多份文件.因此,如何确保云中多个数据副本的完整性成为了另一研究方向.最简单的方式是,在数据存储到云中之前,采用多个密钥分别对每一副本进行加密,然后存储到云服务器上.进行数据完整性验证时,每一个副本文件都作为独立的数据文件进行数据完整性验证.该方法可以确保存放在远程云服务器上的数据是完整的,但也增加了大量的重复计算和通信开销,多个内容完全一致的数据文件需要多次验证才能确保其完整性.为了解决这一问题,Curtmola等人[23]设计实现了针对多副本的MR-PDP机制,该机制能对所有副本数据的副本进行完整性认证,而每次验证所带来的开销与对单个文件的进行数据完整性验证所带来的开销大致相同.MR-PDP机制是在Ateniese等人设计的基于RSA签名的PDP机制上修改而来的,该机制的具体组成如图11所示.MR-PDP机制包含有5个算法:KeyGen(·)、ReplicaGen(·)、TagBlock(·)、GenProof(·)、CheckProof(·).其中,KeyGen(·)和ReplicaGen(·)由用户执行,分别生成需要的密钥对和t个数据副本;TagBlock(·)算法为每个数据副本生成元认证数据集合;GenProof(·)图11多副本数据持有性验证机制(Si为远程服务器)由服务器执行,生成完整性证据;CheckProof(·)由用户或者TPA执行,通过服务器返回的证据,验证数据副本的完整性.MR-PDP机制同样由两个阶段组成:初始化阶段和挑战阶段.初始化阶段,用户调用KeyGen(·)生成密钥,利用私钥为数据文件F生成块签名集合Φ={σi}1in.之后,调用ReplicaGen(·)算法生成m个数据副本,每个数据副本利用随机掩码技术加以区分.最后将t个数据副本和块签名集合Φ存储到远程服务器中.在挑战请求阶段,验证者随机的从m个副本中抽取一个数据副本Fu,决定对其发起完整性验证.用户生成挑战请求的过程和Ateneise等人[9]设计的E-PDP一致,但服务器生成证据参数ρ的过程存在一些差异.当服务器接到验证请求后,服务器从数据副本Fu中抽取指定数据块{bi+ru,i}s1isn,然后做计算ρ=g∑s1isc一致.在检验证据的过程中,由于每个副本引入了随机数,因此需修改图5中挑战阶段的第3步,烄Te∏scH(v‖i烆i=s1MR-PDP机制能有效地验证多个副本文件在远程服务器上的完整性,该机制并不是针对云存储的多副本备份.4.1.7其他的PDP机制Wang等人[28-29]利用双线性函数的性质,设计实现了支持批处理的PDP机制,该机制能让TPA同时处理多个审计任务,优化了审计性能.Wang等人[30]针对云存储中的共享数据,利用群签名机制实现了用于验证多用户共享数据的PDP验证机制,该机制能保护用户数据和身份的双重隐私.4.1.8PDP机制小结要从以下两方面考虑:本结对上述的多种PDP机制进行简单对比,主(1)计算复杂度:包括在用户预处理文件的计算代价、在服务器生成证据的计算代价及第三方验证证据的计算代价;(2)通信复杂度:进行完整性时的通信开销.另外,还考虑了PDP机制的其他一些属性包括:是否支持动态操作、安全模型及文件损坏识别率等.4.2数据可恢复证明POR机制相比PDP机制而言,数据可恢复证明POR机Page10制在有效识别文件是否损坏的同时,能通过容错技术恢复外包数据文件中已出现的错误,确保文件是可用的.以下将介绍几种经典的POR验证机制.4.2.1基于岗哨的POR机制Juels等人[16]最先对数据可恢复证明问题进行建模,提出了基于岗哨(sentinel)的POR验证机制(SPOR),该机制主要解决以下两个问题:(1)更有效地识别外包文件中出现的损坏;(2)能恢复已损坏的数据文件.针对第一个问题,Juels等人通过在外包的文件中预先植入一些称之为“岗哨位”的检验数据块,并在本地存储好这些检验数据块.对于远程服务器而言,这些岗哨数据块与数据块是无法区分的.倘若服务器损坏了数据文件中部分内容,会相应地损坏到岗哨文件块.对比存储在本地的检验数据,能判断远程节点上的数据是否是完整的.另外,通过岗哨块损坏的数目可以评估文件中出错的部分在整个文件中所占的概率.针对第二个问题,Juels等人利用Reed-Solomon纠错码对文件进行容错预处理,使得验证机制可以恢复一部分损坏的数据.文献[16]表明,若采用(223,32)-Reed-Solomon纠错码对文件进行分组编码,文件的大小将增加14%.若文件损坏识别率高于95%,文件添加岗哨块后,大小将增加15.基于岗哨的POR验证机制存在以下缺点:(1)验证次数是有限的,取决于岗哨块的数目及每次认证所消耗的数目;(2)验证机制须在本地存储一定数目的岗哨块数据,并不是一种轻量级的验证机制.表1多种PDP机制对比SchemeServerComp.ClientComp.TPAComp.Comm.Dyn.ModelProbabilityofdetectionO(1)O(1)O(1)NO(1)O(1)O(1)NMAC-PDPI[11]O(1)MAC-PDPII[20]O(1)RSA-PDPI[11]O(1)RSA-PDPII[21]O(1)S-PDP[10]O(c)E-PDP[10]O(c)BLS-PDPO(c)DPDP[25]MR-PDP[13]O(c)M-DPDP[9]O(c)+O(logn)O(n)+O(logn)O(c)O(logn)YRO1-(1-f)cPP-PDP[15,26]O(c)S-DPDP[14]O(c)+O(logn)O(n)+O(logn)O(c)O(logn)Ystandard1-(1-f)c注:c:随机抽取的数据块数目;n:文件分块数;ServerComp.:服务器计算复杂度;ClientComp.:用户计算复杂度;Comm.:通信复杂度;Dyn:是否支持动态操作;“”表示部分支持;Model:证明安全模型;Probabilityofdetection:识别概率;f:数据块损坏的比例.4.2.2紧缩的POR机制为了解决Juels等人的不足,Shacham等人[17,31]分别提出了针对私有验证(PrivateVerifiability)和公开验证(PublicVerifiability)的数据可恢复POR机制(CPOR).这两种POR机制都具有以下优点:(1)无状态的验证,验证者不需要保存验证过程中的验证状态;(2)任意次验证,验证者可以对存储在远程节点上的数据发起任意次验证;(3)通信开销小,通过借鉴Ateniese等人同态验证标签的思想,有效地将证据缩减为一个较小的值.无状态和任意次验证需要POR机制支持公开验证,通过公开验证,用户可以将数据审计任务交由第三方来进行,减轻了用户的验证负担.支持私有验证的POR机制主要用于企业内部数据完整性验证,如私有云数据审计.过程如下:初始化阶段,用户先用Reed-Solomon纠错码对整个数据文件进行编码;之后,用户选择一个随机数α←RZp作为用户的私钥;然后,采用公式σi=fk(i)+αmi∈Zp为每个数据块的生成认证元数据集合{σi}1in;最后将数据文件F和元数据存入远程服务器上.挑战阶段与图6中的挑战阶段大致相同,需要修改第3步,用σ=?αμ+∑sc公开验证的POR机制可以让任意的第三方替代用户来发起对远程节点上数据的完整性检测,当发现数据的损坏程度小于某一阈值ε时,通过容错机制恢复错误,大于ε则返回给用户数据失效的结论.相比图6中的基于BLS签名的PDP机制而言,在进行初始化阶段之前,需要增加冗余编码数据预处理过程,使数据文件具有容错能力,即将F分成n个块,然后对n个块进行分组,每个组为k个块;之后,对每组数据块利用Reed-Solomon纠错码进行容错编码,形成新的数据文件珟F.而挑战阶段与图6中给出的协议完全一致.Page11文献[17]得出以下结论:对POR机制而言,假定ε是在允许的错误范围内(比如说,1000000中出现1次错误,但通过了POR验证),定义ω=1/#B+(ρn)c/(n-c+1)c,只要ε-ω是正的可忽略的值,在O(n2s+(1+εn2)(n)/(ε-w))时间范围内,通过O(n/(ε-w))次交互,POR机制的抽取操作能恢复损坏率为ρ的数据文件,这里B为挑战请求时随机数vi选取空间,ρ为编码率,c为随机抽取的数据块数目.举例:假定POR机制参数选定如下,ρ=1/2,ε=1/1000000,B={0,1}22,c=22,则POR机制能恢复损坏率不超过1/2的数据文件.由此可见,相比PDP机制而言,POR机制增加了初始化时间,但也降低了验证代价和通信开销.另外,执行抽取恢复操作的人必须是可信的,因为通过一定次数的验证请求后其将获取部分文件知识.4.2.3支持动态操作的POR机制由于POR机制在初始化过程中,由于数据块参与了容错编码,更新数据块的同时必须更新相应地冗余信息,导致计算代价较高.Wang等人[18]设计实现了第1种面向云存储的,支持部分动态操作的POR机制,该机制可以检测出云存储中已出现的错误,并获取在云服务器上发生错误的位置(DPORI).支持的动态操作包括:修改、删除和追加等3种操作.具体实现如图12所示,该机制预先利用Reed-Solomon纠错码的生成验证元数据,并将其存储在本地.验证请求时,服务器利用纠错码的线性特性将多个响应聚集成较小的集合.验证者通过返回的证据重新生成验证元数据,比对本地存储的验证元数据,判断文件是否正确,不正确时将获得数据出错的服务器.很明显,该机制是一种私有验证的POR机制,且只能进行有限次的数据完整性检测.Chen等人[19]考虑采用Cauchy-Reed-Solomon纠错码来替表2多种POR机制对比SchemeServerComp.ClientComp.TPAComp.Comm.Dyn.ModelProbabilityofdetectionSPOR[16]CPOR[17,30]O(1)O(n2)O(c)O(1)NROPcm·(1-PcDPORI[18]DPORII[19]O(1)O(n2)注:Pcm:选定的行命中损坏数据概率;Pc5未来的研究趋势结合实际应用需求和数据完整性证明机制的研究现状,我们认为未来云存储环境下数据完整性证明机制的研究趋势主要集中以下几点:换DPORI中的Reed-Solomon纠错码,提高抽取阶段的执行效率.f为随机函数,为随机置换函数Setup阶段:(1)随机选取一个范德蒙矩阵犃作为散布矩阵,经过一些列初等变化后,犃=[I|P].生成挑战密钥kchal和置换密钥KPRP(2)生成编码文件:G=F·A={G1,…,Gm,Gm+1,…,Gn},其中Gj=(gjGn}为冗余信息个验证元数据,每个标签i由下式计算得来:(3)生成验证元数据:为每个服务器j∈[1,…,n],预先生成t其中,αi=fkchal(i),kiprp←KPRP(4)屏蔽冗余信息{Gm+1,…,Gn}:gj(5)将G存入云中服务器,本地保存和认证元数据{vjChallenge阶段:(1)验证者重新生成αi=fkchal(i),kin,1it和P云服务器(2)服务器计算响应集合:{Rjn},并将其返回给验证者(3)接收到响应集合后,去除冗余信息的屏蔽值Rj←Rj-∑rq=1下式是否成立:已损坏(4)不成立,继续比较:Rj动态更新操作:F=(ΔF1,ΔF2,…,ΔFm),用ΔFi表示数据内容发生变化的块,当数据没有改变时,ΔFi=0.根据初始化阶段的第2~4步,重新生成已发生变化验证元数据Δvj对于删除操作,令ΔFi=-Fi.该机制不能支持插入操作.4.2.4POR机制小结表2对4.2节中的4种POR机制进行了对比,从表中可看出,支持全动态操作仍是POR机制所面临的最大挑战.(1)采用多分支路径的方法确保数据完整性云存储环境下具体应用不再只局限于数据存储和数据备份,更多的是面向动态的服务部署,如何有效地确保动态环境下数据完整性,将直接影响用户对云存储的体验和云服务提供商的信用.文献[24]考虑直接在文献[9]提出的静态数据完整性证明机Page12制上做一定程度的修改,使其支持动态数据的完整性证明,但不能支持全部的动态操作,如数据插入等.之后,文献[10,14]采用树形的数据结构来重新组织数据,通过增加访存复杂度来获取全动态的支持,但存在认证路径过程和认证路径所需的辅助信息过多等缺陷.如图13所示,可以采用多分支路径代替二叉树,减少节点的认证路径;同时,采用认证路径所特有的信息作为认证路径,减少认证路径的辅助信息,提高了认证效率并减少了通信开销[32].(2)保护用户数据隐私的数据完整性验证机制云存储环境下的数据完整性验证更多的是采用轻量级的数据完整性验证机制,用来支持泛在接入和移动计算.轻量级的数据完整性验证机制更多的是采用公开的数据完整性证明,用户将数据完整性的验证任务移交给第三方审计方来完成,从而大大的减轻了用户在存储和计算上所需要的开销.采用文献[15]的方法虽可以确保数据的隐私,但对于POR机制而言,采用这种方法将使得验证者无法再通过抽取器去恢复损坏的原文件.如何设计一种既能保护用户数据隐私又能支持抽取器工作的数据完整性证明机制将成为一个有意义的研究方向.(3)适合云存储更高效的数据可恢复证明机制云平台存储的数据通常是大规模数据,为其设计的数据完整性验证机制都是基于抽样的策略,当数据发生位偏转时,用户或可信第三方可能无法及时的发现该小概率事件,如何有效地确保云环境下数据的可恢复性将需要我们需要深入的研究.文献[16-17]提到的采用经典(n,k,d)Reed-Solomon纠错码技术[33]在大规模的数据集下并不可行,需要设计更加高效、合理的适合云存储的数据完整性证明机制,而目前在这方面的研究还相对较少.(4)多副本动态数据完整性验证机制由于云存储可以为用户提供廉价的存储空间,更多的用户喜欢采用多副本方式存储数据.如何确保云服务提供商确实按用户所要求的副本数目对原数据进行备份,将是未来需要考虑的一个问题,存储在云中的多个数据副本信息完全一样,采用原有的数据完整性验证机制将无法区分各个副本,目前最为常用的方法,是采用文献[13]所提到的方法,用户在外包数据之前,预先生成所需要的数据副本,然后为每个数据副本引入随机数加以区分存入云中,但该方法只能确认云服务提供商自身所提供的存储服务确实符合SLAs协议.另外,在确保数据完整性的情况下,如何同时对多个数据副本进行动态操作也将是值得研究的问题.(5)跨云动态数据完整性证明机制用户可能为了提高数据的可用性和可靠性,从而选择多个云服务提供商来存储数据或部署应用,如何确保多个云服务提供商所存储的数据是完整的,也将是值得深入研究的问题.文献[34-36]对跨云数据完整性证明机制进行建模,但并没有详细的实现步骤,仅仅只是定义了模型,需要设计一种可部署到具体实际应用中的跨云数据完整性证明机制,该机制可以很好地支持数据动态操作,以满足更多的应用.6结束语云存储环境下数据完整性验证问题的研究是一个非常活跃的方向.从整体上讲,目前云存储数据完整性验证方面的研究还不成熟,尚未建立起一套完整的理论体系,而且从技术理论的完善到算法的具体应用还有很大的差距.本文首先回顾了近年来学术界在数据完整性证明研究领域的主要成果,对其进行详细的分类归纳,之后,详细介绍了各种面向不同应用的验证机制的实现原理并加以对比,最后通过分析现有研究指明了未来研究的趋势.致谢在此,我们向对本文的工作给予支持和建议的同行,尤其是国防科技大学计算机学院软件所681教研室的老师和同学表示感谢!Page13
