Page1基于连续时间半马尔可夫决策过程的Option算法唐昊1),2)张晓艳1)韩江洪1)周雷1)1)(合肥工业大学计算机与信息学院合肥230009)2)(合肥工业大学电气与自动化工程学院合肥230009)摘要针对大规模或复杂的随机动态规划系统,可利用其分层结构特点或引入分层控制方式,借助分层强化学习(HierarchicalReinforcementLearning,HRL)来解决其“维数灾”和“建模难”问题.HRL归属于样本数据驱动优化方法,通过空间/时间抽象机制,可有效加速策略学习过程.其中,Option方法可将系统目标任务分解成多个子目标任务来学习和执行,层次化结构清晰,是具有代表性的HRL方法之一.传统的Option算法主要是建立在离散时间半马尔可夫决策过程(Semi-MarkovDecisionProcesses,SMDP)和折扣性能准则基础上,无法直接用于解决连续时间无穷任务问题.因此本文在连续时间SMDP框架及其性能势理论下,结合现有的Option算法思想,运用连续时间SMDP的相关学习公式,建立一种适用于平均或折扣性能准则的连续时间统一Option分层强化学习模型,并给出相应的在线学习优化算法.最后通过机器人垃圾收集系统为仿真实例,说明了这种HRL算法在解决连续时间无穷任务优化控制问题方面的有效性,同时也说明其与连续时间模拟退火Q学习相比,具有节约存储空间、优化精度高和优化速度快的优势.关键词连续时间半Markov决策过程;分层强化学习;Q学习1引言强化学习集仿真、统计、学习、逼近等技术于一体,是解决随机离散事件动态系统(DEDS)的一个重要人工智能方法[1].当前DEDS的学习优化主要是基于状态变化的方法,需要考虑整个系统每个组成部分的详细状态信息,而待学习参数的个数随状态变量维数增长成指数级增长,对于大规模系统,这将导致“维数灾”难题,因而耗费巨大存储空间和计算时间,影响算法的实时性,制约在线学习优化方法的实际应用.实际中,有些系统可能由各个相互联系的功能模块(或子决策者)构成,结构上具有分层嵌套等特点;另一方面,为了克服“维数灾”,复杂系统的研究有时需要对问题进行分解和抽象,控制方式上具有分层特点.这时,系统一般只在特定事件发生而需要执行子任务或宏行动时才触发决策.这种基于特定事件发生的分层决策方法,更符合DEDS的特点,可以降低问题研究所需的状态空间和策略空间的复杂度以及计算量,有利于系统的在线优化.强化学习与分层结构或控制相结合,产生了分层强化学习(HRL)方法.HRL是对学习过程进行时间抽象(temporalabstraction)或空间抽象(spa-tialabstraction),通过引入抽象机制将学习任务分解到不同层次上分别实现,每层上的学习任务仅对应较小的状态空间,因而可一定程度上简化策略的执行空间,克服大规模系统的“维数灾”难题[2-3].最具有代表性的HRL方法主要有Option、HAM和Max-Q[4-11],这3种方法的理论基础都是SMDP理论,都是对行动的抽象,但Option和HAM方法转换后的模型为单个SMDP,而Max-Q方法创建了多个SMDP分层并同时进行学习.另外,Option和HAM方法的策略收敛类型一般是分层最优,而Max-Q方法由于隔离了子任务环境,允许使用安全状态抽象,故为较弱的递归最优[12-13].其中,Option方法通过引入宏行动概念,可以显著地改变Agent的学习效率,因而获得了较为广泛的应用.当考虑分层结构或分层控制时,DEDS的决策机制往往是半Markov型的,例如,Option方法中,即便每个Option是马尔可夫型的,定义在这些Option之上的决策过程也是半马尔可夫型的.因此,分层强化学习的数学基础是半马尔可夫决策过程(SMDP)理论,但国内外的相关研究主要是面向离散时间SMDP[2],即学习过程主要关心系统随机转移的离散序列信息,而忽略报酬的时间累积效应.但是,在实际环境中,状态随机转移需要一定的时间,系统一般将会产生与该时间有关的报酬.因此,系统运行的性能报酬往往不仅与决策点的状态和行动有关,还与相邻决策点之间的状态转移过程有关,而且可能是按时间分段累积的,即存在按时间累积报酬.尽管诸多研究把决策点之间的按时间累积报酬的期望值等效为决策点的即时报酬,但受“建模难”和“维数灾”的制约,这种处理有时实际不可行,无法用于在线学习优化.Ghavamzadeh和Mahadevan基于连续时间SMDP,在离散时间MAXQ算法基础上,分别研究了平均和折扣准则下的连续时间Max-Q算法[5],但是面向连续时间SMDP的Option方法,还留有诸多问题有待深入研究和发展.Option算法的特点是把目标划分为多个子目标,并且只在子目标点进行决策,其它时刻按照Option已经学习好的策略或根据先验知识确定的策略执行.换言之,Option方法的策略可分为宏观、微观两个层面,而微观策略可由专家直接设计确定或使用现成的策略,使得学习任务只是优化宏观策略,因而简单易行.但是,现有HRL方法中的Option算法主要建立在离散时间折扣准则SMDP模型上[12,14],不能直接用于解决连续时间大规模无穷任务问题.文献[15]基于性能势理论和连续时间SMDP(CT-SMDP)数学模型,建立了一种适用于平均或折扣准则且与模型无关(model-free)的统一学习优化方法,可在线实现.在这些工作基础上,本文将根据现有HRL方法中的Option算法,研究提出Page3一种适用于平均或折扣性能准则的连续时间统一Option算法(非全新结构的HRL模型,结构上仍属Option方法),可同步进行宏观策略、微观策略学习,以实现在线优化.2连续时间SMDP数学模型在SMDP中,系统的状态在决策点之间也可能发生转移,其中的状态称为自然状态.根据代价的时间累积特性,SMDP可以分为离散时间SMDP(DT-SMDP)和CT-SMDP,两者的主要区别是前者的代价累积只依赖决策点之间自然状态的离散转移序列和相关状态报酬信息,而不考虑自然状态的实际逗留时间和累积报酬.在CT-SMDP模型中,令X(t)表示t时刻的状态,t属于非负实数集,X(t)属于一个有限状态集Φ={1,2,…,M}.记ti为系统第i个决策时刻,i属于自然数集N,且t1=0;a(ti)表示ti时刻系统在状态X(ti)处采取的行动,且a(ti)属于一个有限行动集D;此时,状态到行动集D的一个映射π就可以构成系统的一个平稳控制策略,即π:Φ→D;f(X(t),X(ti),a(ti),X(ti+1)),tit<ti+1,表示系统在状态X(ti)处选择行动a(ti)后,转移到下一决策状态X(ti+1)前,处于状态X(t)时的单位时间期望报酬(或代价)函数.在上述定义下,根据文献[16-17],有限状态集Φ中任意状态x的无穷时段折扣性能准则为α(x)=E∑ηπa(ti),X(ti+1))dtX(0)=]x(1)这里,0<α<1是折扣因子;当α→0时,其极限α(x)表示任意状态下的平均代价ηπ[16],其计算公ηπ式可另表示为ηπ=limT→t(T+1)E∑T根据上述式(1)与(2),在获得一次转移样本情况下,可以构造折扣或平均性能准则下性能势的统一即时差分(temporaldifference)学习公式,详见文献[15],此处不再赘述.3连续时间统一Option算法Option是Sutton等人[10]提出的一种HRL算法,其中,系统的学习任务被抽象成若干个Option,并且这些Option被作为特殊的“行动”(宏行动)加入到原有的动作(称动作原语)集中,动作原语可以视为Option的特例.为方便定义,一般将宏行动和动作原语统称为宏行动.在Option算法中,每个Option可视为完成某项子任务而定义在子状态空间上并按一定策略执行的宏行动序列.与Max-Q等HRL方法相比,Option方法的层次化结构较为清晰,因而易于自动生成,即子任务生成、子任务交互和子任务抽象等分层工作都可通过算法自动完成.Sutton等人提出的离散时间折扣准则Option算法是以DT-SMDP为数学模型和理论基础的,主要考虑Option内部状态的逻辑转移序列和离散决策点代价.但是,在实际环境中,不但系统的状态按概率随机转移,而且系统在一个状态所逗留的时间一般也服从一个随机分布,即一次状态转移需要耗费一定的随机时间,特别地,在相邻的两个决策点之间可能发生状态转移或有多次状态转移.因此,系统运行获得的性能报酬往往不仅与决策点的状态和行动有关,还与相邻决策点之间的状态转移有关,而且往往是按时间分段累积的,即存在按时间累积报酬,例如文献[15]考虑的生产加工系统即为此例.因此,很多实际复杂的系统需要考虑以CT-SMDP为理论基础的连续时间Option方法.本文将以前述CT-SMDP模型为基础,运用性能势理论[17-18],给出一种连续时间统一Option算法.首先,一个任务或子任务可由三元组o=〈Io,μo,βo〉表示,称之为一个Option[2].其中,IoΦ是该Option的状态集,Io包含且仅包含该Option所需的状态;μo:Io×OIo→[0,1]为此Option的内部随机策略,OIo为状态集Io上可执行的宏行动集;βo(X(t))∈[0,1],X(t)∈Io,为此Option的结束条件,表示该Option中状态X(t)以概率βo(X(t))结束,若令该Option的子目标状态集为ΦG,则对X(t)∈ΦG,有βo(X(t))=1.在本文提出的连续时间统一Option算法中,假设其结构分为n层,其中最上层为第1层,最底层为第n层.图1为该算法第u层、第u+1层、第u+2层间的切换及其内部部分状态转移图例.记tui为第u层中当前考查的某个任务(或子任务)的第i个决策时刻,其中u∈{1,2,…,n},i∈{1,2,3,…},并且t11=0.Xu(tui)表示该当前任务在tui时对应的内部状态,ou(tui)表示系统在该状态处根据当前任务的随机策略μo随机选择的宏行动.Page4若ou(tui)为动作原语,则该动作原语执行完毕就立即结束,系统在tui+1时转移到当前任务的下一状态Xu(tui+1),则tui+1-tui为执行该动作原语的逗留时间.对任意α0,令该次转移对应的随时间累积报酬f(Xu(tui),ou(tui),Xu(tui+1))为f(Xu(tui),ou(tui),Xu(tui+1))=k1(Xu(tui),ou(tui))+∫tu图1连续时间统一Option状态转移图例其中,k1(Xu(tui),ou(tui))为系统在状态Xu(tui)处执行动作原语ou(tui)后所获得的立即报酬,该报酬只与所在的状态和执行的动作原语有关,而与时间无关;k2(Xu(tui),ou(tui),Xu(tui+1))为系统在状态Xu(tui)处执行动作原语ou(tui)后,并于下一决策时刻tui+1转移到状态Xu(tui+1)之前的单位时间代价率(不失一般性的,此处假设k2(·)在一次转移中为常数).若ou(tui)为非动作原语,则执行该Option(或宏行动),系统进入第u+1层.其中,Optionou(tui)内的初始状态由第u层状态Xu(tui)决定,记为Xu+1(tu+11),且tu+11为tui映射到该Option内的初始决策时刻,可设tu+11=tui.如图1所示,ou(tui)的初始状态Xu+1(tu+11)是系统从第u层状态Xu(tui)进入到该Option内的跨层物理映射,但两者数学表达或定义一般不同.在第u+1层的ou(tui)内,系统按照策略i)执行一个子任务序列,直到该Option的结束μou(tu条件在tu+1m+1时刻满足,并在内部状态Xu+1(tu+1m+1)处回到第u层.Xu+1(tu+1m+1)对应的第u层状态Xu(tui+1)亦是系统在状态Xu(tui)处执行完宏行动ou(tui)后的下一决策状态.显然,tui+1=tu+1m+1,且tu+1m+1-tu+11,即tui+1-tui为执行宏行动ou(tui)的累积时间,且Xu(tui+1)是系统从第u+1层状态Xu+1(tu+1m+1)回到第u层的跨层物理映射.于是,可按下式计算第u层在执行完宏行动ou(tui)时对应的时间累积报酬f(Xu(tui),ou(tui),Xu(tui+1))=f(Xu+1(tu+11),这里,bu+1Xu+1(tu+1策状态Xu+1(tu+1{Xu+1(tu+1执行宏行动ou(tui)直到结束时,第u+1层经历的决策状态序列和决策状态数.根据式(3)和(4)以及性能势理论[15-16],可计算对应第u层的一次转移(Xu(tui),ou(tui),Xu(tui+1))的即时差分公式i=f(Xu(tui),ou(tui),Xu(tui+1))-dtu这里,Qα(Xu(tui),ou(tui))是折扣因子为α时的状态-宏行动对(Xu(tui),ou(tui))的性能势Q因子;另外,Tα(tui+1-tui)=∫tuT0(z)=limα→0Tα(z)=z,z0,且Ou(Xu(tui+1))为状态Xu(tui+1)所对应的宏行动集.于是,上述式(5)对任意α0都成立,即适用于平均或折扣性能准则.在连续时间Option算法中,单位时间平均报酬珔η通过系统学习过程中的实际学习时间和实际累积报酬计算得到,而系统执行非动作原语的累积时间和累积报酬值,均通过累计执行该宏行动所包含的动作原语的逗留时间和累积报酬得到.因此,珔η的值更新只与动作原语的执行有关,即仅当执行的宏行动为Page5动作原语时才对其值进行更新.其更新公式为珔η=珔η+ζtu其中,fα=0(Xu(tui),ou(tui),Xu(tui+1))表示平均准则下对应动作原语ou(tui)的转移(Xu(tui),ou(tui),Xu(tui+1))的时间累积报酬,ζtu为学习时间tui的倒数.每执行完一个宏行动,就可对状态-宏行动对(Xu(tui),ou(tui))的Q值进行迭代学习,如式(7)所示Qα(Xu(tui),ou(tui))=Qα(Xu(tui),ou(tui))+式(7)中γ(Xu(tui),ou(tui))表示学习步长,在这里可以取为γ(Xu(tui),ou(tui))=1/N(Xu(tui),ou(tui))c.其中,N(Xu(tui),ou(tui))为状态-宏行动对(Xu(tui),ou(tui))的访问次数,c为常数,且0<c<1.式(7)为折扣和平均两种性能准则下统一的连续时间Option优化算法的迭代公式.在Q算法中,常采用ε-greedy机制,但ε的初始值和衰减速率一般只能通过经验确定,若初始值图2SA-Option算法流程根据上述的流程图,我们可以得出SA-Option算法具体步骤如下.1.初始化.令所有状态-宏行动对的Q值为0;令珔η=0;生成一空栈stack{u,i,Xu(tui),ou(tui),bui,累积报酬值f},初始化其他控制条件.2.选择宏行动.在状态Xu(tui)处,根据SA机制选择宏太小或衰减速率太快,则不能进行充分的探索(ex-ploration),得到的策略往往只是次优策略;相反,若初始值太大或衰减速率太慢,虽然有利于探索,但却增加了学习代价,影响算法的收敛性,不利于学习利用(exploitation).模拟退火(SimulatedAnnealing,SA)机制利用学习过程中的Q值变化来平衡探索和利用之间的关系,可克服ε-greedy机制的缺点[20].因此,本文将模拟退火机制引入到分层强化学习中,并称对应的算法为SA-Option算法.SA机制运行规则表述如下:根据当前Q值表,在状态Xu(tui)处选择一个如下描述的宏行动:omaxu(tui)=argmax所对应的宏行动集中随机选取一个宏行动ou(tui),若exp[-(Qu(Xu(tui),omaxu(tui))-Qu(Xu(tui),ou(tui)))/(K·Tm)]random[0,1)成立,则取ou(tui)=ou(tui);否则,令ou(tui)=omaxu(tui).其中,random[0,1)为在区间[0,1)中产生的随机数,K为Boltzman常数,Tm为温度.本文定义的SA-Option算法流程如图2所示.行动ou(tui).3.执行宏行动ou(tui).3.1若ou(tui)为非动作原语.令bui=0,f=0,把记录的样本入栈,即stack{}←〈u,i,Xu(tui),ou(tui),bui,f〉.在Xu(tui)对应的第u+1层状态Xu+1(tu+11)处,根据策略μou(tui)来选择宏行动ou+1(tu+11),并令u··=u+1和i=1,转到步3.Page63.2若ou(tui)为动作原语.执行ou(tui),得到转移间隔逗留时间bu1,观察下一决策状态Xu(tui+1),根据式(3)计算累积报酬值f,把记录的样本入栈;根据式(6)计算平均代价的学习值η-.令h=u.3.3若状态Xh(thh>1,则h··=h-1,并转到步3.3;否则,转到步4.其中Xh(th为Xu(tui+1)对应的第h层第m个决策状态.4.值更新.4.1根据栈的后进先出特性,进行出栈操作,即tui=0,u=1,i=1,〈u,t,Xu(tu据式(5)和(7)计算即时差分dtuu=h,状态为Xu(tui),转到步2.4.2若栈为空,转到步5;否则,对栈顶记录的累积时间和累积报酬进行更新.即先出栈〈u,i,S,action,time,R〉←stack{},然后计算(Xu(tue-α×time×f与转移过程中的累积时间time··=time+bu后进行入栈操作,即stack{}←〈u,i,S,action,time,R〉.4.3若u=h,令珔η=珔η,转到步5;否则,转到步4.5.若算法终止条件满足,学习结束;否则,i=i+1,Singh等人已证明,在标准Q-学习收敛条件下,以Q-学习算法为基础的Option方法以概率1收敛到与任务分层结构兼容的分层最优[20],此结果很容易推广到上述连续时间Option方法.强化学习算法的复杂度主要涉及空间复杂度、样本复杂度和时间复杂度,一般情况下样本复杂度和时间复杂度等同.对于标准Q-学习算法,空间复杂度即为状态-行动对数据所需存储空间,若假设MDP或SMDP模型的状态个数为N,行动个数为A,则其所需存储空间为NA,因而空间复杂度为Θ(NA),也可记为Θ(N).若Option算法分层后的系统状态个数为N,行动个数为A,则所需存储空间为NA(由于Option算法是为解决复杂问题而引入抽象机制来实现空间降维,其存储空间一般小于原MDP或SMDP,见本文实例),因而空间复杂度为Θ(NA).根据文献[21-22],标准Q-学习算法的样本复杂度可以简记为O(Nlog(N)).而Option算法中,考虑到进行抽象而产生的各层之间的相关性,一般难以给出该类算法统一的样本复杂度公式.当然,对于非嵌套Option,若其子空间的状态个数为Ns,则其样本复杂度可记为O(Nslog(Ns)).4实验仿真本节利用单个机器人(Agent)进行垃圾收集的系统为例验证第3节构造的连续时间统一Option算法(SA-Option)的有效性[9],并与基于性能势的连续时间统一SA-Q算法进行性能比较(具体算法见附录),以说明其优越性.4.1仿真模型如图3所示,Agent垃圾收集系统由3个房间和1个走廊组成.现有1个Agent负责从Room1和Room2的垃圾箱(T1或T2)中收集垃圾,并把收集到的垃圾放入Room3中的垃圾场(Dump).垃圾箱中的垃圾按一定的Poisson流产生,假定两个垃圾箱的垃圾产生率相等;机器人一次只能在一个垃圾箱处收集垃圾,并假设机器人一次能把一个垃圾箱中的垃圾全部收集并送往Dump(假设垃圾箱和Dump的容量无限大).Agent可执行7种行动:上、下、左、右、等待、捡取、放置,且令Agent执行每个行动的逗留时间均服从均值为0.5的指数分布.定义Agent从任一位置出发,转移到任一垃圾箱并正确捡取垃圾,然后转移到Dump并正确放置垃圾这一过程,称为正确收集一次垃圾.Agent在T1或T2处正确捡取垃圾以及在Dump处正确放置垃圾,分别会得到相应的立即报酬,其值为1;若Agent错误捡取或放置垃圾,会得到值为负的报酬作为惩罚,其值为-1;若Agent执行上、下、左、右或等待这5种行动,其立即报酬为Page70.另外,Agent执行每个行动都会产生时间累积报酬,其单位时间报酬率为-0.025.当Agent在垃圾箱T1或T2处,若其未持有垃圾,并且所在的垃圾箱中有垃圾,则此时进行垃圾捡取为正确行动,否则执行垃圾捡取为错误行动;同样,当Agent在Dump处,若其持有垃圾,则此时Agent进行垃圾放置为正确行动,否则执行垃圾放置为错误行动.假设A-gent错误地执行行动,即错误捡取或放置以及导致碰壁,则系统状态不变,否则系统状态做出相应转变.该Agent垃圾收集系统为逗留时间服从指数分布的连续时间MDP模型,其优化目标为Agent根据决策时刻的状态,采取最优行动,使系统报酬准则函数值在无穷时间水平下期望值最大.现把第3节介绍的SA-Option算法运用到此系统中,为了实现分层强化学习方法中状态和宏行动的合理抽象,首先把系统环境离散化成8×8的栅格,走廊标记成两个房间并对房间号重新编排,如图4所示.然后,对该Agent进行分层控制,总共可以分为3层,即顶层(1),中间层(2),最底层(3).于是有下列定义:X1(t1i)=(XRoom(t1i),XA(t1i),XT(t1i)):系统顶层在其第i个决策时刻的状态.其中,XRoom(t1i)表示系统在t1i决策时刻所处的房间号状态,XRoom(tci)∈{1,2,…,5},数字表示几号房间;XA(t1i)表示系统在t1i决策时刻Agent的自身状态,XA(t1i)∈{0,1},0状态表示Agent没有运载垃圾,1状态表示运载垃圾;XT(t1i)=(XT1(t1i),XT2(t1i))表示系统在t1i时刻两垃圾箱的状态;XTq(t1i)表示为垃圾箱Tq,q∈{1,2},在决策时刻t1i的状态,XTq(t1i)∈{0,1},0表示垃圾箱Tq处没有垃圾,1表示有垃圾;X2(t2i)=(Xs(t2i),XA(t2i),XTq(t2i)):系统中间层在其第i个决策时刻的状态.其中,Xs(t2i)=(x(t2i),y(t2i))表示Agent所在的位置,x(t2i)和y(t2i)则分别为t2i时刻Agent所处栅格的横坐标和纵坐标,其中x(t2i),y(t2i)∈{0,1,…,7},以图4的左下方为坐标原点;XA(t2i)同XA(t1i),XTq(t2i)同XTq(t1i)分别是一一对应关系;若Agent在上层选择处理T1的垃圾,则q=1;若Agent在上层选择处理T2的垃圾,则q=2.X3(t3i)=(x(t3i),y(t3i)):系统最底层在其第i个决策时刻Agent所在的位置.X3(t3i)与Xs(t2i)是一一对应关系.o1(t1i):t1i时刻Agent在顶层采取的宏行动,o1(t1i)∈{处理T1的垃圾,处理T2的垃圾,等待}.o2(t2i):t2i时刻Agent在中间层采取的宏行动,o2(t2i)∈{移动到Tq,垃圾捡取,移动到Dump,垃圾放置},其中q∈{1,2}.o3(t3i):t3i时刻Agent在最底层采取的宏行动,o3(t3i)∈{上,下,左,右}.在该仿真实例中,若系统在状态Xu(tui)处,选择的宏行动ou(tui)为动作原语,则时间累积报酬值f(Xu(tui),ou(tui),Xu(tui+1))按式(3)计算,有f(Xu(tui),ou(tui),Xu(tui+1))=其中,当执行捡取或放置动作时,正确的行动对应为k1(Xu(tui),ou(tui))=1,而当其是错误行动时则对应为k1(Xu(tui),ou(tui))=-1;当Agent采取上、下、左、右行动时,k1(Xu(tui),ou(tui))=0;k2表示单位时间报酬率.若选择的宏行动ou(tui)为非动作原语,则f(Xu(tui),ou(tui),Xu(tui+1))按式(4)进行计算.4.2实验结果仿真初始条件如下:对任意状态-宏行动对(Xu(tui),ou(tui)),令初始学习步长γ(Xu(tui),ou(tui))=1/8,其衰减规律为1/(8×N(Xu(tui),ou(tui))0.2);模拟退火的初始温度为T0=1.0×1020,且每5000步降温一次,降温系数为0.9.从图4可以直接看出,若Agent从Dump处出发,正确收集一次T1处垃圾最少需要执行34个动作原语,正确收集T2处的垃圾最少需要执行48个动作原语.若以相等的概率选择收集T1和T2处的垃圾,则Agent平均收集一次垃圾最少需执行41个动作原语.由于执行动作原语的时间均值为0.5s,则正确收集一次垃圾平均最少需要20.5s.若每个垃圾箱垃圾产生率λ=0.01,则系统平均50s生成一个垃圾,若λ=0.03,平均约17s生成一个垃圾.因此,在下面仿真实验中,当λ=0.01时,表示系统中的垃圾产生率不是很大,系统中的垃圾箱有较大机会出现空状态;当λ=0.03时,表示系统中的垃圾产生率相对较大,垃圾箱大部分情况下处于有垃圾状态.采用这两种取值,可以更好地将SA-Option算法与SA-Q算法进行比较.图5和图6分别为λ=0.01和λ=0.03时,两种算法的平均报酬学习曲线.这里每学习10步记录一次平均报酬的学习值.对比两图可见,SA-Option算法的平均报酬均优于SA-Q算法.图7和图8分别为图5和图6在学习初始时的截图,可见从初始阶段开始,SA-Option算法的平均报酬就明显优于SA-Q算法.这是因为前者通过分层和行动抽象,在Page8图5α=0,λ=0.01时,两种算法的平均报酬优化曲线图6α=0,λ=0.03时,两种算法的平均报酬优化曲上层进行宏决策,可加快算法的学习速度,并提高其优化精度.图7中SA-Option算法的优化曲线在200×10步左右时达到一个峰值,然后有一个大的下降.这是由于初始学习时,机器人正确收集一次垃圾需要很多仿真步数,导致在仿真的初始阶段垃圾箱一直处于有垃圾的状态,得到该种情况下的较优策略.但由于SA-Option算法收敛很快,在学到200×10步左右时,SA-Option算法中垃圾收集策略就渐渐成熟,成功收集一次垃圾的时间也随之变短,导致垃圾箱有时处于空状态,因此Agent需要重新学习此种情况下的策略,因而导致平均报酬下降.当垃圾产生率很大时,无论是在学习的初始阶段还是后期,Agent学习较多的是系统处于有垃圾的状态,因此不会出现图7中的明显峰值现象,具体可参见图8中的SA-Option曲线.而图7和图8中的SA-Q优化曲线基本上没有出现明显的峰值现象,皆呈平稳的上升趋势.这是由于该算法的学习优化速度比SA-Option算法要慢,不存在初始阶段很快学习收敛的情形.图7α=0,λ=0.01时,两种算法的平均报酬优化曲线图8α=0,λ=0.03时,两种算法的平均报酬优化曲线表1给出SA-Option算法与SA-Q算法在平均准则下的其它性能值比较.在λ=0.01时,SA-Q算法的平均报酬为0.0497,而SA-Option算法的平均报酬为0.1702,相差0.1205.这是因为SA-Option算法通过分层,在上层进行宏决策,可以有效减少微观决策的失误性,从而减少了惩罚次数,可以明显提高学习值,即增加了优化精度.表1两种学习算法平均报酬和所需存储空间比较算法平均报酬(λ=0.01)所需存储单元SA-QSA-Option另外,SA-Q算法在学习过程中需存储的状态-行动对(X(ti),a(ti))为512×7=3584个.在Option算法中,若一个子目标只包含动作原语,则其对应的子策略可以根据先验经验得到.根据先验经验得到某些子目标的策略,不仅由于无需子策略的学习而节约存储空间,还可以加快整个算法的学习速度并有可能提高学习精度.但是先验经验有时难以得到,这也Page9是Option算法的不足之处.因此,在Option算法中,一般可通过同步学习来确定子目标的策略.此时,本仿真实验系统需存储的状态-宏行动对(Xu(tui),ou(tui))的个数为40×3+(256×4)×2+(64×4)×3=2936个,则SA-Option算法比SA-Q算法节约648个存储单元,空间节约率为18.08%.若在SA-Option算法中采用先验经验确定部分策略,则学习过程中需存储的状态-宏行动对个数为40×3+(256×4)×2=2168个,比SA-Q算法节约1416个存储单元,节约率约39.51%.其中节约率κ定义为其中,B表示SA-Q所需存储空间,C表示SA-Option所需存储空间.图9为α=0.1,λ=0.01时,SA-Option算法下状态X11(t11)=(3,0,0,1)、X21(t11)=(1,0,1,0)和X31(t11)=(4,0,0,1)的折扣报酬学习曲线;图10为α=0.1,λ=0.01时,SA-Q算法下状态X1(t1)=(2,6,0,0,1)、X2(t1)=(6,0,0,1,0)和X3(t1)=(3,0,图9α=0.1,λ=0.01时,SA-Option算法不同图10α=0.1,λ=0.01时,SA-Q算法不同初始位置的0,0,1)的折扣报酬学习曲线.其中Xj1(t11)表示SA-Option算法顶层在第一个决策时刻t11的初始状态j,Xj(t1)表示SA-Q算法在第1个决策时刻t1的初始状态j,j∈{1,2,3}.在这两种算法中,初始状态是一一对应的,如X11(t11)对应X1(t1).对比两图可得,折扣情况下,SA-Option算法的优化精度也比SA-Q算法高,并且SA-Option算法的折扣报酬在学习的初始阶段迅速提高,SA-Q算法的折扣报酬上升则比较平缓.这表明在折扣准则下,与SA-Q学习相比,SA-Option算法的学习速度和学习优化结果也都更有优势,表2为部分比较结果.这是由于SA-Option算法通过分层和行动抽象,减少了微观决策的失误性,在仿真的初始阶段就能得到较好的策略,其折扣报酬基本上不受初始状态的影响;而SA-Q算法通过每个行动的选择来学习策略,学习速度比SA-Option算法慢,其初始状态将很大地影响折扣报酬.因此,与SA-Q算法明显不同的是,SA-Option算法的折扣报酬学习值对初始状态很不敏感.表2两种学习算法不同初始状态的折扣报酬比较算法初始状态1初始状态2初始状态3SA-Q0.04050.1166-0.9491SA-Option0.28150.30710.33495总结本文针对连续时间和无穷任务问题,在性能势理论和CT-SMDP模型下,结合分层强化学习中现有的Option算法,提出了一种适用于平均或折扣准则的连续时间统一分层强化学习优化算法(SA-Option).为验证这种算法的有效性,利用机器人垃圾收集系统为例,建立了相应的分层优化模型和Option分层算法.仿真结果表明,构造的统一SA-Option算法在求解连续时间和无穷任务问题时,比SA-Q算法具有明显的优势.一方面,该算法能够节约存储空间,另一方面,它能够提高学习速度,并在一定程度上提高了学习精度.此外,本文研究的算法可进一步扩展到解决连续时间无穷任务的多机器人协作问题.
