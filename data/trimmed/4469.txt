Page1一种低开销的面向节点内互连的网络接口控制器苏勇1),2)曹政2)刘飞龙1),2)王展1),2)刘小丽2)安学军2)孙凝晖2)1)(中国科学院大学计算机与控制工程学院北京100190)2)(中国科学院计算技术研究所计算机体系结构国家重点实验室北京100190)摘要高性能计算和云计算的飞速发展对高性能互连网络的设计提出了越来越高的要求:除了要保证高带宽、低延迟和高可靠性等特性,还要面临成本和系统规模的挑战.该文针对这些特性和挑战提出了一种低开销的基于cHPP体系结构的超节点网络接口控制器:(1)设计了兼容PCIe的网络通信协议,降低协议转换开销、减少通信延迟并增强系统可扩展性能;(2)采用PCIe高速通信接口并支持用户级通信提高软硬件交互效率,面向MPI编程模型抽象出高效通信原语(如NAP、PUT和GET)加速大数据传输;(3)硬件支持I/O虚拟化实现超节点内对网络接口控制器的高效共享.为了对该文的设计进行功能和性能验证,文章基于FPGA实现了系统原型,实验结果显示最低延迟为1.242μs,有效数据带宽可达3.19GB/s.关键词互连;网络接口控制器;直接存储器访问;PCIExpress;I/O虚拟化1引言大规模并行系统广泛应用于高性能计算和云计算领域,两个领域都对通信系统提出新的需求.高性能计算在科学研究、工程技术以及国防军工等方面的应用取得了巨大成功,计算能力突飞猛进.基于异构架构的“天河II”①超级计算机以峰值计算速度每秒5.49亿亿次、持续计算速度每秒3.39亿亿次双精度浮点运算的优异性能成为全球最快的超级计算机.异构计算是一种高效利用各种计算资源的并行和分布式计算技术,在提升计算性能的同时降低成本和能耗,已经成为高性能计算发展的新趋势.因此,对异构计算模式的支持是高性能计算的现实需求,但是在传统结构中,协处理器仅仅挂载在I/O总线上作为加速部件使用,大量通信需要用主处理器内存来中转,难以获得等同的网络性能.计算能力的不断提高也要求互连网络必须具有超高带宽、超低延迟的高性能,因此,异构计算服务器中的通信性能亟待提高.此外,随着系统规模的不断增长,系统的成本和功耗越来越高,有效降低成本和功耗也是高性能互连网络面临的难点问题.云计算在助力企业发展,推动技术进步等方面发挥了重要作用.虚拟化技术在云计算领域得到了迅猛发展,特别是I/O虚拟化,就像光纤入户技术一样,成为虚拟化技术的“最后一公里”.在虚拟化环境下,大量并发的高吞吐率负载对网络接口控制器提出了严峻挑战,因此,迫切需要加强对I/O资源的合理高效的共享.面对上述在性能和共享能力上的挑战,本文基于HPP(HyperParallelProcess)体系结构提出了cHPP(configurableHPP)体系结构,针对面向异构计算的通信加速和基于硬件的节点内I/O资源高效共享设计,实现了一种低开销的面向节点内互连的网络接口控制器.本文第2节介绍cHPP体系结构;第3节阐述网络接口控制器设计的关键技术;第4节阐述网络接口控制器的具体实现方法;第5节进行性能评测并得出有关结论;第6节阐述目前主流高性能计算机网络接口控制器的相关研究,对比各自的特点和不足;最后是全文的总结和对未来工作的展望.2超节点控制器结构2.1HPP体系结构超并行(HPP)体系结构[1]是中国科学院计算技术研究所提出的一种基于超节点通信性能优化的高性能计算机体系结构,在保证分布式系统的高扩展性的同时基于硬件实现了全局物理内存共享,支持基于共享存储的编程模型.超节点是指多个处理单元通过一个超节点控制器连接起来构成的超级节点,该结构能够有效降低超节点内部计算单元间的通信延迟,同时能够减少节点数量、降低互连网络规模,系统的平均通信延迟也随之降低.如图1所示,HPP体系结构实现了芯片、节点、系统多级并行结构:通过多核处理器实现核间并行、超节点内部采用异构加速器和通用处理器实现处理器间并行、超节点间通过互连网络构成机群系统实现超节点间并行;支持全局地址空间,对超节点内的内存和I/O资源统一编址,使超节点的处理器可对全局资源高效共享;支持多通道并发的核到核之间的通信;超节点操作系统具有单一系统映像并有效支持MPI(MessagePassingInterface)和PGAS(PartitionedGlobalAddressSpace)编程模型.2.2cHPP体系结构为满足高性能计算和云计算对性能的共同需求,同时兼顾异构计算和虚拟化对通信系统的差异性需求,在曙光6000HPP体系结构的基础上提出了cHPP体系结构.cHPP体系结构的首要目标是加速异构计算,提高节点计算密度,通过cHPP控制器,结合高效通信接口,实现了异构处理器间的直接通信.相比于传统结构,这种体系结构可以消除通信瓶颈,提高通信效率,并有利于系统规模扩展.基于硬件支持资源的聚集和高效共享是cHPP体系结构的另一重要目标,因此cHPP控制器除了实现处理器对内存和I/O资源的高速访问,还支持灵活有效的资源聚集和高效的共享.cHPP控制器用于构建面向高性能计算应用,①Top500.org(2013)TOP500List.http://top500.org/lists/Page3同时兼顾云计算需求的新型高性能服务器.因此cHPP控制器既要实现高性能的互连,还要提供虚拟化的相关支持.图2描述的是cHPP控制器用于节点内互连的场景,cHPP控制器既可以连接通用处理器和协处理器,也可以直接连接I/O设备或I/O桥.cHPP控制器提供全面的I/O虚拟化支持,节点内的I/O设备可以被虚拟成若干虚拟设备.此外,cHPP控制器本身也可以被虚拟成若干虚拟控制器.所有的虚拟设备和虚拟控制器均可被直接分配给节点内的虚拟机,实现I/O设备被处理器的直接共享.2.3cHPP超节点控制器为满足上述场景的需要,cHPP控制器主要负责超节点数据通信和I/O高效共享的功能,支持全局地址空间和用户级通信,采用PCIExpress标准(PCIe)的高速I/O接口提升链路性能,并增加了对单根虚拟化(SR-IOV)规范的支持,实现超节点内I/O资源的高效共享.如图3所示,每个cHPP控制器含多个网络接口控制器和交换模块,cHPP控制器间通过PCIe链路直接互连,支持任意网络拓扑结构.系统规模可根据需求灵活配置,具有良好的伸缩性.PCIe交叉开关模块用于实现处理器与I/O设备间PCIe消息的交换;IntraDMA交叉开关模块用于实现节点内DMA数据的交换.因此,cHPP控制器可支持多种拓扑的直接网络互连,可提供节点内多处理器间高速通信和I/O高效共享.网络接口控制器是保证节点内高性能通信和资源共享的关键部件,网络接口控制器的微体系结构和网络通信协议是本文的主要研究内容.3网络接口控制器设计cHPP控制器面向高性能计算,同时兼顾云计算的需求,其网络接口控制器需要支持高性能的通信和高效的共享.通信延迟决定了高性能互连网络的性能而可扩展性则决定了网络的规模,二者是衡量网络性能的关键因素.网络通信协议决定了网络接口控制器数据传输的效率,通信接口决定了软硬件的交互效率,通信原语的定义和实现机制则决定了控制器的功能和硬件效率,共享机制决定了控制器被共享的能力,这些方面共同构成了网络接口控制器的有机整体.因此,本节围绕低延迟与可扩展性对节点内通信进行优化,从低开销通信协议、高性能通信接口、高效通信原语和高效的I/O共享4个方面对网络接口控制器进行了设计分析.3.1低延迟与可扩展性能分析LogP[2]模型以很少的参数来反映并行计算的关键技术,但主要是针对短消息进行分析.LogGP[3]模型对长消息传输进行了分析.本文在LogGP模型的基础上对基于Chain-DMA引擎的网络接口控制器执行长消息传输进行了系统延迟分析.端到端传输时间定义为发送节点网络接口控制器开始DMA操作到接收节点接收最后一个字节为止.如图4所示,采用基于Chain-DMA引擎的网络接口控制器传输M字节消息所需的时间由式(1)定义.T(P)=Oc+Ods+(M-1)×G+L(P)+Odr(1)其中,Oc为处理器启动消息发送的时间;Ods是DMA启动开销,包括门铃启动时间和描述符读取时间;M是消息长度(字节);G是网络“间隔”(周期每字节),其倒数为网络带宽;L(P)=H×r是消息包头通过网络的平均时间,H是消息通过的平均跳数,r是每跳处理时间和线路延迟之和,P是处理器数目;Odr是DMA接收处理延迟.从式(1)可以看出:设计高效的软硬件接口可以加速消息发送,有利于压缩Oc开销;建立快速的DMA启动机制,有利于降低Ods开销;构建高效的DMA引擎,设计低开销的通信协议都有利于减少Odr实现数据的快速传递;提供更高的网络带宽可降低G,因此采用高带宽的通信链路有利于减少通信延迟;压缩网络直径,降低平均跳数H也可减少网络延迟.因此,可以围绕上述各项指标设计网络接口控制器以提供高性能的通信网络.对于大规模互连网络来说,网络拓扑结构应具有Page4良好的扩展性.理想拓扑结构应具有对称性、扩展粒度低、等分带宽大、网络直径短、节点度适度等优良特性.为此,比对了不同网络拓扑结构对延迟和可扩展性能的影响.为简化分析,根据以往的经验数据,参数取值分别为Oc=200ns,Ods=528ns,r=240ns,Odr=160ns,M=2KB,G=1/4(ns/Byte).根据文献[2]的统计,直接网络的平均距离H与处理器数目P的关系如表1所示.图5显示了在各种不同拓扑网络情况下的通信延迟性能.数据表明,网络接口控制器在Hypercube网络具有最好的延迟性能,而且随着规模的扩展缓慢的增长,说明具有良好的可扩展性.图4基于Chain-DMA引擎的数据传输时延图6网络包硬件包头格式3.2低开销通信协议由于具有高带宽、低延迟、高吞吐率等特性,PCIe已经成为实际上的I/O总线标准.PCIe提供高速点对点的单/双工通信,物理链路采用差动信令以提高传输距离.最新的PCIe3.0架构单信道(×1)单向带宽接近8GB/s.主流处理器都直接支持PCIe协议,通过PCIe可直接与I/O设备通信,减少协议转换开销,降低通信延迟;支持SR-IOV技术可实现多虚拟机间I/O资源的高效共享;基于信用的流控机制确保链路层的可靠传输;CRC校验支持链路层错误检测,支持出错自动重传,链路可靠性高.因此,从带宽、传输距离、可靠性等方面看,PCIe可用于高性能计算的系统级网络互连,实现大规模高性能互连网络的高速互连,但是作为I/O总线,PCIe的典型应用是树形拓扑,处理机是树的根,而I/O设备则是树的叶子.标准PCIe交换机构成的网络,通常是若干个功能独立的子树的集合,子树之间并无数据交换.虽然这已经满足了在I/O扩展方面的需要,但是根与根之间不能通信,终端叶子节点之间也无法直接通信,难以构造复杂的拓扑网络,无法用于处理器间通信.因此,网络通信协议在PCIe协议的基础上进行了拓展,可充分利用PCIe标准的优良性能并拓展了处理器间直接通信能力.完整的网络包是由硬件包头、有效数据载荷和软件包尾构成.基于PCIe事务层包格式,本文的网络包头格式如图6所示,其中黑色加粗字段为重定义字段,包括PKTType定义了网络包的类型(包括NAP/PUT/GET);SRC_ID包含了源数据的处理器号,虚功能号和队列对号;DST_ID包含了目标处理器号、目标虚功能号和目标队列对号;增加了QPMagic域用于保护信息,并通过S_Flag域进行控制(如完成事件发送控制等).沿用了PCIe类型域(Type)、流量类型域、长度(Length)、地址(Address)、请求ID(RequesterID)等PCIe的链路层关键域,其Page5含义与PCIe协议一致.网络接口控制器根据数据长度域(Length)、地址域(Address)、源节点和目标节点的数据及控制信息进行数据传输.基于PCIe协议定义的网络消息格式更适合于局部互连:(1)充分发挥PCIe高速可靠的链路层性能,使网络消息直接面向内存操作,不仅减少了I/O总线与网络间的协议转换开销,还无缝兼容PCIe设备,实现I/O设备访问与网络通信功能的融合;(2)提高了有效负载率,PCIe包头为16Bytes,数据负载为0~4096Bytes,相比面向大规模系统的互连网络(如InfiniBand[4]的包头最大为94Bytes,IBMBluegene/Q[5-6]和KComputerTofu[7]网络包头均为32Bytes),在相同物理链路带宽条件下,可有效提高有效负载带宽.此外,为使主机可以识别全局资源,实现处理器对节点内资源的高效访问,通信协议支持全局统一地址空间,对超节点内全局物理内存和I/O地址空间统一编址,使得内存及I/O资源均拥有全局唯一的地址.全局地址空间避免了系统中复杂的地址映射和变换关系,用户可以直接访问系统资源,能够简化系统设计,提高系统性能.由于地址空间全局可见,为防止误操作或恶意进程造成的非法访问,DMA引擎还采用了简单高效的地址保护机制:通过绑定密钥来限制访问权限,接收方会检验网络包包头的QPMagic字段(如图6所示),只有与通信建立阶段协商一致的密钥匹配才允许数据传输,否则视为非法访问,接收方拒绝数据请求,进而实现安全、隔离的用户访问.3.3高性能通信接口3.3.1高速PCIe接口网络接口控制器不仅要实现多处理器间的互连通信,也要实现I/O设备的扩展.本文的网络接口控制器端口选择PCIe总线,可充分利用PCIe高带宽、低延迟的传输性能,高可靠的链路层通信和丰富的服务质量支持等方面的优势.通过实现兼容PCIe的网络通信协议(见第3.2节),在无缝兼容现有PCIe设备的前提下,将PCIe总线扩展至处理器间互连领域.3.3.2用户级通信本文设计了用户级通信接口降低通信过程中的软件开销.用户级通信[8]也被称为操作系统旁路,通过消除操作系统转发引入的内存拷贝,实现应用程序对底层硬件设备的直接访问.本文实现了基于QP(QueuePair:队列对)的用户级通信接口,QP包括发送队列(SendQueue)、接收队列(ReceiveQueue)和控制队列(ControlQueue),其中发送队列缓存需要执行的发送请求,接收队列缓存接收到的数据,控制队列用于缓存发送队列和接收队列所需的完成事件通知.需要说明的是,控制队列仅实现了硬件向应用程序的通知机制(发送或接收完成),应用程序向硬件的通知(启动一次发送操作)采用了门铃(Doorbell)机制(详见第3.4节).QP的队列存在于主机内存,与应用程序或通信进程绑定,因此应用程序通过操作其独占的QP实现对硬件的访问.3.4高效通信原语3.4.1通信原语定义通过定义高效的硬件原语,网络接口控制器可实现通信协议的卸载(Offloading),提高通信效率.消息传递编程模型(MPI)是目前高性能计算应用中最常见的编程模型,是高性能计算领域事实上的标准,因此cHPP控制器主要针对MPI的两种通信方式进行加速:适用于小消息的eager通信模式和适用于大消息传递的rendezvous模式,抽象出NAP(Non-AddressPacket)、PUT、GET等通信原语.无地址包(NAP)是为了加速小消息通信抽象出来的通信原语.NAP命令只需知道目标节点信息,不需具体地址信息,因此叫做无地址包.DMA引擎将接收到的NAP消息存放在内存中事先分配好的接收缓冲区内,再将其复制到用户程序空间,主要用于少量数据和控制信息的传递,可以用来实现MPIeager通信和同步操作.NAP操作有两种:立即数和间接数,NAP立即数是指描述符中直接包含待传输的数据,而NAP间接数是指描述符中只包含有需要传输数据的地址和长度信息,两种类型是通过描述符中的类型域区分的.PUT通信原语用于大数据量消息的传输,可看作是大块内存写操作,需要事先通过握手协商获得内存地址信息,可以直接将数据写入到用户程序的内存空间.PUT通信原语支持MPI的rendezvous通信和MPI-2的put操作,采用Chain-DMA描述符,可支持复杂的通信模式.GET通信原语和PUT类似,只是数据流是反向的,用于大数据量消息的读取和写回.图7为PUT和GET操作的示意图,在启动PUT和GET操作之前,发送方均需要2次NAP操作协商传输的源数据和目标地址等信息.对于PUT操作,从发送方读取的数据将封装为若干PUT数Page6据包发往目标节点,待数据传输完成接收方发送NAP消息告知通信完成;而GET操作中,发送方则直接将协商获取的地址等信息打包为GET数据包发送,接收方对GET数据包进行解析,根据所得信息,将发送方GET操作重构为接收方的PUT操作后,执行该PUT操作流程.3.4.2原语启动机制通信原语的启动,依赖于主机向DMA引擎提供包含源数据和目的地址等相关信息.DMA启动方式可以分为描述符启动和门铃启动两种.在描述符启动方式中,处理器直接将描述符写往DMA引擎,DMA引擎根据描述符信息去读取内存数据.描述符启动的优点在于启动速度快,DMA引擎接收到描述符后可以直接发起内存读取操作,缺点是写描述符过程占用处理器时间,且原子性差,为保证写描述符过程中不被其他DMA描述符打断,增加了软件的加锁操作开销.在门铃启动方式中,用户进程先将描述符存储在指定内存中,通过向对应的门铃FIFO(FirstInputFirstOutput)写入门铃信息启动DMA操作,门铃承载描述符在内存空间的具体信息,由地址域和长度域构成:地址域是描述符所在的物理内存空间首地址,长度域表示描述符的长度信息.门铃操作触发描述符命令单元读取描述符内容,DMA引擎得到描述符后再根据描述符信息去读取数据进行传输.门铃启动的优点在于操作的原子性,由于描述符内容较少,因此主机只需写一次门铃寄存器(在网络接口控制器中)即可,不会被其他操作所打断.同时可以使描述符的格式更加灵活和复杂.门铃启动的缺点在于启动速度慢,相对于描述符启动方式,需要先读取描述符,额外引入了一次内存读取操作.通常DMA引擎是被所有应用程序所共享的,当多个进程或线程希望同时使用DMA引擎时会引起竞争.因此,DMA启动操作应该是“原子”的,是指一次DMA操作不会被其它的DMA操作打断.通过门铃启动实现DMA引擎的虚拟化可以避免竞争.门铃启动机制可支持更多的进程高效地进行DMA操作,既保证了DMA启动的原子性,又实现了良好的扩展性.而且,在高性能计算中,进行大数据量的传输时,门铃启动增加的额外开销相比软件加锁开销并不显著.特别是在多核多进程的情况下,门铃启动的原子性尤为重要,因此本文的网络接口控制器DMA引擎选用门铃启动的方式.描述符定义了DMA操作的必要信息,由用户进程事先写入指定内存空间.描述符命令单元根据门铃信息,将描述符从主存读取到DMA引擎,DMA引擎根据描述符判断DMA操作的类型,获得数据传输的控制和数据信息.如图8所示,Chain-DMA型描述符在硬件包头信息域定义了DMA操作的类型、控制信息和地址保护等信息;源数据信息域和目标数据信息域指定了数据的源地址和长度信息以及接收方数据存储信息,采用队列形式的数据结构可以实现灵活的数据传输;软件包尾由上层软件使用并对硬件透明.Chain-DMA型描述符优化了虚地址连续、物理地址离散的数据块的传输,只需读取一次描述符即可实现任意源地址、任意长度、任意目标地址的数据传输.同时Chain-DMA只需源数据信息域队列中的总长度与目标数据信息域队列中的总长度相等即可,并不要求队列中每一项均相等,因此可灵活的支持多个页面的大量数据传输,对于大数据通信有加速作用.3.4.3执行流程及死锁避免通信原语的执行流程开始于DMA根据门铃信息读取描述符信息之后,DMA引擎根据描述符类型域信息执行相应的操作:(1)NAP.将描述符中的数据(立即数)或根据描述符中源地址读回的数据(间接数)封装成一个NAP网络包发往网络,由于NAP网络包不含目标地址信息,接收方的接收引擎接收到NAP网络包后,先向本地申请缓存地址,再将数据写入数据接收缓冲区;(2)GET.图9中标号①至⑤是节点a发送Page7GET描述符向节点b读取数据的流程———发送引擎将GET描述符直接作为数据封装为GET网络包发往节点b(图9①和②),节点b接收引擎从网络包提取到GET描述符后转换为本地发起的PUT操作(图9③),节点b的发送引擎将所请求的数据以PUT网络包的形式发往节点a(图9④),最后节点a根据PUT网络包包头所携带的目标缓冲区地址域信息(在握手过程中获取)将数据直接写入目标进程的接收缓冲区中(图9⑤);(3)PUT.上述GET是转换为PUT实现,因此PUT的主要执行流程已在GET中描述(图9④和⑤).当通信双方同时发起PUT或GET操作时,图9中的执行流程(将GET转换为接收方PUT执行)将存在死锁.如图9中的②至④(节点a发起的GET操作)和⑦至⑨(节点b发起的GET操作)形成环路,而死锁的原因在于GET网络包与PUT网络包形成的网络资源竞争.针对该问题,本文采用虚通道来避免死锁,如图10所示,通过为GET设置独立的虚通道,解除了GET网络包与PUT网络包的网络资源竞争.3.5I/O高效共享在云计算环境下存在大量并发通信请求,这对通信接口提出了更高要求:高性能通信,即承载大量通信请求的能力;其次是高效共享,在极小损失通信效率的前提下,实现多虚拟机对I/O资源的共享访问.高性能计算与云计算对于网络接口控制器在性能方面的需求是一致的,即包括更高的峰值能力和更高的实际通信能力.同时,还应兼容现有PCIe设备,设计支持SR-IOV协议,实现超节点内部的I/O资源在多虚拟机间的高效共享.DMA引擎支持SR-IOV规范,可被虚拟为若干个DMA引擎,每个虚拟DMA引擎都可以被当作独立的设备分配给虚拟机使用,实现虚拟机间对DMA引擎的充分共享.每个虚拟机可通过DMA引擎直接访问I/O资源,从而实现多个虚拟机的用户级I/O高效共享.每个虚功能(VF)拥有多个QP,对应一个独立的门铃FIFO来缓存DMA请求.通过仲裁模块对每个门铃FIFO的请求进行仲裁,基于优先级仲裁可实现VF之间的差异化服务.4网络接口控制器实现如图11所示,网络接口控制器主要由PCIe端口、PCIe配置空间、门铃启动窗口、描述符命令单元和DMA数据发送及接收引擎构成.PCIe端口采用标准PCIeGen2链路.PCIe配置空间则实现了PCIeSR-IOV功能.门铃窗口模块用于接收启动DMA操作所需的门铃信息,门铃将被描述符命令单元解析,并根据解析结果从主存中获取DMA描述符.DMA描述符则提交给DMA发送引擎,DMA发送引擎根据描述符获取源目的信息后,进行数据的读取及发送.DMA接收引擎则负责数据的接收,解封装并将数据送到相应的内存空间.DMA操作需要在内存空间开辟3块缓冲区:发送完成事件缓存用于通知主机本次数据发送完毕;接收完成事件缓存用于通知数据接收成功;而接收数据缓存用于存放接收的数据,只用于NAP操作.每个缓存都是Page8逻辑上的环形结构,每完成一次DMA操作都会将指针指向下一项.4.1DMA发送引擎DMA发送引擎整体框图如图12所示,它负责接收主机发来的门铃,并对门铃请求进行响应,根据门铃读取描述符,然后再根据描述符的相关信息读取数据,并打包成网络包进行传输.门铃模块包含8个FIFO分别用于存储8个功能(PF和7个VF)对应的门铃,每个FIFO深度为32,即每个功能最多能同时支持32个DMA请求.DMA发送端的流控则交由软件负责,保证每个功能同时发起的DMA请求不超过32个.描述符读取模块负责读取门铃模块中的FIFO,并根据门铃中的内容生成读取描述符的PCIe读包.描述符提取模块负责接收并处理读取描述符的返回包,在接收到返回包后,首先从PCIe包中提取出描述符,在描述符提取出来后再根据描述符的种类进行处理.对于PUT描述符和NAP描述符,模块将描述符中的控制信息、源数据信息和目的信息分别提取出来并缓存到相应的FIFO中.对于GET描述符,模块则直接将其打包为GET网络包并发往GET交叉开关传输.除了本地的描述符外,描述符读取模块还接收由上传模块接收到的GET描述符,并将GET描述符转换为PUT描述符后存入相应的FIFO中.数据读取模块根据从描述符中提取出来的源数据信息,生成相应的PCIe内存读包.具体的生成规则同描述符读取模块.数据提取模块负责接收返回的包含源数据的PCIe返回包,并从中提取出相应的源数据.数据重排序模块负责数据整合,方便网络包打包模块使用.由于PCIe协议对于单次内存读取的限制,一个源数据项可能会对应多次内存读取,而一次内存读取也可能对应多次返回.因此数据提取模块提取出的数据是不规则的,为了减轻网络包打包模块的负担,加快打包速度,源数据重排序模块负责将提取出的源数据按照网络包打包模块的要求进行重排序.网络包打包模块负责根据描述符控制项和目的项的信息,将读取的源数据进行打包并发往交叉开关(intraDMA交叉开关).4.2DMA接收引擎DMA接收引擎主要负责接收网络包,并根据网络包的类型进行相应的处理.主要由接收分发模块,数据上传模块和缓冲区管理模块等模块构成,具体结构如图13所示.接收分发模块负责从IntraDMA交叉开关中读取网络包,并根据网络包类型交给相应的模块进行数据处理.对于PUT包和NAP包,分发模块将其交给上传模块中的PUT包处理模块和NAP包处理模块进行处理,而对于GET包,则从中提取出描述符并写往本地发送引擎的描述符提取模块进行处理.PUT数据上传模块负责处理PUT网络包,从缓冲区中读取PUT包将其拆分成多个PCIe内存写包上传.在遵循PCIe协议规定及地址对齐要求等前提下,用尽可能少的逻辑资源实现高效上传.PUT包和NAP包共用接收完成事件环,若需要上传接收完成事件,则需向缓存管理模块申请接收完成事件环地址;如果不要求,则可以直接将PUT包转换为PCIe内存写包上传.NAP数据上传模块负责处理NAP网络包,并将其转换为PCIe标准写包发往缓存空间.NAP包上传过程与PUT包上传过程相似,只是NAP包中无地址信息,因此需要从缓冲区管理模块中读取相应的缓存地址.NAP包必须上传接收完成事件,因此必须先申请得到接收完成事件地址才可以开始上Page9传;PUT包目的地址可以是任意的,NAP包目的地址必须是2KB对齐的.缓冲区管理模块负责仲裁所有NAP数据接收缓冲区地址的读请求,采用请求/应答机制.系统初始化期间分配一定数目的缓冲区地址进行缓存,当接收缓冲区不足时,向缓冲区管理模块发出读地址请求,从内存中读取新的NAP接收缓冲区地址并更新RAM中缓存的NAP地址.缓冲区管理模块还负责维护接收完成事件环的流控.完成事件缓冲区:NAP与PUT共享一个接收完成事件缓冲区,缓冲区首地址在初始化时由主机告知网络接口控制器,缓冲区逻辑上呈环形结构,可循环使用.完成事件队列是一个有限项数的环形缓冲区,每上传一个NAP消息,需要向接收完成事件缓冲区写入一个接收完成事件;每上传一个PUT包则根据标志位决定是否向接收事件缓冲区写入接收完成事件,通过流控计数器防止接收事件队列的溢出.4.3I/O共享实现DMA引擎支持SR-IOV功能:DMA引擎将虚拟出1个物理功能(PF)和7个虚功能(VF)与相应的配置空间和用于通信所需的资源“QueuePair”(QP)建立映射关系(每个功能对应4个QP),使每个虚功能可以分配给不同的虚拟机使用,实现I/O设备的高效共享,如图14所示.同时通过QP和虚功能配置空间使处理器获取若干“虚拟DMA引擎”,这些虚拟DMA可供不同虚拟机使用,并实现安全隔离功能,也即实现了“DMA虚拟化”功能.处理器可以通过DMA引擎对全局统一编址的I/O资源进行直接访问,实现I/O设备的高效共享.5原型系统和性能评测实验基于XilinxVirtex6X365T实现了原型系统,如图15所示,中间风扇下即为网络接口控制器的FPGA原型芯片,顶部插卡为具备SR-IOV功能的Intel82599以太网卡,紧邻FPGA的PCIe线缆连接到另一个主机,作为处理器节点与控制器连接.为了平衡网络接口控制器的逻辑规模和工作频率,根据FPGA的结构特点,网络接口控制器的内部总线设为128bits,工作频率250MHz.为与内部总线带宽相匹配,原型系统的所有PCIe接口均选择PCIe2.0×8(峰值5GB/s,使用8b/10b编码机制,即有效传输带宽为4GB/s).XilinxVirtex6Lx365t芯片共包含56880个Slice,416块BlockRAM,网络接口控制器实际设计消耗资源:Slice消耗量为总量的8%,BlockRAM为总量的6%.5.1峰值带宽测试在PCIe协议中,单个PCIe包所能携带的数据量,即最大传输单元MTU(MaximumTransferUnit),是受设备限制的.对于不同的MTU,PCIe包头所占据的开销比例会受影响.在本实验中,将单次内存读取所能请求的数据量大小与MTU设为相同,即通过改变MTU的值,研究PCIe协议对于DMA引擎峰值带宽的影响.测试在请求2MB数据情况下,MTU变化对带宽的影响,采用带宽效率最高的PUT包测试峰值性能.从图16中可以看出,在MTU<128Bytes时,DMA引擎的峰值带宽随着MTU呈线性增加,而在MTU为128Bytes时达到顶峰,接近3.2GB/s的实际带宽性能上限:16BytesPCIe包头占1个周期,返回最大传输数据128Bytes占8个周期,DMA引擎处理需要1个周期,数据返回的效率为80%.因此PCIe协议的限制传输带宽上限为4GB/s×80%,即3.2GB/s.而当MTU>128Bytes之后,峰值带宽却不再增加,这是由于PCIe协议是通过包头Tag域来分辨返回包所对应的是哪个内存读请求所发起的,因此,在发送前要申请到Tag号才能上传.PCIe协议默认Tag域的高3bits是保留的,只有低5bits有效,即可用的Tag数目为32个,不能随着MTUPage10的增大而相应增加,进而导致带宽难以达到设计的理论性能.DMA发送引擎的描述符读取模块,数据读取模块和接收引擎中用于读取缓存区地址的模块需要申请Tag号进行内存读取操作.因此在设计中,需要对Tag进行统一的分配和调度等方式以免Tag被重用.对于读取内存的模块,满足持续提交PCIe内存读请求所需的最少Tag数目如式(2)所示.n=mintSizepayload16+2×SizepayloadMTU其中t为从内存读取模块得到Tag号并生成PCIe读内存包开始,到响应数据包完全返回所需要的时间,单位为时钟周期,此后,相应的Tag号被释放,回收后可循环使用.Sizepayload则为单个PCIe包所能请求的最大数据量(单位为Byte),Sizepayload/16是所请求的数据返回所需要的时钟周期数(位宽16Bytes),Sizepayload/MTU是指所请求的数据被封装为PCIe包的个数,而每个数据包都需要一个时钟周期封装包头和一个周期的处理时间,因而要乘以系数2.公式表明在这段时间内n个Tag足够使用,可以连续发送数据请求而不必等待Tag返回.当Sizepayload增加时,维持最高性能所需的Tag数目也会随之减少,但是在实际的系统中,由于内存返回数据是与Sizepayload成比例的,且存在竞争等不确定性因素,t可能会需要成百上千个周期,仅32个Tag很难满足持续高效的数据读取.为了增加读取效率,可以通过两种方法提高性能:(1)将PCIe设备中的PCIExpressCapabilityStructure中DeviceControl寄存器的ExtendedTagFieldEnable位置位,使Tag数目扩大为256个,但是该功能需要PCIe核的支持;(2)增大Sizepayload,该功能需要设备本身及其PCIe桥支持大的数据请求量.5.2带宽随负载变化情况由于实际系统MTU为128Bytes,Tag数目为32个,因此,在该条件下测试了带宽随负载变化的情况.如图17所示,当数据量较小时,PUT包的带宽随负载的增加而显著增加,这是因为包头所占比例迅速减小所致.但当负载增加到一定的程度之后,带宽增加变缓,此时负载所占比例的增加已经不够明显.当负载的数据量增加到512KB时,实际速度已达3.19GB/s.GET包类似PUT,但数值略小,是因为接收方会将GET描述符提取给发送DMA引擎,转化为本地PUT操作,延迟会增加.由于NAP包长最大为2KB,因此只分析到传输数据最大2KB.如图17所示,NAP包的带宽均随负载而增加.在负载很小时NAP立即数包的带宽最大,这是因为NAP立即数将待传输的源数据封装在描述符中,可直接提取发送,相比其他操作减少一次内存读取.而随着负载的增加,两种NAP操作的差距开始减小,这是因为描述符为64bits,每两个周期才能提取一个周期的NAP立即数,而对于NAP间接数,内存直接返回128bits的PCIe包,二者开销近似,因而带宽效率开始接近.5.3延迟随负载的变化情况表2描述了各种网络包在不同负载情况下的延迟情况.考虑到描述符大小对于延迟的影响,将源数据以2MB为单位传输,即使用最小的描述符.从表2可知,对于PUT包,当数据量最小1Byte时,延迟最小,达到1.762μs,而延迟随着数据量的增加而线性增加.对于GET请求,最小延迟为1.838μs,是因为接收方会将GET请求转化为本地的PUT操作,因而延迟略大于PUT包.NAP立即数包的延迟最小仅1.242μs,这是因为其源数据直接包含在描述符中,相对其他类型减少了一次内存读取,因此延迟最小.而NAP间接数最小延迟为1.846μs.延迟略Page11Payload/BytesNAP立即数NAP间接数PUTGET11.2421.8461.7621.838641.2541.8621.7781.8542561.4582.0181.8381.91410242.5142.8582.0782.15420483.9223.9782.3982.474409616384655362621441048576大于PUT包,这是因为NAP包没有指定目标地址,在接收端需要申请缓存地址,也需要一次内存读取,因此延迟会大于PUT包.图18显示了NAP立即数和PUT两种网络包在发送1Byte数据负载条件下的延迟情况,时间从图18传输延迟分析5.4吞吐率分析在不同端口相互通信的过程中,随着通信量的增加,连接通信接口的交叉开关会变得拥堵,进而导致DMA传输请求响应时间变长,即延迟增加.增加虚通道数量可有效缓解“队头阻塞”的影响,因此,实验测试了不同端口数配置不同虚通道条件下的通信吞吐率情况.为避免请求和响应导致的死锁,规定GET数据包需要走专用虚通道.为提高吞吐率,本节采用传输效率高的PUT数据包进行测试,因此,这里不考虑GET专用虚通道.定义虚通道的使用策略Dest-Mod:如果虚通道数量为n,而消息的目的端口是d,则消息将会被缓存在虚通道[dmodn]内.图19为4端口2VC(VirtualChannel:虚通道)(每个端口2VC,下同)、4端口4VC、8端口2VC、8端口4VC和16端口4VC条件下采用所设计的基于PCIe标准的通信协议进行互连通信时,延迟和带宽的关系.延迟从写门铃开始计算,到接收端全部DMA接收引擎接收到主机发起的门铃开始,到DMA发送引擎将网络包转换为PCIe数据包上传数据为止,其数据流程与第4节所描述的一致,其中Crossbar延迟是节点内的交换延迟.对于NAP数据包需要请求缓存地址,而虚线框表示DMA会预先读取并保存一定数量的缓存地址,以避免频繁发起地址请求.其中描述符和数据读取和返回的延迟受内存控制器的响应速度和资源竞争的影响是动态变化的,图18中的“130”是经验值.若不考虑内存数据读取的延迟,DMA引擎的处理延迟仅35(NAP)到48(PUT)时钟周期.尤其在Crossbar后的DMA上传阶段,NAP操作(当缓存地址足够时)和PUT操作均将数据直接打包为标准PCIe数据包上传,简化了繁琐的协议转换.接收并处理完网络包结束,最后对所有请求的延迟进行平均.吞吐率定义为传输稳定之后,对每个端口的带宽求平均并除以理论峰值.从图19中可以看出,在端口数目一定的情况下,吞吐率随着虚通道数目增加而增大;在虚通道数目一定的情况下,吞吐率则随着端口数目的增加而减小.4端口时,在每端口2VC的情况下,当吞吐率接近最大理论带宽的65%时,延迟开始迅速上升,此时通信系统的吞吐率为2.6GB/s.在每端口4VC的情况下,系统的吞吐率达到2.8GB/s.带宽较2VC有所提高是由于在4端口4VC的情况下构成VOQ结构[9],可消除队头阻塞问题.没有达到实际性能上限,是因为仿真环境的随机性不足,没有达到完全均匀随机分布.考虑到4VC相对于2VC仅提高了5%的性能,综合资源消耗和性能的考虑,4端口下2VC的设计在性能和资源上都能达到较好的平衡.Page125.5I/O虚拟化性能分析在设计中,虚拟机通过QP可以同时使用多个网卡,为实现合理高效的资源共享,使每个虚功能都能公平的使用系统资源,采取了公平的仲裁策略.针对各个虚功能的门铃请求,实现了公平的调度策略.图20为各个VF分配的带宽随时间的变化情况.测试中使用随机大小的PUT包,并随机的将请求分配给各个功能.测试结果如图20所示,不同曲线描述的是VF0~VF6和PF所分配到的带宽情况.纵轴曲线之间的间隔表示不同功能所占带宽的百分比,横轴为仿真时间,单位为时钟周期右侧标名曲线.从图中可以看出,传输刚开始时,带宽的分配变化很剧烈,这是因为DMA引擎刚开始工作时,已传输的数据量较小,因此单次请求传输的数据占总数据量的比例比较大,因而对带宽的分配有着很大的影响,但很快各个VF之间带宽分配的比例就基本达到均衡.大概在150万个时钟周期带宽分配达到稳定状态,很好的达到了公平分配带宽的目的.6相关研究节点控制器是高性能互连网络的核心组件,是提供高性能通信的重要保障.全局地址统一编址、提供高效通信原语、用户级通信等多种关键技术被许多高性能计算机的大规模互连网络所采用,例如:IBMBlueGene系列[5-6,10]在处理器内集成了高性能网络路由器,其网络接口支持直接PUT和远程GET;Cray的Gemini[11]和Aries[12]系列互连网络的网络接口也在硬件层次提供了支持小消息传输的快速消息访问(FMA)和长消息传输的块传输引擎(BTE);商业网络如Infiniband[4],其主机通道适配器(HCA)提供硬件支持的RDMAPUT/GET操作、采用IPV6兼容的128bits全局地址以支持远程直接内存访问.由于这些网络接口控制器的设计均面向大规模互连网络,因此一次端对端通信要涉及I/O总线协议-网络协议-I/O总线协议间的转换流程(解析→打包→解析→拆包),其消息格式定义也需要涵盖大规模数据传输所需的子网管理、路由、拥塞控制等信息(例如Infiniband的包头最大可达到94Bytes),限制了网络处理的效率和有效负载带宽的提升.然而在规模限定的局部互连中,上述开销均为无效开销,本文正是通过面向局部通信的互连协议和DMA引擎结构设计,实现对上述开销的优化.此外,在局部互连网络中,存在着网络互连和I/O设备扩展两种功能需求,本文通过扩展PCIe协议实现的互连网络,实现了在物理层上两种功能的融合,这是面向大规模互连网络的网络接口控制器所不具备的.正是上述功能融合的需求,国际上已有利用PCIe实现处理器间互连的工作,例如日本瑞萨公司的PERAL[13]使用PCIe作为通信链路设计了功耗感知的,高可靠和高性能的互连芯片,最低传输延迟1.2μs,但带宽仅为1.1GB/s(理论峰值的55%).Dolphin公司使用增强的PCIe互连方案[14]来实现多主机间通信和主机到I/O通信的功能,由于同时面向大规模机群系统,因此它并没有进行针对局部互连的优化,其传输延迟为14μs,带宽为1.27GB/s.表3比较了cHPP网络接口控制器与上述经典互连网络①以及采用PCIe标准的商业网络[13-14]的性能.从表3可以看出,cHPP网络接口控制器的FPGA原型系统在带宽的绝对性能方面(绝对带宽与实现工艺相关)低于Cray的高性能互连网络,但超过同类型的PCIe互连网络(PERAL和DolphinExpress),同时可以获得与峰值带宽相同的Infini-Band(QDR)相近的性能.上述网络接口控制器在延①Networkbandwidthsandlatenciesforsomenetworks.Page13迟方面的性能相似,根据图18的延迟分析可知,若提高cHPP控制器的工作频率,其硬件延迟还可进一步降低.NetworksBandwidth/(GB/s)Latency/μsInfiniBandQDRCrayGeminiCrayAriesBlueGene/QDolphinExpress7结论和展望cHPP体系结构采用超节点设计可支持多种网络拓扑,超节点结构可减少通信路径,缩减网络规模,并且充分利用了超节点内部通信的局部性,可有效降低通信延迟.超节点控制器采用PCIe高速接口提供高带宽和高可靠的链路性能.网络接口控制器支持用户级通信和高效通信原语加速大数据量传输,降低处理器的占用率,进一步提高计算能力.基于硬件支持超节点内I/O资源的高效共享,在极小性能损失的条件下可满足大量并发的通信请求.下一步工作主要是在节点内通信的基础上进行系统的级联扩展,支持高维度网络拓扑的大规模直接网络,实现全系统节点间处理器的高速通信;针对MTU和Tag等限制因素进一步优化DMA引擎结构,改善通信性能;拓展I/O虚拟化性能,实现不同节点间的处理器对全局I/O资源的高效共享;并根据不同层次的通信需求,通过调整QP资源在虚拟机和VF之间的动态分配来实现丰富的QoS服务.致谢感谢李强博士在文章撰写过程中关于通信原语和上层软件库之间关系的深入有益的讨论!
