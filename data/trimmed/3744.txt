Page1复杂文本网数据的主题建模进展曾嘉1),2)严建峰1)龚声蓉1)1)(苏州大学计算机科学与技术学院江苏苏州215006)2)(复旦大学上海智能信息处理重点实验室上海200433)摘要文中介绍了大规模文本网数据的主题建模研究的特点和近年来的重要进展.主题建模方法吸引了世界范围的广泛兴趣,并且促进了许多重要的数据挖掘、计算机视觉和计算生物应用系统的发展,包括文本自动摘要、信息检索、信息推荐、主题检测和追踪、自然场景理解、人体动作识别以及微阵列基因表达分析等.文中重点介绍文本网数据的4个主要特点以及对应的主题模型.文本网数据含有动态、高阶、多通路及分布式的结构,而之前的主题模型仅对部分的结构进行建模.而文中讨论了在三维马尔可夫模型的框架下统一对文本网数据的4个结构特点进行建模,并分析了结合三维马尔可夫模型和二型模糊系统对分布式单词计算和主题建模应用的可能性.除了对文本网数据的结构建模之外,还讨论了一些对三维马尔可夫模型能量最小化的机器学习算法.关键词主题模型;文本网数据;数据挖掘;单词计算;二型模糊系统1引言随着互联网和相关技术的快速发展,信息呈现爆炸式的增长,对大规模电子文档(例如海量科技文献、政府文件、社区网页和新闻存档)进行自动分析的需求与日俱增.有效地组织、管理、理解和摘要大规模文档,才能充分发现并利用文档中潜在的知识,使得人们更加便利、准确地获取所需信息.文本网数据(documentnetworkdata)涉及文本相互引用网(citationnetworks)、文本共标签网(co-tagnet-work)、超链接网页(hyperlinkedWebpages)、社交网(socialnetworks)等.文本网数据中的节点通常表示文本或者图像,节点之间的链接包含引用、超链接、共标签和文本内容相似度等多种类型.网数据是目前应用统计和机器学习的一个重要研究对象,对网数据的建模和分析方法凝聚了跨学科的努力,在自然科学、社会科学以及信息科学有广泛的应用[1-3].统计分析大规模文本网数据不但能提供有用的预测模型,同时能够提供对大量数据的统计刻画.预测模型能够同时预测网数据中节点的内容(例如文本的单词)以及节点和节点的链接关系(例如社交网中的朋友关系、科技论文的引用关系以及网页之间的链接关系等).统计刻画可以直接揭示隐藏在网数据下面的聚类或者社区结构,因此有助于发现潜在的知识层次.主题模型(topicmodels)是一种全新的统计模型,它利用快速的非监督机器学习算法来自动组织、理解并且摘要文本的主要内容,是解决这类问题的新方法.主题模型是一种用于分析大规模文本的层次贝叶斯模型(hierarchicalBayesianmodels),它利用快速的非监督机器学习算法从高维稀疏的单词数据中提取低维的数学表示,从而对文本单词进行聚类,是当前数据挖掘和机器学习领域关注的热点之一[4-15].探索高精度的主题模型及其快速学习算法有两个实际意义.首先,高精度的主题模型可以解决许多传统文本或图像分析中的关键应用问题,例如文本图1随时间演化的文本网数据示意图主题检测及追踪(topicdetectionandtracking)[16]、自动文本摘要(textsummarization)[17]、信息检索(informationretrieval)[18]等.其次,高精度的主题模型可以自动从大量的科技文献中发现并且预测潜在的重要的研究方向,节省研究人员宝贵的前期调研时间和经费,将在生物、物理和化学等研究领域发挥重大作用[19].然而,传统的主题模型LatentDirichletAllocation(LDA)[7,10]仅关注文本之间的内容相似度信息,却忽视了文本之间存在的大量外在联系,例如引用(citation)、超链接(hyper-link)和共标签(co-tag)等,从而会影响抽取的主题在语义上的精度和稳定度.因此,本文将在分析文本网数据的主要特点的基础上,对当前主题模型和机器学习策略的发展进行综述,并讨论设计新型主题模型以及开发相应的快速机器学习算法的新思路,以期刻画并利用文本网数据丰富的结构特点来进一步提高主题建模的精度和稳定度.2复杂文本网数据的4个特点复杂文本网数据主要有以下4个特点:动态(dynamic)、高阶(higher-order)、多通路(multiplex)和分布式(distributed).这4个特点对改进基于LDA的主题模型有如下4点意义:(1)文本网数据具有时间戳来表示文本发布的时间,这样主题会随着时间动态演化(evolution),通过网络结构相互影响,或者产生新的主题,或者消亡旧的主题.对文本网数据的动态主题建模可以应用于新主题的预测或者在线主题爆发(onlineburstytopics)的检测.图1是一个文本网数据在2007~2009年的变化的例子,其中黑色小人表示文本的作者.可观察到每一个时间片段的网数据链接结构不同,因此需要刻画其中动态结构的细微变化,来反映主题分布随时间的演化或者依赖关系.同时,刻画时间对主题分布的影响可以更好地建立预测模型,动态地揭示未来潜在的重要的研究方向,节省科研人员宝贵的文献调研时间.Page3(2)共标签文本网数据(tag-documentnetworkdata)广泛存在大量特定的高阶结构.标签通常可以注释文本某一方面的属性.例如“作者标签”描述了文本的作者属性,“时间标签”表明了文本何时发表.如果用视觉单词(visualwords)来表示图像,那么图像也可以像文本一样具有层次结构.用户一般用标签手工注释图像的局部内容,例如标签“人物”、“建筑物”或者“树木”等.通常,共同标签的文本之间的主题结构不仅仅局限于二阶(2-orderorpairwise)关系即两个文本之间存在链接,还广泛出现三阶甚至四阶的高阶关系,即3个文本或者4个文本中,每两个文本之间都共标签.对文本网数据的高阶结构建模可以描述更加丰富特定的主题结构特征,从而可以抽取不同层次的主题.图2显示了普遍存在于标签文本网数据的高阶结构,(a)文本的作者作为标签;(b)图片的注释作为标签;(c)标签网数据的二分图表示;(d)可以将文本δ2的特定主题从文本并集的主题{δ1,δ3,δ4}区分开的高阶结构.通常,一个文本拥有多个标签.通过多个标签对文本进行标注,多个文本形成了有别于二阶关系的高阶关系.可图2标签文本网数据中的高阶关系(3)文本节点之间通常存在多种链接类型,即从一个节点到另一个节点存在不同类型的通路.融合多种类型的通路信息进行主题聚类可以细致地反映不同通路的网结构对主题建模的影响,从而发现特定关联稳定的主题.图3是一个多链接的文本网数据示意图,其中含有两种文本之间常见链接类型:相互引用(箭头)和共同作者(黑色人形图标).通常,不同链接类型对主题的分布影响不同,因此需要分别刻画建模并且有效融合不同通路的影响.其相互引用文本之间通常显示出很强的主题依赖性,例如“数据挖掘”主题常常引用“机器学习”主题的文本.共同作者的文本却往往显示出很强的主题相似性,例如相同的作者在“机器学习”领域常常发表与“机器学习”相关的文本.很显然,在大多数情况下这两种链接对最终主题的形成有不同的影响.因此,需要对网数据的每一种通路分别进行刻画,以便达到最好的主题建模精度和稳定度.在实际应用中,当通路以用二分图(如图2(c)显示)来抽象表示共标签文本网数据.高阶关系一般对主题建模有积极的影响.例如图2(d)显示,作者{γ1,γ2}合作写了以“机器学习”为主题的文本δ1,表示为两个圆圈{γ1,γ2}的交集.类似的,作者{γ1,γ3}合作写了以“计算机视觉”为主题的文本δ3,作者{γ2,γ3}合作写了以“数据挖掘”为主题的文本δ4.他们3个人共同合作写了文本δ2,表示为3个圆圈{γ1,γ2,γ3}的交集.理论上,如果简单地将多个标签组成的高阶关系{γ1,γ2,γ3}分解表示成为3个二阶关系的并集{{γ1,γ2},{γ1,γ3},{γ2,γ3}},就可以得出一个结论:文本δ2是关于3个主题“机器学习”、“计算机视觉”、“数据挖掘”的并集.然而,实际上文本δ2很可能是关于一个全新的主题,例如“计算生物学”,是3个圆圈{γ1,γ2,γ3}的交集.因此,十分有必要对高阶关系建模,用来区分文本δ2不同于文本{δ1,δ3,δ4}并集的特定主题.除了标签文本网数据,具有超链接的网页以及文本之间的相互引用也存在类似的高阶结构.例如,3个网页相互之间都存在超链接以及3个文本同时引用同一个文本等.类型繁多的情况下,如何平衡各个通路对主题分布的影响是一个极具挑战性的工作.目前,尚未有完备的理论支持多通路网数据分析,很多情况都是依靠设计者的先验知识,这往往导致对应主题模型和机器学习算法缺乏良好的泛化能力.(4)大规模文本网数据通常分布在不同的数据服务器上,每一个服务器仅存储部分网数据信息,从而需要高效的信息融合技术对分布式网数据进行主Page4题建模.图4显示了网数据分布存储在3个服务器上,在每一个服务器上的网数据都只能反映局部的结构信息,因此要求我们对分布式网数据开发并行的主题学习算法.因为在每一个服务器上网数据都反映局部信息,我们需要有效的信息融合技术将不同服务器上的主题分布融合成达到或者接近全局最优的主题分布.首先,需要开发并行的分布式主题模型学习算法分别得到每个服务器上的网数据主题分布,然后利用高效信息融合的技术,将所有服务器上的主题分布融合,使得最终的主题分布达到或者接近全局最优.综上所述,根据复杂文本网数据的结构特点来开发新型的主题建模及其相应的机器学习算法对改善主题建模的精度和稳定性有重要意义,近期在数据挖掘和机器学习领域受到广泛的关注[9,13,20-25].同时,研究复杂文本网数据的主题建模可以启发和促进其它类型的复杂网数据的聚类分析,例如基因调控网(generegulatorynetwork)、蛋白质互动网(protein-proteininteractionnetwork)以及交通网(transportationnetwork)等.例如,研究者利用文本网数据互相引用的特征对相关蛋白质互动网的全文文本进行分类,从而发现蛋白质互动网的潜在联系[26].3网数据主题建模的研究进展根据复杂文本网数据动态、高阶、多通路和分布式这4个主要特点,本节将在LDA的基础上,对国内外文本网数据主题建模的研究进展进行分析.3.1LatentDirichletAllocation(LDA)简介大规模文本集的复杂性使得人们广泛使用生成模型(generativemodels)将文本表示为一组混合主题(mixturesoftopics).通常,一个主题被定义为一个在单词表中所有单词上的多项概率分布,用来刻画高度关联的单词聚类(wordclusters),反映文本特定方面的语义内容.图5给出了目前广泛使用的一类主题模型LDA[7,10]的图模型表示,其中空心圆圈表示隐藏的随机变量或者参数,阴影圆圈表示观测变量,方框则代表重复的次数.例如,文本重复D次表示整个文本集,单词在每个文本d中重复Id次,主题分布重复J次.LDA将主题标签z分配给文本d的每一个单词w,这里d是文本在整个文本集中的索引1dD,w是单词在单词表(vocabulary)中的索引1wW.通常,主题标签z=j是隐藏的随机变量(latentvariables),其中1jJ是主题标签索引.当LDA将主题标签分配给每一个单词之后,实际上将所有单词分成了J类,因此主题建模可以被视为一种单词聚类的范例.理论上,LDA基于两个参数来分配主题标签:一个是文本中主题标签的比例θd,另一个是主题在单词表上的多项分布.这两个参数分别由两个基于超参数(hyperparameters)α和β的共轭Dirichlet分布生成.实际中,超参数α和β起到平滑多项分布参数θd和的作用.图5显示LDA可以通过分享同一个主题分布来表示任意两个文本{d,d}之间隐含的联系.本质上,如果两个文本包含相似的单词,LDA则鼓励两个文本具有相似的主题标签配置.然而,我们注意到LDA仅仅用了文本之间内容的相似程度来交换主题信息,并未充分利用文本网数据中丰富的外部链接结构,例如引用或者超链接.这个缺点促使近来发展了许多LDA的变形,利用网数据的结构来调整主题分布,从而改善主题建模的精度和稳定度.3.2动态主题模型时间戳(timestamp)是文本网数据的一个重要特征,可以揭示主题随时间演化的过程[14,16,27-28].目前大多数时间序列文本数据的主题建模使用基于马尔可夫假设的两大类方法:隐马尔可夫模型(HiddenMarkovModels,HMM)和线性动态系统(LinearDynamicSystems,LDS)[29],分别针对离散和连续Page5的主题状态空间.图6显示了3个典型的动态主题模型.如图6(a)所示,dDTM[14]使用动态线性高斯模型来描述主题分布参数t随时间的变化以及超参数αt随时间的变化.但是,dDTM假设所有的主题在所有的时间片段都存在,并没有考虑主题的产生和消亡过程.此外,类似于HMM的假设,dDTM仅仅考虑t在离散的状态空间变化.如图6(b)所示,cDTM[12]进一步将dDTM的主题分布参数t随时图63个动态主题模型范例dLDA[26]与之前动态主题模型不同,它假设主题分布参数t不随时间改变,但是文本的主题混合参数θd随时间改变.OLDA[30]自动适应文本流数据,根据文本流数据在线更新参数,从而学习并捕捉随时间变化的主题,实现主题的实时跟踪和检测.王金龙等人[31]采用两个独立的步骤,首先利用LDA抽取主题,然后分析主题随时间变化的强度来揭示主题的演化.张小平等人[32]提出一种动态更新主题模型的超参数设置方法,在中医临床诊疗数据的实验中,获得领域专家解释性较好的分析结果.有别于以上所有生成模型,DPM[16]利用概率判别模型(discriminativemodel)对在线主题进行检测,着重最大化主题标签在给定文本下的条件概率而不是主题标签和文本的联合概率.但是以上动态主题模型均未利用文本网数据丰富的结构特征,例如引用、超链接和共标签等外部链接等.因此,ITM[3]利用文本的相互引用信息来调节动态主题模型的参数,从而检测出较为均衡的主题演化过程.同时,作者之间的合作关系也可能影响主题分布的演化,并被模拟成为一个马尔可夫过程[33],来表示主题之间的跳转关系.监督式主题模型STM[5]可以被认为是一种特殊间的演化扩展到连续的状态空间.基于类似于LDS的假设,cDTM利用布朗运动时间参数st来描述主题分布参数t随时间的变化.相比之下,如图6(c)所示,TOT[25]并没有采用马尔可夫假设来描述主题在时间上的依赖关系,而是采用一个基于主题标签z的连续Beta分布ψ来生成时间戳t.这样,具有相同时间戳t的文本有很大的可能性具有相同的主题标签z.的动态主题模型.它利用文本的外部响应,比如用户对文本的评价来调节主题分布.如果这种外部响应是“时间戳”,STM则鼓励具有相同时间戳的文本聚在一个主题内.在这种情况下,STM和如图6(c)所示的TOT[27]模型十分相似.3.3二阶和高阶关系主题模型静态复杂文本网数据的研究主要集中在如何刻画网结构信息对主题形成的影响.其中二阶结构信息被广泛用来调节主题分布参数,主要的建模方法大多可以归纳为无向图模型,即马尔可夫随机场(MarkovRandomFields,MRF).图7显示了3个典型的二阶关系主题模型.如图7(a)所示,LinkLDA[34]使用基于当前文本的主题混合比例θd生成一个主题标签z,然后用一个基于该主题标签z在所有其它文本的多项分布ψ来共同生成被引用的文本d.如果两个文本{d,d}共同引用同一个文本d=d,它们就有较高的可能性分享类似的主题标签z,从而影响到主题分布.但是通过这种方式,LinkLDA仅仅描述了两个文本共同引用同一个文本的关系,并未直接描述文本和文本的引用链接,从而使得引用链接无法直接对主题的形成进行有效的Page6调节.尽管LinkLDA有简化的单个文本的图模型表示,图7(a)更加清晰地显示了两个相互关联文本的图模型表示.如图7(b)所示,PLDA[23]直接用基于主题标签z的伯努利分布Ω生成文本对文本的引用链接变量y,该变量是个二值变量,y=1表示观测到有链接,y=0表示没有观测到链接.但是,图73个二阶关系主题模型范例LTHM[20]假定文本部分的主题变量产生链接,因此LTHM利用链接调节主题分布的作用介于PLDA和RTM之间.CIM[25]允许引用文本的主题依赖于自身的主题混合参数和被引用文本的主题混合参数,从而描述了引用和被引用文本之间的依赖关系.NetPLSA[35]利用文本网络的结构调控主题分布,并鼓励减小主题分布之间的欧氏距离来调整主题分布.CTM[4]并未使用Dirichlet分布,而是使用logistic正态分布来生成文本特定的主题比例,从而达到调控主题之间关系的效果.比CTM更进一步,PAM[36]使用有向无环图来表示主题之间的任意关系,特别是稀疏的主题相互关联.BigramLDA[37]结合bigram语言模型和主题模型从而可以直接对单词之间的二阶关系而非主题之间的二阶关系进行建模,部分地解除了传统的unigram主题建模的局限性.但是,以上所有二阶主题模型并未考虑到网数据中的高阶结构,例如三阶或者四阶的关系,导致常常遗漏特定的主题结构.为了解决高阶关系,MTM[13]使用高斯马尔可夫随机场来描述多个会议文本集之间的关系,这种高阶关系(在实验中使用了6个会议即六阶关系)使得发现的主题不仅捕捉每一个会议特定的内容,而且还捕捉了多个会议之间的交叉主题.陈江峰等人[38]利用高阶谱核测量Web服务文档的相似度,实现Web服务结构化信息的发现.如图2所示,共标签的文本网数据存在大量的高阶结构,例如一个文本有3个不同的标签,那么这3个标签之间的关系(高阶关系)和每两个标签之间的关系(二阶关系)有很大区别.因此,以上所有针对PLDA仅仅随机地采样文本的其中一个主题标签z来产生链接y=1,从而限制链接调控文本主题分布的能力.为了克服这个缺点,如图7(c)所示,RTM[9]使用文本全部的主题变量z-并且利用广义线性模型Ω来生成链接y=1,从而最大效能地利用链接对主题分布形成的影响.二阶结构的主题模型无法精确描述细微的高阶结构.图8显示了两个针对标签文本网数据的主题模型.如图8(a)所示,ATM[39]用一个文本特定的均匀分布ud来产生一个标签t,然后用一个标签特定的主题比例θt来为每一个单词w生成一个主题标签z.所有和标签t联系的文本都共用同一个主题比例θt,因此间接地定义了多个文本之间的高阶关系.如图8(b)所示,L-LDA[40]是一类监督式的主题模型.它将隐藏的主题变量z表示成为观测到的标签t,这样每一个文本可以表示成为混合标签比例θd.在L-LDA中,每一个标签t都对应着一个主题分布,来生成观测到的单词w.显然所有文本都通过标签t特定的主题分布来交换信息,这跟图5所示的LDA模型相似.然而,ATM和L-LDA都没有充分考虑由多个标签引起的文本间的高阶关系,因此不能充分刻画图2显示的普遍情况.通常,一个文本的多个作者会形成高阶的社交网络,因此CNT[41]使用高阶MRF刻画三阶和四阶的作者合作关系,从而发现作者之间的共同研究兴趣,并且发现作者之间潜在的社区结构.Page73.4多通路主题模型多通路是复杂文本网数据的另一个主要特点.不同类型的链接通常连接文本节点,例如相互引用和共同作者两类链接.如何利用不同通路的信息来调整高精度的主题分布目前获得广泛的关注.本质上,上述所有二阶或者高阶主题模型经过变形都适用于多通路文本网数据.图9显示了3个典型的多通路主题模型.MRTM[22]用平行的MRF或者级联的MRF对相互引用和共同作者两种链接来联合建模,通过平衡两种链接的关系来寻找更加精确的主题.如图9(a)所示的平行结构MRTM,它本质上联合了之前讨论的两个简单的主题模型ATM和LDA.MRTM首先从一个文本特定的均匀分布ud生成一个标签t,然后由标签特定的多项分布ψ生成一个主题变量z.同时,该主题变量z还是由文本特图93个多通路主题模型范例事实上,目前多约束的聚类算法大多可应用到多通路网数据信息.例如,FICM[42-43]利用了文本之间多种信息通路,比如标题、主题词以及语义相似度对文本聚类.MGC[44]使用广义凸集投影算法,利用两类不同的约束(基因表达序列和基因本体的语义相似度)将基因聚类.因此,多通路的主题建模可以借鉴当前的多约束的聚类算法.类似之前的监督主题模型STM[5],CTRF[30]使用条件随机场(ConditionalRandomFields,CRF)使得主题的生成过程依赖于各种输入的特征.如果把链接表示成为输入特征,就可以用不同链接类型的输入特征来调节主题的生成.例如,张小平等人[41]用文档中的词符合幂律分布的特征来调控主题分布.3.5分布式主题模型分布式学习策略是机器学习和认知科学的一个重要基本问题.首先,随着多处理器和云计算的发展,我们有必要发展分布式主题模型的学习算法,来利用日趋强大的计算资源.其次,很多大规模网数据定的主题比例θd决定.我们注意到两个文本之间的引用关系可以用{θd,θd}之间的链接来表示.这样,MRTM可以同时表示两类链接类型,即文本之间的相互引用和共标签链接.如图9(b)所示,MRTF[21]使用多个并列MRF分别对共同作者、共同杂志、文本引用、共同年份以及超级链接等多种链接进行建模.它将所有的链接都表示成为文本特定主题比例{θd,θd}之间的关系.大量实验表明使用各种不同的链接通常可以调整出较好的主题分布.如图9(c)所示,TLLDA[24]利用图模型对博客共同作者的社交网信息和博客之间的超链接信息来帮助搜索主题,同时还能自动对博客作者的兴趣进行聚类.它利用文本特定的主题比例θd来生成社交网结构G.这类似于之前的许多二阶主题模型,使用隐藏变量来生成网链接观测.都分布存储在不同的数据中心上.每一个数据服务器只有局部的网数据信息,全局的网数据信息通常无法获得.因此,分布式主题模型主要将计算任务和数据分配到多个局部的计算单元,再利用信息融合的技术得到全局的主题模型参数.然而,将常用的主题模型LDA的学习算法(例如Gibbssampling[10]或者variationalBayes[7])简单地并行化并不容易,需要很多信息融合的技术.为了对大规模独立文本集进行主题提取,Parallel-LDA[45]采用全局同步的思路,首先基于全局初始化的主题模型参数对每一个分布的主题模型进行优化,然后同步融合得到全局的主题模型参数.这种方法要求每个分布的计算单元同步完成优化,并不适用很多实际的情况.因此,Async-LDA[46]采用异步通信的方式,不需要等待所有局部的主题模型优化完毕,只需要随机调整任意两个优化循环完毕的局部主题模型的参数.Async-LDA等到所有局部主题算法收敛之后,再做信息融合得到全局的主题模型参数.Page83.6计算复杂性LDA常用的两大类近似推理和参数估计方法包括Gibbs抽样(Gibbssampling)和变分方法(variationalmethods)[7,10].这两类非监督学习算法的计算复杂度是O(JDId),其中J是主题的数目,D是文本集中文本的数目,Id是每个文本平均的单词数目.动态主题模型的复杂度跟时间戳的总数T以及对相邻时间片段的主题依赖关系有关.如果不考虑主题相互之间的时间依赖,计算复杂度是O(JDIdT).如果考虑主题之间二阶时间相关性,通常计算复杂度是O(J2DIdT).事实上,大多数二阶关系的主题模型的计算复杂度都是介于O(J2DId)和O(JDId)之间,取决于是否对任意二阶主题关系进行建模.高阶主题模型的复杂度跟阶数相关,例如M阶关系通常复杂度有O(JMDId).然而,可以根据实际情况降低如此高的复杂度,例如使用主题平滑(smoothness)的先验知识,复杂度将降到只有O(JDId),和传统LDA相当.多通路的主题模型的计算复杂度跟通路的总数相关.例如,如果存在M种通路的一阶关系,复杂度将是O(MJDId).最后,分布式主题模型的复杂度跟计算单元和单个主题模型复杂度相关.例如,如果有M个计算单元,复杂度将是OJDId()M加上必要的M个计算单元之间通信的复杂度.考虑到计算单元之间通信的计算复杂度通常远远小于单个主题模型的复杂度,分布式主题模型近似是单个主题模型的复杂度的1/M.4网数据主题建模的几个研究方向尽管对独立文本的主题学习算法的研究在过去的十年有了长足的发展,但是对文本网数据的主题提取精度和稳定度仍然不理想,离大规模文本分析中的各项实际应用还有相当大的距离.其中一个主要原因是尚未有完备的主题建模框架可以同时考虑复杂文本网数据的动态、高阶、多通路和分布式等4个主要特点.因此,可以采用一个基于三维马尔可夫模型和二型模糊系统的框架[22,41,47-50],系统地刻画大规模文本网数据的4个特点,期望改善主题模型的精度和稳定度.4.1切片式的三维马尔可夫模型对动态主题的建模在实际应用中,研究主题网关系的演化通常比研究单一独立主题的动态过程更加有意义.从网结构的角度,可以描述主题簇(topicclusters)之间的转化、产生或者消亡的过程,并且通过文本网数据的外在联系进行主题分布调控,从而分析并检测出稳定的主题簇的动态演化过程.目前,时变MRF[3]可以用来恢复残缺动态网数据的结构,对研究动态文本网数据的主题建模有很重要的启发.此外,文本网数据外部的响应,比如对每一篇文章的评价分数或者每一个网页被点击的次数,可以用来开发监督式的主题学习算法[5],对网数据的动态主题分布进行调节.切片式的三维马尔可夫模型(Three-DimensionalMarkovModels,3DMM)可以描述动态文本网数据.如图10所示,我们主要用文本网数据的两种主要链接类型为例(即相互引用和共同作者)来说明如何用切片式的3DMM对动态文本网数据进行主题建模.该模型具有坚实的理论基础[29],即马尔可夫随机场MRF和隐马尔可夫模型HMM.类似的3DMM框架曾被应用到人体运动追踪中[51].如图10所示,不失一般性,用3个MRFs来表示3个时间切片t={1,2,3},对片内的静态文本网数据的链接进行主题建模.如图所示,每一片静态网数据有两种链接类型:相互引用(细线箭头链接)和共同作者(黑色人形图标和无向链接).类似的,切片之间也存在相互引用和共同作者这两种链接类型.此外,深黑色粗箭头表示切片之间的时间依存关系,采用基于HMM的方法来描述主题的动态演化.这样,切片式的3DMM可以刻画复杂文本网数据的主题演化.图10切片式3DMM对复杂文本网数据主题建模示意图一方面,用MRF(或者2DMM)[49]描述每一个切面的链接,假设拥有链接的文本的主题之间具有很强的语义相干性,这样文本主题之间会通过不同的链接类型相互影响.这个MRF“平滑”(smoothness)先验知识被广泛使用在图像分割和机器视觉上[29].具体而言,如果两个文本之间存在链接,它们就有很高的可能性分享类似的主题.另一方面,用HMM(或者1DMM)刻画主题在切片之间的时间依赖关系.同Page9时,主题还会通过切片之间的链接,例如相互引用或者共同作者,来相互影响.研究的目标是最小化以下的能量函数来搜索最优的主题标签配置:其中E3DMM是3DMM的能量,它由2DMM和1DMM的能量E2DMM和E1DMM共同构成.通过最小化E3DMM,不但鼓励了切片内的链接文本的主题相干性,并且鼓励了切片间文本的主题相干性.随着网数据在时间轴上的变化,主题在链接的关联下相互影响不断演化.利用传统的LDA[7]来对文本内容进行主题聚类,然后同时利用链接和时间的依赖关系共同调节主题聚类结果.在3DMM的框架下,需要解决两个关键问题.首先,需要探索文本之间的链接如何影响主题随时间的演化.其次,需要研究主题的演化对未来的链接分布造成的影响.第一个是关于3DMM的推理和学习问题,即如何从动态网数据中推理和学习动态的主题分布.第二个是如何利用3DMM来预测链接的问题,即如何利用学习好的主题分布参数来预测未来网数据的链接分布.为了开发3DMM的学习和预测算法,需要深入研究3种常用的图模型的推理和参数估计算法:BeliefPropagation(BP)、MarkovChainMonteCarlo(MCMC)和VariationalMethods(VM)[29].给定动态网数据,BP对每个主题变量求边际概率,然后利用主题变量的边际概率来估计模型参数.BP算法的缺点是在有环图模型中无法保证收敛.MCMC通过随机采样主题变量来更新模型参数,缺点是计算量庞大,并且不知道何时收敛.VM通过转化原目标函数到一个近似目标来降低主题变量后验概率推理的难度,然后估计近似的参数,缺点是近似目标总是和原始目标有差距,导致学习的精度不高.可以结合三种算法的特点,对基于3DMM的主题模型进行参数估计,开发出快速高效的学习算法.在3DMM学习完毕后,需要进一步研究基于3DMM的预测算法,用来预测未知网数据的主题和链接分布.通过外部响应来调整主题分布主要通过引入广义线性模型(generalizedlinearmodel)[5]或者支持向量机(supportvectormachines)[52]对文本数据的外部响应进行回归或者分类,进而调整主题分布以便能够更好地预测外部响应.据此,可以采用一种不同的方案描述网数据的结构.首先将网数据的链接作为一种外部响应特征,然后使用分类模型来更好地预测链接,进而通过分类模型来影响网数据主题分布随时间的演化.基于上述方案,可以类似地开发出对应的3DMM的监督式学习算法,不但可以学习主题分布,而且同时可以预测文本网数据的结构.4.2高阶MRF对高阶标签网数据建模如图2所示,标签文本网数据是广泛存在的一类具有高阶结构的网数据.研究的目标是利用高阶MRF对高阶标签网数据建模,从而推理和学习出更加稳定且具有语义关系的主题.因为高阶关系是标签文本网数据的重要特征,因此可以采用高阶MRF来系统地表示标签文本网数据的高阶关系.这里主要讨论如何构建快速有效的学习算法来降低优化MRF的复杂度.在3DMM的框架下,进一步将2DMM的能量函数表示成为其中,E2-order表示传统的二阶关系产生的能量,而Ehigher-order表示高阶结构产生的能量.可以采用两类有效策略来降低高阶MRF学习的复杂度.首先,仅针对几类重要并且具有代表意义的高阶关系进行建模.将所有高阶结构进行聚类,然后只鼓励几种典型的高阶结构.这样,可以避免对所有的高阶结构进行学习,从而大大降低学习算法的复杂度.其次,采用BP算法来学习高阶MRF.在信息传递的过程中,仅仅传递特定的主题组合信息,从而避免对所有的主题组合信息进行传递.特别指出,这里在假设链接文本的主题间具有平滑特性(smoothness)的情况下,即链接文本具有很高的可能性共享相同主题.这样,主题组合空间将大大减小,从而使得学习算法具有很好的扩展性.通常,仅考虑三阶或者四阶的高阶关系就可以满足实际标签网数据的建模,因为绝大多数的实际文本关联的标签都少于5个.尽管高阶结构对于抽取更加精确的主题有很大帮助,但是优化高阶MRF的参数通常需要庞大的计算成本[53].如何降低优化或者学习高阶MRF的计算复杂度是未来的一个挑战[54].当前,高阶MRF在机器视觉领域中的高层次图像先验建模方面有很多重要应用,例如图像恢复、视差估计和目标分割[53-57].因此,可以对高阶的主题结构建立合理的能量函数,借鉴以上高阶MRF能量函数优化的方法,有效地搜索特定的高阶主题结构.4.3混合MRF的方法对多通路网数据的建模现有的几种多通路主题模型并不能自动学习每Page10种通路对主题形成的影响,需要用户根据经验手动来调整每一种通路的权重.因此,自动学习不同通路的权重是多通路主题模型迫切需要解决的一个重要问题.可以借鉴基于L1范数调整的MRF结构学习[58]来自动地学习最优的权重参数,或者结合特征选择(featureselection)的方法进行最优通路的选择[59].此外,如何设计有效的基于链接的输入特征并且学习输入特征的权重是一个有待探索的研究方向.最近,多种链接特征被用于文本网数据聚类,可以用来平衡聚类重叠和层次之间的冲突[2].多通路的文本网数据的主题建模的关键是学习不同通路对主题分布影响的权重.可以采用MRF方法结合基于广义投影的Expectation-Maximization(EM)算法[29]来学习多通路对主题形成的影响.每一种链接对文本主题的形成影响不同.尽管在时间轴上,这两种链接都同时存在,为了简便起见,这里仅考虑静态切片内部的多通路网数据.在3DMM的框架下,可以进一步将2DMM的能量函数表示成为混合模型其中,wn是第n种通路的混合权重,且En是每一种通路所产生的能量.通过最小化混合能量函数(3),可以估计出每一种通路的混合权重.在3DMM的框架下,将式(2)和(3)进一步合并成因此,混合高阶MRF(4)可以同时对高阶结构和多通路结构进行建模,并且学习稳定的主题分布.类似的,也可以把时间轴上的高阶和多通路关系用以下目标函数表示其中,E1DMM是1DMM的能量函数,可以写成混合和高阶1DMM的形式.可以发展基于广义投影的EM算法[44]对混合MRF进行优化.在E-step中,可以依次估计出每一个通路En产生链接和节点的后验概率,称之为对En的广义投影.然后,利用后验概率在M-step中最大化混合高阶MRF生成网数据的期望,同时估计出每一个通路的能量函数的参数.其中,权重wn也可以用每一种通路的后验概率估计出来,并且受到L1范数调整,即所有参数的L1范数的和最小化.通过多次广义投影之后,可以搜索出近似最优的主题文本来满足网数据的各个通路的约束.通常,在推理后验概率中会遇到隐藏变量耦合的情况,所以很难得到精确的推理结果.因此,可以借鉴近似推理的方法比如variationalBayes[7]来实现这一目标.4.4深浅学习对分布式网数据的主题建模对于分布式的文本网数据,无论采用同步还是异步的分布计算技术,都需要在局部优化的过程中对全局的主题模型参数做出估计.这对于大规模网数据的主题挖掘并不适用,因为每一个计算单元只能分配到局部的网数据的结构信息,如何在分布计算中融合全局网结构信息来调节动态、高阶和多通路主题模型参数仍然存在挑战.二型模糊集合(Type-2FuzzySets,T2FS)[60]近来被证明是一个能够处理单词计算(ComputingWithWords,CWW)[61]中的不确定性的有效方法,因此可以作为分布式主题建模的信息融合的策略.在T2FS框架下[46-51,60-65]利用深浅学习(deep-shallowlearning)的方法能够较好的完成分布式主题模型的信息融合.首先定义深度学习,即在每一个分布独立的计算单元使用3DMM学习动态高阶多通路的局部网数据的主题.这个学习过程类似一个人在某个专业进行深入钻研,所以称之为深度学习.在每个独立分布的计算单元学习完毕之后,对所有局部3DMM的参数进行融合,这个学习过程类似于一个人快速综合已有的知识,适应新的环境,因此称之为浅度学习.可以采用T2FS来融合所有分布主题模型的参数,使用二级隶属函数来表示一级隶属函数的不确定性,实现深浅学习过程.图11显示从分布式网数据提取主题分布的一个例子.通常用一个多项分布来表示单词表的每个单词出现在一个主题的可能性,称之为主题分布.从分布在3个计算单元上的网数据中学习出3个关于同一主题的分布如图11(a)所示.因为每一个局部网数据的内容和结构不同,学习之后3个主题分布的参数有很大变化(图11(b)).这就要通过刻画主题分布参数的不确定性来融合3个主题分布的参数.因此,可以使用T2FS的方法来刻画3个分布式主题的参数不确定性.Page11图11分布在3个计算单元上的网数据中学习出3个关于同一主题的分布使用T2FS可以刻画局部主题分布参数的不确定性.如图12(a)所示,可以用一个T2FS来表示融合各个局部主题模型之后的主题模型.因为局部网数据的内容和结构有很大变化,融合之后的主题模型的参数有很大的不确定性.在T2FS的框架下,可以用一级隶属函数来表示单词表中的单词隶属于主题的可能性.因为参数的不确定性,一级隶属度本身可以用一个误差棒表示,而在误差棒内的一级隶属度的不确定性可以进一步使用二级隶属函数来刻画.如图12(b)所示,假设在误差棒内的一级隶属度服从一个高斯形状的隶属函数,意味着误差棒内的平均值对应的一级隶属度有很大的可能性来生成对应的主题单词.如图12(c)所示,可以假设误差棒内的一级隶属度服从一个区间,意味着误差棒内的一级隶属度具有等可能性来生成对应的主题单词.这样,就用二级隶属度函数(b)和(c)刻画了一级隶属度(主题分布参数)的不确定性.从图12可以看出,每一个基于T2FS的主题模型的输出都是一个模糊集合而不是一个精确的可能性.这是由于主题模型参数不确定性造成的.这样,所有计算都可以采用二型模糊集合的运算法则[64].深度学习的目标是保持各个局部主题分布的一致性.提取分布在不同计算单元的网数据的主题过程中,可以采用传统的异步通信来保持[46]主题的一致性.当深度学习完毕之后,再采用T2FS来融合不同的局部主题模型的参数,用二级隶属度表示参数的不确定性.浅度学习的目标是估计二级隶属函数的参数,反映每个局部主题模型的可能性.通过浅度学习可以将局部的网信息用模糊集合来融合,增强全局主题模型的精度和稳定性.最后,我们需要从基于T2FS的主题模型中推理出最好的主题分布来解释Page12未知的文本网数据.通过该方法可以在二型模糊逻辑系统[64]的推理框架下完成这一任务,根据实际的网数据内容和结构,推理得到较好的主题分布.5总结和展望本文综述了近十年网数据主题建模研究的进展,总结出了文本网数据的4个主要特点以及多个尚未解决的科学问题,并讨论了几个可能在未来快速发展的研究方向.由于主题建模在过去十年发展迅速,本文没有详尽介绍大量的其它主题建模策略,仅仅介绍了几个典型的主题模型范例.未来网数据的主题建模将持续是机器学习和数据挖掘领域关注的热点,我们寄希望于更多的研究者参与研究并解决本文提出的关键科学问题.
