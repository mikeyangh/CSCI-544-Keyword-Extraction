Page1基于消息传递机制的MapReduce图算法研究潘巍1)李战怀1)伍赛2)陈群1)1)(西北工业大学计算机学院西安710072)2)(新加坡国立大学计算机学院新加坡119077)摘要单机运行环境难以满足基于海量数据的大图算法对时空开销的需求,如何设计高效的面向云计算环境的分布式大图算法越来越受到人们的关注,MapReduce作为云计算的核心计算模式受限于易并行(EP)计算模型的制约不易表达图算法.文中突破了MapReduce基于易并行计算的假设,增强了MapReduce既有的编程规范,新的大同步(BSP)计算模型既能保证兼容旧的MapReduce作业可以无改动的运行,同时引入消息传递机制允许变化的状态数据在并行任务的超级步间进行交互.系统提供高度灵活的消息自定义接口,针对不同应用需求设计了轻量级和重量级两种自适应的消息传递机制,更高效地支持有数据交互需求的包含迭代处理的一大类图算法.在真实大规模图数据集上的实验结果表明,相比于原始的MapReduce作业外部链式处理,该文提出的BSP模型下的内部超级步迭代计算模式大幅降低了大图算法的处理时间.关键词云计算;MapReduce;大同步模型;消息传递;图算法;PageRank1引言云计算、物联网和社交网络等技术的飞速发展,极大丰富了各种海量异构数据的产生渠道,高可扩展的海量数据并行处理是其中关键性的技术之一.MapReduce[1]是Google提出的一种处理超大规模数据集的分布式并行编程模型,也是云计算目前的核心计算模式.MapReduce编程规范(Paradigm)借用函数式语言的映射(Map)和规约(Reduce)原语,通过自动切分输入数据集,在独立的数据切片(split)上应用Map操作产生中间结果的键值对(key/valuepair)集合,然后通过分区操作(partition)确保具有同样键的数据映射到同一分区中并借助混洗操作(shuffle)在无共享(shared-nothing)的集群网络中传递中间结果,最后在不同的中间结果分区上应用Reduce操作产生最终的规约结果.也就是说整个MapReduce作业的执行主要分为Map和Reduce两个处理键值对集合的并行运算阶段,Map阶段同时存在多个异步执行的处理输入键值对的Map操作(下文称之为Mapper),Reduce阶段同时存在多个异步执行的处理中间键值对的Reduce操作(下文称之为Reducer),Map和Reduce阶段之间需要同步交互数据.利用这种方式,MapReduce屏蔽了底层复杂的并行处理细节,极大简化了并行程序的设计,应用开发者只需要关注与具体应用相关的Map和Reduce的处理逻辑本身,而将其余复杂的并行事务交与系统完成.近年来,不少科研机构和公司团体都研发了自己的基于MapReduce设计规范的海量数据并行处理系统,其中Apache的Hadoop是MapReduce的一种开源实现,也是目前学术界和业界事实上的海量数据并行处理标准.Hadoop可以方便地部署在通用的商用机集群中,为简化用户的并行编程环境,高抽象度的Hadoop仅为使用者提供了有限的执行策略,因此在某些应用上(特别是图的算法)只能采取高通用性低效率的方法,意图在易用性和执行性能上进行折衷.大量的分布式图算法都包含明显的迭代过程并且在数据内部存在一定的依赖关系,在原始的MapReduce中,只能通过多趟的外部链式调用MapReduce作业[2-3]来支持迭代和数据交互,这不但需要开发者主动干预执行的过程,还不可避免会引入大量不必要的重复代价.因为对于外部链式调用的作业,每轮迭代都不可避免会产生作业启动开销(包括作业分发、输入划分、任务划分等一些初始化操作,根据实验数据分析这部分warm-up代价占到整个作业执行代价的7~10%左右)、不变数据的序列化和网络传输开销,迭代中间结果的HDFS持久化开销等.某些进化的基于Hadoop的系统或类MapReduce系统,如HaLoop[4]、Twister[5]等试图在作业内部完成迭代,以期减少多轮中间结果的持久化代价,但是在实现上多采用分布式内存和本地缓存来存储图的拓扑,这种设计策略对具有海量数据的大图处理存在一定的局限性.这种并不优雅的解决问题的方式源于MapReduce编程规范中一个很重要的假设:Mapper或Reducer间不存在任何依赖,可以无交互的在不同的数据切片上独立执行.这是一种称之为“易并行计算”[6](EmbarrassinglyParallelComputation,EPC)的“理想”的并行计算模式.基于此模式可以解决的并行问题都可以分解为多个完全独立的部分且他们能够异步独立执行,这种理想模式下异步并行的Mapper之间(或Reducer之间)不存在通信,数据交互仅依赖于Mapper和Reducer之间的Shuffle处理.因此一些有中间数据交互需求的包含迭代过程的并行算法只能借助MapReduce作业的链式调用来满足多轮迭代的数据交互需求,并根据应用的迭代收敛条件决定何时终止链式的调用.本文突破了MapReduce基于易并行计算的假设,设计了基于Hadoop的支持大同步编程规范[7](BulkSynchronousProgramming,BSP)的并行计算框架.改进的框架增强了MapReduce既有的编程规范,既能保证兼容旧的MapReduce作业可以无改动的运行在新的并行运行环境中,同时利用有效的消息传递机制允许变化的中间状态数据在任务间进行交互.新框架中将Map(Reduce)阶段分解为多个同步的超级步,超级步内任务异步高度并行,超级步间利用消息传递机制完成任务间的数据交互.新框架利用图节点驱动的方式更高效地支持有信息交互需求和包含迭代处理过程的一大类图算法,极大地减少了图算法在MapReduce既往处理模式中不必要的代价.本文主要贡献包括:(1)抽象了支持大同步编程规范的改进的并行计算框架的编程模型,利用易并行计算的形式化定义描述了MapReduce的框架实现,通过引入大同步模型增强了MapReduce的编程规范,为解决包含迭代过程且有交互需求的分布式大图算法提供了一种高效的实现途径.(2)定义了改进的并行计算框架中图的通用表示格式,抽象了以节点为驱动的分布式图并行计算模型.Page3(3)设计了障栅消息格式,实现了自适应的基于Hadoop原有消息传递结构的轻量级消息传递机制和独立消息服务器模式的重量级消息传递机制,系统利用对使用者透明的隐式同步完成超级步间的同步和消息传递.(4)通过在72节点的集群环境下使用标准的Stanford大型网络数据集设计并完成了相关实验,实验结果表明在给定的数据集和实验环境下,改进版的Hadoop集群中PageRank算法的执行性能相比于原始的Hadoop集群最高可提升51%.本文第2节对相关工作进行介绍;第3节描述易并行计算和大同步计算的并行编程模型并分析了改进的并行计算框架下作业执行的代价;第4节抽象大同步模型下图算法的计算模型,并以PageRank为典型应用给出具体的示例;第5节给出障栅消息的定义和格式,并详细描述自适应的轻量级和重量级消息传递机制;第6节通过实验结果验证改进的并行计算框架对PageRank算法的高效性;最后一节对全文进行总结并给出未来研究工作的展望.2相关工作MapReduce已经受到了学术界和工业界的广泛关注和讨论[8-13],目前具有代表性的基于MapReduce的海量数据并行处理系统有Google的Sawzall[14]系统,微软的Dryad[15]系统和SCOPE[16]系统、Yahoo的Pig[3]系统以及Apache的Hive[2]系统.其中Sawzall是一种用于极大规模数据集合的平行分析语言,其采用filter-aggregator(过滤器-聚合器)两阶段式的执行方式,通过限制编程模式来保证高并发和扩展能力,它在语言级别保证了平行处理的任务间不存在相互依赖.Dryad提供了能够在Win-dows或者.Net平台上编写大规模的并行应用程序的分布式并行计算基础平台,利用过程式高级语言接口DryadLINQ使得无并行编程经验的程序员可以轻松完成大规模的分布式计算任务.SCOPE系统是建立在Dryad之上的用于大规模数据并行分析的声明式语言,允许用户自定义函数来实现更丰富的计算.Pig和Hive基于Hadoop提供了高层的语言支持,Pig引入了一种SQL-Like语言PigLatin,借助该语言编写的脚本可以被自动转化为MapReduce作业;Hive是一个开源的数据仓库解决方案,提供了SQL-Like的陈述性语言HiveQL,支持类似SQL的海量数据查询方式,查询被编译成MapReduce作业在Hadoop上执行.两者都简化了编写MapReduce作业的代价,但是这种简便是以牺牲执行性能为代价的.此外还有Hyracks[17]、Spark[18]、Nephele[19]等多个受MapReduce启发的海量数据处理系统,上述并行处理系统都是高通用性的并行平台,并没有针对分布式并行图算法的特点设计高效的支持方法,因此处理效率都不理想.针对传统的MapReduce不易于表达迭代式操作的问题,Bu等人[4]设计了HaLoop一种基于Hadoop的支持内部迭代的数据并行处理系统,有效减少了迭代过程中数据重载以及迭代中间结果持久化的开销,但是无法消除中间数据排序以及重复任务启动的代价.相比于Hadoop,Twister[5,20]是一个研究性实验项目,其基于MapReduce思想设计了支持迭代的并行编程模型,但该系统假设待处理数据不需要分布式文件系统支持可完全加载于分布式内存,导致其不满足海量数据处理的实际应用需求,且其输入数据手动切分的策略也增加了开发者设计并行应用的难度.Apache的开源项目Mahout[21]其设计目标是基于Hadoop创建高可伸缩的机器学习算法,几乎所有的机器学习算法都涉及迭代的过程,Mahout专门设计了外部驱动程序来控制迭代的执行,每轮迭代都需要启动新的MapReduce作业,如引言中所述,这种外部迭代的方式会引入很多不必要的执行代价.目前专门面向大规模图算法的分布式并行编程模型及相关优化的研究工作也有很多成果.Google的Pregel[22]编程框架能够为图算法提供并行支持,其根据图的特点是设计了顶点传递信息的多轮迭代处理模式.但该平台并不基于Hadoop,在开放性和通用性上均有所限制,并且其所有的计算状态均保存于内存,因此也缺乏对大规模数据的有效支持.AveryChing等人提出了一种基于Hadoop的大规模图处理框架Giraph①,该框架支持动态资源管理,利用高可用容错的ZooKeeper实现系统工作单元的分布式协调,并提供了支持迭代的图形处理库,其和本文设计的编程框架都已期在Hadoop平台上利用大同步编程规范提升迭代式图算法的处理能力,但在设计策略和实现细节(特别是消息处理)上有诸多不同,相对于Giraph构建于Hadoop之上的多线程架构,本文设计的框架采用了侵入式的设计模式,利用通信实现工作节点间的分布式协调,无需手工切分和分布输入数据等,因此在兼容性、易用性和通用性等特性上更具优势.Surfer[23]系统提供了①Giraph.http://incubator.apache.org/giraph/Page4MapReduce原语和Propagation原语,并利用基于原语的构建块来支持在云上的大图算法,其主要目标在于提供运行期的可视化监控,并没有涉及针对图特征的具体实现细节.Lin等人在文献[24-25]中对MapReduce的图算法实现进行局部优化,提出了Mapper内合并、避免图拓扑重复传递以及范围分区等优化技术,但其还是基于多轮的MapReduce作业调度.还有很多并行处理框架,像Apache的HAMA[26]和CMU的GraphLab[27]等也都支持迭代,但是这些平台均面向特定的问题领域.此外还有很多研究[28-29]是希望借鉴并行计算中的成熟的消息传递接口(MessagePassingInterface,MPI)技术提升MapReduce的处理能力,但这些研究都没有给出基于Hadoop平台的实现,在容错、可扩展、鲁棒性等特性上都存在缺失.本文旨在继承Hadoop原有的诸多特性的基础上通过引入大同步模型,利用消息传递机制和超级步同步来更高效地支持分布式图算法.3并行编程模型抽象和代价分析本节将介绍MapReduce支持的既有计算模式和本文改进的计算模式,并分析了改进模式的相关代价.同时抽象出基于稀疏有向图的并行算法在MapReduce并行编程环境下迭代执行的设计模式.3.1易并行(EP)计算并行计算可以用多种不同的并行编程模型表示和实现,每一种模型都有与其相适应的一类计算应图1并行编程模型用.为简化并行应用开发者的开发工作,使无并行开发经验的程序员也可以正确快速地编写并行应用,MapReduce对编程模型进行了限制,使MapReduce编程模型主要针对易并行(EmbarrassinglyParallel,EP)计算抽象.正是利用这种限制性的模式,MapReduce实现了并行程序的自动并发处理,并在内部提供了诸如输入划分、并行任务调度及通信、容错、负载均衡等并行细节的自动支持,实现了高可扩展性和高度并行性.下面先给出易并行计算的形式化定义.定义1(易并行计算).并行计算中,假定给定一个并行作业J,其可以分解为一系列可异步并行执行的任务T,同时给定一个输入数据集W和并行任务间的通信代价C,如果对输入数据集W的任一划分P={p1,p2,…,pm},若其满足pi∩pj=,∪ipi=W以及C(T(pi),T(pj))=0(1<m<|W|,1<i,j<m)条件时,并行作业J可表示为J=∪mi=1T(pi),称满足以上条件的并行计算为易并行计算.也就是说,若干并行任务可以在互斥的输入集划分上无通信代价地独立执行.下面利用易并行的形式化定义分析MapReduce的编程模型.从图1(a)中可以看到,MapReduce作业执行计划分为Map和Reduce两个阶段,也称之为一次MR过程,MR过程中每个阶段内部的异步并行任务(Map0~Mapm或者Reduce0~Reducen)都运行在易并行的理想模式下.Page5在Map阶段,输入数据被自动切分成等大小的独立输入片段Split0~Splitm(Split的默认值是64MB,和底层分布式系统存储块等大小,这种策略避免了Split跨越块边界而可能引发的数据传输导致的网络代价),输入片段是若干键值对构成的集合.MapReduce并行处理框架会依据数据本地化优化策略将Map0~Mapm分布到输入片段所在的执行节点上运行.执行过程中Map0~Mapm之间不存在任何依赖关系无需通信交互,符合易并行的形式化定义,其产生的中间结果是新的键值对集合.在Reduce阶段,由Map0~Mapm产生的中间结果经过按输出键分区操作后(默认是采用散列分区的方式,在输入数据集的分布不偏斜的情况下,散列能得到分散均匀的中间结果分区)产生的中间结果分区Part0~Partn作为Reduce0~Reducen的输入.执行过程中Reduce0~Reducen之间不存在任何依赖关系无需通信交互,符合易并行的形式化定义,最终Reducer的输出结果会自动被持久化到底层存储介质上(默认是HDFS).Map阶段和Reduce阶段之间是串行同步的,存在一个相对用户透明的隐式同步和通信过程.Reducer必须等到最后一个Mapper执行完毕后才开始执行(但是,其中Mapper产生的中间数据的Shuffle过程是与Mapper以重叠方式执行的,即任一个Mapper结束后,Reducer就可以Shuffle中间结果,这样可以缩短并行流水处理的长度,提高处理的效率).如果一个并行应用需要使用多趟的MR过程,那么一次MR过程与下一次MR过程之间也是链式串行同步的.通信和数据交互也仅发生在一次MR过程中的Map阶段和Reduce阶段之间以及多趟MR过程之间.大量实际应用属于易并行计算模式,如分布式的Grep、倒排索引以及分布式排序等,这类问题非常适合用易并行计算模式处理.然而,易并行计算是整个并行计算设计体系中最理想的一类计算问题.Sutter等人[30]将并行计算模式分为3种类型:无依赖并行(Independentparallelism)、规则并行(Regu-larparallelism)和无结构并行(Unstructuredparal-lelism).其中无依赖并行即是前文讨论的易并行,规则并行是比易并行更高级的并行模式.规则并行适用于并行计算工作存在一定依赖、在并行操作间有通信或同步需求的并行应用类型.本文希望通过引入消息传递机制扩展MapReduce支持的并行模式,使MapReduce以更优雅更高效的方式支持诸如PageRank等大量的规则并行应用.3.2大同步(BSP)计算引入消息传递机制的主要挑战在于在现有的MapReduce并行计算框架中,Mapper之间或Reducer之间不支持消息传递.本文中Map(或Reduce)阶段内部支持消息传递的灵感来源于并行计算的BSP模型,BSP模型相对现有的MapReduce提供了更高级别的并行抽象,利用障栅实现同步控制,使用消息传递机制完成并行任务间数据的交互.下面先给出BSP模型的定义.定义2(大同步计算).大同步计算模型中,一个并行作业由一系列的超级步(Supersteps)组成,每个超级步构成一个相并行(PhaseParallel).大同步计算主要由3个有序的部分构成:(1)易并行计算.超级步内各任务独立的异步并行执行;(2)通信.并行任务在超级步结束前利用消息传递机制完成数据的交互;(3)障栅同步.同步等待同一个超级步内所有并行任务的交互全部完成,则整个并行才可以向下一个超级步移动,进入下一轮的相并行.从图1(b)中可以看到本文设计的支持BSP模型的MapReduce并行处理框架和原始的MapReduce并行处理框架在整体框架逻辑上保持一致.从宏观的角度观察,如果将一次MR过程的整个Map阶段视为一个超级步,整个Reduce阶段视为另一个超级步,两个阶段间的Shuffle过程视为超级步间的障栅同步和通信过程,那么其实MapReduce本身也是符合大同步计算的,这也是为什么通过链式的MapReduce作业调度可以满足有交互需求的迭代运算的实现依据.BSP模型中,利用消息传递机制并行任务间可以交互变化的中间状态数据,并行任务间的消息传递不是任务间单独分散的行为,而是被视为一个整体并约束在相邻超级步之间.任务发送的消息需要在一个超级步内进行汇集,上一个超级步汇集的消息需要等到下一个超级步内并行任务执行时才能使用,消息不能跨越不连续的超级步使用.利用消息传递,同样的并行逻辑可以使用交互的数据在一个新的状态下继续运行.迭代的计算都可以抽象为相同的处理逻辑,在新的状态数据集中期望收敛地连续执行,因此这种模式非常适合存在迭代需求的分布式并行图算法.并且这种不跨越超级步的消息传递模式也有效简化了大量并行任务间消息的维护代价.要保证消息能以整体汇集的形式有序地在超级步间传递,就必须依赖超级步间有效的同步机制.在Page6BSP模型中,超级步之间的同步等待是借助障栅[31]来实现的.障栅是一种可控的粗粒度级的全局同步机制,利用障栅可以将一趟并行任务划分为多个连续的松散同步的超级步,如图1(b)中的SS0、SS1等.其保证了消息仅在一个超级步之内汇集,并在相邻的后继超级步间传递,关于障栅的概念在第5.1节中有更详细的介绍.本文改进的并行处理框架试图在Map或Reduce阶段内部支持超级步,基于这种设计模式,以往需要通过多趟MapReduce作业外部链式调用才能实现的迭代式计算,现在可以在一次MR过程中,利用Map阶段内部(或Reduce阶段内部)的多个超级步的同步执行就可以完成.复杂的消息传递控制由新的运行时系统处理(具体实现细节参看第5节),并行程序开发者只需要利用原有的MapReduce程序开发经验就可以在改进的并行框架下编写更高效的并行应用.改进的并行框架有效减少了占据大量处理时间的外部迭代引入的代价.但是,相对于原有的Hadoop并行计算框架的代价[32],改进的支持BSP模型的并行计算框架也引入了一些新的代价:首先,粗粒度的障栅同步使得单个超级步的总体执行时间对单个最慢完成任务是敏感的,针对非一致性状态下任务完成时间异常的问题可以借助Hadoop的推测执行(SpeculativeExecution)机制[1],利用冗余执行的备用任务有效缓解,并且由于一个超级步内并发的多个任务的计算和通信是重叠执行的,所以此代价还能进一步地被一个超级步内异步并行的多个任务摊销;其次,新代价模型中的障栅同步是一个潜在的可能会造成性能下降的瓶颈,但是,实际上改进模型中的障栅同步只是将原模型中多次MR过程间Map和Reduce阶段的隐式障栅同步转移成一次MR过程中Map和Reduce阶段内的多次障栅同步,所以本质上并没有增加任何新的同步代价.4BSP模型下图算法的计算模型本节根据新引入的BSP模型,介绍如何在此模型下进行高效的图算法的设计和实现,包括图如何在改进的并行计算框架中表示,以及如何建立以节点为驱动的图并行计算模型,并以PageRank为典型应用给出具体的应用示例.4.1图在并行计算框架中的表示基于BSP模型的并行计算框架主要意图在于利用图的特点以更高效的方式支持分布式图算法.众所周知,在单机运行环境中往往难以满足图运算(特别是大图)对时空开销的需求,MapReduce并行计算框架虽然可以满足分布式图运算对可扩展性的需求.但是由于其无状态并行的任务没有利用节点的依赖关系,所以不易直接表达图算法.把图运算放入并行计算框架中执行首先需要把图剖分成适合MapReduce处理的输入键值对集合.邻接矩阵和邻接链表是两种最常用图的表示形式.因为实际应用中的大图常呈现出典型的稀疏特性[22],如社交网络图、网页链接关系图等,所以相对于O(n2)空间需求的邻接矩阵形式,邻接链表更适合稀疏大图的表示需求.下面以邻接链表为图的基本表示形式,进一步构造适合改进框架的图的键值对表示形式.假设有向图G=(V,E)(不失一般性的情况下,下文讨论均以有向图为例,无向图可以用一对节点间互相指向的有向边表示成有向图),其由顶点集V(G)={v1,v2,…,vn},和连接两个顶点的边集E(G)={(vi,vj)|i,j=1,2,…,n}构成.其中与一个顶点直接相邻的图的局部拓扑结构可以用该节点的直接前驱集合和直接后继集合表示,先给出直接前驱集合和直接后继集合的形式化定义.定义3(直接前驱集合).设节点vi∈V是有向图G=(V,E)的一个节点,若图中有节点vj满足(vj,vi)∈E,则集合χp(vi)={vj|(vj,vi)∈E}就是节点vi的直接前驱集合.定义4(直接后继集合).设节点vi∈V是有向图G=(V,E)的一个节点,若图中有节点vj满足(vi,vj)∈E,则集合χs(vi)={vj|(vi,vj)∈E}就是节点vi的直接后继集合.直接前驱集合和直接后继集合代表了基于节点的并行计算任务间的依赖,也是消息传递的路径.我们在基于邻接链表的基础上通过适当扩充变形,抽象出在改进框架下使用的有向图的输入键值对表示格式,如图2所示.Key键名节点标识符图2中输入键可以是任意内容(默认的键是文本行起始处的偏移量),输入值分为5个基本部分:节点标识符、直接前驱集合、直接后继集合、元数据和当前节点状态.其中元数据是图元素代表的实体Page71.节点驱动的功能函数启动处理.如前所述,图经过预处理被剖分为输入键值对集合,每一个键值对即代表以节点为中心的计算元,用户设计Map处理逻辑根据应用需求利用键值对中包含的信息计算首轮迭代中节点的中间状态值;2.节点间状态的交互处理.利用消息传递机制,将节点的中间状态值依据节点在图中的邻接关系进行传递,邻接关系即用节点的直接前驱集合和直接后继集合完全表示;3.节点驱动的功能函数迭代处理.接受并解析上一个超级步传递的消息获取邻接节点的新的中间状态值,在新的邻接节点中间状态值集合和代表图拓扑的原始输入键值对集合上,应用用户设计的Map处理逻辑计算本轮迭代中节点的中间状态值;关键信息,包含节点的元数据和边的元数据两部分.例如,在交通网络图中,节点代表道路交叉口,其中节点元数据可能包括位置坐标、所属区域以及节点名称等信息;边代表节点间连通路段,其中边元数据可能包括路段长度、断面通行能力、单向通行与否等信息.当前节点状态是随着迭代运算不断更新的当前状态值,其代表本次迭代的节点中间状态,当迭代收敛时,中间状态值就成为图运算的结果状态值.4.2图的计算模式下面抽象出在改进框架下,以节点为驱动的包含迭代过程的稀疏有向图的计算模式.每一次迭代过程包含相同的处理逻辑,其包括以下主要处理步骤:4.迭代终止检测.根据具体应用的迭代终止条件决定是返回步2继续执行迭代处理,还是停止迭代返回计算结果.系统可以指定两种迭代终止条件,一是比较相邻超级步间的结果误差是否小于指定阈值,二是迭代次数是否达到设定上限.由上述描述可知,基于MapReduce的图的计算模式的核心内容是基于节点的异步并行计算和基于邻边的同步消息传递.4.3实例:PageRank的实现现实中的很多问题都可以转化为图来处理,如以社交网络为代表的诸多应用,下面就以PageRank为改进框架中的典型应用展开介绍.PageRank[33]是一种用于搜索引擎的基于超连接结构测度网页质量的算法,其是有交互和迭代需求的图算法中最具代表性的例子.假定Web网表示为图G=(V,E),其中节点vi∈V代表网页,PR(vi)是该节点代表的网页的PageRank值,其表示浏览到Web图中页面vi的可能性,这个可能性与Web图的拓扑结构高度相关,例如指向它的页面的状况(即直接前驱集合),也就是说一个页面的PR值是由其它指向页面的PR值计算得到的,一个基本的PageRank计算公式可表示为PR(vi)r=其中,r表示迭代的轮次,s0是节点的PageRank初始值,q表示阻尼系数(详细的介绍可参阅文献[33]).在给定每个节点一个随机的PageRank初始值s0的情况下,经过多轮迭代计算,节点的PR值会趋向收敛.在MapReduce并行处理框架下计算PageRank,首先需要将图转化为键值对集合,图最简单的表示形式形如二元组〈FromVertexID,ToVertexID〉,元组元素表示有向边关联的端节点.这种最原始的图表示形式仅需通过两次简单的MR过程就可以转化为图2所示的通用形式,其中状态值代表节点的PR值.算法1.基于原始MapReduce的PageRank算法.Map阶段://图分解为图2所示的键值对G=(V,E)→[(key,value)1…]//parse:解析键值对的函数//countPageRank:计算节点PR值的函数1.MAP(Keykey,Valuevalue)2.vi=parse(value);χs(vi)=parse(value);3.PR(vi)=countPageRank(value);4.foreachvj∈χs(vi)do5.output(keyvj,valuePR(vi));6.endforeach7.output(keyvi,valuevalue);//传递原图拓扑Reduce阶段:1.REDUCE(Keyvj,Value[w1,w2,…])2.newPagerankSet←;3.foreachwi∈[w1,w2,…]do4.ifwi∈χp(vj)then5.newPagerankSet=newPagerankSet+wi;6.else7.value=wi8.endif9.endforeach10.PR(vj)=countPageRank(value,newPagerankSet);11.value=update(value,PR(vj),newPagerankSet);12.output(keyvj,Valuevalue).上述是一次MR的处理过程,运算需要多次MR过程迭代执行,并且需要在两次MR过程间增Page8加收敛判断来确定何时终止迭代过程.值得关注的是,除了更新的节点中间状态值(本例中即PR值)需要借助Shuffle机制在不同执行节点间传递,为保证下一次迭代的执行,运算过程中并没有改变的整个图的拓扑(如χp(vi)、χs(vi)以及md等)也需要在每次MR过程中反复传递和持久化.实际图运算过程中未改变的部分(图2中浅色部分)比例远大于变化的中间状态值集(图2中深色部分),如式(2)所示.所以,多次MR过程中这个代价相对于整个计算过程而言是非常大的.(∑vi∈V(χs(vi)+χp(vi)+md(vi))+∑ei∈E所以,MapReduce虽然可以执行图的运算但是并不易于表达这种有数据交互依赖和迭代需求的计算.改进的并行框架下图的计算则充分利用图本身的拓扑特征和运算特点,根据图计算内在的迭代需求,利用Map和Reduce阶段内的多个同步的超级步完成迭代.改变的中间状态值在超级步间通过消息传递,未变化的图的拓扑无需传递和持久化.对于PageRank算法甚至仅需Map-only的方式即可完成.下面给出新并行计算框架下PageRank分布式算法的实现伪代码.算法2.基于支持BSP模型的MapReduce的PageRank算法.Map阶段-setup操作:1.i=superstepCounter++;//超级步计数器2.Msgi=newuserdefineMessage();3.resetInputDataOffset();//重置输入数据访问偏移Map阶段-map操作:1.MAP(Keykey,Valuevalue)2.vj=parse(value);χs(vj)=parse(value);3.if(stopflag==false)4.if(i==0)//首次迭代,执行启动处理5.PR(vj)=countPageRank(value);6.else//执行迭代处理7.PR(vj)=countPageRank(value,newValueSeti-1);8.endif9.Msgi.setStateValue(PR(vj));10.else//终止迭代,输出11.value=update(value,newValueSeti-1);12.output(Keyvj,Valuevalue);13.endifMap阶段-cleanup操作:1.if(compare(Msgi-1,Msgi)>threshold)2.headeri=createMessageHeader(i,timestamp);3.barrierMsgi=assembleMessage(headeri,Msgi);4.send(barrierMsgi);//发送消息,执行交互处理5.sleep();//陷入等待状态6.MsgSeti=receive();//被唤醒后接受汇集消息7.newValueSeti=getNewValue(MsgSeti);8.else9.stopflag=true;//设置迭代终止标志10.endifsetup和cleanup操作仅在每次超级步的任务运算前和运算后执行,map操作则循环处理输入数据切片上的键值对.此外,新系统还提供了继承Mapper类的新超类MsgMapper和若干方法来支持在框架内部完成迭代处理,迭代控制过程对用户而言是透明的.5基于消息传递的系统框架设计与实现本节介绍基于BSP模型的改进版并行计算框架所支持的通信协议和基于开源Hadoop0.20.2版本的具体实现细节.系统引入的两种自适应的消息传递机制可以高效地处理图运算的中间状态信息的交互.5.1障栅消息障栅(Barrier)是可用于消息传递系统的一种有效的同步机制.对于计算过程中有数据依赖且不能完全独立执行的计算任务,其需要借助消息传递数据.为保证消息整体有序地传递,同一超级步内的异步并行的计算任务在发送消息后需插入障栅同步等待.插入障栅的任务会陷入等待状态,直到该超级步内所有任务通过消息完成数据的交互,然后此超级步内所有被障栅阻隔并陷入等待状态的任务才能被重新唤醒并利用接收到的汇集消息中的新数据在下一个超级步内继续运行.支持BSP模型的MapReduce并行计算框架,需要利用障栅将MR过程中的Map阶段和Reduce阶段分割成同步执行的若干超级步.一个超级步内异步并行的多个Mapper或者Reducer在发送消息后借助障栅互相等待,本文称这种在一个超级步内被障栅分隔的由任务发送的消息为障栅消息.障栅Page9消息仅能在相邻的超级步间进行传递,超级步i中任务发送的消息经过汇集处理可以被超级步i+1中的任务使用.也就是说,超级步内任务的执行完全异步,超级步间所有任务通过障栅同步,并借助障栅消息交互数据.为支持消息的传递,系统新增了预置的消息接口,用户可以根据应用的需求灵活地设计实现该接口的自定义消息类型.这种可插入式的消息设计模式保证了运行的Hadoop集群可以在不重启的情况下动态支持任意类型的用户消息,只要用户实现了系统预置的消息接口.预置的消息接口中主要设置了系统用于维护障栅消息所需要的消息元数据,而用户自定义消息的具体内容则依赖于应用的需求.障栅消息格式主要由两部分组成:消息头和消息体,每一部分的具体内容如图3所示.消息头包含的是障栅消息的元数据,其由6部分构成(图中灰色部分).其中JobID、TaskID和TaskAttemptID三者构成了具体任务(Mapper或Reducer)在集群中的唯一标识.用户编写的并行应用以作业(Job)为单元提交到Hadoop集群中执行,JobID即集群中该作业的唯一标识,TaskID即任务在作业内的唯一标识,由于推测执行机制,同一个输入片段可能同时会在不同工作节点上存在多个完全相同的冗余任务,TaskAttemptID就是用以区分冗余任务的唯一标识.所以,集群内一个具体执行计算图4轻量级消息传递实现框架的任务可以用〈Job-ID,Task-ID,Task-Attempt-ID〉三元标识组合来唯一确定.障栅消息和发送消息的任务绑定,因此该三元标识组合同时也可被系统用于区分超级步内的障栅消息.MessageType用于区分发送消息的任务类别,包含Map任务和Reduce任务两大类.StepID用于区分消息隶属的超级步,因为Map或Reduce阶段被分为一系列同步的超级步,每一个超级步都伴随着消息的传递和交互,一个超级步中的消息仅在此超级步内进行汇集,不同超级步内的消息不能混和汇集,这种同步方式简化了消息维护的代价.消息体是由用户根据具体应用需求所构造的可序列化的消息实体,其可以是基本数据类型(如整型或实型),也可以是复杂的复合类型(如容器类型或构造类型),用户可以完全控制消息内容的格式.因为消息需在集群中通过网络传递,所以必须强制要求用户按照Hadoop集群序列化的要求实现自定义消息内容的序列和反序列化逻辑.借助Hadoop提供的各种序列化操作基元,可以很容易地实现自定义消息体的序列和反序列化.消息头是由系统自动生成和维护的,消息体发送时系统会自动为其添加相应的消息头,然后完整的消息再被序列化为可在网络中传递的字符序列,在集群的工作节点间传递.此外借助动态加载技术可以让运行中的集群系统在不停机的情况下自动加载用户创建的自定义消息类型.5.2轻量级消息传递机制轻量级的消息传递机制旨在利用Hadoop现有的消息传递机制,在不影响既有功能的情况下,通过修改和新增部分通信协议以实现小体量的消息传递.图4描述了轻量级消息传递机制的实现框架.先介绍Hadoop既有的通信机制以提供技术实Page10现的背景.Hadoop分布式并行计算系统是典型的主从(Master/Slave)架构,如图4中所示集群由一个JobTracker节点和若干个TaskTracker节点组成.用户作业通过JobClient向MapReduce集群投交,JobTracker负责接受作业的投交请求,将作业自动分解为多个并行的任务,并依据数据本地化原则尽可能的将任务调度到输入数据所在的Task-Tracker节点上执行.TaskTracker节点会根据分配任务的类型启动独立的子进程Child执行Map任务或者Reduce任务.JobTracker同时还负责监控和维护整个集群中所有TaskTracker节点的运行状态以及投入作业的执行情况.状态信息及任务调度等信息主要籍由节点间的通信来传递,集群中节点间的通信是使用Hadoop的RPC(远程过程调用)机制来实现的,各节点利用RPC协调各自的运行状态确保集群流畅的运作.TaskTracker节点通过RPC协议向JobTracker发送周期性的心跳(图4中所示的heartbeat)来通知自己的当前状态,并根据自身的负载能力请求新的任务;执行具体任务的Child子进程也会通过RPC协议向TaskTracker节点发送周期性的连通指令(图4中所示的ping),利用该指令Child子进程查看作为父进程存在的TaskTracker节点的当前状态来确定继续还是终止任务的执行;JobClient则通过RPC协议向JobTracker发送投交指令(图4中所示的submitJob)提交用户作业.发起RPC请求的作为RPC客户端,而接受RPC请求并执行处理逻辑的作为RPC服务端.如图4所示,JobTracker和TaskTracker都实现了RPC服务端接受不同的RPC请求,JobClient和Child作为PRC客户端利用动态通信代理发送RPC请求,Child对于JobTracker而言是透明的,没有直接的信息交互.另外,TaskTracker同时也作为JobTracker的RPC客户端与JobTracker进行信息交互.为在集群原有的通信框架内支持任务间的障栅消息的传递,必须修改Hadoop既有的部分通信协议和新增个别专用于障栅消息传递的通信协议.系统新增了两个通信协议:send和repeater.其中send协议用于Child发送障栅消息给TaskTracker,repeater协议则用于TaskTracker把局部汇集的障栅消息转发给JobTracker.系统修改和增强了原有的两个通信协议:heartbeat和ping.其中增强后的heartbeat在保持原有功能的基础上可以通过心跳返回值定期的将JobTracker处理后的汇集障栅消息回带给发送心跳的TaskTracker,ping经过增强后可以将TaskTracker从JobTracker接受并缓存的汇集障栅消息转发给Child.接收到消息后,被障栅陷入等待状态的Child可以被唤醒,通过解析携带交互数据的信息进入下一个超级步的执行态.轻量级消息传递机制的设计原则是:执行具体任务的Child之间不建立直接的通信,以转发和分发的形式建立通信层级,与任务绑定的障栅消息经TaskTracker转发后,统一在JobTracker中汇集,然后再分发给具体任务.这样设计的原因在于:(1)首先JobTracker中维护有基于整个集群的全局的作业与任务、任务与执行节点等诸多映射信息,所以JobTracker可以利用这些全局信息维护和管理不同作业间、同一作业不同任务间、同一任务不同超级步间的障栅消息;(2)其次分布式系统中一个很重要的假设是节点失效是常态,也就是说集群中任务的执行有较大的不稳定性,直接维护Child之间的通信状态过于复杂,在高度并行的情况下也不可行.所以这种层级的通信体系能确保障栅消息有序高效地传递.5.3重量级消息传递机制利用Hadoop现有的消息传递机制实现的轻量级通信机制存在几个潜在的问题:(1)JobTracker节点是整个Hadoop集群资源管理和任务调度的唯一主控节点,其也是整个Hadoop集群中潜在的瓶颈.特别当作业密集投交时JobTracker的管理负担变得非常繁重,其计算和内存资源会更加紧缺,如果障栅消息不是小体量级的,那么维护汇集障栅消息的代价会加重JobTracker的工作负载,并且受限于轻量级通信机制的设计策略,消息的两层传递不可避免地会增加通信开销,进一步会导致集群整体吞吐量、作业完成时间等重要技术指标的下降;(2)其次利用原有的周期性心跳和连通指令回带汇集障栅消息的模式可能会产生一定的作业延迟.因为集群规模的变化会影响心跳和连通指令的工作周期,因此当集群规模增长到一定程度时,即使集群中可用于执行任务的空闲工作节点增多,需要传递消息的作业执行时间反而会变得更长.因此需要一种专用的模式能把障栅消息的维护工作从JobTracker节点中剥离出来,以更高效的方式维护整个集群中的障栅消息.5.3.1主要组件和功能重量级消息传递机制就是为应对这些可能的问题而设计出来的工作模式.系统新增了一个专门用Page11于维护障栅消息的障栅消息服务器(MessageTracker,MT),如图5所示.障栅消息服务运行于集群的独立节点中,该新增节点可以随Hadoop集群一起启动和关闭,也可以单独地启动和关闭而不影响集群既有的功能.借助新增的障栅消息通信协议,图5重量级消息传递实现框架(1)MT中建立了一个RPC服务器用于接收任务进程Child发送的障栅消息以及接受JobTracker发送的作业初始化以及用户自定义消息类型等同步数据;(2)包含一系列的维护障栅消息的数据结构和技术,提供障栅消息的汇集、过滤和裁剪等核心功能的处理;(3)MT中拥有JobTracker的RPC客户端用于与JobTracker建立通信,MT利用动态通信代理获取JobTracker所维护的各种与集群节点和作业相关的映射信息,从而保证MT与JobTracker所持有的集群状态的一致性;(4)JobTracker中建立一个连接MT的RPC客户端,将用户投入作业的信息,特别是使用了障栅消息接口的新作业的用户自定义消息类型等信息主动同步到MT中.因为作业分配等主要的工作仍然由JobTracker完成,这部分操作产生的各种映射关系是维护障栅消息时所必须的.(5)Child中建立一个连接MT的RPC客户端,用于在超级步内发送障栅消息以及以定时轮询的方式获取汇集障栅消息反馈.5.3.2主要执行流程基于独立障栅消息服务器的重量级消息传递机MT可以与原始的Hadoop集群高度协作,在完全兼容既有作业类型的基础上,同时提供对新的可传递消息的作业类型的支持.重量级消息传递机制的主要组件和功能如下(下面仅针对支持障栅消息的新作业类型展开讨论).制的主要执行流程如下(以MT与集群同步启动为例,主要描述与障栅消息相关的执行步骤,其他与原始作业相同的执行处理被省略):(1)用户根据需求在配置文件中新增MT入口随后启动集群.〈property〉〈value〉hdfs://host:port〈/value〉〈/property〉(2)开发者可使用新增接口setMTServerMode在编写并行应用作业时显式开启障栅消息服务器模式,默认设置是关闭.(3)JobTracker和MT在等待各自的RPC服务端启动后,利用动态代理互相建立连接到对方的RPC客户端.(4)JobTracker接受用户投交的支持障栅消息的新类型作业,利用新的synchronize协议把作业包所在的位置信息、作业的任务划分情况以及用户自定义障栅消息类型同步推送到MT中.(5)任务执行子进程Child启动后利用动态代理建立连接到MT的RPC客户端,当任务执行到障栅插入点时,利用send协议将携带交互数据的用户自定义障栅消息发送给MT,同时任务执行线程陷Page12入到等待状态.(6)MT汇集任务发送的障栅消息,同时利用query协议保持与JobTracker任务执行状态的一致性.(7)Child子进程利用周期性的check协议询问MT当前障栅消息的处理状况,当同一作业的同一超级步内的所有任务障栅消息汇集完毕,则借助check协议回带处理后的汇集障栅消息给Child子进程.(8)Child收到障栅消息反馈后,唤醒处于等待状态的任务线程,被唤醒的任务线程解析收到的障栅消息,获取其需要的新数据同时结合原输入数据切片,推动任务在下一个超级步中继续迭代执行.(9)任务在进入下一次障栅点前执行用户定义的收敛检查确定任务是否满足终止条件.5.3.3一致性、容错处理及可扩展性为维护障栅消息,MT中需要持有JobTracker中所维护的若干映射和状态信息的一个副本,系统采用推拉协作的更新模式确保这些信息在Job-Tracker和MT中的最终一致性.信息的更新一是采用客户端(MT)的按需请求模型利用query协议从JobTracker中用拉模式(pull)获取所需信息的最新副本;或者是采用服务端(JobTracker)急切更新(EagerUpdate)模型利用synchronize协议以推模式(push)将JobTracker中的关键更新及时同步到MT中,以期最小化不一致的时间窗口.同时副本信息以只读的形式在MT中被访问也有效消除了资源竞争的问题.一些重要的副本更新时机包括:作业投交、作业的任务切分、任务推测执行、作业完成、工作节点失效等.新作业类型执行中的容错需利用原Hadoop集群的容错机制并辅以适当的增强.Master节点会定期轮询集群中slave节点的状态,若存在无响应节点则重新调度分配该失效节点的任务到其他活动节图6障栅消息树及剪枝示意图点上重新执行,失效节点已完成的任务也需要重新执行因为任务产生的中间结果仅持久化到失效节点的本地存储中,当节点失效时执行结果也无法访问,若失效节点中包含有支持障栅消息的新作业类型的任务,则同时需要通知MT清除与这些任务绑定的障栅消息,等待重新执行的任务发送的新消息.在重量级消息传递机制中,MT也作为一个特殊的工作节点受控于JobTracker,当MT失效时,JobTracker会暂时迁移到轻量级消息传递工作模式下,所有支持障栅消息的运行中的任务都需要重新执行,因为汇集消息在MT中维护,当MT失效时这些汇集消息也无法访问(更细粒度的重做机制也可以定制实现,如指定从某一个超级步开始重做,因为篇幅原因不再详述).扩展性有多方面的衡量指标,主要包括集群工作节点规模、处理数据规模、消息规模等.本文在6.3节的实验环节中对改进框架在工作节点个数和原始数据规模的扩展性指标上进行了验证,实验结果表明框架在这些指标上具有较好的可扩展性.目前本框架适用于传递消息的频度和量都比较小的稀疏图问题,因为在实际应用中稠密图是比较少的[22],而且大多数图算法的迭代过程中仅需要交互一些信息量不大的状态数据.当消息量和频度激增时,系统的工作性能会退化到原始的Hadoop链式调用水平,但是本文设计的系统具有足够的弹性可以进行有针对性的优化,例如采用传递消息元、压缩消息体等优化技术,这部分优化工作是我们下一步重点研究的内容.5.4障栅消息处理技术所有的障栅消息维护工作都基于图6所示的障栅消息树(BarrierMessageTree,BMT),BMT是逻辑多叉树状结构,其由4个层次组成,包含:任务类型层、作业层、超级步层、任务层.Page13先建立只有根节点的空树,当障栅消息维护节点收到任务发送的障栅消息时,用算法3建立BMT,同时在叶节点上关联障栅消息,概要逻辑如下.算法3.BMT的插入算法.输入:障栅消息barrierMsg1.解析障栅消息头barrierMsg.header,获取Message-Type,JobID,StepID,TaskID,TimeStamp以及TaskAttemptID等各部分的值;2.先根据MessageType定位到BMT中正确的任务类型层,然后根据JobID创建或者定位障栅消息所属的作业层,再根据StepID创建或者定位障栅消息所属的超级步层,并将TimeStamp关联到超级步节点上,最后依据TaskID建立叶子节点;节点上.3.将障栅消息中的消息体barrierMsg.Msg关联到叶值得注意的是,具有相同TaskID不同TaskAttemptID的消息只建立一个TaskID叶节点,因为冗余的推测执行任务产生的障栅消息是完全一样的,所以实质上障栅消息是和TaskID唯一对应的.当作业执行完成、作业失效或者时间窗口过期时会自动触发BMT的删除逻辑,删除算法如下.算法4.BMT的删除算法.输入:JobID、过期时间窗口w1.if(作业完成or作业失效)2.依据JobID在BMT中进行定位;3.删除JobID为根的子树;4.else//时间窗口过期5.foreachJobID节点inBMT6.if(不是最新的StepID节点7.删除StepID为根的子树;8.endif9.endforeach10.endif算法4中时间窗口过期是基于维护障栅消息的节点的内存容量和维护复杂性的限制.作业可能在不同超级步中迭代执行多次,当下一个超级步所属的消息被接收时,同时也表明之前超级步中汇集完成的消息已经过期,那么根据用户设定的过期时间窗口,可以删除以既往的超级步节点为根的子树.之所以不在新超级步节点创建时立刻删除旧超级步节点为根的子树,是基于细粒度容错的考虑,当任务执行发生问题时,可以基于超级步的粒度重新投入执行.何时释放障栅以及回传BMT上哪些汇集的障栅消息是两个很重要的问题,这些问题在剪枝算法中解决.算法5.BMT的剪枝算法.输入:JobID,TaskTrackerID1.S←,保存TaskTrackerID标识的TaskTracker2.O←,保存可反馈消息的JobID的集合3.获取发送心跳请求的TaskTracker节点当前运行4.foreachjob∈Sdo5.releaseFlag=true;//可反馈(释放障栅)标志6.在BMT中找到job节点下最新的超级步节点;7.foreachtask∈jobdo8.if(叶子节未关联障栅消息)then9.releaseFlag=flase;10.break;11.endif12.endforeach13.if(releaseFlag==true)14.O=O+job;//所有叶节点都关联消息后该作业15.endif16.endforeach17.foreachjob∈O18.以此job的JobID为根,在BMT中剪下最新超级19.endforeach假设TaskTracker节点中正在运行的作业包括J1、J2和J3,因为J3不满足障栅释放条件,所以只有满足障栅释放条件的J1和J2的最新超级步节点被裁剪,图6中虚线框内显示了剪枝处理的结果.算法5描述的是JobTracker维护的BMT的剪枝算法,裁剪的子BMT回传给TaskTracker,同理如图所示TaskTracker也会裁剪其维护的BMT回传给Task.障栅消息的发送源会周期性地通过ping或者check协议向障栅消息维护节点发送请求,接收到请求后,BMT的剪枝算法就会被触发来决定是否反馈以及反馈哪些消息.整个消息的汇集、删除、裁剪等过程对于用户而言是完全透明的,由系统框架在后台自动处理,用户只需要关注自定义消息的内容和格式.6实验结果与分析6.1集群环境评估,集群节点的硬件环境如表1所示.实验在72节点构成的Hadoop集群[11]中进行Page14表1集群硬件环境MasterNodeE56204(8)@2.4GHz482×146GBSAS15kSlaveNodeX34304(4)@2.4GHz82×500GBSATA7.2kGigabitEthernetCentOS5.5节点间通过3个交换机和千兆比特网卡连接,24个节点为一组共置一个机架中,节点配置Hadoop0.20.2版本以及本文支持BSP模型的基于该版本的开源代码实现的改进版.6.2数据集实验主要采用标准的StanfordLargeNetworkDatasetCollection中的soc-LiveJournal作为测试PageRank的数据集,该数据集的主要特征如表2所示.DataSetNodesLinksMeta-DataVolumesoc-LiveJournal4847571689937730G1GLiveJournal-V148475716899377314G15GLiveJournal-V248475716899377329G30GLiveJournal-V348475716899377344G45Gsoc-LiveJournal是StanfordLargeNetworkDatasetCollection数据集中的有关社交网络的图数据子集.LiveJournal是一个具有强大社交网络功能的博客站点,LiveJournal数据集以〈FromNodeIdToNodeId〉二元组集合的形式提供该网络的高度抽象的有向图拓扑,其节点表示用户,边是用户间的好友关系.由于其省略了图2中提及的节点或边的元数据,原始数据集的大小约为1G,为保证实际应用时处理的合理性,实验以该数据集为基准,在不改变图拓扑结构的条件下以字符串形式增加了节点和边的元数据,形成了3个扩展的LiveJournal数据集LiveJournal-V1~V3.在实验的预处理阶段,需要将二元组形式的图转化为文中图2所描述的并行框架下的图的标准键值对形式,使用两个简单的原始MapReduce作业即可完成所有的转换工作,实验中并行处理框架的输入和输出都基于分布式文件系统HDFS.6.3评估及结果分析实验首先在固定规模的LiveJournal-V1数据集和24个节点上测试原始Hadoop和基于轻量级消息传递以及基于重量级消息传递的改进版Hadoop的总体执行性能,并用线性增加的迭代次数验证并行执行框架的可扩展性.如图7所示,PageRank分布式图算法在基于超HardDisk级步内部迭代的改进版Hadoop上的总体执行性能明显优于基于链式调用外部迭代的原始Hadoop.并且随着迭代次数的增多,改善的性能幅度也随之增大,在第20轮迭代时,基于重量级消息传递机制的改进版Hadoop执行性能相比于原始Hadoop提高近51%.因为对于链式调用的Hadoop作业,每轮迭代都不可避免地产生作业启动开销、不变数据的序列化和网络传输开销、迭代中间结果的HDFS持久化开销.相比之下,改进版的Hadoop,利用障栅消息在超级步间仅传递在整个图数据中所占比例很小的一部分随迭代操作变化的状态数据,执行过程中未改变的图拓扑数据和节点与边的元数据作为任务的输入,仍驻留在执行任务的工作节点中不需要重新加载,在任务执行到下一个超级步时只需要重置输入数据的访问偏移即可.并且由图7可以观察,随着迭代次数的增多,并行执行框架的整体执行性能表现出良好的线性可扩展性,这得益于Hadoop本身的高可扩展特性,且改进版基于Hadoop实现并继承了该并行计算框架的可扩展性.图7PageRank总体执行性能(LiveJournal-V1,24nodes)同时还可以观测到重量级消息传递机制在整体执行性能指标上是优于轻量级消息传递机制的,这是由于轻量级消息传递机制受限于集群心跳和连通指令发送的间隔,为保证JobTracker节点的工作性能,指令发送不能过于频繁,最差情况是在消息汇集完毕后仍需等待整个间隔时间该汇集消息才能被回传到消息接收节点.因此,随着迭代轮次的增加,这个间隔延时也会同步累积,并导致性能下降.图8通过增加节点规模进一步验证了这个问题.Page15MapReduce本身是一个线性可扩展的计算模型,也就是说Hadoop集群的处理能力理论上可随着集群节点数的增加实现近线性扩展.但是由图8中可以看出,当节点初始增加时总体执行时间线性下降,但是当集群节点规模增加到一定程度时,两种模式的总体执行时间的提升幅度都趋于减小,甚至对于轻量级消息传递机制,其总体执行时间在节点增加到60以后还有部分回升.其原因在于同时最大并发的任务数是由输入数据的切片个数决定的,当节点增加到一定程度时已经能满足特定大小的测试数据集最大并发性的要求,此后即使在集群中投入更多的空闲工作节点也无法再提升作业处理的并发度,总体执行时间也不会再进一步缩短.其次,对于轻量级消息传递机制的执行时间回升现象是因为,当集群节点数增加并且JobTracker的压力增大时心跳周期也会平滑地增大,因此借助轮询的心跳指令回带汇集消息的轻量级消息传递机制会受到影响,进而会增加任务移入下一个超级步的延迟.因此轻量级消息传递机制适用于非巨量待处理数据集且集群节点规模和运行压力适中的应用环境中.图9测试的是当数据集规模增大时并行处理框架对于PageRank算法表现的可扩展性.在24节点集群中通过设定的10次迭代可以看到当数据规模的增加时并行框架显示了良好的线性扩展处理能力.同时可以发现重量级消息传递机制的算法总体执行时间增加幅度最小,也说明该机制具有很好的应对海量数据规模的可扩展能力.最后图10表示的是新并行框架的兼容性测试结果,实验在原始的Hadoop集群中和支持BSP模型的改进型Hadoop集群中分别以相同的规模线性增加的数据集在24个节点上进行原始MapReduce作业的运行测试(改进型Hadoop运行时对于原始的MapReduce作业不会触发消息传递机能),实验选择最具代表型的WordCount应用作为原始的MapReduce作业.实验表明改进型Hadoop集群可以无损地兼容原始的MapReduce作业,不同类型的作业的调度由系统运行时自动处理.图10兼容性测试(WordCount,24nodes,无迭代)7总结与展望本文针对目前基于MapReduce的图算法执行性能低下的问题,在开源的Hadoop基础上通过引入大同步模型实现了一种支持障栅消息传递的改进型并行计算框架.通过将迭代过程内化到Map或Reduce阶段的超级步间,有效地减少了以往多轮作业调度的开销,为分布式大图算法的设计提供了一种高效的计算模式.实验证明相比于原始的MapReduce图算法,新计算框架下的分布式大图算法可行、高效.此外,如何解决信息交互频繁的稠密图性能退化的问题以及更广泛的机器学习、聚类等算法在该平台下的实现还有待于进一步研究.
