Page1S-RAID中基于连续数据特征的写优化策略刘靖宇1),2)谭毓安1),3)薛静锋1)马忠梅1)李元章1)张全新1)1)(北京理工大学计算机科学与技术学院北京100081)2)(河北工业大学计算机科学与软件学院天津300401)3)(北京理工大学北京市海量语言信息处理与云计算应用工程技术中心北京100081)摘要节能型磁盘阵列S-RAID通过对磁盘分组,关闭部分磁盘,降低存储系统部分性能来实现节能.为避免启动已关闭磁盘而产生额外能耗,S-RAID中的写操作全部采用“读-改-写”方式,影响了S-RAID的写性能.本研究提出一种S-RAID的优化结构:LS-RAID,在不提高存储系统能耗的条件下,优化S-RAID的写性能.LS-RAID适用于以连续访问为主的应用,通过磁盘分区,分离存储系统中的随机访问和顺序访问,降低了随机访问对存储系统性能的影响;提出一种数据增量校验算法,避免了写过程中对数据盘旧数据的读操作,降低了因“读-改-写”导致的写惩罚.实验表明,与S-RAID相比,在不增加系统能耗的情况下,LS-RAID的写速率可以提升至少56%.在提供相同写性能的条件下,LS-RAID可以关闭更多磁盘,进一步降低了存储系统能耗,提升了节能效果.关键词连续数据存储;节能;写优化;磁盘阵列;校验;绿色计算1引言由于信息量爆炸式增长,数据中心的规模越来越大,具有PB(petabyte)级存储容量的大规模存储系统已经非常普遍,例如Facebook网站的数据中心的规模已达21PB,用于存储每天新产生的约60TB~90TB(经压缩后仍达10TB~15TB)的数据[1].近年来,磁盘价格持续下降,其单位存储价格已接近甚至低于磁带和光盘,并且有着更好的读写性能.同时,固态盘因容量和价格的原因[2-3],还难以应用到大规模存储系统中.因此,硬盘已经成为大规模存储系统的一种主要存储介质[4].大规模磁盘存储系统通常采用独立磁盘冗余阵列(RedundantArraysofIndependentDisks,RAID[5-6]),将大量磁盘组合起来并行工作以提高存储系统的性能、容量和可靠性.然而,大量磁盘并行工作也带来一个不容忽视的高能耗问题.高能耗不仅使存储系统自身能耗成本的增加,还增加了额外的成本消耗.比如,高能耗所产生的高热量不仅增加了冷却系统的能耗,还使存储设备的存放密度无法提高,降低了空间利用率,这些都增加了数据中心的成本.因此,近年来对磁盘存储系统的节能研究引起了广泛关注,并取得了一些重要成果,例如基于Web应用的PARAID[7]、基于归档存储的Pergamum[8]和针对具有连续数据访问特征的应用的S-RAID[9].这些节能存储系统通过重构磁盘阵列内的数据布局,将数据访问集中到阵列中的部分磁盘上,关闭其余磁盘,通过减少处于运行状态的磁盘数量达到节能的目的.因为一些应用系统虽然对存储系统的容量和可靠性有着很高的要求,但对其性能要求却并不苛刻,大量磁盘并行可以提高存储系统的数据传输率,但是,在I/O负载较低的情况下,这种高性能并不能被充分利用,因此可以通过减少并行磁盘数量,适当降低存储系统性能来实现节能.实验表明,这些存储系统都取得了较好的节能效果.然而,以关闭部分磁盘的方式来实现节能目的的存储系统除了因并行磁盘数量减少导致的系统性能下降以外,额外的写惩罚也使得单位磁盘写性能严重下降.例如:Pergamum[8]单个节点(包括一块磁盘、一个NVRAM和一个低性能CPU)写性能低于10MB/s,仅相当于普通磁盘性能的10%左右;S-RAID[9]中所有写操作都采用“读-改-写”(read-modify-write)方式,单位磁盘写性能下降了约50%.提高单位磁盘性能,可以在处于运行状态的磁盘数目相同的情况下,提高存储系统整体性能;同理,在应用系统对存储系统性能要求相同的情况下,可以关闭更多的磁盘,使整个存储系统的节能效果更加显著.在前期研究中,我们提出一种以节能为目的的S-RAID数据布局[9],S-RAID中采用RAID4/5冗余策略,校验数据由同条带所有数据块进行异或运算得出.S-RAID的写操作通常只在部分磁盘中进行,为避免读取处于关闭状态的磁盘中的数据,写操作全部采用“读-改-写”方式.这样虽然节约了能耗,但是“读-改-写”方式需要先读取待写入磁盘的旧数据和旧的校验数据,与新数据进行异或运算,计算出新的校验数据,再写入新数据和新的校验数据,因此写效率非常低,性能损失很大.S-RAID的应用环境是以连续数据写为主要特征的应用系统(例如归档系统、备份系统、VTL系统等),这些应用系统的数据通常为一次性写入,不会被删除或者更改,新数据与旧数据无关,并且读操作非常少.本文针对S-RAID数据布局及其应用系统特征,提出一种基于日志结构文件系统(Log-StructureFileSystem,LFS)的改进型数据布局:LS-RAID(LFS-BasedS-RAID).LS-RAID对阵列中磁盘分区,分离随机访问和顺序访问,并针对LS-RAID结构提出一种新的校验值计算方法:数据增量校验(DataIncrementalParity,DIP),优化LS-RAID的写性能,改善节能效果.2相关工作2.1国内外研究现状早期对存储系统的节能研究主要根据磁盘的运行原理及其物理特征,采用多转速磁盘,通过降低单个磁盘所消耗的能量来实现,比如Carrera等人[10]Page3提出了一种动态多转速磁盘模型和使用于该模型的LD(LoadDirected)算法;Gurumurthi等人[11]提出一种DRPM(DynamicRPM)算法,根据平均响应时间和磁盘请求队列的长度来动态调整磁盘转速;Sony公司开发出一种2-转速商业磁盘[12-13].由于未考虑整个存储系统的能耗及数据安全,因此对大规模的存储系统来说节能效果有限.近年来对存储系统节能的研究主要通过控制整个系统中处于运行状态的磁盘数量来实现节能目的,比如Weddle等人[7]提出的PARAID模型、Zhu等人[14]提出的Hibernator节能存储模型、Li等人[15]提出的eRAID模型、毛波等人[16]提出的绿色磁盘阵列(GRAID).此外,Otoo等人[17]通过动态块交换算法,将“热数据”迁移到始终处于活动状态的“热单元”中,延长其余磁盘的空闲时间.针对归档存储系统,Colarelli等人[18]提出了MAID,Storer等人[8]提出了Pergamum.这些节能方案都是通过重新布局阵列中数据,使大部分I/O访问集中到少数磁盘上,通过关闭无I/O访问的磁盘来实现节能.但是这些节能方案很少考虑由此带来的额外性能损失,由于I/O访问集中在少数磁盘上,存储系统内的写操作全部为“小写”,严重影响系统性能,为满足应用系统的性能要求,不得不增加处于运行状态的磁盘数量,严重影响了整个系统的节能效果.另一方面,对存储系统性能的研究由来已久,Stodolsky等人[19-20]提出了一种Parity-logging,利用日志结构缓冲待更新的校验数据,减少校验数据更新次数.Chao等人[21]提出基于RAID6的写优化方法:RAID6L,在日志结构中缓存待写入数据,在缓冲的数据足够多后,再产生新校验数据并将新数据和新校验数据一次写入.Menon等人[22]提出利用浮动校验数据改善存储系统性能,在每个柱面划分出一定的空间作为自由区,更新校验数据时,新校验数据存入距离旧校验数据最近的自由区,每次更新只需一次I/O操作.以上方法主要针对传统RAID结构,难于应用到前面提到的大部分磁盘处于关闭状态的节能型存储结构中.2.2前期研究成果一些以连续数据访问为主的应用系统(如:归档系统、数据备份、VTL系统等),对存储系统的性能要求并不苛刻,比如数据传输率仅为120MB/s的存储系统就能基本满足归档系统的性能需求.针对这类应用的特点,我们在前期研究工作中,提出一种S-RAID[9]数据布局结构,取得了较好的节能效果.S-RAID采用局部并行策略,对阵列中磁盘分组,组内磁盘并行访问,根据对存储系统性能要求设定磁盘组内的磁盘数量,将数据访问在一定时间段内集中到一组磁盘上,通过磁盘调度算法,将超出特定时间间隔无数据请求的磁盘组关闭.S-RAID通过牺牲存储系统部分性能,在满足应用需求的前提下,取得了很好的节能效果.S-RAID中写操作采用“读-改-写”方式,性能损失较大.本文针对S-RAID数据布局,结合S-RAID内数据特征,提出一种S-RAID优化数据布局结构LS-RAID.利用磁盘分区和数据增量校验,减少由“读-改-写”带来的写惩罚,提升S-RAID的写性能.3S-RAID概述及数据特征分析RAID通过对阵列中磁盘条带化,将写操作分布在所有磁盘上并行执行,提高存储系统的性能.RAID中,即使在低I/O情况下,所有磁盘也始终处于活动状态.在一些对存储系统性能要求不高的应用中(比如归档系统需要海量存储空间和高可靠性,但对存储带宽的需求通常仅单个硬盘的数据传输速率就可以满足),磁盘并行带来的高性能并不能被有效利用,却承受着由此带来的高能耗.针对这类应用系统,我们提出了一种S-RAID[9]数据布局.S-RAID在满足应用系统对存储系统性能要求的前提下,对RAID中磁盘分组,使写操作集中在部分磁盘上,通过关闭处于空闲状态的磁盘,达到节能效果.3.1S-RAID数据布局S-RAID中可以采用RAID4和RAID5两种冗余策略,图1为由5块磁盘组成的S-RAID4的数据布局结构.Page4图1中,D0、D1、D2和D3为数据盘,P为校验盘,4块数据盘分为两个组,分别为G0,G1.G0组包括磁盘D0和D1,G1组包括磁盘D2和D3,记作,G0={D0,D1},G1={D2,D3}.SPs表示第s条带的校验数据.图1中,数据块以Bd,s表示,d表示数据块所在磁盘,s表示数据块所在条带,数据块按组排列,逻辑块地址以箭头所示顺序组内相邻.因此在连续数据写操作时,存储系统中I/O访问在一段时间内集中在某一组磁盘上,这时可关闭其余空闲磁盘组.由于S-RAID4中校验数据存储在固定校验盘上,所以同RAID4一样,校验盘成为系统性能和可靠性的瓶颈.为此,我们引入S-RAID5数据布局,将校验数据平均分配到各数据盘中,解决了S-RAID4中的校验盘瓶颈问题.但是S-RAID5中,因为校验数据块均匀分布在所有磁盘上,当一个条带写满后开始写入下一个条带时,要启动新的校验数据块所在磁盘,增加了磁盘启停频次,产生额外能耗并增加了磁盘的故障率.在条带划分过程中,条带粒度划分过大,虽然校验盘的转换间隔增大、频率降低,但是组内并行性会变差,影响系统性能;条带粒度划分过小,会因校验盘的频繁转换,导致磁盘启停次数增加,产生额外的能耗,使节能效果下降,也降低了磁盘的可靠性.针对这一问题,我们采用了条带分组策略,S-RAID5的数据布局结构如图2所示.简化起见,图2中S-RAID5由5块磁盘组成,分为Stripe0~Stripe9共10个条带,但不失一般性.校验数据均匀分布在5个数据盘上,每个条带组VGroup包括2个条带,条带组内校验盘固定,相当于一个S-RAID4结构.条带分组策略即保证了S-RAID5具有良好的并行性,又降低了磁盘启停频率.数据块逻辑地址按图中箭头方向排列.S-RAID5中校验数据均匀分布,磁盘启停频率较S-RAID4中略高,但有效避免了S-RAID4中的校验盘瓶颈,因此性能和可靠性要高于S-RAID4.S-RAID中数据块的逻辑块地址组内相邻.因此,当系统I/O访问以连续数据写为主时,存储系统的I/O请求在一定时间段内将集中在同一组磁盘上,其余磁盘无I/O请求发生,处于空闲状态.S-RAID中将这部分无I/O请求、处于空闲状态的磁盘关闭,节省整个存储系统的能耗.实验表明[9],由5块磁盘组成的S-RAID5的能耗约为传统RAID的74%,由12块磁盘组成的S-RAID5的能耗仅为传统RAID的35%左右.3.2S-RAID中的写操作与数据特征3.2.1S-RAID中的写操作全部为“读-改-写”不同层级的RAID通过采用不同的镜像和校验技术来提高存储系统的可靠性,但却因此降低了存储系统性能[6,23-24].RAID中写操作分3种情况.(1)当待写入数据恰好与RAID中的整个条带对齐时,采用“整条写”(Full-StripeWrite);(2)当待写入的数据不能覆盖整个条带,但不少于整个条带的1/2时,称为“大写”,这时采用“重构写”(ReconstructWrite);(3)当待写入数据不足整个条带1/2时,称为“小写”,这时采用“读-改-写”.除“整条写”外,“重构写”和“读-改-写”都增加了存储系统的I/O负载和延时[19].在重构写时,I/O总次数为n+1次,其中n为阵列中数据盘个数;“读-改-写”时,I/O总次数为2(m+1)次,其中m为需要写入的磁盘数目.S-RAID采用与RAID相同的校验方法,校验值的计算方法也相同,为避免启动处于关闭状态的磁盘,S-RAID中写操作无论是“大写”还是“小写”,均采用“读-改-写”方式,严重降低了系统性能.3.2.2基于NILFS2文件系统的写操作数据特征NILFS2[25](NewImplementationLog-Struc-tureFileSystem)文件系统是一种基于日志结构的文件系统,从Linuxkernel2.6.30开始,NILFS2已并入Linux内核.NILFS2采用连续快照(continu-oussnapshotting)技术,写新数据时,始终顺序写入磁盘头部.NILFS2文件系统中数据被删除后不立即回收空间,新数据仍然以线性逻辑次序顺序写入,直到逻辑存储空间末端,然后再回收已删除数据空间.NILFS2文件系统在第1次写入时具有良好的写性能.S-RAID的主要应用对象中,如归档系统、备份系统,数据通常不会被删除或修改,并且为一次性写入,非常适合采用NILFS2文件系统.在Linuxkernel2.6.35.32下,配置由5块磁盘Page5构成的S-RAID5.磁盘采用SeagateST3500630AS,容量500GB,转速7200RPM.条带中数据块大小设定为64KB,采用NILFS2[25]文件系统.应用系统、NILFS2文件系统和S-RAID的层次关系如图3所示.使用C语言编写数据模拟发生器,不间断随机产生大小介于4KB~4MB之间的二进制文件,模拟连续数据写入S-RAID5.利用blktrace工具跟踪记录S-RAID5中数据的写操作.图3NILFS2文件系统与S-RAID的层次关系blktrace是Linux内核中一个针对块设备I/O层的跟踪工具,通过blktrace,可以获取I/O请求队列的各种详细的情况,包括进行读写的进程名称、进程号、执行时间、读写的物理块号、块大小等等.于NILFS2文件系统的写操作数据特征.图4和图5分别为10min和12hS-RAID中基图4和图5中,S-RAID中基于NILFS2文件系统的写由三部分组成,其中位于存储空间首部和末端的两部分反复写入,这两部分为NILFS2文件系统的两个超级块,写操作方式为覆盖写;中间部分为一次性顺序写入,逻辑地址连续.4LS-RAID:基于NILFS2文件系统的S-RAID性能优化S-RAID主要应用于以顺序写为主的应用系统中,其目标在于节能.如前所述,“小写”对S-RAID的性能影响,使得S-RAID难以达到更优的节能效果.通过3.2节中对S-RAID基于NILFS2文件系统的数据特征分析,提出一种基于NILFS2文件系统的改进型S-RAID:LS-RAID,优化存储系统写性能.LS-RAID首先对阵列中磁盘分区,将随机读写数据和顺序读写数据分别存放,利用缓存策略,减少随机读写对存储系统性能的影响,同时采用新的校验值计算方法:数据增量校验算法,降低由“读-改-写”带来的写惩罚,优化存储系统写性能.LS-RAID的主要特点如下.(1)在阵列中每个磁盘的尾部划出一个保留分区,用于存放超级块,称为“超级块分区”.将原NIL-FS2文件系统位于存储结构中首端与末端的超级块存放在该保留分区中.(2)改进校验数据计算方法,所有磁盘的数据分区组成S-RAID结构,并采用新的校验值计算方法(数据增量校验).采用数据增量校验算法的前提条件是数据顺序写入.(3)设置一个写地址指针,记录阵列中最后一个写入的数据块地址,用于数据恢复时确定参与校验的磁盘和需要读取的数据块.略,提高写效率.4.1磁盘分区由3.2.2节可知,NILFS2文件系统系统存在两个超级块,分别位于逻辑存储空间的首部和末端.超级块的写操作非常少,但是存在于整个I/O过程中,这主要对S-RAID造成以下影响.(1)S-RAID中的第1组磁盘和最末一组磁盘因为一直存在对超级块的访问而无法关闭,影响S-RAID的节能效果.(2)对超级块的访问为随机访问,增加了磁盘的寻道次数,影响S-RAID的整体性能.另外,采用数据增量校验要求数据顺序写,而超(4)采用磁盘组内写对齐和旧校验数据预读策Page6级块的存在使得存储系统总是存在少量随机写操作.为克服以上缺点,LS-RAID中对阵列中磁盘分区,如图6所示.LS-RAID中每个磁盘分为两部分:数据分区和超级块分区.所有磁盘数据分区组成S-RAID.超级块分区独立于S-RAID,在S-RAID初始化时,不条带化.在写数据时,直接将NILFS2文件系统的超级块写入超级块分区.因为NILFS2文件系统中的两个超级块的大小都为4KB,所以将超级块分区设定为8KB,以同时容纳两个超级块.4.2超级块分区的写操作LS-RAID中将NILFS2文件系统的超级块存放在超级块分区,单独读写.下面分别对由5块磁盘组成的LS-RAID4和LS-RAID5中超级块的写操作做详细介绍.4.2.1LS-RAID4中超级块的写操作由5块磁盘组成的LS-RAID4中超级块的写操作步骤如下.(1)存储系统开始写入数据时,首先写入G0组磁盘,因此G0组内磁盘D0和D1为活动状态,超级块写入到D0和D1的超级块分区中,D0和D1中两个超级块分区互为镜像.(2)当G0组磁盘即将写满时,S-RAID启动G1组磁盘,D2和D3转为活动状态.(3)当G0组磁盘写满后,数据开始写入G1组磁盘,将D0和D1中的超级块分别复制到D2和D3的超级块分区中,超级块的读写转到D2和D3的超级块分区.D2和D3中两个超级块分区互为镜像.(4)关闭磁盘D0和D1.4.2.2S-RAID5中超级块的写操作由5块磁盘组成的LS-RAID5中超级块的写操作步骤如下.(1)存储系统开始写入数据时,首先写入G0组磁盘的条带组VGroup0,此时G0组内磁盘包括D0和D1,超级块写入到D0和D1的超级块分区中,D0和D1中两个超级块分区互为镜像.(2)数据写满前3个条带组后,开始写入VGroup3,此时G0组内磁盘包括D0和D2,将磁盘D1超级块分区中数据复制到磁盘D2中,由D2中超级块分区代替原D1中超级块分区.(3)数据写满VGroup3后,开始写入VGroup4,此时G0组内磁盘包括D1和D2,将磁盘D0超级块分区中数据复制到磁盘D1中,由D1中超级块分区代替原D0中超级块分区.(4)写满G0组后,数据按G1组内数据块逻辑顺序依次写入,超级块的读写位置按步(2)和(3)的方式依次转换.每个磁盘的超级块分区中都同时存放原NIL-FS2文件系统首端和末端的两个超级块,两个磁盘的超级块分区互为镜像,提高数据可靠性.由图4和图5可以看出,超级块的写为覆盖写,但是写超级块的频率非常低,因为超级块常驻内存,只是定期刷回磁盘.另外,超级块分区的大小仅为8KB,所以超级块的读写以及在不同磁盘之间复制几乎不会对存储系统的读写性能产生影响,同样单独划分的超级块分区所造成的空间损耗也几乎可以忽略不计.除超级块分区外,所有磁盘的数据分区以图1或图2中的数据布局结构组成S-RAID4/5,用以存储数据.S-RAID部分中的写操作将全部为顺序写.4.3数据分区的写操作4.3.1数据分区的顺序写操作通过4.1节提出的磁盘分区,超级块存放在“超级块分区”,LS-RAID数据分区(等同于S-RAID)中的写操作将全部转换为顺序写.根据图4和图5中基于NILFS2文件系统的数据访问特征可知:当对LS-RAID中某个数据块进行写操作时,对于任意数据块,如果其逻辑块地址小于当前写入数据块的逻辑块地址,表明该数据块已写入数据;同样,对于任意数据块,如果其逻辑块地址大于当前数据块的逻辑块地址,表明该数据块未写入数据,如图7所示(图中超级块分区部分未标出).Page7图7中,数据块的逻辑块地址LBA按箭头方向升序排列.按箭头指向次序,B0,0~B3,2已有数据写入,B3,2为最后一个写入数据的数据块.由数据块B2,3起始到阵列逻辑地址的最末端,均为未写入数据的空数据块,写地址指针PLBA值为最末写入的数据块B3,2的逻辑块地址.基于以上结论,所以待写入数据在LS-RAID中均为新增数据,通过改进校验数据的计算方法,采用数据增量校验,优化LS-RAID中的写操作性能:只计算同一条带中已写入数据的数据块校验值,当空数据块写入数据时,再被计入校验值.同时,设置一个写地址指针,记做PLBA,记录已写入数据的最末端逻辑块地址,即最后一个写入数据的数据块逻辑地址(如图7中,PLBA←(B3,2)).在LS-RAID初始化时,做如下设置.(1)对阵列中磁盘进行条带化并进行磁盘分组(在S-RAID5中,还要进行条带分组);(2)地址指针PLBA置为“-1”,表示此时阵列内为空,所有数据块均未写入数据.同时,LS-RAID采用磁盘组内写对齐和校验值预读来提高系统写性能.(1)磁盘组内写对齐.设置缓冲区,缓存待写入数据,当缓冲区内的数据占满一个磁盘组的整个条带时,再写入阵列,可以实现同一组内磁盘的整条写.写对齐可以减少计算量,提高并行性,提升写效率.(2)校验值预读.如前所述,通过对磁盘分区,LS-RAID数据分区中写操作全部为顺序写,由此可知,当前写操作的逻辑上的下一个条带为即将写入的条带,可以预读该条带的旧校验值参与下一步新校验值的计算.适当设定预读条带数目可以最大化系统的写性能,降低写延迟时间.“读-改-写”时需要先读取待写入数据块的旧数据和旧的校验数据,但在LS-RAID中,待写入数据块的旧数据无意义,没有计入校验值,因此无需读取待写入数据块的旧数据.另外,在写第1组磁盘时,新数据相异或产生校验数据直接写入检验数据块即可,无需读取旧校验数据.因此,LS-RAID的写操作分两种情况.(1)写第1组磁盘时,校验值计算公式为式(1)中,Pnew为新产生校验数据,Di为第1组磁盘数据.计算出校验值后,将数据写入数据盘,将校验值写入校验盘,设置指针PLBA的值,指向写入的最后一个数据块.以数据块形式表示式(1)可改写为式(2)中SP为校验数据块,s为条带号,B为阵列内数据块,NG为磁盘组内磁盘数.例如:在写G0磁盘内数据块B0,2和B1,2(如图1所示)时,校验数据块SP2的值由B0,2和B1,2直接相异或得出:SP2=B0,2B1,2.计算出校验值后,将数据写入B0,2和B1,2,将校验数据写入SP2,设置指针PLBA的值,指向B1,2,PLBA←(B1,2).“读-改-写”时,需要读取B0,2和B1,2中旧数据和SP2中旧校验数据,计算完成后写入B0,2和B1,2新数据和SP2新校验数据,存储系统的I/O次数为6次,而LS-RAID中只需写入B0,2、B1,2和SP2,I/O次数减少为3次,仅为优化前的50%.(2)写其余组磁盘时,校验值计算公式为式(3)中,Pnew为新产生校验数据,Dnew为待写入数据,Pold为旧校验数据.待写入数据块的旧数据无意义并且没有计入校验值,因此计算新的校验数据时,无需读取待写入数据块的旧数据.但需要读取旧的校验数据.当写入第g组磁盘时,以数据块形式表示式(3)可改写为例如:在写G1组磁盘内数据块B2,2和B3,2(如图1所示)时,校验数据块SP2的值是由同条带内B0,2和B1,2的值计算得到的,这时需要读出SP2内的旧校验数据.新的校验数据通过B2,2和B3,2中待写入数据和SP2内的旧校验数据异或运算得到计算出新的校验值后,将数据写入B2,2和B3,2,将新的校验数据写入SP2,设置指针PLBA的值,指向B3,2,PLBA←(B3,2).在此过程中,存储系统需要读取旧校验数据,写新数据和新校验数据,I/O次数共4次,为优化前(6次)的67%.LS-RAID在写非第1组磁盘时采用校验值预读策略,设当前写入数据的条带为第s条带,因为LS-RAID中为顺序写,则下一个要写入的条带应为第s+1条带,预读取第s+1条带的旧校验数据,可以减少甚至避免因读旧校验数据带来的写延迟.实际应用中,通常预读第s+1~s+n条带,n的值可根Page8据存储系统不同配置来设定,以实现系统最佳性能.此外,LS-RAID初始化时,只需设置条带化和分组信息以及PLBA的初始值,不需要对数据盘和校验盘进行初始化,缩短了系统初始化时间.4.3.2垃圾回收NILFS2文件系统通过连续快照(continuoussnap-shotting)技术能够将随机写转换为顺序写.数据修改时,新数据被写到日志的头部,而旧数据仍然保留,数据删除也不立即回收空间,直到需要对旧数据进行垃圾收集.NILFS2文件系统进行垃圾收集后,回收的存储空间再次写入数据时,不再适用于数据增量校验,只能采用“读-改-写”方式写入数据,这时,LS-RAID的写性能降低到S-RAID的水平.LS-RAID的典型应用中,比如归档系统、备份系统,数据通常为一次性写入,很少被修改或删除,放弃垃圾回收实际损失的存储空间非常有限.因此,放弃垃圾回收,牺牲部分存储空间,可以以极小的代价避免LS-RAID出现性能瓶颈.4.4数据恢复大规模存储系统中,磁盘失效是一个不容忽视的问题.LS-RAID4/5采用RAID4/5冗余校验技术,可以容忍同一时间内任意一块磁盘的失效.当阵列中有磁盘失效后,必须及时更换失效磁盘并进行数据恢复,以避免因第2块磁盘故障导致整个存储系统数据失效.RAID4/5中校验盘失效可通过式(6)重新恢复,式(6)中,Diskparity为失效校验盘校验数据,Diski为数据盘数据.RAID4/5中失效数据盘的恢复可以通过校验值计算公式的逆运算获得,式(7)中,Diskfailure为失效数据盘数据,Diskremain为未失效数据盘数据,Diskparity为校验盘校验数据.未采用写优化的S-RAID4/5可使用与RAID4/5相同的方法恢复数据.但是数据恢复时需要预先启动全部磁盘,使存储系统能耗急剧增大.LS-RAID中,由于校验数据不是阵列中所有数据盘的异或校验,因此当有磁盘失效时,不能使用上述方法进行数据恢复.LS-RAID中磁盘失效分3类:未写入数据的数据盘失效、已写入数据的数据盘失效和校验盘失效.下面分别介绍3类磁盘失效的数据恢复方法.(1)未写入数据的数据盘失效.LS-RAID中数据按逻辑块地址次序顺序写入数据盘,因此阵列中存在着未被写入有效数据的磁盘.因为这部分磁盘未被使用,所以失效概率极低,但并非不存在.这类磁盘失效后只要使用新的磁盘替换失效磁盘并进行简单初始化(如条带化、条带分组等)即可,不需要进行数据恢复.需要指出的是,在传统RAID中,即使无数据写入的磁盘失效,也需要通过式(7)进行数据恢复,因为磁盘中的数据(即使无意义)的改变会使校验值失效.(2)已写入数据的数据盘失效.当一个已写入有效数据的数据盘失效后,根据地址指针PLBA由式(8)和(9)计算当前写入块所在的条带号s和磁盘号d.d=((PLBA-LGg)modNGg)+g×NG(9)式(8)、(9)中,NG表示每组磁盘个数,g表示PLBA指向的数据块所在磁盘组g=PLBAS×NG中划分的条带总数,LGg是该块所在组起始块地址LGg=g×S×NG.(1)如果失效磁盘为当前组内磁盘,只需恢复0~s条带内的数据,条带号大于s的区域还未写入数据,因此不需要进行数据恢复.数据恢复时需要读取0~g组中0~s条带内的数据和校验数据,设失效磁盘编号为i,通过式(10)恢复失效磁盘数据块:Bi,m=B0,mB1,m…Bi-1,mBi+1,m…式(10)中,m为条带号.例如,在LS-RAID4中,当前数据写入到第2条带后,即写入数据块为B2,2和B3,2(如图7所示),此时磁盘D2失效(如图7所示).用新数据盘替换掉失效的D2后,需要恢复B2,0、DB2,1和B2,2的数据,B2,m=B0,mB1,mB3,mSPm(m=0,1,2)(2)如果失效磁盘为非当前g组内磁盘(第0~g-1组),设阵列中最大条带号为S,失效磁盘数据恢复分为两部分:0~s条带以及(s+1)~S条带.0~s条带内的数据恢复需要读取0~g组内的所有磁盘数据以及校验数据,通过式(10)可恢复数据.(s+1)~S条带内的数据恢复需要读取0~(g-1)组内的所有磁盘数据以及校验数据,通过式(12)可恢复数据Page9Bm,i=B0,mB1,m…Bi-1,mBi+1,m…假设图7中失效磁盘为D0,则磁盘数据恢复分为两部分B0,m=B1,mB2,mB3,mSPm(m=0,1,2)(3)校验盘失效.校验盘失效时,如果PLBA的值为“-1”,说明阵列中还没有数据写入,等同于第1种情况,只需将其替换即可.如果PLBA的值非“-1”,根据地址指针PLBA计算出当前写入块所在的组号g和条带号s,校验盘的数据恢复也分两部分:0~s条带以及(s+1)~S条带.0~s条带部分校验值由0~g组同条带同位置数据块计算获得,可根据式(14)计算恢复数据;(s+1)~S条带部分校验值由0~(g-1)组同条带同位置的数据块计算获得,可根据式(15)计算恢复数据SPm=B0,mB1,m…B(g+1)×NG-1,m(0ms)SPm=B0,mB1,m…Bg×NG-1,m(s<mS)(15)假设图7中失效磁盘为校验盘P,则校验数据恢复分为两部分SPm=B0,mB1,mB2,mB3,m(m=0,1,2)与S-RAID相比,LS-RAID在磁盘失效后进行数据恢复时,读盘数量、次数以及计算量都有所减少,磁盘恢复时间缩短,提高了存储系统的可靠性.另外,在能耗方面,由于不需要启动尚未写入数据的磁盘,因此,可以降低存储系统能耗.5性能测试及能耗大规模存储系统通常有数百甚至数千磁盘组成,为了方便管理,通常将整个存储系统划分为若干子存储系统,每个子系统一般有12块或16块磁盘组成,构成一个RAID结构,称为RAID子系统.为测试LS-RAID的性能和节能效果,在Linuxkernel2.6.35.32下,构建了由12块硬盘组成的LS-RAID5,条带内数据块大小为64KB,对比测试相同配置下的S-RAID5,实验结果适用于大规模存储系统.服务器参数见表1,磁盘参数见表2.接口类型接口类型存储容量活动状态能耗空闲状态能耗关闭状态能耗启动时间5.1性能测试LS-RAID分组规模可以根据应用系统需求调整大小,即根据应用系统对存储性能的要求不同,设定不同的磁盘组规模.实验中分别对由12块磁盘组成的S-RAID5和LS-RAID5进行了3种不同分组方案的测试,分别为每组1块磁盘、每组2块磁盘和每组3块磁盘.实验中,我们利用Iometer磁盘测试工具,在100%连续负载下,对不同分组方案的S-RAID5采用写优化策略前后分别进行测试,如图8、图9和图10所示.图8所示为对每组只包含1块磁盘S-RAID5和LS-RAID5进行不同写请求大小的连续写测试结果.当写请求小于64KB时,S-RAID的写性能非常差,数据传输率低于5MB/s(如图8所示),此时写优化对存储系统性能影响也最大,LS-RAID较S-RAID的性能提升非常明显,性能提升最大值出现在写请求为16KB时,加速比达586%,这主要是因为写对齐减少了校验值的计算次数,提高了写效率,并且预读策略减少了存储系统的写延迟,因此对系统性能影响较大.但此时无论是S-RAID还是LS-RAID,写性能绝对值都非常低,LS-RAID的数据传输率最大值仍低于20MB/s.当写请求增大到128KB时,写性能有了较大的提高,最大值出现在写请求大小为1MB时,S-RAID和LS-RAID的数据传输率分别为52.7MB/s和82.3MB/s.当写请求较大时,对于顺序写操作,写对齐策略对性能影响不大,此时加速比仅约为156%,但数据传输率的绝对值提升了近30MB/s.当写请求大于1MB/s时,系统的性能趋于稳定,S-RAID的数据传输率约为50MB/s,LS-RAID的数据传输率为80MB/s左右.Page10图8S-RAID与LS-RAID性能对比测试(1磁盘/组)图9S-RAID与LS-RAID性能对比测试(2磁盘/组)图10S-RAID与LS-RAID性能对比测试(3磁盘/组)图9和图10分别为每个磁盘组包括2块磁盘和3块磁盘的S-RAID和LS-RAID的性能测试结果.与图8对比可知,采用不同分组规模大小的S-RAID和LS-RAID性能变化基本一致.在写请求大小偏小时,LS-RAID较S-RAID性能提升明显,但是存储系统的整体性能并不理想,数据传输率很低.每组2块数据盘的LS-RAID较S-RAID的写性能加速比最大值出现在写请求大小为32KB时,加速比达507%;每组3块数据盘的S-RAID和LS-RAID性能加速比最大值出现在写请求大小为16KB时,加速比达395%.图9中在写请求大于64KB时,图10中在写请求大于128KB时,存储系统性能开始明显提升.每组2块数据盘的S-RAID性能最大值出现在写请求大小为1MB时,数据传输率为80.1MB/s,LS-RAID数据传输率最大值131.6MB/s,性能提升64%.每组3块数据盘的S-RAID性能最大值出现在写请求块大小为2MB时,数据传输率为123.2MB/s,而LS-RAID的最大值出现在写请求大小为4MB时,数据传输率为196.9MB/s.写请求大小为2MB和4MB时的性能提升分别为60%和63%.实验表明,LS-RAID在以连续写为主要特征的应用中,避免了对数据盘旧数据的读操作,降低了因“读-改-写”带来的写惩罚,同时,结合磁盘组内写对齐策略以及对旧校验数据的预读,使存储系统性能得到显著提升.5.2能耗磁盘的能耗包括两部分:电机能耗(±12V)和磁盘中电子设备能耗(±5V).参照文献[9]中的实验方案,在磁盘与电源之间接入万用表(如图11所示).S-RAID和LS-RAID的能耗包括所有处于运行状态的磁盘和处于关闭状态的磁盘能耗之和.服务器定期采样万用表的电流值,通过公式W=∑iIi,计算S-RAID和LS-RAID的实时功率.图12和图13分别为S-RAID和LS-RAID不同分组方案下1h实时能耗.Page11为获得稳定能耗值,图12和图13数据均在系统正常运行0.5h后开始采样,采样周期为1s.图12为S-RAID每组分别包括1块磁盘、2块磁盘和3块磁盘时的实时能耗.3种分组方案的平均能耗分别为14.128W、18.884W和23.393W.图13为相同情况下LS-RAID的实时能耗,3种分组方案的平均能耗分别为13.934W、17.991W和22.522W.LS-RAID数据分区内数据全部为顺序写,同时,由于超级块常驻内存,只是定期写回超级块分区,减少了磁盘的寻道次数因此,LS-RAID的能耗峰值和峰值出现的次数都有所降低,平均能耗也略有减少但并不明显.对比写性能测试结果可以看出,LS-RAID较S-RAID的写性能能耗比有较大提升,图14为S-RAID在写请求大小为1MB/s时采用优化策略前后的写性能能耗对比图.图14S-RAID与LS-RAID1h写性能/能耗对比图由图14可知,在相同能耗条件下,3种分组方案的LS-RAID较S-RAID写性能能耗对比值分别提升了58.34%、72.45%和69.42%.在应用系统对性能需求不变的情况下,LS-RAID需要开启的磁盘数量更少,达到了更佳的节能效果.表3列出了不同系统最高带宽需求下,S-RAID和LS-RAID所需要开启的数据盘数目及其节能效果.表3性能需求与S-RAID所需开启的磁盘数目系统最高带宽需求/(MB/s)<50<80<120<130<160<190表3表明,在对应用系统提供相同写性能时,LS-RAID需要同时处于运行状态的磁盘数目明显减少.例如当应用系统的最高带宽需求低于50MB/s时,存储系统需要开启1数据盘+1校验盘即可满足性能要求.当应用系统的最高带宽需要为100MB/s时,S-RAID每组磁盘数需要3块才能满足系统性能需求,而LS-RAID每组2块数据盘即可,同时处于运行状态的数据盘数目由原来的3块减少到2块(除数据盘之外,还要开启1块校验盘).处于运行状态磁盘数目的减少,进一步降低了LS-RAID的能耗.因此,与S-RAID相比,LS-RAID可以在开启相同数目磁盘情况下,提供更高的系统性能;同理,在提供相同系统性能时,可以减少处于运行状态的磁盘数目,进一步降低存储系统的能耗.5.3数据恢复与可靠性S-RAID4/5采用RAID4/5冗余方案,可以同时容忍1个磁盘失效.其可靠性可由式(17)表示[5,9],MTTFS-RAID=Nactivedisksins-raid×(Nactivedisksins-raid+1)×MTTRdisk式(17)中,MTTFS-RAID表示S-RAID的平均失效间隔时间(themeantimetofailure),MTTFdisk表示磁盘的平均失效间隔时间,MTTRdisk表示磁盘的平均修复时间(themeantimetorepair),Nactivedisksins-raid表示S-RAID中处于活动状态的磁盘数.在LS-RAID中,设磁盘容量表示为Cfull,磁盘失效时平均已写入数据量表示为Cwrite,由4.4节内容可知,与S-RAID相比,LS-RAID修复磁盘时所读取数据量、计算量和写入数据量都有所减少,磁盘修复时间要小于Cwrite/Cfull×MTTRdisk,则LS-RAID的平均失效间隔时间为MTTFLS-RAID≈Cwrite×Nactivedisksins-raid×(Nactivedisksins-raid+1)×MTTRdiskPage12假设磁盘失效时平均写入数据Cwrite=1/2×Cfull,则MTTFLS-RAID≈2×MTTFS-RAID,LS-RAID的平均失效间隔时间比S-RAID延长了近一倍.实验中,由12块磁盘组成的S-RAID5和LS-RAID5,数据盘为D0~D10,分成5个磁盘组,校验数据均匀分布在所有数据盘上,磁盘D11为热备盘.为方便计算,磁盘数据分区大小设定为440GB,分表4数据恢复失效磁盘D3D0D1D2D3D4D5D6D7D8D9D1S-RAID7208906—7203506720350572089067208906720890672089067203505720890672089064644.95221099.60LS-RAID7208906—494906023276216553606553606553606553603616236553606553603952.2869537.15S-RAID720890672089067203505—72089067208906720890672089067203505720890672089064753.15226249.90LS-RAID233308323330832328003—00003615406553606553601081.0119374.01时对旧数据的读操作,降低了“读-改-写”带来的写表4中数据分别对失效磁盘D1和D3数据恢复惩罚,提高存储系统的写性能,时,S-RAID和LS-RAID的修复时间、能耗和读操实验表明,与S-RAID相比,在能耗不变的情作,其中,修复时间不包括存储系统检测故障盘时间况下,LS-RAID的写性能有了较大提高,降低了单以及磁盘替换时间.在写入1TB数据后,数据盘D0位性能下的能耗.和D1被写满,D2和D3写入了前3个条带组,最后写入的数据位于D2和D3的VGroup2,校验数据位于D8(VGroup2未写满).D1磁盘失效后,数据恢复需要读取D0、D2和D3中已写入数据和各盘中校验数据.D3磁盘失效后,数据恢复需要读取D0、D1和D2中相同位置数据以及D8、D9和D10中的校验数据.由表4中可以看出,与S-RAID相比,LS-RAID中数据恢复所需时间、能耗以及对磁盘的读操作都大幅减少.失效磁盘已写入数据越少时,节省能耗和修复时间效果越明显.表4中,D2、D3和D8中读取的数据块较理论值要小,这是因为最后写入磁盘的部分数据和校验数据仍然驻留内存,这部分数据无需再从磁盘中读出.文献[9]表明S-RAID较RAID有着更高的可靠性,表3和表4表明,LS-RAID可以有效降低存储系统中处于活动状态磁盘数Nactivedisksins-raid和磁盘修复时间MTTRdisk,在磁盘参数不变的情况下,增加了LS-RAID的平均失效间隔时间MTTFLS-RAID,提高了存储系统的可靠性.6结论与展望本文针对S-RAID在写数据时因写惩罚导致的性能下降,提出一种优化结构LS-RAID.LS-RAID通过磁盘分区减少了随机访问对存储系统性能的影响.同时数据增量校验算法避免了写入数据为11个条带组,每个条带块大小为64KB.在写入1TB数据后,人为设置磁盘失效,利用blktrace工具跟踪并记录S-RAID和LS-RAID在数据恢复中的I/O请求,并通过5.2节中的实验方案测量并计算数据恢复过程中S-RAID的能耗.表4为数据恢复时的I/O请求和能耗.采用数据增量校验的前提条件是数据顺序写入,为满足条件,本研究中使用了特定的日志式文件系统NILFS2,降低了S-RAID应用的普遍性,未来工作之一是通过区间映射管理和写缓冲策略,使这一优化策略适用于不同文件系统.另外,考虑引入SSD固态盘,单独存储和管理超级块和元数据,提高S-RAID的读写性能.
