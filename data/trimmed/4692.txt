Page1一种频繁模式决策树处理可变数据流王志海2)1)(北方民族大学计算机科学与工程学院银川750021)2)(北京交通大学计算机与信息工程学院北京100044)摘要数据流中可能包含大量的无用信息或者噪声,频繁模式挖掘可以去除这些无用信息,且频繁模式比单个属性包含了更多的信息.因此,挖掘频繁的、有区分力的模式,可以用于有效的分类.该文提出一个两步骤算法PatHT(Pattern-basedHoeffdingTree)生成决策树用于可变数据流分类.第一步,设计增量更新算法CCFPM(Constraints-basedandClosedFrequentPatternMining),用于生成闭合约束频繁模式集合CFPSet(ClosedFrequentPatternSet).CCFPM中采用滑动窗口模型和时间衰减模型处理实例,设计一种均值衰减因子设置方法得到高完整性和准确性的模式集合.第二步,增量更新方法HTreeGrow(HoeffdingTreeGrowing)生成基于CFPSet的概念漂移决策树.该方法使用概念漂移检测器监督概念改变,自动调整分类模型.针对高密度和低密度的数据流,设计了不同使用模式集合的方法.在真实和模拟数据流上的实验分析表明,与其他同类算法相比,提出的方法对稳态数据流处理时可以明显提高正确率或可以明显降低训练时间,在处理不同概念漂移特性的可变数据流时也具有很好的分类效果.关键词分类;可变数据流;决策树;频繁模式挖掘;Hoeffding树;数据挖掘1引言随着数据流挖掘应用日趋广泛,数据流分类问题已成为一项重要且充满挑战的工作.数据流与传统的静态数据或数据库相比具有非常不同的特性,如动态、无限、高维、有序、非重复性、高速和随时间变化[1].在真实的数据流环境中,大部分数据流是可变的,即具有概念漂移[2]特征,称为可变数据流或概念漂移数据流.根据可变数据流的特点,一个有效的分类器必须能跟踪并快速适应其概念的变化.已有的处理可变数据流的分类模型包括决策树、神经网络、规则学习等.尽管存在多种方法,但决策树模型是在线数据流分类的最先进方法.原因很大一部分来自于它们有能力快速地处理大量的数据,这超出任何其他流或批处理学习算法[3].最有影响力的算法之一是快速决策树VFDT(VeryFastDecisionTree)[4].它是一种基于Hoeffding不等式针对数据流挖掘环境建立分类决策树的方法.它通过不断地将叶节点替换为分支节点而生成,即在每个决策节点保留一个重要的统计量,当该节点的统计量达到一定阈值,则进行分裂测试.其最主要的创新是利用Hoeffding不等式确定叶节点变为分支节点所需要的样本数目.该算法仅需扫描一次数据,具有较高的时空效率,且分类器性能近似于传统算法生成的分类器.VFDT的不足在于不能很好的处理概念漂移问题.算法CVFDT(Concept-adaptingVeryFastDecisionTree)[5]对VFDT进行了扩展以快速解决概念漂移数据流的分类.其核心思想是当新的子树分类更准确时,用新的子树替换历史子树.它维持一个滑动训练窗口,并通过在样本流入和流出窗口时更新已生成的决策树使其与训练窗口内样本保持一致.VFDTc[6]算法扩展了增量树学习方法,从两个方面进行提高.一是设计二元搜索树用于处理数值属性;二是改进了使用,在树的叶子节点上使用朴素贝叶斯来训练实例.这种方式可以明显提高树的预测正确率.不足之处是在二元树上的统计可能相对比较大,尤其是当实例的数值属性具有许多独特值时.HOT(HoeffdingOptionTrees)[7]在常规Hoeffding树的基础上增加了附加可选节点,允许进行多个测试,得到多个Hoeffding子树作为独立路径.它们由独立结构组成,可以有效的表示多棵树.一些特殊的实例可以沿树的多个路径向下,有利于以不同的方式进行不同的选择.HAT(HoeffdingAdaptiveTree)[8]也采用了Hoeffding树,它主要的优点在于不需要考虑数据流变化的速度和频度.它使用概念漂移检测器ADWIN[9]来监控树分支的性能.如果新的树枝可以得到更高的正确率,则使用新树枝代替导致正确率降低的树枝.AdoHOT(AdaptiveHoeffdingOptionTree)[10]是在HOT的基础上做了改进:每个叶子存储当前误差的估计值.在投票过程中的每个节点的权重正比于误差的倒数的平方.算法ASHT(Adaptive-SizeHoeffdingTree)[10]是Hoeffding树的衍生,包括两处不同:(1)它设定了分裂节点的最大数目;(2)当一个节点分裂后,如果ASHT的节点数目高于最大限定值,则删除一些节点来降低树的大小.决策树模型可以得到好的分类正确率,且可以用于实现简单和有效的集成分类模型.如EM[1]是处理可变数据流的自适应集成分类方法,它可以用于检测新类.它采用传统的集成分类方式处理数据流,并且不断地自动更新以适应概念变化.但是对于新类,使用聚类方式检测.OBag和ORF[3]是基于决策树的在线数据流回归的集成方法.OBag是一种在线基于Hoeffding模型树的打包集成方法,ORF是一种随机森林方法,采用随机模型树学习方法作为基本的构建模块.已有的决策树分类方法处理的数据流中包含无限数据,这些数据可能包含大量的无用信息甚至是噪声,而模式发现可以去除数据中的无用信息且不受噪声的影响.因此,挖掘有趣的、频繁的和有区分Page3力的模式,可以用于有效的分类.基于模式的分类具有更高的准确性,并且可以很好地解决缺失值的问题.为此,本文研究一种新的基于频繁模式的决策树训练方法,用于处理可变数据流.主要的工作包括:(1)已有的数据流分类流程包括3步骤:输入-训练-模型[10],为了提高模型的创建效率和分类正确率,提出了4步骤处理流程:输入-模式-训练-模型;(2)设计一种均值衰减因子,用于时间衰减模型(TimeDecayModel,TDM),目的是发现更加完整和准确的频繁模式集合;(3)在TDM基础上,设计一种增量更新数据流闭合频繁模式挖掘方法,约定约束使得发现的模式都具有类属性;(4)设计一种基于模式的自适应调整概念漂移决策树,使用概念漂移检测器检测概念变化从而自动调整分类模型.2预备知识本节介绍数据流中频繁模式挖掘和时间衰减模型相关概念,决策树分类方法和概念漂移处理方法等相关知识.2.1频繁模式挖掘数据流DS={T1,T2,…,Tm,…}是一个有时间顺序的、连续的、无限的事务(Transaction)/实例(Example、Instance)序列.其中Tm(m=1,2,…)是第m个产生的实例,它是一个元组,可以表示为形式〈X,Cid〉.其中X是一些条件属性值的集合,Cid是该实例所属类标号.由于数据流是无限的和不断流动的,项集P的支持数定义为已出现的M个实例中包含项集P的实例数据个数,记为freq(P,M)[11](简记为freq(P)).若项集P为频繁模式则应满足定义1.定义1(频繁模式).令M为数据流中已有实例的个数,θ(θ∈(0,1])为最小支持度阈值.如果项集P满足freq(P)θ×M,则P为频繁模式.频繁模式挖掘存在的最大问题是生成结果集中包含数量巨大的模式.为了减少模式的数量,通常挖掘压缩模式.闭合模式是一种常用的无损压缩模式,它包含完整结果集的所有信息且数量减少了很多.闭合模式的概念为定义2所示.定义2(闭合频繁模式).对于频繁项集P,若不存在频繁项集Q满足下列条件:(1)PQ;(2)freq(P)=freq(Q),则称P为闭合频繁模式.示例1.包含8个实例的数据集如表1所示.每个实例长度为5,分别包含4个条件属性和1个分类属性.条件属性A1,A2,A3,A4的取值个数分别为3、3、2、2.类属性为Class,包括两个取值{yes,no}.设定变量Ai表示条件属性,i表示第i个属性.Aij为属性Ai的第j个取值.Ck为类的第k个取值.Vijk表示在Ck条件下,Aij的个数.实例T1T2T3T4T5T6T7T8若设置最小支持数阈值θ为0.3,则项集P1=〈a1,c1,yes〉为闭合频繁模式.这是由于它出现在实例T1,T2,T3,T5中,可以得出其支持数为4,满足freq(P1)>0.3×8=2.4,且不存在与P1支持数相同的父频繁项集.而项集P2=〈a1,yes〉不是闭合频繁模式,这是由于存在父项集P1,且满足条件freq(P1)=freq(P2).由于数据流的连续无限性,其中包含的知识会随着时间推移而发生改变.通常情况下,最近产生事务的价值比历史事务要高的多.因此,需要增加最近事务的权重.时间衰减模型是一种随着时间的推移而逐步衰减历史模式支持数权重的方法[11-12].设模式支持数在单位时间内的衰减比例为衰减因子f(f∈(0,1)),记事务Tn到达时模式P的衰减支持数为freqd(P,Tn).则第m个事务Tm到达时,模式P的衰减支持数满足式(1)和(2).即随着新事务的到达,每次模式的支持数都进行衰减.其中如果新事务Tm中包含模式P,则r的值为1;否则为0.freqd(P,Tm)=2.2决策树分类方法Bifet等人[10]提出数据流分类循环过程包括3个步骤,如图1所示.循环过程包括:(1)传递数据流中下一个可用的数据至算法中,这一步需要实时处理且每个数据被处理一次;(2)算法尽可能快的处理数据,更新数据结构;(3)算法准备好接受下一Page4个数据,且随时可以对未知数据的类值进行预测.如此不断的循环反复对数据流进行处理.决策树是数据流分类中常用的模型之一.常用的属性分裂准则包括信息增益(InformationGain)、增益率(GainRate)、基尼指数(GiniIndex).本文中将重点考虑信息增益,其计算方法如式(3)所示.其中Gain()表示信息增益值,H()表示熵,P()为概率值.Gain(Ai)=H(C)-H(C|Ai),H(C)=-∑kH(C|Ai)=-∑jDomingos和Hulten[4]使用了Hoeffding树(HoeffdingTrees)用于数据流分类,这是一种基于Hoeffding不等式建立分类决策树的方法.Hoffding不等式的定义如下:假定一个实数型的随机变量r的取值范围是R(例如针对信息增益范围是logc,c为类别个数).假定r的n个独立样本点,计算它的样本均值为r-.则r的真实平均值至少是r--ε的概率为1-δ,其中:公式中n的最小取值就是当前节点进行分裂时所需的最小实例数.VFDT的算法描述如算法1所示,其中G()为信息增益.算法1.VFDT.输入:Sdatastream输出:HTdecisiontree1.LetHTbeatreewithasingleleaf(root)2.ComputecountsVijkatroot3.ForeachexampleTnewinSdo4.SortTnewtoleaflofHT5.UpdataVijkatleafl6.ComputeinformationgainGforeachattribute7.IfG(BestAttr.BA1)-G(2ndbestAttr.BA2)>ε2.3概念漂移处理方法可变数据流的特点是观测到的数据潜在分布会随着时间改变.处理此类数据流,分类器应能发现概念改变的信息,并快速调整分类模型以适应概念变化[13].研究中出现了很多方式处理数据流分类中的概念漂移问题,包括使用滑动窗口和实例权重[14]、检测概念改变点[15]、监控两个不同时间窗口内分布[16]等.如Gama等人[17]提出基于错误率的概念检测分类方法,Baena等人[18]提出基于分类错误距离的概念检测方法,Gama等人[13]提出一种两层学习系统来解决周期性概念问题等等.最近常用的监控分布方式是ADWIN方法.它使用Hoeffding边界来保证窗口的最大宽度,且在窗口内没有概念改变.ADWIN是一种概念漂移检测器和评估器,它是一种捕获流平均数的很好方法.它保留一个可变长度窗口大小的最新实例,窗口足够大使得在这个窗口内不存在概念漂移.ADWIN主要工作思路是,当最新窗口W中两个足够大的子窗口W1和W2可以展示足够明显的平均数,并且可以推断出相应的预测值是不同的,则窗口中较旧的部分可以删除.其中足够大和足够明显可以用Hoeffding边界定义,即两个子窗口的平均大于变量εcut,如式(5)所示.其中|W|为最新窗口W的大小,|W1|和|W2|是两个子窗口W1和W2的大小,且满足|W|=|W1|+|W2|.3基于模式的分类决策树方法本节介绍基于模式的分类决策树方法建立过程,该过程分为两步.首先挖掘基于约束的闭合频繁模式,然后基于模式建立分类决策树.首先,本文提出一种基于模式的数据流分类循环过程,包括4个步骤如图2所示.该过程和图1中常规循环过程相比,在进行分类器训练之前,增加了一步生成模式.具体而言循环过程包括:Page5(1)传递数据流中下一个可用的数据Tnew至算法中;(2)对Tnew进行频繁模式挖掘,更新模式相关数据结构;(3)算法尽可能快的处理模式集合中的每个数据,更新数据结构;(4)算法准备好接受下一个数据,且随时可以对未知数据的类值进行预测.如此不断的循环反复对数据流进行处理,这样的目的有两个:一是去除数据中的无用信息或者噪声;二是得到的频繁模式具有的信息多于单个的属性,有利于提高决策树创建的效率.本文设计的算法采用滑动窗口和时间衰减模型对数据流进行增量更新的模式挖掘.时间衰减模型在模式发现过程中,可以提高新事物的权重、降低历史事物权重,目的是与滑动窗口配合解决模式挖掘过程中的概念漂移问题,且分类模型训练过程中使用频度较高的top-k模式,相比原始数据而言包含了更多的信息,会生成更加合理的决策树.因此,采用图2中的数据流分类循环是合理的.3.1基于约束的闭合频繁模式挖掘时间衰减模型是数据流频繁模式挖掘中处理概念漂移问题的有效方法,其关键技术是设置衰减因子的方式.已有的方式通常采用假定100%的查全率(Recall)和100%的查准率(Precision)来估计[11-12],即设定Recall为100%时,f应满足式(6),称为下界值,其中θ为给定最小支持度阈值,ξ为最大允许误差阈值.设定Precision=100%时,f满足式(7),称为上界值.现有的f估计方式是采用满足式(6)和(7)的上下边界值之一.这种方式的最大不足是仅考虑了查全率或查准率,而忽略了对应的查准率或查全率.由于Recall和Precision不可能同时为100%,所以选择f是应考虑对二者的平衡.为此本文提出了均值衰减因子设置方式,即设置f为上下边界的平均值,标记为faverage.ff<例如,假设N=10K,设定θ,ε如表2所示.其中frecall为假定Recall=100%时得到的下界值.fprecision为假定Precision=100%时得到的上界值.在得到frecall和fprecision后,如何选择f的值?可以有3个策略,如式(8)和(9)所示.以θ=0.025,ε=0.05×θ为例,可以选定θ0.050.05×θ0.9999950.9978950.9989450.050.1×θ0.050.5×θ0.0250.05×θ0.9999950.9957890.9978920.0250.1×θ0.0250.5×θf1=frecall,f2=fprecisionf3=f_average=(f_recall+f_precision)/2==f3=通过韩萌等人[19]实验验证,设置均值衰减因子与同类频繁模式挖掘方式MSW[11]、SWP[12]和CloStream[20]相比,可以得到具有高完整性和准确性的频繁模式结果集合.为了下一步的分类使用,文中发现的频繁模式是满足类约束的,如约束1所示.约束1(类约束).发现的模式满足:(1)形式〈X,C〉,必须至少包含一个属性值和一个类值;(2)是闭合的.本文设计算法CCFPM(Constraints-basedandClosedFrequentPatternMiningoverdatastream)增量更新的发现满足类约束的频繁模式,使用滑动窗口模型(SlidingWindowModel,SWM)和TDM处理概念漂移问题,具体如算法4.CCFPM算法主要包括2个方法:一是处理新实例Tnew的方法Page6CCFPMADD,它用于发现新实例带来的模式集合的变化;二是处理历史实例Told的方法CCFPMRE-MOVE,它用于处理移出窗口的历史实例信息.由于CCFPMREMOVE与CCFPMADD结构相似,过程相逆,因此本文重点介绍CCFPMADD的实现过程.CCFPM算法使用了3个数据结构,包括Closed-Table[20]、CidList[20]和NewTransactionTable.其中ClosedTable用于存储闭合模式相关的信息,包括3个字段:Pid,CP和SCP.Pid用于唯一的标识每一个闭合项集CP,SCP是闭合项集CP对应的支持数.PidList用于维护数据流中出现的每个项item和其对应的Pid集合.NewTransactionTable包含与新实例Tnew相关的信息,包括2个字段:TempItem和Pid.其中TempItem存储满足条件{Tnew∩CP,CP∈ClosdeTable}的项集信息.CCFPMADD算法处理新实例的工作过程主要包括3步:(1)参照ClosedTable查找与Tnew相关的频繁项集interS;(2)若itemset∈interS且为新的频繁项集,则加入ClosedTable,同步更新PidList;(3)若itemset∈ClosedTable,则更新已有模式;①如果依然是闭合模式,则更新其支持数;②新数据的到来使之成为非闭合模式,则删除,同步更新PidList.具体的过程是:实例Tnew到达时将Tnew存入NewTransactionTable,接着比较PidList和Tnew中的每个项item,更新NewTransactionTable.然后参照NewTransactionTable,在ClosedTable中添加新的模式或者更新已有的模式.同时更新PidList.如此反复,随着数据的到达不断的更新.图3介绍了频繁模式发现的增量更新过程,输入的是数据流DS,输出的是模式流PS.示例2.以表1中的数据为例,设置模式满足的最小支持数为2,得到满足类约束的频繁模式集合如表3所示,共得到5条模式.这些模式可用作训练实例.如P2〈a1,c1,d1,yes〉增加缺损值,补充为〈a1,?,c1,d1,yes〉即可.表3最小支持数为2时得到的闭合模式集合模式编号P1P2P3P4P53.2基于模式的决策树方法研究由于数据流中可能存在着大量无用信息或噪声,对其进行模式挖掘得到的模式结果集合的优势在于:(1)可以去除其中的噪声;(2)得到信息量更大的模式数据.采用这些模式数据进行决策树训练,理论上可以提高创建分类器的效率以及分类的正确率.示例3.以信息增益为例研究基于模式的决策树建立过程.使用模式建立决策树需要考虑频度/权重的使用.为此,将表1和表3中的数据进行格式修改,得到表4和表5.处理过程是:(1)为原始数据中的每条实例加上权重1,数据集表示为DS;(2)为模式补充缺损值增加至原始实例的长度,且采用频度值表示权重,数据集表示为PS.实例T1T2T3T4T5T6T7T8实例T1T2T3T4T5为了更好的比较在DS和PS上分裂属性的选择方法,引入以下变量:WVk:表示类属性取值为Ck的权重之和;WVij:表示属性Ai为第j个值的权重之和;WVijk:表示类属性取值为Ck的条件下,属性Ai为第j个值的权重之和;SumWV:表示数据集合中实例的权重之和.设Page7置的目的是由于存在缺损值.在DS和PS上得到的Gain(A1)的计算过程如表6所示.从表6中可以看出在PS上的计算量少于在DS上的计算量.分别采用信息增益度量准则在DS和PS上生成决策树模型,得到的树结构如图4所示.从得到的树结构可以看出,在不进行树结构限制的条件下,在DS上得到的树结构大于在PS表6在犇犛和犘犛上得到犌犪犻狀(犃1)的过程WV11=6,WV12=1,WV13=1P(yes)=WV1/SumWV=4/8=0.5P(no)=WV2/SumWV=4/8=0.5H(C)=-0.5×log0.5-0.5×log0.5H(C|A1)=-0.75×(0.67×log0.67+SumWVWVkP(Ck)=P(Aij)=P(Ck|Aij)=H(C)=-∑H(C|Ai)=-∑Gain(Ai)=H(C)-H(C|Ai)采用PS生成决策树可能存在的问题在于:(1)当生成的模式频度低,模式长度短时,生成的树结构可能会过于庞大,且分类正确率不一定提高;(2)当设上得到的结构.相比较而言,DS包含8条实例,需要进行20个概率值WVijk统计,得到的树包含8个节点,其中5个叶子节点;PS包含5条实例,需要进行12个WVijk统计,得到的树包含3个节点,其中2个叶子节点.整体而言,在PS进行分类模型训练可以有效减少训练时间,生成的树结构也会更加紧凑.DS定的k值过低时,取top-k后生成的树结构包含的信息或许会不足,会导致一定的正确率出现降低.因此,对密度高的数据流进行分类时,由于需要得到的模式频度高、长度合适,可以使用PS做训练数据;而对密度低的数据流进行分类时,由于需要得到的模式频度稍低或模式较短,可以考虑取频度最高的k个频繁模式和原始数据一起进行训练生成模型.这么做的优势在于可以提高分裂属性的选择效率,且提高正确率.3.3算法设计本文研究基于频繁模式的分类决策树算法.由于需要模式参与训练但模式数据中存在缺损值,因此在使用数据时会考虑权重的统计策略.为此,算法中设计使用5条规则:规则1.模式挖掘时仅考虑具有类属性的闭合频繁模式.规则2.Top-k个模式选取时,k的取值尽可能使PS中包含类属性的全部或绝大部分取值.规则3.缺损值参与SumWV的统计.Page8规则4.当得到的最好的两个属性分裂准则值差异很小时(<ε),则选择权重之和大的属性(缺损值不参与统计)作为分裂节点.规则5.若使用权重比较依然无法区别最优和次优的分裂属性,则任意选择其中一个作为分裂节点.针对不同特征的数据流,本文设计两种决策树方法.针对高密度数据流,设计基于模式的分类决策树算法PatHT1(Pattern-basedHoeffdingTree)使用PS作为训练实例,如算法2所示;针对低密度的数据流,算法PatHT2采用少量top-kPS与DS的组合集合作为训练实例,如算法3所示.算法PatHT1和PatHT2的输入均包括数据流S,滑动窗口大小SW,最小支持数阈值θ,选择正确分裂节点所需的概率δ;输出为分类决策树模型HT.算法CCFPM用于增量更新的生成模式,其中的函数support()表示支持数.生成的模式是具有类约束的,即必须包含条件属性与类属性,模式长度大于等于2.使用的3个数据结构ClosedTable,CidList和NewTransactionTable在3.1节中已经介绍.算法HTreeGrow(HoeffdingTreeGrowing)使用训练数据增量更新的生成决策树,它也是一种基于Hoeffding边界的衍生算法.不同之处在于增加了实例的权重,为此(1)统计信息时需要考虑权重值;(2)选择最佳分裂节点时,会考虑频度相关的统计信息,即当相同分裂准则值时,采用权重之和作为二次选择的标准.算法中使用ADWIN作为概念漂移估计器,函数G()表示使用分裂准则得到的值;函数MaxWeightAttr()用于找到权重值高的属性.算法2.PatHT1.//基于模式的Hoeffding树1输入:S:数据流输出:HT决策树1.ForeachtransactionTnewinSDoGetnovelsetofpatternsPSnew=CCFPM(Tnew,f,2.LetHTbeatreewithasingleleaf(root)3.InitialcountsWVijkatroot4.Foreachexample(x,y,weight)inPSnewDoHTreeGrow((x,y,weight),HT,δ)算法3.PatHT2.//基于模式的Hoeffding树2输入:S:数据流输出:HT决策树1.ForeachtransactionTnewinSDoGetnovelsetofpatternsPSnew=CCFPM(Tnew,f,2.LetHTbeatreewithasingleleaf(root)3.InitialcountsWVijkatroot4.Foreachexample(x,y,weight)inPSnewandTnewDoHTreeGrow((x,y,weight),HT,δ)算法4.CCFPM.//发现可变数据流中的约束闭合频繁模式输入:Tnew:数据流中的最新实例输出:PS频繁模式集合1.AddTnewtoNewTransactionTable2.Letinters=Tnew∩ClosedTable()accordingto3.AddinterstoNewTransactionTable4.ForeachTempIteminNewTransactionTableDoIfinterS∈ClosedTable5.Ifitem∈TnewAnditemisnotinPidListThenadditemToPidList算法5.HTreeGrow.//创建Hoeffding树输入:(x,y,weight):实例输出:HT决策树1.Sort(x,y,weight)toleaflusingHT2.UpdatecountsWVijkwithweightatleafl3.ComputeinformationgainGforeachattributefrom4.Splitleaf4.1IfG(BestAttr.BA1)-G(2ndbestAttr.BA2)>ε4.2ElseletbestattributePage94.3SplitleaflonBA5.ForeachbranchDo5.1StartnewleaflandinitializeestimatorsADWIN5.2IfADWINhasdetectedchange5.3Ifnosunbtree4实验分析4.1评估方法实验中将比较PatHT算法与贝叶斯分类算法NaiveBayes(NB),规则分类算法DTNB[21],Rule-Classifer(RC)[22],决策树分类算法VFDT[4],HAT[8],HOT[7],AdoHOT[10],ASHT[10].其中DTNB采用Leave-one-out方式评估.其余分类方式采用MOA中的EvaluatePrequential[23-24]评估方式来测试.其中每个实例先作为测试数据而后作为训练数据.这样得到的正确率是增量更新的,且不需要专门留出测试数据,就保证最大化利用每个数据的信息.4.2实验数据实验中采用了3个模拟数据流SEA、RBF、LED和1个真实数据流Poker-hand,具体信息如表7所示.Pocker-hand10101×106SEARBFLED4.2.1SEASEA是数据流挖掘常用的模拟数据,它来源于MOA[23],包含了3个条件属性和1个类属性.3个属性中只有前两个是有关联的,且3个属性的值都在0与10之间.这个数据分为了4块,每块有不同的概念.分类是通过f1+f2η完成的,其中f1和f2表示前两个属性,η是阈值.数据块中最多的值是9、8、7和9.5.SEA是具有概念漂移特性的数据流.为了发现频繁模式,使用WEKA[25]中的Discretize方法对其中的条件属性进行离散化.4.2.2RBF具有不同概念漂移特征的RBF数据由数据流生成器generators.RandomRBFGeneratorDrift[23-24]生成,分别生成包含1000000实例的两种特征数据,即无概念漂移的稳态数据流(nodrift)和漂移度为0.001的可变数据流(drift0.001).RBF的两个类值分布几乎一样,很平均.但是属性值的分布满足高斯函数,它集中分布在中间值附近,如图5所示.为了发现频繁模式,使用Discretize方法对其中的条件属性进行离散化.4.2.3LEDLED由MOA中数据流生成器generators.LEDGenerator[23-24]产生.LED数据用于在一个7段LED显示器上预测显示的数字.其包含24个二进制条件属性,每个属性有10%的可能性会反转;包含1个类属性,有10个不同值.实验生成模拟数据流LED包含1000000条实例.实验生成的数据包含24个二进制属性,其中17个是无关的.为了分析PatHT算法对概念漂移数据流的处理能力,生成的LED具有两种特征:无概念漂移和有概念漂移.设置概念漂移宽度W(widthofconceptdriftPage10change)使用的是ConceptDriftStream方法[23-24].概念改变的宽度设置为小于和大于滑动窗口宽度.4.2.4Poker-hand真实数据Poker-hand来自UCI①,包含1000000条实例,11个条件属性和1个类属性.Poker-hand数据中的每个实例是一手牌,分类属性用于描述“Poker-hand”,包含10个取值,如表7中所示.真实数据流的概念漂移特性不易发现.在Poker-hand的10个类取值中,第1个类值出现在约50%的实例中,第2个类值出现在约40%的实例中,其余的8个类值出现在不足10%的实例中.因此,它是一种不平衡数据.4.3实验表现实验运行环境的CPU为2.1GHz,内存为2GB,操作系统是Win7,所有的实验采用Java实现.文中将对10种算法训练模型的时间、内存消耗以及分类正确率进行比较.4.3.1频繁模式挖掘分析首先分析数据流中发现模式的相关信息.文献[19]中实验验证了用数据流频繁模式挖掘CCFPM中当设定滑动窗口大小SW=1000时,设定剪枝步长为1000是比较合理的.这些参数的设置可以使算法CCFPM得到性能表现优于同类数据流频繁模式的挖掘方法MSW[11]、SWP[12]和CloStream[20].为了说明模式变化的趋势,分别对5个窗口大小内的实例进行处理,先把它们标记为5个数据块:{B1,B2,B3,B4,B5}.然后对每个数据块进行挖掘生成约束闭合频繁模式集合.为了分析窗口大小对得到模式的影响,需要对多个数据流进行模式挖掘,结果如图6所示.在设置相同的支持度的条件下,可以得出结论:(1)数据流Poker-hand得到的模式数量比较多,在5个数据块上得到的模式平均长度和最大频度相似,如图6所示.得到模式长度为2~4,平均长度为2.83.由于数据集合中实例长度为11,因此,模式-实例长度比约为13.9(2.8311).每个数据块中发现的模式个数平均约为300,模式-实例个数比约为13.3(3001000).(2)可变数据流SEA得到的模式数量相比是最少的,如图6所示.数据块中发现的模式长度为2~3,平均长度为2.3.模式个数平均约为57,模式-实例个数比约为117.5.即每个数据块中生成的模式数量与数据块中实例数量相比是很少的.(3)对RBF进行模式挖掘得到的模式数量、最大权重在不同数据块中变化是最明显的.这表明了数据的动态特性.同时,这种方法得到的模式数量是最多的、权重是最大的,明显多于其他几个数据流,如图6所示.RBF数据块中得到的模式平均长度为4.27,因此模式-实例长度比约为12.34.模式的平均个数为635.4,模式-实例个数比约为11.57.图7(a)显示了不同数据块中模式的分布,可以看出得到的模式最多的是3、4和5这3个长度.(4)当概念改变宽度W设置为500时,LED的模式数量在不同数据块中表现的差异较大,如图6(a)所示.模式的数量、长度和权重等信息相比Poker-hand和SEA变化的比较明显.原因除了数据本身的特征外,还在于设置的概念改变宽度是小于滑动窗口大小的,滑动窗口不能很好的解决概念①FrankA,AsuncionA.UCIMachineLearningRepositoryPage11图7数据流LED和RBF中得到的不同长度模式分布变化问题.LED数据块中得到的模式平均长度为7.46,因此模式-实例长度比约为13.5.模式的平均个数为131,模式-实例个数比约为17.6.(5)当W<SW时,数据流LED中得到的模式长度分布情况如图7(b)所示,可以看出得到的模式最多的是7、8和9这3个长度.其中B5和B1中得到的模式分布完全不同.B5和B3中得到的模式数量多于其他3个数据块,B4和B2得到的模式数量居中,B1中的数量最少.总之,模式分布在不同的数据块中变化很大.表9算法在SEA与RBF上的性能NBRCVFDTHATHOT5HOT50AdoHOT5AdoHOT50ASHTPatHT最后比较模拟数据流LED.使用3类具有不同概念改变宽度的数据集合,特征是:无概念漂移、概念漂移宽度W=500(小于窗口宽度)和W=2000(大于窗口宽度).算法在LED上进行分类得到的正(6)表8是4种不同特征数据流上得到的模式-实例信息比.相比而言,RBF得到的模式数量最多,SEA中模式数量最少.对比与原始的实例长度,LED、Poker-hand生成的模式长度是相对较短,SEA与RBF中得到的模式长度是相对较长.表8数据流上得到的模式与实例的长度比和数量比Poker-handSEARBFLED4.3.2模拟数据流分类性能比较具有概念漂移特征的数据流SEA,其概念改变宽度不明确.多个算法对其进行分类处理得到的正确率如表9所示.由于SEA具有概念漂移特性且得到的模式频度较低,因此仅采用模式集合作为训练集合得到的数据分类效果很差.为此,仅对PatHT2(简记为PatHT)算法进行比较.从表中可以看出,PatHT算法相比较其他方法得到的正确率有一定的提高,但提高程度不高的原因在于得到的是短模式且数量较少.接着比较模拟数据流RBF,对稳态(nodrift)和动态RBF(drift0.001)进行分类处理,得到的结果如表9所示.从表中分析,本文提出的PatHT算法的正确率在不同特征的RBF上均是最优的.相比较而言,与已有方式相比,PatHT算法在稳态RBF上提高的正确率较明显.时间和内存的消耗与已有方式(除RC算法之外)相比,PatHT算法的时间消耗会明显增加,内存消耗增加不明显.NoDrift正确率72.9083.1991.6092.4091.9092.0092.3092.1091.6097.70确率如表10所示,可以看出PatHT得到的正确率相比较其他算法有一定的增加.但在W=500时表现稍差,这是因为概念改变的宽度小于滑动窗口的宽度,得到的模式信息分布变化很大.Page12表10比较LED的正确率NBRCVFDTHATHOT5HOT50AdoHOT5AdoHOT50ASHTPatHT4.3.3真实数据流分类性能比较比较多种算法在真实数据流Poker-hand上的分类正确率.本组实验采用两类PatHT算法.PatHT1仅采用模式集做训练实例,采用的模式数量大约是实际数量的30%.算法PatHT2采用模式集与原始数据集的组合集合做训练实例,采用的模式数量大约是实际数量的20%.由于DTNB采用Leave-one-out方式评估分类算法,运行速度相比较而言非常慢,因此仅对50000条实例进行分析.PatHT1得到的分类正确率明显优于NB、DTNB和RC,与其他决策树分类算法得到的正确率几乎一致,如表11所示.RC算法的时间消耗最多,PatHT1消耗的时间最少,大约比其余算法的平均时间减少了80%.这是由于参与训练决策树的模式数量很少,但是额外的时间消耗会出现在生成频繁模式的过程中,使PatHT1消耗时间的优势会减低一些.NBDTNBRCVFDTHATHOT5HOT50AdoHOT5AdoHOT502.9862.301.3435.8679.713.40ASHTPatHT1PatHT2PatHT2采用的是模式与原始数据同时做训练数据的方式.从表11中可以看出,它消耗的时间和内存与其他算法相比没有明显的增加,但是正确率LEDW=500正确率73.4052.6072.9073.9072.8072.8072.9272.9272.9072.20提高了约20%.消耗无明显增加是由于取top-k个模式,使得模式-实例个数比大约为15,没有明显增加训练消耗.但是由于需要生成模式,因此时间和内存消耗会有一定的增加.图8(a)是PatHT算法与两种规则分类方法DTNB和RC的比较.从图中可以分析出PatHT1算法在保证正确率不降低的同时明显减少了时间消耗,而PatHT2明显提高了正确率,且使用的时间和内存消耗相比规则分类而言很少.图8(b)是PatHT算法与NB和两种决策树分类方法VFDT和ASHT的比较.从图中可以看出PatHT算法可以明显增加分类正确率,相比而言时间消耗会明显增加.Page13综合上述实验可以得出结论:(1)针对生成模式频度较高、模式长度较长的稳态数据流,PatHT对其进行处理可以明显提高分类的正确率.(2)针对具有模式频度较高、模式长度较长特征的稳态数据流,如真实数据流Poker-hand,PatHT算法对其进行处理时,PatHT1可以得到与常见算法相似的正确率,且可以节省大量的时间消耗.(3)针对不同概念改变特征的动态数据流,PatHT算法对其进行处理时正确率也会增加.但是概念改变宽度较小时(小于窗口宽度),得到的算法正确率可能会降低.(4)由于需要生成模式,因此PatHT算法会额外的增加时间和内存消耗,但是由于使用的模式数量少,时间消耗增加不明显.由于使用的是高频度的模式,使得生成的分类模型更加的紧凑,因此消耗的内存相比经典算法会有一定程度的降低.5总结常用的数据流分类流程为输入数据、训练数据、生成分类模型.这些训练实例集合中可能存有大量无用信息或噪音.因此本文提出一种新的分类流程:输入-模式-训练-模型,即在数据用于训练之前先进行模式挖掘.对每个实例进行增量更新的模式挖掘,发现具有类约束的闭合频繁模式,这样可以去除原始数据中的无用信息,且得到信息量更大的模式数据.针对不同的数据流特征,这些模式集合会以独立或组合的方式参与分类决策树模型的训练.从大量的实验结果分析可以得出,在真实数据流和不同概念改变特征的模拟数据流上,使用基于模式的分类决策树可以有效的提高分类的正确率或明显降低训练时间.但是由于需要发现频繁模式,因此不足之处是不能对连续值的属性直接进行处理,需要离散化,且由于需要增加频繁模式生成过程,因此时间或内存消耗上会有所增加.
