Page1基于节点预测的直接Cache一致性协议张骏1)田泽1)梅魁志2)赵季中2)1)(中国航空工业集团公司西安航空计算技术研究所西安710068)2)(西安交通大学电子与信息工程学院西安710049)摘要处理器性能的提升依赖于对存储系统性能的挖掘.随着片上集成内核数量的不断增大和特征尺寸的持续缩小,延迟、存储可扩展的Cache一致性协议已经成为提升访存效率的关键性因素.文中提出一种基于节点预测的直接Cache一致性协议-NPP协议,研究一致性交互延迟隐藏和目录存储开销减少技术.针对读、写缺失中存在的间接性问题和现有解决方案破坏已有数据局部性、无法获得最近数据副本等问题,分别提出节点挂起技术和直接写缺失处理技术,有效隐藏了目录访问延迟.为了实现准确的节点预测,作者还提出基于“签名”回收的历史信息更新算法,避免了冗余更新和不完整更新.使用SPLASH-2测试程序集,在基于2DMESHNoC互联的64核CMP下,相对于全映射目录协议,NPP协议的平均执行时间降幅为21.78%~31.11%;平均读缺失延迟降低14.22%~18.9%;平均写缺失延迟降低17.89%~21.13%.而获得上述性能提升的代价是网络流量平均增加6.62%~7.28%.关键词单芯片多处理器(CMP);预测;一致性协议;目录;可扩展1引言良好的可扩展性和对芯片面积的高效利用促使CMP处理器(ChipMulti-Processor)被广泛使用.然而,芯片上连线的电气特性不可能与半导体工艺等比例缩放,线延迟问题已经成为未来CMP设计的重要限制因素[1-2].研究者提出延迟非一致的Cache访问(Non-UniformCacheAccess,NUCA)[3-5]结构来减轻线延迟对访问片上末级Cache延迟的负面影响.NUCA结构旨在减少片上末级Cache的平均数据访问延迟;而片上每内核私有的L1Cache间在进行一致性交互时,消息和数据传输效率以及Cache一致性协议[6-7]的可扩展性同样受到长线延迟影响,本文着重研究片上多核结构下的低延迟、低存储开销的可扩展Cache一致性协议.Cache一致性协议是CMP处理器正确运行的保证,考虑到扩展性问题,相对于总线监听协议来说,基于目录的Cache一致性协议是大规模CMP更合适的选择[8-9].目录能够跟踪全局的一致性状态和L2Cache中数据所有共享者的标识.缺失内核将通过网络把消息发送到相应目录内核中进行处理.然而,不断提升的线延迟和多个内核间复杂的一致性交互直接导致了访存延迟的提升.如何为深亚微米工艺下的多核处理器设计高效、可扩展的Cache一致性协议已经成为体系结构领域研究的重要内容.为了降低一致性交互延迟,一种直接的方法是减少一致性交互步骤[10-11].通常基于历史信息预测来直接获取或者作废数据,需要额外片上空间来存储历史信息,且预测准确性不高,在预测错误时所增加的额外延迟可能比原先的一致性交互延迟还大.另一种有效方案是使一致性交互局部化[12-17].这类方法通过构建多层目录、内核分区域管理和目录压缩等策略来减少一致性消息延伸的距离,使一致性交互尽可能在小范围内完成,从而减少延迟.但存在目录信息重复存储、存储空间利用率低、多层目录间交互延迟大等问题.有时目标数据就在请求内核附近,但仍需访问远程高层目录获得共享信息.本文提出一种基于节点预测的直接Cache一致性协议NPP(NodePredictingProtocol),具有以下创新:(1)将所有内核划分为并列关系的多个节点,每个节点包含2n个内核.采用全局和节点两层目录,全局目录以粗向量方式跟踪所有节点,节点目录以位向量方式跟踪节点内的内核.与直接预测有效数据所在内核的方案[10-11]不同,NPP通过增加节点预测CacheNPC(NodePredictingCache)在发生读、写缺失时预测与读缺失内核最近的有效数据节点和需要作废的数据副本节点.由于不是以内核,而是以节点为单位进行预测,同样内核数量情况下预测有效数据位置的历史信息所占用的存储空间显著减少;(2)为了能够保证预测最近有效数据节点的准确性和有效性,我们分别提出了基于签名回收机制的读缺失和写操作NPC更新机制,进一步减少由于预测错误或不准确造成的额外延迟;(3)NPP协议在具备使一致性交互局部化能力的同时,还能够避免有效数据存在于请求内核附近但不在同一目录层次情况下的远程高层目录访问;(4)与省略目录访问步骤的方案[10-11]不同,我们提出节点挂起技术将读缺失处理中更新目录步骤推迟到请求内核得到数据以后进行,将读缺失一致性交互的关键路径长度缩短为2-hops,而且用较小的改动保证了协议的简单性;(5)提出直接写缺失处理技术来隐藏写缺失目录访问延迟,当写缺失数据处于M、E状态,且预测信息正确时,能将写缺失一致性交互过程缩短为2-hops.本文第2、3节分别说明相关研究和动机;第4节介绍基于节点预测的直接Cache一致性协议(NPP);第5节介绍节点预测Cache(NPC)的更新方法;第6节对NPP协议的可扩展性进行分析;第7节介绍实验平台和模拟结果,描述仿真过程并分析仿真结果;第8节是本文结论.2相关研究随着芯片上集成的内核数量不断增加,NUCAPage3缓解片上末级Cache访问延迟的作用愈发明显,一致性协议的可扩展性也显得越发重要,研究人员已经从多个角度进行了研究.在NUCA方面,文献[3]提出非一致性Cache架构(NUCA)来解决片上连线延迟问题,NUCA将片上末级Cache划分为多个较小的CacheBanks,允许处理器内核访问临近的CacheBank时相对于较远的那些Banks获得较小的延迟,从而达到减轻末级Cache内部长连线延迟的影响.对于NUCA结构的CMP来说,一个重要的问题是如何确定数据块在NUCA空间中合理的放置位置.文献[18]提出了动态NUCA(D-NUCA),允许数据被映射到NUCA空间中的多个Banks上,然后根据程序行为将数据动态的迁移到距离访问内核较近的Banks中,从而降低延迟.为了避免在多个NUCABanks间迁移数据时的“乒乓”问题,文献[9,19]先将目标数据的使用情况以及处理器对它的共享情况等参数进行综合考虑,然后通过计算得到目标数据的优化放置位置.除D-NUCA外,文献[8,20]提出了S-NUCA,S-NUCA通过操作系统或者选择性数据复制等手段在避免数据迁移的同时解决数据放置问题,从而有效地减少了“乒乓”问题,但代价是显著增加了设计复杂度.文献[21]提出了协作Cache方案,结合了私有和共享Cache的优势,在程序运行过程中将本地活跃数据聚集到私有Cache,而全局活跃数据在被识别后存储在集合Cache中,不但能减少片上数据访问延迟,还能减少片外访存的数量.文献[22]提出了NUcache的末级Cache组织结构,根据Cache中数据的使用情况及其所对应的PC在逻辑上按路组方式将Cache分为两个部分,访问时采用不同的方式和策略,研究证明该方案能够将性能提升9.6%~33%.在多处理机结构下,为使一致性交互局部化和减少目录存储开销,文献[23]首次提出了层次化的多处理机结构,通过将连接多个Cache的共享总线构建为树形来使一致性交互局部化.SCI[24]、STP[25]、LimitLESS[26]、SCI树扩展[27-28]和基于锁一致性协议[29]等虽然具有良好的可扩展性,但代价却是牺牲读缺失或写缺失时一致性交互的性能.文献[30]提出了基于片上网络拓扑结构的Cache一致性协议,这种协议不从目录内核返回数据,而是通过目录内核将请求转发至距离请求内核最近的目标数据副本内核,使之响应请求并发送数据.这种机制最小化了数据在连线上的传输距离,能使网络链路功耗减少9.3%;并且通过减少链路拥塞加快通信速度,使整体性能提升1.4%.该协议虽然能提供距离最近的数据副本,有效降低功耗,但仍需要先访问目录,间接事件仍然存在,一致性交互延迟难以降低.在片上多核结构下,文献[13,16]提出了一种两级目录方案,第1级目录采用小的全映射方案,第2级目录采用压缩方式来减少目录的存储开销.文献[17]提出一种3层目录组织结构,包括片上的目录Cache和主存中的压缩目录结构.文献[14]提出面向NoC的层次化簇Cache一致性协议,构建了双层树结构目录来实现一致性交互局部化.文献[31]提出了虚拟树协议(VTC).VTC使用粗粒度协议跟踪技术,一片存储器区域的共享者通过一个虚拟树进行连接.由于虚拟树的共享根用作访问的排序点,并且根节点也是这片存储区域的一个共享者,对于一些缺失来说,就可以避免发生间接事件,减少一致性交互延迟,但需要对路由器及其算法进行重新设计.文献[12,15]提出层次化Cache目录协议.通过对内核做逻辑上的区域划分,构建了相互间有包含关系的多层目录结构,允许从底层目录到高层目录逐步扩大一致性交互的延伸范围,尽量使一致性交互局部化,从而达到降低延迟的目的.本文提出的NPP协议目录层次少,对目录空间使用效率高,没有多层目录间的复杂共享信息交互.为了减少一致性交互延迟,文献[10]提出Writer一致性协议,该协议的创新在于使用基于预测的透明读操作,能够使一部分读操作在获得数据后不在目录中保留“踪迹”,而是在同步操作时自作废,后续写操作不用作废这些透明读数据.这降低了一致性状态转换频率(E或M→S),简化了写操作一致性交互过程.文献[11]提出DiCo-CMP一致性协议,通过将目录信息集成在数据拥有者内核中,DiCo-CMP能直接将请求发送到数据的拥有者,而不是先发送到目录内核,再转发到数据拥有者,避免了间接数据访问.相对于普通目录协议,DiCo-CMP能够使访存延迟平均减少6%左右.本文提出的NPP协议不但能直接提供数据,而且能提供距离请求内核最近的有效数据.另外,NPP以节点为单位进行预测,同样内核数量情况下,NPP协议的每个预测指针宽度更小.Page4与以上研究相比,本文提出的NPP协议不依赖软件支持,改动小,一致性交互延迟和目录存储开销可扩展性好.另外,NPP协议不满足于直接获取正确数据副本,而是精确的直接获取最近的正确副本,使得误预测导致的额外延迟大大减少.3研究动机一致性交互延迟和目录存储开销不随内核数量增加而快速增长是可扩展Cache一致性协议的特征,也是本文研究的出发点.3.1减少一致性交互延迟(1)读缺失延迟通过研究已有的基于目录的Cache一致性协图1读缺失一致性交互过程多核处理器出现片上存储器数据不一致的根源是多个内核可能对同一地址的数据进行写操作.假设没有对同一地址数据的写操作,则在同步机制下无论哪个内核读,或者什么时刻读都不会产生不一致现象.写操作导致来自不同内核的共享读操作都必须在目录中留下“踪迹”,以便当写操作到来时对这些副本进行作废.换句话说,只要能够正确的对副本进行作废,以何种方式留下“踪迹”可以有不同的实现方案.通过研究发现,“踪迹”的保留方式可以成为隐藏目录访问延迟的有效手段.如文献[10]提出的透明读的方案.读操作不写目录标志位,而是在同步事件后将数据自行作废,这样就不用发送作废消息,但需要新的软件模型,并会破坏已经建立的数据局部性.文献[11]提出将目录和数据拥有者内核集成在一起,访问目录和访问数据同时进行,省去了专门访问目录的步骤,但集成目录的数据拥有者内核位置固定,虽然能够通过预测直接获得数据,但是无法获得距离议,我们发现发生读缺失内核需要访问目录内核来间接得到一致性信息,如图1(a)、(b)中的第1个步骤,这将增加Cache读缺失延迟.更重要的是,在较为常用的瓦片式CMP结构[32]下,这种间接性将导致更多的Cache缺失.原因是目录信息通常是通过物理地址映射关系分布在这些瓦片式的内核中[33],而不考虑内核可能访问任何一个数据块,因此需要访问远程Cache的可能性大大增加.间接性的读操作将在以下两方面对性能产生负面影响.首先,相对于写操作来说,读操作的效率能够在更大程度上影响系统的整体性能.其次,在多个请求同时访问目录的情况下,目录访问会发生拥塞,多个读操作不得不被排队依次响应,从而增加了读缺失的平均访存延迟.请求内核最近的副本,读缺失延迟仍有降低空间.可见,如何隐藏目录访问延迟已经成为阻碍Cache一致性协议性能提升的瓶颈.如果读缺失时能够省略目录访问步骤,或者将访问目录步骤推迟到请求内核获得数据之后,那么就能直接减少读缺失延迟的关键路径长度.本文提出使用节点预测技术和节点挂起技术将更新目录步骤推迟到读缺失内核得到数据以后进行,详见4.5节.(2)写缺失延迟与读缺失存在的间接性类似,普通基于目录的Cache一致性协议在处理写缺失时也存在间接性.当一个内核发生了写缺失,首先要访问目录内核来获得数据副本的位置和数量,然后由目录通知相关内核作废数据副本(M状态需写回),最后进行写操作,这一过程通常需要3-hops或4-hops,如图2.显然,在大规模CMP中,去除这种间接性将显著降低写缺失延迟.Page5图2写缺失一致性交互过程3.2减少存储开销一致性协议的相关存储开销直接关系到该协议的性能和可实现性.从已有的研究来看,要实现直接数据访问、避免间接事件,基于历史信息预测的直接数据访问是较为有效的方法.但存储历史信息需要片上存储空间,如何使用相对小的历史信息存储空间实现精确预测,进而减少误预测带来的额外延迟是本文研究的出发点之一.文献[11]为了实现直接数据访问,为每个内核增加了L1C$和L2C$用于预测目标数据的内核位置,每个条目由Tag和一个预测指针构成.对一个N内核的CMP来说,预测指针为logN2位.假设一个节点包含2n个内核,如果以节点为单位进行粗粒度预测,首先预测目标数据所在的范围(节点),然后在节点内部快速定位数据所在的内核位置,将有以下优势:(1)减少存储开销.由于节点数为N即每个预测指针节省n位;(2)减少网络流量.发生在节点内部的局部化一致性交互不会影响到其它节点预测信息的正确性,因此由预测信息更新导致的网络消息数量也将减少;(3)由于节点中内核数量相对较少,访问节点内目录的延迟较小,不会对性能造成影响.本文提出的两级目录结构是实现以节点为单位进行粗粒度预测的基础.4基于节点预测的直接Cache一致性协议4.1基准CMP结构片上网络(NetworkonChip,NoC)[32]已经成为CMP处理器广泛使用的内核互联方式,NoC以阵列的方式将多个同一尺寸内核连接起来.本文基于2DMESHNoC互连和MESI状态转换协议进行研究,所有内核私有L1Cache,共享L2Cache和主存.共享L2Cache被分为多个片段以S-NUCA的方式分布在所有内核中,整个L2Cache在物理上分布放置.相对私有L2Cache来说,共享Cache能够使每个内核使用更大容量的L2Cache,并且能够避免在私有Cache中复制同一个Cache行的多个副本.片上两级Cache间采用Inclusive模式.因此,目录用来维护一个共享的L2Cache与多个私有L1Cache之间的数据一致性.目录跟踪每个L1Cache数据块的状态,同时,目录和关联的一致性控制器也与它们控制的L2Cache片段在片上分布放置,如图3.4.2节点的划分在NPP协议中,片上所有内核被划分为多个节点,每个节点包含2n个内核,假设片上有N个内核(N为2n的整数倍),则该处理器包含N个内核构成2DMESH阵列.一个节点内的2n个内核使用n位二进制数进行节点内编码索引,如n=2时,节点内的4个内核分别编码为00、01、10、11.多个节点间是同一层次并列关系.后文均以n=2,即每节点包含4个内核来进行说明,如图4.Page64.3目录的组织结构全局目录和节点目录.图5说明了NPP协议的两级目录结构,分别是(1)全局目录全局目录以粗向量[34]的方式跟踪数据所在节点.由于数据地址以静态的方式映射到所有L2Cache片段,属于S-NUCA结构,因此全局目录只负责跟踪其对应L2Cache片段中数据副本.在内核数量一定的情况下,全局目录大小与节点规模成反比.(2)节点目录由于节点规模相对较小,节点目录采用全映射位向量目录,负责在节点内跟踪数据副本所在的内核位置.相对于文献[14],考虑到目录访问拥塞问图5NPP协议的目录组织结构题,NPP协议没有将节点目录集成在一个固定的内核中,而是提出共享信息分布存储.将节点目录分成多个片段,每个内核存储一个目录片段,简称为NDS(NodeDirectorySlice),按照数据地址末段与节点中内核编码相对应的方式将节点内的数据共享信息分布到所有节点中内核的目录片段上.假设节点规模为2n,由于一个内核的目录要存储来自2n个内核的共享信息,如果2n个内核L1Cache中所有数据的末n位地址均相同,则对应编码内核的NDS就会发生溢出.避免溢出的一个方法是将内核中节点目录的大小变为原先的2n倍,但会造成目录空间的巨大浪费.NPP协议提出节点目录两步访问方案,访问节点目录时第1步首先根据数据地址的末n位访问对应编码内核的NDS,如果数据确实存在于该节点中,那么绝大多数访问将在该目录中命中.如果在该NDS没有命中,则第2步同时访问剩余的2n-1个NDS,如果仍然不命中,说明该数据在该节点中不存在.举例来说,假设节点包含4个内核,即n=2.使用2位二进制数对节点中内核进行编码,分别是00、01、10、11,共享数据地址末两位为11的数据首先将其共享信息存储在编码为11内核的NDS中.如果该内核的NDS已满,再写入其它3个内核的NDS中,其它数据以此类推.访问节点目录时首先根据数据地址的末两位访问对应编码内核的NDS,如果不命中再访问其它3个内核的NDS.Page7节点目录是对全局目录的细化,能够将部分节点内读缺失与其它节点隔离开,同时避免了普通粗向量方案在写操作时可能引起的不必要作废消息.节点目录的信息分布存储、两步访问方案在不增加目录大小的前提下实现了节点内共享信息的分类分布存储.第1步访问较小的NDS能够以较小的访问延迟使绝大多数目录访问命中,第2步并行访问其它内核NDS的延迟与第1步相同,且需要进行第2步访问的几率很小,不会对性能造成影响.4.4NPC和SHCNPP协议对基准内核结构作了改动.为了实现在内核发生读缺失时能够预测距离最近的有效数据副本节点,记录历史信息是必要的,加入了节点预测CacheNPC(NodePredictingCache).另外,为了能精确更新历史信息,还加入了节点共享历史信息CacheSHC(SharingHistoryCache),如图6.(1)CNPNPC条目中除Tag外还保存了最近节点指针CNP(ClosestNodePointer),该指针指向距离请求内核节点最近的有效目标数据副本节点.发生读、写缺失时,内核根据NPC中存储的CNP信息就能够直接进行访问,而不需要访问目录.与节点目录类似,每个内核的NPC只存储地址末n位与该内核n位节点内二进制编码相同的数据的预测信息,也就是说发生读、写缺失的内核需要根据缺失数据的地址末n位来访问该数据CNP预测信息所在的NPC.(2)SHVCNP的有效性和准确性需要更新算法来保证,为了保证更新算法的精确性,SHC中保存了共享历史向量SHV(SharingHistoryVector),用来保证更新时只对那些曾经缓存数据副本的节点发出更新请求,避免造成网络拥塞和不完整更新.NPP协议的更新算法在第5节中讨论.NPC和SHC均为保存历史信息的Cache部件,所存历史信息被访问的频率越高,降低一致性交互延迟的效果越明显,因此NPC和SHC均使用PLRU策略对其中最近最少被访问的进行替换.4.5NPP协议描述以下只讨论Cache中存在被请求数据副本的情况,Cache中不存在被请求数据副本时需要访问主存获得数据.另外,NPP协议不对MESI协议的状态迁移图做改动,只讨论对读、写缺失一致性交互过程的优化.表1是文中缩略语的含义.简称CNPClosestNodePointerDWPDirectWrite-MissProcessing直接写缺失处理GDVGlobalDirectoryVector全局目录向量LWPLastWriterPointerNPPNodePredictingProtocol节点预测协议NPCNodePredictingCacheNDSNodeDirectorySliceNHPNodeHangingPointerSHCSharingHistoryCacheSHVSharingHistoryVector共享历史向量SCUSignatureCollectionupdating签名回收更新4.5.1读缺失NPP协议的主要目的之一是在读缺失时快速得到有效数据副本.通过两级目录和精确更新的NPC支撑,NPP协议实现了基于节点的预测;而通过节点挂起技术,NPP协议则实现了2-hops直接数据访问.本文提出一种简单的读缺失目录访问延迟隐藏技术———节点挂起技术,为每个节点目录片段(NDS)增加一个节点挂起指针NHP(NodeHangingPointer),如图7(a),设节点A中某内核缓存了节点B请求的数据D副本,当节点A收到来自节点B的数据请求时,则将对应NDS关联的NHP指向节点B(挂起节点B),然后向节点B发送数据D的同时也向全局目录发送目录更新请求,全局目录收到请求并加入节点B信息后返回一个确认消息,节点A收到该确认消息后清除对应NHP指针,节点挂起过程结束.节点挂起只针对来自读缺失节点的数据请求,对于来自全局目录的请求不予挂起.该方案用较小的代价隐藏了目录访问延迟,能同时发送数据和更新全局目录,实现了2-hops直接数据访问.节点挂起技术的核心是全局目录加入读缺失节点信息前在数据提供节点A中临时保留读缺失节点的“踪迹”,该“踪迹”的存在是节点A发送请求数据的前提条件.Page8如图8,在NPP协议中,发生读缺失的节点B内核根据缺失数据地址按照两步目录访问方案访问图7节点挂起技术示意图节点B目录,如果在节点目录命中,则访问节点B中内核,并更新节点目录;如果节点目录不命中,则查找节点B内对应NPC的预测信息,如果NPC不命中,则按照3-hops读操作流程访问全局目录,由全局目录通知缓存有效数据副本的节点向请求内核发送数据,请求内核收到数据后根据数据地址更新相应的NDS,同时全局目录加入节点B共享信息.如果NPC命中,则根据CNP向预测节点A发出读数据请求.节点A按照两步目录访问方案访问节点内目录,如果不命中则节点A将该请求转发至该请求数据的全局目录,剩余流程与3-hops读操作相同;如果在节点A内目录命中,则按照图7的节点挂起流程进行处理.4.5.2写缺失在NPP协议中,每内核的NPC中存储了部分数据的节点预测信息CNP,这为写缺失时直接访问数据副本提供了支撑.本文提出一种简单的写缺失目录访问延迟隐藏技术———直接写缺失处理技术DWP(DirectWrite-MissProcessing).利用CNP来预测有效数据副本所在的节点,当数据处于M、E状态,NPC命中,且对应CNP预测正确时,能将写缺失一致性交互过程缩短为2-hops.当NPC不命中时,按照3-hops流程处理.而对处于S状态的写缺失数据仍维持4-hops交互,只是交互过程稍有不同.对处于I状态的写缺失数据将从下一级存储器先读入数据,再进行修改.在写命中的情况下,仍需Page9访问全局目录将其它副本作废.假设写缺失数据D处于M或E状态,则片上只存在唯一的数据副本,这时当写缺失内核所在节点的NPC命中且对应CNP预测正确时,写缺失内核就根据该CNP可以直接将该唯一的数据D副本作废,从而降低了全局目录访问延迟.而当缺失数据D处于S状态时,即使NPC命中且CNP正确也只能预测一个节点,当其它节点也共享数据D时,在不访问全局目图9DWP对M、E状态写缺失数据的直接处理流程图10DWP对S状态写缺失数据的4-hops处理流程对于状态为M或E的写缺失数据,写缺失内核同时发出两个访问请求,发往全局目录的请求查询到数据D状态为M或E后,全局目录忽略该请求,如图9中的1a消息.根据CNP预测发出的请求在录的情况下仅依靠CNP无法将其它节点中的数据D副本作废掉,因此DWP对处于S状态的写缺失数据无效,依然需要访问全局目录来进行4-hops交互,如图2(c).一个问题是发生写缺失的内核访问全局目录前并不清楚该数据的当前状态,那么到底该根据CNP进行预测访问(M、E)还是直接访问全局目录(S)?DWP的解决方案是向CNP预测节点和全局目录同时发送访问请求,如图9和图10所示.预测正确的情况下将直接作废掉处于M或E状态的唯一数据D副本,实现了2-hops写缺失一致性交互;而在预测错误的情况下请求将被错误预测节点转发到全局目录进行处理,整个过程为4-hops交Page10互,错误预测代价为1-hops.对于状态为S的写缺失数据,写缺失内核也同时发出两个访问请求,发往全局目录的请求查询到数据D的状态为S后,继续按照4-hops交互流程处理.而发往CNP预测节点的请求在预测正确的情况下将查询到数据D的状态为S,此后该数据D共享节点将忽略该请求,如图10(a)中1b消息.在CNP预测错误的情况下,请求将被错误预测节点转发到全局目录,而全局目录将忽略该转发的请求,如图10(b)中2c消息.4.5.3替换NPP协议的双层目录结构可以将部分替换操作的影响范围限制在单个节点内,不但减少了消息传播的距离、缓解片上网络拥塞,还增加了替换操作的效率.根据替换数据的一致性状态,替换过程也不同.(1)M或E状态当被替换的数据处于M或E状态时,片上只存在唯一数据副本,这时需要先发送消息剔除该数据所在节点目录中的内核信息和全局目录中对应的节点信息,收到确认消息后进行替换操作.另外,M状态数据替换前还需要写回主存.(2)S状态替换S状态的数据时根据节点中被替换数据图11过时、不准确历史信息的产生过程NPC通过CNP来指明距离当前发生缺失内核最近的有效数据副本所在的节点.假设NPC中的指针是静态的,而随着程序的运行,数据副本的数量及其在多个内核上的分布位置会发生变化,如上所述,NPC中的CNP信息可能是过时的、不准确的,正确性无法得到保证.因此,动态精确更新NPC中的CNP信息是NPP协议有效性的关键.5.2基于签名回收的NPC更新为了能精确的更新NPC中的CNP信息需要解的副本数量分为两种情况:①所在的节点中只存在唯一副本,替换过程与E状态类似;②所在的节点中存在多个副本.只需要通知节点目录剔除发生替换操作的内核信息后进行替换,而无需通知全局目录.在这种情况下,交互只发生在节点内部,短距离的消息传输增加了替换操作效率.替换操作可能引起NPC预测访问的扑空,该类扑空均按照NPC预测错误处理,以便减少可能造成的网络拥塞.5NPC更新算法5.1精确更新NPC的目的实现直接数据访问的基本条件是请求内核能够准确预测数据副本所在的位置.一种基于局部性原理的实现方案是在发生写缺失时,被作废的数据副本所在内核记录写操作内核的标识作为历史信息,我们称其为Writer-Recording算法.则在下次写缺失到来前,这些被作废内核能够准确的预测有效数据副本所在的内核.但这种方法存储的历史信息可能是过时的和不准确的.在基于图3的CMP结构下,以对数据Data_A的读写序列为例,图11说明了上述缺陷.决以下两个问题.5.2.1签名回收由于NPP协议以节点为单位进行预测访问,该“签名”信息是指那些拥有数据副本的节点在全局目录中留下的共享标志信息.对于数据D,无论节点中的D副本处于M、E或S状态,只要是有效副本都需要在全局目录中进行“签名”形成共享信息,已形成的“签名”信息随着数据D发生写缺失会被作废掉成为历史共享信息SHV,而写缺失节点在全局目Page11录“签名”形成新的共享信息.本文提出基于签名回收的NPC更新算法SCU(SignatureCollectionUpdating)对CNP信息进行精确更新.图12(a)说明了“签名”信息随数据副本一致性状态迁移的流向.写操作(包括缺失和命中)发生时,当前目录中的“签名”信息将由写操作节点图12“签名”与SHV信息的流向与变化5.2.2NPC精确更新算法(1)更新时的CNP与SHV当一个数据D副本存在于某节点的L1Cache中时,该节点对应的NPC中无需保存该数据的CNP信息,因为其本身就是该有效数据的最近节点,只有当该节点中所有数据D副本都被作废掉(替换操作不分配CNP)时才在NPC中为数据D分配CNP指针,这样能提升NPC空间的利用率.对数据D来说,其对应SHV信息在数据D发生第一个写缺失时首次出现,并随后续写缺失节点传递.当发生替换操作,并且被替换数据与其对应SHV信息位于同一内核时,SHV信息也从SHC中被替换.我们修改了PLRU替换算法,优先替换那些没有在SHC中缓存对应SHV信息的数据.(2)基于写操作的更新由于写操作命中和缺失时NPC更新流程相同,这里只说明写缺失时的更新过程.当数据D处于M状态时,OldWriter节点(上一个写操作节点)在收到NewWriter节点(当前写缺失节点)的请求后,作废自身数据D副本并在本节点对应NPC中分配空间,将CNP指向NewWriter节点,接着根据节点对应SHC中的SHV信息更新历史共享节点,将它们的数据DCNP信息更新为回收,并与之前的SHV信息(如果有的话)进行叠加产生新的SHV信息.该SHV信息在后续写操作节点间继续进行叠加传递,从而实现历史信息的保存,如图12(b).另外,替换操作也会改变共享信息的内容,但文章认为发生替换的节点重用该数据的可能性不强,对其历史信息不予保存.NewWriter节点,最后将SHV信息传递给New-Writer节点.而NewWriter节点将收到来自全局目录的数据D当前共享信息GDV(GlobalDirectoryVector)和来自OldWriter节点的SHV信息,New-Writer节点将这两者做“OR”操作,将结果作为新的SHV存储在节点SHC中.如图13(a)、(b).当数据D处于S状态时,NPC的更新流程与M状态时类似.不同的是,在有多个S状态节点时,需要这些S状态节点同时访问自身的SHC,并从发生命中的OldWriter节点中取出SHV送往New-Writer节点,如图13(c);如果所有S状态节点的SHC均未命中,说明OldWriter节点中所有数据D副本和对应SHV已经被替换出L1Cache和SHC,这时则不用向NewWriter节点发送SHV,而New-Writer节点直接将收到的GDV作为新的SHV信息.另外,S状态时如果CNP预测错误,由错误节点转发的请求不会被全局目录响应,因此NPC更新流程与CNP预测正确时相同.当数据D处于E状态时,数据D的SHV信息没有生成,如果这时发生数据D写操作,缓存数据D唯一副本的节点收到NewWriter节点的请求后不用进行历史共享节点的NPC更新,也不用进行自身SHV信息的传递.只需作废自身数据D副本并Page12图13M、S状态数据基于写缺失的NPC更新在本节点对应NPC中分配空间,将CNP指向New-Writer节点即可.而NewWriter节点只会收到来自数据D全局目录的GDV信息,并直接将其作为新的SHV信息.(3)基于读缺失的更新当发生读缺失时,新出现的数据共享节点将缩短部分历史共享节点与有效数据副本节点的最小距离,而另一部分则保持不变.SCU只对s况数据从下一级存储器中取入,SHV信息还未生成,无须更新NPC;第二种情况中当数据由片上处于E状态节点提供时,SHV信息不存在,也无须更新NPC.而当数据由片上处于M或S状态(该S状态由M状态,而非E状态转换而来)节点提供时,SHV信息已经存在,故SCU只对该情况下发生的读缺失进行Page13NPC更新.当数据处于S态,并且该S态是从M态转换而来时,存有SHV信息的OldWriter节点已经“淹没”在多个S态节点中,导致无法定位SHV信息的位图14基于读缺失的NPC更新算法根据4.5节提出的基于节点挂起的读缺失一致性交互流程,当读缺失节点NPC命中时,全局目录向数据提供节点发出目录更新确认消息的同时也将GDV信息发送到LWP指向的OldWriter节点,在那里将进行两次过滤操作.第1次过滤先用GDV与SHV信息进行“AND”操作,再将结果与SHV信息进行“XOR”操作,过滤掉当前已经缓存数据副本置.SCU更新算法为全局目录增加一个LWP(LastWriterPointer)指针来跟踪OldWriter节点,如图14(a).LWP指针在每次发生写操作时被更新一次,始终指向最后发生写操作的节点.的历史共享节点.第2次过滤对SHV指向的历史共享节点与数据提供节点和读缺失节点之间的距离进行计算,过滤掉所有与读缺失节点间距离大于与数据提供节点间距离的历史共享节点.节点间距离的计算方法如下:对于一个有2n×2n个节点的2DMESHCMP来说,需要2n位二进制数对节点进行编码.编码的高n位指明节点位于哪一行,低n位指Page14明节点位于哪一列.设节点A和B的编码分别为a2n-1a2n-2…a1a0和b2n-1b2n-2…b1b0,则节点A与B之间的距离为|b2n-1b2n-2…bn-a2n-1a2n-2…an|+|bn-1bn-2…b0-an-1an-2…a0|.这两次过滤操作可并行进行,最后将两次过滤的结果进行“AND”操作生成最终需要更新的历史共享节点信息,如图14(a).OldWriter节点根据该最终SHV进行NPC的更新.更新时OldWriter节点将读缺失节点编码发送到需要更新的历史共享节点中,这些节点收到后与自身NPC中存储的CNP节点编码做距离比较.如果读缺失节点与历史共享节点间的距离小于CNP节点与历史共享节点间的距离,则用读缺失节点编码更新历史共享节点NPC中存储的CNP指针,否则,保持原CNP内容不变.如图14(b)中,假设S状态节点S1是距离读缺失节点R最近的有效数据节点,则所有深色区域节点与R节点的距离均小于与S1节点的距离,所有位于深色区域且未缓存读缺失数据副本的HS节点都应更新其NPC信息,如HS3(CNP=S2)、HS4(CNP=S1)和HS5(CNP=S1).更新时发现HS3虽然位于深色区域,但HS3与其存储的CNP指向的S2节点间距离仍小于HS3与R间的距离,故保持HS3节点原先存储的CNP不变.图15全映射目录协议和NPP协议的读缺失一致性交互对比当读缺失节点NPC不命中或命中但预测错误时,全局目录响应请求并通知距离读缺失节点最近的有效数据副本节点发送数据.数据发送节点向全局目录返回确认消息后,全局目录根据LWP向OldWriter节点发送GDV,并开始与NPC命中相同的更新过程.如果LWP指向的OldWriter节点中读缺失数据副本和对应SHV信息已经被替换出去,则本次NPC更新终止.如果LWP为空,当前读缺失数据处于S状态,这说明该S是由E状态转换而来,SHV信息不存在,本次NPC更新终止.6NPP协议可扩展性分析6.1对读缺失一致性交互延迟的影响通常所说的3-hops或者4-hops读实际是指完成一个共享读操作需要3或者4个步骤,并不是途经3或4个内核间距离的长度.1-hop的具体物理长度是多少需要视目录内核、请求内核和数据共享内核的网络相对拓扑关系和片上的物理距离而定,如对一个基于4×42DMESH互联的16内核处理器来说,假设相邻两内核间的物理距离为1,则1-hop所代表的实际距离从1到6均有可能.假设两相邻内核间距离长度为1,则图15(a)Page15中,基于全映射目录的3-hops读缺失处理时关键路径长度为16,而图15(b)中NPP协议处理相同的读缺失时关键路径长度为6.另外,在节点已经缓存数据副本的情况下,双层目录结构能够将节点内读缺失和替换操作的一致性交互限制在本节点内,无需访问远程全局目录,从而降低访存延迟.可见本文提出的基于节点挂起的读缺失处理不但能将共享读操作步骤减少为2-hops,而且能有效减少一致性交互关键路径延迟,具有良好的延迟可扩展性.6.2对写作废延迟的影响NPP协议能够在CNP预测正确的情况下直接作废掉处于M和E状态的写缺失数据副本,完全隐藏了目录访问延迟,提升了写缺失一致性处理延迟.另外,虽然DWP对S状态数据写操作的处理过程做了些许改动,但仍属于4-hops一致性交互,没有增加额外延迟.由于NPP协议采用两层目录,当发生写操作需要进行作废操作时,粗向量全局目录只需将作废请求发至缓存数据副本的节点即可,每个收到作废消息的节点则根据节点内目录并行作废节点内可能存在的多个数据副本,这不但减少了作废消息在片上连线传播的距离,还增加了作废操作的效率.6.3存储开销为了隐藏目录访问延迟、实现直接的读缺失和写缺失一致性交互,NPP协议增加了必要的存储部件来跟踪历史信息.NPP协议的存储开销包括以下几个方面:(1)双层目录双层目录中,全局目录负责跟踪节点缓存数据副本的情况,当处理器内核数量固定时,全局目录大小与节点规模成反比.节点内目录采用全映射方式(4)SHCSHC用来保存节点的共享历史.SHV的宽度与全局目录条目的宽度相同,与处理器的节点数量成正比.对于23×23内核、节点规模22内核的CMP来说,SHV的宽度为16bits.由于实际应用程序中读操作数量通常是写操作数量的几倍至十几倍,因此本文设计的SHC容量也少于NPC容量.表2统计了在表3参数配置下NPP协议和普通全映射目录协议的存储开销情况.假设使用40位物理地址,根据表3中私有L1Cache和每内核L2CacheSlice的具体配置参数,可以得到这两个Cache需要的存储空间大小.其中Tag宽度=物理地址宽度-Cache索引宽度-块内偏移宽度,Cache索引宽度=Cache容量/(块大小×组相联度),块内偏移宽度=Log块大小表264核CMP每内核一致性协议的存储开销数据Cache全映射目录DiCo-CMP(Base)NPP(节点规模4)25bitstag+64bytes8bytes8bytes8bytes29bitstag+4bits30bitstag+4bits32bitstag+2bytes在64内核的CMP中,假设使用扁平全映射目录方案,对一个共享数据来说,每个内核中的L2跟踪节点中内核缓存的数据副本,因此节点内目录的大小与节点规模成正比.对于23×23内核、节点规模22内核的CMP来说,全局目录的每一个条目需要16bits空间来跟踪16个节点.而节点内目录的每一个条目需要4bits来跟踪节点中的4个节点.(2)LWPLWP的作用是在基于写操作的NPC更新过程中指明OldWriter节点.考虑到所有节点中缓存的数据均有可能经历M→S的状态转换以及NPC更新算法的精确性和性能,我们为全局目录的每一个条目都增加一个LWP指针.对一个有16个节点的CMP来说,LWP的宽度为log16(3)NPCNPC中存储的CNP在发生读缺失时用来指明距离最近的数据副本节点.CNP的宽度与处理器节点的数量有关,与LWP类似,对一个有16个节点的CMP来说,CNP的宽度同为log16CacheSlice中的目录需要跟踪包括自身在内的64个内核的数据共享状况,跟踪每个内核需要Page161bit,则64个内核需要64bits,即8bytes.因此,DiCo-CMP方案每内核中L1D-Cache和L2CacheSlice目录各需要8bytes,而DiCo-CMP中的L1和L2一致性Cache除了标签外,还需要跟踪O状态数据所处的内核,跟踪64个内核需要6bits指针.对于NPP协议来说,在表2所列具体情况下,由于将4个内核分为一个节点,64个内核共分为16组,L2CacheSlice中的目录需要跟踪包括自身在内的16个节点,每个节点需要1bit,16个节点共需要16bits,即2bytes.另外,为了能够跟踪最后写节点,还需要为L2CacheSlice中目录的每个条目增加一个4bits的LWP,能够跟踪16个节点.对于节点目录,NPC和SHC来说,由于采用直接映射方式,并且不需要块内偏移,通过Cache块的数据可知其Cache索引宽度,然后根据物理地址可知其Tag宽度.对于全映射目录来说,由于片上两级Cache采用Inclusive模式,省去了L1Cache中的目录结构,所有共享信息均保存在L2CacheSlice一侧,因此相对于Non-inclusive模式来说,全映射目录存储开销较小,但仍占到10.65%.DiCo-CMP方案的一致性存储开销为13.27%.对于NPP协议来说,存储开销不随内核数量增加呈指数级增长,而是由多个存储规模线性增长的存储部件之和构成.假设节点规模为n,则粗向量全局目录的存储开销和存储开销随内核数量的增长速度仅为全映射目录协议的1/n.节点内目录虽然采用全映射方式,但L1Cache条目数量少,节点规模小,新增内核在所有节点中均匀分布等特点使节点目录的存储开销较小并呈线性增长.另外,考虑到目前Cache较高的读/写命中率、实际应用中数据共享规模不大以及SHV信息的产生、传递特点等因素,NPC和SHC的存储开销也较小.7性能模拟7.1实验平台本文使用全系统仿真器Simics[35]和威斯康星大学研制的GEMS[36]仿真器对基于MESI状态转换的扁平全映射目录协议、Dico-Base协议(L1和L2CoherenceCache为4路512组、所存储指针宽度为6位)和多种NPC、SHC容量配置方式下的NPP协议进行评估.GEMS具备详细的存储系统时序模型,能够对多核处理器Cache一致性协议中涉及的消息、数据传递和数据状态迁移等事件进行详细模拟.表3列出了GEMS仿真器的参数配置.本文从SPLASH2测试程序的应用集和内核集中各选择4个程序进行测试,表4是测试程序及其数据输入情况.CMP规模节点规模内核参数Cache层次关系InclusiveCache块大小64字节私有L1I/DCache共享L2CacheNPCSHC主存访问时延200个时钟周期NoC参数测试程序CHOLESKYtk18.OFFTLURADIXBARNESVOLRENDOCEAN-NC258×258WATER-NS512molecules,3timestepsWATER-SP512molecules,3timesteps7.2仿真结果针对本文提出的节点挂起技术和直接写操作处理技术,我们评估了L1Cache平均读缺失延迟和写缺失延迟两项指标.另外,还对程序的执行时间、网络流量变化情况和SCU更新算法的有效性进行了评估.以上对NPP协议性能的评估均在NPC和SHC容量变化的情况下进行,且图16~图19的所有模拟结果数据均经过相对于全映射目录协议方案的归一化处理.(1)程序执行时间相对于全映射目录来说,Dico-Base协议对所有测试程序的性能表现较为均衡,程序执行时间降幅在5.02%~14.1%之间,平均为9.3%;而NPP协议根据NPC中保存的动态CNP信息对有效数据所在节点进行预测直接访问,降低共享数据的读缺失Page17和写作废一致性交互延迟,从而减少程序执行时间.另外,双层目录结构能够在同一节点内包含同一数据多个副本的情况下增加作废操作的并行性,减少作废延迟.图16中,程序执行时间降幅最大的是OCEAN-NC和WATER-SP,在NPC(6.5625KB)+SHC(1.5KB)的NPP协议配置下,降幅分别达到了53.10%和51.30%.在3种不同的NPC+SHC配置下,程序平均执行时间降幅分别为21.78%~31.11%.可以看到,NPC和SHC在最大配置下性能仍有提升,但执行时间降幅已经非常有限.(2)平均读缺失延迟在降低L1Cache读缺失延迟方面,相对于全映射目录来说,基于Dico-Base协议时,VOLREND的降幅最大,达到18.9%,而FFT的降幅最小,达到7.02%,平均降幅为11%.NPP协议中的CNP预测和节点挂起技术缩短了读缺失关键路径长度,减少了读缺失延迟.NPC容量变化时,随数据共享规模的不同,其对数据共享历史信息的捕捉能力也不同,从而影响节点预测的有效性;而SHC容量变化时,随程序在同一内核中发生写操作频率和密度的波动,其对过往历史共享信息的捕捉能力也会不同.图17中,所有程序的平均读缺失延迟降低14.22%~18.9%.其中BARNES和WATER-NS降幅最大,分别达到23.98%~24%和21%~24.12%.BARNES和WATER-NS在NPC、SHC的不同配置下性能几乎没有波动.(3)平均写缺失延迟在降低L1Cache写缺失延迟方面,相对于全映射目录来说,在Dico-Base协议下,OCEAN-NC的降幅最大,达到16.09%,而CHOLESKY的降幅最图17NPP协议对L1Cache读缺失延迟的影响小,达到1.89%,平均降幅为8.3%.NPP协议只对处于M或E状态的写缺失数据有加速访问效果,但也没有增加S状态写缺失数据的访问延迟.另外,延迟减少的程度与目录节点,写缺失节点和数据副本节点之间的位置关系有关,如果目录节点原先就位于写缺失节点和数据副本节点的中点附近,则加速效果不明显.由于测试程序中数据共享规模为1(M或E状态)的情况占有很大比例,因此NPP协议明显的降低了写缺失延迟.图18中,所有程序的平均写缺失延迟降低17.89%~21.13%.其中LU和VOLREND降幅最大,分别达到45%~46.14%和29.97%~32.09%.同样,LU和VOLREND也对NPC+SHC的配置情况不敏感,原因可能是NPC+SHC的容量已经足够捕捉历史共享信息.图18NPP协议对L1Cache写缺失延迟的影响(4)网络流量Dico-Base协议作废数据时不会发出大量的冗余请求,而且其对L1和L2CoherenceCache的更Page18新较为简单,数据量小,所以Dico-Base协议相对于全映射目录协议和NPP协议来说,网络流量是最小的,比全映射目录协议平均低12.67%,比最大配置的NPP协议平均低19.3%.为了保证NPC更新的精确性,SCU更新算法增加了NoC网络流量,这些增加的流量主要是在传递更新过程需要的控制消息,节点数据共享历史信息(SHV,GDV),节点指针信息(CNP,LWP)时产生的.但另一方面,双层目录的NPP协议具备的节点内并行写作废能力和节点内读缺失化解、替换能力能减少网络流量.实验证明,NPP协议的网络拥塞程度略大于全映射目录.由于NPC和SHC对共享历史信息的捕捉能力会影响到读缺失和写缺失的性能,所以在不同的NPC+SHC配置下,网络流量也会有变化.图19中,相对于全映射目录,NPP协议的网络流量平均增加6.62%~7.28%.其中CHOLESKY和WATER-NS网络流量增加最多,分别达到16%~17%和10%.(5)SCU算法对NPC预测准确率的影响基于NPP协议框架,我们对使用Writer-Recording算法(见5.1节)和SCU算法时NPC的预测准确率做了评估.从图20可知,采用Writer-Recording算法时,NPC的预测准确率最高为65.37%,平均为48.04%;而使用SCU算法时,NPC的预测准确率最高为93.52%,平均为85.84%,NPC预测准确率明显提升.SCU更新算法增加了NoC网络流量,但也提升了NPC的预测准确率.图20是使用SCU更新算法前后NPC预测准确率的变化情况.对于使用了SCU(签名回收的NPC更新)算法的NPP协议来说,NPC的节点预测准确率受到共享数据重用模式图20SCU更新算法对NPC预测准确率的影响的影响,重用程度不高或者重用间隔过长都有可能导致NPC替换频率的升高,从而导致NPC访问时缺失率增加,准确率下降;另外,在基于2DMESH的NoC互联网络中,所有的数据传输和消息传递都依赖于片上连线和路由器,某个时刻某个通路上的网络拥塞都有可能导致路由器响应不及时和链路饱和,这会导致NPC更新实时性的下降,从而降低NPC预测的准确率.而对于没有使用SCU的NPP协议来说,除了以上两个原因,写缺失所产生的过时历史信息也会严重降低NPC的预测准确率.8结论存储系统的发展滞后导致CMP处理器性能不能随内核数量的增加等比提升.随着线延迟和芯片规模的不断增大,Cache一致性交互延迟在访存延迟中所占的比重越来越大,一致性协议的存储开销也严重限制了系统的可扩展能力.本文以延迟、存储可扩展为目标,提出基于节点预测的直接Cache一致性协议,针对一致性协议中存在的数据间接访问问题,分别提出适用于读缺失和写缺失的节点挂起技术和直接写缺失处理技术,有效隐藏了目录访问延迟.为了实现准确的节点预测,我们还提出基于“签名”回收的历史信息更新算法,避免了冗余更新和不完整更新的发生.在基于2DMESHNoC互联的64核CMP测试环境下,对8个SPLASH-2测试程序进行了性能模拟.相对于全映射目录协议,NPP协议的平均执行时间降幅为21.78%~31.11%;平均读缺失和写缺失延迟分别降低14.22%~18.9%和17.89%~21.13%.由于历史信息更新算法需要发送的相关控制消息和历史信息,网络流量平均增加Page196.62%~7.28%.未来我们将进一步研究减少一致性延迟的方法,如数据自作废技术.目前数据自作废主要依靠软件同步和历史信息预测,与程序中实际写作废的契合程度还不理想,在一定程度上会破会已经建立的数据局部性.高效的数据自作废技术在写操作时不依赖于目录,能大幅提升写操作效率.我们还将研究Cache一致性协议的低功耗技术,包括如何提升一致性交互效率,如何减少一致性交互消息数量,如何避免大位宽数据在连线上长距离传输的方法等问题.
