Page1Redbud并行文件系统的可扩展存储管理机制易乐天舒继武郑纬民(清华大学计算机科学与技术系北京100084)摘要数据规模和并发访问的需求日益增长,可扩展能力成为并行文件系统的重要需求之一.文中提出了一种基于非对称并行文件系统Redbud的高可扩展资源管理机制.该管理机制根据数据的访问特征,使用不同的树形结构管理不同类型的数据,满足了文件数据和元数据的并发检索需求;该管理机制还使用文件级的数据分布机制,允许用户利用各种策略进行目录和文件的管理,能满足文件级的数据访问性能、目录级数据可靠性等实际应用需求.多个基准测试程序和实际应用程序的测试结果表明,文件的独占访问能达到磁盘95%的性能;同时,随着设备和应用节点的增加,数据和元数据的并发访问性能线性增长.关键词Redbud并行文件系统;高性能计算;存储资源管理;共享磁盘1引言在具备存储区域网络(StorageAreaNetwork,SAN)的数据中心中,基于共享磁盘的并行文件系统被广泛地部署,为各种应用提供存储需求.这些数据服务通常呈现两种趋势:一方面,数据服务规模越来越大,比如,科学计算已经成为数据密集型的应用,许多实验数据每周以TB级增长,其存储数据量逐渐迈向EB级;2008年的Jaguar系统[1]则有10PB的Page2存储容量和192个IO节点.另一方面,并发访问的需求越来越高,比如,高性能科学计算产生大量的中间文件,这些文件通常被多个应用节点并发访问[2];CPU多核技术的发展也加剧了并发访问的趋势.这些趋势对存储资源的管理提出了更高的可扩展性要求.在传统的基于SAN环境的并行文件系统中,所有共享磁盘以逻辑卷的方式管理,比如,GFS2(GlobalFileSystemVersion2)采用LVM(LogicalVolumeManager)建立存储池①,GPFS(GeneralParallelFileSystem)采用分段放置的方式,将所有数据以条带化的方式存储[2].当系统规模扩展时,这种方法难以为每个逻辑卷增加存储空间;而且,这些方法采用静态的方法分布数据,比如GFS2条带卷的宽度被设定后,无法针对指定文件或者目录进行布局调整,降低了系统的扩展性.Redbud是一种基于SAN环境的并行文件系统,与许多分布式文件系统类似②,它采用非对称存储结构:将数据和元数据分布在不同的存储设备中.为了高效支持海量数据的并行访问,基于这种非对称存储结构,本文提出一种高可扩展的存储管理机制———Rstore.通过把存储设备划分为并行分配组,Rstore允许文件存储空间的分配和回收分布到不同的区域中,避免访问冲突产生的IO等待,满足了并发访问的需求.Rstore以分层的方法管理海量存储空间,分为语义感知层和空间映射层.其中,语义感知层跟踪文件的分配语义,开发文件分布的局部性,比如将同一客户端的文件分配在临近的位置;同时,它允许用户根据应用需求为目录配置分布策略,比如,用户可以为重要的目录定制更高的可靠性级别,保证子文件以多副本的方式分布在多个硬件失效域中.空间映射层利用基于区间范围(extent)记录文件的分配情况,使用多个树形结构建立空间的快速索引,比如,使用B树提供元数据的快速索引,将键值与文件访问的上下文语义相联系,保持了目录元数据和进程的访问局部性.我们采用基准测试程序和真实的并行应用程序进行了测试,结果表明,Rstore在两个方面获得了较好的可扩展性:首先,在文件的数据访问中,对文件的独占访问性能达到磁盘95%的性能,而在并发访问负载下,性能基本与独占访问性能持平;并且,在设备和客户端增加时,Redbud系统的数据和元数据访问性能接近线性增长.其次,在典型的并行文件系统元数据操作中,Redbud的磁盘访问性能相比传统的文件系统提高20%以上;在元数据服务器为IntelXeon1.60GHzCPU,2048MB内存的配置下,元数据服务引入的CPU开销维持在1%~6%范围中,体现出较好的元数据服务器可扩展能力.2背景与相关工作与传统的并行文件系统类似[3-5],Redbud主要由客户端(Client)、元数据服务器(MetaDataServer,MDS)和存储节点三部分组成.其中,客户端为上层应用提供标准的POSIX语义接口,是Redbud文件系统的文件访问入口.2.1Redbud非对称存储结构Redbud的数据存储采用非对称结构,将元数据和文件数据分别存储在不同的存储设备中.通过SAN环境的Zone技术,元数据的访问被隔离在MDS访问域内,避免客户端访问保存了元数据的磁盘设备.由于依据存储内容划分存储设备,这种非对称结构允许对元数据存储和数据存储进行不同的优化,比如管理员可以把海量的数据存储节点在物理上配置为廉价的存储介质,以保证文件系统的预算和IO带宽;元数据存储节点可配置为延迟低的存储介质甚至新型的固态存储设备,以满足元数据请求的服务质量.Redbud利用元数据文件系统保存所有元数据,构建全局目录结构.进行元数据操作时,MDS解析来自客户端的元数据请求,并转发到元数据文件系统;在访问文件数据时,客户端采用带外的方式与MDS交互,在获取文件布局信息(layout)后,直接访问存储文件数据的存储设备,避免了单服务器成为数据传输瓶颈.2.2并行文件系统的存储管理机制并行文件系统的存储管理机制主要分为文件布局方法、数据分布机制和元数据检索机制三个方面.其中,文件布局方法通常与存储模式相关,比如,基于对象存储设备的并行文件系统一般利用对象存储目标器(objectstoragetarget)提供对象的分布机制[3-4],我们专注于基于SAN环境的并行文件系统的相关工作:文件布局(layout)方法.用来维护文件的逻辑位置到物理位置的映射关系.目前,在基于SAN环境的并行文件系统中[6-7],文件布局都采用基于块①②Page3(block)的方法,利用多个块位图检索磁盘空间.GPFS首先设定普通块的大小为256KB,多个小文件可以被填充在同一个块内,但是由于许多科学计算中产生的文件相当大(TB级),导致索引块的元数据过大[2].Frangipani采用两种不同的块大小,大文件使用较大的块存储,这种方法在一定的程度上减少了元数据开销较大的问题,但是由于难以预测将来产生的文件大小,这种方法可能造成大量空间的损失[6].Rstore使用B+树检索空闲空间,满足多种空间分配的需求(见3.2节).数据分布机制.用来将数据分布在多个存储设备中,影响着系统IO性能和可靠性.比如,为了利用磁盘的并行访问性能,GFS2和CXFS①利用逻辑卷,在块级别将数据分布在多个磁盘设备中,以获得较高的IO聚合带宽,GPFS、Zebra[7]在文件级实现数据条带化策略,允许进行数据访问的负载均衡;在可靠性方面,它们通常依靠硬件RAID(RedundantArraysofIndependentDisks)的方式实现数据的冗余.一些基于智能设备的文件系统提出了多副本的策略,比如,通过轮询的方式,Google文件系统[5]将文件chunk分布在多个数据服务器中,Ceph[4]则允许客户端使用文件名生成hash值,并根据该值选择对象存储设备,在系统默认的情况下,它们均为所有文件保存3个副本,避免副本失效带来的数据丢失.这图1Rstore体系结构Rstore采用分层的方法提高存储管理的可扩展能力.存储设备被划分为多个并行分配组(ParallelAllocateGroup,PAG),系统运行时,每个在线的PAG些分布机制在系统构建之初就被设定,无法随着需求的变化灵活更改.然而,大型数据中心往往存在多种复杂应用[8],比如,某些计算应用要求存储系统提供更高的IO带宽,而某些关键数据服务则希望其具有更高的可靠性.Rstore提供了基于策略的目录级文件分布机制,允许为应用的工作目录配置灵活的数据分布,提高了系统的可扩展性和可管理性(见3.3节).元数据检索机制.用于目录结构以及文件信息的索引.目前,大多数并行文件系统采用直接索引的方法②,建立inode号和目录entry的关系,在进行目录遍历时查找相应的元数据区域,然后根据目录内容继续访问其子目录.这种方法的缺点是难以在遍历目录的过程中充分挖掘访问的局部性[9].Ceph采用类似日志文件系统的方法,任何元数据被顺序地写入对象存储设备中,当有元数据被释放时(如文件删除),后台进行空间的回收,这种方法提高了元数据写性能,但是由于无法保持进程的局部性,读操作的性能将受到较大影响.在元数据存储中,Rstore使用B树目录建立元数据索引,利用键值维护进程和目录路径访问局部性(见3.4节).3Rstore设计如图1所示,基于Redbud非对称的存储结构,①②Page4隶属于一个MDS,由该MDS负责其空间管理;所有文件系统元数据被存储在专用的元数据PAG(Meta-PAG)中,而文件数据及其副本则被存储在数据PAG(Data-PAG)中.下层是空间映射层(Mappinglayer),它负责将文件布局的逻辑地址映射到物理地址,并为PAG构建空间的索引结构,实现空闲空间的检索方法,满足空间并行分配和回收的需求.上层是语义感知层(Context-Awarelayer),它感知文件访问的上下文语义信息,利用访问局部性为Meta-PAG聚集inode等文件元数据信息,实现元数据索引机制,并利用基于策略的分布机制为文件数据分配存储空间.3.1文件逻辑地址映射如图1所示,Rstore采用extent的方法建立文件布局索引,实现文件逻辑地址到物理空间的映射,每个extent表示为一个四元组〈文件偏移(f_offset),PAG起始地址(p_offset),长度(length),〈PAG号+状态(status)〉〉,所有的extent存储在Meta-PAG中.Extent的状态标识表示了每个extent在磁盘上的空间占用状态,主要分为3种:normal表示普通的可读写文件空间,hole用于支持文件的空洞,表示未分配的空间,pre-allocated表示为文件持久预分配的空间,提高文件分配的连续性.为了提高存储空间效率,在extent的表示中,块(block)是最小的数据空间单元,每个块大小为4KB,使用32位的块号记录Extent在存储设备上的偏移和长度,每个extent结构大小为20字节.在较坏的情况下,如文件系统碎片较多或者文件平均大小较小的办公环境等,假设平均4个4KB的块表示一个连续的物理段,则extent仅会使用约文件数据大小0.2%的内存空间,而DrewRoselli等人在2008年针对网络文件系统的非HPC应用统计中发现文件平均大小正在逐年增长[10-11],因此extent的表示方法具有较高的空间效率.如图1所示,客户端为访问的文件建立layout缓存,以开发文件访问的局部性.每次在获取layout之前,客户端首先检查缓存中是否存在.客户端的每次文件写操作可能更新extent的状态,比如对一个文件的预分配段进行写入操作时,需要将extent的状态从pre-allocated更新到normal,因此需要跟踪写操作对extent的状态更新,并将这些更新发送到MDS,以确保文件extent的一致性.然而如果在每次写页面操作就更新extent的状态会引入巨大的通信开销,因此客户端以页面为单位建立extent的状态索引,并以聚集(batch)的方法将多次更新一次性发送到MDS.为了提高缓存中的extent状态检索效率,客户端以基树(radixtree)的方法记录每个页面的写状态变化,一旦页面因为写入而改变,则被加入该树中.为了保证extent的一致性,只有MDS才有修改extent状态的权限,因此,在客户端每次写操作之前,如果页面对应的extent缓存是不可写(hole)或者未写(pre-allocated)状态,则它首先检索基树,查看该页面是否已经被修改过,如果已经被修改,则表示对应的内存存储了最新的数据.3.2空间检索方法空间映射层为每个PAG提供两棵独立的B+树进行空闲空间的快速索引,其中一棵是偏移树,它采用空闲空间的起始块号(offset)作为键值,另一棵是长度树,它使用空闲空间的长度(length)作为键值.Rstore根据不同的文件空间分配请求,使用不同的B+树进行空间分配,比如,如果进行文件的扩展写,为了保持文件数据在磁盘上的连续性,选择第一棵B+树,搜索距离当前文件末尾最近的空闲空间进行分配;如果需要分配大段的连续空间,那么使用第二棵B+树可以迅速找到和需要的长度接近的一段连续空间.两棵B+树都索引某个PAG内所有的空闲空间,因此每次空间分配以后需要同步两棵B+树,以保证两棵树的一致性.在这两棵B+树中,每段连续空闲空间通过一个segment来表示,表示为〈块号(blockNumber),长度(length)〉.图2所示的B+树使用块号作为键值,每个B+树的外部节点按顺序保存所有被索引的extent;B+树的每个内部节点或者外部节点是磁盘Page5的一个块,每个块都用一个头部结构来标明此块在树中的层数,并用左块号和右块号标明左右兄弟节点;每个B+树的内部节点由键值(key)和相应的子节点的块号组成.如图2所示,根节点在属于内部节点的时候,只包含两个子节点〈key23,block24〉和〈key600,block25〉;为查找到start为733的segment,依次查找了root、block25、block355,最终到〈start=733,length=10〉.Rstore将B+树本身存储在PAG中,其占用的存储空间也从B+树中进行分配,因此,在某些智能存储设备上构建PAG时,允许其自主管理空闲空间,减少MDS的负载,提高资源分配的灵活性.然而,因为分配空间会改变B+树本身的状态,如果通过B+树管理自身的空间,可能陷入多层循环.比如,为文件分配空间可能造成节点的分裂,因此需要向B+插入一个新的segment,而此时正好需要额外的空间供B+树使用,此时一种选择是从插入的segment分配空间,但是插入的索引的空间可能并不够用,产生矛盾;而如果再次从B+树分配空间仍然可能出现分裂,产生新的节点插入,陷入循环状态.在Rstore中,PAG使用一个专用的空闲列表(free-list)来管理两棵B+树的存储空间,free-list所占用的块与普通空间一样,由B+树进行分配和释放.假设树的深度表示为level,sizeof(free-list)表示列表的大小,那么对任一B+树进行更新操作时:(1)如果插入一个索引,则只会发生节点增加.最极端的情况下,B+树的块会发生分裂而整体深度(level)会增加1,所以需要free-List提供额外的Level+1个块;而需要同步两棵B+树,假设另外一棵B+树的层数为Level2,因此在插入一个节点的情况下必须满足:Level+Level2+2<sizeof(free-list).(2)如果删除一个索引,只会发生节点的减少.最极端的情况下B+树的块会发生合并,而视整体的Level减少1层,所以需要Free-list提供额外的Level个空闲块满足释放需要,因此必须满足sizeof(free-list)<128-Level-Level2.因此,在每次对B+树进行更新操作前,PAG首先确保free-list有足够的空闲块,供B+树增加深度时使用;同时,确保free-list有足够的剩余空间,当B+树降低深度时,释放相应的存储空间.如果free-list不满足要求时,使用B+树进行分配或者释放,由于此时分配造成的B+树状态变化虽然会影响free-list的上下限,却不会引起空间分配的循环.3.3基于策略的数据分布机制Rstore利用多线程PAG的方法,从外部避免对同一棵树操作带来的瓶颈,比如,当有多个文件分配和回收操作并发进行时,空间的分配被分布在不同的PAG中,满足了海量存储系统中大量客户端并发访问的需求.另一方面,Rstore采用基于策略的文件分布机制,允许用户使用配置文件的方法,指定特定目录及其子文件的分布,比如,为了满足应用的性能需求,其工作目录中的文件可以被配置为条带化;而为了提高某个目录的数据安全保护级别,可以为该目录选择更复杂的加密算法等.为了减少文件系统操作的复杂度,Rstore提供了一系列的策略,以隐藏物理硬件的复杂性,并满足用户的各种需求,目前Rstore中主要实现了4种常见的数据管理策略,如表1所示.策略名称可靠性策略(R)文件数据以RAID或者副本的方式冗余性能策略(P)安全策略(S)文件数据通过可选择的加密算法进行加密分级管理(H)根据文件的各种访问属性,如size等,文件数据用户可以通过用户工具随意地为每个目录配置不同的分布策略.在每种策略中,语义感知层为每个文件保留一个标志分布策略的扩展属性组,以标识当前的分布信息;如果是目录文件,则其所有的子文件继承该目录的分布策略.比如,在典型的条带化性能策略(RF-striping)中,语义感知层为文件分配三元组〈AGG,strip_width,dup_alg〉,其中,AGG表示文件允许被分配到哪些PAG中,strip_width表示文件的条带化宽度,dup_alg表示副本的放置策略,如副本的位置、数量等(用于负载均衡).在解析性能策略时,语义感知层对大文件采用文件级条带化的方法,当文件大小超过系统预设阀值时,尽可能地将大文件分割,并存储到AGG指定的PAG中;同一个进程分配的小文件,则尽可能地存储在同一个PAG中的磁盘的连续位置.如图3所示,“/dir_1”目录中的文件被配置分布到PAG1和PAG2两个分配组中,请求创建文件foo_1时,语义感知层首先使该文件继承其父目录的资源分布策略,获取AGG指定的PAG号,并进行排列,然后根据文件的请求的offset和inode号进行hash产生空间的分配逻辑号,最后对该逻辑号进行掩码操作决Page6图3分层资源管理机制定其extent分配的PAG号.而客户端从MDS获取了layout之后,为了定位文件数据的物理位置,将首先检查extent所处的PAG号,然后将其内部偏移转化为具体设备上的物理位置.“/dir_2”目录中的文件副本则被分配在与PAG2处于不同失效域的PAG3中,如两者使用不同的电源、不同的阵列控制器等.在我们的测试中使用了两种典型的性能策略(RF-striping和RF-Roundrobin),结果表明,这种目录级的数据分布方法为用户提供了更多的选择和更高的灵活性.为了减少MDS的负载,在大部分的策略中,额外的计算工作由客户端进行处理,比如在可靠性策略中,文件如果以RAID-5的方式分布,则客户端总是在更新文件时同时更新文件校验数据块的信息;在安全策略中,客户端则根据不同的加密模式进行安全元数据文件的加解密.而分级管理策略要求监视文件的变化过程,并根据迁移的策略进行具有不同性能属性的存储介质间的迁移,因此与其他策略不同的是,它需要MDS指定一个专用的客户端进行分级管理,以避免客户端离线带来的异常情况.3.4元数据检索机制Rstore以B树构建元数据索引,其中树节点以块号为键值,对应于文件系统中每个目录的ID号,文件的inode和layout被存放在叶节点,每一个文件对应一个ID号,进行路径查找时从根目录开始以目录ID逐级解析.采用B树构建文件系统的传统方法中,通常尽可能地将目录中所有子文件保存在磁盘的邻近位置,如XFS在目录项中记录文件名和用于索引文件的inode号[11]、Reiserfs将目录项存储在directory-item中,采用item号索引对应文件的inode块地址①,这些方法开发了目录中子文件的存储局部性.在元数据存储的构建方法中,仅仅开发目录的子文件存储局部性是不够的,这是因为子文件的inode索引与其父目录的索引号相差较远,因此在进行目录lookup操作之后需要经过较长的磁盘定位时间才可以访问其子文件inode与内容,这种问题在本地文件系统中的影响较小,但是在Redbud中,客户端为了访问某个文件,首先需要访问元数据存储的元数据文件以获取layout信息,因此长时间磁盘定位延迟将对元数据访问效率有着重要影响.根据元数据文件较小的特点,元数据存储的构建中使用了目录嵌入机制以提高Meta-PAG的磁盘访问效率.在目录嵌入机制中,进行元数据文件空间分配时尽可能地将各种相关的元数据信息存放在一起,如同一个目录或同一个进程相关的文件inode和layout信息.创建文件时,文件inode被嵌入目录的内容中,每个inode占据一个块大小,如果是子文件,则当文件进行空间分配时layout被存储在inode块中,由于元数据文件较小,绝大部分的layout将存储在inode的末尾,对于较为极端的碎片情况,文件大小超过一个块大小时,新的块将被分配在目录块的预分配位置.因此,客户端进行元数据访问时,即使采用了操作聚合的方法[12],绝大部分情况下只需要一次IO和网络交互即可预取所有需要的相关元数据信息,而存在多个并发IO操作时(如多个客户端共享访问同一个目录),期间仅存在极少的磁头定位时间.元数据存储以〈父目录ID,进程ID的hash值,①ReiserfsforLinux.http://rfsd.sourceforge.net/resources.Page7类型,文件名ID的hash值〉四元组表示目录inode的索引键值,在查找某个目录时可以通过该键值找到对应的inode块号,其中父目录ID是其父目录索引键值中的一部分,用以聚集某个目录的子文件,进程ID是一个与客户端ID相关联的ID号,在键值中加入进程ID的hash值则可以聚集同一进程的所有相关文件,文件名ID的hash值用于大目录的索引支持,类型用于区分文件mode.图4表示Redbud文件系统的树形结构,其中p表示树中的块指针,inode集合表示聚集在同一个目录的子文件inode及其layout内容,元数据存储在为“/dir/foo1”分配inode节点时,首先在根目录(“/”节点)的目录内容中查找并获取其直接父目录“/dir”的inode结构ID号(图4中假设为3),以该ID为索引键值重新遍历文件系统树并找到父目录图4元数据存储树形结构Rstore通过扩展哈希的方法支持大目录,当目录项数超过10k时,目录的内容中包含子文件的键值和文件名,以文件名为关键字产生哈希值,并通过二级间接块的方法存储,使用链表的方法处理哈希冲突.指向二级间接块的指针以类似inode-tail的机制存储在目录inode块的末尾,以指针的存储空间大小为2KB,每个指针为4字节计算,每个目录可以支持几亿个子文件或子目录的快速搜索.4测试与评估4.1测试介绍我们测试使用的MDS和客户端均采用双核IntelXeon1.60GHzCPU,2048MB内存,每台机器配置千兆以太网卡及Qlogic2432光纤卡;各个节点之间通过catalyst3750千兆以太网交换机相连,MDS及客户端通过silkworm3800FC交换机单个的inode块,然后使用进程ID进行hash得到其inode结构ID的第二部分(为18),并为该inode分配一个新块,以目录嵌入的方法存储在目录的内容中,当客户端对foo1文件进行写操作时,新分配的layout记入该inode的末尾,至此,foo1的inode以及layout与其父目录存放在连续的磁盘块中;当该进程再次在该目录中分配inode结构时(如图中foo2文件),由于其父目录ID与foo1一样,进程ID通过hash计算又产生相同的key值,该inode将被存储在foo1相邻的磁盘位置.对于来自不同进程但处于同一目录下的文件inode,由于它们的key值中父目录ID项相同,因此在磁盘上处于邻近位置,通过较少的磁头定位时间即可遍历.为新的子目录分配空间时,元数据存储为其进行持久预分配8个块用于将来子文件的inode存储.大小为146GB的磁盘组成的FC磁盘阵列相连.在测试之前,我们先对Redbud文件系统使用的磁盘进行了读写测试作为性能比较依据,在Linux-2.6.27内核基础上,采用标准的磁盘设备读写访问接口测得性能峰值为读为70.2MB/s,写为74.3MB/s.4.2数据块索引效率我们首先比较Redbud、EXT3和NFS[12]三种文件系统的性能,以验证Redbud采用extent的方法进行数据块索引的效率.在本实验中部署于NFS服务器端的文件系统为EXT3①,Redbud配置一个客户端访问文件.我们在前端节点上运行文件系统性能的专用测试工具IOzone②,文件读写访问的吞吐率结果如图5所示.①②Page8图5Redbud、EXT3、NFS读写文件的吞吐率我们首先测试了在配置一个硬盘的情况下,顺序读写和随机读写的性能,其中随机读写为512KB的大块读写.从图5可见,Redbud文件系统顺序写(seqwrite)和随机写(randomwrite)访问的吞吐率则分别比EXT3高出20%和14%,这是因为写过程中Redbud客户端采用layout缓存机制,根据文件的访问特征缓存了MDS预分配的layout,经过缓存查找获取PAG的磁盘写地址;EXT3则首先查找文件相应组中bitmap并从中分配磁盘块,即使其采用了固定8个块大小的预分配机制,但在写访问过程中仍然进行频繁的文件元数据更新.Redbud顺序读(seqread)和随机读(randomread)文件的吞吐率略高于本地文件系统EXT3,这是因为在该文件的创建过程中空间映射层为整个文件分配了连续空间,而读过程中Redbud客户端尽管需要与MDS交互获取文件映射信息,但是由于操作聚合的作用,一次交互即可获取并缓存文件所有layout;相比之下,EXT3在空闲空间分配过程中将大文件的不同部分分布到不同的组(group)中,因此增加了读访问的磁盘定位时间.这些测试结果显示,Redbud的访问性能达到了磁盘峰值的95%,证明Redbud文件系统采用extent的数据块索引方法能将物理设备的性能几乎完全提供给上层应用.从图6还可以看出Redbud读写文件的吞吐率比NFS高出65%和75%,这是因为NFS客户端频繁地与NFS服务器进行文件页面的获取和提交交互,如readpage、writepage、commit等操作,所有文件数据经过NFS服务器,最后由其将IO请求转发至本地文件系统,这些耗时的操作流程均限制了NFS的吞吐量和可扩展能力.4.3数据访问可扩展性我们通过测试系统规模增大的情况下Redbud文件系统聚合带宽的变化,以验证其数据访问的可扩展能力.首先在IO节点规模不变的情况下为Redbud增加客户端的个数,并进行文件的创建写(write-after-create)、覆盖写(overwrite)和读(read)操作,系统聚合带宽统计结果如图6所示.结果显示,在Redbud文件系统中,应用程序进行读写操作时,聚合吞吐量随着客户端节点个数的增加而线性地增长;其中创建写操作的性能略低于其他操作,这是因为尽管客户端与MDS的交互过程中采用了预分配方法减少客户端从MDS获取layout的操作次数,但空间映射层分配空闲空间时进行B+树的更新操作,这些元数据的更新先被写入共享日志区,然后响应客户端,而在其他操作中,客户端可以直接获取文件layout.图7是在客户端个数不变的情况下增加存储设备的系统聚合带宽测试结果,在该测试中我们将Redbud的测试目录配置为RF-striping和RF-Roundrobin两种性能策略,在RF-striping策略中,MDS在为该目录的子文件分配空间时,根据文件id和PAG号以fchunk(100MB)为单位分割文件,将每个文件的不同部分分布在不同的存储设备中;而RF-Roundrobin则表示MDS以文件为粒度,将整个文件分布在同一个PAG中,查找空闲PAG时采用Roundrobin的方法.从图7中可以看出,在采用两种不同的文件存储空间分配方法时,系统的吞吐Page9量均随IO节点的增加呈线性增长.相比图5的性能结果,在系统规模扩展能力测试中,性能的增长并不呈现成倍增长,这是因为多个客户端并行访问时增加了磁头的定位时间,带来了一定的性能损失.由于MDS集中管理各种元数据的状态,在客户端的访问过程中对layout进行检索、分发等处理,因此MDS的CPU负载程度直接影响着系统的可扩展性.接下来,我们检验在客户端节点增加的情况下各种不同的应用对MDS的访问负载的影响.图8(a)显示了运行IOzone时,MDS的CPU负载结果.在本实验中,每个客户端上运行5个IOzone测试程序对结果文件分别进行读写访问,以增加MDS的访问压力.我们可以观察到,MDS负载情况随着客户端的增加而增加,这是因为逐渐增多的文件layout访问请求导致MDS的任务增加.写操作的负载略高是因为MDS要进行extent的状态转换.为了提供更多的文件访问模式,接下来我们运行两个实际的IO密集型应用,高性能计算机的性能评估程序(NASParallelBenchmarks①和一个典型的建筑网格计算程序Abaqus②,运行过程中,两种测试程序均对结果文件进行随机和顺序访问.图8(b)显示了随着应用客户端节点扩展,MDS的CPU负载结果,在实验中,我们在每个客户端上独立地运行一个计算程序,其中1.00和0.05表示Abaqus两种具有不同精确度的计算用例,精度越小表示磁盘数据访问量越多.我们可以观察到,虽然节点的数目增加,但是MDS的CPU利用率控制在1.6%内,具有较强的可扩展能力.我们接下来为Redbud文件系统配置了多个客户端对不同文件和同一个文件的不同部分访问,每个被测试的文件大小为50GB,所有被访问的文件以RF-striping(fchunk=1MB)方式存放在10个IO节点中.图9显示了不同的并发访问模式下的测试结果.其中concurrent模式表示多个节点访问同一个文件,与当前许多并行程序一样[13],客户端将已经创建好的文件分为多个连续的区间,每个节点首先分别给其中一个区间加上Linux字节锁,然后再进行读写访问.为了增加并发访问压力,每隔1min各个客户端节点交换访问区间.exclusive模式则表示表示多个客户端节点访问不同的文件,节点之间不存在同一文件的并发访问.从图9中可以看出当并发访问的结点个数增长时,并发访问与独占访问的性能基本持平.少量的性能损失主要来自于不同客户端节点对同一磁盘访问时,磁头访问定位时间增长.同时我们能观察到在客户端节点增加的过程中,CPU的平均最高使用率略有增加,其原因是随着并发度的增加,MDS需要更新和跟踪layout的状态和使用情况,但是CPU的利用率仍然维持在非常小的范围内(2%).4.4元数据访问性能在Redbud中,与本地文件系统类似,文件布局组成每个用户可视文件的元数据文件,并以B树的方式检索.本实验中,我们对比采用本地文件系统存储元数据文件和我们专用的元数据存储在典型的layout工作负载下的访问性能.为了避免网络延迟①②Page10带来的干扰,我们首先将元数据存储的代码单独抽取出来进行测试.图10显示了元数据存储的测试结果,其中MFS(metafilesystem)表示RedbudMeta-PAG的访问结果.在本实验中,makefile工作负载递归地创建一个5000层的目录,每个目录里面有500个文件,并且所有的文件在创建之后进行缀后写操作(append-write);readlayout则首先对目录进行readdir操作,然后读刚才创建的文件内容.为了避免缓存的效果,我们在创建文件之后清空了缓存.在PostMark的测试中,文件个数设置为100k个,事务数目为500k次,文件的大小与每次事务大小一样.在makefile和readlayout的工作负载中,由于采用了嵌入目录机制,write-after-create和read-after-readdir操作一个小文件只会引入一次磁盘访问,因此在文件的大小不大于4KB时,MFS相比其他文件系统的访问性能提高出20%以上,而在文件大于4KB时,由于新的块也被分配在inode附近,因此与其他文件系统的性能相当.在PostMark测试中,由于Redbud采用扩展hash的方法支持大目录,因此在PostMark随机选择文件进行访问时,相比其他两个采用线性检索的文件系统,提高了总体的访问性能.随后我们通过计算测试用例的完成时间检验元数据操作的性能,我们在客户端上嵌套地创建1000个子目录,在每个子目录中创建100个文件,并分别进行readdir、stat操作,最后将不同操作的完成时间与导出ext3的NFS第四版本对比,其中在进行了创建操作之后首先清空文件系统缓存再进行后续的访问,以避免更新操作产生的缓存对读访问的影响.测试结果如图11所示,Redbud的子文件和目录创建完成时间是NFS的81%左右,这是因为文件的创建过程中文件系统首先需要查找(lookup)对应文件是否存在,即使NFS允许其客户端缓存所有目录项元数据,但由于其缓存的弱一致性,这类lookup操作必须被发送到NFS服务器,而在Redbud的文件创建过程中,客户端通过回调更新机制,客户端缓存了目录的所有子目录项,并具有修改权限,因此能确保目录内容与MDS一致,对新创建文件的lookup操作均在本地缓存中进行.尽管Redbud在其通信协议中聚合了readdir-stat操作对,但是从图11中我们发现其性能仍然较NFS高,这是因为NFS读目录的过程中其客户端与服务器的每次交互仅仅传输一个页面的数据,当目录较大时,为了取得所有目录项,NFS需要多次交互;而在Redbud协议中避免了这种问题,而且元数据存储将文件属性信息聚集在目录内容中,通过较少的磁盘定位时间即可将文件属性信息读入MDS缓存中.Redbud的stat操作完成时间为NFS的24%左右,这是因为Redbud的客户端在前述readdir操作获取了目录中子文件的属性信息之后,除了一些验证消息外,所有的getattr查找操作均在本地缓存中进行.如2.1节所述,Redbud的非对称结构分离了元数据和数据的存储,并因此避免大块的数据访问对元数据访问的干扰.我们接下来验证在有数据访问干扰时,Redbud的元数据访问性能.图12显示了Redbud和NFS的比较结果.在本实验中,客户端创建5000个递归的目录,每个目录里面有50个子文件,然后其他客户端进行utime和stat操作.在每次运行中,一半客户端上运行5个IOzone程序用于实现大块数据访问,其他的客户端进行元数据访问.从图中我们可以观察到Redbud的聚合元数据访问性能随着客户端的增加而增长.同时,虽然客户端在进行数据操作时与MDS交互进行大量的layout操作,但是Redbud通过减少元数据存储的磁盘定位和检索时间,比NFS的元数据访问性能提高了8%以上(10个节点的情况下获得的提高比例最少).图12中stat操作的性能结果也证明了由于元数据服务器的缓存避免了磁盘操作,因此极大地提高元数据访问性能.Page115小结本文提出了在基于共享磁盘的并行文件系统Redbud中分层的资源管理机制,正是由于采用非对称的结构,分离元数据和数据的存储,因此存储资源管理机制可以针对不同的存储需求进行优化,提供了高可扩展能力.资源管理机制将文件分布的语义和空间的管理分离在不同的级别,在语义感知层,它专注于文件的分布策略,开发元数据访问的局部性,并提供基于策略的文件级分布方法,满足当前大规模数据中心多种应用的需求.在空间映射层,Redbud专注于空间分配策略,为了提高资源的检索效率,采用不同的树型结构检索文件的空间分配、文件布局等.评价结果显示,这种资源管理机制能有效地适应各种访问模式,在客户端节点和存储节点增长时,性能同时获得扩展,并具有较高的灵活性.
