Page1基于非主属性值的实体匹配杨强1)1)(苏州大学计算机科学与技术学院江苏苏州215006)2)(昆士兰大学信息技术与电子工程学院布里斯班澳大利亚4067)摘要实体匹配旨在找出不同数据源中指代同一实体的实例.已有的实体匹配方法大都基于实体主属性值的相似度进行匹配,而很少有工作考虑到使用实体的非主属性值来辅助实体匹配.然而,当两条指代同一实体的主属性值差异较大的时候,这两个实体可能不会被认为是匹配的实体.另一方面,这两个实体很可能共享一些特别的非主属性值,而这些非主属性值恰好可以反映出两个实体的匹配关系.基于这种思想,文中提出了一种新颖的基于非主属性值的实体匹配算法.该算法以类似于决策树的结构为基础,通过使用这种结构,不仅可以解决噪声值和空缺值带来的问题,而且可以极大地提高发现匹配记录以及尽可能早地排除不匹配记录的效率.多个数据集上的实验结果表明我们的方法比现有的实体匹配方法具有更高的准确率和召回率.此外,使用我们提出的基于决策树的匹配算法等有关技术较Baseline匹配算法在匹配效率上高出10倍多.关键词实体匹配;非主属性;数据质量;性能;算法1引言随着信息时代数据量级的剧增,数据之间的不一致和冲突问题日益凸显[1].为了将不同来源的不一致数据进行融合,前人在“实体匹配”方面做了大量研究工作,旨在发现不同数据库中表示同一实体的实例[2].目前,大多数的实体匹配方法都是借助前缀过滤或Q-gram等方法通过度量各实例主属性值之间的字符串相似度,从而根据预定义的相似度阈值做出是否匹配的决策[3-4].然而,含义相同的主属性值形式上可能千差万别,含义不同的主属性值也可能表示同一实体.因此,没有任何一种相似度度量方法可以准确度量所有主属性值之间的相似度,且一个t1t2t3t4t5t6t7t8t9t10s1s2s3s4s5s6s7s8s9表1从天猫网站收集的在售手机信息512MB512MB512MB表2从中关村在线网站收集的在售手机信息2.0GB512MB512MB然而,使用非主属性进行实体匹配并非易事.与主属性相比,非主属性中会存在更多的噪声值,而且数据间的不一致问题更为严重.此外,通常情况下数据表中非主属性个数比主属性个数多,因此基于非统一给定的相似度阈值也会极大影响实体匹配的准确率和召回率.虽然有时匹配实体之间没有相似的主属性值,但是它们的部分非主属性值却可能很相似.例如,表1中的实体AscendM2和表2的实体Mate2,两者没有很高的主属性相似度,但是它们具有一些相同的非主属性值,如Huawei,Bar,Android4.3,且具有相同的发布时间2014.03,从中我们可以发现两者很有可能表示同一实体.换句话说,非主属性值也可以帮助我们识别实体之间的关系.基于上述观察,本文提出一种基于非主属性实现实体匹配的方法.我们的方法是对目前已有的基于主属性的实体匹配方法的补充.与仅使用主属性的方法相比,我们主要关注如何使用非主属性以提高实体匹配的准确率和召回率.RAM1GB1GB2GB1GB2GB2GB1GBRAM1GB—3GB1GB2GB1GB主属性的实体匹配中存在着严重的效率问题.使用非主属性进行实体匹配至少面临以下3个挑战:(1)属性选择.面对诸多的非主属性,如何评估每一个非主属性识别匹配实体的能力,从而做出较优选Page3择进行匹配;(2)噪声数据和丢失数据.非主属性中可能存在噪声数据和丢失数据.在实体匹配过程中噪声数据和丢失数据会影响匹配的效果,因此处理噪声数据和丢失数据也是一个很大的挑战;(3)效率问题.基于非主属性的实体匹配方法与只使用主属性的实体匹配方法相比前者时间开销要多出很多倍,因为前者需要对实例之间更多的属性值进行比较.针对上述三方面的挑战,我们提出了一种基于非主属性值的实体匹配方法.具体来说,我们首先考察并衡量每一个非主属性识别匹配实体的能力和过滤不匹配实体的能力.并基于考察结果以及非主属性之间的关联关系,我们动态建立一种特别的基于规则的概率决策树(ProbabilisticRule-basedDecisionTree,PRTree).通过该概率决策树,我们能够尽早发现匹配实例对或尽早排除不匹配实例对.与我们将要在文中描述的Baseline算法相比,基于PRTree的实体匹配方法极大地提高了实体匹配效率,且不易受到噪声数据和丢失数据的影响.此外,在文章的第3节,我们将对结合使用主属性和非主属性进行实体匹配的方法进行探讨.我们在本文中的贡献如下:(1)不同于以往基于主属性的实体匹配方法,我们创造性地提出使用非主属性进行实体匹配.(2)通过考察各非主属性对于实体匹配的相关特性及它们之间的关联关系,我们动态建立了一棵基于规则的概率决策树,并基于此树设计实现了高效准确的实体匹配算法.我们在3个数据集上对我们的算法进行了验证.结果表明我们提出的基于规则的概率决策树算法不仅能达到较好的匹配准确率和召回率,且节省了超过80%的匹配开销.2问题定义给定两个关系表T1={t1,t2,…,tn}和T2={s1,s2,…,sm},T1中的每一个实例ti∈T1(1in),T2中的每一个实例sj∈T2(1jm),实体匹配旨在发现存在于两个表间表示同一实体的实例对(ti,sj).以往的实体匹配方法主要是使用主属性值,依赖于一些字符串相似度函数,如编辑距离(Editdistance)以及一个相似度阈值决定两个表中的实体是否匹配.其形式化表示如下:设定A0是表T1和表T2的主属性,给定一个字符串相似度函数sim(·,·)以及一个相似度阈值θ,对于实例对(ti,sj),如果满足sim(ti[A0],sj[A0])θ,则ti,sj为同一实体,其中ti∈T1(1in),sj∈T2(1jm).然而,仅仅使用主属性值判定实例对是否匹配是不够的,因为主属性值通常具有多种表现形式,如LiHua和HuaLi实则表示同一个人.又或是主属性值直接无法使用,如丢失数据和噪声数据.本文中,我们使用非主属性值进行实体匹配.形式化表示如下,假定表T1和表T2具有相同的非主属性集合SNK={A1,A2,…,Ap},使用非主属性进行实体匹配的定义如下.定义1.非主属性的实体匹配(RMwithNon-KeyAttributes,NokeaRM).给定两个数据表T1={t1,t2,…,tn}和T2={s1,s2,…,sm},两者具有相同的非主属性集合SNK={A1,A2,…,Ap},NokeaRM问题旨在找到一个基于SNK的函数F(ti,sj)和一个相似度阈值τ,使得ti∈T1(1in),sj∈T2(1jm),(ti,sj)为指向同一实体的实例,当且仅当它们满足(1)F(ti,sj)τ;(2)sk∈T2,F(ti,sj)F(ti,sk).3使用非主属性进行实体匹配我们首先在3.1节展示一个Baseline算法,接下来在3.2节介绍基于规则的概率决策树的算法.3.1Baseline:基于DifScore的实体匹配算法Baseline算法主要使用非主属性区分某一实体不同于其他实体的能力进行实体匹配,这一能力被定义为属性的区分度得分(DifferentiationDegreeScore,difScore).属性的difScore大致上反应了该属性下属性值分布情况.例如,在没有重复记录的情形下数据表中所有的主属性值都是不同的,因此主属性具有最高的difScore也即是1.0.在表1中属性“Size”也具有较高的difScore,因为其值的分布比较多样,而属性“Type”则具有一个相对较低的difScore.其具体形式如下所示:在关系表T中,在所有的实例当中假定属性Ai(1ip)有distinct(Ai,T)个不同的属性值,则属性Ai的difScore计算方法为Page4其中:|T|表示关系表中实体的总数目;distinct(Ai,T)表示属性Ai下属性值不同的个数.在获得关系表中每个属性的difScore的基础上,我们计算实例t∈T1和s∈T2之间的相似度,其计算公式如式(2)所示:Fbaseline(t,s)=其中:t[A],s[A]分别表示实体t和s在属性A下的属性值;sim(t[A],s[A])表示属性值t[A],s[A]之间的相似度;SNK表示两表中共同的非主属性集合.然而通过实验发现,Baseline算法只能识别出一半左右的匹配实体(大约45%~55%的召回率),准确率也只达到80%左右.其原因在于:(1)difScore并不能完全反应属性的区分度,因为其并未考虑属性之间的组合关系;(2)丢失数据干扰实例对相似度的计算.更为重要的一方面是Baseline算法的复杂度为O(pnm),其中|T1|=n,|T2|=m,|SNK|=p.3.2基于规则的概率决策树算法我们提出了一个更为可靠的NokeaRM算法,该算法通过使用非主属性建立的基于规则的概率决策树进行实体匹配.众所周知,决策树是一种提高效率的方法,因为树中的每一个节点都有机会对输入的数据做出最终的决策.因而,通过使用决策树这种结构可以在尽可能短的时间内获取匹配实例对或者排除不匹配实例对.此外,我们提出了一种特殊的决策树可以克服Baseline算法的两个缺陷,既考虑到了属性之间的组合关系,又不易受缺失值的影响.接下来,我们首先给出该算法的基本思路,然后介绍如何建立基于规则的概率决策树,最后我们将展示基于此决策树的NokeaRM算法.3.2.1算法基本思路首先,给定的实体对ti∈T1,sj∈T2(1in,1jm)匹配与否是由其各个属性下的值匹配与否所决定的.然而,鉴于不同的属性在唯一确定实体的能力上有所不同,我们选择一组最具有决策能力的属性集进行匹配;然而,为了尽可能地减少决策的开销,我们期望尽早发现匹配实体对或尽早排除不匹配实体对,因此我们决定使用类似于决策树的结构进行实体匹配,即将各个属性放入决策树中对实例对是否匹配进行逐层联合判定;此外,属性集中的各个属性在决策实体匹配时并不是相互独立的而是相互依赖共同作用的,因而建树过程以及通过树做匹配计算时,还需要考虑各属性间的相互依赖度.如图1所示,实体对的匹配结果R是由属性集S={A1,A2,…,A5}决定的,以路线1为例,A2的结果是在A1的条件下得到的,而A4的结果是在A2的条件下得到的,以此类推,最终的结果是由以上各步的结果综合作用得到的.实体ti与sj其匹配结果可形式化表示为另外,我们从属性的角度上考虑匹配结果,用Mk代表实体所在属性组合下的第k个属性对于R的一个贡献,如果两者值相等则是一个正的贡献,否则是一个负的贡献.具体如下:Mk=对于关系表中的非主属性的集合SNK={A1,A2,…,Ap},实体间的匹配结果可通过条件概率进行量化,其概率可通过贝叶斯条件概率进行评估,其公式如式(5)所示:p(SNK)=Pr{R=1|SNK}=Pr{SNK|R=1}Pr{R=1}+Pr{SNK|R=0}Pr{R=0}3.2.2建立基于规则的概率决策树基于上一小节的分析,我们考虑使用非主属性建立基于规则的概率决策树(ProbabilisticRule-baseddecisionTree,PRTree).PRTree的理论依据如下:(1)决策树有利于减少实体匹配的时间开销;(2)贝叶斯概率决策树有助于反应具有一定决策能力的属性之间的依赖关系;(3)不同的属性具有不同的决策能力,具体表现为属性的两个重要性质,即决策匹配的充分性(Sufficiency)和决策匹配的必要性(Necessity),其定义如下.定义2.决策匹配的充分性和必要性(Suffi-ciencyandNecessity).对于给定的两个关系表T1=Page5{t1,t2,…,tn}和T2={s1,s2,…,sm},假设两者具有相同的属性集合S,属性Ak∈S(1kp)的充分性概率是指当满足ti[Ak]=sj[Ak]时,实体ti∈T1(1in)和sj∈T2(1jm)为同一实体的概率,而属性Ak的必要性概率是指当ti和sj为同一实体时,也即当ti=sj时,ti[Ak]=sj[Ak]的概率.属性的充分性特性和必要性特性在决定PRTree的结构上起着重要作用.其中属性的充分性反应了属性识别匹配实体的能力,即当属性的值相同时,能够在多大概率上反应其所对应的实体为同一实体;而属性的必要性反应了属性排除不匹配实体的能力,即当实例对表示为同一实体时,其在某一属性上也具备相同属性值的概率.根据定义2,属性的充分性概率和必要性概率可通过训练得到,其中训练集中已知两个数据表中匹配的实例对,其定义为(犜狉1,犜狉2).假定训练集中的数据分布接近于实体匹配的两个真实数据表,则我们可以得到其计算公式如式(6)和(7):suf(Ak)=∑{(ti(B(ti=sj|ti[A]=sj[A])+B(ti≠sj|ti[A]=sj[A]))nec(Ak)=∑{(ti(B(ti[A]=sj[A]|ti=sj)+B(ti[A]≠sj[A]|ti=sj))其中,B(bool1|bool2)为布尔函数,其表示如式(8):B(bool1|bool2)=1,如果bool2=TRUE并且bool1=TRUE烄1,如果bool2=TRUE并且bool1=FALSE烅0,如果bool烆其中,犜狉c是一个在建树不同阶段中动态改变的限制集,初始化为犜狉c={(ti,sj)|ti∈犜狉1,sj∈犜狉2}.下面我们讨论如何建立PRTree决策树.鉴于实例对间的匹配度计算需要考虑属性间的相互依赖关系,我们并不是一次找出top-k个具有较大difScore的属性元素作为概率决策树上的属性节点,而是采用贪心算法,每次只在上一步的决策结果的条件之下计算出未使用属性的条件充分性概率和必要性概率,从而选择出当前条件最具有决策力的属性放入树中的对应位置.此建树过程将迭代进行直到达到一定的停止条件.基于上述思路,接下来我们将从两方面介绍如何建立PRTree:其一为属性的选择;其二为停止条件.(1)属性选择.根据当前待选节点的类别(包括根节点、非叶节点和叶节点),在已经确定的候选节点集的基础上,每次选择具有最大充分性或必要性的节点作为当前节点,并加入到候选节点集中.通过每次选择具有最大充分性概率或最大必要性概率的属性,可以尽可能早地发现匹配实体,并且排除不匹配实体.具有较大充分性概率的属性说明其识别实例对匹配的能力较强;具有较大必要性概率的属性表明其在排除实例对为不匹配实体的能力较强.(2)停止条件.当所有待考察属性在当前位置的条件充分性概率或条件必要性概率都低于某一值(τstop)时,此时应停止创建更多的节点,将上一次刚加入候选节点集的属性作为叶子节点.我们在算法1中给出了建立PRTree的流程,具体描述如下:(1)根节点(RootNode).在计算出每个属性的充分性和必要性后,我们选择充分性和必要性得分最大的属性作为根节点.而根节点有3个分支,分别是匹配(Matched(Y)),不匹配(Unmatched(N))和无效(Invalid(Null)).如果根节点是具有最高充分性得分的属性,则节点的匹配分支是叶节点并将以一定的置信度输出“Matched”,而不匹配分支和无效分支则是非叶节点.相反地,如果根节点为最大的必要性得分的属性,节点的不匹配分支则成为叶节点并以一定的置信度输出“Unmatched”,而匹配分支和无效分支都应是非叶节点.根节点的计算如式(9)所示:(2)非叶节点(Non-LeafNode).PRTree中的非叶节点代表了该节点的充分性概率或必要性概率不能直接判断出匹配的结果,需要结合其他属性,直至能够做出判断,也即是到达PRTree的叶节点.我们以目前已确定的候选节点集为基础,当计算余下的其他属性的充分性得分和必要性得分时,必须在满足候选节点集中所有元素的条件下进行.为了构建PRTree的非叶节点,我们在目前已有的所有祖先节点集的条件下,计算余下属性的充分性和必要性,选择具有最大条件充分性或条件必要性得分的属性作为当前节点.具体地,属性的条件充分性和条件必要性可通过式(6)和(7)并将犜狉c改变为一个更小的训练集结Page6合计算而得到,其中所有的实例都应满足祖先节点的条件.非叶节点的计算方法如式(10)所示:Pnonleaf=PA|Candit(PRTree)其中Candit(PRTree)为已选中的祖先节点构成的集合.(3)叶节点(LeafNode).PRTree的叶节点表明了从根节点到当前节点构成的属性集合可以判断给定的实例对的匹配结果或者目前已有的数据无法给出较为明确的匹配结果.PRTree的每个叶节点都会输出“Matched”或“Unmatched”,通过输出结果可以判断实例对是否匹配.当属性的suf(A)和nec(A)一直很低的时候,也即低于一个合理的阈值τstop时,我们将停止获取更多的属性用以创建PRTree的节点.然后,我们会创建一个“Exit(NotDecided)”节点作为最后的节点,其表明我们无法判断实例对是否匹配.叶节点的计算如式(11)所示:Pleaf=A∈(SNK-Candit(PRTree)),suf(A)≈τstop根据算法1,通过对表1和表2所在的大量训练数据的训练,我们建立了基于非主属性的PRTree,如图2(a)所示.在接下来的部分中,我们将介绍如何使用PRTree进行实体匹配.算法1.建立PRTree.输入:具有属性集S的训练表犜狉1和犜狉2,充分性阈值输出:PRTree的根节点的引用rootWHILES≠DOFORALLA∈SDO根据式(6)和(7)计算或更新suf(A|AncsCond)和IFA.suf<τsTHENA.suf=0;ELSEIFA.nec<τnTHENA.nec=0;ENDIFENDIFENDFOR在所有的非主属性中将具有最大值的suf(A|AncsCond)cur.conf=Max(suf(A|AncsCond),nec(A|AncsCond));IFcur.conf<τstopTHENcur=“Exit(NotDecided)”BreaksENDIFcur.NullChild=BuildPRTree(犜狉1,犜狉2,S=S-{A},IFcur.conf=suf(A|AncsCond)THENcur.YChild=“Matched”AncsCond=AncdCond+{(A,unmatched,cur.suf)}cur=cur.NChildELSEcur.NChild=“Unmatched”AncsCond=AncdCond+{(A,matched,cur.nec)}cur=cur.YChildENDIFENDWHILERERUTNroot3.2.3基于PRTree的NokeaRM算法通过在训练集上不断地筛选符合上述要求的属性,创建了如图2(a)所示的PRTree.我们在算法2中给出如何借助PRTree执行NokeaRM算法.对于测试集数据表中的每一个实例对(t,s),从PRTree的根节点开始访问,并在到达叶子节点时停止访问.每次当实例对(t,s)到达属性Ak这个节点时,我们检查实例对在当前节点是否具有相同的属性值.如果相同,则访问当前节点的“Matched”子节点,否则访问当前节点的“Unmatched”子节点.但是,如果实例对在当前节点的属性值为空时,则应访问该PRTree的“Invalid”子节点,表明待比较的实例对在该属性下出现缺失值.当实例对访问叶节点时,我们会根据叶节点的性质从而决定输出“Matched”或者“Unmatched”.决策的置信度是由实例对所经过的从根节点到当前节点组成的所有节点共同决定的,Page7除根节点外,每一个节点结果的计算并不是独立于其他节点而得到的,而是在满足该节点所经历祖先节点的条件下所得.此外,PRTree中处于不同高度的节点,在计算置信度时应给予不同的权重分配,因为越是靠近根节点的属性,其受其他属性的影响越小,而越是远离根节点的属性,其受其他属性的影响越大.因而,我们采取了一定的手段保证属性权重的合理分配.综合以上的考虑我们采用如式(12)计算实例对之间的相似度FPRTree(t,s)=∏1iH(A)其中,H(A)代表当前节点在PRTree中的高度.我们使用(·)1/i调整第i个节点的影响,因为越接近当前节点的节点,对当前节点所做出的决策影响越大.当节点所处的层次越低,其削弱的力度越大;当节点所处的层次越高,其削弱的力度越小.因为高层次的节点是在低层次节点的基础上建立的,也即高层次的节点依赖于低层次的节点,其对整棵树的影响较低层次的节点影响小,故应给予较低的削弱力度;对于高层次的节点其原理类似.我们使用Conf(Ai)表示沿着路径节点Ai的置信度,其计算方法如式(13)所示:Conf(Ai)=suf(Ai),如果Ai为“Matched”节点烄1-nec(Ai),如果Ai为“Unmatched”节点烅1,烆算法2.使用PRTree实现实例对的NokeaRM算法.输入:PRTree的根节点root,实例对(t,s)输出:实例对(t,s)的相似度:FPRTree(t,s)cur=root;level=1;sim=1;WHILETRUEDOIFcur.output=“Exit”THENRETURN-1ENDIFIFcurisaleafNodeTHENRETURNsimENDIFIFt[cur]=NULLORs[cur]=NULLTHENcur=cur.NullChildELSEsim=sim×(cur.conf×sim(t[cur],s[cur]))1/iENDIFIFt[cur]=s[cur]THENcur=cur.YChildELSEcur=cur.NChildENDIFlevel++ENDWHILERETURNsim通过上述的步骤结合式(6)~(13)可以计算出给定实例对的相似性,并通过与相似度阈值的比较判断给定的实例对是否表示为同一实体.例1.我们以表1中实体t1和表2中的实体s1为例介绍如何使用基于PRTree的NokeaRM进行实体的匹配.(1)Step1.我们在训练集中根据式(6)~(13)通过不断地迭代运算找出每一次迭代过程中符合要求的候选属性.从图中可以看出Manufacturer具有较大的必要性概率,也即对于手机而言当Manufacturer不同时,两个手机实体为同一实体的概率基本为0;又如当Manufacturer和Release一样时,OS具有较大的充分性概率.如果OS一样,则可判定这两个手机实体为同一实体,如果OS不一样则需要从OS的“N”分支继续寻找符合要求的其他属性.最终,我们得到了如图2(a)所示的PRTree.(2)Step2.在Step1完成之后,根据Step1计算所得结果,我们得到了图2(a)中节点的Conf值,如表3所示.NodeConfidence(3)Step3.对于给定的表1中的实体t1和表2中的实体s1,我们根据算法2中介绍如何使用图2(a)中的PRTree进行实体的匹配,设定属性值的编辑距离相似度阈值为0.7,实体相似度阈值为0.55.我们首先从根节点“Manufacturer”开始遍历,由于sim(t1[Manuf.],s1[Manuf.])=1>0.7,所以我们遍历“Manufacturer”的“Matched”子节点“Release”;因为sim(t1[Release],s1[Release])=0.86>0.7,所以我们遍历“Release”的子节点“OS”;接下来我们继续计算sim(t1[OS],s1[OS])=0.86>0.7,所以我们到达“OS”的“YES”节点也即是叶子节点;最终,通过式(12)结合表3中各节点的Conf值,得到表1中的实体t1和表2中的实体s1的相似度FPRTree(t1,s1)=1×0.940.55,所以表1中的实体t1和表2中的实体s1两者是匹配的,也即t1和s1为同一实体.Page83.2.4基于PRTree的NokeaRM算法复杂度分析基于PRTree的NokeaRM算法较Baseline算法而言极大地减少了时间上的开销,具体体现在实例对之间属性的比较次数的减少,因为前者使用尽可能少的属性做出最终的决策,而后者使用全部的属性才能做出决策.算法2的时间复杂度为O(qnm),其中q是在匹配过程中每个实例对所到达的平均高度.注意到qp,因为我们可以排除一大部分不匹配的实例对,并在尽可能短的时间内找到匹配的实例对.鉴于主属性在实体识别上的功效(唯一标识某一实体),其能够在一定程度上辅助非主属性进行实体匹配,因而我们在本小节讨论如何将基于PRTree的NokeaRM算法与基于主属性的实体匹配方法(keyRM)相结合.一种可能的方式就是分别从keyRM和NokeaRM中获取实例对的相似度权重之和,但是此方式的问题是没有一个合适的权重能够满足所有的情形.另外一种方式是先执行KeyRM或者NokeaRM并限定一个严格的准确率阈值,接下来执行另外一种RM算法用以提高召回率.然而,通过我们的观察发现,这种方式也不能达到一个较高的准确率和召回率.最终,我们发现在建立PRTree的过程中使主属性参与进去,这种结合方式可以达到最好的性能.如图2(b)中所示,新的PRTree将主属性“Product”作为其一个节点存在.我们在接下来的实验部分对这种结合方式的效果加以评估.4实验部分我们在两个真实数据集和一个合成数据集上验证了我们的方法.手机数据集(Mobile).我们分别从天猫网(Tmall)和太平洋电脑网(PConline)搜集了在售的手机信息.Tmall数据表中含有4k个元组,53个属性,PConline数据表中含有5.6k个元组,46个属性.这两个数据表共有的实例个数为3.8k,共有的属性个数为38,其中共有的属性如ReleaseDate,OperationSystem,RAM,ScreenSize,Type等.相机数据集(Camera).我们分别从天极网(Yesky)和太平洋电脑网(PConline)搜集了在售的数码相机的信息.Yesky数据表中含有2.5k个元组,50个属性,PConline数据表中含有3.4k个元组,44个属性.这两个数据表共有的实例个数为2.5k,共有的属性个数为31,其中共有的属性如Type,Pixels,Panel,Wifi,Manufacturer等.仿真数据集(Synthetic).我们生成了两个仿真数据表,具有100k个元组,60个共有属性.我们使用了一定的规则使得数据的分布近似于真实数据集.例如,匹配实体的属性值相似度统一地分布在0和1之间.注意到真实数据集上有一些缺失的属性值,我们在生成合成数据集时对非主属性也随机产生一定数目的缺失值.我们大致上使用3个度量标准评估方法的效果.准确率(Precision):所有匹配的实体中正确匹配的实例所占的比例;召回率(Recall):所有应该匹配的实体中正确匹配的实体所占的比例;F1Score:对准确率和召回率的综合考虑,计算方法如下所示:2×precision×recallF1=销评估方法的效率.4.1参数设定在我们的算法当中有一些重要的参数需要设定,这些参数共同影响着算法的效果和效率.在评估我们提出的方法之前,先对使用到的参数加以说明:(1)创建PRTree的阈值(ThresholdsforBuildingtheTree):图3展示了F1Score随着两个阈值τs和τn在0和1之间变动时发生的改变.通过观察发现,当τs=0.8,τn=1时,F1Score达到最大值.同时我们也使用一个阈值τstop控制树的高度以及质量,观察发现当τstop≈0.6时,树的效果达到最好.注意到在图3(a)中由于τs的值太高,一些比较重要的属性被排除而不能用于创建PRTree,导致了F1Score的急剧下降;(2)字符串相似度阈值(ThresholdforStringSimilarity):通过对3个数据集的实验发现最佳的字符串相似度阈值取值大约为0.7;(3)匹配实例的阈值(ThresholdforMatchingInstances):我们也需要一个用于判定实例对匹配结果的阈值τmatched.通过对3个数据集上的实验发现τmatched=0.55为一个比较合适的取值.Page94.2和以往方法的比较我们将本文中提出的实体匹配方法(Baseline算法和PRTree(Nokey)-based算法)与现有的多种技术合成的最先进的Key-based实体匹配方法以及最先进的使用非主属性进行实体匹配的方法Matchingtree-based[5]比较各自的准确率和召回率.在我们算法中使用的字符串相似度函数统一选择Editdistance.(1)Key-based.该方法合成了在多篇文章中提及的几种基于键值匹配的最新技术.我们首先创建基于Q-gram的后缀数组,并将可能匹配的实体根据已知的可靠匹配实体中提取的“分块主键”对实体进行分块操作[6],此外,我们融入了prefix-basedfiltering术进行块内实体间的相似性计算,此方法提升了实体匹配的效率.(2)Matchingtree-based.基于非主属性的匹配算法目前唯有Dey等人在文献[5]中提出的另外一种基于0-1分支决策树的匹配树算法Matchingtree-based.简而言之,此方法在筛选匹配树的节点时,根据属性的匹配概率选择具有最大熵的属性,只是简单地根据属性值是否一致将其分为0-1分支从而建立matchingtree.我们除了将上述两种方法与本文中提出的Baseline方法和无主属性的基于PRTree的NokeaRM方法作比较之外,也考虑了将基于PRTree的方法和Key-based的方法结合的方法,通过在3个数据集上的实验结果发现将主属性也用于创建PRTree时匹配效果达到最优.因此,我们也使用了PRTree(Key)-based参与比较.首先,我们将上述5种实体匹配方法在准确率图4在准确率和召回率方面与以往方法的对比图5在准确率和召回率方面与Matchingtree-based方法的对比和召回率方面做出评估,实验结果如图4所示,上述5种方法随着召回率地不断增大,其准确率整体上呈下降趋势.不同的方法在下降的平稳程度上有所不同.我们发现Key-based方法与我们提出的方法相比在准确率和召回率方面具有最差的效果,而Baseline方法效果优于Key-based方法,低于Matchingtree-based方法,而Matchingtree-based方法较PRTree方法相比效果略差,使用了主属性和非主属性的PRTree方法在准确率和召回率方面达到最优的效果.没有使用主属性的PRTree方法也达到一个比较好的效果,之所以这种方法比使用主属性的PRTree方法效果略差是因为缺少了主属性的提供的信息.我们对上述各种方法所达到的效果做出如下分析:Key-based方法整体上波动较大,因为此方法易受数据的影响,当数据在表达方式上相差较大时,其准确率和召回率会受到极大的影响,导致本应该匹配的实体无法正确的匹配.如图4(b)所示,当Key-based方法的召回率超过0.75后,其准确率急剧下降.Baseline方法在稳定性上优于key-based方法,因为Baseline方法使用了非主属性,受数据表达方式的影响较小.如图5所示,Matchingtree-based方法和基于PRTree的方法的准确率随召回率稳定变化,但Matchingtree-based方法的变化率比使用主属性的PRTree方法大.因为一方面在数据集中有缺失值的存在,Matchingtree-based方法易受到缺失值的影响,而使用主属性的PRTree方法考虑了缺失值的情况;另一方面,Matchingtree-based方法忽略了不同层次的节点对匹配结果的影响,而使用主属性的PRTree方法考虑了不同层次节点的影响.使用主属性的PRTree方法效果优于不使用属性的PRTree方法,原因在于使用主属性Page10的PRTree方法通过使用主属性获取了匹配所需的重要信息,在一定程度上较少了影响的力度.大体上,在3个数据集上的实验结果表明,我们的方法在准确率方面提升了近15%,召回率方面提升了20%左右.其次,我们对上述5种方法在效率方面做出了对比,比较了这些方法在3个数据集上各自的时间开销.如图6所示,Key-based方法的时间开销最少,因为此方法只使用了主属性进行实体匹配,而且使用了block、prefixfiltering和invertedindices等技术减少匹配的开销;Baseline方法的平均时间开销是Key-based方法的30倍左右,因为Baseline方图6在效率方面与以往方法的对比图7在效率方面与Matchingtree-based方法的对比4.3可扩展性评估如图8所示,我们在仿真数据集上对PRTree方法的可扩展性做出了评估.从图8(a)中可以看出,随着记录数目从100变化至10W,其时间开销以一种近似于指数增长的方式缓慢地增加,也从另一方面证明了PRTree可以高效地减少比较时间.图8不同数目的记录数和属性数对PRTree在效法使用了所有的属性进行匹配,而基于PRTree的方法其时间开销比Baseline算法少10倍多,因为基于PRTree的方法筛选了充分性或必要性概率最大属性,并结合合理的停止建树阈值剔除一些影响因子小的属性,这样能够尽可能早的做出决策,极大地减少属性值的比较次数,从而提高匹配效率.从图7中可以看出,基于PRTree的方法在效率上与Matchingtree-based方法相差不大,但在平均效率上看,Matchingtree-based方法稍低一些,主要因为Matchingtree-based方法在创建matchingtree时在属性的选择上开销较大,特别是在假阳性错误出现的时候.从图8(b)中得出,随着属性数目从10增加至60,其时间开销以近似于线性方式增加.5相关工作实体匹配近年来一直被广泛地研究[9-11],其应用领域涉及到医疗卫生[12-13]、国际安全[14]、信息检索[15-16]、商业数据管理[17]等.之前各种字符串相似性度量方法被提出,如基于字符的算法(如editdistance[18]、Jarodistance[9]、Q-gram[19])和基于符号的算法(如atomicstring些混合的方法被提出[22-23],这些方法或是直接用于实体匹配或是间接辅助进行实体匹配,有的方法注重于匹配效率的提升,如Q-gram算法,而有的方法注重于提高匹配的质量,如WHIRL.此外,文献[24]提出了两种可学习的文本字符串相似度度量方法,包括可学习的字符串编辑距离算法和使用SVM的向量空间度量算法,针对数据表不同的字段使用不Page11同的方法,在实体匹配过程中该方法具有较好匹配准确性.近几十年来,一些基于分类[25]和语义网络[26]的方法也有被提出,然而存在灵活性差,时间开销大等问题.实体匹配的一个重大问题是算法的时间复杂度.为了减少实例对的比较次数,近年来有许多高效的方法被提出.例如,一些工作将Q-Gram算法和倒排索引算法(InvertedIndices)[27]结合,通过这两种方法可以提高匹配效率,以尽可能少的时间开销为目的进行实体匹配,但此类方法的适用性存在一定的问题,如当参与匹配的实体存在缺失值,此类方法将不再适用.通过使用一些提高效率的算法利用主属性进行实体匹配的方法[6-8]在进行实体匹配过程中易受数据的影响,如缺失值、错误值等,而且在数据表达方式不同时其匹配的结果也会受到严重的影响.此外,还有一些基于前缀修剪(prefix-basedpruning)的方法[28]、基于top-kmode[29]的方法以及基于批量匹配(batch-based)的方法[30],通过这些方法过滤那些“明显”不匹配的实例对,减少匹配过程中的比较次数.文献[31]提出了一种使用正例和反例的方式结合相似度连接(similarityjoin)和结合(similarityunions)以及用户指定的相似度函数构建SJU操作树,查询某种适用于待匹配实体属性的相似性度量算法,达到每个属性以最合适的相似性算法进行匹配的目的,该方法不仅适用于小中型数据,而且在大型数据库上也适用.此外,也有一些方法[32]将属性值映射到多维欧几里得空间,根据给定的合并规则结合属性的相似性选择一个属性的集合,应用多维相似性连接用以判断实例对的相似性.文献[7]中提出了一种使用非主属性进行实体匹配的方法,该方法通过利用非主属性创建一棵类似于决策树的Matchingtree进行匹配.该文提出的Matchingtree是一种0-1结构的二分树,通过一定的概率计算方式选择属性作为Matchingtree的节点,然后从根节点到叶节点遍历进行匹配.但是这种方法存在适用性问题,当属性值出现缺失时,匹配过程便无法进行.而本文提出的PRTree却能很好地处理该问题,当遍历时出现缺失值PRTree能够产生独立的分支,即属性值为空时遍历该属性下的其它节点,进行后续的匹配过程.此外,Matchingtree-based方法没有考虑Matchingtree中不同层次的属性对匹配结果的影响度,粗糙地将靠近根节点的属性给予匹配结果较大的影响,将靠近叶子节点的属性给予匹配结果较小的影响,而实际上应该根据节点所在的层次给予合适的权重分配,而基于PRTree的方法使用了平衡函数合理地分配权重.6总结以及未来的工作在本文中,我们研究了基于非主属性的实体匹配问题即NokeaRM问题.我们首先提出了一种使用所有属性进行实体匹配的Baseline方法,该方法使用属性区分某一实体不同于其他实体的能力,综合考虑所有属性值的相似度对实例对进行实体匹配计算.然而Baseline方法因为要比较很多的非主属性值对因而效率很差.因此我们又提出了一个高效的基于概率决策树的NokeaRM方法.其思想为在实体匹配的过程中,能够尽可能早地发现匹配的实体以及尽可能早地排除不匹配的实体,从而减少实体之间比较的时间开销,大大提高了基于众多非主属性值进行实体匹配的效率.通过与现有的实体匹配方法相比,可以看出我们的方法达到了更高的准确率和召回率.此外,我们扩展了基于非主属性的PRTree方法,将主属性也用于概率决策树的创建之中,进一步改善了实体匹配的精确度.基于PRTree的方法也有需要改善的地方.在未来的工作中,我们会考虑如何更好地解决错误属性值对匹配的影响.我们可能考虑将数据纠错和实体匹配相结合,实现两者之间的交互,使两者相互促进.更多匹配的实体有利于纠正更多的错误值,而更多错误值的纠正有利于发现更多匹配的实体.此外,我们还考虑将众包技术(Crowdsourcing)融入到我们的方法之中,将一些机器无法做出的判断交给众包平台,从而进一步提升匹配精准度.
