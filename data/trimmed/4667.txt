Page1协同过滤推荐系统中的用户博弈1)(清华大学电子工程系北京100084)2)(中国传媒大学信息工程学院北京100024)摘要在以协同过滤算法为核心的推荐系统中,一个用户能否获得高质量的推荐不仅取决于用户自身是否积极地参与项目评分,还取决于其他用户是否能提供足够多的评分.由于对项目评分是需要付出成本的,理性的用户总是希望以尽可能少的评分换取高质量的推荐.该文用博弈论的方法对协同过滤系统中的用户评分行为进行分析.考虑到一个用户通常无法观察到其他用户的评分和得到的推荐,该文将用户间的交互建模为不完全信息博弈,并引入“满足均衡”的概念来分析该博弈.该文假定每个用户都对推荐质量有一个预期,当所有用户的预期都得到满足时,博弈即达到均衡.针对所建立的博弈模型,该文设计了一种均衡学习算法,该算法允许用户以逐渐增加评分数量的方式来寻找均衡策略.理论分析和仿真结果均表明,当所有用户对推荐质量有着相似的预期时,所提算法可收敛到满足均衡.这一分析结果可以为协作式系统中激励机制的设计提供启发.关键词协同过滤;博弈论;满足均衡;均衡学习;收敛条件;社交网络;社会媒体1引言作为一种有效的信息过滤手段,个性化推荐已广泛应用于电子商务、视频分享、在线社交等领域.协同过滤(collaborativefiltering)是目前推荐系统最常用的一类算法[1],其基本思想是分析大量用户对项目的“评分”以发现用户偏好的相似性,然后向用户推荐符合其偏好的项目.这种推荐算法能否取得良好效果很大程度上取决于用户是否积极地参与项目评分.用户对项目的评分主要基于用户自身的体验.由于给项目评分是需要付出“成本”的———评分过程会占用一些时间、评分数据可能泄露用户的隐私,用户通常不会对其体验过的所有项目都进行评分.若用户出于成本考虑只提供很少的评分数据,则推荐服务器就无法准确计算用户之间的相似性,进而无法为用户提供高质量的推荐.为了鼓励用户参与评分,推荐服务器可向用户提供奖金、积分等形式的激励.目前在协同过滤的研究中对激励机制的讨论还比较少见,但在P2P资源共享[2-3]、参与感知[4-6]、众包[7]等类似的问题场景中已有较多关于激励机制的研究.在个性化推荐系统中,除了奖金等形式的外部激励,推荐结果本身即可视为一种促使用户对项目评分的内部激励———用户若要获得准确的推荐,则有必要向推荐服务器提供足够的评分数据以清楚地表达自己的偏好.在没有外部激励的情况下,用户可否自发地提供足够多的评分数据以使推荐服务器产生令人满意的推荐?如果可以,用户在选择项目进行评分时应如何决策?本文拟通过研究用户之间的“博弈”回答上述问题.如前所述,用户对项目评分是有成本的.用户提供的评分越多,付出的成本就越高.用户在决定是否对一个项目进行评分时,需仔细权衡评分成本和推荐质量.此外,推荐服务器采用“协同”过滤算法计算推荐,这意味着一个用户获得的推荐不仅与用户自己提供的评分有关,还与其他用户提供的评分有关.换言之,不同用户的评分行为是相互影响的.若假定用户是理性的,即每个用户都希望以尽可能低的评分成本获得满意的推荐,那么可以认为推荐系统中的各用户是以推荐服务器为媒介进行着某种形式的博弈,因此本文考虑用博弈论[8]的方法对用户的评分行为进行分析.目前在协同过滤的相关研究中,博弈论的应用并不多见.Halkidi等人在文献[9]中用博弈论的方法分析了协同过滤推荐系统中的隐私保护问题.在他们研究的博弈模型中,用户向推荐服务器提供的评分向量被视为用户的策略.为避免评分数据泄露隐私,用户可向推荐服务器提供虚假评分,但这会降低推荐准确率.用户面临的问题是如何修改评分才能在不明显降低推荐准确率的前提下最大限度地保留隐私.为寻找博弈的均衡[8],Halkidi等人将用户与推荐服务器之间的交互建模为一个迭代过程.在每一轮迭代中,每个用户可利用其他用户在前一轮迭代提供的评分来确定自己当前的最优策略.但在实际的推荐系统中,用户通常不会反复更新自己的评分,并且用户也无法全面准确地获得其他用户的评分信息,因而上述博弈分析的实际意义有待商榷.考虑到在实际的推荐系统中,一个用户既无法完整的观察到其他用户的评分,也无法了解推荐服务器为其他用户提供了怎样的推荐,因此,与Halkidi等人的研究不同,本文将用户间的交互建模为一种不完全信息博弈,并应用“满足均衡”(satisfactionequilibrium)的概念来分析该博弈的均衡.满足均衡描述的是这样一种状态:博弈中所有参与者的个体约束条件都得到满足.Ross和Chaib-draa[10]最先提出满足均衡这一概念,用于解决不完全信息博弈的纳什均衡(Nashequilibrium)难以分析的问题.为将满足均衡应用于协同过滤系统,本文假定每个用户都对推荐结果的质量有一个预期,当实际的推荐质量高于预期时,用户即感到“满足”.不同用户的预期一般是不同的.当所有用户的预期都得到满足时,用户间的博弈达到满足均衡.基于所建立的博弈模型,本文提出了一种均衡学习算法.所提算法参考了perlaza等人[11-14]在研究满足均衡学习算法时提出的基于迭代的行动规则,但现有研究主要是针对无线通信网络中QoS(QualityofService)配置问题的,与本文研究的协同过滤问题不同,因而现有的学习算法不能直接用于本文构建的博弈模型.结合推荐服务器的工作原理,本文定义了如下的行动规则:在每一轮迭代中,用户根据当前所得推荐是否达到预期以决定是否选择新的项目进行评分.换言之,本文设计的均衡学习算法是让用户以逐渐增加评分数量的方式来寻找均衡策略.在适当的简化假设下,本文对所提算法的收敛性进行了理论分析,并通过一系列仿真实验对理论分析的结果进行了验证.本文第2节介绍本文所提的博弈模型并给出满足均衡的描述;第3节介绍均衡学习算法并给出算Page3法收敛性的理论分析;第4节介绍仿真实验的设计并对仿真结果进行分析;第5节给出结论.2博弈模型2.1场景描述在一个推荐系统中,用户集合为N},项目集合为S={s1,s2,…,sM}.用Si表示用户i体验过的项目的集合,用珟Si表示用户i给出了评分的项目集合,则有珟SiSiS.用户i对珟Si中的项目评分,相当于向推荐服务器提供了评分向量狉i=(ri1,ri2,…,riM).本文规定:对任意j∈{1,2,…,M},若sj∈珟Si,则0<rijrmax;若sj珟Si,则有rij=0.所有用户提供的评分构成一个“用户-项目”评分矩阵犚=[rij]N×M.在得到评分矩阵后,推荐服务器应用某种协同过滤算法预测矩阵中的未知评分(rij=0),而后根据预测结果向用户推荐项目.用狉^i=(r^i1,r^i2,…,r^iM)表示与用户i对应的预测结果,其中r^ij(j=1,2,…,M)定义如下:CFij(犚)表示预测的评分由全体用户的评分和所采f用的协同过滤算法决定.例如,若采用基于用户的最近邻协同过滤算法,f式中:Neighbour(i)表示与用户i最相似的若干个用户;Fsim(i,k)表示用户i与用户k的相似度,可用Pearson相关系数或余弦相似度度量[1].式(2)的含义是:找出与用户i具有相似偏好的若干用户,然后根据这些用户对项目sj的偏好程度来预测用户i对项目sj的偏好程度.在实际的推荐系统中,推荐服务器会根据狉^i生成用户i的推荐结果,即推荐服务器将预测评分最高的若干个项目推荐给用户.为便于分析,本文假定推荐服务器直接将狉^i返回给用户.用户i在得到狉^i后,会根据自身的兴趣对这一结果进行评估.用狆i=(pi1,pi2,…,piM)表示用户i的兴趣,其中pij(j∈{1,2,…,M})表示用户i对项目sj的偏好度.规定:0pijrmax,并且若sj∈珟Si,则pij=rij,即用户给出的项目评分等于用户对该项目的偏好度.若sj珟Si,可将pij理解为用户对项目的“潜在”评分,即如果用户在将来的某一时刻对sj评分,他给出的评分就会是pij.为每个用户定义函数gi:M→[0,1]来度量推荐结果狉^i的质量:式中,右侧第2项中的∑分向量狉^i与用户兴趣向量狆i的距离,rmax槡M表示这一距离可能取得的最大值.gi(狉^i)的值越大表示推荐结果与用户兴趣的匹配程度越高,即推荐质量越高.从式(1)~(3)可以看出,一个用户所得推荐的质量与其他用户提供的评分密切相关.协同过滤系统中的每个用户实际上是以向推荐服务器提供评分的方式与其他用户进行着博弈.下一小节给出该博弈的满足式表述(satisfactionform)[11].2.2“满足”博弈2.2.1参与者和行动本文将集合者,将珟Si视为用户i的行动(action),即ai=珟Si.用表示用户i的行动空间(actionspace).所有用户共享同一个行动空间,即对任意i∈{A(1),A(2),…,A(K)},其中A(k)1,2,…,K),K=2|S|-1.用户i从遵循特定的概率分布πi=(π(k)i表示用户i选择行动A(k)的概率.不同用户的πiπ一般是不同的.由式(1)可知,若给定协同过滤算法,则推荐服务器返回给每个用户的推荐结果狉^i完全由评分矩阵犚确定,而犚是由所有用户的行动组合犪=(a1,a2,…,aN)确定的.为体现用户行动对推荐质量的影响,可将gi(狉^i)改写为gi(狉^i)=hi(ai,犪-i),其中犪-i=(a1,…,ai-1,ai+1,…,aN),hi(·)表示从2×…×N到[0,1]的映射.直观上理解,无论是用户i自己提供更多的评分还是其他用户提供更多的评分,用户i都能获得更好的推荐.为度量用户提供评分的数量,定义用户i的评分完整度如下:由aiSi且ai≠可知0<σi1.用σ-i表示其他用户的评分完整度的平均值,即引入σi和σ-i之后,可将hi(ai,犪-i)改写为h(σi,σ-i;狆i).函数h(·;狆i)以狆i作为参数,以σi和σ-i作为输Page4入变量.本文对h(·;狆i)的单调性做出如下假设.假设1.对任意用户i∈对所有σi∈(0,1]和σ-i∈(0,1]都成立:(1)(2)上述假设表明,当每个用户都对其体验过的所有项目给出评分时,用户可得到最好的推荐.用Γmax表示最优的推荐质量,则有Γmax言部分提到过,用户对项目评分是需要付出成本的.用ci(ai)表示用户i选择行动ai时所付出的成本.对任意ai∈Ai和ai∈Ai,若aiai,则有ci(ai)<ci(ai).2.2.2满足式表述由于存在评分成本,用户通常不会对其体验过的所有项目都进行评分,这就意味着最优推荐Γmax很难实现.假定每个用户i对推荐质量有一个低于最优值的预期Γi.给定用户的行动组合犪,只要hi(犪)Γi,用户i即可满足.定义映射fi:(-i1×…×i-1×i+1×…×N)如下:fi(犪-i)=ai∈i:hi(ai,犪-i)Γ映射fi通常被称为correspondence[11].基于上述讨论,本文用如下的三元组描述所建立的博弈模型:上述形式称为博弈的满足式表述.Perlaza等人[11]在研究分布式自配置网络中的QoS保障问题时最先正式定义了这一形式的博弈.2.2.3满足均衡与博弈的满足式表述对应的均衡称为满足均衡(SatisfactionEquilibrium,SE).SE的规范定义如下[11].定义1.满足均衡.给定博弈的满足式表述G^CF=(足i∈足均衡.因本文假定对任意i∈犪max(S1,S2,…,SN)是G^个用户付出最高的评分成本ci(Si),而事实上由于i,用户可能并不需要用如此高的成本去换Γi<Γmax取满意的推荐结果.本文主要考虑如何找到符合如下两个条件的满足均衡犪+=(a+(1)i∈(2)至少存在一个用户不需要对其体验过的所有项目都给予评分,即i∈3均衡学习在上文建立的博弈模型G^动空间上的概率分布πi、对推荐质量的预期Γi、兴趣向量狆i、评分成本ci都是其私有信息(privateinformation),因而直接分析该博弈的均衡是很困难的.一种可行的方法是设计某种行为规则,让用户在迭代交互的过程中依据该规则不断调整自己的策略,最终学出一种均衡策略.结合推荐服务器的工作原理,本文设计了一种学习算法,该算法允许用户以逐渐增加评分数量的方式寻找均衡策略.本节首先介绍算法的基本流程,然后对算法的收敛性进行分析.3.1均衡学习算法在推荐系统中,用户与推荐服务器的交互是长期的———用户不断向推荐服务器提供评分数据,推荐服务器则不断调整反馈给用户的推荐.本文规定,每个用户在与推荐服务器迭代交互的过程中按如下规则行动:初始时刻(n=0),用户i计算其在行动空间上的概率分布πi(0)=(π选择行动ai(0).用户选择行动A(k)的概率π(k)i(0)=π其中,参数α(α>1)表示用户对评分成本的重视程度.α越小,用户越有可能选择较多的项目进行评分.归一化因子βi(0)按如下方式计算:当所有用户选择完初始行动后,推荐服务器根据得到的评分矩阵犚(0)计算推荐,然后将狉^i(0)返回给用户i.在此之后,用户以迭代的方式与推荐服务器交互.在第n次迭代开始时(n=1,2,…),用户i首先判断目前的推荐狉^i(n-1)是否已达预期.定义指示变量vi(n-1)如下:vi(n-1)=用户根据vi(n-1)的取值更新概率分布πi(n)=(π需要特别指出的是,推荐服务器会利用用户以往提(1)i(n),πPage5供的全部评分来计算推荐,所以从推荐结果来看,用户在第n次迭代中选择若干个新项目(记为珟Snewi(n))进行评分,其效果等价于用户在初始时刻对集合珟Snew评分.因此,本文将ai(n)定义为从初始时刻至第n次迭代结束时,用户i已经评过分的所有项目的集合.易知ai(n)ai(n-1).i(n)∪…∪珟Snew如果当前的推荐质量未达预期,即vi(n-1)=0,用户i可能将这一结果归结为以下两种原因之一:自己提供的评分太少;自己已提供足够多的评分,但其他用户提供的评分太少.若是前者,用户会对更多的项目进行评分;若是后者,用户倾向于不选择新的项目评分.行动选择概率π按如下方式计算:(k)i(n)=πσi(n-1),烄βi(n)/αci烅0,烆其中,σi(n-1)表示用户i当前的评分完整度:σi(n-1)越高,表明用户i已提供的评分越多,则用户继续提供评分的概率越低.归一化因子βi(n)定义如下:ππ如果当前的推荐质量已达预期,即vi(n-1)=1,则用户很有可能不再提供更多的评分.此时,概率(k)i(n)按如下方式确定:(k)i(n)=μ,烄βi(n)/αci烅0,烆其中,参数μ表示用户维持原有行动的概率,通常有0.5<μ1.归一化因子βi(n)定义为当所有用户选择了行动后,推荐服务器根据得到的评分矩阵犚(n)计算推荐,然后将狉^i(n)返回给用户i,之后进入下一轮迭代.若经过ns(ns>0)次迭代之后,所有用户都得到了满意的推荐,则迭代终止,称学习过程收敛于满足均衡犪+=(a1(ns),a2(ns),…,aN(ns)).我们将上述行为规则归纳为算法1.算法1.满足均衡学习算法.1.n=0:k∈{1,2,…,K},π其中,βi(0)=2.选择ai(0)~πi(0);3.FORALLn>0DO:4.更新πi(n):k∈{1,2,…,K},(k)i(n)=π其中,γi(n)=5.选择ai(n)~πi(n)6.ENDFOR3.2收敛性分析为了分析上文所提均衡学习算法的收敛性,本小节首先给出“用户状态”的定义,然后推导算法的收敛条件.3.2.1用户状态如图1所示,给定用户预期Γi,等式h(σi,σ-i;狆i)=Γi定义了σi~σ-i平面上的一条曲线段.由假设1可知,只有当σi和σ-i分别高于某个阈值时,用户i才有可能得到满足.阈值σi,min和σ-i,min分别由h(σi,min,1;狆i)=Γi和h(1,σ-i,min;狆i)=Γi确定.σi,min和σ-i,min图1用户状态示意图(网格区域:“满足”;斜线区域:Page6分别代表了对用户i自身提供评分数量和对其他用户提供评分数量的最低要求.在学习过程中,每个用户的评分完整度随着迭代次数的增加而增加,即有σi(n)σi(n-1).根据式(12),定义σ-i(n-1)如下:σ-i(n-1)=易知σ-i(n)σ-i(n-1).假定存在某个n0(n01)使得σi(n0)σi,min对所有i均成立,则从第n0+1次迭代开始,每个用户i处于以下3种状态之一:(1)满足(Satisfied).用户i已得到满意的推荐,即h(σi(n-1),σ-i(n-1);狆i)Γi.用户进入满足状态后会一直保持该状态,因为随着迭代次数的增加,σi和σ-i会增加或保持不变,且由假设1可知h(σi,σ-i;狆i)亦增加或保持不变.(2)接近满足(Proximitytosatisfied).用户i尚未得到满意的推荐,即h(σi(n-1),σ-i(n-1);狆i)<Γi,且用户i尚未对Si中的所有项目都进行评分,即σi(n-1)<1,但此时其他用户提供的评分数量已达到用户i的最低要求,即σ-i(n-1)σ-i,min.在这种情况下,即使其他用户不再提供更多的评分,用户i也可以通过增加自己的σi进入满足状态.(3)远离满足(Farfromsatisfied).用户i尚未得到满意的推荐,并且其他用户提供的评分数量未达到用户i的最低要求,即σ-i(n-1)<σ-i,min.在这种情况下,如果其他用户在后续的迭代过程中能提供足够多的评分,那么用户i可进入接近满足状态,否则用户将永远停留在远离满足状态.分别用ZS,ZP,ZF表示上述3种状态.用户i在第n(nn0)次迭代开始时的状态记为zi(n).3.2.2收敛条件在第n(nn0)次迭代开始时,可根据用户状态将全体用户分为两组:已满足的用户,zi(n)=ZS},未满足的用户zi(n)=ZP∨zi(n)=ZF}.随着迭代次数的增加,未满足的用户数逐渐下降.若算法在第nS次迭代开始时达到满足均衡,则有|US(nS)|=0.从上文对用户状态的定义可知,为了判断能否达到所有用户都处于满足状态的均衡,关键在于分析清楚用户评分完整度在迭代过程中的变化.由算法1可知,评分完整度在一次迭代中的增量Δσi(n)σi(n)-σi(n-1)是随机的,因此很难定量分析用户状态的变化.此处对算法1进行简化,然后推导简化后算法的收敛条件.(1)简化的均衡学习算法初始时刻,每个用户i从集合Si中随机选择一个项目进行评分.因而对任一i∈1.在第n次迭代开始时(n>0),若h(σi(n-1),|Si|σ-i(n-1);狆i)Γi,则用户i维持原有行动,即ai(n)=ai(n-1);若h(σi(n-1),σ-i(n-1);狆i)<Γi,则用户i从集合Si\ai(n-1)中随机选择一个项目进行评分.由上述行为规则可知:若i∈S(n),则有Δσi(n)=0;若i∈US(n),则有Δσi(n)=(2)两类用户为了推导均衡学习算法收敛条件的解析表达式,除了对算法本身进行简化,本文亦做出如下假设.假设2.全体用户可分为两组,分别记为和B.①i∈A,Γi=ηAΓmax②i∈B,Γi=ηBΓmax③i∈1M0<|S|;④i∈等式:其中,1M0根据上述假设,σi,min和σ-i,min可按如下方式计算:若0<ηiσ-i,min=则σi,min=σ-i,min=2ηPage7(3)评分完整度的变化基于上述简化和假设,我们可定量分析算法的收敛条件.考虑着迭代的进行,用户在正方形区域[0,1]2中从左下角向右上方移动.由简化的均衡学习算法可知,第1次迭代开始时,用户i位于点1入满足状态之前,如下两个等式对所有用户均成立:Δσ-i(n)=σ-i(n)-σ-i(n-1)=A中的用户对推荐结果有相对较低的预期,因此这些用户先于某个nA∈{1,2,…,M0-1}使得US(nA)=B.在第nA次迭代开始前,用户始终沿着直线σ-i=σi移动.第nA次迭代开始时,用户i位于点nA若zi(nA)=ZP,则用户i可在若干次迭代后进入ZS状态.下面重点分析另一种情况,即zi(nA)=ZF.在第nA次迭代中,进行评分,而在σi~σ-i平面上沿着斜率为kB(kB<1)的直线移动:kB=之后,用户i继续沿着该方向移动,直到下述两种情况之一发生:用户i进入满足状态;用户i尚未满足但已对集合Si中的所有项目进行了评分,即σi=1.当第2种情况出现时,用户i将永远不能满足.其原因是期,这意味着此时除用户i外提供了完整的评分,σ-i无法继续增加.如图2所示,当kB小于某个阈值kmin时,上述第2种情况就会出现.kmin定义如下:kmin=将式(20)和(21)代入kB<kmin可得σ-i,min>式(22)的右半部分实际上就是如下情况对应的σ-i:B中的所有用户都提供了完整的评分,而用户只提供了能使自己得到满足的必要数量的评分.(4)用户预期与收敛性式(22)说明如果一个用户对推荐结果有很高的预期,即需要其他用户付出较高的评分成本,那么该用户就可能无法得到满意的推荐.由假设2可知,当用户的预期不同时,σ-i,min的定义方式不同:①如果ηA<ηBσ-i,min=因M01且|B|1,上述不等式不成立.也就是说,当ηA<ηB法必然收敛.②如果1σ-i,min=2ηzi(nA)=ZS可知式(24)成立,则算法不能收敛:基于上述讨论,本文给出如下定理.定理1.在假设2成立的前提下,若下述两个条件之一成立,则简化后的均衡学习算法可以收敛Page8到博弈G^①ηA<ηB②(5)收敛阈值为了更好的理解用户预期对算法收敛性的影响,本文给出如下分析:①给定ρNB式成立,则简化后的均衡学习算法无法收敛:ηB>用θB表示上述不等式的右半部分.如图4(a)所示,在给定ρN的增长速率相对较低.这表明当用户对推荐结果的整体预期变高时(ηA差异不显著,仍会有部分用户无法得到满足.从图4(a)还可以看出,给定ηA这意味着当有越来越多的用户对推荐结果抱有较高图4均衡学习算法的收敛条件((a)给定ρN和ηA,若ηB>θB,则简化后的学习算法无法收敛.绘制曲线时,设0.3ηA0.9,N=10000;(b)给定ρη,若ηB>ηB,min且ρN<θN,则简化后的学习算法无法收敛.绘制曲线时,设14仿真分析为了检验所提均衡学习算法的可行性,本文用真实的评分数据进行了一系列仿真实验.本节首先对实验所用的数据集以及仿真参数的设置进行说明,然后对不同参数设置下的仿真结果进行分析,最后对3.2节给出的算法收敛条件进行验证.预期时,用户可期望得到更高质量的推荐.②给定ρη且下述不等式成立,则简化的均衡学习算法无法收敛:用θN表示上述不等式的右半部分.该不等式暗含着θN>0这一条件.由θN>0可得ηB>-2Nρη+,θN随着ηB用ηB,min定ρη户的预期变得更高时,只有当所增加时,才有可能实现满足均衡.另外,给定ηBθN随着ρη上的差异变大时,可以有更多的用户预期得到高质量的推荐.上述结论与从图4(a)中得到的结论是一致的.4.1数据集和参数设置本文分别用来自Jester数据集[15]和MovieLens数据集①的评分数据进行了仿真.4.1.1Jester本文使用的Jester数据集包含了24983个用户①http://files.grouplens.org/datasets/movielens/ml-1m.zipPage9对100个笑话的评分.用户对笑话的评分在区间[-10,10]上取值.若用户未给出某个笑话的评分,则用“99”表示这一未知的评分.在所有用户中,有7200个用户对全部的100个笑话都给出了评分.考虑到在均衡学习过程中需根据用户对所有项目的“潜在”评分来评估推荐质量(参见式(3)),本文仅选择这7200个用户的评分作为实验用数据.我们将原始评分数据转换为评分矩阵犚=[rij]7200×100,并将rij调整至区间[10.0,30.0](以rij=0表示未知的评分).均衡学习算法涉及的参数按如下方式设置:矩阵犚的每一行被视为对应用户的兴趣向量狆i;假定每个用户体验过的项目总数均为70,即|Si|=70.为确定每个用户的Si,从矩阵犚的每一行中随机选择30个元素置为零.处理后的评分矩阵记为犚′;按式(2)预测未知评分,设|Neighbour(i)|=按式(3)计算推荐质量gi(狉^i);以评分的数量作为评分成本,即ci(ai)=|ai|;由式(11)和(14)可知,算法的收敛速度与参数α的取值密切相关.考虑到函数f(x)=1/αx在区间[0,70]上的变化趋势,仿真时设α=1.2;分别设μ=0.9和μ=1以模拟如下两种情况:已满足的用户继续提供评分,已满足的用户不再提供评分.为设置Γi,首先利用犚和犚′计算每个用户所能获得的最优推荐质量Γmax(0<ηi<1).ηi4.1.2MovieLens本文采用的MovieLens数据集包含了6040个用户对3900部电影的评分.与上文对Jester数据集的处理类似,此处亦假定每个用户体验过的项目的总数为70,因而我们滤除那些评分数量低于70的用户,并滤除那些未获得评分的项目,最终保留3631个用户对3675个项目的评分,对应的评分矩阵为犚=[rij]3631×3675,其中rij∈{0,1,…,5},rij=0表示评分未知.与Jester数据不同,这一评分矩阵是非常稀疏的,矩阵中非零元素的比例仅为6.78%.均衡学习算法的参数按如下方式设置:矩阵犚的每一行被视为对应用户的兴趣向量狆i;从犚的每一行中随机选择70个非零元素构成对应用户的完整评分集合,即用户只能对这70个元素对应的项目进行评分;按式(2)预测未知评分,设|Neighbour(i)|=20;按式(3)计算推荐质量;参数α,μ和Γi的设置方法与前文所述相同.4.2均衡学习仿真结果为检验算法1的可行性,本文测试了多组{ηi给定评分矩阵犚和参数μ,分别在如下4种设置下运行学习算法:(1)i∈(2)i∈(3)随机选择1%的用户,令其对应的ηi=0.85,其余用户的ηi=0.5;(4)随机选择20%的用户,令其对应的ηi=0.85,其余用户的ηi=0.5;为降低随机性的影响,给定一组参数,重复运行算法5次.在每一次运行中,当所有用户都已满足或迭代次数达到10000次时,算法终止.每次运行后,记录算法终止时的迭代次数nstop、已满足的用户数NS和用户评分完整度的平均值σ-|Si|.表1和表2分别给出了Jester数据集和Movie-Lens数据集上的仿真结果.可以看出:当所有用户对推荐质量有着相似的预期时,即使预期很高(ηi=0.85)并且用户在满足之后不再参与评分,满足均衡也是可以实现的.给定μ的取值,随着用户预期的升高,算法的收敛时间变长,σ-的项目进行评分才能得到满意的推荐.给定一组{ηi}Ni=1,比较不同μ对应的仿真结果可以看到,相比于μ=0.9的情况,当μ=1,算法的收敛时间变长,但用户评分完整度下降.收敛时间变长的原因在于当μ=0.9时,已满足的用户会继续为推荐质量的提升做出贡献,因此其他那些间内得到满意的推荐.而当μ=1时,未满足的用户只能依靠他们自己提升推荐质量,因此需要更多的时间才能达到满足均衡.从评分完整度来看,μ=0.9意味着在达到满足均衡时,用户提供的评分数量可能远高于与其预期相对应的必要的评分数量.而当μ=1时,用户倾向于只提供能满足其个人预期的最少的评分,因此当算法收敛时,用户评分的完整度较低.当大多数用户对推荐结果有着适中的预期(ηi=0.5)而小部分用户的预期很高时(ηi=0.85),若μ=0.9,满足均衡仍是可以实现的,只不过此时算法的收敛速度要慢于所有用户预期都不高的情况,并且在达到均衡时用户评分的完整度接近于所有用户都持有高预期的情况.这一结果表明,为了满足少部分用户的需求,那些预期不高的用户需要在得到满意的推荐之后继续提供很多评分.当μ=1时,满足的用户不再提供评分.因而当大多数用户满足之后,余Page10表1Jester数据集上的均衡学习仿真结果表2MovieLens数据集上的均衡学习仿真结果ηi=0.5ηi=0.851%:ηi=0.85,99%:ηi=0.520%:ηi=0.85,80%:ηi=0.5ηi=0.5ηi=0.851%:ηi=0.85,99%:ηi=0.520%:ηi=0.85,80%:ηi=0.5下那些未满足的用户很难再使推荐质量有显著提升,算法无法在10000次迭代内收敛.观察σ-可以看出,大多数用户只是提供了能够满足其中等预期的评分,但这一数量的评分不足以产生很高质量的推荐.为更清楚地观察用户预期对均衡学习结果的影响,我们将不同参数设置对应的|S(n)|变化曲线绘制于图5中(以Jester数据集为例).可以看到,在第4种设置下(图5中圆圈标记的曲线),经过约15次迭代,80%的用户就已满足,这与第1种设置的仿图5均衡学习过程中“满足”用户的数量的变化52672000.543120372000.91440372000.88190372000.91057336310.92156036310.99115736310.96467836310.993真结果(图5中用圆点标记的曲线)类似.而在此之后,|S(n)|的增长明显变缓,经过很多次迭代才达到满足均衡.由上述仿真结果可以得到关于协同过滤系统中满足均衡的直观理解:若所有用户对推荐质量的预期都不高,则低成本的满足均衡是可以实现的,即每个用户只需提供较少的评分就能得到满意的推荐.稍后将通过另一组仿真对3.2节提出的收敛条件进行验证.4.3奖励机制从上一小节给出的仿真结果可以看出,当不同用户对推荐质量有着相似的预期时,仅靠推荐质量这一内部激励就可以让用户自发地提供足够多的评分数据以使推荐服务器产生符合所有用户预期的推荐.而当不同用户对推荐质量的预期相差较大时且用户在满足之后不再对更多的项目评分时,均衡学习算法无法收敛到满足均衡,这意味着此时推荐服务器有必要提供一些外部激励以促使用户参与评分.假定当一个用户选择行动A(k)的成本为ci(A(k))|A(k)奖励为b(A(k))κ|A(k)个项目评分后所能获得的奖励(0<κ<1).由算法1可知,用户在选择行动时倾向于选择低成本的行Page11动.推荐服务器向用户给予奖励,这相当于用户的评分成本由原来的ci(A(k))降低为ci(A(k))-b(A(k)),因而用户选择较多项目进行评分的概率变高.另一方面,当用户的预期得到满足之后,虽然推荐质量这一内在激励失效,但如果推荐服务器提供奖励,用户仍有动机对更多的项目评分.本文规定,在均衡学习过程中,若存在评分奖励,已满足的用户维持原有行动的概率为μ1-κ.由0<κ<1可知,此时μ<1,因而均衡学习算法总能收敛到满足均衡.本文在Jester数据集上对带有评分奖励的均衡表3不同奖励下的均衡学习仿真结果(Jester数据集)κ=0.01κ=0.1κ=0.5κ=0.9上述关于奖励机制的简单讨论表明,推荐服务器可通过向用户提供外部奖励的方式促使用户间的博弈尽快达到满足均衡.推荐服务器可通过观察用户以往的评分行为大致估计用户的评分成本、用户对推荐质量的预期等,进而设计合适的奖励规则.具体的设计方法有待进一步研究.4.4推荐质量和评分完整度之间的关系第3.2节给出的关于均衡学习算法收敛性的理论分析是基于若干假设的.在检验收敛条件之前,此处先利用Jester数据集对假设1和假设2的合理性进行说明.实验方法如下:利用评分矩阵犚′为每个用户i构造一组矩阵{犚i,k},其中每个矩阵犚i,k对应于一组特定的σi和σ-i.σi在集合1取值,σ-i在集合1给定σi=机选择5个非零元素,然后将这些元素置为零,接着再从犚′的其他行中随机选择90%的非零元素,并将这些元素置为零.将协同过滤算法应用每个犚i,k,并根据用户i的兴趣向量对推荐结果进行评估,得到与每组(σi,σ-i)对应的h(σi,σ-i;狆i).根据每个用户学习算法进行了仿真.在仿真中,分别设κ=0.01,0.1,0.5,0.9,其他参数的设计与上一小节相同.仿真结果如表3所示.对比表1和表3可以看出,当推荐服务器实施评分奖励时,均衡学习算法的收敛速度明显加快.奖励越高,算法的收敛速度越快.例如,当仅有1%的用户对推荐质量的预期很高时,由表1可知,若没有评分奖励且μ=0.9,学习算法要经过至少400次迭代才能达到均衡;而当推荐服务器提供评分奖励且κ=0.1时,此时亦有μ=0.9,但学习算法只需经过约200次迭代便可达到均衡.510272000.93820272000.8991872000.904372000.925的实验数据{(σi,σ-i,h(σi,σ-i;狆i))},可以绘制出与该用户对应的h(σi,σ-i;狆i)曲面图.图6(a)给出了某一用户对应的曲面图,其他用户的曲面图与之类似.从图6(a)可以看到,推荐质量的确会随σi和σ-iPage12的增加而提升,这表明假设1是合理的.另外,从图6(b)所示的等高线图可以看出,给定h(σi,σ-i;狆i)的取值,σi和σ-i之间的关系可大致用二次曲线描述,这表明假设2也是合理的.4.5收敛条件测试,先按式(26)计算θB,然后将ηB本文利用Jester数据集对3.2节给出的均衡学习算法收敛条件进行了验证.按照假设2的描述,全部7200个用户被随机分为NA和NB两组,ηA0.5,ρNρNθB-0.05,ηB=θB和ηB=θB+0.05.为证明在所有用户都有高预期的情况下算法是可以收敛的,对ηA=ηB=θB+0.05的情况也进行了仿真.给定一组(ρN),将简化的均衡学习算法运行10次.在每一ηA次运行中,当出现如下两种情况之一时算法终止:所,ηB有用户均已满足,未满足的用户已经对其体验过的表4收敛性测试结果ρN0.010.50.7920.10.236870706568707065687070675结论在协同过滤推荐系统中,用户的评分行为是相互影响的,各用户以推荐服务器为媒介进行着某种复杂的交互.本文将用户间的这种交互建模为一种不完全信息博弈,定义了该博弈对应的满足均衡,并提出了一种均衡学习算法.所提算法的基本思想是:用户在与推荐服务器迭代交互的过程中,根据当前所有项目进行了评分.算法终止时的迭代次数记为nstop,满足的用户数记为NS.从表4所示的仿真结果可以看到,给定ηA,当ηB=θB-0.05时,满足均衡总是可以实现的;ρN当ηB=θB时,θB+0.05时,结果与3.2节的理论分析结果有些许出入,因为由式(26)可知,当ηBθB时满足均衡应是可以实现的.造成理论分析与仿真结果不一致的原因可能是用户评分完整度与推荐质量之间的关系并不严格符合假设2所定义的二次曲线关系.从与ηA=ηB=θB+0.05对应的结果可看出,即使所有用户都有较高的预期,用户不提供完整的评分也能实现满足均衡(nstop<|Si|).这一结果进一步表明,在一个协同过滤系统中,用户能否通过自发的评分实现满足均衡,取决于不同用户对推荐质量是否有着相似的预期.4677070656870706669707066所得推荐是否达到预期来决定是否提供更多的评分.在适当的简化假设下,本文分析了算法的收敛条件.在真实数据上的仿真结果表明,当所有用户对推荐质量有着相似的预期时,通过用户自发的评分行为确实是可以实现满足均衡的.用户的积极参与对协同过滤系统的良好运作至关重要.本文将推荐质量视为促使用户参与评分的一种内部激励,但给出的博弈分析也可为外部激励机制的设计提供一些启发.在接下来的工作中,我们Page13将研究如何结合内部激励和外部激励来引导用户在协作式系统中的行为.
