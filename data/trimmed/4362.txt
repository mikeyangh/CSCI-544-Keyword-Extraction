Page1面向目标检测与姿态估计的联合文法模型陈耀东李仁发李实英黄鑫谢国琪(湖南大学信息科学与工程学院长沙410082)摘要针对部件模型在描述目标上的局限性,提出了一种判别化的视觉文法模型.该模型利用文法的可描述性和可扩展性能够对通用目标类别进行描述并且处理一般化的识别任务.根据目标检测和姿态估计的特点将文法模型实例化为两个单任务文法,同时对比了文法的异同.通过分析检测与姿态估计在应用背景和研究方法上的互补性,进一步提出了一种联合识别文法.联合文法由一组判别符号合并两个单任务文法,其特点是实现了并行化的目标检测与姿态估计,而且能同时提升检测和估计性能.鉴于参数训练所面临的弱监督环境,引入带隐变量的结构化学习框架优化文法参数.实验分别在单任务和多任务场景下对比了部件模型与提出的联合文法.实验结果说明联合文法在性能上优于当前主流的检测模型和姿态估计模型.关键词视觉文法;部件模型;目标检测;姿态估计;基于隐变量的结构化SVM;计算机视觉1引言文法(Grammar)是一种基于规则的认知模型,它通过符号和产生式来描述客观对象以及对象之间的关系,适于结构化模型计算.文法已在人工智能的多个领域得到深入应用,如自然语言处理、专家系统等,并展示出诸多特点:(1)可描述性,文法基于有限的符号表现近乎无限的结构形式,例如英语通用词汇为2000~3000个,通过该词汇集能够完成一般的日常对话;(2)可扩展性,构建文法规则可采用自动学习或人工设定等多种方式,符号和产生式可根据应用需求动态调整;(3)可计算性,文法模型大多基于上下文无关文法,可利用动态规划在多项式时间内完成推断.不同于文本语言,视觉目标的复杂性使得视觉文法模型面临更大的挑战:其一,空间无序性.视觉目标及其子目标在空间上是无序的,不存在显式的语法规则,致使假设空间庞大,计算复杂度偏高;其二,特征连续性.视觉目标的特征具有多源化特点,涉及颜色、光线、尺度以及局部形变.相比文本的离散取值,视觉特征的取值基本处在连续空间,致使特征学习更为困难.目标识别是计算机视觉的一个基本问题.该问题在特定场景中表现为某个具体的识别任务,例如目标检测、图像分类、场景理解等.部件模型在目标识别中是一种重要的结构化建模方法.部件模型以预设部件集描述整体目标,部件的相对位移基于弹性计算,能较好地处理目标的局部变形.近年来,基于可变形部件的目标模型[1-2]在目标检测和姿态估计上取得较大的性能提升,与之相关的扩展算法成为当前识别领域的一个研究热点.许多视觉应用(例如事件检测)需要同时涉及多个识别任务,以“打开车门”这一事件为例,一般先检测到指定目标(人体、汽车),再定位该目标的某些重要部件(人手、车门等),最后对这些部件的关系建模.然而,这种串行处理方式势必造成错误传播,换言之,若人体或汽车检测失败则必然导致事件检测失败.对于上述应用场景,开发一个能同时解决目标检测与部件定位的统一框架将会更加行之有效.但是当前已有的部件模型均面向单识别任务,模型结构固定,无法扩展至多任务应用.文法已经在视觉识别领域获得了一定的研究,表现出较好的描述性和识别性能.在总结已有文法研究的基础上,本文提出一个判别文法模型,该模型具有两个特点:(1)通用性,在给定结点和产生式的具体定义后可处理各种识别任务;(2)扩展性,文法可根据应用需求调整层次结构.其次,本文使用该模型重新定义和描述了经典的检测模型DPM(DeformablePartModel)[2]和姿态估计模型PS结构(PictorialStructure)[1].进一步,本文通过整合单任务文法实现检测与姿态估计的并行化处理,合并后的文法在单项任务达到更高的识别性能,还能提高整体识别速度.对于文法训练,本文引入基于隐变量的结构化SVM算法,在给定语义部件标注集的条件下使用结构化损失函数优化文法参数.2相关工作结构化描述与计算是目标识别的一种重要的研究方法,视觉文法则是实现该类描述与计算的一种有效手段.视觉文法已经在目标识别的各个领域获得应用,包括场景分析[3]、形状侦测[4]、目标检测[5-6]、事件识别[7]等.根据规则推导方式,视觉文法可大致分为概率文法[3-4]和判别文法[5-7]两种类型.视觉概率文法是自然语言处理(NaturalLanguageProcessing,NLP)的文法理论向视觉领域的延伸,其特点是结构简洁,适于处理目标遮挡或类内形状变化等问题.具有代表性的概率文法是Zhu提出的And-Or图(And-OrGraph,AOG)[3,8].AOG是一个通用的视觉描述框架,其中And类结点表示指定对象分解为多个必有的成份,Or类结点代表分解具有多种可选方式.AOG采用两个独立的概率计算模型即随机上下文无关文法和马尔科夫随机域来分别计算结点的上下层次关系和左右邻接关系.文献[4]研究了基于文法的形状描述结构,通过概率上下文无关文法建立目标的局部外观与形状曲线之间的推导关系.概率文法尽管描述直观,但文法规则的学习依赖于大规模深层标注集,限制该类文法的实用性.其次,概率文法的结点一般定义为具有语义的子目标,当这些语义目标的表征性不强时将影响整体模型的识别效果.判别文法近几年成为视觉文法的研究热点,该类文法基于当前成熟的判别学习模型,在特征学习和模型表示上具有更强的鲁棒性,识别精度更高.一个典型的判别文法是Felzenszwalb等人[5]面向目标检测提出的文法结构.该文法从训练语料中自动学习文法结点,在弱监督学习环境下可获取较好的泛化能力.其缺点在于该文法固定为3层结构,限制Page3了模型对目标的描述能力,也使得该模型无法适用于其他识别任务.文献[6]将AOG框架扩展为一个判别的目标检测模型.在该模型中文法结点不再是语义组件而是聚类产生的局部特征块,从而避免对深层标注集的依赖.但该模型采用网格搜索方式提取目标特征,巨大的特征空间导致学习复杂度非常高.文献[9-10]基于判别化AOG处理形状检测,该类方法利用可变形部件动态选取目标的局部轮廓特征,部件参数的训练采用CCCP算法(Concave-ConvexProcedure)[11].文献[12]将判别化AOG应用于多目标类别的检测,特点是以部件共享的方式在一个AOG中涵盖多个目标类别的描述,该文献同时对CCCP扩展,提出了一种动态结构优化算法.本文提出了一种可扩展的判别文法结构,文法的一个重要特点是不受结点形式和层次的限制,可根据应用需求自动或手动调整结构.本文采用基于隐变量的结构化SVM实现文法推断和参数学习,这一点与文献[9-10,12]的思想一致,区别在于面对的应用问题不同.PS结构和DPM是具有代表性的两个部件模型,前者采用概率生成方式处理非刚性目标的姿态估计①,后者基于判别得分函数解决目标检测问题.文献[13]对PS进行了扩展,将部件按方向属性划分为多个子类型以描述其内部差异.与DPM相关的后续研究非常多,例如文献[14-15]针对部件学习问题分别采用监督方式和自适应方式提高学习效率,文献[16]提出部件共享的思想以降低模型参数的数目.本文所提出的文法结构是对上述部件模型的泛化,文法结点视所处场景可以是判别部件也可以是语义部件,或者是两者的混合.本文的主要贡献是利用文法的扩展性特点将PS和DPM进行合并,实现目标与其部件的联合识别.3视觉文法模型3.1模型定义一个视觉文法G形式化为四元组(Σ,,S,),其中为非终结符集.S∈为文法开始符号,具体指代为视觉上的语义目标或者抽象目标.一个非终结符Y(a)∈表示目标的子成份,例如预设部件或者是部件组合.实际识别时每个非终结符均带有图像空间Ω的一个赋值a.典型的Ω是一个6维空间(x1,y1,x2,y2,σ,),包括坐标向量(x1,y1,x2,y2)、尺度σ、方向.Σ为终结符集,每个终结符t∈Σ由某个非终结符导出,实际代表该非终结符映射在图像某局部区域的特征集.产生式集合划分为3种类型:(1)模式产生式(简称m产生式),形如S→M1,…,S→Mn,Mi∈.每个m产生式表示目标分解的一种方式,Mi代表其中的某个类别.例如{M1,…,Mn}可代表左/右视角模式,或者依据姿态分解为站立、奔跑、坐卧等多个模式.(2)关联产生式(简称e产生式).有两种形式Y(a0)→{X1(a1),X2(a2),…,Xn(an)}及Y(a0)→{t,X1(a1),X2(a2),…,Xn(an)},Y,Xi∈,t∈Σ.产生式右端为非终结符X1,…,Xn组成的包集合,每个非终结符Xi在集合中的出现是无序的.第2种产生式实际由第1种产生式与下面的叶产生式合写而成.e产生式定义了非终结符Y与符号X1…Xn之间的条件相关性.若结点Y在图像位置a0出现,则可以推导出X1…Xn分别在a1…an出现.(3)叶产生式(简称f产生式),形如Y(a)→t,Y∈,t∈Σ.f产生式定义了非终结符Y在图像位置a上的特征外观.当前的识别方法通常将每个目标依据外观分布划分为多个子类型t1,…,tK,因而每个非终结符可能具有多个叶产生式,分别导出不同的终结符类型.视觉文法G的语言表示为其中T表示从开始符号S导出句子犵的产生式集.本文限定视觉文法为上下文无关文法,因而产生集T是一棵树结构,称为推导树(derivationtree).3.2模型推导区别于文本语言,视觉文法的产生式计算涉及结点之间的空间关系,定义产生式权重函数为w:×Ω→R.m产生式和f产生式不涉及结点的相对位移,其权重恒为0.给定视觉文法G,结点的得分计算采用自下而上的方式.注意,这里讨论的是一般化文法结构,不针对具体的识别任务,因而不涉及具体的函数计算与特征形式.(1)每个终结符t∈Σ,根据f产生式Y(a)→t的含义,指向非终结符Y在位置a上的特征集,结点t的得分为式(2)采用线性计算方式,即特征模板与权重向量①姿态估计狭义上仅面向人体目标的部件定位,广义上可延Page4θ的乘积.(a)表示在图像窗口a抽取的特征描述子,典型的特征模板为梯度方向直方图(HistogramofOrientedGradients,HOG)[17].(2)对非终结符X,若存在多个f产生式X→t1,…,X→tK,则该结点的得分为由于f产生式的推导权重为常数0,因而X的得分直接取其所有导出终结符的最大得分.(3)对每个非终结符Y,以e产生式的第1种形式Y(a0)→{X1(a1),…,Xn(an)}为例,Y的得分为Sc(Y)=max(a1,…,an)w(a0,a1,…,an)+∑i∈{1,…,n}Sc(Xi非终结符Y取所有推导子树的最大值,每棵子树由产生式权重w(a0,a1,…,an)与对应子结点得分之和.因每个子结点Xi相互独立(产生式右端为包集合),因而上式w的计算等价于求取每个子结点的空间属性偏差之和:wa0,a1,…,a(whereφ(a0,ai)=[dx,dy,dx2,dy2,dσ,d]=[(ax0-axφ(a0,ai)是一个空间偏移函数,λi是结点Xi的空间参数,于是式(4)改写为Sc(Y)=∑i∈{1,…,n}maxai[-λi·φ(a0,ai)+Sc(Xi)]值得注意的是,w(a0,a1,…,an)可以定义为更复杂的关系函数或者建立统计模型.(4)对目标结点S,根据S→M1|…|Mn有基于上述讨论,给定图像I以及某个导出句子犵={(t1,a1),…,(tp,ap)},该句子对应的推导树计算如下:Sc(I,犵)=∑e∈E这里E表示树结构的边集.θi对应部件非终结符Xi的外观参数向量,λe为树边e相关的空间参数.由式(9)可知,文法推导的过程实际上是找到最佳的部件格局犵满足:犵=argmax犵Sc(I,犵).这一过程详述为两个步骤:(1)自上而下搜索所有结点的空间定位.首先指定根结点S的位置,依次计算S导出的非终结符并计算对应终结符类型的所有导出位置;(2)执行式(2)~式(8)自下而上计算所有可能的推导树并从中选择最佳结果.4联合文法模型本节讨论使用视觉文法模型处理具体的识别目标检测和姿态估计.首先给出这两个任务的文法形式,然后阐述如何合并文法以实现并行化识别,最后介绍联合文法的训练框架.4.1基于部件的单任务文法图1展示了两个部件模型的基本结构.DPM(图1(a))采用判别部件描述目标类的局部特征,部件之间基于简单的星状结构(图1(c)),即设定一个中心部件(一般为待检目标的低采样图像块),其他部件只与中心部件关联.值得注意的是,DPM设定的部件用于整体目标的判别分类,本身不具有语义性(图1(b)).PS(图1(d))通过部件定位来识别目标的姿态.部件通常是预定义的语义部件,具有方向属性(图1(e)),部件关系采用树结构描述(图1(f)).(1)DPM文法参照文献[2]的基本方法,表1给出了部件模型对应的文法结构:DPM文法(简称为文法A).文法A为3层树结构:第1层由两个m产生式组成,对应DPM[2]的两个混合组件(即左/右视角),其中Page5ML,MR∈分别代表左/右视角的中心部件.每个m产生式指向一个单独的星状结构,直接导出所有的部件非终结符X1,…,XP(DPM设定部件数为P=8).Xi∈表示目标的局部判别部件.每个Xi同样具有左/右两种视角的外观子类型,因此Xi导出两个终结符tLi,tRi.每个终结符使用HOG线性SVM分类器[17].文法A的语言描述为LA(G)={犵:ST犵,and犵∈H0×H1×H2×…×HP},Hi={t1i…tKi}式(10)说明文法A导出一个句子等价于DPM推断出目标的一个部件格局.值得提出的是,LA(G)允许不同视角类型的终结符组成混合句子.换言之,终结符类型在各m产生式中共享,例如S→ML可导出句子(…,tLi,…,tR共享思想.序号1S→ML|MR2ML→{t13MR→{t14X1→t15…6XP→t1:为简写方便,使用“|”代表两个产生式的合并.(2)PS文法表2展示部件模型PS[13]对应的文法结构(简称文法B),该文法采用广度优先方式描述了一个树结构.每个Xij∈表示预设的语义部件或者关节点(以下均以语义部件为例),下标i,j分别指代树结构第i层从左至右的第j个结点.文法B假定根结点处于第0层,且树结构一共有l层,每层结点数依次为n1…nl.文法B的e产生式描述了PS部件的条件相关性.根据文献[13],文法B默认设定10个非终结符,每个非终结符可导出K=4个终结符,即4种外观类型.每个终结符代表一个HOG线性SVM分类器.文法B的语言和句子形式与文法A相同,只是每个句子对应一种目标的姿态,不难计算文法B一共可以产生K10个句子形式.序号1S→X0,12X0,1→{t13X1,1→{t14…5X1,n1→{t16…7Xl,nl→t1由于应用场景不同,两个文法的训练方式差别较大.参照文献[2],文法A的训练集一般只给定目标的整体标注,因而模型需要采用弱监督学习算法自动获取高判别性的部件.参照文献[13],文法B的训练集给定了部件层标注,因而每个部件可执行独立的监督学习过程.文法B采用结构化学习框架训练部件之间的关联参数.详细过程请分别参阅文献[2]和文献[13].4.2联合文法目标检测与姿态估计是两个互补性较强的识别任务,其一,确定目标的姿态可以为目标类别提供更精确的外观分类,提高检测精度.例如人体可以按站立、坐卧、奔跑等姿态区别外观,这比简单地按左右视角划分更能反映目标外观的真实分布;其二,一个目标的检测得分本身受到自身语义部件的影响,若部件的外观得分低,则目标存在的可能性显然较小.联合两个识别任务,综合考虑目标与部件的特征相关性,有利于获取更佳的识别结果,且对于需执行两个识别任务的应用场景,任务联合可有效地降低运算量;其三,当前姿态估计的研究是假定已给出目标标注,但此假设在现实环境下难以保证,因而目标检测是实现姿态估计的前提条件.对比文法A和文法B,两者具有类似的模型结构(星状模型是一种特殊的树结构)以及相同的语言形式.一个简单的整合方法是将文法A的非终结符X1…XP直接定义为语义部件,修改后的文法A是一个基于语义部件的星状结构.然而,直接以语义部件作为目标检测的依据存在许多弊病:(1)独立检测语义部件通常比检测整体目标的难度更大,有可能因为定位不到语义部件或者部件得分过低而导致整体目标检测失败;(2)描述语义部件的特征并不一定对整体目标具有判别性.正是基于上述考虑,DPM选择了自动获得判别部件而非语义部件.文献[14-15]对此有进一步讨论和实验论证.4.2.1文法描述本文借鉴浅层分析思想(ShallowParsing)[18]合并文法A与文法B.图2(a)和图2(b)展示了深层分析树与浅层分析树的基本结构.浅层分析树预定义一个组块集(例如名词组块、动词组块、形容词组块等),将句子分析从深层的树结构转换为浅层的平面结构.对比观察,组块的描述特征比基本语法单位更为丰富,例如浅层树的C-NP组块涉及4个单词特征,而深层树的底层NP仅包含1~2个单词特征.而且组块识别过程还可避免底层语法单位的细Page6节分析,例如深层树和浅层树所需分析的关系边分别是15和11.上述两个特点决定了浅层分析能提升准确性和计算效率.联合文法的基本思想是在文法B中添加类似组块的平面层,该层由一组预定义的非终结符组成,称为判别非终结符或者简称为判别部件(文法B原有的部件非终结符简称为语义部件).观察图2(c)表3展示了联合文法(简称文法C)的具体内容.文法C包含多个m产生式,每个m产生式(Mi)相当于一个星状结构,且导出一个判别非终结符集合的赋值Z={Z1,…,Zm},每个Zm代表n1个语义部件的共现,即有Zm→{zm,X1,…,Xn},根据浅层分析思想,Zm的描述特征较单个语义部件更为丰富,从中可以获取更多的判别特征.为提高识别的鲁棒性,本文使用混合判别部件,集合Z由单判别部图2文法联合的基本思想(句法结构成份:IP-从句符号,NP-名词短语,VP-动词短语,DNP-的字短语,DEC-的字,NN-普通名词,NT-时间词,VV-普通动词,OD-序数词,NR-专有名词,C-NP-名词性组块,C-ADVP-副词性组块,C-VP-动词性组块.视觉文法成份:S-文法开始符号,X-语义非终结符,Z-判别非终结符(所有Z处于同一层次,其下标仅表示该符号所对应的语义非终结符索引),每个非终结符的右侧图像块表示该符号对应的一个终结符类型)件(Zi)和双判别部件(Zi,j)两种判别部件组成,单部件是对双部件的补充.值得注意的是,Zi与Zi,j是处于同一层面的非终结符,符号的上标只用于标识对应的语义部件.结符类型数目为K,则集合Z的元素理论上有C2P个,每个判别部件的终结符类型数最大为K2,所有可能的候选m产生式数量可以达到C2P(P-1)/2+…+CP-1/2C4参照姿态估计的常规做法将部件关联树预设为该目标类别的KinematicTree(KT).人体目标的KT树参见图1(d).具有P个结点的KT树含有P-1条边,因而文法C实际最多含有2P-1个判别部件,m产生式为C1算,判别终结符从原有55降至19,候选m产生式从1012降到102量级.类似的,文法C中tki,zki仍然使用基于HOG的线性SVM分类器.序号1S→M1|…|Mn2M1→{Z1,Z2,…,ZP}3…4Mi→{Zn1,Zn1,…,Znh}wheren1∪…∪nh={1…P}5…6Zi→{z17…8Zi,j→{z19X1→t110…11XP→t1和图2(d)两棵树,判别部件的功能在于:(1)向下推导多个语义部件;(2)向上表征整体目标.对目标S而言,每个判别部件相当于文法A的判别部件,表示局部特征集.而对语义部件而言,判别部件度量部件之间的空间共现性,即对应文法B的一条树边.联合文法支持多语义部件的关联计算,但关联越多,计算复杂度越高.假定文法C的语义部件为P个,每个部件的终文法推断.文法C仍然采用自上而下的结点搜Page7假定语义部件数为P,判别部件数为2P-1,文索和自下而上的得分计算.区别于文法B,文法C的整体目标得分取所有m产生式的最大值Sc(S)=maxi[Sc(Mi)].通过动态规划算法,文法C可以在多项式时间完成推断,其理论计算复杂度为O(l2·n),其中l为每个终结符类型可能存在的位置,n为文法的终结符总数.4.2.2文法训练法C的得分公式可表示为Sc(I)=maxy,zβ·Φ(I,y,z),wherez={z1,…,z2P-1}∈Ωandβ=(θ,λ)=(θX1,…,θXP,θz1,…,θz2P-1,…,λMi→{Zn1,…,Znh},…,λZi→{zi,Xi},…)(11)其中β,Φ同式(9)中的含义,y表示语义部件的格局(即所有语义部件的位置).式(11)表示在图像I中具有最佳格局的推导树得分即为目标得分.得分函数Sc带有隐参数狕代表判别部件的位置.这里将{θz1…θz2P-1…λMi→Zn1,…,Z{}nh…λZi→狕i,X{}i…}和{θX1…θXP}分别记为隐参数和目标参数,其中θ为终结符对应的特征权重,λ由式(5)和式(6)定义,表示空间相对位移的计算权重.文法C的训练目标是在给定部件层标注的条件下同时找到最佳的隐参数和目标参数.本文引入LatentStructuralSVM[11]解决式(11)的参数优化.LSSVM可描述为Lβ,z=minβ,ξ1s.t.for1kN,fory∈Ωy,z∈Ωz(12)ξk=maxy,z[β·Φ(Ik,y,z)+Δ(yk,y,z)-max珔zβ·ΦIk,yk,珔()z]对每个训练样本,式(12)通过搜索部件格局空间Ωy和隐变量空间Ωz以获得具有最小惩罚度的ξ值.式(13)的3个分项分别代表当前训练样本推断出的部件格局得分,标注格局与推断格局的差别度Δ以及标注格局得分.注意前两项的隐变量z与第3项的隐变量珔z可能具有不同赋值.差别度Δ定义为Δ(yk,y,z)=1代表推断到的语义部件与标注部件在覆盖率上的平均值.函数overlap(tki,ti)度量标注部件tki与推断部件ti的窗口覆盖率.显然覆盖率越大,差别度越小.本文具体采用CCCP[11]计算式(12),详细训练过程参见算法1.算法以迭代方式逐步计算隐变量和模型参数.首先是参数的初始化.算法需要确定每个判别部件Zi的初始位置.我们根据KT树的每条关联边e采用能量函数ENERGY(S+,e)自动获取判别部件的空间初始值aZi.ENERGY(·)对正样本的所有特征计算能量累积(能量为HOG特征值),并且定位具有最大能量值的局部区域.此方法与Latent-SVM[2]初始化判别部件的思想一致.确定判别部件的初始位置后文法C的结构即已确定,于是对文法内的每个产生式的空间参数赋初始值{0,0,0,1,1,1}.由于训练样本不提供终结符类型标注,对每个语义部件以及判别部件,算法采用聚类算法CLUSTER(例如k-means)将对应正样本集划分成K个子样本集s1,…,sK,其中语义部件预设K=4,判别部件预设K=8.每个样本集均使用常规SVM(·)训练相应的外观参数,于是每个非终结符可以得到一个外观参数向量θl={θ1l,…,θKl}.初始化参数后算法1重新扫描样本集,执行CCCP优化文法参数,这一过程简单的描述为:根据当前参数β在每个训练样本中搜索判别部件的位置,即确定隐变量狕的取值(算法1第2.1步);固定样本的狕值,执行常规的结构化SVM优化模型参数(算法1第2.2步).迭代τ次(默认为8)后训练完毕.限于篇幅,关于CCCP的详细步骤请参阅文献[11].算法1.联合文法的参数训练过程(伪代码).输入:S+={IM,yM},S-={IN,yN},迭代次数τ,判输出:β=(θ,λ)1.执行以下步骤初始化β01.1FOREACHe∈E(a)aZi←ENERGY(S+,e)(b)λi←{0,0,0,1,1,1}1.2FOREACHl∈犣∪犡(a)s1∪…∪sK←CLUSTER(S+l,al)(b)执行θkENDFOR2.FORt··=1:τ2.1zi←argmaxzβ·Φi,yi,z,fori∈S+2.2βt←Struc_SVM(Lβt-1,z),withS+∪S-ENDFOR算法结束.5实验与讨论为全面评估文法模型在各识别任务中的性能,实验使用3个标准的数据集:(1)IteratorImageParsing(IP)①.该数据集包含305张人体Pose,每个Pose由14个关节点标注;(2)LeedsSportPose①参见http://www.ics.uci.edu/~dramanan/papers/parse/index.html.Page8(LSP)①.LSP是当前专门针对姿态估计研究的大规模标注集,共包含2000张图像.每个人体实例标注14个关键点(同IP数据集);(3)带部件标注的PASCALVOC[14]②.该数据集对PASCALVOC2007/2010中的6种非刚性目标类(horse、cow、cat、bird、dog、sheep)进行了部件层标注,每个目标类别含6~9个语义部件.5.1单任务场景下的识别性能评估5.1.1姿态估计的性能评估首先通过数据集1和数据集2评估视觉文法在姿态估计场景下的识别性能.常规的姿态估计是在给定目标标注的前提下测试模型对预定义语义部件的定位率.实验选取3个模型进行评估:文法B、PS-DPM[19]和文法C.PS-DPM亦是整合PS与DPM来联合解决目标检测与姿态估计,但联合方式与文法C不同.基本思想是单独学习一个人体躯干的SVM分类器,该分类器用于确定人体的大致位置,然后考虑其他语义部件与躯干的相对位置来定位这些部件.姿态估计的标准指标是PercentageofCorrectParts(PCP).参见图3,相比文法B和PS-DPM,文法C在IP数据集上的部件定位率具有一定优势,这说明文法C所添加的判别部件能提高语义部件的检测效果.尤其是F-arms,该部件的训练样本的外观差异很大,导致分类器检测率最低.文法C使用双判别部件考虑F-arms与U-arms的联合出现,实际上增加了单个部件的表征能力.图3给定目标标注下的部件定位率(IP数据集)(注:Torso躯干,U-legs大腿,L-legs小腿,U-arms上臂,F-arms前臂,Head头部,Part-Ave各部件的平均定位率)图4在LSP数据集上展示了姿态估计性能.LSP的样本较IP数据集更多,姿态变化的形式更为丰富.3个模型的平均定位率差异不大,其中文法C较文法B约有2%的性能提升.与图3的结论类似,文法C利用判别部件所隐含的部件共现信息有效地指导了部件定位.这3个评估模型本质上都是基于PS结构建模部件关系,在目标内部检测部件时处理方式相仿.PS-DPM的躯干检测器只对目标特征建模,对部件定位的影响较小.图4给定目标标注下的部件定位率(LSP数据集)5.1.2目标检测的性能评估实验在数据集3上评估4个部件模型:文法A、文法C、S-DPM[14]和AOG[6].S-DPM是DPM的一个改进版本,采用部件层标注初始化部件,并在部件搜索中加上语义约束(即限定搜索到判别部件与标注的语义部件之间的覆盖率必须大于0.3),从而克服DPM搜索过程的盲目性[14].AOG[6]是AOG文法在目标检测上的最新应用.表4展示了评估结果.评测指标为AccuracyPercentage(AP).Horse57.0/41.163.1/48.765.8/50.558.9/48.2Cow25.4/21.034.9/26.035.2/25.827.7/23.6Cat25.4/21.026.5/26.122.9/24.023.0/21.6Bird9.9/9.512.4/11.313.7/11.611.0/10.8Dog11.1/10.318.8/23.518.2/22.513.1/22.9Sheep17.9/26.622.9/27.922.5/28.222.4/30.4Average23.43/21.8329.77/27.2529.72/27.1026.02/26.25不难看到文法C的平均检测率略低于S-DPM,两个模型同样采用判别部件作为检测目标的依据,区别在于文法C还进一步考虑了语义部件的外观得分与空间格局.这一结果表明了语义部件对整体目标检测作用不大.相比文法A,文法C有约6%的性能提升.文献[15]指出,弱监督环境下的目标模型应尽量选取语义相关的部件,有助于平衡目标特征的判别性与相关性.我们分析认为文法C的判别部件是从语义部件标注集当中自动产生,训练这些判别部件实际上是提高了目标特征的学习效率.图5(a)展示了文法C与文法A的部分检测结果.我们①②Page9看到,尽管两模型均能正确检测到目标(图5(a)的第1、2列),但文法C的检测结果与真实目标窗口的拟合度更佳.文法C和文法A均基于自动学习到的判别部件检测整个目标,但文法C的判别部件来自于多语义部件的联合出现,与整体目标具有更强图5文法C的识别结果与对比(注:点线框代表目标检测结果,实线框为姿态估计结果,5.2多任务场景下的识别性能评估现实环境下的姿态估计需要先识别目标的位置.本实验在数据集1和数据集2上分别评估3个模型在目标检测和姿态估计上的联合性能识别.为更好的评估联合文法,实验采用Recall-vs.-FalsePositivePerImage(Recall-FPPI)①观察部件定位率与目标检测率的相关性.给定FPPI的具体取值后PCP可通过(Chicago:part_recall)/(object_recall)计算得到.在IP数据集上(图6)文法C与其他两个模型的性能差距有明显扩大.文法C的部件识别率较PS-DPM平均有5%的性能提升(FPPI=1,2,3,4,5).PS-DPM尽管利用了躯干检测器辅助其他部件的定位,但性能受限于躯干的检测效果,联合识别过程存在错误传播,因而部件的实际定位性能会受的相关性,而文法A的判别部件来自于目标的全局搜索,这些部件可能包含大量噪声.对比两个基于文法的检测器,AOG[6]的检测率稍逊于文法C.一个重要的原因是AOG的候选部件数量过多,单部件的训练样本较少,导致检测器的学习效率受限.到影响.而文法C综合考虑了姿态估计得分(即所有语义部件的得分)与目标检测得分(即所有判别部件的得分),实现了检测与估计同时进行,不存在条件关系,有效地抑制了错误传播.其次,PS-DPM的扩展存在局限性,因为有些目标不存在类似人体躯干的稳定部件.图5(b)对比了文法C和PS-DPM的一些识别示例.对于目标检测,文法C的检测结果比PS-DPM更精确,与真实目标的覆盖度更大.从姿态估计的效果上看,PS-DPM仅在确定目标大致位置后搜索姿态,搜索过程容易受各种背景因素的干扰,典型的如多目标重叠(图5(b)的第1、2列).①SunM.Technicalreportofarticulatedpart-basedmodel.Page10图6联合目标检测与部件定位的性能评估(IP数据集)(PartAve部件平均定位率,Obj目标检测率)对比文法C和文法B,尽管两者在图3中某些部件的识别性能差别不大,例如torso为2%、U-legs为1%,平均相差约2%.但在图6中差距扩大5%.这说明文法C在实际环境中具有更高的应用价值.对于LSP数据集(图7),文法B的平均部件定位率下降幅度较大,而PS-DPM和文法C则能有效地利用目标特征辅助部件定位.对比PS-DPM和文法C的“Obj”曲线,文法C的目标检测率优于PS-DPM约5%(FPPI=1,2).分析认为,PS-DPM仅提取某个局部区域(躯干)的判别特征来检测整体目标,而文法C则考虑了目标所有的局部区域,因而判别能力更强.图7联合目标检测与部件定位的性能评估(LSP数据集)识别效率.我们观察文法C与S-DPM在训练与测试上述6个目标类的时间.在HPBL460服务器(Intel2.4GHz四核,16GB内存)上训练一个指定目标类,文法C需要平均花费约7h,S-DPM平均值约5h.一个主要原因是LatentStructuralSVM的训练时间长于LatentSVM.文法C从指定图像中(以800×600像素计)检测目标和定位部件需约5s~10s.若采用先检测后定位的方式,例如文法A+文法B,实测中至少需要20s以上的推断时间.6总结尽管部件模型在当前大多数视觉识别任务中获得了成功应用,但其结构固定不宜扩展.文法作为一种开放式建模方法具有更强的应用扩展能力.本文定义了一个面向一般识别任务的判别文法模型,并使用该模型解决目标检测和姿态估计问题.大多数的视觉识别问题均可采用结构化建模方式,例如事件检测(EventDetection)、基于属性的目标识别(Attribute-basedRecognition)等.我们下一步将扩大文法的应用范围,进一步研究其他任务之间的相关性,设计更有效的联合文法模型.影响结构化模型性能的一个重要因素是标注训练集的规模.而当前的现状是已有的标注集无论在质量还是数量上均难以支撑各类学习算法,例如构建一棵完整的视觉分析树需要对所有结点进行标注.大多数结构化模型通过聚类或半监督方式自动获取结点标注,这类算法的缺点在于训练过程难以跟踪,无法保证训练参数持续收敛.文法可通过规则干预模型的构建、计算与训练,这意味着研究者可以根据业务逻辑加入启发式规则,增强模型在学习和推断过程中的鲁棒性,这将是视觉文法的另一条研究途径.
