Page1DBCC-Join:一种新的高速缓存敏感的磁盘连接算法韩希先1)杨东华2)李建中1)1)(哈尔滨工业大学计算机科学与技术学院哈尔滨150001)2)(哈尔滨工业大学基础与交叉科学研究院高性能计算中心哈尔滨150001)摘要随着CPU和内存的性能差距越来越大,系统设计者在CPU寄存器和内存之间插入高速缓存来弥补这个差距.高速缓存的数据存取速度远高于内存,所以数据库操作要获得更好的性能就必须考虑充分利用高速缓存.基于磁盘的连接操作是一种常用并且耗时的数据库查询操作,可是大多数传统的连接算法在设计时都没有考虑高速缓存的使用,从而使得这些连接算法无法充分利用CPU的能力.文中分析了传统的连接算法在高速缓存利用方面的问题,并且提出了一种新的可以充分利用高速缓存的磁盘连接算法DBCC-Join.连接位置索引对表JPIPT是用到的数据结构,说明了每个连接结果元组在各自表中的位置索引对.DBCC-Join的执行包括两个阶段:JPIPT构建阶段和结果输出阶段.JPIPT构建阶段对列存储化的连接属性执行高速缓存敏感的算法来构建连接位置索引对表.利用获得的JPIPT,结果输出阶段只需要对数据表执行一遍顺序扫描就可以获得结果.该文是第一篇提出利用高速缓存的磁盘连接算法的文章.实验表明,和传统磁盘连接算法相比,DBCC-Join算法可以获得一个数量级的加速比.关键词DBCC-Join;JPIPT构建阶段;结果输出阶段;缓存敏感算法1引言摩尔定律预测CPU的处理速度每18个月增长一倍,主存的存取速度也有类似的定律,可是其增长速度却比CPU的要慢得多(每6年提高一倍).如图1所示[1],这使得CPU和内存之间的性能差异越来越大(每21个月差距增大一倍).系统设计者在CPU寄存器和主存之间插入高速缓存来弥补这个差距.高速缓存的数据存取速度远高于内存,如果数据存储在高速缓存中,引用该数据需要几个CPU时钟周期;如果数据存储在主存,则需要几十个甚至几百个时钟周期才可以访问该数据.所以,数据库操作要获得更好的性能,就必须考虑充分利用高速缓存[2].连接操作是一种常用的关系数据库查询操作,它可以根据公共信息将两个或多个表的元组组合起来.连接操作也是数据库操作中最耗时的操作之一,其性能直接影响着数据库的整体性能.连接操作已经得到了广泛和深入的研究[3],并且已经提出了一些经典的连接算法:nested-loop方法、Sort-merge连接方法和基于hash的连接方法.大多数传统的连接算法在设计时没有考虑高速缓存的使用,从而使得这些连接算法无法充分利用CPU的能力[4-5].现有的研究工作提出了新的利用高速缓存的内存连接算法[6],可是这些方法只适用于内存数据库.而在实际应用中,用户需要更多的是基于磁盘的连接操作.本文分析了传统磁盘连接算法在高速缓存利用中存在的问题,并且提出一种新的高速缓存敏感的磁盘连接算法DBCC-Join(Disk-BasedCache-Con-sciousJoin).该算法解决了现有连接算法的低缓存利用率问题.在本文中,数据表根据列存储模式维护.连接位置索引对表JPIPT(JoinPositionalIndexPairTable)是DBCC-Join用到的数据结构,说明了每个连接结果元组在各自数据表中的位置索引对.DBCC-Join的执行分为两个阶段:JPIPT构建阶段JCS(JPIPTConstructionStage)和结果输出阶段ROS(ResultOutputStage).JCS类似于普通的连接操作,可是该阶段只处理连接属性.本文提出一种缓存敏感的构建算法来快速构建JPIPT.ROS的执行包括3个连续操作:JPIPT划分、外关系扫描和内关系扫描.我们设计了元组抽取算法来保证只需要对关系表执行一次顺序扫描就可以获得结果.本文还提供了DBCC-Join算法的正确性证明.需要注意的是,JCS要求连接属性列存储化,可是在ROS中,只要存储模型支持利用位置索引来抽取元组,DBCC-Join就可以应用于该存储模型.这实际上包括了目前的所有存储模型(行存储和列存储).本文作者设计了较全面的实验分别从元组数量、表宽度、输出属性数量和连接选择度4个方面来评价DBCC-Join的性能.实验表明,和传统磁盘连接算法相比,DBCC-Join算法可以获得一个数量级的加速比.本文的主要贡献如下:(1)本文分析了传统磁盘连接算法在高速缓存利用中存在的问题,提出一种新的高速缓存敏感的磁盘连接算法DBCC-Join.据我们所知,本文是第一篇提出利用高速缓存的磁盘连接算法的文章.(2)本文提出一种高速缓存敏感的JPIPT构建算法来获得连接位置索引对表.(3)利用获得的JPIPT,本文提出元组抽取算法只需要对数据表执行一遍顺序扫描来获得结果,并且提供理论分析证明了算法的正确性.(4)本文通过较全面的实验来评价DBCC-Join的性能.实验结果表明,和传统的磁盘连接算法相比,DBCC-Join算法可以获得一个数量级的加速比.本文第2节介绍高速缓存结构;第3节分析传统Join算法在利用高速缓存方面的问题;DBCC-Join算法在第4节中详细描述;第5节是性能评价部分;相关工作和结论分别在第6和第7节中给出.2高速缓存结构计算机读取数据的顺序是先查看需要的数据是否在寄存器,如果不在就继续检查是否在高速缓存,如果也不在就继续检查是否在内存和磁盘.如果Page3CPU请求的数据存储在高速缓存,称为缓存命中(cachehit),否则称为缓存不命中(cachemiss).高速缓存包括L1高速缓存和L2高速缓存.本文主要考虑L2高速缓存,因为L2cache比L1cache要大得多,并且L1datacachemiss对性能的影响要远小于L2datacachemiss的影响[5].图2表示了存储器的层次结构.高速缓存由昂贵的静态RAM构成,所以其容量一般不大,一个桌面系统的高速缓存一般不会超过几M字节.高速缓存被划分成一组固定大小的高速缓存行(cacheline),其大小在16~256字节.当高速缓存从主存中读取数据时,即使只需要一个字节,高速缓存也会从主存中读取可以填满整个cacheline的数据.如果计算机程序具有较好的局部性,即程序倾向于访问相同的数据项集合,或者倾向于访问临近的数据项集合,则系统就可以很好地利用缓存来提高程序的性能.与主存和磁盘不同,人们无法直接利用软件来控制哪些数据放置到高速缓存,因为高速缓存是由硬件控制的.所以要充分利用高速缓存,我们的程序就必须具有良好的局部性,根据高速缓存的数据读取特点来设计程序操作.本文我们主要考虑如何利用缓存来提高磁盘连接操作的性能.3当前连接算法利用高速缓存的问题连接操作是数据库系统中的一个重要并且耗时的操作,其性能对于整个数据库系统的性能有着很大的影响.人们提出了很多的连接算法:nested-loopjoin、sort-mergejoin和hashjoin算法等.Hashjoin由于其较好的性能更多地被人们所采用,所以本文以hashjoin为例来讨论连接算法.本文讨论的hashjoin的形式是GRACEhashjoin[3],一方面因为GRACEjoin具有严格区分的划分阶段和连接阶段,方便我们对性能的分析,另一方面由于我们处理的数据量较大,hybridhashjoin[3]和GRACEhashjoin的性能差距不大.本节中,我们分析hashjoin算法在利用高速缓存方面存在的问题.(1)Tuple-oriented处理模式.当前主流数据库采用行存储模式,同一元组的数据连续存储,并且采用tuple-oriented处理模式.在执行查询处理时,tuple-at-a-time迭代器每次读取一个元组,提供给上层的关系操作符进行处理.假设高速缓存的cacheline大小是Wcl,关系的元组宽度是Wtuple,在tuple-at-a-time迭代器模型下,高速缓存读取tuple的过程如图3所示.高速缓存每次执行主存读时,会同时获得Wcl字节的数据放入cacheline,加快接下来的读取操作.所以,执行一次内存读,高速缓存会读取Wcl/Wtuple个元组.典型的cacheline大小是64字节,而数据表的属性数量一般都较大,几十个属性的数据表是很常见的.一般来说Wcl<Wtuple,使得每次读取下一个元组都需要执行主存读操作,所以tuple-oriented处理模型并没有很好地利用高速缓存.(2)较大的内关系块.Hashjoin划分阶段把内关系划分成多个块,使得每个块都小于内存的容量,再对外关系执行相应的划分,划分后内关系的每个块通常比高速缓存大得多.内关系构建Hash表完毕后,外关系的元组开始探测Hash表以得到连接结果.内关系构建Hash表时,为保证外关系探测Hash表的效率,Hash表的桶数较大.由于Hash函数的随机性以及较大的内关系块,当前的内关系元组映射到的Hash桶所在的数据页一般不在高速缓存,即每次内关系元组插入Hash表的操作通常引起内存读.同样,在外关系探测阶段,被探测桶所在的数据页一般也不在高速缓存中,从而使得每次探Page4测Hash表的操作都会引起一个内存读.基于同样的原因,hashjoin的Hash表构建和探测过程中,当前需要读取的数据页很可能没有存储在TLB(翻译后备缓冲器,存储最近处理过的64个虚拟页-物理页地址影射,如果当前请求的虚拟页地址转换没有存储在TLB,则说发生一次TLBmiss操作),从而使得每次主存读写操作还需要执行虚拟页-物理页地址转换,进一步增加了其执行费用.我们知道,具有良好局部性的程序比局部性差的程序运行得更快.局部性通常有两种形式:时间局部性和空间局部性.时间局部性指的是重复引用相同的数据项集合,而空间局部性指的是倾向于引用当前访问数据附近的数据项集合.很明显,连接操作不具有良好的时间局部性,因为它处理完一个元组后就继续处理下一个元组.我们利用步长为k的引用模式来讨论空间局部性,具有步长为k的引用模式指的是顺序访问连续空间第k个字节的数据,k表示跳过的字节数.步长越小,则空间局部性就越好,在存储器中以大步长访问不同位置的数据的程序不具有良好的空间局部性.处理每个元组时,连接操作具有步长为Wtuple的访问模式,Wtuple的值一般较大,则关系的顺序扫描操作不具有良好的空间局部性.Hash函数的随机性以及较大的内关系块使得构建和探测Hash表具有较大的步长,所以构建和探测Hash表的操作不具有良好的空间局部性.总的来看,现有的hashjoin算法不具有良好的局部性,从而无法充分利用高速缓存的能力.下一节中,我们提出一种新的高速缓存敏感的磁盘连接算法DBCC-Join,该算法可以充分利用高速缓存以提高磁盘连接操作的性能.4DBCC-Join算法通过分析,我们知道传统连接算法较低的高速缓存利用率的主要原因在于:较大的引用步长和较大的内关系分片.DBCC-Join通过减少引用步长和内关系分片大小的方法来获得较好的高速缓存利用率.减小引用步长的一个方法是不采用面向元组的处理模式,DBCC-Join算法采用列存储策略[7]存储关系,关系不是一行行地连续存储,而是把关系的每个属性的值顺序存储到单独的文件.采用列存储的好处不但可以减小扫描操作的引用步长,在查询处理阶段,还可以只处理相关的属性,而忽略其他的属性.DBCC-Join的执行分为两个阶段:JPIPT构建阶段(4.1节)和结果输出阶段(4.2节).4.1节介绍如何利用高速缓存来快速构建JPIPT.利用获得的JPIPT,4.2节介绍了如何获得最终的连接结果.4.1JPIPT构建阶段JCS(JPIPTConstructionStage)假设T1和T2是参与连接操作的数据表.在执行阶段,T1作为外关系而T2作为内关系.用户提交的查询Q的形式为“selectattribute-listfromT1,T2whereT1.key=T2.key”.我们首先给出本文用到的一些定义.定义1(位置索引).给定表T,元组t∈T的位置索引PI(PositionalIndex)是i,如果t是T的第i个元组.我们用T(i)来表示表T中位置索引为i的元组.定义2(连接位置索引对表).给定表T1和T2,T1.key1和T2.key2分别是T1和T2的连接属性,T1T2包括N个结果,JPIPT是T1T2的连接位置索引对表,如果JPIPT满足条件:(1)|JPIPT|=N,(2)i,j,(PI1:i,PI2:j)∈JPIPT,当且仅当T1(i).key1=T2(j).key2.在本文中,我们把JPIPT看作具有两个属性(PI1,PI2)的表.DBCC-Join把T1和T2的连接属性存储为两个独立的列文件:C1和C2.每个列文件都包含一个隐含的属性:PI.C1和C2的模式可以分别看作C1(key1,PI1)和C2(key2,PI2),其中,key1和key2分别是T1和T2的连接属性,PI1和PI2分别表示key1和key2在表T1和T2的位置索引.JPIPT的构建操作就是执行如下的连接操作:“selectPI1,PI2fromC1,C2whereC1.key1=C2.key2”.传统的磁盘hashjoin并没有考虑高速缓存,从而导致了较低的高速缓存和CPU利用率.本文提出高速缓存敏感的JPIPT构建算法C3-JPIPT算法(Cache-ConsciousConstructionofJPIPT),该算法充分利用高速缓存来更快地构建JPIPT,其执行过程如算法1[8]所示.注意:本文涉及的所有算法的伪代码都包含在文献[8]中.C3-JPIPT的执行包括两个步骤:划分和连接.划分把每个表划分成Np1=2×(Sc1+Sc2){C1i}和{C2i}(1iNp1).其中,SC1和SC2分别表示列文件C1和C2的大小,M表示C3-JPIPT利用的内存容量.C3-JPIPT产生比传统Hash连接操作更多的分片数量,这是因为该算法的连接阶段要求同时把内关系和外关系的单个子表对读入内存.在划分时,C3-JPIPT实例化每个元组的位置索引值.注意:如果和外关系相比,内关系非常小,那么JCS就采用传统的连接算法而不是C3-JPIPT.Page5当C3-JPIPT进入连接操作时,每一对子表C1i和C2i依次读入内存,并且划分为Np2=SC2iC个分片,使得每个内关系子表的分片可以放入高速缓存.其中,SC2i表示读入内存的内关系子表大小,C表示高速缓存的大小.如果Np2>64,则该划分操作采用多轮划分方法来减少TLBmiss.每一轮划分的分片数量少于64,并且前一轮获得的分片进一步划分成多个子分片.该过程继续执行直到获得的分片数量达到Np2.C3-JPIPT采用的多轮划分方法类似于Radix-cluster算法[6].经过第二次的划分,C1i和C2i分别被划分为两个含有Np2个分片的集合.Hash连接操作在C1i和C2i的每个分片对上执行.因为当前的内关系的分片可以放入高速缓存,所以构建和探测Hash表的操作都可以在高速缓存中执行.C1i和C2i的第j分片对的连接结果是第j分片对对应的连接位置索引对表JPIPTij.我们在内存中维护JPIPTij以方便接下来的操作.定理1.产生的JPIPTij根据PI1域排序.证明.k1,k2(k1<k2),(PI1:mk1,PI2:nk1)∈JPIPTij,(PI1:mk2,PI2:nk2)∈JPIPTij.假设在JPIPTij中,(PI1:mk1,PI2:nk1)的出现要早于(PI1:mk2,PI2:nk2).这意味着,读取T1(mk1)的时间不能晚于读取T1(mk2)的时间.已知在连接操作中,T1作为外关系,并且T1的元组按顺序读取来探测Hash表,所以mk1mk2.C1i和C2i的连接操作产生Np2个连接位置索引对表.根据定理1,每个JPIPTij根据PI1域排序.我们利用一次多路合并排序操作来合并JPIPTij(1jNp2)获得JPIPTi,JPIPTi输出到磁盘.类似的操作在每个C1i和C2i(1iNp1)上执行,获得Np1个JPIPTi文件.此时,再一次执行多路合并排序来合并JPIPTi(1iNp1)获得最终的JPIPT.4.2结果输出阶段ROS(ResultOutputStage)(PI1:i,PI2:j)∈JPIPT意味着T1(i)和T2(j)满足连接条件.利用JPIPT,一个直观的输出结果的想法是根据PI1和PI2域的值,抽取表T1和T2的指定位置索引的元组,分别存储到文件R1和R2.R1和R2中具有相同的新位置索引(Ri的第一个元组具有新位置索引1)的元组对构成一个连接元组.这个想法比较简单,可是其效率却较低.JPIPT的PI1域是排序的,对T1的一次顺序扫描就可以获得需要的T1元组.可是,JPIPT的PI2域却不是排序的,根据PI2的域指定的位置索引来抽取T2元组会引起大量的磁盘seek操作,执行效率较低.在本节讨论时用到的参数定义如表1所示.在结果输出阶段中,有3个步骤顺序执行:JPIPT划分、外关系扫描和内关系扫描.JPIPT划分过程把JPIPT划分为多个分片来保证外关系扫描和内关系扫描的内存需求.外关系扫描利用PI1域来抽取T1元组,内关系扫描则利用PI2域来抽取T2元组,当然只返回attribute-list涉及到的属性.接下来,我们依次介绍3个步骤的执行过程.符号SJPIPTSJTST24.2.1JPIPT划分JP(JPIPTPartition)在DBCC-Join中,外关系扫描要求每个JPIPT分片可以放入内存,内关系扫描则要求每个JPIPT分片和对应的T2元组都可以放入内存.JP独立地把JPIPT划分成两个不同的分片集合:PS1和PS2.其中PS1包括m=SJPIPTM个分片,PS2包括n=SJPIPT+N×ST2M个分片.PS1的第k个元素PS1(k){(i,j):(i,j)∈JPIPT,i∈[PI1k-1,PI1k],PI1k=k×N1m,1k}m.[PI1k-1,PI1k]是T1的位置索引的一个连续子范围(为方便讨论,假设T1和T2的连接属性在其值域内均匀分布).[PI1k-1,PI1k]满足i,j(i≠j),[PI1i-1,PI1i]∩[PI1j-1,PI1j]=,∪mi=1[PI1i-1,PI1i]=[1,N1].类似的,PS2的kth元素PS2(k){=(i,j):(i,j)∈JPIPT,i∈[PI2k-1,PI2k],PI2k=k×N2n,1kn}),[PI2k-1,PI2k]是T2位置索引的一个连续子范围.[PI2k-1,PI2k]满足i,j(i≠j),[PI2i-1,PI2i]∩[PI2j-1,PI2j]=,∪ni=1[PI2i-1,PI2i]=[1,N2].JPIPT划分过程的子范围划分如图4所示.JPIPT构建阶段最后的多路合并排序操作和JPIPT划分操作结合为连续的操作,从而节省Page6JPIPT的一次读和一次写操作.JPIPT划分只需要对JPIPT进行一次扫描就可以把JPIPT元组分配到对应的PS1和PS2元素中,其执行过程如算法2[8]所示,该过程具有步长为8的引用模式.4.2.2外关系扫描ORS(Outer-RelationScan)这部分介绍如何利用PS1来抽取T1元组.在ORS过程中,PS1从第一个元素PS1(1)开始遍历.根据定义,PS1(1)中PI1的值在范围[PI10,PI11],并且根据PI1的值排序.ORS从涉及的T1表的起始位置开始扫描,抽取PS1(1)中PI1值指定的T1元组.注意,获得的结果元组不是输出到单个文件,而是根据该元组对应的JPIPT元组的PI2值输出到n个文件的某一个文件.给定JPIPT元组(PI1,PI2)指定的T1元组t,如果PI2∈[PI2j-1,PI2j],元组t输出到文件file1_1_j(1jn)).继续遍历PS1的元素,得到PS1(2)的数据.根据PS1的划分规则,PS1(2)中的PI1的值在范围[PI11,PI12].i0,i1,j0,j1,(i0,j0)∈PS1(1),(i1,j1)∈PS1(2),则必然有i0i1.所以ORS可以从当前T1的偏移位置继续扫描,而不需要重新从头开始扫描.该处理过程类似于对PS1(1)的讨论,根据对应的JPIPT元组的PI2值,T1元组输出到对应文件file1_2_j(1jn)).该过程继续执行直到PS1的元素都处理完毕.图5给出了ORS的一个示例.此时,我们获得一组文件FS1={file1_i_j,1im,1jn}.ORS的执行过程如算法3[8]所示.ORS执行采用批量读写机制,当需要读取连续的大块数据时,每次读操作不是读取单个元组,而是一次性读取多个数据块,从而分摊了磁盘寻道和旋转时间.但是,当一次性读取的数据块增长到一定程度的时候,批量读取带来的增量收益就会变得很小.根据实验的机器配置,本文的批量读操作一次性读取4个数据块.类似的,每个输出文件维护一个输出缓冲区,当该缓冲区满的时候才真正执行磁盘写操作.4.2.3内关系扫描IRS(Inner-RelationScan)这部分介绍如何利用PS2来抽取T2元组.由于JPIPT的PI2域不是排序的,所以IRS的执行过程比ORS更加复杂.在IRS过程中,PS2从第一个元素PS2(1)开始遍历.根据定义,PS2(1)中PI2的值在范围[PI20,PI21].要对T2执行顺序扫描来获得结果元组,PS2(1)先根据PI2的值执行一次排序操作.因为PS2(1)的大小通常都要大大超过高速缓存的容量,并且常用的排序算法具有较大的引用步长,本文提出一种新的缓存敏感的排序算法C2-Sort来更快地完成排序操作,其执行代码如算法4[8]所示.C2-Sort的基本思想是:不同于一次性对大数据块执行排序操作,我们先把数据块划分成多个子块,使得每个子块可以放入高速缓存,然后依次对每个子块执行排序操作,最后利用一次多路归并排序来合并排好序的子块.在每一个执行步骤中,C2-Sort都可以较好地利用高速缓存从而获得较好的性能.我们排序PS2(1)的方法不是一次性把它读入内存,而是每次读入C/SJT个JPIPT元组并且对这些元组执行排序.排好序的元组在内存中维护.该过程持续进行直到所有的PS2(1)的数据都读入内存,此时一次合并操作就可以把排好序的子块合并为单个排好序的PS2(1).PS2(1)排序完毕后,IRS从涉及的T2表的起始位置开始扫描,因为此时PI2的值是排序的,所以顺序扫描足够抽取PS2(1).PI2指定的T2元组.IRS在内存中缓存获得的元组.当所有的PS2(1)指定的T2元组都获得后,IRS利用C2-Sort根据PS2(1):PI1对缓存的T2元组执行第二次排序操作,来保证结果的正确性(正确性证明在4.2.4节).每个排好序的T2元组根据对应的JPIPT元组的PI1的值来决定输出到m个文件中的某一个.如果PI1的值落入范围[PI1i-1,PI1i],则对应的T2元组输出到文件file2_i_1(1im).继续遍历PS2的元素,得到PS2(2)的数据.根据定义,PS2(2)中的PI2的值在范围[PI21,PI22].Page7i0,i1,j0,j1,(i0,j0)∈PS2(1),(i1,j1)∈PS2(2),则必然有j0j1.所以IRS可以从当前T2的偏移位置继续扫描,而不需要重新从头开始扫描,该处理过程类似于对PS2(1)的讨论,此时连接结果根据对应的JPIPT元组的PI1值输出到文件file2_i_2(1im).IRS的执行过程见文献[8]算法5.该过程继续执行直到PS2的元素都处理完毕.我们获得一组文件FS2={file2_i_j,1im,1jn}.此时,ROS阶段执行完毕.我们获得m×n对文件:file1_i_j和file2_i_j(1im,1jn),在每对文件中,具有相等的新位置索引的元组构成连接操作的结果元组.4.2.4正确性证明这部分证明ROS的正确性.我们首先证明定理2的正确性.在定理2中,没有内存限制的ROS不包括JPIPT划分阶段,即m=1和n=1.定理2.没有内存限制的ROS生成的结果是正确的.证明.假设BUF_T1保存ORS操作获得的T1元组,BUF_T2保存IRS操作获得的没有经过第二次排序的T2元组.JPIPT的PI1域的数据可以看图6定理2证明实例由于PI1和PI2根据各自范围进行划分,PS1和PS2的元素不存在直接的关系.我们用矩阵犛来构建PS1和PS2的元素之间的联系,犛的第i行对应于PS1(i),犛的第j列对应于PS2(j),犛(r,c)={(i,j):(i,j)∈JPIPT,i∈[PI1r-1,PI1r],j∈[PI2c-1,PI2c],1rm,1cn}.注意,犛并没有像PS1和PS2那样被实例化,只是虚拟地维护指定数据的范围.定理3.ROS生成的结果是正确的.证明.ROS阶段产生m×n对文件:file1_i_j和file2_i_j(1im,1jn).犛的元素S(i,j)对应文件对file1_i_j和file2_i_j,这是因为a,b,(a,b)∈S(i,j)满足如下条件:(1)a∈[PI1i-1,PI1i],(2)b∈[PI2j-1,PI2j].S(i,j)是JPIPT的子序列,已知JPIPT根据PI1排序,则S(i,j)也根据PI1排序.定理2证明file1_i_j和file2_i_j的正确性,所以i,j,file1_i_j和file2_i_j都产生正确的结果.证毕.作序列S1={i1,i2,…,iN},PI2域的数据可以看作序列S2={j1,j2,…,jN}.由于JPIPT根据PI1排序,所以i1i2…iN,BUF_T1保存的元组具有新的位置索引P1={1,2,…,N},并且P1的元素和S1的元素具有1-1对应关系(虽然S1可能存在重复值,可是这些重复值表示相同的T1元组,所以其顺序不会影响结果的正确性).我们把S1和P1结合而获得一个新的序列SP1={(i1,1),(i2,2),…,(iN,N)}.类似的,我们获得SP2={(j1,(i1,1)),(j2,(i2,2)),…,(jN,(iN,N))},其中(jk,(ik,k))表示T2(jk)是JPIPT(k)对应的连接结果的一部分.因为SP2不是根据jk排序的,为方便对T2的顺序扫描来获得结果,IRS对SP2根据jk排序,获得SP2={(j1,(is1,s1)),(j2,(is2,s2)),…,(jN,(isN,sN))},j1j2…jN,{s1,s2,…,sN}是P1的一个排列.很明显,BUF_T1和BUF_T2中具有相同的新位置索引的元组不是连接操作的结果元组,因为通常sk≠isk.要保证结果的正确性,BUF_T2需要执行另一次根据sk排序的操作.由于S1和P1的元素有1-1对应关系,所以BUF_T2也可以根据isk排序,即根据对应的PI1排序.该证明过程如图6所示.证毕.5性能评价和分析我们通过实验来评价DBCC-Join的性能.由于现有的利用高速缓存的连接算法[4,6,9-11]只适用于内存数据库,而本文的重点是海量数据上的磁盘连接算法,所以本文的实验部分比较DBCC-Join和传统的hashjoin算法.我们用Java实现hashjoin和DBCC-Join,jdk版本是jdk-6u17-windows-x64,实验程序在HPxw8600workstation(8×2.8GHzXeonCPU+6MBL2cache+32GB内存+1.4TB硬盘+64bitwindows)上运行.我们用tpc-h生成元组定长的实验数据,采用的数据表是lineitem和orders.1sf的lineitem和orders表分别含有6M和1.5M条元组,lineitem的每个元组为160字节,orders的每个元组为128字节,1sf数据表的大小分别是0.89GBPage8和0.18GB.实验中采用的查询Q如下所示:实验从4个方面评价DBCC-Join算法的性能:元组数量、表宽度、输出属性数量和连接选择度.考虑的元组数量是30sf、60sf、90sf、120sf和150sf.令tpc-h的默认表模式作为表宽度1(默认值),表宽度2则表示把tpc-h的表模式扩大一倍,即重复一遍初始表模式,以此类推得到表宽度为3、4和5的数据集.输出属性数量考虑:表模式的20%、40%、60%(默认值)、80%和100%的属性.本文考虑相对连接选择率,连接属性l_orderkey和o_orderkey在给定的值域范围(0,MAX)内随机分布.默认情况下,MAX=2×N1,N1表示lineitem表的元组数量,假设当MAX=1×N1时的选择率为1,则默认选择率为1/2.讨论连接选择度的效果时,我们考虑的相对选择度分别为1、1/2、1/3、1/4和1/5.在实验中,内存使用量限制为8G.5.1实验1:元组数量的效果实验1.评价在元组数量变化的情况下DBCC-图7元组数量变化时执行时间的比较5.2实验2:表宽的效果实验2.评价在表宽度变化的情况下DBCC-Join的性能,其中,sf设置为30,输出属性比例设置Join的性能,其中,表宽设置为1,输出属性比例为60%,相对选择率为1/2.如图7(a)所示,在实验1中,DBCC-Join的执行时间平均比hashjoin快7.26倍.该加速比的获得一方面是由于DBCC-Join只需要对原数据表执行一次扫描就可以完成,而传统的hashjoin则需要对原数据表执行多次扫描操作,另一方面由于是DBCC-Join充分利用高速缓存而获得的更好的性能.JPIPT构建阶段执行Hash表的构建和探测操作时,采用多次划分操作使得内关系子分片可以放入高速缓存,然后对每个子分片执行Hash操作,Hash操作的结果在输出之前还需要执行归并排序操作,使得输出的位置索引对根据外关系的位置索引排序,如图7(b)所示,即使需要如此多的步骤来执行内存内的Hash操作,DBCC-Join的执行时间仍然比不考虑高速缓存的Hash操作要快5.62倍.图7(c)和(d)说明在内关系扫描阶段,采用C2-Sort算法比普通的排序算法运行得更快,分别获得了1.79和1.51倍的加速比.为60%,相对选择率为1/2.如图8(a)所示,在实验2中,DBCC-Join的执行时间平均比hashjoin快了10.58倍.通过实验我Page9们发现,在其它条件不变的情况下,增加表宽度会提高DBCC-Join对hashjoin的加速比,这是由于在固定元组数量的情况下,JPIPT构建阶段的执行时间不会随着表宽的增大而增加,而且如图8(b)所示考虑高速缓存的Hash操作比不考虑高速缓存的Hash操作要快5.31倍,而hashjoin的划分和连接操作都会随着表宽的增大而增加.图8(c)和(d)表图8表宽变化时执行时间的比较5.3实验3:输出属性数量的效果实验3.评价在输出属性数量变化的情况下DBCC-Join的性能,其中,sf设为30,表宽设置为1,相对选择率为1/2.如图9(a)所示,在实验3中,DBCC-Join的执行时间平均比hashjoin快了6.44倍.我们发现,在输出属性从80%提高到100%时,执行时间的增长幅度较大,这是由于lineitem和orders表的最后一个属性l_comment和o_comment比较长,该属性的字节长度分别占整个元组长度的1/3和1/2.在固定元组数量的情况下,JPIPT构建阶段Hash操作的时间不会随着输出属性数量的变化而变化,如图9(b)所示.可以看到,考虑高速缓存的Hash操作比不考虑高速缓存的Hash操作仍然具有较大的优势.随着输出属性的增加,如图9(c)和(d),采用C2-Sort算法比不考虑高速缓存的排序算法运行得示在右扫描阶段中,采用C2-Sort算法比不考虑高速缓存的排序算法运行得更快,分别获得了1.53和1.24倍的加速比,和实验1相比,C2-Sort(orders)获得加速要小一些,这是由于随着表宽的增大,更多的属性需要执行排序操作,这使得合并操作的费用增加,从而减小了所获得的加速比.更快,分别获得了1.98倍和1.63倍的加速比.5.4实验4:连接选择度的效果实验4.评价在输出属性数量变化的情况下DBCC-Join的性能,其中,sf设为30,表宽设置为1,输出属性比例设置为60%.如图10(a)所示,在实验4中,DBCC-Join的执行时间平均比hashjoin快了6.35倍.可以看到,随着相对选择度的降低,hashjoin和DBCC-Join的执行时间也在下降,但是DBCC-Join的下降幅度更大.这是由于,更小的选择度使得结果输出阶段所需要处理的数据越少,所以内关系扫描和外关系扫描所耗费的时间基本呈线性下降.DBCC-Join的70%的时间花费在结果输出阶段,所以该阶段所耗费的时间随着选择度降低而线性下降,使得DBCC-Join的整体时间下降幅度较大.而hashjoin的划分阶段不受选择度变化的影响,在连接阶段的Hash表构Page10图9输出属性数量变化时执行时间的比较图10连接选择度变化时执行时间的比较Page11建的费用也不变,选择度降低只是会使得Hash表的探测阶段的时间降低,所以选择度对hashjoin的影响没有对DBCC-Join那么大,所以DBCC-Join比hashjoin的性能优势逐渐变大.如图10(b)所示,在执行Hash构建操作时,随着选择度的降低,其执行时间也在逐渐下降,但是和不考虑高速缓存的算法相比,高速缓存敏感的Hash算法仍然获得了6.06倍的加速比.同样,如图10(c)和(d)所示,选择度降低使得内关系扫描需要排序的数据也减少,从而减少了执行时间,但是相对于普通的排序算法,C2-Sort仍然获得了1.69和1.59倍的加速比.6相关工作连接操作是数据库中的一种基本并且耗时的操作,所以得到了研究人员的广泛关注.Nested-loop算法、Sort-merge算法、GraceHashjoin和HybridHashjoin是4种常用的连接算法[3,12].Bucketskipmergejoin[13]的方法假定关系表已经根据连接属性排序,并且虚拟地把数据表划分成多个分片,利用每个分片的连接属性的最大值和最小值信息来跳过不满足连接条件的元组及分片,从而提高了连接算法的性能.Graefe[14]利用5种方法来改进Hybridhashjoin的性能:压缩、大规模的机群及多层递归、内关系和外关系的角色转换、利用数据分布的柱状图以及多表连接的算法等.Chen等[15-16]利用预取机制来充分利用计算机的硬件特点,提高了hashjoin的性能.本文利用位置索引对信息来减少所需要的IO代价,加快join查询的处理速度,其概念和Valduriez[17]提到的连接索引类似.Lei等[18]提出stripe-join方法处理join操作,可是该算法假设已经得到所需要的连接索引.大多数现有的数据库查询算法并没有充分利用计算机硬件,从而没有充分发挥CPU的能力[4-5].一部分研究人员提出利用缓存的join算法.Shatdal等[2]提出缓存划分算法,使得在划分阶段把内关系的每个分片划分得尽可能小,使得每个分片可以放入高速缓存,但是该方法引起昂贵的磁盘IO费用.Boncz等[4]指出了数据库操作的新瓶颈是内存存取,而不是通常认为的IO存储,并且发现在主存数据库的划分操作中,划分分片过多时,会引起大量的TLBmiss,所以他们提出了radix-clustering算法[6],通过执行多遍划分的方法,使得每遍划分的分片数量小于TLB维护的映射数量,从而减少了TLBmiss的数量.He等[9-11]提出cacheoblivious查询处理方法,该方法意识到多层存储结构的存在,但是不需要知道存储结构的具体参数,而是采用自动调节的方法获得较好的性能.和本文的方法不同,Boncz和He等处理的是主存数据库的join操作,而不是磁盘join操作.Kim等[19]根据Radix-clustering算法,并且结合了计算机SIMD和多核的特点实现了join算法,来比较同样充分利用计算机硬件特性的sort-mergejoin的性能.他们得出如下结论:当SIMD的宽度达到512-bit的时候,sort-mergejoin就优于hashjoin.Ny-berg等[20]指出要获得高性能的排序算法,必须考虑如何利用高速缓存,他们采用的方法也是利用划分操作把数据表数据分为多个分片,每个分片可以放入高速缓存,从而获得较快的cache执行效率,可是其划分过程存在的问题和文献[2]中的问题类似.7结论为弥补CPU和内存之间日益增大的性能差异,人们在CPU和内存之间插入了高速缓存.要提高CPU的利用率,应用程序必须充分利用高速缓存.本文分析了传统的磁盘连接算法在高速缓存利用方面的问题,并且提出了一种新的可以充分利用高速缓存的磁盘连接算法DBCC-Join.连接位置索引对表JPIPT是用到的数据结构,说明了每个连接结果元组在各自表中的位置索引对.DBCC-Join的执行包括两个阶段:JPIPT构建阶段和结果输出阶段.JPIPT构建阶段对列存储化的连接属性执行高速缓存敏感的算法来构建连接位置索引对表.利用获得的JPIPT,结果输出阶段只需要对数据表执行一遍顺序扫描就可以获得结果.实验表明,和传统磁盘连接算法相比,DBCC-Join算法可以获得一个数量级的加速比,从而强调了高速缓存在数据库查询处理方面的重要性.
