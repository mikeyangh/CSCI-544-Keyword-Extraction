Page1基于直径为2的摩尔图网络的并行矩阵乘算法张冰(深圳大学计算机与软件学院广东深圳518060)摘要提出了一个并行矩阵乘算法IPBPMM(InterconnectedProcessor-BasedParallelMatrixMultiplication).该算法运行在以五角形、Petersen图和Hoffman-Singleton图等直径为2的摩尔图(满足n=d2+1,n为节点数,d为度)为拓扑结构的由n个独立处理器构成的机群并行计算环境中.与基于二维环绕网孔阵列拓扑结构的Cannon和Fox等并行矩阵乘法算法相比较,IPBPMM算法通信开销较小,加速比更高,同时还具有矩阵分块可随机分布在各个节点中,无需事先按一定规律装入各节点中的特点.同时IPBPMM算法也能很好地扩充到由多个直径为2的摩尔图为拓扑结构组合构成的并行计算环境中,且随着网络的扩大,算法的并行加速比更高.关键词并行算法;并行矩阵乘法;摩尔图;网络拓扑结构;并行与分布式计算;高性能计算1引言矩阵运算大量用于控制工程、图像和数字信号Page2基本过程包括装入、对准、循环单步移位和乘加3个步骤.装入是将矩阵犃,犅按棋盘方式进行分块后按一定的规定将各个分块矩阵ai,j,bi,j加载到处理器Pi,j中.Canon算法的对准步骤是将矩阵分块ai,j向左循环移动i步,将矩阵分块bi,j向上循环移动j步.Fox算法的对准步骤是将对角矩阵分块ai,i向所在行的其余处理器进行一到多播送.装入和对准两个步骤占据了Canon和Fox算法的很大一部分通信开销.本文提出一个称之为IPBPMM(InterconnectedProcessor-BasedParallelMatrixMultiplication)的并行矩阵乘算法.该算法运行在如Petersen图[4]等以直径为2的摩尔图(满足n=δ2+1,n为节点数,δ为度)为拓扑结构的由n个独立处理器构成的机群并行计算环境中.与Cannon和Fox算法相比较,采用了犃矩阵按行犅矩阵按列划分矩阵分块的分块方法,并利用Petersen图拓扑结构的特点省去了装入和对准的步骤,矩阵分块可随机分布在各个处理器中.同时IPBPMM算法还具有同步性好、通信开销小、加速比高、便于扩展等特点.2IPBPMM算法2.1处理器网络不同于Canon、Fox和DNS等矩阵并行乘算法所采用的二维环绕网孔阵列和超立方拓扑结构,IPBPMM算法采用直径为2的摩尔图作为处理器连接的拓扑结构.此时,图的直径就是某一处理器和图中其他处理器进行通信所需的最长的步数.图的度即图中某个处理器邻接的处理器的最大个数.直径为2的摩尔图的拓扑结构就是在直径为2和某个给定度的前提下包含的处理器节点数为最多的结构.图1所示的3种图就是直径为2,度分别为2、3和7时的最优拓扑结构,它们分别具有5、10和50个节点.如图1(b)所示的Petersen图可如图2那样连接10个处理器.每个处理器能与3个处理器相连,且每个处理器与其他任一处理器的通信可在两步之内完成,具有正则性、对称性和容错性以及并行通信能力强等特点.Petersen图作为计算机互联网络结构受到了广泛的关注与研究[5-8].在IPBPMM算法中假设处理器的个数为p,输入矩阵犃M×N按行分解为p个大小为M/p行N列的矩阵分块,输入矩阵犅N×Q按列分解为p个大小为N行Q/p列的矩阵分块,乘积矩阵犆M×Q的计算由p个处理器完成,其中第i个处理器Pi,完成矩阵犆第i个分块的计算,其大小为M/p行Q/p列.图2基于Petersen图拓扑结构的处理器互联网络2.2算法原理假设IPBPMM运行在以图2所示的摩尔图为拓扑结构组成的处理器网络上,处理器的个数为10,调整矩阵犃M×N,犅N×Q的行和列(可通过添加若干值为0的行和列),使m=M/10和q=Q/10为整数.然后将矩阵犃M×N按行划分成10个大小分别为m×N的矩阵方块,将矩阵犅N×Q按列划分成10个大小分别为N×q的矩阵方块,并随机分配给每个处理器一个犃矩阵分块和一个犅矩阵方块.IPB-PMM算法的步骤为:(1)每个处理器将分配给自己的1个犃子矩阵分块传送给相邻的3个处理器;(2)每个处理器收到来自相邻处理器传送的犃矩阵3个矩阵分块后,各处理器Pi(i=0,1,2,…,9)检查其中及本身初始分配的犃矩阵分块中是否有犃矩阵的第i个分块,若有则保存到处理器相应的寄存器中;(3)每个处理器将第(2)步收到的犃矩阵3个矩阵分块中除了来自要发送处理器的另外2个分块分别传送给相邻的3个处理器;(4)每个处理器收到来自相邻处理器传送的6个犃矩阵分块后,各处理器Pi(i=0,1,2,…,9)检查其中是否有犃矩阵的第i个分块,若有则保存到处理器相应的寄存器中;Page3(5)每个处理器将分配给自己的1个犅子矩阵分块传送给相邻的3个处理器;(6)各处理器Pi(i=0,1,2,…,9)将保存到各自寄存器的犃矩阵分块分别与本身初始分配的犅矩阵分块,以及每个处理器收到来自相邻处理器传送的犅矩阵3个矩阵分块相乘,得到犆矩阵第i个行分块的4个列子分块;(7)每个处理器将第(6)步收到的犅矩阵3个矩阵分块中除了来自要发送处理器的另外2个分块分别传送给相邻的3个处理器;(8)各处理器Pi(i=0,1,2,…,9)将保存到各自寄存器的犃矩阵分块与每个处理器收到来自相邻处理器传送的犅矩阵的6个矩阵分块相乘,得到犆矩阵第i个行分块的剩余的6个列子分块.2.3算法描述假设IPBPMM运行在以度为d直径为2的摩尔图为拓扑结构组成的处理器网络上,处理器的个数为p,调整矩阵犃M×N,犅N×Q的行和列,使m=M/p和q=Q/p为整数,并将矩阵犃按行矩阵犅按列划分的方式分别划分成p个大小分别为m×N和N×q的子方阵ai,N和bN,j.其中,i,j=0,1,2,…,p-1.每个子方阵表示为(z,k)的二元组形式随机指派给一个处理器Pi(i=0,1,2,…,p-1),其中,z表示子方阵a或b;k表示子方阵的序号.符号=>表示发送,<=表示接收.IPBPMM算法的描述见算法1.算法1.IPBPMM并行矩阵乘算法.输入:犃M×N,犅N×Q输出:犆M×QBEGIN(1)FORi=0top-1PARALLEL-DOFORPi’sdneighborprocessorsPjDOxi=(au,N,u)=>PjENDFORENDFOR其中,xi表示初始随机指派给处理器Pi的犃矩阵分块a的二元组;u为矩阵分块a的序号.(2)FORi=0top-1PARALLEL-DOFORallxjbothreceivedbyPiandinitiallydistributedENDFORENDFOR其中,xj表示处理器Pi接收到的来自邻接处理器的d个二元组以及初始指派给Pi的二元组;Ria为处理器Pi的寄存器,用于保存第i个犃矩阵分块.(3)FORi=0top-1PARALLEL-DOFORPi’sdneighborprocessorsPjDOENDFORENDFOR(4)FORi=0top-1PARALLEL-DOFORallxjreceivedbyPiDOENDFORENDFOR其中,xj表示处理器Pi接收到的来自邻接处理器的d(d-1)个二元组.(5)FORi=0top-1PARALLEL-DO//forallprocessorPiFORPi’sdneighborprocessorsPjDOENDFORENDFOR其中,yi表示初始随机指派给处理器Pi的犅矩阵分块b的二元组.v为矩阵分块b的序号.(6)FORi=0top-1PARALLEL-DOFORallyjbothreceivedbyPiandinitiallydistributedENDFORENDFOR(7)FORi=0top-1PARALLEL-DOFORPi’sdneighborprocessorsPjDOENDFORENDFOR(8)FORi=0top-1PARALLEL-DOFORallyjreceivedbyPiDOENDFORENDFOREND2.4算法举例表1~表5举例说明了IPBPMM矩阵乘算法在Petersen网络的执行过程.开始时矩阵犃M×N,犅N×Q划分成10个矩阵分块,并如表1所示以二元Page4组形式随机分布在各个处理器中.表5为乘积矩阵犆M×Q的计算结果,其中处理器Pi完成ci,j(i=0,1,表1子方阵随机分布处理器随机分配的二元组P0x0=(a1,1),y0=(b2,2)P1x1=(a2,2),y1=(b0,0)P2x2=(a0,0),y2=(b1,1)P3x3=(a9,9),y3=(b7,7)P4x4=(a4,4),y4=(b3,3)P5x5=(a3,3),y5=(b4,4)P6x6=(a5,5),y6=(b6,6)P7x7=(a8,8),y7=(b9,9)P8x8=(a6,6),y8=(b8,8)P9x9=(a7,7),y9=(b5,5)表4IPBPMM算法第(6)步执行结果示例处理器接收的二元组ci0ci1ci2ci3ci4ci5ci6ci7ci8ci9P0y1,y4,y7x2y1P1y0,y2,y3x0y1x0y2x0y0P2y1,y5,y8x1y1x1y2P3y1,y6,y9x5y1P4y0,y5,y6P5y2,y4,y9P6y3,y4,y8P7y0,y8,y9P8y2,y6,y7P9y3,y5,y7x1y5x4y0x4y4x4y5x8y4x3y5x3y9表5IPBPMM算法第(8)步执行结果示例处理器接收的二元组ci0ci1ci2ci3ci4ci5ci6ci7ci8ci9P0y2,y3,y5,y6,y8,y9x2y1x2y2x2y0x2y4x2y5x2y9x2y6x2y3x2y8x2y7P1y4,y7,y5,y8,y6,y9x0y1x0y2x0y0x0y4x0y5x0y9x0y6x0y3x0y8x0y7P2y0,y3,y4,y9,y6,y7x1y1x1y2x1y0x1y4x1y5x1y9x1y6x1y3x1y8x1y7P3y0,y2,y4,y8,y5,y7x5y1x5y2x5y0x5y4x5y5x5y9x5y6x5y3x5y8x5y7P4y1,y7,y2,y9,y3,y8x4y1x4y2x4y0x4y4x4y5x4y9x4y6x4y3x4y8x4y7P5y1,y8,y0,y6,y3,y7x6y1x6y2x6y0x6y4x6y5x6y9x6y6x6y3x6y8x6y7P6y1,y9,y0,y5,y2,y7x8y1x8y2x8y0x8y4x8y5x8y9x8y6x8y3x8y8x8y7P7y1,y4,y2,y6,y3,y5x9y1x9y2x9y0x9y4x9y5x9y9x9y6x9y3x9y8x9y7P8y1,y5,y3,y4,y0,y9x7y1x7y2x7y0x7y4x7y5x7y9x7y6x7y3x7y8x7y7P9y1,y6,y2,y4,y0,y8x3y1x3y2x3y0x3y4x3y5x3y9x3y6x3y3x3y8x3y73性能分析假设M=N=Q=n,d为图的度,IPBPMM算法第(1)步同步传输1个犃矩阵分块,其数据传输时间为ts+twn/p.其中,ts为启动时间(包括打包、执行选路算法和建立通信界面的时间),tw是传输每个字的时间.算法第(3)步同步传输d-1个犃矩阵分块,其数据传输时间为ts+(d-1)twn/p.算法第(2)和第(4)步分别为d+1次和d(d-1)次比较运算,该运行时间可忽略不计.算法第(5)步和第(7)步分别同步传输1个和d-1个犅矩阵分块,其数据传输时间分别为ts+twn/p和ts+(d-1)twn/p.算法第(6)和第(8)步分别执行d+1次和d(d-1)2,…,9;j=0,1,2,…,9)的计算,ci,j的大小为M/10行Q/10列.次(n/p)×n和n×(n/p)子矩阵乘法运算,其计算时间为(d2+1)×n3/p2=n3/p.因此,IPBPMM算法总的并行运行时间为表6分别列出了IPBPMM算法(处理器个数p)和运行在由槡p×槡p个处理器构成的二维网孔上的Canon和Fox算法的运行时间.算法IPBPMMCanonFox1由表6可见,IPBPMM算法的计算时间与Page5Canon和Fox算法基本相同,而其通信时间大约是Cannon和Fox算法的1/槡p倍,这是因为IPBPMM算法充分利用处理器多通道同步通信的能力和网络互联的对称性使其通信开销大大小于其它2种算法.因此IPBPMM算法更适用于大规模矩阵的乘法运算.另外,Canon和Fox等算法要求开始时矩阵分块ai,j和bi,j要装入到固定的处理器Pi,j中,表6所示的Canon和Fox算法的运行时间并未考虑这部分大小为2p×(ts+twn2/p)的通信开销,而IPB-PMM算法由于矩阵分块可随机分布在各处理器中而无此开销.4IPBPMM算法的可扩展性若每个处理器有多个通道同时向相邻的处理器传输数据,并且每个处理器有足够的缓存器保存接收的数据,则由式(1)可见,随着度的提高通信开销反而由于同步传输能力的提高有所下降,加速比越趋近于理想值.在图1所示的3种基本的直径为2的摩尔图的基础上,可通过对其在维度空间上进行拓展来构造更多处理器节点的并行计算网络.如图3为将2个Petersen图拓展构成直径为3度为4的具有20个处理器的并行计算网络.图4为将10个Petersen图拓展构成直径为4度为6的具有100个处理器的并行计算网络.图3直径为3度为4的具有20个处理器的并行计算网络图4直径为4度为6的具有100个处理器的并行计算网络采用图3所示的结构,处理器间的数据通信通过3步传输完成:第(1)步每个处理器接收相邻4个处理器传输的4个矩阵分块,如P0收到来自P1,P4,P7和P10的4个矩阵分块;第(2)步每个处理器接收来自同一个Petersen图结构相邻的3个处理器接收的9个矩阵分块(不含第1步收到的来自要发送的处理器的数据),如P0收到来自P1(接力传递的P2、P3和P17的矩阵分块)、P4(接力传递的P5、P6和P14的矩阵分块)和P7(接力传递的P8、P9和P11的矩阵分块);第(3)步每个处理器接收来自同一个Petersen图结构相邻的3个处理器中与另一个Petersen图结构处理器相邻的3个处理器接力传输的6个矩阵分块,如P0收到来自与P7相邻的P11(接力传递的P12和P13的矩阵分块)、与P4相邻的P14(接力传递的P15和P16的矩阵分块)和与P1相邻的P17(接力传递的P18和P19的矩阵分块).因此,IPBPMM算法在图3所示的20个处理器上的运行时间为4个Petersen图可合并构成直径为3度为6的具有40个处理器的并行计算网络.此时IPBPMM算法的运行时间为同理,可分析得出IPBPMM算法在图4所示的100个处理器上的运行时间为表7列出了IPBPMM算法在由直径为2的摩尔图及由其组合构成的并行计算网络上的加速比特性.由表7可见,随着处理器个数p的增加,通信开销基本相同,在p值由10增加到50时,通信开销反而由于同步传输能力的提高有所下降,而计算时间降低了4倍.表明IPBPMM算法在以直径为2的摩尔图(满足n=d2+1,n为节点数,d为度)拓扑结构组建的并行计算网络上具有理想的加速比和良好的可扩展特性.处理器个数1510204050100Page65结束语IPBPMM算法与基于二维环绕网孔阵列拓扑结构的Canon和Fox并行矩阵乘算法进行了比较,在多核工作站Windows平台上采用MPICH对3种算法加以编程实现,主要对IPBPMM算法的同步通信性能进行了评估.Canon算法在实现过程中首先需要对犃矩阵分块按行移位,对犅矩阵分块按列移位,这一对准过程处理器最多需要调用2(槡p-1)次MPI_Sendrecv函数.在乘加阶段每个处理器需要进行槡p次每次使A矩阵分块按行向左移位一步,犅矩阵分块按列向上移位一步,故Canon算法中每个处理器共需要调用4槡p-2次MPI_Sendrecv函数.Fox算法在对准阶段需要对犃矩阵的对角线分块按行进行一到多的传送,每个处理器需要调用槡p-1次MPI_Sendrecv函数.在Fox算法的乘加阶段每个处理器需要进行槡p次每次使一个犃矩阵分块向所在行进行一到多的传送,犅矩阵分块按列向上移位一步,故Fox算法中每个处理器共需要调用p+槡p-1次MPI_Sendrecv函数.IPBPMM算法无需对准,运行在Petersen图网络上每个处理器在算法的第(1)步和第(5)步共需要调用2次,在算法的第(3)步和第(7)步共需要调用4次,总共6次MPI_Sendrecv函数.表8列出了用MPI实现Cannon,Fox和IPBPMM算法时每个处理器需要调用的通信函数次数.IPBPMM(p=5)IPBPMM(p=10)IPBPMM(p=20)IPBPMM(p=50)IPBPMM(p=100)理论分析和实验结果表明基于Petersen图处理器互联拓扑结构的IPBPMM算法无需事先将矩阵分块装入,矩阵分块可随机分布在各个处理器中.同时IPBPMM算法还具有同步性好、通信开销小、加速比高、便于扩展、便于硬件化实现[9]以及可对任意大小的矩阵进行分块计算等特点.IPBPMM算法是建立在图1所示3种直径为2的摩尔图拓扑结构或是由这3种结构组合构成的拓扑结构上的,因此要求处理器的个数必须为5的倍数,这也是IPB-PMM算法的一个局限性.
