Page1一种基于节点密度分割和标签传播的Web页面挖掘方法张乃洲1)曹薇1)李石君2)1)(河南财经政法大学计算机与信息工程学院郑州450002)2)(武汉大学计算机学院武汉430072)摘要获取Web页面中的重要内容如文本和链接,在许多Web挖掘研究领域有着重要的应用价值.目前针对该问题主要采用Web页面分割和区块识别的方法.但现有的方法将Web页面中重要文本和链接的识别视为两个相互独立的问题,这种做法忽略了Web页面中文本和链接的内在语义关系,同时降低了页面处理的效率.文中提出了一种Web页面重要内容挖掘的统一框架,该框架主要由3个部分组成:第一,先将Web页面转换为DOM树表示,然后采用节点密度熵为度量将DOM树分割为不同的页面块;第二,采用基于K最近邻标签传播的半监督方法自动扩展页面块训练集;第三,在扩展的页面块训练集上对SVM分类器进行训练,并用来对页面块进行分类.采用该框架可以将Web页面块区分为多种类型,并且该框架独立于Web页面的类型和布局.我们在真实的Web环境下进行了广泛的实验,实验结果表明了该方法的有效性.关键词页面分割;节点密度;标签传播;DOM树;块分类;社会计算;社交网络1引言随着Web技术的飞速发展,特别是Web2.0的出现,当前Web页面包含越来越丰富的内容,以满足商业和终端用户的需求.目前,一个典型的Web页面除了主要内容(相关文本和链接)外,还包括导航菜单、广告、用户注释、版权和服务信息等内容.但对于许多Web应用来说(如Web信息检索、Web数据挖掘、移动互联网等),这可能会成为一种障碍.例如:(1)Web数据挖掘任务(如Web信息抽取),需要分析Web页面中的主要文本内容以完成有用知识的发现.但当前Web页面中包含的大量“噪音”内容会对数据挖掘任务造成不小的干扰;(2)搜索引擎爬虫在抓取给定页面中的链接时,会遇到许多“无用”链接(如页面中的导航菜单、广告链接、服务链接等),如果不能有效去除这些链接,将会大大降低爬虫的爬行效率,甚至造成爬行主题的漂移;(3)为满足Web信息检索的需要,被抓取的Web页面往往需要被缓存(Cached)和存储在本地数据库中,而Web页面中大量冗余内容的存在会造成系统存储空间和网络带宽的严重浪费;(4)在移动互联网方面,由于手机或PDA设备具有屏幕尺寸较小和内存容量有限等特点,使得包含大量冗余信息的Web页面不适合在这些移动终端中直接传送和显示,而必须对页面中的主要内容进行抽取,然后进行页面重构[1-3].基于以上这些问题,使得Web页面重要内容挖掘成为当前学术界的一个研究热点[1-15].根据应用目的不同,该问题又可以分为两个子任务:一是Web页面主要文本内容(MainContent)的挖掘;二是Web页面主要链接(MainLinks)的挖掘.但当前很多研究是将这两个子任务视为相互独立的任务对待,这种分解的做法存在两个方面的主要问题.其一,降低了页面处理的效率.实际上很多应用同时包含了这两个子任务,如Web信息检索、移动互联网应用等.其二,忽略了Web页面中主要文本和主要链接的内在语义关系.例如,通过直觉观察可以得到这样的结论:在一个Web页面中,主要文本块中包含的链接以及邻近该主要文本块的链接,与该主要文本块具有较强的主题相关性.这一规则对Web聚焦爬行来说,具有重要的意义,可被用于指导聚焦爬虫进行主题爬行.因为对于链接来说,其锚链文本越短,包含的语义信息越少;主要文本块往往具有丰富的语义信息,更容易度量其主题相关性.而上述分解的做法却无法利用这种内在的语义关系.本文提出的方法,能够在一次页面处理过程中同时完成这两个任务,并能够在此基础上充分利用Web页面块内在的结构和语义关系,进行深层次的数据挖掘工作.由于Web页面结构存在很强的异构性,即相同的页面内容可以有不同的页面布局,而相同页面布局又可以使用不同的实现方式,并且不同的Web站点风格也千差万别.因此,当前Web页面重要内容挖掘研究还存在不少问题和挑战.该研究的关键在于:(1)如何有效分割(Segment)Web页面;(2)如何提取页面分割块的特征;(3)如何利用这些特征来区分不同的分割块;(4)如何提高上述几个步骤的精度和效率.而现存研究普遍存在的问题主要有:提出的页面分割算法要么采用的方法较简单,很难适应复杂的页面[4],要么比较复杂,使得算法复杂度过高[5];提出的方法不能同时解决Web页面内主要文本块和链接块的识别问题等.本文提出了一个新的Web页面重要内容挖掘的统一框架,用于发现给定Web页面中重要的文本和链接.本文的主要贡献如下:(1)提出了一个高效的基于DOM树的页面分割算法.该算法首先将给定的Web页面转换为DOM树表示,然后采用节点密度熵为度量将DOM树自动分割为若干页面块,并采用启发式规则将相似页面块融合为较大的块.实验表明该算法独立于页面布局,具有很好的泛化能力和较好的页面分割精度.Page3(2)提出了一个有效的基于半监督学习的页面块分类算法框架.该框架首先确定页面块分类特征集F,然后在较小的已标记训练集上,采用基于K最近邻标签传播(KNNLP)算法自动生成较大的已标记训练集,并利用该训练集训练一个基于SVM的块分类器.最后采用该分类器来识别Web页面中重要的文本和链接块.实验表明本文提出的KNNLP算法在精度上要优于现有基于图的标签传播算法,并且整个算法框架具有较好的性能.(3)提出的方法能够在一次页面处理过程中同时完成Web页面中主要文本内容和主要链接的发现,从而显著提高了页面处理的效率.此外,该框架能够在页面分割的基础上充分利用Web页面块内在结构和语义关系,进行深层次的数据挖掘,并与其他Web应用有效结合.本文第2节简要介绍相关的研究工作;第3节给出问题的定义和形式化描述;第4节介绍所提出框架各部分的主要设计思想和算法;第5节是实验和分析部分;最后一节给出了结语以及对未来工作的展望.2相关工作2.1Web页面挖掘从研究目标的角度来看,当前Web页面重要内容挖掘研究主要集中在Web页面的内容抽取(ContentExtraction)[4].该任务的主要目标是发现和识别Web页面中的主要内容.这一问题的一个重要研究方向是Web信息抽取(WebInformationExtraction),该研究的主要任务是采用基于规则或机器学习的方法从海量的Web数据中发现和获取指定主题或格式的半结构化或非结构化信息,并最终将其转换为结构化信息.但该研究不属于本文研究的范围.本文研究的内容抽取问题可以视为一种“浅层”(Shallow)的信息抽取,可作为WIE问题的预处理步骤.而从所采用技术的角度来看,内容抽取问题可以分为两类:第一类是采用抽样的方法从同一Web站点获得一定数量的Web页面,然后发现其中的重复模式,或称为模板(Template).如Yi等人[6]提出了一种称为站点风格树(SiteStyleTree)的结构来捕获给定Web站点中公共的表示风格和页面的实际内容,然后使用一种基于信息的度量方法来确定站点中的噪音和主要内容.Ramaswamy等人[7]利用同一Web站点内的页面具有内容共享性的特点,提出了一种Shingling算法来识别Web页面中的片段.利用该方法可以去除多个页面中的共享内容,从而能显著减少对Web缓存的需求.而Debnath等人[8]也利用了同一Web站点的页面在内容和结构上具有相似性的特点,定义了四种抽取器来识别页面中的主要内容块.针对许多实际的模板检测任务是在没有任何有关站点结构先验知识的情况,Kolcz等人[9]提出一种使用站点内部模板发现技术来驱动站点独立的模板检测器的归纳过程,该方法减少了对人工标注数据的要求.其他类似的文献有[10-11]等.该类方法的缺点是需要通过获取和比较多个Web页面才能得到有关站点的结构信息,这将导致在无法获取足够页面的情况下,该方法的失效.此外,这类方法还会产生额外的系统存储耗费和处理延时[11].另一类是在单一的Web页面中发现重要的区域.如Yin等人[1]提出了一种类似于PageRank的排序算法来排序一个Web页面中的内容块,使得在一个Web页面中,仅仅重要的部分才会被抽取,然后将其传送到移动设备中.而Baluja[3]提出了一个关于Web页面分割的机器学习的框架,通过采用一个有效的页面分割算法,利用透镜信息熵的减少和决策树学习来实现Web页面在移动设备屏幕上不失真地缩放.以上两类方法大多使用了页面分割的思想,特别是后者.当前已有不少方法被用来将Web页面分割成块(Block)或区域(Region).如Cai等人[5]提出了一个基于视觉的页面分割(VIPS)算法.该算法利用了Web页面中的视觉线索如页面元素的高度、宽度、背景色、位置等,来获取页面的最优划分.该算法的优点是块的划分粒度适中,块的语义内聚度好.但缺点是使用了较多的启发式规则,算法复杂度较高,系统的运行效率存在问题.Chakrabarti等人[12]提出了一个基于图切割的Web页面分割方法.该算法将Web页面分割问题形式化为一个权重图上最优化问题,并提出了一个在手工标记的数据集上进行学习的框架,用来学习这些权重.而在进行图切割时根据节点间的权重大小来决定节点是否需要分割.Bing等人[13]提出了一个基于Web页面分割图模型的结构化预测方法.该方法将页面分割问题形式化为一个结构化标记问题,而Web页面分割图建模了一个Web页面的候选分割区边界以及相邻页面分Page4割区之间的依赖关系.Web页面分割图上的每一个标记模式对应于一个可能的Web页面分割,而算法的核心是采用整数线性规划方法来发现最优的Web页面分割图标记.近来一些研究提出了使用DOM树节点的密度特征来进行页面的分割[4,14-15].如Sun等人[4]提出了一个使用文本密度来进行内容抽取(CETD)的方法.该方法首先将Web页面转化为DOM树,然后给出了一个复合文本密度公式(CTD),并利用该公式计算每个DOM树节点的CTD值,以决定该节点的内容是否为待抽取的文本内容.本文的工作属于第二类方法,并吸收了文献[4,14-15]等的思想.但与这些文献不同的是本文使用了多个节点密度的度量,并通过计算节点密度熵来划分Web页面,而页面块的识别是使用SVM分类器来完成的.这样做的目的是为了适应多种不同的应用(文本块、链接块或其他类型块的识别和抽取),同时又能提高页面块识别的精度和系统在线处理的效率.2.2标签传播算法将Web页面分割成页面块后,即可使用现有的分类算法来进行页面块的识别.当前的分类算法主要分为3类:(1)有监督积极学习(EagerLeaner)方法.如决策树、人工神经网络(ANN)、支持向量机(SVM)等.该类算法的特点是在给定标记训练集上学习到一个最优模型,然后通过模型来预测未知数据.其优点是分类精度和运行效率高,但缺点是分类精度依赖训练集的规模,且训练数据一般需要手工标注、人工耗费大、训练过程较慢等;(2)有监督懒散学习(LazyLeaner)方法.典型的是K最近邻(KNN)分类.该类算法的优点是不需要训练模型,在新数据点加入时直接分类.缺点是需要记住所有的训练样本,分类开销大,效率较低;(3)半监督学习方法.如标签传播算法[16-18]、半监督支持向量机[19]等.该类算法的优点是利用了未标记数据参与分类,需要的人工耗费较小.但缺点是随着未标记数据比例的增加,分类精度下降,且运行效率低于有监督方法.为保证页面块分类器的在线运行效率,本文主要采用SVM模型来进行页面块的分类.但本文Web页面重要内容挖掘研究的一个显著特点是:一个页面会产生多个块(几个到数十个不等),因此需要的训练数据较大.如果完全手工训练数据,则会产生很大的人工耗费.因此本文结合了有监督和半监督方法来完成页面块的分类问题.主要思想是:先采用半监督的标签传播算法来扩展初始的标记训练集,当训练集的大小达到指定值时,再使用SVM模型在扩展的训练集上进行模型训练和分类.标签传播(LabelPropagation)算法最早由文献[16]提出,其主要动机是在现实的数据挖掘应用中(如分类),未标记数据丰富而容易获得,而标记数据稀少且需要大的人工耗费进行标记.由此提出的问题是能否利用未标记数据进行分类,从而减少人工标记的代价?而其主要思想是利用少量标记数据和大量未标记数据内在的流形结构来辅助分类.其底层的基本假设[18]为:(1)空间中的近邻点很可能具有相同的标签;(2)在相同结构上的点很可能具有相同的标签.本文提出的KNNLP算法与文献[16,18]提出方法的主要区别如下:(1)文献[16]采用的图模型为完全连接图,即在进行标签传播过程中,对任意一个给定的数据点狓,其他所有点的标签均会按照其与狓相似度的大小对其施加影响.其缺点是增加了算法的时间和空间复杂度.同时,在实验中我们发现,这会降低算法的鲁棒性.在某些情形下,如当分类数据集中存在桥接点(BridgePoint)[18]时,算法的性能会严重下降.(2)文献[18]提出了LNP(LinearNeighbor-hoodPropagation)算法.其核心思想是借用了流形学习中经典的LLE(LocalLinearEmbedding)算法的思想,即对任意一个给定的数据点狓,可用其k个最近邻的线性组合来进行重构,而学习到的最优系数向量则做为狓与其最近邻相似度大小的度量.与文献[16]相比,其优点在于:在进行标签传播过程中,对任意一个给定的数据点狓,仅使用了k个最近邻点对其施加影响,这大大降低了算法的时间和空间复杂度,同时也提高了算法的鲁棒性.但与LLE算法相比,由于LNP算法要求重构系数非负,由此可能导致其学习到的某些数据点对应的最优系数向量存在较大的重构误差.在这种情况下,表明该系数向量并未真正反映数据点狓与其最近邻相似度的大小关系,这会降低算法的性能,特别是当存在桥接点时.我们的相关实验也证明了这一点.因此,为了保证算法的性能,文献[18]采取了预处理步骤,去除了数据集中的桥接点和孤立点.(3)本文提出的KNNLP算法考虑了以上两种Page5算法存在的问题.其一,与文献[18]类似,在进行标签传播过程中,对任意一个给定的数据点狓,仅使用其k个最近邻点对其施加影响.其二,与文献[16]类似,数据点狓与其最近邻相似度大小的度量采用了高斯函数来计算.但与文献[16]不同的是,提出了相似度度量中平衡因子的定量计算方法.因此,从算法效率来看,KNNLP算法要优于文献[16],同时也优于文献[18].因为LNP算法在学习最优系数向量的过程中,使用二次规划是比较耗时的.而从鲁棒性上来看,我们进行了不同数据集下的相关实验,包括存在桥接点的情况,实验表明KNNLP算法要优于以上两种方法.因此,KNNLP算法并不需要对数据集进行额外的预处理.3问题描述及相关定义为说明本文提出的算法框架,本节给出Web页面重要内容挖掘问题描述及相关概念的定义.定义1.每一个Web页面均可以表示成一个DOM树Td.Td是一个有向图〈V,E〉,其中V为顶点的集合,V={v|v∈html标签集Tag}.E为有向边的集合,E={〈u,v〉|u,v∈V,其中u称为v的父顶点,而v称为u的子顶点,且在html结构上,v对应的标签被u对应的标签所包含}.Td中离开顶点v的边的个数称为v的出度;指向顶点v的边的个数称为v的入度.Td中入度为零的顶点称为Td的根;而出度为零的顶点称为Td的叶节点.定义2.一个DOM树Td可表示为一个页面块的集合B={bi|bi∈V,且bi为叶节点}.定义3.页面块集合B中的任意元素bi可表示为一个k维向量狓i=〈f1,f2,…,fk〉,其中fj代表bi的第j个特征.定义4.给定Web页面集合P,其上的训练集珦D可表示为{〈狓1,y1〉,〈狓2,y2〉,…,〈狓n,yn〉},其中yi∈Y={1,2,…,C},Y为类标签集合.由以上给出的4个基本定义可以得到本文关于Web页面重要内容挖掘问题的形式化描述,该问题可分为如下3个部分:(1)Web页面分割.该过程可定义为映射Sp:P→B,Sp将给定Web页面集合P中的每个页面p映射为块的集合.(2)基于半监督学习的训练集扩展.给定已标记训练集珦D犔={〈狓1,y1〉,〈狓2,y2〉,…,〈狓l,yl〉},yi∈Y犔={1,2,…,n},1il.这里Y犔为已知的类标签yl+2〉,…,〈狓l+u,yl+u〉},其中,yj∈YU,l+1jl+u.YU为未知标签集合,且lu.该训练集扩展过程是在珦D犔和珦DU上,产生一个映射:YU→Y犔,最后合并珦D犔和珦DU为珦D.(3)页面块分类.使用珦D训练一个分类函数β.β可定义为β(珦D):B→Y犔,其作用是将任意的页面块bi映射为一个已知的标签yi.给定一个Web页面p,使用Sp得到相应的页面块集B,然后使用块分类函数β对B进行分类.为了说明本文提出的Web页面重要内容挖掘方法,图1给出了一个例子页面(http://tech.sina.com.cn/t/2012-04-20/05006988760.shtml)①.该页面中虚线矩形框表示页面分块,其中标记为①和②的块是该页面的重要内容,分别对应于相关文本块和链接块.本文提出的Web页面重要内容挖掘方法的目标是自动发现和标记出给定页面中的这些重要区域.4所提出的方法上节给出了本文提出Web页面重要内容挖掘①由于该页面较长,基于篇幅的原因,部分内容被删去.Page6方法的形式化描述,本节详细介绍该框架3个组成部分的原理及算法描述.4.1基于节点密度熵的自适应Web页面分割方法当前对Web页面分割问题已进行了深入的研究.文献[15]对各种分割方法从视觉问题、语言学问题、密度问题和一维问题等四个不同角度分别进行了详细的讨论.本文在设计Web页面分割算法时,主要考虑了如下因素:(1)应充分利用HTML文档自身的结构;(2)选择能反映待挖掘内容的本质特征;(3)简单高效.为此,结合现存研究本文采用了基于密度度量的DOM树自适应分割算法.该算法首先需要将给定的Web页面转换为DOM树表示.在实际的系统实现中,我们采用了开源项目Nekohtml①来完成DOM树的转换.Nekohtml是一个强大的HTML扫描器和标签平衡器.与其他解析器相比,Nekohtml对非良结构的HTML文档具有很强的鲁棒性.我们的实验表明它能够正确解析大部分的HTML文档,这为我们的研究工作提供了很大的便利.下面先给出3个密度定义:定义5.设ni为DOM树Td中的一个节点(顶点),则ni的文本密度定义为其中,Nni为节点ni代表的子树中,去除所有html标签(包括链接文本)后的纯文本数(字符数);NT为Td代表的整个文档中去除所有html标签(包括链接文本)后的纯文本数(字符数).text反映了在全局页面中,文本内容在某个局ρi部区域的相对集中度.通过观察和实验,我们发现text越大,往往意味着该节点越有可能包含待发现ρi的主要文本块.从图1中的标记块①可以直觉地看到这一点.定义6.设ni为DOM树Td中的一个节点,则ni的链接密度定义为其中,lNni为节点ni代表的子树中,所包含的链接数(〈a〉标签数);lNT为Td代表的整个文档中所包含的链接数.link反映了在全局页面中,链接在某个局部区ρi域的相对集中度.一般而言,ρi越有可能包含待发现的主要链接块(如图1中的标记块②).引入ρi的重要链接块.定义7.设ni为DOM树Td中的一个节点,则ni的链接文本密度定义为其中,lTni为节点ni代表的子树中,所有链接所包含的文本数(包含在〈a〉〈/a〉标签之间的文本数);lTT为Td代表的整个文档中所包含的链接文本数.ltext反映了链接文本在页面某个局部区域的相ρi对集中度.引入该指标主要考虑到这样一种页面分割场景:某些节点包含较少的链接,但所包含的链接文本数却较多(具有较长的平均链接长度),而这些链接往往具有相关链接的特征(如图1中的标记块②).给出了3个密度度量后,可以引入如下节点密度熵②的定义:ρiltext割算法.本文中取n=3,p1=ρi1/2.利用节点密度熵H(ni),可以得到如下页面分算法1.segmentPage.输入:DOM树根节点root输出:页面融合块集合BfBegin1.B=segmentDomTree(root)//B为页面分割块集合2.Bf=fuseBlocks(B)End函数:segmentDomTree(Nd)输入:DOM树节点Nd;输出:页面分割块集合B;Begin1.B←2.ifNdhaschildnodesthen3.nds[n]←getChildNodes(Nd)4.foreachnodek∈nds[n]do5.subnd←nds[k]6.ifsubndisELEMENT_NODEthen7.H(subnd)←calNodeDensityEntropy(subnd)8.ifH(subnd)>αthen9.segmentDomTree(subnd)10.elseifH(subnd)>βthen11.B←subnd①②Page712.elseifsubndisTEXT_NODEthen13.H(subnd)←calNodeDensityEntropy(subnd)14.ifH(subnd)>γthen15.B←subndEnd函数:fuseBlocks(B)输入:页面分割块集合B输出:页面融合块集合BfBegin1.Bf←2.whileB≠do3.bi∈B,removebifromB4.forbj∈Bdo5.ifbiandbjsatisfytherulesetRthen6.bi←bi∪bj7.removebjfromB8.Bf←biEnd算法1由两个部分组成:函数segmentDomTree和函数fuseBlocks.首先执行第1步:调用函数segmentDomTree.该函数是一个递归函数,主要完成页面的初次分割,并产生页面分割块集合B.其中,subnd表示当前子节点.ELEMENT_NODE和TEXT_NODE为节点的类型,分别代表元素节点和文本节点.H(subnd)表示节点subnd的节点密度熵,而函数calNodeDensityEntropy用于计算给定节点的节点密度熵.函数segmentDomTree初始的输入是整个DOM树Td,该函数只处理DOM树中的元素节点和文本节点,其中α、β和γ是分割控制参数,其值由实验来确定.从算法描述中可以看出,当节点subnd为元素节点时,α决定了subnd节点是否被继续分割.因此,α决定了块分割的大小.α越小,页面块分割得越小.如果α取得太小,可能使得本来在语义上属于同一个块的内容被分割开,同时增加了算法的时间复杂度;而α取得太大,又可能由于页面块分割得太大使得最后得到的页面块中的信息区域没有进行有效区分,从而降低了分割召回率.而当节点subnd不再被分割时,β决定了subnd(非文本节点块)是否被放置到集合B中.因此,β决定了非文本节点块尺寸的最小值.若该值取得太大,则可能漏掉一些分割块;取得太小,则可能会保留一些没有实际意义的块.当节点subnd为文本节点时,subnd(文本节点块)是否被放置到集合B中由γ决定.γ的作用类似β,不同之处是γ决定了文本节点块尺寸的最小值.在后续的实验部分我们将讨论它们的取值问题.图2给出了采用函数segmentDomTree来分割例子页面(图1所示)所得到的节点密度熵分布图,其中(a)部分是分割前所有的节点(只包括元素节点和文本节点)对应的节点密度熵H(ni);而(b)部分是分割过程中所有被处理的中间节点(被分割节点)和叶节点(块),它们分别对应于图3中的黑色节点和白色节点.从图2可以看到,在分割过程中大部分的节点被过滤掉了(未被处理).原Web页面共有1594个节点,而实际处理的只有63个节点,约占4%.图2直观地说明了使用节点密度熵为度量来分割DOM树的高效.调用函数segmentDomTree对DOM树进行分割后还需要进行的一个重要处理是块融合问题.函数segmentDomTree仅仅考虑了节点的密度属性,而未考虑到节点之间的语义关系.例如,若DOM树中存在两个互为兄弟节点的叶节点,其标签均为〈P〉,那么这意味着它们对应于页面中某一文本区中的段落,则在语义上它们应该属于同一个块.如果页面分割过程中,这两个节点被分割为两个块,则必须将它们融合为一个块.为此算法1执行第2步:调Page8用函数fuseBlocks.该函数主要用于扫描页面分割块集合B,并完成对分割块的融合操作.需要指出的是,函数fuseBlocks中任意两个块节点必须同时满足规则集R中的3条规则,才能进行块融合操作.其中相似标签是指标签具有相似的功能,例如格式标签〈em〉、〈strong〉等都是对文本格式进行修饰,因此这些元素类型的节点等价于文本类型的节点.对相邻关系本文采用如下方法判别:对页面分割块集B按ID号对块排序,如果两个块在B中相邻,则它们为相邻关系.图3显示了调用算法1对例子页面进行DOM树分割后的结果.其中白色节点为叶节点(执行算法1后得到的块节点),图中虚线矩形框表示调用算法1中的fuseBlocks函数进行块融合后,得到的更大页面块.从图上可以看到最后整个页面被划分为11个不同的区域(块),其中包括8个融合块和3个未融合块.图3对例子页面进行DOM树分割及页面块融合的结果4.2页面块特征选择bi建模为一个6元组:为了对页面块进行有效分类,本文将任意的块text、ρi式中,ρi密度和链接文本密度,它们的定义参见式(1)~(2).而式(5)中,后3个特征为块的视觉特征:width代表块的宽度,height表示块的高度,而sp表示块在页面中的位置所对应的重要度分值.页面块的视觉特征一般包括:块的大小、块内字体大小和颜色、块的背景色、块在页面中的位置等.之所以使用视觉特征是源于这样的观察:Web页面的设计者在进行页面布局设计时是遵循一定原则和惯例的.例如,页面布局大体上是按头部、内容区和尾部划分;一般重要的内容放在页面的中部等.因此通过块的视觉信息能够在一定程度上区分块的不同.如导航菜单在视觉上一般具备狭长的特征,其位置往往位于页面的头部;而页面的主要文本内容一般具有较大的width和height,且往往位于页面的中部等.Web页面块的视觉信息可以通过抽取并分析页面的CSS(CascadingStyleSheet)来获取,为此在实际的系统中我们设计了CSS抽取和分析器,以获取每个DOM树节点和页面块的视觉属性.在width和height取值上,我们以屏幕的宽度和高度为基准,将width和height映射到(0,1].而sp的取值复杂一些,本文采用了如表1的策略.Y表1中的X表示块在页面中水平方向的位置,其值可以取:“左”、“中”、“右”,该值可以通过CSS来获取.而Y表示块在页面中垂直方向的位置,其值可以取:“上”、“中”、“下”.本文通过块内节点ID号来近似计算该值.其依据是:本文采用的DOM树遍历算法(前序遍历)产生的节点ID号与浏览器对HTML页面标签渲染的顺序大致相同.从图3可以直观地看到这一点.这样,具有较小ID号的节点一般位于页面的头部,而ID号较大的节点一般位于页面的尾部.表1单元格中的数值为sp的分值.从表1可知,一个Web页面大致被分成了头(上)、左、中、右、尾(下)5个区域.4.3基于犓最近邻标签传播增量训练集扩展算法本小节主要描述本文提出的基于K最近邻标签传播(KNNLP)的增量训练集扩展算法.给定已标记训练集珦D犔={〈狓1,y1〉,〈狓2,y2〉,…,〈狓l,yl〉},yi∈Y犔={1,2,…,n},1il.这里Y犔为已知的类标签集合.给定未标记训练集珦DU={〈狓l+1,yl+1〉,〈狓l+2,yl+2〉,…,〈狓l+u,yl+u〉},yj∈YU,l+1jl+u.其中YU为未知(未观察到)标签集合,且lu.令珦D=珦D犔∪珦DU,则标签传播问题定义为:如何从珦D和Y犔中估计YU.给定一个近邻连接图G=〈V,E〉,其中V为顶点的集合,V={v|v∈珦D}.E为边的集合,E={〈u,v〉|u,v∈珦D,且v∈Nk(u),Nk(u)为顶点u的k个最近邻Page9集合}.则G中任意一条边〈i,j〉的权重wij定义为其中,δ为平衡因子,本文利用一个珦D上所有数据的方差函数来计算:其中,v为珦D上所有数据的方差.定义如下(l+u)×k的近邻概率转移矩阵犜:其中,犜反映了数据集中各个数据点与其k个最近邻数据点之间相似度的大小.定义犔为(l+u)×|Y犔|的标签矩阵,其行代表数据点,列代表类别,且犔ij定义为定义犎m为数据点m的k×|Y犔|近邻标签矩阵,其行表示数据点m的近邻,列表示类别,则犎m定义为其中Nk(m)表示数据点m的k个最近邻集合.设犜i表示近邻概率转移矩阵犜的第i个行向量,犔i表示标签矩阵犔的第i个行向量,犔(0)为初始的标签矩阵,则KNNLP算法的主要步骤为1.对于所有数据点i,计算犜i犎i,并将犔i←犜i犎i.2.对犔i进行行规范化,并重新恢复犔i中已标记数据的初始值:犔i=α犔i+(1-α)犔(0)3.重复步1和2,直到犔收敛.4.根据犔获取珦DU中任意数据狓i(l+1il+u)的标签yi.其中yi=argmaxj犔ij.关于犔收敛性的证明,可以参看文献[16-18].此外,KNNLP算法中最优近邻数k的取值并无确定的方法,与数据点本身的维度大小和数据集本身的特征有关,通常需要通过实验来确定.根据我们的实验,一般当k4时,才能取得较理想的结果.因此在本文后续实验中,我们取k=7.利用KNNLP算法,可以给出如下的增量训练集扩展算法.算法2.extendTrainSet.输入:初始的训练页面集合Pl和Pu;Y犔={1,2,3},分输出:扩展的页面分割块训练集TSBegin1.预处理.对初始的训练页面集合Pl采用算法1进行页面分割,并获取页面块集B.bi∈B,采用式(5)将其表示为特征向量狓i,并使用手工方式将其标记为(狓i,yi),其中yi∈Y犔,最后得到TSl.对训练页面集合Pu采用相同的方法进行页面分割和块特征向量表示,但不对块进行类别标识(即设置yi=0),最后得到TSu.2.令TS←.3.任取TSu中k个元素构成TSkTSu删除.在TSl和TSk移矩阵犜,并对犜做行规范化.利用式(9)构造标签矩阵犔.4.使用KNNLP算法,得到被自动标记的训练集TSk(即获得标记的TSk5.回到步骤3,继续进行训练集的扩展,直到TSu为空.6.TS←TS∪TSl.7.返回扩展的训练集TS.End使用算法2进行标签扩展时,每次需要添加k个未标记数据到训练集中.为保证标签传播的精度,必须合理选择k的取值.假设初始已标记训练集大小为m,实验表明,当取k≈0.56m时,效果较好.具体情况将在实验部分讨论.4.4基于SVM的页面块分类算法使用算法2获取足够数量的已标记页面块训练集之后,可以利用现有的分类算法对待识别页面块进行分类.本文采用基于支持向量机(SVM)的页面块分类器,页面块被分为3个类别:相关文本块、相关链接块、其他类别.基于SVM的页面块分类器的工作过程如下:(1)调用算法2构造页面块训练样本集TS.(2)在TS上训练SVM分类器,得到训练模型M.本文采用高斯径向核函数作为空间变换函数.(3)对于待分类的页面块,先根据式(5)对页面块进行特征表示,然后使用M进行分类.在实际的系统实现中,我们采用了开源项目libsvm①来实现SVM分类器.5实验评估为了验证本文提出方法的有效性,我们使用Java语言在Eclipse平台上实现了相应的原型系统———WPBMiner.该原型系统的输入是给定的Web页面,①http://www.csie.ntu.edu.tw/~cjlin/libsvm/Page10输出是该页面的分块及相应的块分类标记.实验环境为CPU(IntelCoreDuoProcessor,1.73GHz)+RAM(2GB)+WindowXP+Eclipse3.4.5.1数据集本文所使用的数据集SLPWebSet是我们通过半手工方式(种子URL+爬虫+手工筛选)从互联网上收集得到的.该数据集包括3200个页面,分别来自新浪、搜狐、IT168、东方财富、百度知道、51job、CSDN、猫扑、土豆网等国内10多个著名的门户网站以及论坛、博客.主题类目分别涉及新闻、财经、购物、IT产品、招聘、房产、视频等类别.数据集中的页面涵盖了新闻页面、产品页面、论坛页面、博客页面、视频页面等多种类型,以验证提出的方法对不同类型页面处理的鲁棒性.而在具体实验过程中,我们又从SLPWebSet中产生了3个子数据集:(1)页面分割数据集SLPWebSet-1.包括1000个页面,从SLP-WebSet中随机抽取,但要求覆盖到每个站点和主题.经过页面分割后,一共得到8623个页面分块;(2)标签传播数据集SLPWebSet-2.包括500个页面,4306个页面分块;(3)页面块分类数据集SLP-WebSet-3.包括500个页面,4275个页面分块.这里SLPWebSet-1、SLPWebSet-2和SLPWebSet-3互不相交.5.2评价指标本文的实验主要分为4个部分:页面分割、基于标签传播的训练集扩展、页面块分类和Web聚焦爬行.下面分别介绍各部分的评价指标.5.2.1基于节点密度熵的Web页面分割回率(Recall)指标.其定义如下:对页面分割算法的评价,本文采用了传统的召式中,Ns是当分割页面数为N时,由算法1得到的正确的分割页面块的数量.Nr是当分割页面数为N时,实际正确的分割页面块的数量.由于对页面分割结果的评价是一个较主观的过程,为了给出尽可能客观的结果,我们采用了如下的评价过程:(1)聘请了3个人员独立从事页面手工分割工作,主要从视觉的角度对测试页面进行分割并标记.他们相互之间不了解其他人的评价结果,以获取客观的评价结果.(2)将3个人员的分割结果进行表决,最后得到对测试集页面的手工分割结果.(3)使用算法1对测试集进行自动页面分割,然后将分割结果与手工标记的结果进行对比和统计,最后计算出R@N.为了获取算法1中α、β和γ这3个分割控制参数的最优值,本文专门设计了相关实验,给出了在不同参数组合条件下其页面分割情况.5.2.2基于标签传播的训练集扩展用了传统的精度(Precision)指标,其定义如下:对基于标签传播的训练集扩展的评价,我们采式中,N是指当前测试数据集中未标记页面块的数量.Sr是测试数据集中具有真实类别标记的所有页面块的集合.Sg为使用算法2后,测试数据集中被自动标记类别后的所有页面块集合.5.2.3基于SVM的页面块分类指标,精度定义如下:对页面块分类的评价采用了精度、召回率和F1式中,tp是指测试数据集中,当页面块的数量为N时,被正确分类为正例的数据数量;fp是指被错误分类为正例的数据数量.召回率定义如下:式中,tp含义同式(13);fn是指被错误分类为反例的数据数量.而F1定义为5.2.4Web聚焦爬行实验对Web聚焦爬行实验结果的评价,我们采用了平均收益率(AverageHarvestRate).该指标用于模拟精度指标(Precision).其定义[20]如下:式中,harvest_rate@N是指在t时刻,系统已爬取的页面数量为N时的收益率.V是当前系统已经爬取的页面集合.ri代表页面i与主题的相似度,如果页面i与主题相似,则ri取1;否则取0.5.3实验结果及分析按照5.2节的评价指标,我们对本文提出的方法进行了系统实现,并在SLPWebSet数据集上进行了实验.下面分别按照系统实现的步骤,对实验结Page11果进行分析.5.3.1基于节点密度熵的Web页面分割实验(1)页面分割控制参数取值分析为了确定算法1中页面分割控制参数α、β和γ的最优组合,我们采用了正交实验法来确定这3个参数的近似最优组合.根据初步的实验和经验,我们首先确定了3个参数的大致取值范围.其中,α的取值在0.4~0.6;β的取值在0.10~0.25;γ的取值在0.03~0.2.为此选用正交表L25(56),即选用5水平、6因素正交表,这样最多进行25个单独实验,便可确定最优参数组合.对于本文,只需选取3个因素,分别使用正交表的1、3、6列.表2给出了实验因素和水平的取值情况.实验结果选取系统的平均页面分割召回率R@50为指标.为此,我们从SLPWebSet-1数据集上随机抽取了50个来自不同站点且具有不同布局风格的页面,然后计算其R@50.水平因素1因素2因素312345表3给出了实验结果和分析.通过极差可以看到,参数的主次因素顺序为α、γ、β,其中α对分割召回率的影响较大,这与我们的实际观察是吻合的.此外,最优参数组合为(α4,β4,γ4),即(0.52,0.12,0.05).因素1因素2因素3αβγ实验号11110.41174540.8521220.56184150.8331330.63194210.8241440.76204330.8451550.73215520.8062250.72225130.8772310.71235240.8082420.79245340.8192530.72255450.79102140.73t10.6180.6960.682113340.77t20.7340.7220.698123450.76t30.7120.7520.773133510.68t40.8420.7940.782143120.64t50.8140.7560.770153230.71极差0.2240.0980.100164430.87α4β4γ4(2)页面分割平均召回率结果分析图4显示了在页面分割控制参数(α,β,γ)=(0.52,0.12,0.05)下,系统的平均召回率R@N的情况.其中,X轴表示系统已分割的页面数,Y轴表示在系统已分割的页面数量为N时平均召回率.我们从SLPWebSet-1数据集上随机抽取了500个页面进行页面分割实验,并统计了在分割页面数量递增的情况下的平均召回率.从图4上可以看出,R@N的值具有一定的波动性,但随着N的增加R@N的值趋于稳定,并保持在0.82以上.由此表明了本文提出的页面分割算法的有效性,同时也证明了该算法独立于待分割页面的布局风格和内容.图4基于节点密度熵的Web页面分割算法的平均召回率5.3.2基于KNNLP算法的训练集扩展算法实验(1)增量添加未知标签页面块数k的取值分析算法2中每次需要增量添加k个未标记页面块到初始已标记训练集中.其中k的取值是算法的关键,它决定了算法的精度.为了确定k的最佳取值,我们选取了基于SVM、KNN(K=4)的分类方法与KNNLP算法进行对比实验.针对本文使用的KNN算法,我们做了K=1~5的对比实验,发现K越大,分类精度越高,但K=4与K=5差别不大,考虑到算法效率,取K=4较合适.初始的未标记数据集M是从数据集SLPWebSet-2中随机抽取的100个页面,然后使用算法1对其进行页面分割,共产生876个块.再分别向M中添加r%(此处r是指已标记页面块占M数量的百分比)的标记页面块.图5给出了实验结果.其中,X轴表示训练集中已标记页面块占未标记页面块数量的百分比.Y轴表示在不同r下的平均标签分类精度.KNNLP算法的最优近邻数取7.从图5中可以看到3种算法的平均P@r(与P@N的定义略有不同,但作用完全相同)随着r的增加均呈上升趋势.在r较小时,由于已标记的页面块数量较少,基于监督的SVMPage12算法精度对训练集的大小较敏感,所以此时其P@r较低.而非监督的KNN算法由于缺少足够的标记数据,因此初始时,P@r也较低;对半监督的KNNLP算法,由于能利用非标记数据和标记数据内在结构的相似性,因此在初始时,其P@r明显高于SVM和KNN算法.随着r的增加,3种算法的精度都得到了提高,其中KNN和SVM算法提高得较显著,而KNN算法在r>10%后,迅速提高到0.6以上,但其后基本趋于平缓.这表明当数据集中的已标记数据达到一定数量后,KNN算法对已标记数据数量的增加并不敏感,对其产生影响的只是其K个最近邻(如K=4).总体来看3种算法中KNNLP算法取得的精度最好.而当r>150%时,KNNLP算法的精度达到0.86以上.因此本文确定k=1/1.8为最优值,即未标记页面块和标记页面块数量的比率在11.8.这样可以在保证KNNLP算法精度的前提下,通过设置较小的初始未标记页面块数量(这样标记页面块数量也相对较小,从而减少了手工标注的耗费),来自动扩展训练集.(2)KNNLP算法实验结果分析图6显示了算法2取得的平均精度的情况.其中,X轴表示KNNLP算法迭代的次数,右侧Y轴表示算法迭代次数为n时的平均标签分类精度,左侧Y轴表示当前训练集中已标记的页面块数量.KNNLP算法的最优近邻数取7.按上一小结的实验,我们取k=1/1.8,初始的已标记数据集是从数据集SLPWebSet-2中随机抽取的5个页面,共44个块,对其进行手工标记.然后分别按比例k向该已标记数据集中添加未标记页面块,即每轮添加25个未标记页面块,共进行了53轮添加,并测试在不同n下算法的P@N.图6中灰色的柱状图表示每次迭代后,当前扩展训练集中已标记页面块的总数量.曲线为平均精度曲线P@N,可以看到P@N呈一定的波动性.其原因是每轮用于扩展的已标记训练集(44个)是固定的,而每轮加入的25个未标记页面块是不同的,因此每次扩展时,其P@N出现一定的波动.需要指出的是算法2提出的扩展策略保证了本轮扩展产生的标签分类误差不会传播到下一轮迭代.因此,算法的总体标签分类误差等于每轮迭代分类误差的平均值:ε=1n=53时,N=1369,算法的平均P@N约为0.87.对后续的SVM分类器来说,其精度和已标记页面块的数量已经能够达到分类的要求.图6基于KNNLP的训练集扩展算法平均精度5.3.3基于KNNLP+SVM的页面块分类算法实验为了评价KNNLP+SVM页面块分类算法的实际运行效果,在相关文本块识别实验中,我们选取基本标签传播算法(LP)[16]+SVM、LNP[18]+SVM、半监督SVM(S3VM)算法[19]、KNN(K=4)算法和CECTD-DS算法[4]进行对比实验.其中S3VM算法采用了SVMlight工具①,该工具的原理是:训练数据集中包含少量的标记数据和大量未标记数据,算法直接在该数据集上学习分类模型,然后使用该模型进行分类.在相关链接块识别实验中,因为CECTD-DS只能识别页面主要文本块,因此我们只选取了LP+SVM、LNP+SVM、S3VM进行对比实验.测试数据是从数据集SLPWebSet-3中随机抽取8组测试数据,每组页面数N分别取10~250不等(见表4和表5所示).下面对6种算法使用的训练集做一说明:(1)对CECTD-DS算法来说,并不需要训练数据,只需直接对输入的Web页面进行识别即可.①http://svmlight.joachims.org/Page13(2)对于KNNLP+SVM算法来说,使用的训练集为上一小节实验获得的扩展训练集TS,TS共包含1369个页面块,分为3个类别(相关文本块、相关链接块、其他类别块).而对于LP+SVM、LNP+SVM算法来说,其获取训练集的过程与KNNLP+SVM算法相似,区别仅在于采用的标签传播算法不同.(3)对S3VM算法来说,其训练集中包含标记训练集和未标记训练集.其中标记训练集为KNNLP+SVM算法采用的初始标记训练集,即从数据集SLPWebSet-2中随机抽取的5个页面,共44个块,而未标记训练集为其后添加的未标记数据集(见上一小节).而KNN(K=4)算法采用的训练集只使用了KNNLP+SVM算法的初始标记训练集.表4给出了相关文本块识别的实验结果.从表4可以看到,在页面相关文本块的识别方面,6种算法的P@N和R@N随着N的增加均呈一定的下降趋势,但下降幅度不大.总的来说,P@N要好于R@N.从F1@N指标来看KNNLP+SVM算法性能最好,而CECTD-D算法性能与之较接近;LNP+SVS算法的性能好于LP+SVM算法,LP+SVM算法的性能好于S3VM算法,而KNN算法的性能最差.P@NR@NF1@NNKNNLP+SVMP@NR@NF1@N100.8850.7850.8320.8320.7660.7980.7920.7350.762300.8710.7790.8220.8160.7230.7670.7810.7260.752500.8550.7820.8170.8050.7260.7630.7720.6930.7301000.8200.7520.7850.7970.7210.7570.7570.6810.7171200.8180.7580.7870.7870.7160.7500.7230.6740.6981500.8230.7390.7790.7720.6930.7300.7320.6510.6892000.8180.7310.7720.7630.6890.7240.7160.6680.6912500.8270.7220.7710.7580.6770.7150.7060.6780.692NS3VM100.7810.7260.7520.7240.6610.6910.8620.7630.809300.7630.6920.7260.6950.6620.6780.8410.7580.797500.7720.6930.7300.6820.6540.6680.8230.7520.7861000.7570.6810.7170.6650.6470.6560.8150.7560.7841200.7230.6740.6980.6720.6420.6570.7910.7380.7641500.6950.6620.6780.6580.6470.6520.8140.7120.7602000.7240.6610.6910.6430.6280.6350.7980.6950.7432500.7120.6510.6800.6250.6130.6190.8120.6860.744表5给出了相关链接块识别的实验结果.与表4的实验数据显示的结果类似,从F1@N指标来看,4种算法中,KNNLP+SVM算法性能较好,在较大的实验数据集上(N=250),其F1@N达到了0.715,LNP+SVM算法性能次之,而S3VM算法性能最差.P@NR@NF1@NP@NR@NF1@NNKNNLP+SVM100.8350.7340.7810.8250.7360.778300.8310.7310.7780.8140.7250.767500.8150.7450.7780.8210.7360.7761000.7930.7380.7650.8070.7160.7591200.7910.7260.7570.7950.7060.7481500.7860.7150.7490.7730.6940.7312000.7720.6880.7280.7650.6920.7272500.7640.6720.7150.7520.6770.713NLP+SVM100.7680.7350.7510.7560.6820.717300.7730.7210.7460.7380.6740.705500.7650.7150.7390.7410.6710.7041000.7530.6930.7220.7260.6630.6931200.7460.6910.7170.7180.6540.6851500.7250.6860.7050.7060.6560.682000.7150.6670.6900.6920.6510.6712500.7080.6520.6790.6810.6450.663总之,从表4和表5的实验结果看,本文提出的方法在页面块分类方面是有效的.另外,采用标签传播(KNNLP、LNP和LP)+SVM算法的页面块分类效果要好于S3VM算法和KNN(K=4)算法.其原因可能在于:标签传播+SVM算法充分利用了未标记数据的结构特征,从而提高了分类的性能.5.3.4基于KNNLP+SVM的Web聚焦爬行实验上节的两个实验只是从相关文本块和相关链接块识别两个方面单独进行的,但如引言部分所述,本文提出的Web页面重要内容挖掘统一框架的重要意义还在于充分利用Web页面块内在的结构和语义关系.除了相关文本块和相关链接块识别和抽取的基本功能外,本文提出的方法还可以应用到其他许多Web应用领域,如Web页面多主题内容抽取、Web页面噪音内容过滤、Web聚焦爬行等.下面给出使用KNNLP+SVM方法用于辅助Web聚焦爬行的实验.Web聚焦爬行对于搜索引擎有着重要的作用[20-21].在本实验中,我们拟通过利用主要文本块来辅助爬虫增强主题链接的识别精度,以提高Web聚焦爬行的爬行收益率.实验采用了较经典的ATA(AnchorTextAnalysis)聚焦爬行算法[20],该算法主要通过分析锚链文本来获取链接主题相关的评分,以控制聚焦爬行的方向.提高Web聚焦爬行收益率的关键在于精确估算页面中链接的主题相关度.本文利用了主要文本块来增强主题链接的识别Page14精度.通过观察和实验,我们确定了如下的启发式规则:(1)主要文本块中包含的链接具有较强的主题相关性.(2)邻近主要文本块的链接块所包含的链接具有较强的主题相关性.为此我们制定了如下的爬行策略:首先爬取并分析主要文本块中的链接,然后是主要文本块近邻链接块中的链接,最后是其他链接块.本实验采用了文献[20]的数据集,并在3个不同的主题域(手机、数码相机、笔记本电脑)上进行了实验.图7显示了两种爬行策略在3个不同的主题域上取得的平均收益率情况.图7横坐标表示系统已爬行的网页数量,纵坐标表示在系统已爬取的页面数量为N时平均收益率harvest_rate@N(参见式(16)).从图7可以看到,两种爬行策略随着N的增加均出现了下降的趋势.出现下降趋势的主要原因是聚焦主题具有很强的稀疏性,在爬行过程中,已发现的主题相似页面的数量和系统已爬取的页面数量的增量幅度不同,很显然前者是小于后者的,所以造成平均收益率总体上出现下降.从图7中还可以看到,KNNLP+SVM+ATA的平均收益率明显高于ATA,在0.79~0.52之间.由于使用了KNNLP+SVM算法对页面块进行分类,并采用了优先爬取并分析主要文本块以及近邻链接块中链接的策略,增加了爬取主题相关链接的概率,因此显著提高了平均收益率.ATA的平均收益率在0.68~0.37之间,由于其仅仅使用了锚链文本的分析技术,但无法利用页面中链接与相关文本块主题相似度的语义关系,所以当带有相似锚链文本的多个链接出现在页面中的不同块中时(可能属于不同的主题),容易造图7KNNLP+SVM+ATA和ATA两种爬行算法的成爬行主题的漂移,从而使其平均收益率出现下降.6结语获取Web页面中的重要内容如文本和链接,是许多Web研究领域的热点问题.本文提出了一种Web页面重要内容挖掘的统一框架,采用该框架可以将Web页面块精确区分为多种类型,并且该框架独立于Web页面的类型和布局.通过真实Web环境下进行广泛的实验,证明该方法是非常有效的.然而本文在所提出的方法及其应用方面还存在一些不足:(1)对KNNLP算法来说,平衡因子δ的选择是一个难点问题,尽管本文提出了一个定量公式,但对其机理未做深入研究;(2)初始已标记数据集对基于KNNLP的训练集扩展算法的精度具有一定影响,对如何选择已标记数据集的问题,本文未加讨论;(3)本文提出的Web页面重要内容挖掘的统一框架除了能够完成基本的页面块有效分割和识别外,其更重要的意义应在于如何充分利用Web页面块内在的结构和语义关系,进行深层次的数据挖掘,以更好地应用到其他Web应用中.但本文对此问题,仅作了基于KNNLP+SVM辅助的Web聚焦爬行实验,而对其他方面的数据挖掘工作未做更深入的探讨.以上这些问题需要在未来的工作中加以解决.
