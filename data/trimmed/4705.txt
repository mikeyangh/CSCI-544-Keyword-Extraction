Page1通用图形处理器线程调度优化方法研究综述1)(武汉大学计算机学院武汉430072)2)(武汉大学软件工程国家重点实验室武汉430072)3)(东华理工大学软件学院南昌330013)4)(湖北工业大学计算机学院武汉430068)摘要随着通用图形处理器(GPGPU)并行计算能力的日益增强,其应用范围越来越广.然而由于不规则计算任务使得通用图形处理器资源难以得到充分利用,其性能并未达到最大化.在论述GPGPU微体系结构的基础上,该文重点讨论了多种针对GPGPU性能提升的线程调度优化方法,主要从针对优化分支转移提升线程级并行度、针对访存效率的提升和针对标量指令执行以提升资源利用率三方面的线程调度优化方法进行了分析和比较.另外,由于功耗问题已成为制约GPGPU发展的主要因素之一,该文还分析了影响GPGPU功耗效率的主要原因及目前提高GPGPU功耗效率的主要低功耗技术,并对目前针对GPGPU功耗优化的典型线程调度优化方法进行了分析比较.最后,该文指出了未来线程调度优化方法需要进一步探讨的一些问题.关键词通用图形处理器;线程调度优化;性能;功耗1引言图形处理器(GraphicsProcessingUnit,GPU)拥有强大的并行计算能力,因此,它已经成为目前面向高吞吐量、满足高性能计算需求的主流计算加速部件.GPU最初用于3D图形加速处理和计算,随着处理图形分辨率和图形处理速度要求的日益提高,它的并行计算能力得到了快速发展.2006年,NVIDIA公司推出了GeForce8800,它首次使用统一的渲染部件代替了各种可编程部件,解决了可编程GPU片上负载均衡的问题,成为当今通用图形处理器(GeneralPurposeGraphicsProcessingUnit,GPGPU)的雏形[1].而后,流式图形处理器的出现为图形处理器在通用计算领域的应用奠定了基础.随着GPGPU并行计算体系结构的快速发展,它在面向高吞吐量、高性能计算通用计算领域的应用越来越广泛.GPGPU展现出强大的并行计算能力、高吞吐量和优秀的能效比,使它成为了目前构建高性能平台的首选计算加速部件.在2010和2013年的全球超级计算机TOP500排行榜中,由国防科大主导研制的天河一号和天河二号均排在榜首[2-3].它们强大的浮点运算能力来自于其内部采用了大量的图形加速处理部件.另外,根据2014年上半年全球超级计算机TOP500排行榜,前500的超级计算机中有62台超级计算机采用了图形加速处理部件,比2013年增长了9台①.近年来,VLSI技术及制造工艺的快速发展使得GPGPU的并行计算能力有了很大提升.然而在体系结构和编程模型相同的情况下,GPGPU性能的高低主要受访存效率和计算任务并行度的影响.对于访存和计算模式不规则的应用程序,它的性能往往无法达到最大化.这种不规则性体现在:(1)应用程序中存在分支转移,使得GPGPU在同一时刻的处理任务分散,降低了任务处理的并行度;(2)访存的数据分布离散,使得并行处理任务的数据访问延时增加.GPGPU采用SIMD(SingleInstructionMultipleData)执行模式,即一条指令同时处理多个不同的数据[4-6].为了获得计算的高度并行,主流的图形处理器均派生出多个不同的线程对同一指令的不同数据进行并行处理,这种模式又称为SIMT(SingleInstruc-tionMultipleThread)执行模式[7-8].SIMT执行模式获得的并行处理属于线程级并行(ThreadLevelParallelism,TLP),它与指令级并行(InstructionLevelParallelism,ILP)和数据级并行(DataLevelParallelism,DLP)并为影响处理器并行计算能力的3个不同的并行级别[9-10].GPGPU通过支持大量线程的同时执行获得很高的TLP,并通过一定的线程调度策略保证执行任务的高效并行处理.然而由于不规则计算和不规则访存模式的存在,GPGPU中不同的线程调度策略对各种并行计算任务的执行效率会产生不同的影响.早在2007年,国外就有学者展开了针对GPGPU性能提升的线程调度优化研究[11].另外,随着功耗问题日渐成为制约GPGPU发展的重要瓶颈,也有部分线程调度策略对GPGPU的功耗优化进行了考虑.本文重点关注了近几年来国内外关于提升GPGPU性能和功耗效率的线程调度优化方法的关键技术和相关问题.本文第2节阐述主流GPGPU的体系结构和相关概念,并阐述GPGPU中线程层次结构和线程调度层次结构;第3节重点分析比较多种针对GPGPU性能提升和功耗优化的线程调度优化方法;最后提出GPGPU线程调度优化面临的挑战和进一步研究的方向.2GPGPU体系结构及相关概念NVIDIA、AMD和Intel是目前主流通用图形处理器的三大生产厂商,它们生产的GPGPU在宏观结构上相似,但在微观结构上存在较大差异.为了论述的统一,本文以NVIDIA公司生产的通用图形①世界超级电脑2014年上半年排名:中国天河二号三度夺Page3处理器体系结构为基准进行分析论述,后续的相关概念均以NVIDIA公司提出的GPGPU相关术语为参考.2.1GPGPU线程层次结构及线程调度层次结构为了便于线程管理,GPGPU对线程的组织通常分为三级:即线程组级(ThreadGroup,TG)、线程块级(ThreadBlock,TB)、线程网格级(Grid)[12].线程组(被称为warp)是线程并发执行的基本单位,通常由32个线程构成.它一般按锁步的方式执行指令[6,13-14],即线程组中所有线程执行完当前指令后方可执行下一条指令.线程块由多个线程组构成,是发射到GPGPU核上执行的基本单位.多个线程块又构成了线程网格.GPGPU采用三级线程层次结构,主要是为了方便在线程和线程处理的数据元素之间建立映射关系.图1展示了GPGPU的线程层次结构[12].线程调度是将计算任务分配到不同的处理单元并按照一定顺序执行的过程.GPGPU中的线程调度可以分为3个层次:kernel级、线程块级和线程组级.本文对GPGPU线程调度层次结构的描述主要基于文献[12,15-16].kernel是在GPGPU核上能被多个线程并发执行的函数,它需要在CPU端启动并发射到GPGPU的流式多处理器(StreamMulti-processor,SM)上执行.NVIDIA从Fermi架构的GPGPU开始支持同时发射多个kernel,之前的GPGPU只允许kernel的顺序发射.当kernel被发射之后,线程调度器将派生出来的线程块按一定的顺序发送到可用的SM上执行,通常情况下按轮转的方式发送.可以被发送到每个SM的线程块数依赖于每个SM上的资源使用情况,且不能超过物理规定的上限.被发送到某个SM上的线程组将按照一定的调度顺序在执行通道上执行.当线程组图2GPGPU宏观体系结构在执行过程中因访存失效等原因被阻塞时,该线程组将被插入到线程组阻塞队列,然后调度其他准备就绪的线程组继续执行.目前对线程调度优化的研究主要集中在线程块级和线程组级,对kernel级的调度优化研究相对较少.2.2GPGPU宏观体系结构GPGPU的宏观体系结构如图2所示①,它由多个SM组成,每个SM又被称为核.为了有效地控制逻辑控制单元的数量和芯片面积,通常将多个SM组织为一个图形处理核组(GraphicsProcessingCluster,GPC).每个SM包含了众多的处理单元(Processing-Unit,PU),每个处理单元负责最终的指令执行.①GPGPU-Sim3.xManual.http://gpgpu-sim.org/manual/index.php5/GPGPU-Sim_3.x_ManualPage4为了提高数据访问效率,GPGPU采用多级存储层次结构.常见的GPGPU存储层次结构分为寄存器级、高速缓存(cache)级和片外存储器级三级.其中高速缓存级又可以分为多级,目前常见的GPGPU采用两级高速缓存结构.每个SM中包含数量众多的寄存器,称之为寄存器文件,负责直接提供PU处理任务所需的数据.通常情况下,寄存器和一级cache放置在SM内部,二级cache和片外主存储器则通过片上互联网络和所有的SM相连.为了提高二级cache的访问效率,二级cache通常被划分为多个分区,每个分区通过片外存储通道与片外主存储器相连.另外,GPGPU通过片上互联网络将多个GPC和片外存储控制器连接起来,主要负责GPC与片外存储之间的数据通信.2.3SM微体系结构由于本文重点讨论GPGPU线程调度方法,在此图3SM微体系结构由指令发射器发射的指令到达SIMD流水线后端执行通道后需要通过寄存器访存部件获取执行所需要的操作数.为了满足大量并发线程的同时执行,GPGPU设置了数量众多的寄存器,并将这些寄存器组织为多个单端口的寄存器块(Bank),以便于多个并发线程同时访问.分支处理单元用于正确处理程序中的分支转移指令,并为每个线程组分配一个栈结构,称之为重汇聚栈.该栈结构由执行指令PC、分支重汇聚PC(RPC)和执行指令对应的活跃线程掩码位向量三部分组成.访存部件主要实现数据存仅重点介绍了SM的微体系结构.本文对SM微体系结构的描述基于文献[1,17-18].SM微体系结构如图3所示.整个SIMD流水线[19]可以分为前端和后端,前端主要包括取指部件、译码部件、记分板及指令发射器等部件,其余部件均属于后端[20].取指部件根据每个线程组对应的PC值负责从片外存储将指令取到指令cache中.指令cache中的指令则依次送到指令译码部件,经过译码后的指令被送到指令buffer中.指令buffer为每个线程组缓冲译码后的指令,便于指令的快速执行,并为每个线程组分配一定数量的专用指令槽.图3中的记分板部件用来记录每个线程组当前执行指令的目的寄存器的使用情况,主要用来判断执行指令之间的数据依赖关系.指令发射器按一定的策略选取准备就绪的指令发射.取,其中冲突检测部件确保对同一共享cache块的多个访问能正确执行;访存合并部件实现对访问同一数据cache行的多个访存请求进行合并,以减少访存次数;MSHR(MissingStatusHoldRegister,失效状态保留寄存器)用于记录访问cache失效的请求,并对同一cache行的多个访问失效请求进行合并.3线程调度优化方法GPGPU在越来越多的通用计算领域得到了广泛Page5应用,然而对于访存和计算模式不规则的应用程序,GPGPU的性能往往未能达到最大化.影响GPGPU性能达到最大化的原因是多方面的,程序执行过程中出现的线程级并行度降低和访存效率下降是其中的两个主要原因.其中线程级并行度的降低会使GPGPU计算资源不能得到充分利用,它主要由程序中存在的分支转移指令导致.访存效率下降则会使计算过程延时增加,访存离散和资源竞争导致的数据局部性破坏是其产生的主要原因.另外,致使GPGPU性能不能达到最大化的原因还包括由于线程执行速度不一致等因素导致核上资源利用率下降、标量指令冗余执行而导致的资源利用浪费等.目前的线程调度优化方法通常都以提高程序执行过程中的线程级并行度、访存效率和资源利用率等为主要目标.另外,由于功耗问题的日渐突出,越来越多的线程调度方法在提升GPGPU性能的同时,也考虑了对GPGPU产生的功耗进行优化.3.1针对分支转移的线程调度优化分支转移指令在通用应用程序中非常普遍,它主要分为结构化分支转移指令和非结构化分支转移指令两类.其中结构化分支转移指令一般包括if语句和循环语句中的分支转移指令等,而非结构化分支转移指令一般包括break指令、continue指令、goto指令以及异常跳转指令等[21].如果线程组在执行过程中遇到分支转移指令,那么该线程组中的每个线程将根据分支指令的计算结果而选择后续执行的分支路径,这容易出现部分线程执行不同分支路径的情形.而且由于SIMT执行模式只允许某线程组在某一时刻执行一条指令,这会致使不同的分支路径只能串行执行,从而必然降低任务执行的TLP.另外,如果同一线程组中执行不同分支路径的线程在执行完相应的分支路径后没有进行重汇聚处理,那么这也会影响到后续规则指令执行的TLP.基于栈的重汇聚机制(PostDominator,PDOM)可以有效地解决由于分支转移带来的性能降低[22-24],它在分支重汇聚处将属于同一线程组的线程重组为分支前的线程组,以提高后续指令执行的TLP.图4展示了一个传统的、基于栈的重汇聚机制的分支转移控制流的处理流程示例.图中左边部分是一个基本的分支转移控制流结构,其中每个字母代表一个基本语句块;每位数字表示一位掩码位,代表对应SIMD通道的状态,“1”表示该通道上有活跃线程执行,“0”表示该通道空闲.图中右边部分表示左边控制流图中的线程在通道中的执行情况,其中黑色箭头表示通道上有活跃线程执行,灰色箭头表示该通道空闲.该图表明出现分支转移时,SIMD通道未能满负载运行.由图4分析可以看出分支转移引起性能下降的根本原因在于分支转移降低了TLP,因此在出现分支转移时,应尽量提高并发执行的线程数.目前针对分支转移的线程调度优化方法主要有两类:(1)线程重组机制;(2)多路并行执行机制.3.1.1线程重组调度优化线程重组机制的基本思想是当遇到分支转移时,将执行相同指令、相同分支路径上的、对应不同SIMD执行通道的多个线程重新组合在一起,动态地形成并行执行宽度更大的线程组.当该分支路径执行结束后,该分支路径上的线程将在重汇聚点等待其他分支路径的线程执行结束,并与之重新组合成原先的线程组继续向前执行.该机制以提高某一分支路径上并行执行的线程数为目标,进而提高SIMD执行通道的利用率.线程重组又可以分为线程组间的线程重组、线程组内的线程重组及二者结合的线程重组这三类策略.Cervini[25]提出了针对包含SIMD功能单元的通用微处理器的动态线程重组机制.Fung等人[11]则提出了针对GPGPU的线程重组策略DWF(DynamicWarpFormation).在2011年,Fung等人[17]又提出了另一种线程重组调度策略TBC(ThreadBlockCompaction).相对于DWF,TBC将线程重组的范围限制在一个线程块中,而且不需要在每个时钟周期进行线程重组检查,仅在出现分支转移指令时进行线程重组,大大地减少了线程重组的次数以及由此产生的开销.另外,TBC对基于栈的重汇聚机制进行了扩展,这在一定程度上减少了SIMD资源利用率的下降.实验表明,相对于DWF,TBC的平均性能提升了17%.Narasiman等人[26]针对分支路径活跃线程数量不够的问题提出了LWM(LargeWarpMicroarchitecture)策略.该策略中定义的largewarp实质上是由多个线程组构成的线程Page6块,因此与DWF和TBC一样,LWM也属于线程组间的线程重组.但是由于LWF是以largewarp为调度单位,因此相对于TBC,它能在一定程度上减少因为对不同线程组进行同步而产生的开销.Malits等人[27]则提出了全局范围内的线程重组策略ODGS(OracleDynamicGlobalScheduling),将线程组间的线程重组从SM内扩展至SM之间.虽然线程组间的线程重组可以提高SIMD通道的利用率,但是此类方法需要将属于不同线程组的线程重组在一起,这会在一定程度上破坏了线程组内的数据局部性.另外TBC等策略还需要对多个线程组同步,因此会产生一定的开销.在此基础上,Vaidya等人[28]提出了线程组内的线程重组机制BCC(BasicCycleCompaction)和SCC(SwizzledCycleCompaction),其利用实际SIMD物理通道数比标准线程组宽度小的特点,仅将线程组内的线程重组为与实际SIMD物理通道宽度相等的线程组.Jin等人[29]提出的线程重组机制HWS(HybridWarpSize)也是基于这样的思想.线程组内的线程重组避免了对线程组内的数据局部性产生破坏,也无需对多个线程组进行同步.然而此类方法也存在着重组率不高、受到实际SIMD物理通道宽度的影响等不足,尤其是当SIMD硬件宽度与线程组大小相等时,该类策略对系统性能提升的意义不大.部分研究者将上述两类线程重组调度机制进行了有效地结合.Brunie等人[30]同时提出了线程组内的线程重组策略SBI(SimultaneousBranchInterweaving)和线程组间的线程重组策略SWI(SimultaneousWarpInterweaving),并将二者进行了有效地结合.但是与前面所提到的线程组间表1主要的线程重组调度优化策略特点线程重组策略DWF[11]TBC[17]LWM[26]SCC+BCC[28]HWS[29]SBI+SWI[30]CAPRI[31]3.1.2多路并行执行的线程调度优化在提高SIMD资源利用率的同时,线程重组调度在一定程度上会破坏线程组内的数据局部性,而多路并行执行线程调度则是在不破坏数据局部性的情况下尽量提升SIMD资源的利用率,并通过提供更多的线程调度实体来提升隐藏长延时访存的能和线程组内的线程重组不同的是,SBI和SWI可以将执行不同指令的线程重组在一起.Jin等人在文献[29]中对DWF算法进行了改进,允许线程改变执行通道,提升了线程重组率,并将HWS和改进的DWF进行了有效结合,平均性能比DWF提升了27%.在进行线程重组时,对应相同SIMD通道的线程组合在一起可能会产生冲突,这种情况下进行线程重组并不会提升性能,相反会产生额外开销.对此,Rhu等人[31]提出了CAPRI(Compaction-AdequacyPridictor)策略,在不同的线程组间进行选择性的重组压缩调度,使得线程重组调度时机的选取更加合理有效,减少了无效的线程重组调度.CAPRI策略在较大程度上解决了线程重组的冲突问题,但是该问题并没有完全得到避免.之后Rhu等人[32]又提出了SLP(SIMDLanePermutation)策略,通过置换线程对应的SIMD通道进一步解决了线程重组出现的冲突问题,提高了线程重组策略的有效性.然而该策略进行的SIMD通道置换操作会产生较大开销,而且频繁置换通道产生的开销对系统性能提升的影响是不可忽略的.线程重组调度优化策略能在出现分支转移的情况下提高不同分支路径上的SIMD资源利用率,减少闲置的SIMD通道数,它在一定程度上提高了系统性能,但是在提高线程重组的有效性、平衡线程重组的收益和减少开销等方面依然没有得到很好的解决.表1归纳了几种主要的线程重组调度优化策略的特点.从表中可以看出组间的线程重组能获得较高的SIMD资源利用率,而组内的线程重组保持了线程组的数据局部性,其访存失效率更低.受限SIMD硬件宽度不受限不受限不受限不受限不受限力.它是另一类常见的、提升分支转移处理效率的线程调度优化机制,主要通过改变传统分支路径顺序执行的方式,尽量让多个分支路径上的线程交替调度执行或同时调度执行.当某一个分支路径上的线程组由于长延时访存等待而造成流水线空闲时,交替调度执行其他分支Page7路径的线程组能够填补该空闲,这样既能提高并发执行的线程数量,也能在一定程度上提升访存级并行度(MemoryLevelParallelism,MLP).Meng等人[33]、Yu等人[34]和ElTantawy等人[35]分别提出了多路交替执行的线程调度优化策略DWS(DynamicWarpSubdivision)、PDOM-ASI(PDOMwithAllSub-warpsIssuable)、MPIPDOM(Multi-PathIPDOM).其中DWS和MPIPDOM采用warp分片表实现了多个子warp(warp中执行相同分支路径的部分线程集合)的交替执行,PDOM-ASI则通过对分支控制结构进行二叉树分析实现了多个子warp的交替执行.而Rhu等人[36]提出的双路径交替执行策略DPE(Dual-pathExecution)通过对重汇聚栈结构的修改,仅实现了双分支路径上的线程交替执行.另外,多路线程交替执行需要考虑当子warp执行结束后,及时对它们进行重汇聚,以保证后续指令保持高的SIMD通道利用率.PDOM-ASI、DPE实现了及时重汇聚策略,MPIPDOM实现了尽早重汇聚策略,DWS策略则采用延迟重汇聚机制,但一定程度上降低了后续规则指令执行时的SIMD通道利用率.另外,这4种多路线程交替调度策略在一定程度上均能有效地解决SIMD执行深度不够(可供调度的线程实体不足)的问题.Lashgar等人[37]提出的HARP(HarnessinginActivethReadsinmany-coreProcessors)策略则有效地结合了线程重组机制和多路交替调度机制,它能同时解决分支转移引起的SIMD执行宽度和执行深度下降的问题.交替执行的线程调度方式在宏观上实现了多路分支路径的并行执行,增加了可供调度的线程实体,相对于传统的顺序执行,它能进一步提升TLP.但在一般情况下,这种优势只有在活跃线程组不够的情况下方能体现出来,因为当存在其他活跃线程分组时,快速切换到其他活跃线程组可以隐藏线程调度实体的长延时访存操作.另外,交替执行的线程调度也可以看做是将后面调度线程组的调度次序提前,提前执行的线程组产生的访存结果可以被先前调度的线程组使用,从而提高并发线程的访存并行度,有利于提升系统的访存效率.表2主要的多路并发执行线程调度策略特点多路并发执行策略SBI+SWI[30]DWS[33]MPIPDOM[35]HARP[37]MSMD[39]多路交替调度执行能使不同分支路径上的线程组相互隐藏各自的长延时访存操作,然而微观上某一时刻仍然只有一条分支路径上的线程执行,SIMD资源的利用率仍然没有达到最大化.多路线程同时调度执行是另一类提高SIMD通道利用率的方法,它可以使不同分支路径上的线程同时执行各自的指令.相对于多路交替执行的线程调度方法,多路线程同时调度执行能进一步提高SIMD通道的利用率.文献[38]提出了双指令多线程(DualInstructionsMultipleThreads,DIMT)的思想,通过对指令发射等部件的改动来实现双分支路径的同时执行.3.1.1小节提到的SBI和SWI策略通过对指令发射部件的修改也实现了双分支路径上同时发射指令.Wang等人[39]提出并实现了MSMD(MultipleSIMD,MultipleData)执行模型,设置了多个可灵活划分的、独立的SIMD数据通道,使得不同分支路径上的线程组可以同时执行,大大地提高了程序执行的TLP.MSMD模型实质上是对MIMD执行模型的发展,它包含了SIMD模型的思想,但在硬件实现上更加复杂.Dasika等人[40]针对科学计算提出的PEPSC(Power-EfficientProcessorforScientificComputing)体系结构和文献[41-43]提出的TSIMT(TemporalSIMT)执行模式均支持多个分支同时执行,与MSMD模型的思想相似.但是,相对于SIMD模型,它们和MSMD模型一样,在物理实现上要复杂得多,需从面积和功耗等方面进行更多的考虑.多路并行执行的线程调度优化能在出现分支转移时进一步提升程序执行的TLP,还能在一定程度上提升系统的MLP.但是多路并行执行的线程调度优化策略仍然存在着提升TLP有限、硬件物理开销较大、重汇聚时机选取欠合理等不足,可以在这些方面做进一步的研究.另外,可以综合考虑线程重组和多路交替执行的两类线程调度机制的优势,对它们进行有效地结合,使得各分支路径的SIMD执行宽度和执行深度得到更有效的提升.表2归纳了几种常见的多路并行执行线程调度策略的特点.从表2中可以看出,由于DWS采用了延迟重汇聚策略,它对TLP提升的幅度最小.重汇聚时机选择性重汇聚延迟重汇聚尽早重汇聚及时重汇聚及时重汇聚及时重汇聚Page83.1.3分支转移中的重汇聚机制将执行不同分支路径的线程在重汇聚点上进行重汇聚同步,将它们重新组合成分支转移前的线程组,以保证后续规则指令执行过程能保持较高的TLP.传统的PDOM重汇聚机制选择立即后支配块作为重汇聚点[11,44].然而重汇聚点的选取对存在分支转移程序的执行效率有较大的影响.如果重汇聚点选择较晚,会在一定程度上降低后续规则指令执行的TLP,但是对于某些应用程序尤其是访存密集型程序,选择较晚的重汇聚点对提升整个程序执行的MLP是有益的.这主要得益于此类程序的线程组内部存在数据局部性的可能性较大,提前执行的部分线程获取的数据则可能被落后的线程所使用,从而提高落后线程后续的cache访问命中率.因此,针对不同的计算任务,应选择不同的重汇聚策略.但是,采用何种重汇聚策略需要综合考虑重汇聚策略带来的性能收益和由此产生的开销等因素,只有当收益大于开销,重汇聚策略才是有益的.Meng等人[33]提出的DWS策略对所有的情形均采用延迟重汇聚策略.文献[17,30,35]则认为尽早重汇聚以同步不同执行分支路径的线程对提升TLP有利,并对尽早重汇聚机制及实现进行了分析讨论.Diamos等人[45]专门针对非结构化的分支转移,提出了TF(ThreadFrontier)重汇聚机制,结合编译技术实现了尽早重汇聚机制,减少了某些基本指令块的重复执行.上述重汇聚策略的最大不同在于重汇聚时机的选择,但是它们各自对所有的应用程序均采用统一的重汇聚机制,并未综合考虑性能收益和开销等多种因素.适宜的方法是根据不同应用程序的特点选择合适的重汇聚时机和相应的重汇聚机制.3.2针对访存效率提升的线程调度优化访存效率是影响GPGPU整体性能高低的主要因素之一.除了本身物理条件的限制,访存离散和片上存储资源访问竞争引起的数据局部性破坏等因素也会影响GPGPU的访存效率.针对访存效率提升的线程调度优化方法通常以减少片上存储资源访问竞争和减少访存离散为主要目的,并通过数据预取等手段来提升数据访问的命中率.3.2.1针对片上存储资源访问竞争的线程调度优化存储系统的带宽是影响GPGPU性能的重要瓶颈,在片上集成层次化的存储资源有利于提升GPGPU的访存吞吐量.GPGPU片上存储资源通常包括寄存器文件、一级数据cache、一级指令cache和纹理cache等.由于芯片面积的限制,相对于片上同时执行的成千上万个线程,这些资源显得非常有限.当调度到核上的线程数超过一定限度,存储资源将会出现不足.如果继续调度线程到这些核上执行,则会产生对核上存储资源的访问竞争,此时会置换部分已分配的资源,必然会降低被置换资源对应线程的访存效率.因此当出现资源访问竞争时,系统的性能会出现一定程度的下降,严重时会出现访存“抖动”现象[46-48].因此,尽量减少线程对存储资源的占用,或尽快释放对存储资源的占用,是减少片上存储资源访问竞争的两种直观思路.Bakhoda等人[49]研究发现,有相当部分的应用程序在并发执行的线程块数量受限的情况下能获得更高的性能.线程调度优化成为了目前解决片上存储资源尤其是cache资源访问竞争问题的重要方法,它可以从3个方面进行考虑:(1)通过优化调度到GPGPU核上的并发线程数量减少对片上存储资源访问竞争;(2)通过混合执行不同类型的计算任务减少对片上存储资源访问竞争,并尽可能地提升片上资源的利用率;(3)通过优化线程的调度顺序减少片上存储资源的访问竞争.改变线程调度的顺序和时机、有效控制和优化调度到核上的线程数量,是目前线程调度优化解决片上存储资源访问竞争的主要思想.Rogers等人[50]提出的CCWS(Cache-ConsciousWavefrontScheduling)策略通过对线程组cache访问失效情况的分析,动态调整调度到GPGPU核上的线程组数量,但是它需要等到数据局部性破坏之后才进行相应的调整.随后Rogers等人[48]又提出了另一种降低cache资源访问竞争的线程调度策略DAWS(Divergence-AwareWarpScheduling).与CCWS策略不同的是,该策略通过分析cache的使用情况主动地动态调整调度到核上的线程组数量,破坏数据局部性的情况大幅度地减少.Kayiran等人[16]则结合不同计算任务的特点动态调整调度到核上的线程块数量.他们通过分析发现,不同类型的计算任务运行不同数量的线程块时会表现出不同的性能效果,并提出了DYNCTA(DynamicCTAscheduling)线程调度策略.与CCWS和DAWS策略不同,该策略定期对GPGPU核的状态进行定量分析,然后动态调整调度到GPGPU核上的线程数量.Cheng等人[51]针对多核处理器的访存竞争问题也提出过类似的模型.不同的是,他们通过对多核处理器性能的定量分析来动态地确定最优并发访存任务数.与前面几种动态调整调度到核上的线程块数量的方法不同,Lee等人[52]提出的LCS(LazyCTAScheduling)+BCSPage9(BlockCTAScheduling)的线程调度策略对调度到核上的最优线程组数量进行静态量化.限制调度到核上的线程组数量是前面几种算法的主要特点,可以有效地减少cache的访问竞争和对数据局部性的破坏,但在一定程度上限制了片上资源的利用率.Xiang等人[53]提出了WarpMan(Warp-levelresourceManagement)线程调度策略,充分考虑了片上存储资源竞争的问题,在保证不破坏数据局部性的基础上,尽量调度更多的线程组到核上运行.文献[54-55]通过实验分析发现,一次运行一个计算任务会造成GPGPU资源利用率较低,通过对GPGPU资源空间划分或细粒度控制,让多个kernel并发执行,可以有效地提高片上资源的利用率.但是在组合计算任务时,应尽可能选择不同特点的kernel,以尽可能减少kernel之间产生的存储资源竞争.Zheng等人[47]提出的CCA(adaptiveCacheandConcurrencyAllocation)线程调度策略充分利用了这一性质,对于表现出数据局部性的线程组采用动态数量调节调度策略,而对于不具有数据局部性的线程组则采用cache绕行策略调度执行,这既限制了访问cache的线程数量,又提高了并发执行的线程数量.相对于WarpMan,由于有更多的执行任务,CCA策略具有更大的SIMD执行深度,对长延时访存操作的隐藏能力更强.Awatramani等人[56]也提出了同时调度不同类型kernel的线程调度策略KITBS(KernelInterleavedThreadBlockScheduling),并明确指出将访存密集型kernel和计算密集型kernel进行组合调度.Lee等人[52]在文献中提出的mCKE(mixedConcurrentKernelExecution)线程调度策略也支持发射多个kernel到同一个核执行,但是文中对于如何组合kernel没有进行深入的讨论.调度多个kernel到同一个核上执行确实能提线程调度策略解决资源竞争的方式线程调度粒度是否提升资源利用率是否提升TLP保持数据局部性表3常见的针对片上资源竞争的线程调度优化策略特点DYNCTA[16]动态限制调度数量TL[26]CCA[47]DAWS[48]CCWS[50]mCKE[52]混合调度不同计算任务WarpMan[53]动态限制调度数量OWL[57]3.2.2针对访存离散的线程调度优化GPGPU通过支持大量并发线程的同时执行来高片上资源的利用率,但是需要同时调度不同类型的kernel,否则会加剧对片上资源的访问竞争.如何合理组合kernel成为了该类线程调度方法的关键,对于包含kernel类型相同的应用程序并不适用,因此它的应用具有一定的局限性.和上述两类方法一样,改变线程的调度顺序也能有效地减少片上存储资源访问竞争.Narasiman等人[26]为了提升长延时访存操作的隐藏能力,提出了两级调度策略TL(Two-Levelscheduling),将连续的线程块进行分组,限制某一时间段内同时执行的线程数量,从而有效地避免了片上资源访问竞争,但这会在一定程度上降低程序执行过程的TLP,也会降低片上资源的利用率.Jog等人[57]在TL的基础上提出了OWL(CooperativeThreadArrayAwareWarpScheduling)线程调度策略,它也是按线程块组进行两级调度.另外OWL策略还通过调度不连续的线程块组来提高访存的并行度,有利于系统整体访存性能的提升.优化线程的调度顺序可以在一定程度上减少片上资源的访问竞争,但是如果调度顺序不当,有可能破坏线程组之间的数据局部性,甚至会降低程序执行过程中的TLP和片上资源的利用率.上述三类线程调度优化方法在一定程度解决了片上资源访问竞争问题,有的方法甚至进一步提升了片上资源的利用率.然而这些线程调度优化方法仍然存在以下不足:限制核上并行执行的线程数量与提高片上资源利用率存在冲突;同时调度多个kernel的适用性较差,对kernel进行定性分析机制有待完善;改变线程的调度顺序与保持数据局部性存在冲突.因此,在解决片上存储资源访问竞争的问题上可以在这些方面继续展开研究.表3对几种常见的针对片上存储资源竞争的线程调度优化方法进行了归纳和比较.隐藏长延时访存操作,当某个线程组处于长延时访存等待状态,GPGPU会立即切换到其他准备就绪Page10的线程组继续执行,因此只要有足够活跃的线程组,SIMD流水部件就会一直处于工作状态.然而,同一个线程组中不同线程访问的地址可能不在同一个cache行,且它们的访存情况也可能存在差异,部分线程由于L1cache访问失效而处于等待状态,另一部分线程的L1cache访问命中.另外,由于SIMD执行模式采用锁步执行方式,线程组中L1cache访问命中的线程只能等待L1cache访问失效的线程,导致整个线程组的执行被延时,我们称这种情况为访存离散(MemoryDivergence)[33].通常情况下,访存离散比分支转移发生的概率大很多[58].如果在发生访存离散时没有足够活跃的线程组进行切换,SIMD流水线将处于空闲状态,从而对系统性能造成影响.另外,访存离散还会导致相应线程组的执行被挂起,延缓了其执行进度,其持续占有的资源对其他线程的执行也会产生影响.文献[59-61]针对CPU的cache访问失效提出了向前执行(RunaheadExecution)的策略,他们将这种向前执行看作是无效的“预执行”,但是可以将后续指令及其数据提前预取到cache中,减少后续指令的cache访问失效率.借助于该思想,在GPGPU中,当某个线程组出现访存离散时,也可以允许部分L1cache命中的线程继续向前执行,从而加速整个线程组的执行进程.Tarjan等人[58]针对访存离散提出了AdaptiveSlip线程调度策略,重点讨论了针对循表4常见的针对访存离散的线程调度优化策略特点线程调度策略AdaptiveSlip[58]RobustSIMD[62]DWR[63]3.2.3针对数据预取的线程调度优化数据预取是提前将数据从片外存储取到缓存中以提升后续数据访问命中率的技术,它是提高处理器执行效率的重要手段之一.然而GPGPU采用的SIMT执行模式可以通过众多并行执行的线程来隐藏长延时访存操作,并出于考虑能效优化的目的,在GPGPU中去掉了硬件预取部件[64-65].通过线程调度优化提高数据预取能力,有利于进一步地开发线程之间的数据局部性、提高cache访问命中率、提升隐藏长延时访存操作能力.GPGPU中的数据预取可以通过线程调度优化来实现,其主要思想是在线程执行过程中根据一定的预取距离和预取度,在为自己获取数据的同时,将环中出现访存离散时的线程调度优化机制.但是由于该策略总是在下一次循环迭代中才对线程组进行重汇聚,容易造成较大的等待延时.在此基础上,Meng等人[33]提出了DWS策略,能对任意发生的访存离散进行处理,并提出了相对更加有效地重汇聚机制.随后Meng等人[62]分析认为宽度越大的SIMD越容易出现访存离散,他们提出了Robust-SIMD自适应调度策略,对不同应用程序的执行性能进行抽样测试分析,以此来调节SIMD的执行宽度和执行深度,减少了访存离散的发生.Lashgar等人[63]同样认为宽度大的线程组容易发生访存离散,他们提出了DWR(DynamicWarpResizing)策略,通过动态调节线程组大小来减少访存离散的发生.访存离散发生时允许部分访存命中的线程继续向前执行,实质上是以减小warp宽度而降低TLP为代价的,因此需要考虑尽早重汇聚线程组中的所有线程.而且,当存在足够多活跃的线程组时,切换到其他活跃线程组可以隐藏由于访存离散带来的长延时等待,这种情况下此类方法对系统性能提升的意义不大,因此还需要考虑策略选择的时机.另外,如果能尽量保持数据局部性不被破坏,且尽可能开发线程之间的数据局部性,则能更有效的解决访存离散问题.表4归纳了几种常见的针对访存离散的线程调度优化方法的特点.是否提升TLP同一个线程组或其他线程组中其他线程的所需数据提前预取到缓存中.其中预取距离是指相对于特定数据请求而进行预取的提前量,它制约着数据预取的准度;预取度是指一次预取数据的大小,它的大小的选取对片上资源的竞争会产生一定的影响,因为预取过多的数据会挤占其他线程所需的资源.3.2.2小节中针对访存离散的线程调度优化策略允许cache访问命中的线程继续执行,实质上也可以看作是一种数据预取机制,因为继续向前执行的线程后续访存获得的数据有可能成为落后线程后续访问的数据,可以提高落后线程的cache访问命中率.这类数据预取机制主要利用了连续线程之间具有数据局部性的特点,然而却以牺牲一定的TLP为代价.NVIDIA公Page11司Fermi系列的GPGPU采用的轮转线程调度策略RR(Round-robinScheduling)和Narasiman等人[26]提出的TL线程调度策略同样利用了数据局部性原理来实现数据预取机制.但是由于连续的线程组之间距离太近,可能会使数据预取“太迟”.Jog等人[66]提出了预取感知的线程调度策略PAS(Prefetch-awareScheduling),通过改变连续线程组的调度顺序使得原本连续的线程组之间保持一定的预取距离.该策略在调整预取距离的同时,还考虑了对预取度的控制,仅对访问频度较高的内存块实行数据预取.他们提出的另一种线程调度策略OWL与PAS相似,也是通过调整连续线程块的发射距离来进行数据预取.但是与PAS不同的是,OWL在数据预取时还考虑了如何提升访存并行度.通常情况下对于规则的访存模型容易确定数据预取的距离,但是对于不规则的访存模型来说,则容易出现数据预取“过早”或“过晚”,导致预取的数据未及时到来或被覆盖.这种无效的数据预取会造成有限存储带宽的浪费,甚至有可能造成整体访存性能的下降,因此应根据程序执行过程中系统的状态变化动态调整数据预取的距离和预取度,从而进一步提高数据预取的准度和有效性.前面提到的PAS和OWL策略均未考虑后一种情况,它们采用静态方法确定数据预取的距离,对于访存不规则的应用程序来说,其预取的准确度势必会下降.Lee等人[67]则提出了自适应预取机制MT-prefetching(Many-Threadawareprefetching),在线程执行过程中对预取数据替换率和预取访存合并率两个度量进行周期性采样分析,并以此自动调节数据预取行为.Dahlgren等人[68]通过对预取准确度的度量来自适应地调整预取距离.Srinath等人[69]提出了更复杂的、基于反馈的预取机制,综合考虑了预取的准确度、预取的及时性以及cache污染等多个因素.无论是规则的数据预取,还是基于反馈的动态预取机制,一般只有在活跃线程数较少的情况下才能充分发挥其优势,因为在活跃线程数足够的情况下长延时访存可以被隐藏.Sethia等人[70]提出的APOGEE(AdaptivePrefetchingOnGPUsforEnergyEfficiency)预取策略着重研究了活跃线程组少的情况下的数据预取,取得了较好的能效比.另外,Yang等人[71]通过编译的方式提前将数据预取到寄存器中.但是在一定程度上增加了对寄存器资源的使用,对系统功耗产生影响.有效地结合线程调度优化方法,根据系统状态变化进行动态的数据预取能进一步地提升数据预取的有效性.但是对于不同类型的计算任务,数据预取表现出来的重要性不同.例如,对于计算密集型的计算任务,数据预取的优势并不能得到很好的体现.因此,针对不同的计算任务应采取不同的预取策略.表5归纳了几种常见的针对数据预取的线程调度优化方法的特点.表5常见的针对数据预取的线程调度优化策略特点线程调度策略DWS[33]静态的数据预取OWL[57]静态的数据预取预期距离PAS[66]静态的数据预取MT-prefetching[67]3.3针对标量指令执行的线程调度优化SIMT执行模式让同一条指令在不同的数据上执行相同的操作,以此获得高的计算吞吐量和高性能.然而在应用程序执行过程中存在相当数量的SIMD指令,它们对应的线程不仅处理的操作数完全相同,而且其输出结果也完全相同,这些指令被称为标量指令[20,72-73].标量指令的执行会产生大量重复冗余操作.在同一个线程组或线程块中,如果一条标量指令只需要一个线程执行,则可以节省大量的计算资源和存储资源.对此首先需要识别应用程序中存在的标量指令,其次是对标量指令的执行进行优化.目前常见的识别标量指令的方法有两类.一类是通过编译的方式对代码进行静态分析识别[72,74-75],另一类则是通过硬件机制进行动态识别[76].识别后的标量指令一般只需执行一次,如果利用传统的SIMD通道执行,则会造成很大的片上资源浪费.为此有不少做法是对硬件进行修改,增加特定的标量指令执行部件和标量数据存储部件.AMD发布的GNU体系结构中引入了专门的标量处理单元[77].Xiang等人[73]则通过增加单独的标量寄存器文件解决了标量指令操作数的存储问题.然而这些方法都需要对传统的SIMT系统结构进行较大的改动,线程调度优化则可在不用对SIMT系统结构进行较大改动的情况下实现标量指令的高效执行.Yilmazer等人[20]提出了SW(ScalarWaving)+SSSW(SimultaneousScalarandSIMDgroupWaving)线Page12程调度策略.一方面将执行相同PC的标量指令组织为标量波执行,另一方面将标量波和线程组组合在一起执行,实质上也是一种线程压缩重组调度策略.由于进行了标量识别,SW+SSSW策略比普通线程重组调度策略具有更好的“压缩”性能.但是SW策略只对线程组内的指令进行标量化执行,对线程组间指令的标量化执行未进行分析讨论.对标量指令的识别和执行能够大大减少某些指令的执行次数,不仅可以节省大量的计算资源,还为降低系统功耗提供了很大空间.若能从线程调度优化的角度充分利用这两方面的优势,则能更好地提升系统的性能和功效,目前这方面开展的研究工作相对较少.3.4针对功耗优化的线程调度优化相对于CPU,GPGPU能获得更高的能效[78-80].目前应用广泛的动态电压频率调节技术(DVFS)能有效地降低GPGPU的动态功耗[81-83],但是随着片上集成的晶体管数量越来越多及制作工艺的提升,静态漏电流功耗在整个GPGPU功耗中所占的比重越来越大[81-82,84-85].动态功耗和静态漏电流功耗已经成为GPGPU功耗的两个主要来源.过高的功耗会产生大量的热量,直接影响电子元器件的稳定性、可靠性和使用寿命,功耗问题成为制约GPGPU发展的重要瓶颈之一.动态功耗通常是由晶体管的充放电产生的功耗,其大小通常与电子元器件的工作电压和频率相关.动态降低不同部件的工作电压和频率是目前有效降低GPGPU动态功耗的主要手段之一.Gu等人[86]针对交互式游戏软件,利用DVFS技术对图形处理器的动态功耗进行优化.Mochocki等人[87]在分析了手机3D图形处理器不同流水阶段负载不均衡特性的基础上,结合DVFS技术有效地降低了手机3D图形处理器的动态功耗.静态功耗通常是由漏电流产生的功耗.漏电流的产生是由于CMOS工艺的提升使得晶体管绝缘层变薄,导致电流容易透过绝缘层.通过门控技术适时关闭或休眠空闲部件是目前降低静态功耗的常用技术.Wang等人[81]针对着色器部件、固定功能几何单元和非着色执行单元分别提出了PSS(PredictiveShaderShutdown)、DGP(DeferredGeometryPipe-line)和超时门控的3种体系结构级门控策略.Wang等人[88]着重针对L1cache和L2cache的访问进行能耗门控,将它们的工作状态在激活、休眠和关闭3种状态间动态切换.Abdel-Majeed等人[89]则专门提出了针对GPGPU中寄存器文件功耗优化的门控方法,通过设置三模态寄存器访问控制单元和对活跃掩码行为感知的门控单元,降低了静态漏电流功耗和动态功耗.DVFS和功耗门控技术能有效地降低GPGPU的动态功耗和静态漏电流功耗,然而单一的根据各个部件的运行状态进行功耗控制优化的空间有限.目前有部分对GPGPU功耗进行优化的研究工作结合了线程调度优化方法,通过对线程调度的优化使得各部件的运行状态呈现更佳的规律性,从而使之能更好的匹配DVFS和功耗门控技术的应用.Abdel-Majeed等人[82]提出了门控感知的二级warp调度策略GATES(Gating-awareTwo-levelWarpScheduler),优先调度发射执行同类型指令的线程组,以使其他执行单元获得较长的空闲时间,从而有利于通过功耗门控技术降低功耗.Xu等人[85]提出了模式感知的二级warp调度策略PATS(PatternAwareTwo-levelScheduler),优先执行具有相同分支模型的线程组.林一松等人[90]提出的功耗优化模型结合了计算并行度和访存并行度,根据并发执行的线程数确定调频因子的大小,并结合DVFS技术对GPGPU的动态功耗进行了有效地优化.上述几种线程调度优化方法均以功耗优化为主要目的,而前面章节分析的线程调度优化方法则以性能优化为主,其中也有部分研究工作分析了这些线程调度方法对功耗产生的影响.Gebhart等人[91]提出的二级warp调度策略TL通过设置专门的寄存器缓冲来保留活跃线程组的上下文,减少了对寄存器文件的访问,从而大大降低了访问寄存器产生的功耗.Xiang等人[73]提出了对标量指令执行标量化,大大减少了某条标量指令执行所需要的各种资源,结合功耗门控技术能有效地降低静态功耗.随后他们提出的WarpMan线程调度策略及时回收执行完成的线程组占用的资源,也为功耗门控优化提供了机会.Rogers等人[48]对其提出的DAWS线程调度分析指出,由于提高了Cache访问命中率,减少了片外访存次数,能有效地降低由于访存产生的动态功耗.尽管目前有部分线程调度优化算法考虑了功耗优化,但是以功耗优化为主要目标的线程调度优化研究工作仍然不多,尤其是结合低功耗优化技术的线程调度优化方法的研究还相对较少.表6归纳了几种常见的针对功耗优化的线程调度优化方法的特点.Page13表6常见的针对功耗优化的线程调度优化策略特点线程调度策略主要目标DAWS[48]WarpMan[53]提高性能GATES[82]PATS[85]基于并行度分析模型[90]TL[91]4结束语目前研究者提出的线程调度优化方法均在一定程度上解决了影响GPGPU性能和功效提升的相关问题,但是这些线程调度优化方法都只侧重考虑解决其中某一个或几个方面的问题,导致GPGPU的性能和功效仍然未能达到最大化.GPGPU的不断发展使得线程调度优化在以下方面还存在挑战:(1)随着GPGPU微体系结构的发展,尤其是片上融合CPU-GPU微体系结构的发展,可以大大减少CPU和GPU之间数据传输产生的开销,然而还需要针对新的微体系结构的特点对线程调度进行分析并优化,解决在CPU和GPU间合理分配计算任务、利用CPU数据预取优势以减少GPU的访存开销及片上资源的访问竞争等方面的问题;(2)当前新型存储技术在存储密度、访问速度和功耗方面都有一定的优势,为GPGPU的性能提升和功耗优化提供了新的空间,需要将线程调度优化方法和新的存储技术进行有效地结合;(3)在不少线程调度优化方法中,某些参数的选取及某些自适应线程调度机制还有待完善,结合编译技术对代码进行静态或动态的分析,为线程调度优化提供更加有效合理的决策支持,例如通过编译技术实现对标量指令的识别、线程重汇聚点的合理判断等;(4)随着新的GPGPU微体系结构对多kernel并行执行的支持,通过线程调度优化更好的解决多个kernel混合执行产生的资源竞争等问题;(5)功耗问题仍然是影响GPGPU发展的主要瓶颈之一,需进一步结合线程调度技术更好的为功耗优化服务;(6)结合多种线程调度优化方法的优点综合考虑影响GPGPU性能的多种因素,使线程调度优化方法更加合理有效.未来以进一步提升GPGPU的性能和能效为主要目标的线程调度优化可以在以上方面做进一步的研究.致谢感谢何炎祥教授主持的博士讨论班上各位师生所发表的意见和建议.
