Page1基于线性阵列处理器的GRAPES核心代码优化王为1)张悠慧1)姚骏2)李艳华1)郑纬民1)1)(清华大学计算机科学与技术系北京100084)2)(奈良先端科技大学院大学信息科学学院奈良日本)摘要我国气象局研究开发的数值天气预报系统GRAPES作为典型的高性能计算应用,在人民日常生活中有着极其重要的作用,如何提高GRAPES系统性能并控制其功耗以支持因增加系统分辨率而急剧增加的运算量是一个重大课题.该文使用软硬结合的多种方法对GRAPES系统的核心代码进行优化.采用线性阵列流水处理器LAPP实现循环级并行;采用循环切分、数据预取、缓存分区、多路预取等方法来进行加速;采用电源门控等低功耗技术来降低功耗.实验结果表明,优化后的GRAPES核心模块运行时平均IPC可以达到11.3,是面积相同的通用多核处理器的2.3倍;低功耗技术使其功耗仅为通用多核处理器的12%;同时优化后的LAPP集群性能功耗比可以达到相同计算能力IntelXeon集群的11.7倍.关键词计算机系统结构;数值天气预报;赫姆霍兹方程;Stencil计算1引言随着高性能计算机技术的发展和应用,单纯依靠提升单核的频率来提高性能已经变得十分困难,并行计算已成为保证大规模计算应用业务运行时效的关键技术之一.然而,随着处理器数量的增加系统性能提升逐渐放缓,并可能遭遇性能瓶颈甚至性能下降.其中一个重要原因便是通信量随着核数增加而极具增加.与此同时,随着应用规模的不断增加,系统所消耗的电力能源不断增加,高性能计算所带来的功耗问题越来越严重.从绿色计算的角度看,必须优化系统以降低功耗.如何在有效控制功耗的基础上提高性能成为一个必须解决的难题.超级计算机系统的使用消耗了大量的电力,计算机系统对于环境的负面影响日益显现出来.目前流行的多核、多线程的处理器体系结构更是加剧了功耗问题.E3报告[1]中指出,按照现在的技术发展,一个百亿亿次的系统将会有高达200mW的功耗,这是一个不可想象的数字.功耗问题已经成为一个制约高性能计算系统发展的一个瓶颈.数值天气预报是高性能计算的典型应用之一.天气预报在人民的日常生活中扮演着极其重要的角色.随着高性能计算的快速发展和数值天气预报理论的完善,数值天气预报已成为了最重要的量化天气预测的方法.我国气象局研究开发的数值天气预报系统GRAPES(Global/RegionalAssimilationandPredictionSystem)作为典型的高性能计算应用在国内人民日常生活中有着极其重要的作用.目前所使用的GRAPES系统的水平分辨率为25km.为了提高天气预报的准确度,提高系统分辨率是一个最直接最有效的方法.然而更高的分辨率会导致计算规模极具增加,并且计算时间步需要随着分辨率增加而相应减小以获得最有效的预测.在一定的计算资源下并忽略多核系统通信影响,计算时间和系统分辨率的3次方以及时间步频率与预测时间的一次方成正比.因此,提高GRAPES系统的性能并控制其功耗以支持在线业务的需求是一个重大课题.如何提高GRAPES系统性能同时降低功耗,以支持更高分辨率的天气预报,对提升我国气象预报水平至关重要.目前国内外对于GRAPES研究主要在算法等较为宏观的层面展开,而本文的研究点为以软硬件结合的手段提升系统整体性能.赫姆霍兹方程(Gibbs-HelmholtzEquation)的求解是GRAPES系统动态核心中最耗时的部分.方程求解部分限制了整个系统性能的提升.我们以赫姆霍兹方程求解模块作为研究对象实施优化.因为此模块具有密集的Stencil访存模式,故我们选取了线性阵列处理器(LinearArrayPipelineProcessor,LAPP)对其进行优化.LAPP是一种粗粒度可配置结构(CoarseGrainedReconfigurableArchitecture,CGRA),可以通过配置功能阵列互连来改变执行模式,具有灵活性强,开发周期较短,应用代码改动少,并且不需要特殊编译器的支持等特点.LAPP通过将超长指令字(VLIW)映射到功能阵列(FunctionArray)[2]上来实现循环级并行,可以用来加速图像处理等循环迭代之间无数据依赖的特定类型的算法.使用LAPP对特定种类的算法加速,可以得到极高的加速比,同时,其采用的各种低功耗技术能有效降低功耗[3].实验结果表明,在LAPP上对GRAPES进行的优化,相对于相同面积的多核处理器,性能可以提升到2.3倍,功耗只有其12%.跟Intel集群相比,LAPP集群可以显著减少通信,使得性能功耗比达到前者的11.7倍.本文第2节对LAPP进行介绍;第3节讲述对赫姆霍兹方程求解部分的软硬结合的加速方法;第4节为实验结果与分析;第5节为总结.2线性阵列流水处理器研究已经证明,针对特定应用或算法设计的专用处理器,通常能够获得远高于通用处理器的性能,同时具有很低的功耗.典型的如GreenFlash[4-5]、Anton[6]等系统,在空气动力学、热力学等应用计算中获得了非常理想的效果.应用特定集成电路(ASICs)通过针对某一类应用而定制,根据特定应用的特点做特殊设计,具有非常好的性能和功耗特性.但是具有灵活性差、设计周期长等缺点.相比之下,粗粒度可配置结构可以通过配置功能阵列互连来改变执行模式,开发周期短,灵活性强,并且具有非常理想的性能功耗比.LAPP是一个粗粒度可配置结构的处理器.LAPP的基本结构如图1所示.其前端是一个普通流水线,后端使用一个功能阵列.功能阵列是一个二维结构.每行用来映射同一执行时钟周期的多条指令,纵向用来映射不同的循环迭代.指令映射单元(MAP)[7]负责把VLIW指令映射到功能阵列上.Page3图1LAPP结构L0$为0级缓存,用于向功能阵列中的执行单元提供数据,并通过缓存控制器(LSU)实现灵活可变的数据通路,避免数据的重复载入.LAPP拥有普通模式和阵列模式两种执行模式.阵列模式可以把程序的内层循环映射到其功能阵列上,利用循环迭代之间的并行来提升性能.LAPP的加速的基本思想是把内层循环的每条指令在每个流水段里逐时钟周期地映射到功能阵列上,这样循环不同迭代的指令可以同时存在于功能阵列之中进行循环级并行.LAPP阵列模式下执行的性能可以用式(1)表示,其中N为循环次数,SNEW为Load指令的时间开销,SOUT为Store指令的时间开销,ST为Store指令数,OA为功能单元阵列填充的时间开销.TA=SNEW·N+ST·N+OA·ST+SOUT·NLAPP采用了电源门控(PowerGating)、时钟门限(ClockGating)、动态变压(DynamicVoltageScaling,DVS)等低功耗技术来控制功耗[8-10].在普通模式下,传输寄存器组(PropagationRegisters)、映射单元(MapUnit)和数据0级缓存等处于关闭状态.数据一级缓存在缺失时采用时钟门控关闭,直到数据获取后才开启;在阵列执行模式下.只有有指令映射的执行部件才会开启,没有指令映射的执行单元将用电源门控技术关闭.当指令映射完成时,LAPP进入阵列执行模式,指令的一级缓存不再需要,这时指令一级缓存用动态变压技术进入睡眠模式.取指令和指令译码只在普通模式下使用.在阵列模式下用电源门控技术关闭.MAP单元用来将指令映射到对应的单元之中,只有在阵列建立阶段才会使用,其它阶段用电源门控技术来关闭.LAPP的动态的指令调度器和一个直接调度器模型相比,芯片面积和延迟分别减少到直接模型的43%和70%.同时,LAPP采用分时机制来减少所需要的功能单元,消除物理上的面积限制,使得每块面积的性能提升了16%,同时能耗减少20%[11].实验结果表明,低功耗技术使LAPP在执行GRAPES核心代码时功耗仅有相同芯片面积多核处理器的12%.3核心代码优化3.1GRAPES系统数值天气预报系统已经作为一种最主要的预报Page4天气的手段.GRAPES是我国研制的具有自主知识产权的数值天气预报系统.提升模式分辨率是提高天气预报准确度最直接有效的方法.因此提升预报系统的模式分辨率是研究发展的一个重要方向,而性能和功耗的要求是制约其提高模式分辨率的一个关键因素.如何提高该大规模计算的性能同时降低功耗,以支持更高分辨率的天气预报,对提升我国气象预报水平至关重要.GRAPES中5个主要部分分别为grapes_input、colm_init、med_before_solve_io、med_last_solve_io、solver_grapes.solver_grapes为模式计算的核心,包括了模式的动力框架计算部分和模式物理过程计算部分.其中赫姆霍兹方程求解部分占据了整个计算核心的近30%[12].提高该核心算法的计算效率对提升整个系统的性能起着关键作用.3.2赫姆霍兹方程求解赫姆霍兹方程的求解最后转化成为求解一个大规模的稀疏线性方程组.通过分析天气预报GRAPES核心代码的计算和访存模式,我们发现该核心代码具有循环之间无数据依赖,并且数据访存有一定规律.针对这一规律,我们选择对该核心算法进行软硬结合的循环级并行来加速并降低功耗.硬件方面,我们采用将循环指令映射到功能阵列上进行执行,横向映射循环内的每一条指令,纵向映射不同的循环迭代.软件方面,我们采用循环切分、地址合并、数据预取、缓存分区的方式来加速程序的执行.方程(2)是典型的赫姆霍兹方程,其中,Π是模式的预报变量,εΠ1、εΠ2、εΠ3、εΠH1、εΠH2、εΠH3、εΠH4、εΠH5、εΠH6是不随时间变化的方程系数,为球面坐标的纬度,λ为球面坐标的经度,z为高度地形追随坐标,α为地球半径.(Π)n+1=[εΠ1αcosεΠH2α22εΠH5αcos将方程(2)有限差分离散化以后得到的相关系从程序方面来看,计算部分由一个三重循环和两个二重循环组成.其中第一个循环选取三维矩阵犅内部的点和犃阵的对应点相乘得到矩阵犆的对3.3Stencil计算模式数如图2所示.图2三维赫姆霍兹方程离散格点与相关系数分布应点.两个二重循环对边界情况进行处理.第1个循环占据绝对主要时间,我们称之为循环体L1.循环体L1执行的工作为一个四维矩阵(称为犃阵)和一个三维矩阵(称为犅阵)乘法,得到一个新的三维矩阵(称为犆阵).乘法规则为:犅阵内部每个点以及以该点为中心的周围18个点,分别和犃阵对应点的系数相乘,结果累加,作为犆阵对应点的值.数值计算可以用等式(3)表示.犆i,j,k=∑δ1=1,δ2=1,δ3=1δ1=-1,δ2=-1,δ3=-1其中αx,i+δ1,j+δ2,k+δ3=这种计算模式被称为Stencil计算.在Stencil计算中,网格中的每个格点由其周围格点的子集的加权和值来更新.这种计算模式普遍存在于热传导,流体动力学等各类科学计算之中.从简单的Jacobi迭代到复杂的自适应性mesh网格加密[13]都需要用到Stencil计算模式.因此,对Stencil计算模式的优化有非常典型的意义.相关领域研究者分别对此模式在cache上的优化以及在多核系统中的优化做了相关的研究[14-15],但是这些优化方案都没有使用可定制化的处理器.3.4优化方案从等式(3)中可以看出,在每个内层循环中,计算需要从19个一维矩阵中和一个三维矩阵中分别获取数据,这造成了非常密集的内存访问.并且,这些对内存的访问表现出了非常差的局部性,进一步阻碍了性能的提升.Page5Stencil计算中存在潜在可利用的数据相关性.但是,这些相关的数据分布在相隔较远的区域中.例如,Bi,1,1可能在j=1时载入,当j=2时,该值又会被访问一次,但是因为通用cache的瓶颈,该点的值在再次访问之前就已经被换出,此时需要再次访问内存并放入cache,这就造成了不必要的替换.由于该部分计算具有非常差的局部性,加上其密集的内存访问,造成了此部分计算耗时过多.因此,我们主要从Stencil模式计算上来加速GRAPES的核心代码.为了充分利用Stencil模式计算的特性,并在LAPP上建立优化的数据流,我们在GRAPES上采用了如下所述的软件优化方法:(1)重新组织犃矩阵.原始的4维矩阵犃x,i,j,k导致了最内层循环的随机地址访问.为了去除随机内存访问,我们将犃矩阵重新组织,由犃x,i,j,k变为犃i,x,j,k.在内层i循环时,x,j和k是固定不变的,数据预取可以从i=0到i=N.犃矩阵的重新组织非常显著的增强了最内层循环的局部性.(2)循环体重新调度和划分.LAPP是按矩阵流水线的方向线性增长的.循环体的最大映射数量由LAPP的大小所限制.在本文中,我们采用20段的LAPP.我们将最内层循环拆分成3个子循环来适应LAPP硬件上的要求.同时,我们手动调整了指令的顺序来协调乘法与加法运算的内存访问.连续的地址如Bi-1,j,k,Bi,j,k和Bi+1,j,k被调度到同一个子循环体内来避免跨循环的数据共享.(3)缓存预取指令.我们在GRAPES中加入指导性的预取指令来指导LAPP将数据预取到一级缓存中.所有最内层循环需要用到的数据都被读取到一级缓存中,在之后的循环并行执行.(4)缓存分区和Stencil数据再利用.如图3所示,沿着中间层循环j的方向,例如从j=1到j=2,之前访问的Bi,1,k±1和Bi,2,k±1变成了新一次访问的Bi,j-1,k±1和Bi,j,k±1.我们无需在j=2中载入重复的数据.只有Bi,0,k±1需要被Bi,3,k±1替换掉.我们在预取指令中增加了一个缓存分区信息,来指导LAPP硬件在预取时避免中层和最外层循环中重复的数据预取.(5)硬件上的优化策略包括了对于缓存分区和与取指令的实现.同时,我们添加了缓存的硬件来实现4路并行预取来进一步提高预取速度.经过优化后,最终内层循环的的数据预取次数为犃矩阵18条,犅矩阵3条.犆矩阵的数据存储要求被写回更低一层的缓存.所以,每个内层循环中一级缓存和更低一级的缓存中的数据交换有22条,是优化前的59%.4结果分析4.1模拟器参数我们使用一个时钟精确的模拟器进行模拟.模拟器的主要参数如表1所示.我们使用Synopsis编译器来获得LAPP的各部件门电路数,具体数据如表2所示.功耗参数使用PrimeTimePX来测得,使用180nm工艺,时钟频率频率为100MHz.实验中我们使用的LAPP为20段流水.通用寄存器(GR)数量数据传输率(外部缓存)一级数据缓存(L1)数据传输率(L1→L0)单元逻辑单元数功耗/mWPC10751.58110IF5115053.30110ID2515422.20110RF8727837.00110I1$17683793.40110L1$258419143.28110EAG33132.21111ALU111095.82333MEDIA78444.41444BRC18010.77111MAP2477326.30011SEL3414625.56011L0$241244.04011LSU95571.18111普通流水线679287388.84Stage0762330Stage1…162417Page64.2优化效果分析图4为GRAPES核心代码分别在没有经过优化、经过软件优化以及经过软硬结合优化后的性能.纵坐标为IPC,横坐标为内层循环迭代次数.实验结果表明,经过循环切分,指令调度和软件预取等软优化技术应用后,GRAPES核心代码的性能得到了有效提升,当循环迭代次数为720时,软件优化措施获得了63%的性能提升.在进行硬件优化后,GRAPES核心代码的性能得到了更加明显的性能提升,当迭代次数为720时,硬件优化在软件优化的基础之上又进一步提升了148%,相对于未经优化的程序,总体性能提升了306%.4.3LAPP与多核处理器性能比较LAPP由两部分构成.其前端是一个普通流水线,后端使用一个功能阵列.前端我们可以当作一个传统的处理器来使用.在实验中我们分别用不使用功能阵列和使用功能阵列两种处理器配置对GRAPES核心代码进行测试,以分析我们的优化结果.通过表2我们可以计算出,拥有20段流水的LAPP拥有6446941个逻辑单元,而只是用普通流水线的LAPP拥有679287个逻辑单元.因此可以计算得到,20段流水的LAPP面积是普通处理器面积的9.5倍.因此,保守情况下,我们可以假设多核之间没有通信开销,在实验中用9.5核的多核处理器来进行性能对比,即用单核普通LAPP系统性能乘以9.5,作为多核系统的性能,来与20段流水LAPP进行比较.如图5所示,经过优化后的LAPP能够获得2倍于多核处理器的性能.当最内层循环为720次时,优化后的LAPP其IPC为12.46,相同面积的多核处理器为4.69,优化后的LAPP性能为相同面积多核处理器的2.66倍.4.4功耗分析LAPP采用了电源门控、时钟门限、动态变压等低功耗技术来控制功耗,实验中我们发现,采用这些低功耗技术可以让LAPP在阵列执行模式下非常有效地控制功耗.在实验中,我们将GRAPES核心代码分别运行在不使用功能阵列的传统处理器和使用20段功能阵列的LAPP上,以对采用功能阵列后的功耗进行分析.通过图6我们可以看出,LAPP具有极其出色的功耗控制.如图6所示,LAPP的功耗仅有多核处理器的12%,由此可见GRAPES核心代码运行在经过优化的功能阵列处理器上,可以将功耗降到一个非常低的水平.考虑性能功耗比,LAPP的平均性能功耗比是多核处理器的19.8倍.4.5LAPP集群和IntelXeon集群的性能比较我们用IntelXeonE5540处理器作为参照来评估我们系统的性能.IntelXeonE5540的主要参数以及GRAPES核心代码运行在IntelXeonE5540上的性能如表3所示.当采用8个IntelXeonE5540处理器时,其IPC可以达到4.28,GFLOPS可以达到4.51.Page7表3GRAPES核心代码在IntelXeonE5540上的性能GFLOPS单核GFLOPS1.040.860.56峰值功率GRAPES核心代码在20段流水的LAPP上运行的性能指标如表4所示.我们假设LAPP采用了45nm工艺制作,则其频率可以达到400MHz.测试使用的代码在Stencil计算部分最内层循环数位360,且代码已经应用了循环切分,指令调度以及缓存分区等软优化技术.如表4所示,计算时的IPC达到了38.3.当我们采取4路并行预取时,预取访存时间大大减小,在只有1路预取时,一级缓存访存时间和计算时间比为6.631,而是用4路预取时,访存时间和计算时间比下降到2.111,总的IPC提升到了12.32,GFLOPS提升到了1.64.一级缓存访存时间计算时间6.6312.111我们可以计算得出,单核LAPP的IPC是IntelXeonE5540的2.91倍,单核LAPP的GFLOPS是IntelXeonE5540的5.81倍.保守起见,我们认为一个LAPP处理器的计算能力是一个IntelXeonE5540处理器的2.91倍.在集群计算的比较中,我们以相同IPC的处理器数量为一个组来进行性能比较.例如,87个LAPP的IPC和256个IntelXeonE5540的IPC相当,所以将87个LAPP的集群和256个IntelXeonE5540作为一组进行对比.图7为LAPP集群和Intel集群的性能比较结果.由图7可以看出,在相同的IPC下,LAPP相对于IntelXeonE5540有着更加的表现.两者的计算时间相同,但是LAPP集群的访存时间少于IntelXeonE5540的通信时间,且随着集群核数的增加,LAPP和IntelXeonE5540集群通信时间的对比越发明显,2048核的Intel集群获得了最佳的性能,而351核的LAPP集群已经可以达到同样的性能,并且可以随着核数增加而继续提高性能.LAPP集群的性能功耗比是Intel集群的11.7倍.5结束语本文提出了一种软硬结合的应用加速方法对赫姆霍兹方程的求解进行加速.采用线性阵列流水处理器将指令映射到功能阵列上来进行循环级并行加速;同时软件上采用数据重排、循环切分、地址合并、数据预取,硬件上实行多路预取等方法来对其进行进一步加速.实验结果表明,我们的优化方法取得了非常理想的性能提升,并且非常显著地降低了功耗,同时可以减少通信开销,进一步提升性能.LAPP对于可进行循环级并行的部分具有非常好的加速效果,不可并行加速的部分的执行能力相对较弱.因此我们计划将LAPP作为Intel处理器的协处理器进行研究.通过向应用程序的中间代码中加入伪代码来指导协处理器工作.在Intel主处理器执行到可并行的代码部分时,通过DMA数据通道,把数据和程序块放到SRAM中,由协处理器来加速执行可并行部分,执行完毕再返回主处理器继续执行.这样我们不仅可以优化核心部分的性能,同时整个应用的性能也将得到提升.
