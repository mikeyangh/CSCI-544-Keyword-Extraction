Page1CCN中选择性缓存机制的研究刘外喜1),2)余顺争2)胡晓1)朱萍玉1)1)(广州大学电子信息工程系广州510006)2)(中山大学电子通信工程系广州510006)摘要现有Internet架构存在着众所周知的缺点,未来网络架构的研究成为了热点.其中,CCN(Content-CentricNetworking)在众多新架构中正逐渐被大家认为是最有前途的方案之一,它要求网络中的每一节点都要有缓存功能.所以,研究高效的缓存机制就成为实现CCN预期目标的关键.针对CCN现有缓存机制中存在的“无序缓存”的问题,文中提出了选择性缓存机制SC(SelectiveCaching).它根据用户的潜在需求和内容的流行规律,只在必要的节点上选择性地(而不是沿着内容传输路径处处地)缓存.同时,采用带宽换缓存的思想,利用链路的冗余带宽,将内容分流到相邻节点缓存,进一步提升缓存效率.我们首先提出了缓存空间消耗的理论分析模型,然后在多种实验条件下对SC进行了验证,实验结果表明SC可以提高网络的缓存效率和性能,同时SC的通信、计算、状态记忆等额外开销也都很小.关键词内容中心网络(CCN);选择性缓存;内容放置;带宽换缓存1引言当前,各种新的互联网应用层出不穷,但现有的协议架构却无法适应这一应用需求的发展,存在着一系列的问题:移动性的支持、扩展性、安全性问题等.为了彻底地克服上述问题,设计一种全新的互联网架构逐渐成为了研究者们的共识.缓存是一种改善Web服务的重要技术,它使得内容更加靠近用户,从而可以减少网络带宽的消耗、服务器的负载、用户的访问时延[1].正是考虑到这些优势,在设计全新未来互联网的时候,这一针对Web服务的思想被扩展到全网以及所有应用.CCN(Content-CentricNetworking)[2]正是这一思路的典型代表,它正逐渐被大家认为是众多未来互联网架构中最有前途的方案之一.CCN要求每个节点都能缓存经过的内容,覆盖全网的缓存成为了网络体系结构中固有的一部分,因此网络不仅是一个传输体,也成为了一个存储体.当用户请求某一内容时,任何缓存有该内容的中间节点都可以做出响应.这样一种全网缓存机制使得信息快速地扩散到网络中.对于每一个请求,网络都可以提供多个数据源,从而大幅度提高网络的性能.这实际上将以前只有CDN(ContentDistributionNetworks)和P2P(Peer-to-Peer)等专有网络中才会提供的多方通信服务扩展到全网络.基于这一新型的通信模式,要将覆盖全网的缓存的潜在优势发挥到最大,设计一种高效的缓存机制成为了关键.高效地缓存意味着:在部署相同大小的缓存的情况下,用户能够更容易地从缓存中获取想要的内容,原始内容服务器的负荷得以减少.而在当前已提出来的众多缓存机制中,依然还有“无序缓存”的问题需要解决,主要表现如下:(1)无效缓存.在众多CCN的原始提案中,执行的是处处缓存(CacheEverythingEverywhere,CEE),即所有的内容被要求在去往目的地途中所有节点缓存,但这种处处缓存的机制饱受质疑[3],会导致严重的缓存浪费:因为请求可以被任意中间节点响应后而不会往上游转发,那么其上游某些节点缓存的内容可能根本没有机会响应后来的请求,但由于空间受限而必须被替换掉.我们将这种内容在被替换之前没有机会发挥作用的现象称为“无效缓存”.在如图1所示的网络中,当H和F节点都从服务器C节点请求了内容o后,如果按照处处缓存的机制,那么o会在B、A、D节点被缓存.当后来的E节点再次请求内容o时,会从A节点得到响应,那么此时在B和D节点缓存的内容o就无法发挥作用,成为“无效缓存”.可见,这种现象在处处缓存机制中普遍存在.(2)同质化缓存.对于非协作式缓存,各个节点独立地缓存和替换将会导致内容的分布存在以下不合理:①各节点缓存类似的内容;②在时间上分布不合理,在热门时间里,每个节点都缓存相同的内容,然而热门时间一过,该内容在各节点上又几乎同时地消失了.以上不合理将导致缓存的效率不高,全网缓存的潜能没有完全发挥出来.为了解决“无效缓存”和“同质化缓存”等问题,我们提出了选择性缓存机制SC(SelectiveCaching),本文的主要贡献如下:(1)提出一种轻量级的改善缓存效率的机制:节点根据用户的潜在需求和内容的兴衰规律,选择性地缓存,节点之间隐性地协作,避免“无效缓存”;实现冗余的带宽换取缓存空间;同时也实现了差异化缓存,避免“同质化缓存”.(2)提出了一个分析缓存空间消耗的模型.(3)定义了一系列适合CCN这一新架构的性能测量参数,并通过广泛的实验验证了SC的优越性能.2相关工作自Web缓存提出以来,研究主要包括内容的放置(placement)和替换(replacement)机制等领域.在内容的替换机制方面,实现方法主要是利用内容被请求所具有的时间局域性(TemporalLocality)和空间局域性(SpatialLocality)来尽量提高替换的准确率,即让缓存能够最大可能地保留用户最需要的内容[4],如LRU(LatestRecentlyUsed)、LFU(LeastFrequentlyUsed)等[4]既简单又实用.本文只研究Page3放置机制,下面综述与其相关的研究工作.关于如何更好地将内容放置在合适位置的问题,Tang和Chanson等人[5]针对线性拓扑进行了研究:从client发送请求到服务器端时,将沿途各节点的信息带到服务器端,这些信息包括目标内容的大小、访问频率、因为替换而需要付出的代价等信息.服务器根据这些信息进行计算,获得缓存节点的最佳分布,目标是使全网对所有目标内容的接入代价(accesscost)最小.这个计算结果会由逆向沿途返回的数据报文带到各个节点,各节点据此更新自己的缓存.在此基础上,Li和Shen等人[6]进一步讨论了树型拓扑网络中内容放置的优化问题.而Shen和Xu[7]则考虑在多服务器的网络中内容的放置问题.显而易见的是,因为采用了集中式的优化计算,这类方法能够很好地得到合适的放置地点,但s(o)、f(o)、m(o)的精确度极大地影响着该方法的效果.而实际上,这些信息并不容易获取,要精确就需要额外开销(如文献[5]利用内容o在过去被请求的历史来进行推测);而要节省开销则会以降低性能为代价.同时,该类方法的计算量也不小,被用到的动态规划算法的时间复杂度达到了O(k2),k是沿途存有描述符的节点数量,接近于沿途节点的数量.Psaras等人[8]提出基于概率的缓存机制,Interest沿途节点按照如下原则来确定对内容进行缓存的概率:(1)节点距离服务器越近,概率越低;(2)节点缓存资源越多,概率越大.该方法对缓解“同质化缓存”有一些帮助,但依然无法克服“无效缓存”的问题.Hosseini-Khayat[9]则从合适地选择缓存对象的角度提出了选择性缓存机制,中心思想是:如果缓存一个新到的内容需要替换掉一个对提升系统性能更有“价值”的内容,那么就没有必要缓存.但该方法中的“价值”依赖于对流行度指标T的判断,而文中的T是一个预设的固定值,所以不能成为一种在线的解决方案.Miao等人[10]则针对互联网的视频应用,提出了类似的思想:仅仅是一小部分对提升系统性能有着重要贡献的帧才会被缓存起来.这种缓存机制比较适合于节点缓存容量相比于每个被缓存的对象比较小的情况,即当节点的缓存容量只够缓存热门视频内容时,该机制是有效的.而当节点的缓存容量相比于被缓存的对象很大时,根据缓存的时间局域性,要被替换掉的内容,往往是长时间没有被击中的不热门的内容,因而是对系统性能影响较小的内容.与上述不同的是,Lauinger等人[11]利用选择性缓存机制来实现ICN中的隐私保护.刘外喜等人[12]提出了把内容的放置、发现、替换统一起来考虑的APDR机制,实现内容的有序缓存.APDR的主要思想是:Interest报文除了携带对内容的请求,还收集沿途各节点对该内容的潜在需求、空闲缓存等信息,使得Interest的汇聚点和目的地节点,可以据此计算出一个缓存方案,并把该方案附加在Data报文之上,通知返程途中的某些节点缓存该内容并设置指定的缓存时间.Cho等人[13]提出WAVE机制:上游节点利用CMW(ChunkMarkingWindow,块标记窗口)给下游节点提供缓存建议,CMW会随着内容的流行程度而成指数增长,从而使得越流行的内容分布越广,主要是实现一个文件多个分块在网络上的合理分布.该方法中在上下游节点之间传递的缓存建议将是一个不小的开销,并且随着文件被划分块的数目的增加而增加.Fiore等人[14]提出了“Hamlet”算法:利用无线信道的广播特性,节点n偷听到某一个节点m在请求内容r,那么n就认为m自此以后会缓存该内容r;或者节点n偷听到某一个节点p回应了内容r的请求,那么n就认为p拥有内容r.知道了周围节点缓存什么内容后,自己在做替换决定的时候就要求自己和周围节点的缓存内容区别开来[15].该文虽然实现了差异化缓存,但可以看出,偷听信息是这种方法的实现基础,所以不可靠,具有随机的波动性.Rosensweig等人[16]提出Breadcrumbs算法:每个节点记住目标内容被转发与缓存的历史,方法是记住该目标内容的五元组(5-tupleentry).五元组包括:目标内容的上游节点号、目标内容的下游节点号、目标内容经过该节点的最近的时间、目标内容在该节点被请求的最近的时间等.利用这些信息就可以分析出目标内容在网络中的缓存情况,将内容的放置、内容的路由等集成在一起.但Breadcrumbs有可能会出现循环路由,并且记住每个内容历史的开销也不小.Carofiglio和Muscariello等人[17]也针对带宽与缓存联合优化问题进行了研究,假设用户请求到达过程遵循马尔可夫过程,并已知网络拓扑和内容流行度分布等先验知识,在网络资源(缓存容量、带宽)总量一定的前提下,如何合理地在节点之间分配他们,使得用户获取内容的时间最短.但该文仅仅可看作是一个静态的网络的规划,是一个资源分配的优化问题,不能用作实时动态的缓存机制.Page4叶润生等人[18]将缓存的信息引入到路由决策中,提出了邻居缓存路由机制,在减少网络冗余流量的同时提高了整体网络的性能,但该方法的主动探测机制需要付出一些额外的通信开销.针对分布式缓存系统中如何优化内容的放置位置,使访问开销最小化,李文中等人[19]提出一种图算法来解决该问题,但该方法假设已知各节点上用户的需求以及替换开销,只能看作是一个静态的规划方法.李春洪等人[20]针对多媒体内容分发服务中节点负载不平衡的问题,设计了一种无热点的覆盖网协同缓存机制———HFOCC(HotspotsFreeOverlayCooperativeCaching).通过将“热点”对象复制到低负载节点,分散服务请求,达到消除热点的目的.该文采用一种“软”副本生命期控制机制,当工作负载发生变化时,冗余副本被及时删除.该文从平衡负载的角度考虑缓存的优化放置问题,实际上是一种基于推送的机制,牺牲少量的命中率换取吞吐率和资源利用率.节点需要记录每个内容在网络中所有的位置以便于查询,在内容比较多时,查询速度以及建立该记录所需要的通信开销,都是需要考虑的问题.Liu等人[21]为CCN中缓存效率的分析建立理论模型,并证明利用网络编码可以改善CCN的缓存效率.与上述工作不同的是,本文采用的是按需缓存的思想:只缓存在那些有潜在需求的节点上.同时,我们在缓存机制中用带宽换取缓存,进一步提高缓存效率.SC机制也考虑到了内容兴衰的规律,使缓存该内容的节点在空间上的分布更加符合需求.而且,SC非常简单,不需要复杂的计算,因而对节点计算资源的开销很小.3CCN的基本机制在本节,我们将综述CCN的一些重要内容,有关CCN更加详细的内容请参考文献[2].CCN架构有两个特点:通信是由接收者(也就是内容的需求者)发起;缓存是全网所有节点的必备功能.CCN有两类报文:兴趣(Interest)和数据(Data)报文.为了获得一个内容,需求者发出一个带有内容名字的Interest,Interest不断地在网络中被转发,直到有中间节点的缓存或内容服务器对其响应,然后,数据报文会沿着Interest报文走过的路径逆向回到数据需求者,在本文中,我们称这一路径为Interest路径.由于CCN中的节点并没有地址标识,所以整个过程的路由都是基于内容名字的.同时为了加强网络的安全,在整个转发过程中,每个数据报文总是和内容产生者的签名信息绑定在一起的,这样中间节点和内容需求者都可以对其进行验证[3].在CCN中,为了实现内容的路由和转发,每个路由器会有3个功能模块:FIB(ForwardingInfor-mationBase)、PIT(PendingInterestTable)和CS(ContentStore)[2].其中,FIB和TCP/IP架构中路由器的转发表的功能基本类似,但它允许匹配多个端口进行转发.4SC的运行机制在我们的系统模型中,网络是由原始内容服务器(OriginalContentServers,OCS)和一些配置了缓存的路由器节点以及一些用户组成.其中只有OCS会产生内容,并且会保存该内容直到其过期.我们将整个网络记为G(V,E),其中所有节点的集合记为V,所有链路的集合记为E.为了简单起见,假定在内容传输的路径上,节点i-1在节点i的下游(即更靠近请求者那一边).本文不考虑泛洪路由的情况,即节点在收到Interest时,只会根据路由协议选择一个下一跳,而不会向所有的端口广播.4.1SC的基本原理SC的基本思想是基于以下考虑:对于内容o,节点i上的某个端口如果曾经收到过对该内容的请求,并且已经把内容o从这个端口转发出去,因为下游节点缓存的存在,那么在未来一段时间内不会再次从这个端口收到对该内容的请求.所以,当节点i的所有端口都请求过内容o并被满足以后,节点i则没有必要缓存内容o.因此,节点i上没有请求过内容o的端口数量越少,那么未来一段时间内节点i收到对内容o的请求的概率就会越低,则节点i需要缓存内容o的概率就可以越低,节点i上因此而腾出的空间就可以用于缓存其它内容.节点之间的这种差异化缓存可以提升缓存的效率,可以尽可能地将内容缓存于靠近用户的边缘节点,实现隐性协作缓存.研究表明,任何内容在一定的空间区域内,其受欢迎程度在时间维度上都会经历从上升、流行高峰到最后衰减的过程.通过聚类分析,可以找到这个过Page5程的规律[22],甚至可预测内容未来一段时间的流行度[23].我们称流行高峰及其之前的时期为传播早期,其它为传播晚期.显然,在内容o传播的不同时期,用户的需求不一样:在早期,需求会多一些,并且呈上升趋势;在晚期,需求会少一些,并且呈下降趋势.所以,缓存内容o的节点数量在不同的时期应该有所区别,我们总体的原则是:早期多于晚期.那么相应地,节点i缓存内容o的概率,也是早期高于晚期.4.1.1传播早期时的SC在传播早期,为了进一步提高系统的整体缓存效率,SC中还嵌入带宽换缓存的思想:当两个节点之间的带宽宽裕时,可以将内容有意地缓存在远一点的、缓存容量大一些的节点上.具体策略如下:(1)在内容传输的Interest路径上,节点i与其下游i-1之间的链路利用率越小,节点i缓存该内容的概率越小,即下游节点i-1缓存该内容的概率越高.(2)在内容传输的Interest路径上,节点i的缓存容量越大,节点i缓存该内容的概率越大.为了实现上述策略,把式(1)定义的pi(o)作为节点i缓存内容o的概率:其中,ci表示节点i的缓存容量大小;ui是在内容传输的Interest路径上节点i与其下游节点i-1之间的链路利用率,0ui<1.在带宽换缓存的过程中,需要上下游节点的配合:我们用ai(o)表示节点i+1对节点i的缓存请求,即ai(o)=0表示节点i+1缓存内容o,它对节点i没有要求,节点i可以自主决定是否缓存内容o;ai(o)=1表示节点i+1不缓存内容o而要求节点i必须缓存内容o.这样做的目的是,当节点i+1不缓存内容o时,它可以利用冗余的链路带宽,随时从相邻节点i那里获取内容o,以满足用户对内容o的请求.在上述协作策略中,每个节点只会向其相邻的下游节点寻求协作,而不是向上游节点寻求协作,这样做的原因是,上游节点的缓存负载更大一些,应该减轻其负载;而下游节点更靠近用户端,其缓存更有利于改善对用户请求的响应时间.4.1.2传播晚期时的SC当内容o在节点i处于传播晚期时,我们用qi(o,t)(qi(o,t)<1)表示内容o在节点i被缓存的概率,定义如下:其中,ni表示节点i的端口总数;mi(o)表示节点i在收到内容o之前收到过对内容o的请求的端口数量.θ(t)(θ(t)<1)是调节因子,根据网络中内容需求的状况调节qi(o,t)的大小.在传播晚期,时间越往后,需求越少,所以θ(t)应该随时间衰减.根据文献[22]中的研究结果,内容在时间上的兴衰过程可以被聚类为6类,对于常见的一类内容的衰减模型可以用式(3)来表示,τ=0.2,θ0=0.8,C=0.为了实现上述关于内容在传播早、晚期以不同的、合理的概率被缓存的思想,用ri(o)记录内容o在节点i被请求的历史,用以推断该内容处于传播早期还是晚期.ri(o)=0是初始状态,表示节点i从其任意一个接口最多只收到过一次对内容o的请求.在节点i向收到过内容o请求的接口转发内容o之后,由于下游节点的缓存作用,在未来一段时间内不会再从该接口收到对该内容的新请求.所以,ri(o)=0表示内容处于传播的早期.ri(o)=1表示节点i从曾经转发过内容o的任意一个接口再次收到对内容o的请求.这说明内容o在这个接口下游已经很长时间没有被请求,从而导致其在下游节点的缓存中已经没有了.所以,ri(o)=1表示内容o处于传播的晚期.所以,我们把节点i从曾经转发过内容o的任意一个接口再次收到对内容o的请求的时刻,作为θ(t)衰减的起点时间(t=0).关于传播早期和晚期模型,我们将会在6.2节更详细地讨论.4.2SC的主要算法算法1给出了节点i实现选择性缓存机制的伪代码.第1行,初始化,获取上游节点带来的参数,如ai(o);计算mi(o).ai(o)是节点i是否需要帮助其上游节点的标记.我们在初始化时,所有节点的a(o)都置为零.即每个节点在默认情况下是不需要帮助上游节点的,可以独立地根据算法确定缓存的概率.第2~17行,ai(o)=0表示节点i要根据算法1进行选择性地缓存.第3~11行,在内容传播的早期,为满足用户的潜在需求,内容需要被缓存在多个节点,因此以较高的概率在节点i或相邻节点中缓存.Page6算法1.选择性缓存算法.1.初始化;2.IF(ai(o)=0)3.IF(ri(o)=0)4.Case1:ni-1>mi(o),5.Case2:ni-1=mi(o),6.IF(ni=2)7.8.ELSE9.10.EndIF11.Case3:ni-1<mi(o),节点i不做缓存处理.12.ElseIF(ri(o)=1)13.Case1:ni-1>mi(o),14.节点i以概率qi(o,t)缓存内容o;15.Case2:ni-1=mi(o),节点i不缓存内容o;16.Case3:ni-1<mi(o),节点i不做缓存处理.17.EndIF18.ElseIF(ai(o)=1),节点i必须缓存内容o;19.EndIF第4行,ni-1>mi(o)说明节点i还有一部分端口没有请求过内容o,其下游用户对内容o有潜在的需求.所以,内容o需要在节点i和i-1中被100%地缓存,要么在节点i(以概率pi(o)),要么在节点i-1(以概率1-pi(o)).同时,这里也体现了带宽换缓存的思想:当节点i缓存较小,而同时ui也较小的时候,那么在节点i缓存的概率较低.也就是说,把本来应该在节点i缓存的内容放到节点i-1缓存,即故意让Interest跑远一点的路途.因为两个节点之间的带宽充裕,所以这样做不会影响用户的接入代价,但却可以充分地利用节点i-1的缓存空间,用带宽换取缓存的策略得以实现.条件ni-1>mi(o)中减1的原因是:由于本文采用非泛洪路由,有一个端口是去往上游的,不计入到Interest来源端口.文中其它地方减1的原因都类似,限于空间,不再赘述.第5~10行,ni-1=mi(o)说明在未来一段时间内,下游节点向节点i请求内容o的概率很低.所以不需要缓存该内容,以腾出空间缓存其它内容(第9行).但是,存在ni=2的特殊情况(第6~7行),此时,如果按照上述原则,这种节点将永远不会缓存任何内容,缓存空间白白地浪费了.所以此时可以按概率v(v<1)缓存内容o,本文中v=0.5.第11行,实际上等效于:ni=mi(o),即所有端口都有请求,说明节点i已经缓存了内容o;或者是内容o的服务器,所以不做任何处理.第12~17行,内容o在节点i不是第一次被请求,处于内容传播的晚期,内容不需要被缓存在太多的节点.所以以较低的概率在节点i缓存,也不需要节点i-1的协作.第13~14行,ni-1>mi(o)说明节点i还有一部分端口没有请求过内容o.但此时,处于内容传播的晚期,很多节点会满足这个条件,因此以较低概率qi(o,t)(qi(o,t)<1)在节点i中缓存.第15行,节点i不缓存内容o,原因同第9行.第16行,节点i不做任何处理,原因同第11行.第18行,ai(o)=1表示上游节点没有缓存内容o,所以节点i必须缓存内容o.此时,节点i-1的缓存概率与节点i没有关系,它只会向节点i-2寻求协作.4.3SC的性能分析SC的一个主要目标是:只在必要的地方选择性地缓存,以达到节约缓存的目的,从而提高缓存效率.本节对SC的这一性能表现进行理论分析,为简化分析,这里暂不考虑带宽换取缓存.实[24],节点的度数为n的概率为其中n2,β是幂律分布参数,一般认为2β3,c=1∑n2假设度数等于2的节点是网络边界节点,用于用户接入网络,并设任意一个边界节点收到用户对某个内容的至少一个Interest的概率是r2.根据互联网络中节点度数是幂律分布的事如果网络中共有N个节点,则其中度数等于2的节点数量是Nc2-β,网络中度数等于n的节点数量是Ncn-β.所以,每个度数为n的节点平均能够收到来自vn=(2/n)-β个度数等于2的节点的请求.当这些度数等于2的节点中至少有一个用户发出Interest时,该度数为n的节点会收到至少一个Interest.因此,度数为n的节点收到对某个内容至少一个Interest的概率是所以在该节点的n-1个接口中(其中一个接口假定与上游的更高度数的节点连接),平均每个接口收到至少一个Interest的概率是wn=1-(1-r2)vn/(n-1).因此,它的m个接口都收到对该Page7内容的Interest的概率是其中,1mn-1.当该节点收到所请求的内容后,以一定的概率缓存该内容,根据算法1,该内容在不同的传播时期被该节点缓存的概率不一样.对于该节点来说,其度数为n,并且已经有m个接口请求过该内容.我们假设内容o处于传播早期的概率为z1,在此传播阶段该节点缓存内容o的概率是pn,m;内容o处于传播晚期的概率为z2,在此传播阶段该节点缓存内容o的概率是qn,m,其中z1+z2=1.因此,内容在该节点不被缓存的概率PS可以用式(7)表示.这说明,相比于让内容在每个节点100%地缓存的CEE机制,SC机制减少了每个内容在网络中被复制的平均份数.所以,在给定网络缓存容量的情况下,SC相比于CEE提高了网络缓存的能力.PS=∑n2其中,根据算法1,我们可以得到pn,m和qn,m在不同情况下的取值如表1所示,pn,m=1表示的含义是:在相邻两个节点中肯定有一个会缓存.其中,调节因子θ(t)的期望是θ=θ(τ)=θ0τ2=0.032.n与m的关系传播早期n-1>mpn,m=1qn,m=1-mn-1=mpn,m=v,n=2,m=1n-1<mpn,m=0图2给出了幂律分布参数β和边界用户请求概率r2对SC提高网络缓存的能力的影响,其中纵轴是式(7)的PS.不失一般性,我们这里的z1=z2=0.5;max{n}=100;r2变动范围为1%r210%;β遵循实际网络中的分布:2β3.图2明显地显示,SC提高网络缓存的能力会随着β的减小而增加,也随着r2的增加而增加,总体保持在3.64%~19.56%区间.也就是说,在用户请求越多的繁忙网络中,SC提高网络缓存的能力更强.我们也看到,r2对PS的影响更大一些,当r2从1%增加到10%,PS的平均值从4.61%快速地增加到17.72%.5仿真实验本文研究的是内容放置机制,而LRU是一个被普遍使用的内容替换机制,SC的放置机制结合LRU的替换机制简称为SC+LRU.我们从相关工作中选择代表性的放置机制作为SC性能比较的对象:一个是传统的处处缓存的放置机制与LRU的组合,称为CEE+LRU.另外一个是最近提出来的一个选择性缓存机制———基于介数的放置机制[25],它与LRU的组合,称为EgoBetw+LRU.在Chai等人[25]提出的EgoBetw+LRU中:只将内容缓存于Interest路径中节点介数(Betweenness)最大的节点.即,使内容放置于更加重要的节点上来实现更精准的选择,从而提高缓存效率.但其潜在的问题是:介数最大的节点的缓存被挤满了,并且由于空间有限必然会导致需要频繁地替换对象,而其它的节点的缓存却没有被充分利用.这样做会将流量引向介数大的节点,从而形成拥塞点,进而降低网络的容量[26].并且,为了构建Egonetwork[27],节点之间需要互相通信,这样势必会带来通信开销.同时,EgoBetw+LRU中每个节点为了计算自己的介数而需要付出巨大的计算量,并且在网络拓扑变化后,又需要为此而重新计算和通信.即使介数计算的复杂度降低到O(d2m)[28],但要将该方法应用于实际网络,这些问题依然是一个不小的挑战,这里的dm是网络中节点度的最大值.本文采用多个性能参数将SC+LRU与CEE+LRU、EgoBetw+LRU进行对比,其中包括相对击中度(RelativeHitDegree)、平均接入代价(AverageAccessCost)、无效缓存度(InvalidCachingDegree)、相对内容差异度(RelativeContentDiversityDegree)、缓存消耗率(PercentofConsumedCache)、节点负载的基尼系数等.另外,我们也研究了各个参数对网络性能的影响,如相对缓存大小(RelativeCacheSize)、Zipf参数(Zipfparameter(α))、内容服务器数量等.5.1性能参数CCN根本性地改变了传统的网络架构以及通Page8信模式,所以需要定义一些能够更加准确地反映CCN性能的测量参数,本文对此进行了尝试.其中约定:Total_num是网络中所有节点总的缓存数量;Consumed_num是网络中被消耗的平均缓存数量;Request_num是用户请求的总次数,这些参数的值表示的是我们在实验周期内统计的结果,如没有特别说明,后面的参数也都是据此约定.(1)相对击中度RHD(RelativeHitDegree)缓存击中率是一个传统的测量缓存性能的参数.它被定义为:由缓存而不是OCS响应用户请求的概率.但这不能准确地反映缓存的效率,因为该定义中没有考虑到代价,即实现一个击中率需要消耗多少缓存空间.所以我们定义相对击中度如下:其中,Hit_num是被缓存响应的次数;显然,相对击中度越高,缓存的效率就越高.(2)平均接入代价AAC(AverageAccessCost)AAC被定义为:Interest找到所需内容的平均时间.它实际上可看作是测量QoE(QualityofExperience)的参数.(3)无效缓存度ICD(InvalidCachingDegree)为了准确地反映前文所述的“无效缓存”的程度,我们定义无效缓存度如下:其中,Invalid_num表示“无效缓存”的总次数,即在实验期间,对于网络的所有节点,内容在被替换之前没有发挥任何作用的次数的总和.ICD实际上反映的是由于每一次用户请求而引起的“无效缓存”的次数,显然,ICD越低,缓存效率越高.(4)相对内容差异度RCDD(RelativeContentDiversityDegree)实现差异化缓存从而提高全网的整体性能是SC的目标之一.而在CEE+LRU中,由于各节点的缓存和替换决定是独立做出的,所以“同质化缓存”非常严重.同时,各种机制对缓存空间的消耗程度也不同,为了定量地比较这种差异程度,我们定义相对内容差异度如下:其中,Content_num表示OCS产生的所有的内容种类的数量;Type_num表示缓存中内容种类的平均数量.RCDD的定义实际上反映了实现差异化缓存的缓存效率,即单位缓存消耗率实现的内容差异率.显然,它越大,缓存效率也越高.(5)缓存消耗率PCC(PercentofConsumedCache)不同的缓存机制消耗的缓存数量是不一样的,我们定义PCC如下:(6)节点负载的Gini系数(Load_G)设节点i的负载为γi,其被定义为节点i在一段时间内响应用户的Interest的次数.为了衡量各节点负载的均衡情况,我们引入Gini系数(Ginicoefficient).该系数是一个用于衡量概率分布不均匀程度的测度,Gini系数越大,说明越不均衡,其被定义为[29]其中,γ-是各节点的平均负载,N是节点的数量.5.2实验参数设置我们的实验网络的拓扑模型来自于Tierpro-gram[30],由100个节点和386条链路组成.它由广域网WAN和城域网MAN组成,广域网是骨干网,城域网完成接入的任务.内容的请求过程服从泊松过程,亦即,请求的间隔时间是指数分布,平均是1s~10s.同时,我们假设用户对内容的访问模式遵循Zipf分布[31].就是说,如果用Pr{Ck}表示第k级受欢迎程度的内容被请求到的概率,那么它遵循以下规律:Pr{Ck}∝k-α,α称为是Zipf参数(Zipfparameter(α)).表2列出了本文主要的实验参数以及默认值.WAN节点数/MAN节点数4:96用户的数量内容的数量内容的大小节点缓存的大小服务器的数量访问模式5.3实验结果本节给出实验结果,如没有特别说明,所有结果Page9都是我们经过10次实验后得到的平均值.同时,为了观察网络性能受某个参数的影响,我们一次只让一个参数变化,其它参数保持不变,其取值如表2默认值所示.5.3.1相对缓存大小(RelativeCacheSize)的影响一般情况下,互联网中的缓存大小显然会远远小于内容的大小.本文使用相对缓存大小的概念,即全体缓存大小占全体内容大小的百分比,它反映了缓存空间的稀缺程度.在本节,我们研究相对缓存大小对系统性能的影响,通过它可以更加准确地观察到缓存大小如何影响网络性能的情况.图3给出了系统性能在不同缓存大小下的表现,其中相对缓存大小从0.4%大范围地变化到100%.从图3(a)可以看到,正如所预料的那样,3种机制的RHD都会随着相对缓存的增加而增加.但在这过程中,SC+LRU和EgoBetw+LRU的性能一直比CEE+LRU好,这正是因为SC+LRU和EgoBetw+LRU中选择性的缓存机制节约了缓存空间而带来的好处,这与前文的理论分析结果是一致的.而SC+LRU和EgoBetw+LRU的性能接近.图3(b)显示,3种机制的平均接入代价会随着相对缓存的增加而减小.这也很容易解释,因为缓存命中率提高了,接入代价自然也就下降了.SC+LRU和EgoBetw+LRU的平均接入代价在相对缓存比较小的时候比CEE+LRU要低.但是也注意到的是,在相对缓存大于36%的时候,CEE+LRU的表现超越了SC+LRU和EgoBetw+LRU.上述观察说明选择性缓存机制在相对缓存较小情况下的优势更加明显一些,而在缓存空间不稀缺的情况下,处处缓存机制可以将内容大量地缓存于网络中的各个节点,就近地满足用户的需求.而SC+LRU和EgoBetw+LRU的性能接近.从图3(c)可以看到,3种机制的ICD会随着相对缓存的增加而减小.这是因为缓存增加了,缓存就不会那么稀缺,那么内容在被替换之前可以保留更长的时间,那么“无效缓存”的现象就会减少.同时,我们也注意到,SC+LRU和EgoBetw+LRU的表现一直比CEE+LRU好,并且不管在什么情况下,SC+LRU始终保持着大约50%的优势.而由于EgoBetw+LRU能够更加精准地选择缓存位置,所以其ICD比SC+LRU更低一些.图3(d)显示,3种机制的RCDD会随着相对缓存的增大而增大,这是因为缓存大了,缓存内容的种类会增多.而在这一过程中,SC+LRU总是比CEE+LRU和EgoBetw+LRU要大.5.3.2Zipf参数(α)的影响现在,大家已经普遍认为,用户对内容的偏好是服从Zipf分布的[31].不同网络应用的分布参数也不一样,Zipf参数(α)越大,说明用户的偏好越集中.Page10在本节,我们研究用户的偏好模式对缓存机制的性能的影响,更准确地说,我们想看看缓存机制面向不同网络应用的表现.图4是实验结果,其中Zipf参数(α)从0.1变化到1.图4(a)明显地显示,3种机制的RHD会随着Zipf参数的增加而增加,但SC+LRU和EgoBetw+LRU一直优于CEE+LRU.并且随着Zipf参数的增加,SC+LRU机制相对于的CEE+LRU优势在增加,并且在α0.86的时候也超越了EgoBetw+LRU.从图4(b)可以看到,3种机制的平均接入代价都会随着Zipf参数的增加而降低,但SC+LRU相对于CEE+LRU一直保持着优势,EgoBetw+LRU在α0.4后在三者之中是最好的.图4(c)显示,随着Zipf参数的增加,SC+LRU和CEE+LRU的ICD会减少,但SC+LRU一直优于CEE+LRU,而EgoBetw+LRU在此过程表现最好并保持着相对的平稳.而在图4(d)中,随着Zipf参数的增加,3种机制的RCDD会缓慢地减少.这主要是因为随着Zipf参数的增加,用户更加偏好于更少的一部分内容.由于LRU替换机制的存在,网络缓存会实时地根据这种变化调整缓存的内容,所以缓存中内容的种类数量在这一过程中会降低.但也可以明显地看到SC+LRU一直高于CEE+LRU和EgoBetw+LRU.5.3.3内容服务器数量的影响我们最后一组实验是想检验SC机制的可扩展性,以及部署在有不同数量内容服务器的应用场景下的性能表现.从图5(a)可以看出,3种机制的RHD随着OCS数量的增加而在减少.这主要是因为随着OCS数量的增加,内容的地点变得更加分散,Interest在早期经过的路径更多了,内容会被扩散到更多的节点上.所以我们的实验结果表明,传统意义上的击中率Hit_num加的.只是由于它的增加速度比缓存空间消耗的速度要慢,所以从式(8)可以看出,RHD的减少也是正常的.也就是说在这一过程中,击中率在提高,但缓存的效率在降低.但不管在哪种情况下,SC+LRU一直高于CEE+LRU,EgoBetw+LRU则更高.从图5(b)可以看到,3种机制的平均接入代价都会随着OCS数量的增加而提高,这是伴随RHD降低的必然结果,但SC+LRU一直保持着优势,EgoBetw+LRU则更少.图5(c)显示,随着OCS数量的增加,3种机制的ICD会缓慢地增加,但SC+LRU一直少于CEE+LRU,EgoBetw+LRU则最少.而从图5(d)可知,随着OCS数量的增加,3种机制的RCDD的波动不大,与OCS数量变化的联系也不明显,但SC+LRU最高.Page115.3.4缓存的消耗本节分析在3种机制下节点缓存的消耗情况.图6给出了3种机制在各种实验场景情况下的PCC,它们的变化趋势基本一致,其中横轴是场景序号,包括5.3.1节~5.3.3节中展现的各种场景.因为CEE+LRU是处处缓存,所以其PCC显而易见地是最高的,而SC+LRU和EgoBetw+LRU比较接近.SC+LRU相对于CEE+LRU节省的空间平均大约是18%左右,这与前文4.3节的理论分析是一致的.5.3.5节点的负载本节分析网络中各节点在3种机制下承担负载的均衡情况,图7给出了3种机制在各种实验场景情况下的Load_G,其中横轴是场景序号,包括5.3.1节~5.3.3节中展现的各种场景.选择性缓存机制由于将内容集中缓存到有限的节点上,所以网络的流量显然会比处处缓存机制更加集中,图7中呈现的结果也证明了这一点:在绝大部分情况下,CEE+LRU是最低的,EgoBetw+LRU最高,SC+LRU适中.因此,实验结果也就证明了EgoBetw+LRU的缺点之一:会将流量引向介数大的节点,从而形成一些负载过重的拥塞点,进而降低网络的容量.6讨论6.1SC的开销现,而为此付出的代价也很小:前文的实验结果证明SC具有不错的性能表(1)通信开销在SC机制中,上下游节点之间需要传送的仅仅是一个比特的ai(o);而在SC算法中需要用到的ni,mi(o)则可以从FIB和PIT中获取,不需要额外的开销.而在EgoBetw中,为了计算介数,节点之间的通信量是O(|V|)量级的[28],这里的|V|是网络的节点数量.Page12(2)计算开销SC的复杂度是O(1),与网络规模无关.而EgoBetw中介数计算的复杂度是O(d2m),这里的dm是网络中节点度的最大值[28],其计算量是与网络规模相关的.在2.8GHz的CPU(IntelPentium4)和2GB内存的PC机上测量,表3给出了不同网络规模中的单个节点在两种机制下的计算开销.表3的结果也证实了上述分析,SC的计算开销远远小于EgoBetw,并且是与网络规模无关的.网络节点的数量(3)状态记忆开销在SC中,节点i需要记住内容o的状态ri(o),但是只需要1bit.虽然互联网的内容规模达到亿级别,但鉴于内容流行的规律[22],很多内容都是在一段时间内流行,过了这段时间后,我们就没有必要记住它们的状态.所以,我们可以采取定时清除的机制减少这一开销.采用定时清除机制带来的潜在后果是:有些内容本来已经不是第1次请求,但被当作是第1次请求.亦即,节点i缓存这些内容的概率会被错误地提高,为此会浪费一些缓存空间.但上文的实验结果证明,即便如此,SC还是能够较好地改善缓存的效率和性能.我们通过调节定时的大小,来调节状态被记住的内容的数量.根据内容流行规律遵循Zipf分布的事实,在本文中,我们将这一数量控制在当前内容总的数量的5%左右,并且我们这里也是采用LRU来实现替换的.6.2内容处于传播的早、晚期的判断如前文所述,在内容传播的不同时期,SC采取了不同的策略.所以,如果能够准确地判断内容传播所处的时期,SC将会使缓存节点的数量及分布更好地与实际需求相吻合,从而更好地发挥缓存的潜能.从CCN的通信模式可知,对于每一次响应Interest的过程来讲,内容o的传播路径都是一个以响应节点为根部的树.按照SC机制,这个树中至少会有一个节点缓存内容o,可能在中间分叉节点也可能在叶子节点,并且经过多次SC这种机制的选择后,内容o会逐渐地沉向下游的叶子节点.这些缓存内容o的节点实际上起到了把关的作用,是在为下游一群潜在的用户服务,我们称它们为关隘节点.在传播的早期,那些潜在的用户连续不断地发出请求的概率很高,因为LRU的存在,那么内容o在关隘节点被替换掉的概率会很小.那么这些潜在用户的请求会从关隘节点得到满足,而不会往上游转发.如果当某个时候,用户的请求不能从关隘节点得到满足而被转发到了上游节点的时候.亦即,Interest第二次出现的时候,那可能是因为关隘节点很久没有收到内容o的请求了,根据LRU的替换机制,内容o在关隘节点被替换掉了.这样,我们就有理由相信,此时内容已经过了传播的早期,正处于晚期.所以,我们可以将Interest在同一个接口第二次出现看作内容处于传播晚期的标志.而在实际情况中,由于关隘节点缓存容量的限制,一些传播早期的内容也可能在缓存中被替换掉,从而出现以下情况:在传播早期,Interest在同一个接口也会出现第二次请求.这种情况会带来以下后果:内容本来处于传播早期但被当作是晚期,从而降低了该内容被缓存的概率.但其影响往往局限于局部,因为只有在网络的整体缓存容量都严重不足的情况下这种影响才会体现出来.从另一个角度来看,降低内容被缓存的概率并不一定都是坏事,它可能有利于改善网络整体缓存容量不足带来的性能下降.上文实验结果也证明:即便存在这种情况,SC还是能够较好地改善缓存的效率和性能.6.3θ(狋)参数的选择因为不同内容的衰减规律存在差异,为了做出准确的缓存决定,θ(t)的衰减规律需要和内容流行度的衰减规律保持一致,而式(3)中θ0,C,τ等参数决定了θ(t)衰减的过程.本文从文献[22]中选取了一个常见内容兴衰曲线作为参考模型.为了更加准确地选择θ(t)参数,使其与内容的实际衰减过程保持完全一致,我们可从以下两方面着手:一方面,通过聚类技术获取不同类型内容的衰减曲线,例如借助于CCN命名机制中包含的内容类型信息进行聚类,然后为不同类型的内容选择合适的θ(t)参数.另一方面,我们可以利用文献[23]中的研究成果,预测内容的衰减过程.但加入预测过程将增加系统计算负担,所以如何在性能和计算负担之间取得平衡,是需要进一步研究的内容.7结论为了使CCN的全网缓存尽可能地发挥出潜在的优势,本文提出了一种轻量级的隐性协作缓存机Page13制SC:根据用户的潜在需求选择缓存地点,兼顾带宽换缓存的思想,克服了“无序缓存”的问题.实验结果显示:在各种实验条件下,从相对击中度、平均接入代价、无效缓存度、相对内容差异度、缓存消耗率等指标来看,SC+LRU相对于CEE+LRU的优势很明显,而付出的代价仅仅是节点负载的基尼系数的少量提高,以及很少的计算与通信开销.而SC+LRU和EgoBetw+LRU则互有优劣,SC+LRU虽然在某些指标上逊于EgoBetw+LRU,但SC+LRU的负载更加均衡,算法的通信开销、计算开销也远远小于EgoBetw+LRU.同时,SC在各种实验条件下都有良好的性能,展现了良好的扩展性.所以,经过综合比较,从实用的角度来看,SC相比于EgoBetw具有优势.展望未来,还有很多地方可以研究,例如,可以将SC拓展到移动网络环境中,此时可以研究将推送机制嵌入到SC.
