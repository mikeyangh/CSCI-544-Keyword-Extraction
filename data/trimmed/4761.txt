Page1一种基于局部Lipschitz下界估计支撑面的差分进化算法周晓根张贵军郝小虎俞立(浙江工业大学信息工程学院杭州310023)摘要为了减少智能优化算法求解复杂问题时所需的目标函数评价次数,降低算法计算代价,在差分进化算法框架下,结合Lipschitz估计理论,提出一种基于局部Lipschitz下界估计支撑面的差分进化算法.首先,对新个体的N邻近个体构建Lipschitz下界估计支撑面,进而通过支撑面获取新个体的下界估计值;然后,根据下界估计值设计Lipschitz估计选择策略来指导种群更新;其次,利用下界估计区域的极值信息排除部分无效区域,逐步缩小搜索区域;最后,根据N邻近个体下降方向和主导支撑面下降方向设计广义下降方向做局部增强.数值实验结果表明,所提算法与文中给出的主流算法相比,能够以较少的目标函数评价次数获得高质量的最优解.关键词差分进化;智能优化算法;Lipschitz下界估计;全局优化;支撑面1引言近年来,智能优化算法被广泛用于各种实际应用问题的求解.典型的智能优化算法包括遗传算法(GeneticAlgorithm,GA)[1]、差分进化算法(Differen-tialEvolution,DE)[2]、粒子群算法(ParticleSwarmOptimization,PSO)[3]以及蚁群算法(AntColonyOptimization,ACO)[4],这些算法不仅鲁棒性强,而且实用性强,被成功应用于电力、化工、通信以及网络等领域[5-7].然而,绝大多数智能优化算法面临的一个主要问题就是求解时需要大量的目标函数评价次数,从而导致计算代价极高.尤其对于一些实际优化问题,由于仿真模型运行时间的限制,在求解过程中,对新产生的候选解进行目标函数评价极其费时.例如,对于一个二维粗水动力仿真模型,运行一次可能耗时一分钟左右,对于一个完整的三维水动力模型运行一次可能耗时几分钟,甚至达到几小时[8].又如蛋白质结构预测问题中,对能量函数评价时有时需要调用第三方能量包,从而导致评价一次需要几秒,甚至达到几分钟.因此,对于这些目标函数计算复杂的实际优化问题,如何减少求解过程中所需的目标函数评价次数,降低算法计算代价是一个极其重要的问题,也是计算机科学领域面临的一个亟需解决的问题.针对上述问题,国内外学者相继提出了各种解决方法,其中代理模型(SurrogateModel)[9]和k-近邻预测(k-NearestNeighborPredictor)[8]为两种最常用的方法.代理模型方法利用代理模型来代替计算代价昂贵的目标函数评价.基于此方法,Liu等人[10]提出了一种基于高斯代理模型的进化算法(GaussianProcessSurrogateModelAssistedEvolutionaryAlgorithmforMedium-scaleComputa-tionallyExpensiveOptimizationProblems,GPEME),利用一种降维技术将原优化模型映射到一个低维空间,并通过一种代理意识模型搜索机制使得算法集中搜索希望较大的子区域.Zhou等人[11]提出了一种基于多代理的文化基因算法,在进化过程中,通过精确插值代理模型来提高算法的局部搜索能力,以降低算法的计算代价.Lim等人[12]提出了一种广义代理进化算法,利用各种不同代理模型预测值的加权和来指导种群进化,并通过各代理模型的不确定性预测来自适应调整加权值.上述算法最大的优点就是代理模型的计算复杂度远低于原目标问题,因此可以有效地降低计算代价.然而,没有一个通用的代理模型可以适用于所有问题,代理模型的选择直接影响着算法的性能[9].k-近邻预测方法利用与个体最近的训练样本来预测目标函数值.基于此方法,Liu等人[8]提出了一种基于k-近邻预测的差分进化算法(DifferentialEvolutionusingk-NearestNeighbourPredictor,DE-kNN),在进化前期,将所有个体作为训练样本进行目标函数评价,而在进化后期,利用k-近邻预测个体的目标函数值,同时从新种群中选取部分较优个体更新训练样本.Park等人[13]提出了一种基于加速k-近邻预测的差分进化算法(DifferentialEvolutionusingSpeed-upk-NearestNeighbourEstimator,DE-EkNN),该算法与Liu等人[8]提出的算法最大的区别就是通过训练样本的加权平均值来预测个体的目标函数值,并有选择性的保存样本.Pham[14]提出了一种基于近邻比较的差分进化算法,在进化过程中,根据新个体的近邻预测值与目标个体的函数值比较来判断是否需要对新个体评价.上述算法的最大优点就是结构简单,但是其面临的主要问题就是需要内存来保存大量的训练样本,随着问题维数的增大,所需内存也急剧上升,因此,k-近邻预测算法并不适用于高维问题[14].Lipchistz估计方法[15]作为一种确定性方法,不需要进行模型选择和样本训练,而是利用一系列支撑函数建立原目标函数的下界估计,并通过不断增加支撑函数的数量使得下界估计不断逼近原目标函数,从而使得支撑函数的全局最优解不断向原目标函数的全局最优解收敛.因此,可以通过高效枚举下界估计的局部最优解得到原目标函数的全局最优解.Lipchistz估计方法现已被应用于各种问题,如分子结构预测[16]、拟凸规划[17]和半无限规划[18]等.然而,为了得到更加精确的最优解,Lipchistz估计方法需要使用大量的支撑函数来逼近原目标函数,从而导致极高的空间复杂度,因此,Lipchistz估计方法仅适用于维数小于10的问题[19-20].为了减少智能优化算法的函数评价次数,论文作者在群体算法框架下,结合抽象凸理论[21-22],提出了一种基于抽象凸下界估计的群体全局优化算法(AbstractConvexUnderestimatePopulation-basedAlgorithm,ACUP)[23],该算法首先对整个初始种群构建抽象凸下界支撑面,然后利用不断收紧的下界信息来指导种群更新,最后根据进化信息更新下界支撑面.然而,对于一些复杂的高维优化问题,空间复杂度问题需要改进.因此,在此基础上,进一步引入局部抽象凸估计策略,提出了一种基于抽象凸Page3下界估计选择策略的差分进化算法(DifferentialEvolutionAlgorithmBasedonAbstractConvexUnderestimateSelectionStrategy,DEUS)[24],该算法通过提取新个体的邻近个体建立局部抽象凸下界松弛模型,进而利用下界松弛模型估计目标函数值来指导种群更新,并根据下界极值信息排除部分无效区域,同时借助支撑面的下降方向实现局部增强.实验结果表明,ACUP和DEUS算法能够有效地降低算法计算代价,提高算法性能.然而,在使用ACUP和DEUS算法求解时,需要将原目标函数转化到单位单纯形约束条件下,并且需要对原目标函数加上常数C转化为正齐次递增函数(IncreasingPositivelyHomogeneousFunctionofDegreeOne,IPH)[25],由于目标函数曲面的粗糙复杂,不同的区域需要的常数C不同,所以C的值有时需要设置得异常的大,导致算法获得的下界估计值与实际目标函数值的误差较大,从而影响算法的性能.本文在DE算法框架下,结合Lipschitz估计理论,对种群中的局部个体(新个体的N邻近个体)构建Lipschitz下界估计支撑面,进而通过支撑面获取目标函数的下界估计信息来指导种群进化,从而提出一种基于局部Lipschitz下界估计支撑面的差分进化算法(DifferentialEvolutionAlgorithmBasedonLocalLipschitzUnderestimateSupportingHyper-planes,DELLU).本文工作与文献[24]的主要区别在于:(1)采用的Lipschitz估计支撑函数不需要通过模型转换将原目标函数转化为单位单纯形约束条件下的IPH函数,因此能够获得更加精确的下界估计值;(2)设计了一种局部Lipschitz估计选择策略,在种群更新环节,根据新个体的下界估计值分3种情形来判断是否需要对新个体进行函数评价,有效地减少不必要的函数评价次数;(3)根据邻近个体下降方向和主导支撑面下降方向提出了一种局部增强广义下降方向,利用此方向对新个体作局部增强,从而加快算法的收敛速度.2预备知识2.1基本定义考虑如下全局优化问题其中,x=(x1,x2,…,xn)为n维优化变量,D为可行域空间,f(x)为定义在可行域D上的目标函数,在可行域空间D中可能存在着多个全局最优解和大量的局部最优解.定义1.若函数f:x→R对于任意的x1,x2∈D都满足则称函数f关于可行域DLipschitz连续,其中,M称为Lipschitz常数.定义2.若存在函数族UH,使得函数f满足其中,H为定义在可行域D上的函数族,则称函数f是关于函数族H的抽象凸函数(H-convex).定义3.设H为定义在可行域D上的函数族,函数f为H-convex函数,则称Hf(x)={h∈H:h(y)f(y),f(x)=h(x)},为函数f在点x处的H-次微分(H-subgradient).定义4.考虑一个由r=n+1个半空间相交形成的有限凸多边形P=∩r则称dP(x1,x2)=maxmaxj=1,…,r(x1为点x1和x2之间关于凸多边形P的单纯形距离.2.2DEUS算法定义5.设种群P={x1,x2,…,xNP},狓trial为新生成个体,则称与狓trial之间欧氏距离最近的N个种群个体为狓trial的N邻近个体,其中NP为种群规模,欧氏距离为假设式(1)的目标函数f(x)满足定义1,为了建立f(x)的抽象凸下界估计松弛模型,首先需要根据线性变换式(7)将原目标函数f(x)转化到单位单纯形空间S(S≡{x∈Rn+1xi≡(xi-ai)∑n烄烅xn+1≡1-∑n烆其中,ai和bi分别为xi的下界和上界.通过上述线性变换,原目标函数被转换为Page4然后,进一步对式(8)的目标函数加上常数C(C2M,M为Lipshitz常数)将其转化为IPH函数,即g-=f-+C.经过上述模型转换过程,当新个体狓trial产生后,根据式(6)提取新个体的N邻近个体(xk,g-(xk)),k=1,2,…,N可以建立式(9)所示的局部抽象凸下界估计松弛模型第k个支撑函数为hk(x)=mini=1,…n+1lk其中为点(xk,g-(xk))的抽象凸下界估计支撑向量.建立了局部抽象凸下界估计松弛模型以后,可以获取目标函数的下界信息来指导种群进化.如图1所示,假设A为新个体,B为目标个体,C和D为A的N邻近个体,根据式(11)对个体C和D建立抽象凸下界支撑面,从而可以根据式(9)计算出新个体A的下界估计值y-比较来判断是否需要对新个体A进行目标函数评价,有效减少目标函数评价次数;其次,通过比较下界估计区域的极值dmin与当前种群的最优值来判断下界估计区域所对应的优化区域(即C和D之间的区域)是否为无效区域;最后,根据抽象凸下界估计支撑面的下降方向获取下界估计区域的极值解作局部增强,即判断下界估计区域的极值解在目标函数上对应的点E是否优于新个体A.图1DEUS算法的局部抽象凸估计选择策略示意图3DELLU算法在DE算法中,为了判断产生的新个体是否优于目标个体,算法需要对每次迭代产生的新个体进行目标函数评价,然而,对于一些实际优化问题,由于其仿真模型的复杂性,导致其目标函数评价极其费时.为了减少DE算法的函数评价次数,降低算法计算代价,同时提高算法性能,本文结合Lipschitz估计理论,通过对新个体的N邻近个体构建Lipschitz下界估计支撑面来指导种群进化.3.1局部Lipschitz估计选择策略假设式(1)的目标函数f(x)满足定义1,则根据Lipschitz估计理论可知[16],函数f是关于以下支撑函数的H-convex函数其中,M为Lipschitz常数.由于任一Lipschitz函数的H-次微分不为空[20],则式(12)的支撑函数可进一步转化为其中,dP(x,xk)为点x和xk之间的单纯形距离.通过引入松弛变量xn+1=1-∑n{1,2,…,n+1},可将单纯形距离dP(x,xk)简化为则式(13)的支撑函数可简化为其中犾k=f(xk)(经过变异、交叉操作生成新个体狓trial后,提取称为点xk处的Lipschitz下界估计支撑向量.狓trial的N邻近个体xt下界估计支撑向量犾t个体的支撑向量可以计算出狓trial的下界估计值y-trial,即定理1.设xt其中,ltN邻近个体,则狓trial的下界估计值y-trial满足其中,f(狓trial)为新个体狓trial的实际目标函数值.证明.由于目标函数f满足定义1,则(1)若f(xtf(xtf(xt由式(12)可知Page5ht(狓trial)=f(xt根据式(13)和式(14),式(20)可简化为ht(狓trial)=minj∈I(f(xt则由式(17)有(2)若f(xt-f(狓trial)+f(xtf(xt由于‖狓trial-xt同理可得y-trialf(狓trial),由于对于t∈{1,2,…,N}有xt定理1表明,新个体狓trial根据其N邻近个体的Lipschitz支撑向量得到的下界估计值y-trial小于其实际目标函数值f(狓trial).进而在更新过程中,可以根据新个体的下界估计值来指导种群更新,即根据下界估计值判断是否有必要对新个体进行目标函数评价.以图2所示的一维问题为例说明更新过程中可能出现的情形.图2局部Lipschitz估计选择策略中的两种情形情形(1).如图2(a)所示,A为目标个体xg表示进化代数),B为新个体狓trial,个体C和D为B的邻近个体,则根据C和D的L下界估计支撑面可以计算得到B的下界估计值y-trial,由于y-trial大于目标个体的函数值f(xgy-trial,则f(狓trial)>y-trial>f(xg狓trial进行目标函数评价(即计算f(狓trial)),即可判定新个体狓trial比目标个体xgbest).因此,也无需对新个体狓trial进行目标函数情形(2).如图2(b)所示,A为目标个体xg新个体狓trial,E为当前种群中的最优个体xg根据B的邻近个体C和D的下界估计支撑面可以计算出狓trial的下界估计值y-trial,由于根据y-trial小于目标个体的函数值f(xg优于目标个体xg优个体xgy-trial>f(xgf(xg评价,即可判定抛弃狓trial,而保持目标个体xg情形(3).除了情形(1)和情形(2)以外的情形,则需要对新个体狓trial进行目标函数评价,通过比较f(狓trial)和目标个体的函数值f(xg狓trial是否能够替换目标个体xg略可进一步概括为根据上述3种情形,局部Lipschitz估计选择策xg+1i=由此可以看出,通过对新个体的N邻近个体建立Lipschitz下界估计支撑面获取新个体的下界估计值来指导种群更新,只在情形(3)中需要对新个体进行目标函数评价,因此相对于传统DE算法,可以有效地减少目标函数评价次数.与DEUS算法相比,DELLU算法在建立下界估计的过程中,采用的Lipschitz估计支撑函数不需要通过模型转换过程将原目标函数转化为单位单纯形中的IPH函数,避免了在转化过程中的信息丢失,从而能够求得更加精确的目标函数下界估计值;另外,DELLU算法在选择策略中增加了情形(2),只有在情形(3)中才需要对新个体进行目标函数评价.因此DELLU算法能够节省更多的函数评价次数.3.2无效区域排除策略为了进一步提高算法的搜索效率,减少函数评价次数,根据Lipschitz下界估计区域的极值信息排除部分无效子区域(即此子区域不可能包含全局最优解),逐步缩小搜索区域,从而使得算法集中搜索希望较大的区域.Page6定义6.定义新个体狓trial的N邻近个体的Lipschitz下界估计支撑面犾t成的区域为下界估计区域.由式(17)可知,与下界估计区域A(L)对应的搜索子区域中的任一个体xi均可根据式(17)计算得到其下界估计值y-i=HN(xi).又由定理1可知,y-i均小于xi的实际目标函数值f(xi),因此,当下界估计区域A(L)的极小值(即与A(L)对应的搜索子区域的最小下界估计值)大于当前种群(即第g代种群)的最优值f(xg搜索子区域不可能包含全局最优解,从而可以可靠排除.由Lipschitz估计理论可知[16],任一下界估计区域A(L)均对应着一个由n+1个支撑向量组成的支撑矩阵犔:其中,Li,j=f(xki)定理2.设xmin为下界估计区域A(L)的局部极小解,dmin=HN(xmin)为其对应的值,则A(L)所对应的支撑矩阵犔满足以下性质:(1)xmin,i=dmin(2)dmin=Trace(L)+1(3)i,j∈I,i≠j:lkj(4)r{k1,…,kn+1},i∈I:Li,i=lkiilr证明.设R(x)={k∈{1,…,N}:HN(x)=hk(x)}(27)则由文献[26]可知,对于u∈U(xmin,D)={u∈Rn+1:α0>0:xmin+αu∈D,α∈(0,α0)},存在k∈R(xmin)使得设m∈I,对于i≠m存在λi>0,使得∑n+1考虑向量狌显然狌∈U(xmin,D).因此存在k∈R(xmin),使得(hk)(xmin,狌)0.如果Qk(xmin)≠{m},则则与式(29)矛盾.因此,对于任意的m∈I,存在km∈R(xmin)使得Qk(xmin)={m}.由此可知,设kmi为某一索引下标值使得Qkmi(xmin)={mi},i=1,2,若m1≠m2,则km1≠km2.(1)设km∈R(xmin)使得Qkm(xmin)={m}.因为km∈R(xmin)且m∈Qkm(xmin),则因此,则由式(33)有xmin=HN(xmin)(2)由xn+1=1-∑ni=11=∑n+1=dmin(n+1)因此,根据式(35)有dmin=MTrace(L)((3)因为kj∈R(xmin),Qkj(xmin)={}j,则i+xmin,i)>hkj(xmin)=M(lkjM(lkj由式(37)有又因为xmin,i=dminM-lkii,xmin,j=dminM-lkji≠j时(4)由式(17)有HN(xmin)=maxkNmini∈IM(lk因此,Page7maxkNmini∈IM(lk设r{k1,k2,…,kn+1},则在选择过程中,当新个体的下界估计值ytrial大于因此,r{k1,…,kn+1},i∈I:lkiilr目标个体xg中最优个体的目标函数值f(xg或y-trialf(xg出下界估计区域的极小值dmin.如图3所示,由于dminf(xg索子区域(即A和B之间的区域)中不可能包含全局最优解,从而可将此区域看作无效区域,并建立数组IR记录其对应的支撑矩阵.与DEUS算法相比,由于DELLU算法能够求得更加精确的下界估计值,而无效区域的排除与dmin的精确度有很大关系,因此DELLU算法能够找出更多的无效区域.定理3.设狓trial为新个体,Lu为某一无效区域对应的支撑矩阵,若狓trial不在此无效区域中,则i,j∈I,i≠j:xkj其中,xkji为Lu中,则根据定理2的性质(4)有证明.由于狓trial不在Lu所对应的无效区域将式(16)代入上式有由式(45)有M(xkjj-xtrial,j)f(xkj)-f(狓trial)<M(xkji-xtrial,i)则由式(46)有xkj当新个体狓trial生成后,若无效区域存在,即IR不为空时,根据定理3即可判断出新个体是否被包含在无效区域中,如果狓trial在无效区域中,则保持目标个体xg由此可以看出,通过无效区域排除策略可以有效排除部分无效区域(即在此区域中不可能包含全局最优解),逐步缩小搜索区域,使得算法集中搜索希望较大的子区域,不仅能够提高算法的搜索效率,降低算法的计算代价,而且在一定程度上能够防止算法陷入局部最优,提高算法的可靠性.3.3广义下降方向局部增强策略传统DE算法全局搜索能力较强,但是局部搜索能力较弱,尤其在进化后期收敛速度较慢,因此后期需要进行大量的目标函数评价.针对此问题,DEUS算法通过直接获取抽象凸估计区域的极小解在目标函数曲面上对应的点与目标个体进行比较,以获得更优个体.而在本文中则结合N邻近个体的下降方向和主导支撑面的下降方向提出一种局部增强广义下降方向,利用此方向引导个体作局部增强,提高算法的局部搜索能力,从而加快算法的收敛速度,进一步减少函数评价次数.y-trial=HN(狓trial)=maxtNminj=1,…,n+1M(lt即下界估计值y-trial可以根据邻近个体xknb的支撑面lknb求出,则称支撑面lknb为主导支撑面,且称支撑面lknb的下降方向定义7.若新个体狓trial的下界估计值dom=(ddom,1,…,ddom,n+1)d→为主导支撑面下降方向,其中xmin为下界估计区域的极小解.N邻近个体,则称方向定义8.设xtnb=(dnb,1,…,dnb,n+1)d→为N邻近个体下降方向,其中,xBnb和xWnb分别为N邻近个体中的最优个体和最差个体.导支撑面下降方向和N邻近个体下降方向,则称方向定义9.设d→为狓trial的广义下降方向.如图4所示,设B为新个体狓trial,个体C和D为B的邻近个体,图中给出了B的主导支撑面下降方向、N邻近个体下降方向和广义下降方向,则当新个体狓trial优于目标个体xgPage8敛速度,根据狓trial的广义下降方向作局部增强,即其中参数F与变异策略中的步长因子相同,如果xtest优于狓trial,则xtest替换目标个体xg3.4算法设计DELLU算法的主要思想是:在基本DE算法框架下,提取新个体的N邻近个体构建基于Lipschitz估计理论的下界支撑面,通过支撑面获取新个体的下界估计信息指导种群更新,并根据下界估计区域的极值信息排除部分无效区域,同时根据基于N邻近个体下降方向和主导支撑面下降方向的广义下降方向作局部增强,从而有效提高算法的性能,减少进化过程中所需的目标函数评价次数,降低算法的计算代价.以极小化问题为例,算法整体流程如下:1.初始化.设置种群规模NP,交叉概率CR,增益常数F,N邻近个体数目N,并随机生成初始种群P={x1,x2,…,xNP},初始化无效区域IR为空,进化代数g=0.3.1设置i=1;3.2如果IR不为空,则根据式(43)判断xi3.3根据式(6)找出xitrial的N邻近个体xt3.4根据式(16)计算xt3.5根据式(17)计算xitrial的下界估计值y-trial,并根据3.6如果y-trialf(xg3.7计算当前种群的最优值f(xg2.对每个目标个体xg3.对每个目标个体xg个体xitrial.操作:4.g=g+1.5.如果不满足终止条件,则转至步骤2.6.输出结果,退出.注:步骤2中的变异交叉过程参见文献[2];步骤3.17在每次迭代结束后删除所有支撑向量,只保留无效区域的支撑向量.3.5复杂度分析根据3.4节的算法步骤分析DELLU算法相对于传统DE算法所增加的时间复杂度.步骤3.2中判断新个体是否在无效区域中时,只需判断新个体是否满足式(43),因为本文通过树的形式保存无效区域,因此时间复杂度为O((n+1)logT),其中T为无效区域的数目;步骤3.3中找出新个体的N邻近个体时需要计算各个体与新个体的距离,时间复杂度为O(n·NP),根据距离找出N个邻近个体的时间复杂度为O(N·NP);步骤3.4中计算支撑向量的时间复杂度为O(N(n+1));步骤3.5中计算下界估计值的时间复杂度为O(N(n+1));步骤3.6的判断下界估计值是否大于目标个体的函数值的时间复杂度为O(1);步骤3.7中计算当前种群的最优值的时间复杂度为O(NP);步骤3.8、步骤3.9和步骤3.10的时间复杂度均为O(1);步骤3.11中判断新个体是否优于目标个体的时间复杂度为O(2);步骤3.12中计算主导支撑面下降方向的时间复杂度为O(n+1);步骤3.13中计算N邻近个体下降方向时需要找出N邻近个体中的最优个体和最差个体,则时间复杂度为O(2N+1);步骤3.14中计算广义下降方向的时间复杂度为O(n+1);步骤3.15中计算局部增强个体的时间复杂度也为O(n+1);步骤3.16中判断局部增强个体是否优于Page9新个体时,需要计算局部增强个体的目标函数值,若增强个体优于新个体,则增强个体替换新个体,则时间复杂度为O(n+1);步骤3.17中删除所有支撑向量的时间复杂度为O(1).上述步骤中,某些步骤在满足条件的情况下才会执行,当上述步骤都执行时,最大时间复杂度为O(2N·n+n·NP+N·NP+NP+n·logT+logT+4n+4N+12),忽略常量、低次幂和最高次幂的系数,则最终时间复杂度为O(max{N·n,n·NP,N·NP}).如果只对新个体附近的两个个体建立支撑向量(即N=2),则当问题维数大于2时,算法的时间复杂度为O(n·NP),表120个标准测试函数函数名SphereTabletSchwefel2.22f3(x)=∑nSchwefel1.2f4(x)=∑nStepZakharovRosenbrockf7(x)=∑n-1Griewankf8(x)=1+1Schaffer2f9(x)=∑n-1AckleySchwefel2.26f11(x)=-∑nHimmelblauf12(x)=n-1∑nLevyandMontalvo1f13(x)=π(10sin2(πy1)/n+∑n-1LevyandMontalvo2f14(x)=0.1(sin2(3πxi)+∑n-1i+1)0.1)+1)(yi-1)2(1+10sin2(πyi+1))+(xi-1)2(1+sin2(3πxi+1)+RastriginCosineMixturef16(x)=0.1∑nKowalikSix-humpCamel-backf18(x)=4x2BraninGoldstein-Pricef20(x)=[1+(x1+x2+1)2(19-14x1+3x22+4x42i+bix3+x4))2由此可以看出,DELLU算法没有显著增加DE算法的时间复杂度,且DELLU算法主要针对目标函数计算复杂的优化问题,因此,建立下界估计所增加的计算代价小于实际目标函数评价所需的计算代价.4数值实验4.1测试函数及算法参数设置应用20个典型的标准测试函数来验证所提算法的性能,表1给出了各测试函数基本参数和数学表达式.其他特性见文献[27-28].4cos(2πxi))+20+e30(-30,30)0i=1Page1020个标准测试函数中包含了7个高维单模函数(f1~f7)、8个高维多模函数(f8~f15)和5个低维多模函数(f16~f20),其中,高维多模函数局部最优解的数量随着维数的增大呈现指数增加.上述所有测试函数均满足Lipschitz条件.DELLU算法参数设置:用于建立Lipschitz下界估计支撑面的新个体的邻近个体数目N=2,种群规模NP=50.其次,为了公平比较,所有算法均设置相同的运行终止条件,且每种算法对于每个测试函数均独立运行30次.实验环境为Intel(R)Corei5-2410MCPU@2.30GHzwith8GBRAM,Windows7,算法实现代码采用VisualStudio2012C++和Matlab8.2编写.4.2DELLU算法与基本DE算法比较为了验证DELLU算法相对于基本DE算法的优势,选用3种使用不同变异策略的基本DE算法进行比较:DE/rand/1[2]、DE/best/1[2]和DE/rand-to-best/1[32],上述所有比较算法的参数设置均为F=0.5,CR=0.5,NP=50[29].实验中,采用3种评价指标来比较分析各算法的性能:(1)在设定的最大函数评价次数MaxFE(MaximumNumberofFunctionEvaluations)内达到给定的函数误差值(f(x)-f(x))精度δ时所需的目标函数评价次数FE(FunctionEvaluations);(2)成功率SR(SuccessRate);(3)在设定的最大函表2DE/rand/1、DE/best/1、DE/rand-to-best/1和DELLU算法的平均函数评价次数和成功率函数维数f1303.01E+041001.09E+041009.40E+03601.20E+04100f2303.11E+041001.17E+041009.47E+03502.56E+04100f3303.69E+041001.38E+041001.11E+041001.64E+04100f430NAf5301.65E+041006.40E+03205.14E+03535.66E+03100f6302.56E+051004.78E+041005.71E+04608.09E+04100f730NAf8303.29E+04971.12E+04531.47E+04402.00E+04100f9301.32E+05100f10304.03E+041001.49E+04671.41E+04332.84E+04100f11301.29E+05100f12305.18E+0490f13301.96E+041006.38E+03875.34E+031009.09E+03100f14301.90E+041006.82E+03905.16E+03938.53E+03100f1530NAf1642.22E+031001.22E+031001.34E+031009.60E+02100f1746.62E+031002.85E+03903.75E+03904.78E+03100f1821.49E+031007.58E+021009.16E+021001.20E+03100f1921.54E+031007.89E+021009.71E+021001.39E+03100f2021.64E+031009.40E+021001.04E+031001.30E+03100总平均值8.55E+0484.49.07E+0464.79.70E+0454.02.40E+0499.700000数评价次数内所求得的最小函数误差值的平均值(MeanError)和标准偏差值(StandardDeviation,StdDev).在函数误差值(f(x)-f(x))中,f(x)表示当前所找到的最优值,f(x)表示函数的全局最优值(见表1).对于指标(2),规定算法在MaxFE内能够达到设定的函数误差值精度δ时为求解成功,则SR为成功运行次数和总运行次数之比.在此实验中,对于指标(1)和(2),最大函数评价次数MaxFE设置为300000,误差值精度δ设置为1E-04,对于指标(3),MaxFE设置为1000×n,其中n为维数.另外,为了验证所提算法相对于其他比较算法是否有显著性优势,选用WilcoxonSignedRankTest[30]对各算法30次优化得到的结果进行非参数假设检验,显著性水平为5%,并且通过“+”表示所提算法显著优于所比算法,“-”表示所提算法显著差于所比算法,“≈”表示两种算法之间没有显著性差异.表2给出了DE/rand/1、DE/best/1、DE/rand-to-best/1和DELLU4种算法30次独立运行的平均目标函数评价次数和成功率,其中“NA”表示对应的算法在给定的MaxFE内不能达到指定的函数误差值精度δ.从表中可以看出,DE/rand/1算法对3个函数无法求解成功(f4、f7和f15);对于DE/best/1和DE/rand-to-best/1算法,其变异策略具有Page11贪婪性,即在迭代过程中促使每个个体向当前种群中的最优个体收敛,而在DELLU算法中仅采用DE/rand/1变异策略,因此在对部分测试函数求解时,DE/best/1和DE/rand-to-best/1算法所需的函数评价次数少于DELLU算法,但是由于DE/best/1和DE/rand-to-best/1算法的贪婪性,使得算法很容易陷入局部最优,因此其成功率较低.DE/best/1算法对5个函数无法求解,其中包括1个高维单模函数(f4)和4个高维多模函数(f9、f11、f12和f15),DE/rand-to-best/1算法对6个函数无法求解,其中包括2个高维单模函数(f4和f7)和4个高维多模函数(f9、f11、f12和f15);除了函数f7,DELLU算法对其他函数能够以100%的成功率求解.尤其对于函数f7(Rosenbrock问题),当其维数大于3时则变为多模函数[31],虽然算法能够很快找到其局部最优解,但是由于其函数曲面极其复杂,使得算法极易陷入局部最优,而无法找到全局最优解,4种比较算法在对函数f7求解时,只有DE/best/1和DELLU算法能够成功求解,且相比于DE/best/1算法,DELLU算法可以以较少的函数评价次数和较高的成功率求解.另外,从表2最后一行的平均结果也可以看出,DELLU算法对于20个测试函数的总平均函数评价次数为2.40E+04,DE/rand/1、DE/best/1和DE/rand-to-best/1算法分别为8.55E+04、9.07E+04和9.70E+04,也就是说,DELLU算法相对于DE/rand/1、DE/best/1和DE/rand-to-best/1算法分别节省了71.9%、73.5%和75.3%的函数评价次数;其次,DELLU的可靠性最高,成功率为99.7%,由于DE/rand/1、DE/best/1和DE/rand-to-best/1算法对部分函数无法成功求解,所以成功率分别为84.4%、64.7%和54.0%.总之,相比于3种传统DE算法,DELLU算法在保证成功率的同时,可以有效减少函数评价次数.为了验证DELLU算法相对于3种传统DE算法在FE和SR方面的整体优势,首先根据表2中各算法对于各测试函数在最大函数评价次数MaxFE内达到给定的函数误差值精度δ所需的函数评价次数及成功率计算各算法(a)对各测试函数(p)的成功性能(SuccessPerformance,SP)然后通过各算法对于各测试函数的SP值除以最优算法的SP值进行归一化处理;最后绘制成功表现归一化经验分布图.图中,SP值最小而经验分布值最大的算法最好.图5给出了DE/rand/1、DE/best/1、DE/rand-to-best/1和DELLU算法对20个测试函数的成功表现归一化经验分布图,可以看出,DELLU算法的经验分布值最先到达1,由此表明,DELLU算法在FE和SR方面的整体性能优于其他3种传统DE算法.图5DE/rand/1、DE/best/1、DE/rand-to-best/1和表3给出了DE/rand/1、DE/best/1、DE/rand-to-best/1和DELLU4种算法对各测试函数30次优化得到的最小函数误差的平均值和标准偏差值,最优结果通过加粗标出.从表中可以看出,DELLU算法对于大部分函数优于3种基本DE算法;其次,从表中最后一行列出的显著性检验结果可以看出,DE/rand/1算法没有优于DELLU算法任何问题,而DELLU算法优于DE/rand/1算法17个问题,对于其余3个问题,两种算法相比没有显著性差异;虽然DE/best/1算法优于DELLU算法4个问题,其中包括3个高维单模问题(f2、f3和f6)和1个低维多模问题(f19),但是DELLU算法优于DE/best/1算法13个问题,对于其余3个问题,两种算法相比没有显著性差异,由此可以推断出,DE/best/1算法仅适合求解高维单模和低维多模问题;DE/rand-to-best/1算法仅优于DELLU算法1个单模问题(f19),而DELLU算法优于DE/rand-to-best/1算法16个问题,对于其余3个问题,两种算法相比没有显著性差异;对于DELLU算法,除了函数f2、f3、f6和f19,其余函数的结果均优于3种所比基本DE算法.Page12表3DE/rand/1、DE/best/1、DE/rand-to-best/1和DELLU算法函数误差的平均值和标准偏差函数维数f1301.17E-04±5.27E-05+9.66E-19±2.97E-18+4.33E-02±1.64E-01+8.98E-21±5.23E-21f2301.95E-04±7.75E-05+1.63E-18±2.49E-18-4.21E-01±8.71E-01+2.65E-12±1.71E-12f3301.41E-03±3.20E-04+1.18E-11±2.03E-11-1.25E-07±3.62E-07+8.12E-11±3.06E-11f4306.02E+04±2.03E+04+6.69E+03±3.20E+03+3.62E+03±1.43E+03+1.07E+01±6.38E+00f5300.00E+00±0.00E+00≈6.17E+00±9.81E+00+1.60E+00±1.89E+00+0.00E+00±0.00E+00f6309.64E+01±2.11E+01+8.92E-02±9.90E-02-1.82E-01±2.31E-01+1.74E+00±2.38E+00f7302.56E+01±3.42E-01+2.37E+01±1.04E+01+2.81E+01±1.41E+00+3.89E-02±3.49E-02f8303.38E-03±9.06E-03+7.95E-03±1.51E-02+2.05E-02±5.98E-02+3.60E-08±2.13E-08f9301.30E+01±1.23E+00+6.41E+00±5.44E+00+1.03E+00±2.03E+00+8.66E-01±3.31E-01f10302.52E-03±5.14E-04+7.48E-01±8.93E-01+1.04E-01±3.08E-01+4.24E-05±1.91E-05f11305.51E+03±3.55E+02+1.56E+03±5.09E+02+1.78E+03±1.06E+03+0.00E+00±0.00E+00f12301.22E+01±3.75E+00+1.06E+01±2.79E+00+9.43E+00±2.71E+00+0.00E+00±0.00E+00f13301.25E-07±7.64E-08+2.07E-02±4.22E-02+2.75E-08±1.04E-07+6.50E-14±3.43E-14f14307.32E-08±3.97E-08+1.46E-03±3.80E-03+1.10E-03±3.35E-03+3.50E-17±1.60E-17f15301.46E+02±9.82E+00+2.73E+01±1.20E+01+7.65E+01±2.43E+01+4.06E-08±2.84E-08f1640.00E+00±0.00E+00≈0.00E+00±0.00E+00≈0.00E+00±0.00E+00≈0.00E+00±0.00E+00f1743.29E-04±8.44E-05+1.46E-03±5.07E-03+8.82E-04±3.63E-03+2.13E-04±8.34E-05f1822.32E-05±5.02E-05+0.00E+00±0.00E+00≈0.00E+00±0.00E+00≈0.00E+00±0.00E+00f1926.87E-05±7.87E-05≈7.00E-06±1.69E-16-7.07E-06±2.54E-07-4.96E-05±6.29E-05f2029.67E-06±1.47E-05+0.00E+00±0.00E+00≈0.00E+00±0.00E+00≈0.00E+00±0.00E+00+/≈/-4.3DELLU算法与state-of-the-artDE算法比较4.3.1与基于估计的DE算法比较为了验证DELLU算法相对于其他基于估计的DE算法的优势,选取2种基于k-近邻预测的DE算法DE-EkNN[13]、DE-kNN[8]和1种基于代理模型的DE算法GPEME[10]与DELLU算法进行比较.实验中,DELLU算法的参数设置均与上述所比算法的原文献相同.为了客观比较,DE-EkNN、DE-kNN和GPEME算法的实验数据均来自原文献.由于原文献只对表1中的部分测试函数进行了测试,所以实验中只选取部分函数进行分析比较.表4给出了DE-EkNN、DE-kNN和DELLU算法对部分测试函数30次求解所得到的结果,其中最优结果通过加粗标出.从表中可以看出,对于函数f5,3种算法均能够得到全局最优解;DE-EkNN算法仅优于DELLU算法1个问题(f8);DE-kNN算法对函数f5、f18和f19求解时与DELLU获得了相同的结果,即找到了全局最优解,但对于其他问题均无法优于DELLU算法;DELLU算法优于DE-EkNN和DE-kNN算法大多数问题,尤其对函数f1、f11和f15,DELLU算法的优势更加明显.对于函数f1,DELLU算法优于其他算法6个数量级;对于函数f15,DELLU算法优于其他算法11个数量级;对于函数f15,只有DELLU算法能够成功求得全局最优解.此外,需要说明的是,由于所比算法在原文献中没有提供30次独立运行的结果,所以在比较中没有进行显著性检验.表5为GPEME和DELLU算法对函数f7、f8和f10求解所得结果的性能数据,其中,“Best”表示30次独立运行所得结果中的最优值,“Worst”表示最差值,各项性能指标的最优结果通过加粗标出.从表中可以看出,对于函数f7,GPEME算法仅在20维和30维的最优结果方面优于DELLU算法,对于其余性能,DELLU算法均优于GPEME算法.对于函数f8,在20维时,GPEME算法在结果的最优值、平均值和标准偏差方面优于DELLU算法,但在最差值方面逊于DELLU算法;在30维和50维时,DELLU算法除了对于30维的标准偏差不如GPEME算法外,其余性能均优于GPEME算法.对于函数f10,GPEME算法仅在20维的最优值和平均值方面及30维的标准偏差方面优于DELLU算法,其他性能均逊于DELLU算法;在50维时,DELLU算法的所有性能均优于GPEME算法.由此可以看出,DELLU算法总体上优于GPEME算法.Page13表4DE-E犽NN、DE-犽NN和DELLU算法函数误差的平均值和标准偏差函数维数f1f3f4f5f7f8f1010f1110f1510f18f19f20表5GPEME和DELLU算法的优化结果性能对比数据函数维数f7201.51E+017.59E+012.24E+018.16E+011.91E+012.80E+012.12E+012.87E+00f7302.63E+018.82E+014.62E+012.55E+012.94E+017.07E+013.94E+011.26E+01f7501.72E+024.01E+022.58E+028.02E+015.17E+012.46E+021.06E+026.46E+01f8202.00E-042.23E-013.07E-026.82E-021.03E-031.86E-013.57E-026.13E-02f8307.37E-011.08E+009.97E-011.08E-013.80E-021.01E+004.73E-013.31E-01f8502.25E+016.50E+013.66E+011.32E+011.71E-031.14E+004.37E-015.10E-01f10203.70E-031.84E+001.99E-015.77E-011.37E-011.67E+005.60E-014.61E-01f10301.95E+004.96E+003.01E+009.25E-011.41E-023.13E+001.59E+001.19E+00f10509.25E+001.49E+011.32E+011.58E+003.25E-022.51E+008.35E-019.18E-01率CR和增益常数F的设置采用SaDE算法中提出4.3.2与基于自适应机制的DE算法比较的参数自适应机制,具体参见文献[32].在此实验为了验证DELLU算法相对于基于参数和策略中,通过记录最大函数评价次数MaxFE内30次独自适应机制的state-of-the-artDE算法的优势,选取以下3种算法进行比较:立运行所得的函数误差值的平均值和标准偏差来(1)Qin等人[32]提出的策略自适应差分进化算比较分析4种算法的性能,算法终止条件均为法(DifferentialEvolutionAlgorithmwithStrategyMaxFE=2000×n.Adaptation,SaDE):利用均匀布策略对变异率和交表6给出了SaDE、EPSDE、CoDE和DELLU叉概率进行调整,并通过一种学习机制来自适应调算法对各测试函数30次独立运行得到的最小函数整进化过程中不同阶段的变异策略及其参数.误差值的平均值和标准偏差,各函数的最优结果通(2)Mallipeddi等人[33]提出的具系综变异策略过加粗标出.从表中可以看出,除了函数f2、f3、f9及参数的差分进化算法(DifferentialEvolution和f10外,DELLU算法对于其他函数的性能均优于其他3种对比算法,对于函数f5、f12、f16、f18和f20,AlgorithmwithEnsembleofParametersandMuta-tionStrategies,EPSDE):对种群中的每个个体随4种算法均达到了全局最优,而获得了相同的结果;机分配不同的变异策略及参数,同时保存能够产生其次,从表中最后一行的显著性检验结果可以看出,较优个体的变异策略及参数,并对产生较差个体的SaDE算法优于DELLU算法3个问题,其中包括2个高维单模问题(f2和f3)和1个高维多模问题策略及参数重新初始化.(3)Wang等人[34]提出的具有混合新个体生成(f10),DELLU算法优于SaDE算法10个问题,对于策略及参数的差分进化算法(DifferentialEvolution其余7个问题,两者相比没有显著性差异;EPSDE算法仅优于DELLU算法2个高维单模问题(f2和withCompositeTrialVectorGenerationStrategiesandControlParameters,CoDE):设置一组变异策f3),而DELLU算法优于EPSDE算法10个问题,对于其余8个问题,两者相比没有显著性差异;略池和参数池,从而通过随机组合策略池中的变异CoDE算法没有优于DELLU算法任何问题,而策略和参数池中的参数来竞争产生后代个体.DELLU算法优于CoDE算法15个问题,对于其余上述3种比较算法均采用原文献中的参数设置,其次,为了公平比较,DELLU算法中的变异概5个问题,两者之间没有显著性差异.DE-kNNMeanError±StdDev3.52E-08±4.17E-088.79E-05±4.52E-053.84E-02±2.06E-010.00E+00±0.00E+009.15E+00±1.14E+008.17E-02±1.43E-019.28E-05±5.14E-051.63E+03±3.11E+024.24E+01±5.27E+000.00E+00±0.00E+000.00E+00±0.00E+000.00E+00±1.07E-14Page14表6SaDE、EPSDE、CoDE和DELLU算法函数误差的平均值和标准偏差函数维数f1301.29E-23±3.07E-23+3.87E-31±9.46E-31+2.16E-10±1.73E-10+1.65E-37±2.25E-37f2308.85E-23±4.35E-22-8.85E-30±1.75E-29-4.04E-10±3.04E-10+5.38E-21±8.12E-21f3308.70E-16±5.17E-16-1.29E-16±2.15E-16-3.86E-06±1.32E-06+2.91E-11±4.07E-11f4302.83E-02±2.46E-02≈3.82E+03±5.34E+03+8.57E-01±1.06E+00+2.41E-02±1.98E-02f5300.00E+00±0.00E+00≈0.00E+00±0.00E+00≈0.00E+00±0.00E+00≈0.00E+00±0.00E+00f6309.27E-02±1.30E-01+1.42E+01±2.25E+01+5.69E-04±7.53E-04+1.53E-04±2.33E-04f7305.07E+01±3.36E+01+8.61E+00±2.09E+00+1.97E+01±5.80E-01+1.48E-05±1.62E-05f8302.22E-03±4.73E-03+6.57E-04±2.50E-03+2.47E-07±7.34E-07+0.00E+00±0.00E+00f9307.51E-04±7.06E-04-2.35E-01±1.31E-01+1.97E+00±4.24E-01+1.83E-02±6.16E-03f10302.40E-01±4.45E-01+6.04E-15±1.66E-15≈3.75E-06±1.88E-06+7.55E-15±1.47E-15f11300.00E+00±0.00E+00≈0.00E+00±0.00E+00≈1.66E-02±3.13E-02+0.00E+00±0.00E+00f12300.00E+00±0.00E+00≈0.00E+00±0.00E+00≈0.00E+00±0.00E+00≈0.00E+00±0.00E+00f13302.47E-27±4.90E-27+2.91E-29±1.14E-28+1.80E-13±3.36E-13+1.09E-30±8.88E-31f14301.46E-03±3.80E-03+1.75E-32±9.04E-33+1.31E-13±1.51E-13+1.36E-32±2.26E-34f15304.11E-02±1.82E-01+5.48E-02±1.39E-01+3.30E+01±5.81E+00+6.37E-07±5.39E-07f1640.00E+00±0.00E+00≈0.00E+00±0.00E+00≈0.00E+00±0.00E+00≈0.00E+00±0.00E+00f1749.38E-06±1.95E-05+2.26E-05±5.70E-05+9.60E-06±4.06E-05+2.52E-06±1.11E-05f1820.00E+00±0.00E+00≈0.00E+00±0.00E+00≈0.00E+00±0.00E+00≈0.00E+00±0.00E+00f1924.67E-07±1.17E-06+0.00E+00±0.00E+00≈1.33E-07±5.71E-07+0.00E+00±0.00E+00f2020.00E+00±0.00E+00≈0.00E+00±0.00E+00≈0.00E+00±0.00E+00≈0.00E+00±0.00E+00+/≈/-图6给出了测试函数f1、f7、f10和f15的平均收敛曲线图.为了图形的清晰直观,图中纵坐标值为函数误差值的对数.从图4(a)可以看出,SaDE、EPSDE、CoDE和DELLU这4种算法在对Sphere函数求解图6SaDE、EPSDE、CoDE和DELLU算法对部分函数的平均收敛曲线时,DELLU算法收敛速度最快,EPSDE算法次之.从图4(b)可以看出,虽然4种对比算法在对Rosen-brock这一经典复杂问题求解时均陷入了局部最优,但是DELLU算法的收敛速度仍显著优于SaDE、Page15EPSDE和CoDE算法,且最终结果也显著优于SaDE、EPSDE和CoDE算法.从图4(c)可以看出,对于Ackley问题,SaDE算法陷入了局部最优而无法成功求解,虽然DELLU算法和EPSDE算法最终结果相差无几,但是DELLU算法前期收敛速度仍优于EPSDE算法;从图4(d)可以看出,在对Rastrigin函数求解时,虽然DELLU算法前期的收敛速度优势不明显,但是最终还是超过了其他算法,并非常稳健地向着全局最优收敛,且最终结果显著优于其他3种对比算法.4.4DELLU算法与ACUP和DEUS算法比较在ACUP和DEUS算法中,建立下界估计时均采用式(10)所示的抽象凸下界估计支撑函数,而DELLU算法采用的是式(15)所示的Lipschitz下界估计支撑函数.因此,DELLU算法不需要通过线性变换公式将原目标函数转化到单位单纯形空间中,而且也不需要通过增加足够大的常数C将原目标函数转化为IPH函数,而是引入单纯形距离来对Lipschitz下界估计支撑函数进行简化,避免了转化过程中的信息丢失,因此DELLU算法能够得到更加精确的下界估计值.为了验证这一点,以1维Shubert函数为例,对DEUS算法和DELLU算法设置相同的参数,通过建立函数的全局下界估计进行比较.2000个采样点的平均估计误差显示(限于篇幅,文中没有列出),DEUS算法的平均估计误差为3.03,DELLU算法的平均估计误差为1.60.图7也给出了DEUS算法和DELLU算法对Shubert函数表7ACUP、DEUS和DELLU算法的平均目标函数评价次数和成功率函数维数f1301.58E+041001.311.43E+041001.191.20E+041001.00f2302.99E+041001.172.82E+041001.102.56E+041001.00f3301.69E+041001.101.62E+041001.001.64E+041001.07f4308.90E+041001.357.30E+041001.066.59E+041001.00f5306.10E+031001.085.97E+031001.045.66E+031001.00f6309.50E+041001.178.91E+041001.108.09E+041001.00f7306.52E+04901.135.89E+04901.025.96E+04931.00f8303.10E+041001.552.50E+041001.252.00E+041001.00f9308.90E+041001.128.50E+041001.077.98E+041001.00f10303.50E+041001.233.32E+041001.132.84E+041001.00f11302.50E+041001.172.30E+041001.032.14E+041001.00f12302.10E+041001.171.85E+041001.001.91E+041001.06f13301.13E+041001.249.91E+031001.099.09E+031001.00f14309.51E+031001.158.39E+031001.008.53E+031001.03f15302.10E+041001.211.90E+041001.091.74E+041001.00f1649.50E+021001.009.79E+021001.039.60E+021001.01f1745.21E+031001.094.90E+031001.034.78E+031001.00f1821.40E+031001.161.31E+031001.081.20E+031001.00f1921.35E+031001.001.39E+031001.031.39E+031001.03f2021.50E+031001.151.40E+031001.081.30E+031001.00总平均值2.86E+0499.51.182.59E+0499.51.072.40E+0499.71.01所建立的下界估计,可以看出,DELLU算法所建立的下界估计明显比DEUS所建立的下界估计精确.图7DEUS和DELLU算法的下界估计对比为了进一步验证DELLU算法相对于ACUP[23]和DEUS[24]算法在函数评价次数FE和成功率SR方面的优势,首先根据式(52)计算各算法(a)对各测试函数(p)的成功性能,然后根据式(53)计算对应的性能比PR(PerformanceRatio)[35]其中A表示对比算法集.如果性能比ra,p等于1,则表明算法(a)对测试函数(p)在FE和SR方面的综合性能最好,且ra,p越接近于1,则表明对应算法的性能越接近于最优算法.在此实验中,ACUP、DEUS和DELLU算法均采用相同的参数设置:F=0.5,CR=0.5,NP=50,算法终止条件均为δ=1E-04.表7给出了ACUP、DEUS和DELLU算法对各测试函数的函数评价次数FE、成功率SR和性能DEUSPage16比PR,其中性能比为1的算法,即性能最优的算法通过加粗标出.可以看出,ACUP算法仅在2个低维多模问题(f16和f19)上获得了最优性能,DEUS在3个问题上获得了最优性能,其中包括1个高维单模问题(f3)和2个高维多模问题(f12和f14),而DELLU在其余15个问题上获得了最优性能.另外,从表中最后一行的总平均值可以看出,虽然3种比较算法的成功率相差无几,仅在对函数f7求解时有所区别,其他函数的成功率均为100%,但是由于DELLU算法所使用的Lipschitz下界估计支撑函数能够得到更精确的目标函数下界估计值,且对选择策略进行了改进,因此,DELLU所需的函数评价次数图8ACUP、DEUS和DELLU算法对部分函数的平均收敛曲线4.5CEC2013偏移和旋转问题测试为了验证DELLU算法求解复杂问题的性能,应用CEC2013的偏移和旋转(ShiftedandRotated)问题进行测试,表8列出了所用测试问题的基本参数,具体数学表达式见文献[36].上述15个测试函数中,F1~F5为单模函数,F6~F15为多模函数.实验中,DELLU算法与以下3种先进的非DE算法进行比较:最少,总平均值为2.40E+04,ACUP和DEUS算法分别为2.86E+04和2.59E+04,也就是说,DELLU算法相对于ACUP和DEUS算法分别节省了16.1%和7.3%的函数评价次数.图8给出了ACUP、DEUS和DELLU算法对2个单模函数(f2和f6)和2个多模函数(f8和f9)求解时的平均收敛曲线图.从图中可以看出,由于DELLU算法根据提出的广义下降方向做局部增强比DEUS算法中仅根据支撑面的下降方向获取下界估计区域的极值解作局部增强更合理,因此DELLU算法对于上述4个函数的收敛速度总是优于ACUP算法和DEUS算法.(1)Liang等人[37]提出的综合学习粒子群算法(ComprehensiveLearningParticleSwarmOptimizer,CLPSO).该算法中,每个粒子均利用其他所有粒子的个人历史最优信息来更新速度;(2)Hansen等人[38]提出的基于协方差矩阵的自适应进化策略算法(EvolutionStrategywithCovarianceMatrixAdaptation,CMA-ES).该算法通过一种多元正态分布来枚举新的候选解,并利用协方差矩阵更新正态分布;Page17F1:SphereFunctionF2:RotatedHighConditionedEllipticFunction-1300F3:RotatedBentCigarFunctionF4:RotatedDiscusFunctionF5:DifferentPowersFunctionF6:RotatedRosenbrock’sFunctionF7:RotatedSchaffersF7FunctionF8:RotatedAckley’sFunctionF9:RotatedWeierstrassFunctionF10:RotatedGriewank’sFunctionF11:Rastrigin’sFunctionF12:RotatedRastrigin’sFunctionF13:Non-ContinuousRotatedRastrigin’sFunction-200F14:Schwefel’sFunctionF15:RotatedSchwefel’sFunction(3)Garcia-Martinez等人[39]提出的基于父代中心交叉操作的遗传算法(GeneticAlgorithmBasedonParent-centricCrossoverOperators,GL-25).该算法利用父代中心实参交叉操作来提高算法的全局和局部搜索能力.CLPSO、CMA-ES和GL-25算法均采用原文献中的参数设置.实验中,通过记录最大函数平次数MaxFE内30次独立运行所得的最小函数误差值表9CLPSO、CMA-ES、GL-25和DELLU算法函数误差的平均值和标准偏差函数维数F1100.00E+00±0.00E+00≈0.00E+00±0.00E+00≈0.00E+00±0.00E+00≈0.00E+00±0.00E+00F2101.06E+06±5.92E+05+0.00E+00±0.00E+00-4.27E+04±4.26E+04+5.52E-12±4.13E-12F3101.54E+06±3.01E+06+0.00E+00±0.00E+00-8.92E+01±1.95E+02+1.21E-02±1.54E-02F4106.22E+03±1.22E+03+0.00E+00±0.00E+00-1.12E+03±7.58E+02+6.35E-08±2.11E-07F5100.00E+00±0.00E+00≈1.14E-13±1.31E-13+5.68E-14±5.99E-14+0.00E+00±0.00E+00F6103.01E+00±3.65E+00+7.85E+00±4.14E+00+7.92E+00±3.98E+00+2.32E+00±3.13E+00F7101.26E+01±8.03E+00+2.87E+01±1.88E+01+4.87E-01±3.91E-01+2.76E-04±2.05E-04F8102.03E+01±8.12E-02+2.03E+01±8.11E-02+2.04E+01±7.08E-02+2.02E+01±1.07E-01F9103.91E+00±6.50E-01+1.45E+01±5.16E+00+2.97E+00±1.36E+00+2.59E+00±2.05E+00F10107.28E-01±2.42E-01+1.48E-02±1.65E-02-1.69E-01±9.27E-02+3.63E-02±1.75E-02F11100.00E+00±0.00E+00≈2.04E+02±2.27E+02+3.50E+00±1.72E+00+0.00E+00±0.00E+00F12101.14E+01±3.94E+00+3.25E+02±2.45E+02+1.09E+01±9.76E+00+6.45E+00±2.14E+00F13101.56E+01±4.49E+00+1.66E+02±2.85E+02+1.44E+01±7.78E+00+6.07E+00±2.02E+00F14106.64E-02±6.33E-02+1.69E+03±4.48E+02+2.02E+02±1.17E+02+1.56E-02±2.84E-02F15107.74E+02±1.52E+02+1.74E+03±3.16E+02+1.19E+03±1.89E+02+4.12E+02±1.36E+02+/≈/-4.6参数选择分析在DELLU算法中,当新个体生成后,为了获取其下界估计信息来指导种群进化,需要选择新个体的N个邻近个体建立Lipschitz下界估计支撑面,因此,需要对参数N的选择进行分析.实验中,首先对参数N设置不同的值,即N=2,3,4,5;然后通过记录对函数f1~f15求解所需的函数评价次数和成功率来分析参数N对算法性能的影响,算法终止(f(x)-f(xo))的平均值和标准偏差来比较分析4种算法的性能,其中xo为测试函数的全局最优解,所有算法终止条件均为MaxFE=10000×n.表9给出了CLPSO、CMA-ES、GL-25和DELLU算法对CEC2013偏移和旋转测试函数优化得到的结果,其中,最优结果通过加粗标出.可以看出,4种算法在对函数F1优化时均达到了最优,因此得到了相同的结果.DELLU算法虽然仅在2个单模问题上达到了最优,但是对于多模问题,除了函数F10外,DELLU算法对于其余9个多模函数均优于CLPSO、CMA-ES和GL-25算法.其次,从表中最后一行的显著性检验对比结果可以看出,CLPSO算法没有优于DELLU算法任何问题,而DELLU算法优于CLPSO算法12个问题,对于其余2个问题,两者之间没有显著性差异;与CMA-ES算法相比,虽然CMA-ES算法在4个问题上优于DELLU算法,其中包括3个单模函数(F2~F4)和1个多模函数(F10),但是DELLU算法在其余10个问题上优于CMA-ES算法;GL-25算法同样没有优于DELLU算法任何问题,而DELLU算法优于GL-25算法14个问题.条件为函数误差值精度δ=1E-04.表10给出了DELLU算法在参数N不同设置下对于各测试函数的函数评价次数FE和成功率SR.从表中最后一行可以看出,在参数N设置不同的情况下,DELLU算法可以对所有函数成功求解,且所需函数评价次数相差无几.此外,为了进一步分析参数N的设置对不同问题结果的影响,分别对单模和多模函数的平均函数评价次数进行分析.图9Page18给出了单模函数和多模函数在N不同取值下的函数评价次数箱型对比图,可以看出,无论是单模函数还是多模函数,在N取值不同的情况下,所需函数评价次数几乎没有差异.由此可以推断出,DELLU算法的性能对参数N的选择并不敏感.然而,随着表10DELLU算法中参数犖不同设置下的平均函数评价次数和成功率函数维数f1209.45E+031009.42E+031009.32E+031009.51E+03100f2202.03E+041002.16E+041002.15E+041002.03E+04100f3201.39E+041001.39E+041001.33E+041001.40E+04100f4205.69E+041005.73E+041005.65E+041005.62E+04100f5202.23E+031002.31E+031002.21E+031002.33E+03100f6204.51E+041004.53E+041004.45E+041004.57E+04100f7202.43E+041002.53E+041002.51E+041002.55E+04100f8201.29E+041001.26E+041001.32E+041001.28E+04100f9205.52E+041005.67E+041005.44E+041005.51E+04100f10202.21E+041002.18E+041002.12E+041002.28E+04100f11201.02E+041001.15E+041001.12E+041001.09E+04100f12201.65E+041001.73E+041001.70E+041001.74E+04100f13206.93E+031006.81E+031006.82E+031006.71E+03100f14208.37E+031008.50E+031008.45E+031008.31E+03100f15209.76E+031009.52E+031009.91E+031009.81E+03100总平均值2.09E+041002.13E+041002.10E+041002.12E+04100图9参数N不同设置下的函数评价次数箱型对比5总结本文提出了一种基于局部Lipschitz下界估计支撑面的差分进化算法来减少智能优化算法的函数评价次数.所提算法通过提取新个体的N邻近个体构建基于Lipschitz估计理论的下界估计支撑面,从而利用下界估计支撑面获取信息设计3种策略来指导种群进化:局部Lipschitz估计选择策略,无效区域排除策略和广义下降方向局部增强策略.局部Lipschitz估计选择策略根据下界支撑面获取新个体的下界估计值设计选择策略指导种群更新;无效区域排除策略利用Lipschitz下界估计分段线性N的增大,即需要建立Lipschitz下界估计支撑面的个体的数目增多,算法的空间复杂度也会相应的变大,因此,文中只对与新个体邻近的2(即N=2)个种群个体构建下界估计支撑面,且在每次更新环节完成后删除所有下界估计支撑面.特性,根据下界估计区域的极值信息排除部分无效区域,逐步缩小搜索区域;广义下降方向局部增强策略则结合邻近个体下降方向和主导支撑面下降方向的组合方向来指导算法作局部增强,提高算法的局部搜索能力.数值实验结果表明,与传统DE算法、基于估计的改进DE算法、基于自适应机制的改进DE算法及一些先进的非DE算法相比,DELLU算法能够对大部分测试函数以较少的目标函数评价次数求得高质量的解.另外,所提出的3种策略不仅限于DE算法,而且可以应用到其他智能优化算法中.下一步的工作将集中在局部Lipschitz下界估计智能优化算法在实际优化问题中的应用.Page19致谢感谢中南大学王勇教授在文献[34]中提供的CoDE、SaDE和EPSDE算法代码,同时感谢对本文工作给予支持和建议的所有评审专家和同行!
