Page1基于一般二元关系的粗糙集加权不确定性度量滕书华鲁敏杨阿锋张军庄钊文(国防科学技术大学电子科学与工程学院长沙410073)摘要不确定性度量是粗集理论的研究热点.考虑到实际数据中样本重要性的不同,在一般二元关系下构建一种带有可调参数的加权不确定性度量———α熵,证明了现有的多种不确定性度量是α熵的特例,进而对完备和不完备信息系统中知识的不确定性度量进行了统一.在此基础上基于一般二元关系提出了一种加权不确定性度量———α精度和α粗糙度,证明了α精度和α粗糙度的单调性;理论分析和实例表明α精度和α粗糙度比现有的不确定性度量更精确,更符合人们的认识规律.最后,在一般二元关系下利用α精度设计了一种加权属性约简算法,实验结果表明文中的变参数加权不确定性度量方便地融入了主观偏好和先验知识,通过改变参数α构造的组合分类器有效地提高了约简结果的分类精度.这些结论发展了基于粗糙集的不确定测度理论,提高了方法的普适性和可解释性,为一般二元关系下的信息系统知识获取提供了理论依据.关键词粗糙集;熵;不确定性;一般二元关系;加权度量1引言不确定性度量是人工智能的研究热点和重大前沿课题[1].粗糙集理论[2],作为一种新的处理不精确、不相容和不完全数据的数学工具,是处理不确定性问题的有效方法.针对不确定性度量问题,粗糙集理论中产生了两种观点:代数观点和信息观点[3-14].相比于代数观点,信息观点在更深层次揭示了知识的本质.但对于实际问题,在知识获取时通常面临的是不完备信息系统,而经典粗糙集理论以等价关系为基础,不能对不完备信息系统进行直接处理,使得粗糙集理论的实用化受到很大限制.据此学者们提出了多种可以直接处理不完备信息系统的扩展粗糙集模型,即将等价关系放宽为容差关系[15]、限制容差关系[16]、非对称相似关系[17],甚至放宽为一般二元关系[18-19],以此为基础直接对粗糙集扩展模型的不确定性度量引起了广泛关注[6,20-22].文献[23]讨论了基于等价关系和容差关系的多种不确定性度量的共性,但并没有指出各种不确定性度量的差异.文献[24]提出了粗糙集不确定性度量的基本准则和扩展准则,为设计新的度量方法提供了依据.文献[25]基于可调节参数提出了一种灵活的粗糙集不确定性度量,但其仅针对等价关系和容差关系进行分析,缺乏对可调参数的解释;文中提出的粗糙测度随着粗糙集合X的正域或负域中的知识颗粒(与X无关)的粒度变化而改变,不符合认知规律[8].以上提出的各种不确定性度量大部分是针对特定二元关系,没有普适性,并且在有些情况下不能给出正确的度量;特别是在实际应用中,数据中的样本并不一定同等重要,如在类别不平衡学习和代价敏感学习[26-28]中样本的重要性通常是不同的,而现有的不确定性度量却将论域中对象视为同等重要.不同数据通常具有不同的先验知识,并且决策者的偏好也不相同,从而使得样本往往具有不同的重要度.因而研究适用性强、精度高的加权不确定性度量具有非常重要的理论价值和应用价值[29-32].本文首先将样本的先验信息以样本权值的形式引入到粗糙集的不确定性度量中,使与待处理问题相关的先验信息能够方便融入到信息系统的不确定性描述中;在此基础上,借鉴熵的思想提出了一种新的加权不确定性度量———α熵,通过调节可变参数α讨论了现有的多种不确定性度量的关系,指出已有的多种不确定性测度是一般二元关系下α熵的特例.此外,在分析现有不确定性度量不足基础上,构造了两种新的集成加权不确定性度量:α精度和α粗糙度,通过实例说明本文提出的加权不确定性度量方法不仅解决了现有不确定度量的缺点,而且更加符合人类的认知规律.最后构造了一种变参数的约简算法,通过心电数据和UCI数据的处理验证了本文方法的有效性.2基本概念2.1粗糙集的相关概念如下相关定义:完备信息系统S=(U,A)中,对于Q,PA,有(1)知识P的不可区分关系IND(P),U/IND(P)构成了U的一个划分,简记为U/P={[ui]P|ui∈U}.知识P生成的等价类[ui]P称为知识粒度.XU,X在知识P下的上近似珚PX={ui∈U|[ui]P∩X≠}、下近似PX={ui∈U|[ui]PX}.BNP(X)=珚PX-PX表示X在知识P下的边界域.(2)考虑到现实中存在的对属性值排序的问题,Greco等人[33]提出了基于优势关系的粗糙集研究方法,给出的优势关系RPD={(ui,uj)∈U×U|a∈P,f(ui,a)f(uj,a)}.RP利用优势关系替代等价关系后得到的优势信息粒度表示为也称RPsets).对XU,X在知识P下的上、下近似为不完备信息系统S=(U,A),PA,有以下定义:(3)文献[15]指出,空值“”在信息系统中是确实存在的,只是被遗漏,据此给出容差关系RPT={(ui,uj)∈U×U|a∈P,RPf(ui,a)=f(uj,a)∨f(ui,a)=∨f(uj,a)=}.在知识P下与ui可能不可区分的对象的最大容差类为RP容差关系下的信息粒度.U/RP示容差关系下的分类.对XU,X在知识P下的上近似为RP似为RP(4)文献[16]在不完备信息系统中提出了一种非对称相似关系RPPage3NS={(ui,uj)∈U×U|a∈P,RP根据以上思想定义了两个非对称相似集合NS(ui)(ui与之非对称相似,或称ui的相似类)和RPNS(ui)(非对称相似于ui):RP-1NS(ui)={uj|ui∈U∧(ui,uj)∈RPRPNS(ui)={uj|uj∈U∧(uj,ui)∈RPRP-1集合XU关于知识P的上近似和下近似为[34]NS(X)={ui∈U|RPRPNS(X)={ui|ui∈U∧RPRP(5)鉴于相似关系太严格,而容差关系又太宽松,据此,王国胤教授提出了限制容差关系RPL={(ui,uj)∈U×U|a∈P,RP其中,BP(ui)={a∈P|f(ui,a)≠}.相应定义了限制容差类RP似和下近似:L(ui)={ui|ui∈U∧(ui,uj)∈RPRPL(X)={ui|ui∈U∧RPRPL(X)={ui|ui∈U∧RPRP在实际应用中,信息系统中属性导出的二元关系一般不是经典的等价关系,而是一般二元关系.本文中用RP表示由知识P导出的一般二元关系.显然IND(P),RP在信息系统S=(U,A)中,PA,RP为论域U上的一般二元关系,有以下定义.(6)定义集值函数表达式为RP系RP下ui的后继近邻RPRP}.关系RP与其对应的后继近邻RP唯一确定,即uiRPujuj∈RP的分类表示为U/RP={RPS(ui)也可理解为在知识P下与对象ui具有相同RP性质的对象集合,就是RP相对于ui是不可区分的,应属于同一类.一般二元关系对论域的分类U/RP中的元素不必是其划分或覆盖[35].本文中用RP息粒度.集合X在一般二元关系下的上近似集和下近似集定义为(7)Q,PA,PQ,表示ui∈U,RPS(ui),这意味着Q比P粗糙,或称知识Q依赖于RQ知识P.若PQ,则表示ui∈U,RP且uj∈U使得RP细,或Q完全依赖于知识P.若P≈Q,则表示对ui∈U,有RP2.2现有信息系统中典型的不确定性度量在完备信息系统S=(U,A)中,Q,PA,令U/P={P1,P2,…,Pm},U/Q={Q1,Q2,…,Qn}.通过引入香农熵来度量知识的不确定性如下(便于统一,下文中熵度量公式中的对数全部取以2为底).定义1[36].知识P对应的H信息熵HH知识Q关于知识P的H条件熵HHE(Q|P)=-∑mHH知识Q和知识P的H互信息IH其中上标H表示H信息测度,下标E表示等价关系.此外,在完备信息系统中还提出了一种具有补的性质的、可以度量粗糙集模糊性的信息熵.定义2[6].知识P对应的E信息熵HE知识Q关于知识P的E条件熵HEHE知识Q和知识P的E互信息IEIE其中Qc用下标E来代表等价关系,上标E代表E信息测度.以上给出了基于等价关系的不确定性度量的定义,但在实际应用中,经常遇到基于优势关系的信息系统,文献[9,11]给出了优势关系下的知识不确定性度量.在优势信息系统S=(U,A)中,知识PA.优势关系RPD(u|U|)}表示知识P的分类,相关定义如下.RP定义3[11].知识P的优势H信息熵HHPage4知识Q关于知识P的优势H条件熵HHD(Q|P)=-1HH知识Q和知识P的优势H互信息IH定义4[9].知识P的优势E信息熵HE实际应用中的信息系统往往是不完备的,由于完备信息系统中的不确定性度量方法并不适用于不完备信息系统,进而人们针对不完备信息系统提出了多种不确定性度量:不完备信息系统S=(U,A)中,知识PA,容差关系下分类U/RPT(u|U|)},则有定义5.RP定义5[6].知识P的H粒度度量GH若S=(U,A)是完备的,在等价关系下H粒度度量T(P)退化为HHGH定义6[6].知识P的E信息熵HE若S=(U,A)是完备的,等价关系下E信息熵HE退化为HE在完备和不完备信息系统中已定义了多种不确定性度量,文献[23]指出了现有多种不确定性度量的共性,但并没有涉及不同度量之间的差别.针对同一个信息系统,不同的不确定性测度所得结果也将不同,因此有必要对现有不确定性度量的异同点作进一步分析.3一般二元关系下知识不确定性的度量等价关系是经典粗糙集理论基础,其对论域的划分形成知识,划分越粗糙(粒度越大),知识越不精确.而在粗糙集扩展模型中,由相似关系、容差关系、限制容差关系等形成的相似类之间可能存在交叠,因而对论域形成覆盖而不再是划分,等价关系将不再成立.在一般二元关系下也存在同样问题,而基于一般二元关系的不确定性度量方法鲜有研究[20].下面首先给出一般二元关系下知识在单个对象上的不在一般二元关系RP下,RP确定性度量,在此基础上考虑对象重要性的不同,提出知识的整体加权不确定性度量.3.1一般二元关系下的α粒度熵满足(ui,uj)∈RP,它们具有相同的性质.因此在知识P下uj∈RP分.若RP与ui属于同一类的对象就越多,即知识粒度越大,此时在对象ui上知识P表现出来的分类能力越弱.据此给出单个对象ui∈U上属性集的不确定性度量如下.定义7.知识PA,在对象ui(1i|U|)上的不确定性度量———α粒度熵定义为Hα(P,ui)=烄(1-2α-1)-1×|RP烅-log2烆S(ui)||U|<1式中,α∈R(R表示实数集);如果RP0<|RP即当RP|U|)1-α-1].显然不同的α值可得到不同的度量函数.定理1.PA,ui∈U,则Hα(P,ui)是α的连定理2.P,QA,PQ,则对任意ui∈U,续函数.Hα(Q,ui)Hα(P,ui).定理3.ui∈U,PA.当α<2时Hα(P,ui)在0<|RP时Hα(P,ui)在0<|RP函数.推论1.ui∈U,PA,当且仅当RP时Hα(P,ui)取最大值(1-2α-1)-1×(101-α×|U|1-α-1);当且仅当对任意ui∈U且RP时Hα(P,ui)取最小值0.对于Hα(P,ui),调整α值可得到不同的不确定性度量.令Hα(P,x)=(1-2α-1)-1(xα-1-1),0<x1,则当x=|RP图1给出了参数α与函数Hα(P,x)的关系图.定理3、推论1以及图1表明,Hα(P,ui)随着知识粒度的Page5减小而单调递增.对于ui∈U,在知识P下如果ui和uj∈U都满足uiRPuj,即RPHα(P,ui)最小,此时知识P在对象ui上的分类能力最弱,具有最大粗糙性;如果ui和uj∈U都不满图1不同的α值对应的函数Hα(P,x)定理4.ui∈U,PA.α,α∈R.如果α<α,则有Hα(P,ui)Hα(P,ui),当且仅当RP时等号成立,此时Hα(P,ui)=Hα(P,ui)=0.全距R=max(xi)-min(xi)在统计理论中用来描述数据xi的离散度.由定理4可以看出,若0<S(ui)||U|,则对α∈R,有min(Hα(P,ui))=0.|RP因此全距R由max(Hα(P,ui))确定.随着α的不断减小,α粒度熵的全距逐渐变大,Hα(P,ui)的离散度也随之变大(图1中相应的表现为Hα(P,ui)随着α的减小迅速增大,但Hα(P,ui)的最小值不变).3.2一般二元关系下的α熵及其性质经典粗糙集理论把每个样本重要性视为相同,但在实际应用中并不一定.数据的先验知识以及决策者偏好或主观判断的不同,使得数据对象的重要性也不同[29].因此将对象重要性融入到不确定性度量中具有重要意义.本节在α粒度熵基础上给出知识在整个论域中的加权不确定性度量.足uiRPuj,即RP知识P具有最小粗糙性(如果ui∈U,RP则当|RPHα(P,ui)从单个粒度上描述了知识P的粗糙程度.定义8.ui∈U,1i|U|.PA的α熵为权系数wi(0wi1,假设对象重要性不完全相同)表示对象ui在论域中的重要度,实际应用中可利用用户先验知识来给出.由α熵定义可以看出,α熵利用加权的方法从整体上对知识P的分类能力进行了描述,是在整个论域上对知识P不确定性的度量.定理5.知识PA,ui∈U,1i|U|,如果w1=w2=…=w|U|,则有Hα(P)=1由定理5可知,若w1=w2=…=w|U|,即论域中不考虑对象的权值,则α熵变成α粒度熵的简单算术平均.由式(15)可知,α熵将领域的先验知识和Page6决策者的主观偏好等通过引入权值方式融入到了不确定性测度之中.由图1和式(14)可以看出,不同α值对应的不确定性度量值不同.下面给出不同α值和现有的各种不确定性测度之间的关系.引理1[37].设a1,a2,…,an>0且∑nf(x)是[a,b]上的严格下凸函数,则对于x1,x2,…,xn∈[a,b]有f(a1x1+a2x2+…+anxn)a1f(x1)+a2f(x2)+…+anf(xn),当且仅当x1=x2=…=xn时等号成立.若f(x)是严格上凸函数,则不等式反向.定理6.PA,α,α∈R,α<α.则Hα(P)Hα(P).i=1定理7.令|RP∑Uwi,其中ui∈U,1i|U|,PA.则(1)当α>2时,Hα(P)Hα(P,u),当且仅当S(u1)=RPRP(2)当α<2时,Hα(P,u)Hα(P),当且仅当S(u1)=RPRP(3)当α=2时,Hα(P,u)=Hα(P).定理7的证明由引理1和定义8可得.定理7S(u)|表示知识P的加权平均信息粒度,中|RPHα(P,u)表示加权平均信息粒度下知识P的不确定性.下文用|U/RP|=[|RPS(u|U|)|]来表示论域中知识P的分类粒度分布.|RP定理7表明,当知识P在每个对象上的分类粒度加权和相等时(即∑U(α<2)条件下,知识P分类粒度分布的越均匀,其α熵变得越大(越小),当且仅当在每个对象上知识P的分类粒度大小相等时,α熵取到最大(小)值.而当α=2时α熵与知识P分类粒度分布的均匀性无关,Hα(P)=Hα(P,u)恒成立.假设论域中对象权值都相等且ui∈U,|RP坐标是平均信息粒度|RP|U|2,虚线的纵坐标是相应的α粒度熵.当2<α时,如图1中的(a)、(b)所示,Hα(P,x)的全距非常小,较大信息粒度的Hα(P,x)之间很容易区分,而较小信息粒度的Hα(P,x)之间则很难分辨,Hα(P)位于虚线右侧,也即Hα(P)Hα(P,u),此时较大信息粒度的Hα(P,x)将决定Hα(P)偏离Hα(P,u)的远近;随着α的减小,Hα(P,x)的全距变大,Hα(P)不断向Hα(P,u)靠拢,较小信息粒度的Hα(P,x)之间的区分度逐渐变大,较大信息粒度的Hα(P,x)之间的区分度逐渐变小.当α=2时(如图1中(c)所示):Hα(P,x)的全距等于1,不同信息粒度的Hα(P,x)之间区分度一样,知识P的整体不确定性与其在平均信息粒度下的不确定性相同.当α<2时(如图1中(d)~(i)所示):Hα(P,x)有较大全距,较大信息粒度的Hα(P,x)之间区分度比较小,较小信息粒度的Hα(P,x)之间区分度比较大,此时Hα(P)在虚线左侧,即Hα(P,u)Hα(P),较小信息粒度的Hα(P,x)将决定Hα(P)偏离Hα(P,u)的远近;随着α值不断减小,Hα(P,x)的全距不断变大,Hα(P)不断远离Hα(P,u),较大信息粒度的Hα(P,x)之间区分度继续变小,较小信息粒度的Hα(P,x)之间区分度继续变大,在计算Hα(P)时较小信息粒度的Hα(P,x)将占支配地位.例1.信息系统S=(U,A)中,U={u1,u2,…,u9},A={P1,P2,…,P5},w1=w2=…=w|U|=1.知识P1,P2,…,P5的分类粒度分布分别为显然知识P1,P2,…,P5对应的平均信息粒度值都一样,即|RP1S(u)|=|RP2S(u)|=…=|RP5S(u)|=5,但知识P1,P2,…,P5的分类粒度分布变得越来越不均匀.根据定义8可得在不同α值下对应的α熵,如表1所示,可以看出:(1)知识的α熵随着α值的减小而变大.(2)当α=2时,α熵不受知识分类粒度分布的影响,知识P1,P2,…,P5对应的α熵相等;当α>2时,知识的粒度分布越均匀,α熵变得越大,也即Hα(P1)>Hα(P2)>…>Hα(P5);当α<2时,知识的粒度分布越均匀,α熵变得越小,即Hα(P1)<Hα(P2)<…<Hα(P5),与α>2时的不确定性大小排序相反.(3)不同α值对应的α熵在知识由P1变为P2时(α≠2)变化幅度最明显,在知识P4变为P5时变化幅度不明显,这可以由函数Hα(P,ui)的性质看出.Page7当把一个较大粒度9和较小粒度1引入知识P1的分类粒度中时(即知识P1变为P2),在α>2时,最大信息粒度9决定了Hα(P)偏离Hα(P,u)较远,图1中Hα(P)在虚线右边,因而α熵减小的明显;表1知识犘1,犘2,…,犘5在不同α值下对应的α熵1.6当且仅当P≈Q时取等.Hα(Pk)\α10Hα(P1)19.47×10-46.03×10-24.44×10-15.76×10-18.48×10-11.171.602.995.52Hα(P2)17.31×10-45.43×10-24.44×10-16.06×10-110.12×10-11.673.0214.1796.74Hα(P3)16.59×10-45.11×10-24.44×10-16.21×10-110.83×10-11.863.4716.40107.01Hα(P4)16.38×10-44.97×10-24.44×10-16.27×10-111.11×10-11.923.6217.02109.23Hα(P5)16.35×10-44.94×10-24.44×10-16.28×10-111.18×10-11.943.6517.14109.62定理8.P,QA,PQ,则Hα(Q)Hα(P),差关系)信息系统都适用.α熵也可以用于其它粗糙集扩展模型的不确定性信息处理(参看第5节的相似关系或限制容差关系).因此一般二元关系下的α熵是现有多种不确定性度量在对象权重不同的信息系统中的扩展.定理11.ui∈U,1i|U|,假设w1=w2=…=w|U|,P,QA,P和Q对应的平均信息粒度值相同,即|RPE(P)=HEHE由定义8和定理2可得定理8.推论2.PA,当且仅当ui∈U,RP时Hα(P)取最小值0;当且仅当ui∈U,RPHα(P)取最大值(1-2α-1)-1×(101-α×|U|1-α-1).定理8和推论2表明,α熵具有单调性,即α熵随着分类粒度减小而单调增加.如果对ui,uj∈U,i≠j,都能被知识P区分,则α熵取得极大值,此时知识P区分能力是最强的,不确定性最小;如果对ui,uj∈U,i≠j,都不能被知识P区分,那么α熵取得极小值,此时知识P区分能力最小,粗糙性则最大.因此α熵从粒度角度刻画了知识的整体粗糙程度.熵之间的关系.下面分析现有的几种典型的不确定性度量与α定理9.PA,ui∈U,1i|U|.若w1=w2=…=w|U|,当α=1时有如下结论:(1)在等价关系下,Hα=1(P)=HH(2)在优势关系下,Hα=1(P)=HH(3)在容差关系下,Hα=1(P)=GH定理10.PA,ui∈U,1i|U|.若w1=w2=…=w|U|,当α=2时,有以下结论:(1)在等价关系下,则Hα=2(P)=HE(2)在优势关系下,则Hα=2(P)=HE(3)在容差关系下,则Hα=2(P)=HE由定理9和定理10可知,现有的几种典型的不确定性度量(等价关系、优势关系和容差关系下)是在w1=w2=…=w|U|时α熵的几种特殊形式.因此α熵对于完备(等价关系及优势关系)和不完备(容当α<2时,最小信息粒度1决定了Hα(P)偏离Hα(P,u)较远,图1中Hα(P)在虚线的左侧,使得α熵变大的明显.而当把两个近似平均粒度引入知识分类粒度中时(当知识P4变为P5),则α熵变化不显著.(1)如果RP和RQ为等价关系,则有(2)如果RP和RQ为优势关系,则有(3)如果RP和RQ为容差关系,则有D(P)=HEHET(P)=HEHE由定理7、定理9和定理10即可得定理11.定理11表明,如果两个知识P和Q的平均信息粒度相同,则它们的E信息测度(HE相等,但都小于相应的H信息测度(HHD(P),GHHH知识P和Q下并不一定相同,这是由于H信息测度将知识粒度分布的均匀性大小也包括在其中(参看表1).显然H信息测度相比E信息测度包含的信息更丰富.3.3一般二元关系下知识间的不确定性度量本节在3.2节单个属性不确定性度量基础上,讨论信息系统中多个属性之间的不确定性度量.定义9.ui∈U,1i|U|,P,QA.知识P和Q的α联合熵定义为Hα(P∪Q)=∑U式中Page8Hα(P∪Q,ui)=(1-2α-1)-1×|RP烄烅-log2烆α∈R;wi为对象权重.若RPHα(P∪Q,uk)=(1-2α-1)-1[(10×|U|)1-α-1].知识P和Q的α联合熵表示P和Q联合作用的分类能力,它度量了知识P和Q作为一个整体的不确定性.定理12.P,Q,DA,如果PQ,则有Hα(D∪P)Hα(D∪Q).定义10.ui∈U,1i|U|,P,QA.知识Q相对于知识P的α条件熵定义为Hα(Q|P)=∑Uα条件熵可理解为在知识P中加入知识Q后增加的分类能力.显然Hα(Q|P)=Hα(Q∪P)-Hα(P).定理13.P,DA,PDHα(D|P)=0.定理13表明若Hα(Q|P)=0,则知识Q依赖于知识P.所以α条件熵可用于度量不同对象权重信息系统中的知识间的依赖关系.定义11.知识P,QA,两者间α互信息为α互信息表示两类知识共同具有的分类能力.若PQ,即知识Q依赖于P,则Hα(Q;P)=Hα(Q).定理14.ui∈U,1i|U|,P,QA.如果w1=w2=…=w|U|,且一般二元关系RP和RQ变为等价关系,则有定理15.ui∈U,1i|U|,P,QA.如果w1=w2=…=w|U|,且RP和RQ变为优势关系,则当α=1时,Hα=1(Q|P)=HHD(Q;P).IH由式(8)、(9)以及定理14中(1)可得定理15.定理14和定理15表明,本文中的α条件熵和α互信息是现有的多知识间不确定性度量在一般二元(1)Hα=1(Q|P)=HH(2)Hα=2(Q|P)=HEE(Q;P);IHE(Q;P).IE关系下的进一步扩展,即经典香农条件熵和互信息[36],具有补的性质的条件熵与互信息[6]及基于优势关系的条件熵和互信息[11]都是α条件熵和α互信息的特例.但需要指出,现有的条件熵和互信息并不能处理扩展粗糙集模型,而本文中的α联合熵,α条件熵和α互信息既适用于各类粗糙集扩展模型,还能处理具有不同权重对象的信息系统.4一般二元关系下集成不确定性度量经典粗糙集理论中的不确定性产生的原因[2]包括两方面:一方面是由信息粒度带来的知识不确定性;另一方面是由粗糙集边界带来的集合不确定性.针对这两个原因许多学者在特定的二元关系下提出了一些集成不确定性度量[6,9-10,25,38],既考虑了集合的不确定性,又考虑了知识的不确定性.但是,由于这些度量随着与集合XU负域中的颗粒的变化而改变,因而与人们对现实中不确定性问题的认知不相符[8].直觉上,考虑多种不确定性后的度量值(粗糙集的粗糙度)应大于单独考虑某一种不确定性的度量值,但现有集成不确定性度量与此相反,与人的直觉不符.特别是目前的集成不确定性度量在有些应用中给出的结果并不准确.文献[8]基于等价关系提出了一种粗糙集不确定性的模糊度度量方法,在一定程度上弥补了现有不确定性度量的不足,但此方法并不能处理粗糙集扩展模型,且没考虑论域中对象的不同权重.为解决以上集成不确定性度量的限制,本节在分析现有不确定性度量基础上,基于一般二元关系提出一种新的加权集成不确定性度量.定义12[39].PA,XU,X在一般二元关系RP下的精度和粗糙度分别为αP(X)=|RP(X)|显然0αP(X),ρP(X)1.αP(X)越大,ρP(X)则越小,粗糙集不确定性越小.从定义12可知,精度和粗糙度由上近似和下近似来确定,即它们仅考虑了集合的不确定性,并不能度量论域中知识的不确定性.据此文献[10,38]基于容差关系对粗糙度和精度给出了新的定义:粗糙度为RoughnessP(X)=ρP(X)×GK(P),相应的,精度定义为AccuracyP(X)=1-RoughnessP(X).定义13[38].PA,XU在容差关系RP定义13中GK(P)=∑UPage9量信息系统中知识的不确定性,ρP(X)用来度量集合的不确定性.显然Roughness同时考虑了集合的不确定性和知识的不确定性,在一定程度上弥补了粗糙度的缺陷.然而当集合X负域中的知识颗粒被细分时(知识粒度减小),ρP(X)不会改变,符合认知规律;但Roughness却严格递减,不符合认知规律.此外,考虑两种不确定性的Roughness,反而比考虑一种不确定性的ρP(X)更小,显然也不符合认知规律,并且Roughness在某些情况下并不能正确的反映知识和集合的不确定性,举例如下:例2.信息系统S=(U,A),PA,U={1,2,…,7},U/RA{5,6},{6,5,7},{7,6}},U/RP{3,4,5},{4,3},{5,3,6},{6,5,7},{7,6}}.(1)若X={1,2,7},则有RP{1,2},RPρA(X)=0.50;GK(P)=0.35,GK(A)=0.31;RoughnessP(X)=0.17,RoughnessA(X)=0.15.由于知识P和知识A对于集合X的上、下近似相同,且对上近似{1,2,6,7}中的对象的分类粒度也相同,因此粗糙度没有改变(符合认知规律),而Roughness却减少了,这是因为:与X无关的RP于集合X的负域,它们被知识A细分进而导致以上结果,因此不符合认知规律[8].(2)如果X={2,3},则有RPT(X)={1,2,3,4,5},RARPρP(X)=ρA(X)=1;GK(P)=0.35,GK(A)=0.31;RoughnessP(X)=0.35,RoughnessA(X)=0.31.在知识P和A下X的下近似为空,且RP显然BNP就越粗糙[40].但是,RoughnessP(X)=GK(P),RoughnessA(X)=GK(P);ρP(X)=ρA(X).即ρP(X)对粗糙集X的不确定性度量完全失效;而Roughness则只度量了知识的不确定性,并没有准确度量集合的不确定性.(3)在(1)和(2)中有RoughnessP(X)<ρP(X),RoughnessA(X)<ρA(X),即当集成不确定性度量考虑两种不确定性时,反而比仅考虑一种不确定性时的值还要小,显然与人们的认知不符.例2表明,现有的集成不确定性度量还存在一些问题,有时的度量结果并不合理.此外,序信息系统中的集成不确定性度量[9]由于具有Accuracy和Roughness相似的性质,因此也存在上述缺陷.特别是现有集成不确定性度量[8-14,25,38]都没有考虑论域中对象的重要性.为此有必要在一般二元关系下寻求一种更准确的加权粗糙集不确定性度量.下面首先给出扩展的全局精度定义,然后基于α熵和扩展的全局精度提出一种新的加权集成不确定性度量.文献[40]为更精确度量边界粗糙集,给出了基于等价关系的全局精度σP(X)与全局粗糙度GP(X)的定义:σP(X)=|U-BNP(X)|全局精度虽能较精确的度量边界粗糙集的不确定性,但对于信息系统中知识的不确定性却没有给出度量.下面首先将全局精度与全局粗糙度扩展到一般二元关系下,并将对象权重融入到不确定性度量中,即定义全局精度为其中,B(w)=∑ui∈BNP(X)wi,BNP(X)=RP(X)-RP(X);权系数wi表示对象ui的重要度.定义全局粗糙度为GP(X)=1-σP(X).在此基础上,我们提出一种新的集成加权不确定性度量如下.定义14.信息系统S=(U,A),PA,XU,X在一般二元关系RP下的α粗糙度和α精度为式中,BN(P)=Hα[∑ui∈BNP(X)∑uj∈{U-BNP(X)}w]j∑U用来度量知识不确定性.σP(X)用来度量集合不确定性.显然定义14中α精度通过Hα识在边界域上划分粒度的大小,通过σP(X)来度量粗糙集合边界域的大小.因此α精度和α粗糙度包含了知识和集合两种不确定性,是一种新的集成不确定性度量.定理16.PA,XU,论域中对象对应的权值为w1,w2,…,w|U|.知识P对uiU能正确分类,即uiBNP(X).若将ui的权值wi变为wi,wi>wi,其余对象权值不变,相应的知识P的α精度由ααP(X)变为αα定理16表明,如果增加某对象权值,则可以提Page10高能够对此对象正确分类的属性的α精度.因此若以α精度作为属性重要性度量进行约简,则可以通过增加某对象的权值来优先选择那些能够对此对象正确分类的属性作为约简属性(参见5.2节).定理17.P,QA,PQ,XU,则有(1)σQ(X)σP(X);(2)ρα推论3.P,QA,粗糙集XU,且PQ.令由HαU={uk∈U|RPU,ukBNQ(X)时,有ρα度和α粗糙度与RQ化程度无关,在一般二元关系下它们随知识粒度变小分别非严格单调递增和递减.如果存在被知识P细分的某个知识颗粒RQ则α精度严格递增.因此新的加权集成不确定性度量符合人类认知规律.推论4.PA,XU.则0ρα仅当对ui∈U,RP当BNP(X)=时左等式成立.定理18.PA,XU.则ααGP(X)ρα定理18表明由于α精度和α粗糙度考虑了集合和知识粒度两类不确定性,和我们认知直觉一致.推论5.P,QA,PQ,XU.则(1)若X为边界粗糙集(即RP(X)=RQ(X)=)且RQ(X)=RP(X),则ρQ(X)=ρP(X),GQ(X)=GP(X),但ρα(2)如果ραGQ(X)=GP(X);(3)如果ρP(X)<ρQ(X)或GP(X)<GQ(X),则P(X)ραρα推论5的(1)表明,对于边界粗糙集X,α粗糙度既度量了其集合的不确定性,也度量了其知识的不确定性,而ρP(X)和GP(X)则只包含了集合不确定性;(2)表明如果粗糙集的α粗糙度不随知识粒度的变化而变化,则ρP(X)和GP(X)也不随之改变.但粗糙集的α粗糙度减小,ρP(X)和GP(X)未必减小;(3)表明如果ρP(X)和GP(X)变小,将使得粗糙集的α粗糙度变小.推论5表明本文的集成不确定性度量对分类粒度变化更“灵敏”,是一种更为精确的不确定性度量.以上特性描述了α粗糙度随知识粒度变化的规律.下面以实例来说明新的集成不确定性度量的有效性.为了便于比较,假设论域中对象同等重要,权重为1.例3.当α=2时(α取其它值也可)在容差关系下利用α粗糙度对例2的计算结果为(1)当X={1,2,7}时根据定义14可得因此当集合X负域中的知识颗粒RP(与X无关)被细分时并不会引起α粗糙度的变化,即α粗糙度更符合人类的认知规律;(2)当X={2,3}时根据定义14可得显然对于边界粗糙集X,α粗糙度既考虑了信息系统中知识的不确定性(α熵),也考虑了粗糙集不同上近似带来的不确定性(全局精度).因而α精度和α粗糙度是一种更精确的集成度量方法.(3)从(1)和(2)可知,对粗糙集合XU都有即与考虑一种不确定性时粗糙集合的粗糙度相比,考虑两种不确定性时粗糙集合的粗糙度变大,因此α粗糙度与人的认知直觉相符.例3表明α粗糙度更准确地度量了粗糙集中的两种不确定性,弥补了现有的集成不确定性度量的缺陷,更符合人类的认知规律.5基于α精度的属性约简算法及应用本节利用α精度构造一种新的启发式属性约简算法,进而说明加权的重要性和不同参数α值的有用性.5.1基于α精度的属性约简算法属性约简是粗糙集理论核心内容.已证明寻找信息系统全部约简或最小属性约简是NP-难问题,实际应用中往往采用启发式约简算法搜索最优或次优约简.下面给出一种新的基于α精度的启发式约简算法.决策表S=(U,C,D),PC,对任意b∈{C-P},由定理17可知αα性P已知条件下属性b相对于决策属性D的加权属性重要度为定义15.决策表S=(U,C,D)中,PC,在条件属性P已知条件下b∈{C-P}相对于决策属性D的加权属性重要度为Page11P(D)=∑X∈U/D其中αα属性D的加权逼近精度.加权属性重要度将先验知识融入属性重要性中,它反映了在现有属性基础上增添的新属性的重要性程度.SIG(b,P,D)越小,则属性b在决策分类中的作用越小,重要性越低.定义16.决策表S=(U,C,D)中,RC.当且仅当b∈R,ααR为C的D约简.属性约简通常不唯一,所有约简中都包含核属性.为了避免重复计算属性重要度,在计算约简过程中可先求出核属性.定义核属性如下.定义17.决策表S=(U,C,D)中,对b∈C,核属性定义为Core={b|αα根据定义15~定义17构造的基于α精度的属性约简算法(ARAαA)如下.算法ARAαA.输入:决策表S=(U,C,D),参数α和权值w1,w2,…,w|U|输出:相对约简R//核属性Core的计算:Core←;求ααFORb∈C求ααIFααENDEND//相对约简R的计算:R←Core;求ααWHILEααFORb∈{C-R}表2心电图V3导联信息表U/A心率(a1)T波振幅(a2)Q波时间(a3)ST形态(a4)PR间期(a5)P波振幅(a6)疾病类型(D)u1正常u2u3正常u4正常u5正常u6过缓u7过缓u8过速u9过速u10过缓u11过缓u12过缓u13正常u14过缓增宽弓背向上抬高缩短增宽弓背向上抬高正常END输出R;需要指出的是,本文中的α精度是基于一般二元关系的,因此算法ARAαA适用于一般二元关系的信息系统,且考虑了论域中对象的重要程度.3.2节已证明在不同的α值下求得的α熵不同,进而使得α精度也不相同,因此不同α值求得的属性重要度也将不同,这将导致算法ARAαA中不同α值可能求得不同的约简.5.2算法ARAαA在心电图诊断中的应用现代临床诊断中,心电图[41]是诊断心脏疾病的重要方法之一,它具有简单实用、无创性等特点.研究心电图智能诊断系统具有重要的社会意义.表2给出了14个病人的心电图V3导联的特征变量及其离散取值的信息表S=(U,A),其中U={u1,u2,…,u14}表示14位患者的集合;A=C∪D,C={a1,a2,…,a6}为患者心电图6个特征的集合,D为对应患者的疾病类型.以表中“PR间期”为例,PR间期为0.12s~0.20s表示正常,大于0.20s则为PR间期延长,小于0.12s为PR间期缩短,因此PR间期包括3个属性值:“正常”、“延长”和“缩短”.表2中“”表示缺失值.心脏病中,心肌梗死往往给人类带来致命的伤害,而心律失常则比较常见,显然心肌梗死的危害程度远大于心律失常,即心肌梗死的误诊代价相应的也高于心律失常.据此医学专家对患有心肌梗死病人给出的权值较大,心律失常次之.专家给出不同患者权值为w1=w2=…=w5=0.16,w6=w7=…=w10=0.24,w11=w12=w13=w14=0.60.正常正常正常正常正常正常Page12为了在心电诊断中考虑诊断代价,首先在限制容差关系下用本文算法ARAαA对V3导联信息表进行约简.对于给定的权值,表2中数据约简唯一,这里取α=3(取其它值也可)得到的约简结果属性顺序为a2→a3→a1;而当所有病人对应的权值均相同,不考虑诊断代价时,在限制容差关系下基于正域的约简算法[42]的约简结果中属性顺序为a2→a5→a6.由心电图诊断知识可知,患者的6个条件属性中,T波振幅、Q波时间和ST形态对心肌梗死诊断的敏感性和特异性较高,而心率是诊断心律失常的重要参考指标.P波振幅和PR间期则是衡量心律失常的一个普通的指标.由以上实例可知,算法ARAαA方便的将医学专家的心电图知识引入到不确定性度量中,通过增加严重疾病(心肌梗死)病人的权值来优先选择那些能够对此类病人正确分类的属性作为约简属性,也即优先选择对心肌梗死具有特异性的特征作为约简属性,保证对误诊代价高的心脏病进行优先、有效诊断,更加符合实际.5.3算法ARAαA在分类器组合中的应用基于代数观点和基于信息论观点的约简算法是现有的两种典型启发式约简算法[5],实际应用中二者只考虑了在尽可能短时间内如何得到最好特征,并没有考虑分类能力.一个理想的约简算法应该在保证分类能力不降低的基础上来约简属性.然而已有的多种启发式约简算法[43]并没有考虑分类信息,很难保证求得的约简分类精度较高.基于多约简的组合分类方法利用多约简在分类时的互补信息提高了分类精度,受到广泛关注[44-45].文献[45]通过串行计算每个约简,然后搜索最优约简进行组合,在一定程度上提高了分类精度,但时间复杂度较高.直观上,多个约简间的差异性越大,组合分类时产生的互补信息就越多,组合约简的分类精度就越高.但目前并没有求差异较大约简的方法.由3.2节可知,属性信息粒度分布的不均匀将导致不同α值对应的α熵不同,从而使得算法1中属性重要性排序不同(参见例1的(2)),进而可能得到多个不同的约简.尤其是对于那些冗余属性比较多的大数据集,具有很多约简,不同α值可获得不同约简,可以利用投票原则将不同约简求得的分类结果进行表决,最终利用多约简之间互补信息达到组合分类的目的.图2给出了本文组合分类算法流程图,可以看出,相比串行计算约简的EROS系统[46],基于α精度的组合分类算法利用并行计算,由单次约简和分类的时间即可得到最终结果,复杂度大大降低.下面通过实验来验证不同α值求得的不同约简在构造组合分类器中的有效性.算法1.基于α精度的分类器组合算法(Clas-sifierCombinationAlgorithmBasedonα—Accuracy,CCAαA).输入:信息表S=(U,C,D),参数α1,α2,…,αm,输出:组合约简的分类精度1.利用算法ARAαA并行计算不同参数α1,α2,…,αm2.删除R1,R2,…,Rm中重复的约简后,得到的组合约3.并行计算RiR的分类精度Ci;4.根据多数投票原则计算组合约简的最终分类精度.为了验证本文组合约简算法的有效性,将代数观约简算法(算法1)[47]、信息观约简算法(算法2)[5]、最小冗余最大相关准则约简算法(算法3)[48]、邻域依赖度函数方法(算法4)[49]和本文算法CCAαA(算法5)在约简属性数量和分类精度上进行比较.我们从UCI数据集中挑选了5组数据集(均具有上百个约简),如表3所示,其中前4组数据的条件属性全为数值型.5种算法中前两种算法只能处理符号数据,为便于计算,算法3也处理符号型数据.因此,首先对前4组原始数据进行离散化,这里采用基于Shannon和最小描述长度原则的离散化算法.由于邻域关系可直接处理数值型数据,因此算法4和算法5基于邻域关系直接求约简,文中参数δ=0.15.为减少属性量纲不一致对约简结果带来的影响,在计算每个样本的邻域时,将所有数值型属性标准化到[0,1]区间.算法5中令w1=w2=…=w|U|=1,m=5,α1=-3,α2=0.5,α3=1,α4=2,α5=3.5.为了验证约简结果的分类能力,以10折交叉验证方Page13式对5类算法的约简结果采用CART算法进行分类,来评价算法约简质量.表3和表4分别给出了5种算法在特征个数和分类精度上与原始数据的比较.表3中R1,R2,R3和R4分别表示算法1、算法2、算法3、算法4选择的特征数量,R5表示算法5中不同α值选择的不同约简中特征数量的平均值.表4中Raw-acc表示原始数据对应的分类精度;Min-acc和Max-acc则为组合约简对应的分类精度的最小值和最大值;此外,Accuracy1,Accuracy2,Accuracy3和Accuracy4分别表示算法1,算法2,算法3,算法4的约简结果的分类精度.Combined-Acc1表示算法1、算法2、算法3和算法4的约简按照本文方法组合后的分类精度,Combined-Acc2表示算法5约简结果的分类精度.由表3可以看出,5种约简算法都有效的实现了对特征的约简.算法1和算法2处理后的特征维数最低,但以丢失很多信息为代价,导致分类精度较低,甚至远低于原始数据;算法3由于利用最小冗余最大相关准则求得特征重要性排序,然后按重要性顺序选择分类精度最表45种约简算法约简结果对应的分类精度DataRaw-AccAccuracy1Accuracy2Accuracy3Accuracy4Combined-Acc1Min-AccMax-AccCombined-Acc2Wine0.89860.87010.87010.91530.92570.91890.89100.94370.9438WDBC0.90500.93150.86300.94270.94020.93150.91900.94190.9473Sonar0.72070.65360.74070.69690.69670.70240.69670.78330.8077Iono0.87550.87630.85530.92680.91500.91740.88150.91500.9145Derm0.92260.68370.75520.93330.93210.93990.90120.96670.9645Average0.86450.80300.81690.88300.88190.88200.85790.91010.9156表3和表4表明,对于拥有很多个约简的大数据集,本文提出的分类器组合算法是可行的,在不增加算法时间复杂度基础上大大减少了特征维数,提高了分类精度.由于算法5中参数α1,α2,…,αm是通过大量实验给出的经验值(实际应用中可参考),下一步将对不同α值的选择和分类精度间关系展开研究.6结论不确定性度量是不完备信息系统中信息处理和知识获取的基础.本文在现有的粗糙集多种扩展模型基础上,将数据对象的权重融入到不确定性度量中,提出了新的加权不确定性度量———α熵、α精度和α粗糙度,具有如下特点:(1)基于一般二元关系的α熵、α条件熵和α互信息扩展了现有的多种不确定性度量方法.本文中高的特征作为约简,因而算法3选择的特征数量相对较多,特别是数据Derm(见表3),而在分类精度上,算法3取得了较高的分类精度;算法4的约简分类精度较高,但特征数也相对较多;对前4种约简算法的结果按照本文方法组合分类后得到的分类精度Combined-Acc1并没有完全优于单个约简的分类精度,甚至平均分类精度还低于算法3的平均分类精度,这说明参与组合的约简不仅要存在差异,而且要有互补性;算法5中组合后分类精度整体上优于其它4种算法和Combined-Acc1对应的分类结果,平均选中的特征维数也较低.因而本文组合约简算法中不同α值求得的约简具有互补性和差异性,能够在选择较少特征的同时提高分类精度.表35种约简算法对UCI数据集的约简结果DataSamplesFeaturesClassesR1R2R3R4R5Wine17813366.07.05.05.75WDBC56930233.07.012.010.6Sonar20860222.012.07.06.2Iono35134235.06.08.08.0Derm36634666.026.010.09.75Average提出的不确定性度量可通过改变参数α来获得不同的特性,因而更具一般性和灵活性,实际应用中可根据需要来灵活选择;(2)构建了一种集成加权不确定性度量,将集合的不确定性和知识的不确定性融为一体.实例表明本文的集成加权不确定性度量不仅符合认知规律,而且是一种更精确的不确定性测度;(3)将领域知识引入到知识发现过程中在实际应用中是非常重要的.本文利用数据对象的权重,方便地将主观偏好和先验知识等因素融入到信息系统的不确定性度量中,更加符合实际,为基于一般二元关系的粗糙集不确定性度量提供了一种引入先验信息的途径;(4)本文中一般二元关系下的不确定性度量不仅适用于经典的等价关系,而且对于各种粗糙集扩展模型,如优势关系,容差关系,相似关系,限制容差关系,邻域关系等也适用,具有更广泛的普适性.Page14粗糙集理论是为开发规则的机器自动生成而提出的[50],而本文提出的不确定性度量也正是进行知识获取的基础.进一步研究加权不确定性度量中权值获取方法以及利用本文加权不确定性度量对规则获取进行更好的指导是我们下一步工作.
