Page1基于非近似求导过程的加更新和乘更新分类算法刘建伟李双成罗雄麟(中国石油大学(北京)自动化研究所北京102249)摘要自从Kivinen和Warmuth提出权衡正确性与保守性的在线学习框架后,此在线学习框架已被广泛引用.但是在Kivinen和Warmuth提出的梯度下降和指数梯度下降算法中,对目标函数中的损失函数求导过程中使用近似步骤会引起在线学习结果恶化.文中,运用对偶最优化理论,提出了非近似的基于平方距离相关熵损失函数分类算法和基于相关熵距离相关熵损失函数分类算法,通过4种不同维数的真实数据集的实验研究,验证了提出算法的分类预测性能.关键词最优化对偶理论;非近似更新;在线学习;相关熵度量;相关熵损失1引言自从Rosenblatt[1-2]根据神经信息处理机理提出感知器学习算法后,Novikov[3]又给出了感知器算法的收敛性的严格的数学证明,自此,基于感知器Page2组成的弱分类器构成的提升分类器算法[8]、核化最大间隔感知器算法[9],类标签具有复杂结构的分类器算法的构造,如具有层次结构的文本解析、标注和名实体抽取排序算法[10-13].假定存在样本序列集S={(狓1,y1),(狓2,y2),…,(狓k,yk),…}Rn×R或者Rn×{±1},狓k叫样例,回归学习时,ykR,yk为目标函数值;分类学习时,yk{±1},yk为样例狓k的类标签.感知器学习算法学习过程可以用猜谜游戏过程模拟,学习的过程分为一轮一轮的,第k轮时感知器学习算法接收到一个样例狓k,感知器学习算法根据判别规则或映射函数f(狓)=狑Tyk的猜测值y^k=f(狓),此时,感知器学习算法根据获得的真实的目标值yk判断返回结果的对与错,调整自己下一轮使用的判别规则或映射函数,猜对了,不更新狑k,即狑k+1=狑k,只在预测类标签出现错误时,即sign(狑Tk·狓k)≠yk时,采用规则狑k+1=狑k+yk狑T狓k+1,感知器学习算法利用刚才修正的判别规则或映射函数狑Tyk+1的猜测值y^k+1,感知器学习算法根据获得的真实的目标值yk+1判断返回结果的对与错,又一次调整自己下一轮使用的判别规则或映射函数的权值狑k+2,如此学习过程不断进行下去.在Kivinen和Warmuth的论文[14-15]发表之前,感知器学习算法主要为Rosenblatt提出的加权更新形式.Kivinen和Warmuth在文献[14-15]中提出了基于梯度下降的加权更新(以下简记为GD)和指数梯度下降(以下简记为ED)的乘权更新模式.算法的更新过程有两个需要权衡的因素:即算法应尽量保持前一次学到的内容和使得算法的预测误差尽可能地小,尽量保持前一次学到的内容即是使得依次学习到的两个权值狑k与狑k-1之间的距离d(狑k,狑k-1)尽可能地小;使算法的预测误差尽可能地小即为使代表预测值与真实值之差的损失函数L(y^k,yk)尽可能地小,因而算法的目标为使得函数Ο(狑k)=d(狑k,狑k-1)+ηL(yk,y^k)的值最小,此处η>0.为了最小化Ο(狑k),对Ο(狑k)求导并令其为0,即Ο(狑k)狑k=0;因为L(yk,狑T就意味着对于k=1,…,m,狑k的值满足如果选用d(狑k,狑k-1)=∑nL(yk,y^k)=(yk-狑Ti=1如果假定狑k,狑k-1∈[0,)n为概率向量,即∑nwk,i=1,选用相关熵距离dre(狑k,狑k-1)=wk,iInwk,iwk-1,i∑ni=1wk,j=0时,0In0=0,当wk-1,i=0时,dre(狑k,狑k-1)=.此时求解式(1)则得到乘权更新形式:这里rk-1,p=e-m(y^k-1-yk-1)xk-1,p.对于式(1),由于某些损失函数L(yk,狑Tk·狓k)在狑k-1点的一阶泰勒近似Lyk(狑T于狑k的导数不存在,Kivinen和Warmuth提出使用Lyk(狑T代替Lyk(狑T式(1)就转化为如果模型向量更新量很小,那么用Lyk(狑T代替Lyk(狑TKivinen和Warmuth在文献[14]中对目标函数求导过程中使用近似步骤引起的问题进行了详细的讨论.在zk=狑TL(yk,zk)/zk=L(zk)求导,在使用相关熵损失函数L(y,y^)=yIn(y/y^)+(1-y)In1-y严重的问题.此时,L(y,z)=-ykzk+1-yk1-zk0<yk<1,当zk接近0或1时,L(yk,zk)的值将变化很大,即当预测值狑Tk-1·狓k接近于0或1时,L(yk,狑T离很大.从而使得算法求出的导数值非常不准确,影响算法的分类结果.论文[14-15]发表后,已被广泛引用,但所有的工作都是引用Kivinen论文中的近似迭代算法,没有解决权更新过程使用近似方法带来的问题.Cristianini和Potluru等人[16-17]使用权乘更新训练支持向量机.Saul和Sha等人[18]分别使用权值乘更新形式实现混合模型分类、大间隔分类器设计[19]、Page3非负的二次规划支持向量机[20-23]和L1-范数正则化线性及逻辑斯谛回归[24],Saul和Sha提出的算法没有用到相关熵等概念,和Kivinen得出权值乘更新的原理迥然不同.Bartlett和Globerson等人[25-26]分别提出使用大间隔ED算法对类标签具有偏序结构的样例进行分类处理.Collins使用贝叶斯理论对马尔可夫随机场和马尔可夫网络建立后验函数,用极大后验函数构造判别函数,对判别函数增加大间隔约束条件,最终求解约束优化问题时,使用了ED算法[27].Kleinberg等人[28]提出用ED算法解决无政府状态下拥塞博弈问题,实现无后悔纳什相对平衡.Takimoto等人[29]提出使用乘更新构造路径核函数.Warmuth等人通过引入量子相关熵和冯·诺依曼离差,把向量ED模式推广到矩阵作为权,在线迭代更新权矩阵.矩阵权ED学习算法分别可用于训练置换矩阵[30-31]、主分量在线随机学习[32-33]、Bregman投影矩阵学习[34]、在线学习最小方差矩阵[35]和熵正则化线性规划提升算法构造[36].针对上述问题,本文主要完成了以下工作:(1)提出非近似平方距离相关熵损失函数更新形式的在线分类算法(以下简记为s-rel).(2)提出非近似相关熵距离相关熵损失函数更新形式的在线分类算法(以下简记为r-rel).(3)用实际数据实验验证了提出的分类算法的分类效果.本文第2节回顾相关知识背景;第3节对于分类算法中将要用到的几个基本定理加以证明;第4节提出基于相关熵距离相关熵损失函数的分类算法;第5节提出基于平方距离相关熵损失函数分类算法;第6节对算法进行实验评价;第7节给出本文的结论.2知识背景2.1加权更新和乘权更新在线学习方法以下讨论中,为了简便,省略变量的迭代序列下标.用R表示实数集,R+表示非负的实数集,R++表示正的实数集,Rn表示n维向量集,Rm×n表示m×n实矩阵集.狑代表模型向量,狑的上标表示第i轮学习得到的模型向量狑i,狑的下标i表示模型向量的第i个分量wi.Kivinen和Warmuth在文献[14]中,对目标函数Ο(狑)=d(狑,狊)+ηL(y,y^)中的距离提出了3种度量准则:相关熵距离函数:dre(狑,狊)=∑n处,狑,狊的分量非负且各分量之和必须为1,即狑,狊为概率向量.未归一化相关熵距离:此处,狑,狊∈[0,)n.卡方距离:dχ2(狑,狊)=1此处,狑,狊∈0,[)n.失函数提出以下两种形式:对目标函数Ο(狑)=d(狑,狊)+ηLy,y()^中的损平方损失函数:L(y,y^)=y-y()^2.相关熵损失函数:L(y,y^)=yIn(y/y^)+(1-y)In1-yKivinen等人在原文对于相关熵距离和卡方距离讨论中,允许狑,狊∈[0,)n,实际应为狑,狊∈(0,)n.Kivinen和Warmuth针对目标函数Ο(狑)=d(狑,狊)+ηL(y,y^),提出了用于回归的加更新和乘更新算法.其中,对于相关熵损失函数的EG算法均采用损失函数的一阶台劳展开作为相关熵损失函数的近似值参与迭代过程的推导.本文中我们给出了相关熵损失函数非近似的迭代过程,并用于分类算法.3预备定理的证明以下给出分类算法中将要用到的几个基本定理.Rn→R为凸函数.引理1.在狑∈Rn中的任意范数f(狑)=‖狑‖:证明.证明过程见文献[37].引理2.设狑∈(0,1)n,y^=狑T·狓∈(0,1),则L(狑)=yIn(y/y^)+(1-y)In1-y证明.L(狑)=yIn(y/y^)+(1-y)In1-yPage4L(狑)=-yL(狑)=y当y=0,L(狑)=狓2L(狑)=1L(狑)=yIn(y/y^)+(1-y)In1-ywi-si+siInsiw(∑n证明.设函数r(狓,狔)=∑ni=1引理3.设狑,狊∈0,[]1n,则dreu(狑,狊)=向量狔,狓∈Rn熵的r(xi,yi)=-xilogxi/yi的和.由于r(xi,yi)=-xilogxi/yi在定义域Rn的性质:凸函数的和仍是凸函数,可知r(狓,狔)=∑nxilogxi/yi为凸函数.又由于dreu(狑,狊)=wi-si+siInsiw(∑ni=1由凸函数的性质知dreu(狑,狊)为凸函数.证毕.引理4.假定d(狑,狊),Ly,y()^为凸函数,求解i=1问题(P1):的最优解等价于以下引入拉格朗日乘子后的无约束对偶问题(P2):的最优解.证明.在问题(P1)中引入变量c,使得不等式约束问题(P1)等价转化为以下等式约束问题(P3):根据敏感性分析定理(文献[38]中的339页),对于任意的t-c,存在一个连续依赖于t-c的解狑(t-c),使得当t-c=0时,存在问题(P3)和问题(P2)的解狑(0)=狑,而且问题(P3)中的t-c与问题(P2)的η之间存在关系:4提出的基于相关熵距离相关熵损失函数分类算法定理1.在Ο(狑)=d(狑,狊)+ηL(y,y^)目标函数中,令d(狑,狊)=dreu(狑,狊)=∑n损失函数选用相关熵损失函数即L(y,y^)=yIn(y/y^)+(1-y)In1-ywi,si∈R+,狑,狊为概率向量,即∑n对于样本序列(狓1,y1),(狓2,y2),…,(狓m,ym),其中,狓j∈Rn,yj∈{0,1}.则每得到一个样本时,使得Ο(狑)取最小值的迭代公式为当y=0时,当y=1时,证明.我们的证明思路源于文献[39-40].令Οrr(狑)=∑n其对偶等价形式为Οrrd(狑)=∑n以下分y=0和y=1两种情况讨论.(1)y=0时Οrr(狑)|y=0=∑nΟrrd(狑)|y=0=∑nPage5Οrr(狑)|y=0wi=Οrrd(狑)|y=0由于dreu(狑,狊)=∑nL(y,y^)=yIn(y/y^)+(1-y)In1-yΟrr(狑)和Οrrd(狑)均为凸函数.又由于存在狑,使得-In(1-狑T·狓j[立,故满足slater条件,存在狑,当Οrr(狑)和Οrrd(狑)取最优值时,使得互补松弛条件η[-In(1-狑T·狓j)]=0,In(1-狑T·狓j)-①Οrr(狑)取最优值时,Οrr(狑)|y=0In(1-(狑)T·狓j)=0,解得把式(15)代入式(14)解得把式(17)代入式(15)得xj,1s1e-ηxj,1+xj,2s2e-ηxj,2+…+xj,2s2e-ηxj,2=0利用ex≈1+x,得xj,1s1(1-ηxj,1)+xj,2s2(1-ηxj,2)+…+xj,2s2(1-ηxj,2)≈0解得②Οrrd(狑)取最优值时,η[In(1-(狑)T·狓j)-t]=0,故(1-(狑)T·狓j)=t+1,((狑)T·狓j)=t,由于Inwisi+ηxj,i(2)y=1时Οrr(狑)|y=1=∑nΟrrd(狑)|y=1=∑nΟrr(狑)|y=1同上所述,存在狑,当Οrr(狑)和Οrrd(狑)取最优值时,使得互补松弛条件-ηIn((狑)T·狓j)=0,-η[In((狑)T·狓j)-t]=0成立,即①Orr(狑)取最优值时,η[In((狑)T·狓j)-t]=0,(狑)T·狓j=1,故同y=0时的讨论方法,得xj,1s1eηxj,1+xj,2s2eηxj,2+…+xj,2s2eηxj,2=1xj,1s1(1+ηxj,1)+xj,2s2(1+ηxj,2)+…+xj,2s2(1+ηxj,2)≈1②Οrrd(狑)取最优值时,η[In((狑)T·狓j)-t]=0,((狑)T·狓j)=1+t,从以上分析可以看出,两个目标函数Οrr(狑)和Οrrd(狑)求出的最优解之间只存在一个比例系数1+t,故用目标函数Οrr(狑)和Οrrd(狑)求出的结果没e1有本质的区别,与引理4结果一致.以下给出基于相关熵距离相关熵损失函数分类算法的实现步骤.算法1.r-rel算法.输入:(狓1,y1),(狓2,y2),…,(狓m,ym),学习率η∈输出:误差error,测试时间t,模型权向量狑1.预置w(1)=s.2.当收到第i个样本狓i后,预测值为y^i=(狑(i))T·狓i.3.当收到第i个样本的类标签yi后,判断是否预测错误,如果正确yi=y^i,权值不更新狑(i+1)=狑(i),否则按以下规则更新权值.4.计算误差.定理2.令样本序列为S={(狓1,y1),(狓2,y2),…,(狓i,yi),…}Rn×{0,1},定理1中的相关熵距离相Page6{yk=1时,损失函数为ly=1((狑(i))T狓k,yk)=关熵损失函数在线分类算法学习得到的模型向量狑为概率向量,即∑nρ,αxi,jβ,j=1,2,…,n,yk=0时,定义损失函数为ly=0((狑(i))T狓k,yk)=max(狑(i))T狓k-(-min(狑(i))T狓k-(η,且存在某个b∈R,N∈R满足b2ηxk,ib+N,令C=e-b/N+b(eb/N-e(b/N)+1)b分类算法共犯M次分类错误,其中M1次为类标签为0时的预测错误,M2次为类标签为1时的预测错误,狌∈Rn为任意参考概率向量,即∑n学习算法使用定理1中的更新公式所犯错误总数M满足以下不等式M=M1+M2<[(U-CV+αDU-ηρ)+‖狌‖2[(U-CV+αDU-ηρ)∑M1(U-E·V-ρFV-ηρ)∑M2证明.(1)yk=0时,定义狑(i+1)=(w(i)=(w(i)(1),w(i)=狑(i)e-η狓k,则又δi=狑(i)-狌2-狑(i+1)-狌2=狑(i)-狌2-狑(i)e-η狓k-狌2=(狑(i))2-2(狑(i))T狌+狌2-(狑(i))T狑(i)e-2ηyk+2(狑(i))T狌e-η狓k-狌2=(狑(i))2-2(狑(i))T狌-(狑(i))T狑(i)e-2ηyk+2(狑(i))T狌e-η狓k①对于e-η狓k,由于ex>1+x,x≠0[41]②对于e-2η狓k,令由于0<x1.5936,e-x<1-xe-2η狓k=e-b/Ne(-2η狓k+b)/N<e-b/N·1-这里,C=e-b/N+bδi>(狑(i))2-2(狑(i))T狌-(狑(i))T狑(i)(C-D狓k)+式(32)和式(35)代入式(31)得2(狑(i))T狌(1-η狓k)=(狑(i))2-2(狑(i))T狌-C(狑(i))T狑(i)+D(狑(i))T狑(i)狓k+2(狑(i))T狌-2η(狑(i))T狌狓k=(1-C)狑(i)2+D狑(i)(狑(i))T狓k-2η狑(i)(狑(i))T狓k2η狑(i)狌T狓k=2η狑(i)狌T狑(i)-()1η狑(i)+2η狑(i)max狌T狑(i)-()1=η狑(i)+2η狑(i)ly=0(狌T狓k,yk)(37)所以由于δi>(1-C)狑(i)2+D狑(i)(狑(i))T狓k-对所有yi=0的样本更新序列求∑M1‖狌‖2>(1-C)∑M1由Uwi2V,σwiρ,αxk,iβ,得Page7M1<‖狌‖2(2)yk=1时,定义δi=狑(i)-狌2-狑(i+1)-狌2=狑(i)-狌2-狑(i)eη狓k-狌2=(狑(i))2-2(狑(i))T狌+=(狑(i))2-2(狑(i))T-狌(狑(i))T狑(i)e2ηyk+①对于eη狓k,由于x≠0时,ex>1+x,故②对于e2η狓k,由于e>0,当0x1时,ex1-x+ex[41],故定义e2η狓k=eb/Ne(2η狓k-b)/Neb/N·1-2η狓k-b=eb/N-2ηeb/N狓k-beb/N=eb/N+(eb/N-e(b/N)+1)b=E+F狓k这里E=eb/N+(eb/N-e(b/N)+1)b把式(46)和(49)代入式(45)得δi>(狑(i))2-2(狑(i))T狌-(狑(i))T狑(i)(E+F狓k)+2(狑(i))T狌(1+η狓k)=(狑(i))2-2(狑(i))T狌-E(狑(i))T狑(i)-F(狑(i))T狑(i)狓k+2(狑(i))T狌+2η(狑(i))T狌狓k=(1-E)狑(i)2-F(狑(i))T狑(i)狓k+2η狑(i)狌T狓k所以δi>(1-E)狑(i)2-F(狑(i))T狑(i)狓i+对所有yi=1的样本预测错误权更新序列求i=1∑M2δi和得‖狌‖2>∑M2由于Uwi2V,σwiρ,αxk,iβ,故‖狌‖2>U·M2-E·V·M2-ρFVM2-ηρM2-M2<‖狌‖2(3)由式(43)和式(55),得在线分类算法总的误差上界为M=M1+M2<[(U-CV+αDU-ηρ)+‖狌‖2[(U-CV+αDU-ηρ)∑M1(U-E·V-ρFV-ηρ)∑M2考向量的错误界.5提出的基于平方距离相关熵损失结论是我们的算法的错误上界总是低于任意参函数分类算法基于以上相关熵距离相关熵损失函数分类算法Page8的分析,在基于平方距离相关熵损失函数分类算法分析中,只考虑原问题,不再就对偶等价问题进行讨论.定理3.在Ο(狑)=d(狑,狊)+ηLy,y()^目标函数中,令d(狑,狊)=∑n损失函数即L(y,y^)=yIn(y/y^)+(1-y)In1-y狑,狊的分量非负,wi,si∈R+,且狑,狊为概率向量,即∑nwi=1,∑ni=1(狓2,y2),…,(狓m,ym),其中,狓i∈Rn,yi∈{0,1}.则使得Ο(狑)取最小值的迭代公式为y=0时学习率为y=1时证明.根据定义知:Ο(狑)=(狑-狊)T(狑-狊)+=(狑-狊)T(狑-狊)+=(狑-狊)T(狑-狊)+η[yIny-yIn(狑T·狓i)+即Ο(狑)=(狑-狊)T(狑-狊)+η[yIny-yIn(狑T·狓i)+(1)当y=0时,式(60)为Ο(狑)|y=0=(狑-狊)T(狑-狊)-ηIn(1-狑T·狓i)Ο(狑)由于d(狑,狊)=∑n(1-y)In1-yηyIn(y/y^)+(1-y)In1-y在狑,使得In(1-狑T·狓i)<0,故满足slater条件,存在狑,Ο(狑)|y=0取最优值时,使得互补松弛条件ηIn(1-(狑)T·狓i)=0成立,即所以(狑)T·狓i=0代入式(62)得把式(64)代入式(63)解得(2)当y=1时Ο(狑)|y=1=(狑-狊)T(狑-狊)-ηIn(狑T·狓i)(66)同上所述,Ο(狑)|y=1取最优值时,互补松弛条件η[In((狑)T·狓i)]=0成立,即(狑)T·狓i=1成立.(狑)T·狓i=1代入式(67)得把式(68)代入(狑)T·狓i=1解得基于平方距离相关熵损失函数分类算法可以描述如下.算法2.s-rel算法.输入:(狓1,y1),(狓2,y2),…,(狓m,ym),学习率η∈输出:误差error,测试时间t,模型权向量狑1.预置狑(1)=狊.2.当收到第i个样本狓i后,预测值为y^i=狑(i)·狓i.3.当收到第i个样本的类标签yi后,判断是否预测错误,如果正确,权值不更新狑(i+1)=狑(i),否则按以下规则更新权值:4.计算误差.定理4.令样本序列为S={(狓1,y1),(狓2,y2),…,(狓i,yi),…}Rn×{0,1},定理2中的基于平方距离相关熵损失函数分类算法学习得到的模型Page9向量狑为概率向量,即∑nΚ,‖狓i‖2Η,假定分类算法有分类错误,其中M3次为类标签为0时的预测错误,M4次为类标签为1时的预测错误,狌∈Rn为任意参考概率向量,即∑nui=1,则在线学习算法使用定理2中的更新公i=1式相关熵损失函数错误界满足以下不等式L(yi,狑(i))∑M3+M4∑M3+M4i=1证明.(1)yi=0时,定义相关熵损失函数L(y,y^)=yIn(y/y^)+(1-y)In1-y得L(yi,狌)-L(yi,狑(i))(狓i)T(1-(狑(i))T狓i)[L(yi,狑(i))-L(yi,狌)]δi1L(yi,狑(i))L(yi,狌)+‖狓i‖2L(yi,狑(i))L(yi,狌)+Η(1-Κ)+2δi∑M3L(yi,狑(i))∑M3(2)yi=1时,定义i=1δi=狑(i)-狌2-狑(i+1)-狌2相关熵损失函数L(y,y^)=yIn(y/y^)+(1-y)In1-yL(yi,狌)-L(yi,狑(i))-1狑(i)L(yi,狑(i))-狑(i)L(yi,狌)狌-狑(i())(85)δi112η(狑(i))T狓iL(yi,狑(i))δi+1L(yi,狑(i))2δii=1i=1i=1∑M4L(yi,狑(i))η(狑(i))T狓i+∑M4∑M4∑M4L(yi,狑(i))2‖狌‖2M4(3)由式(58)和式(69),得到在线分类算法总Page10的误差上界为∑M3+M4L(yi,狑(i))∑M3+M4i=16实验研究在4个实际数据集letter、USPS、mnist、INEX上对EG、s-rel、r-rel3种分类算法的性能进行实验比较.另外与Crammer新提出的两种感知器算法即被动主动感知器(PA)[42]和信任权感知器(CW)[43]进行实验比较.实验中数据归一化算法分为线性归一化(用l_n表示)和二范数归一化(用2n_n表示).Letter、USPS数据集取自UCI①,mnist为手写体数字识别基准数据库②,INEX数据集包含IEEE18种杂志和会议论文集中的论文,使用TF/IDF特征提取方法组成文本向量数据集③.4个真实数据集的类标签及特征个数等参数可用表1描述.使用这4组数据做实验的目的是针对不同的数据集、低维(16维)和高维(167295维)数据,对4种分类算法进行测试,检验其性能.letterUSPSmnistINEX样本在进行分类前,需要进行归一化处理.样本进行归一化处理采用两种处理方式:(1)线性归一化(l_n).求各样本狓i的分量之和xi,j,令狓i··=狓i∑nj=1(2)二范数归一化(2n_n).求各样本狓i的二范数‖狓i‖2=∑n在我们的实验中,用训练错误率曲线表示分类结果的准确性.实验过程如下:将训练样本集合中的样本狓i依次输入训练程序,每输入一次,都会得到一个训练模型W和预测误差err,当最后一个训练样本输入训练程序后,算法得到最终训练模型狑,记录的预测错误的个数err和已输入训练样本的个数total,算法预测的错误率为err/total,按照样本输入顺序,把所有的样本对应的错误率绘制成曲线,就得到了训练错误率曲线.6.1Letter数据分类实验Letter数据的特征值全部为自然数,鉴于此,我们只对Letter数据进行线性归一化.Letter数据为多类数据,我们每次只取出其中的两类数据进行实验.实验结果如表2所示,从表中可以看出:在Letter数据上,EG和PA算法的正确率一直维持在0.5左右,表明EG算法和PA算法不适合对Letter数据集分类;CW算法的正确率略微优于EG和PA算法,最差情况下也有80%左右的正确率;s-rel和r-rel算法的正确率在99%左右,在此数据集上的分类优势明显.EG+l_n0.52360.52830.49150.50520.4920s-rel+l_n0.00500.00410.00650.01510.0048r-rel+l_n0.01080.00900.01610.01990.0112PA+l_n0.46820.47010.48370.48210.4344CW+l_n0.06040.03440.19370.22830.0784标签7和17对应字母‘g’和‘q’,是区分难度很高的两个字母.标签3和8对应字母‘c’和‘h’,区分难度也较大,这从表2中可以得到验证.为此,我们给出其分类误差率曲线见图1中的(a)和(b)所示.从图中不难发现在训练过程中,s_rel算法和r_rel算法的收敛速度相当,但s_rel算法收敛于更低的误差.6.2mnist数据分类实验同letter数据一样,mnist数据也为多类数据,我们每次只取出其中的两类数据进行实验.共取4组进行实验,对于每组实验各采用线性归一化和二范数归一化对数据进行预处理.实验结果如表3所示,可以看出:EG算法预测准确率很低;PA算法在mnist数据上的分类能力较之Letter数据得到提高;s-rel算法的分类能力最强;CW算法和r-rel算法分类误差率接近,分类能力稍次于s-rel算法.另外从表中可以发现同一数据在不同的归一化方法后在同种算法下的训练误差率①②③Page11图1Letter数据分类实验结果会有些微的差别,这是因为归一化相当于对数据进行了压缩,这势必影响到生成的预测标签,进而对误差率产生影响.EG+l_n0.46910.46770.48830.4860EG+2n_n0.46910.46770.48830.4860s-rel+l_n0.00790.00430.01050.0027s-rel+2n_n0.07540.00430.01090.0075r-rel+l_n0.03750.05160.03750.0174r-rel+2n_n0.06210.04190.09330.0400PA+l_n0.24130.16360.39980.2831PA+2n_n0.20030.08990.35170.1861CW+l_n0.01580.00300.06280.0077CW+2n_n0.01570.00290.06340.0085限于篇幅,对mnist数据我们仅给出标签3和8下的训练误差率曲线,如图2所示.由曲线可知:无论对mnist数据进行线性单位化,或是对mnist数据进行二范数单位化,收敛特性是相同的.具体表现为:EG和PA算法表现稳定,s-rel算法收敛速度最图2mnist数据分类实验结果快,CW算法次之,r-rel算法收敛速度又次于CW.整体而言,后面3种算法差距不明显.6.3usps数据分类实验实验结果如表4所示,从表中可以看出在预测准确度方面:EG算法同样预测准确率很低;PA算法优于EG算法;s-rel算法表现突出,能够获得0.99~1.00的准确度;CW算法的分类准确度与s-rel算法相当;r-rel算法稍次于s-rel和CW算法,EG+l_n0.45700.35040.46880.4924EG+2n_n0.45700.35040.46880.4924s-rel+l_n0.00320.00650.01310.0069s-rel+2n_n0.00140.00220.00440.0038r-rel+l_n0.03730.03920.04220.0894r-rel+2n_n0.03820.08320.12350.1078PA+l_n0.30600.29160.38950.3341PA+2n_n0.19690.25630.34740.3219CW+l_n0.00270.00870.02110.0092CW+2n_n0.00270.00820.01960.0092Page12但是,除个别数据外,也能够获得0.95以上的准确度.限于篇幅,在Usps数据上我们仅给出标签1和2的训练误差率曲线以便观察算法的收敛性,如图3所示.s-rel算法和CW算法的收敛速度最快,图3类标签为1和2时usps数据分类实验结果6.4inex数据分类实验Inex数据已经过归一化处理,故本实验未对其进行归一化处理.针对inex数据实验发现:inex为高维稀疏数据,样本个数相对较少,这样,每个特征的有效训练数据很少,训练模型不能有效收敛,导致预测精度极差.以标签1和2为例,训练结果见表5,共有训练样本496个,统计每个样本的非零特征值个数:最多为3404个,最少为12个,而全部特征数却为167295个,平均一个特征可用有效训练数据为0.036~10.092个(12×496/167295=0.036,3404×496/167295=10.092),显然每个特征的有效训练数据太少,不能正确生成训练模型,训练预测模型分类效果很差.EGs-relr-relPACW0.31270.31270.31270.46540.3762综合以上所有实验得到的结论是:结合收敛速度,预测准确度,算法普适性3个因素不难发现s-rel算法最好,r-rel算法次之,EG算法最差,这也是我们这篇文章的论证所在.7结论与展望本文利用最优化对偶理论,提出了基于平方距离相关熵损失函数分类算法和基于相关熵距离相关且收敛性最好;PA算法仍很稳定;EG算法的收敛速度快于r-rel算法,但是在个别数据上,EGnn算法和r-rel算法都有不稳定现象出现,s-rel算法没有出现不稳定现象,这表明s-rel算法的普适性优于EG算法和r-rel算法.熵损失函数分类算法.克服了文献[14]中目标函数求导过程中使用近似步骤引起的问题.运用4个不同维数的真实的数据集letter、USPS、mnist、INEX验证了提出的算法分类准确性.我们的研究也引起了许多问题,今后的可能研究方向为:高维稀疏数据的分类:我们的算法没有涉及到维数约减,当维数高且稀疏程度高时,算法无法应用.归一化:我们采用线性归一化和二范数归一化两种模型,除了s-rel算法外,EG、r-rel、PA和CW算法对归一化方式敏感,我们下一步深入研究线性归一化和二范数归一化对数据的影响,即两种归一化模型的适用范围.因而未来的工作是:减少算法复杂性,改进算法的可解释性,如利用多目标最优化理论,对算法进一步改善;引入稀疏性,对于高维数据集构建在线稀疏模型,在构建模型时引入权L1-范数正则化因素.
