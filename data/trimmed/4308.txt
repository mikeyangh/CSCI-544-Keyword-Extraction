Page1一个支持访存带宽敏感调度的跨执行优化方法徐地武成岗冯晓兵(中国科学院计算技术研究所计算机体系结构国家重点实验室北京100190)摘要片外访存带宽是共享存储多核系统的主要性能瓶颈.访存带宽敏感的任务调度可以有效缓解并发程序间的访存竞争,提高系统吞吐率.然而调度策略的实施需要关于程序执行的先验知识,给系统用户增加了额外负担;另一方面,并发程序间的带宽竞争使得运行时收集的程序带宽需求信息不精确,影响了调度效果.在该文中,作者提出了一个低开销、对用户透明的跨执行优化方法解决上述问题.它在运行时识别程序的阶段性(phase)行为,并估算每个phase的独占执行性能;上述信息被存储到数据库中,在程序未来的执行中指导调度,并且信息精度随着程序的多次执行持续增加.上述过程使得带宽敏感调度策略的进行不再需要任何用户信息制导,并且优化了调度效果.作者在基于IntelXeon处理器的8核系统上实现并评估了该系统,测试结果表明:相对于Linux操作系统(OS)默认的调度策略,该文的方法能平均提高系统吞吐率3.7%,对于某些特定程序组达8.5%.关键词进程调度;访存带宽;总线竞争;跨执行优化1引言随着制造工艺、功能复杂度、散热、能耗等因素限制,计算机已经转入了多处理器、多核的时代.共享存储的多核系统作为一种高效的构建并行计算系统,在高端服务器、个人电脑、嵌入式系统,甚至移动计算设备中都有广泛的应用①.在这类系统上,片外访存总线被多个处理器/处理器核共享争用,导致访存延迟增大,从而成为一个重大的性能瓶颈[1].目前工业界的趋势是将越来越多的处理核集成到一个处理器上[2],而总线带宽由于受到封装和工作频率限制,已经接近了实际极限[3].因此,访存带宽将持续成为制约并行系统性能的重要因素.当系统中并发程序数超过可用的处理器核数时,相关工作提出了访存带宽敏感的调度策略[4-8],它根据每个程序各自的带宽需求选择合适的程序并发,以缓解它们对访存带宽的竞争.作者本人已在先前的工作[9]中定量研究了程序带宽需求波动对并发程序性能的影响,并提出了一个新的带宽敏感调度原则:如果每个调度时间片内并发程序的总带宽需求都保持为程序组的平均带宽需求,那么程序组所遭受到的性能下降幅度将最小.上述策略能更有效的提高系统吞吐率,然而它需要用户提供程序执行的先验知识,以便在程序组执行前估算其平均带宽需求,这给系统用户提出了额外的要求,影响了策略的实用性.另一方面,大多数策略使用处理器的性能计数器(PerformanceMonitoringUnit,PMU),在运行时监测程序的带宽使用量变化,然后预测出它在未来时间片内的带宽需求.然而由于带宽竞争的存在,在运行时直接获得的程序带宽使用量通常会小于它的实际需求,并且下降比率不可预测.这使得程序带宽需求预测的精度较低,影响了调度效果.针对上述问题,本文提出了一个基于跨执行的优化方法,用以支持带宽敏感调度策略的进行.该方法利用了程序执行过程中的阶段性行为(称为程序phase),在程序的多次执行中不断收集其包含的phase,并估算每个phase的独占执行信息,存放到数据库中.在程序的未来执行中,使用数据库中的信息能更准确的预测程序的带宽需求;同时,我们结合phase出现的频度,估算出程序在特定输入集下的整体独占执行时间和主存访问量,最终能在一个程序组执行前估算出它的平均带宽需求指导调度.上述步骤无需用户干预,能够自动进行.本文的实验在基于Intel处理器的8核系统上进行,测试结果表明,相对于操作系统(OS)默认的调度策略,本文提出的方法可以获得平均3.7%的系统吞吐率提升,对某些程序组可达8.5%.与文献[9]提出的方法相比,本文的工作使得带宽敏感调度不再需要任何用户制导信息和离线训练,并且调度效果也获得了提升.本文的贡献包括:(1)提出了一个支持带宽敏感调度的跨执行持续优化方案,对用户透明,并且提升了带宽调度策略的效果.(2)使用运行时独占性能估算技术,解决了带宽需求预测以及程序整体执行时间估算的问题.(3)通过实验讨论了共享cache竞争对带宽敏感调度策略的影响.比较了不同信息获取方法对调度性能的影响.本文第2节介绍研究背景和动机;第3节介绍跨执行优化方法设计以及其中关键问题和解决方法;第4节介绍系统实现;在第5节进行测试和结果分析;第6节总结全文.2研究背景和相关工作2.1访存带宽敏感的任务调度带宽敏感调度策略可以有效缓解访存带宽竞争,且相对容易实现,不需要特殊硬件支持,已有很多相关工作对其做了研究[4-8].这些策略的共同基本思想是,在每个调度时间片内选择合适的程序并发,使得它们的带宽需求之和恰好等于系统的实际峰值,其动机是既不造成总线饱和也不浪费可用带宽,提高系统吞吐率.然而我们在工作[9]中发现,实际程序的访存带宽需求在远小于调度时间片的粒度上剧烈波动,即使并发程序在时间片内的平均带宽需求之和小于系统峰值,它们内部仍然会造成明显的带宽竞争,导致性能下降.通过进一步的测试,作者发现了一个带宽竞争的基本规律:当并发程序总的带宽使用量线性提高时,竞争导致的性能下降会超线性增加.最后通过数学证明得到了新的带宽调度原则:对一个程序组,如果它的每个执行时间片内并发程序的带宽需求之和①TheNVIDIATegra2mobileprocessor.http://www.Page3都等于程序组的平均带宽需求,或者说,程序组造成的访存带宽压力在所有时间片中均匀分布,那么该程序组所遭受的性能损失将最小.为了实现上述调度思想,需要在程序组执行前估算它的平均带宽需求.假设系统中有C个处理器核,有N个程序并行执行(N>C).对一个程序j,它的独占执行时间为Tj,主存访问数为Mj.则该程序组的主存访问总数为然后,我们假设该程序组的执行不会因为带宽竞争带来任何性能下降,从而估算出它的理想执行时间IdealTurnaroundTime=∑N-C其中Tni(1iN)是把所有程序按照执行时间从短到长重新排列后获得的序列.上述公式模拟了程序组在时间片轮训策略下的理想执行时间.最后,程序组的平均带宽需求(IdealAverageBandwidth,IABW)定义为程序组中包含的所有主存访问数除以它的理想执行时间.换句话说,IABW是让该程序组达到理想执行时间所需要的最小带宽,因此它能刻画一个程序组对访存带宽的需求.IdealAverageBandwidth=TotalMemoryRequest在运行时,调度策略会预测出程序在下一个时间片的带宽需求,在调度时选择合适的程序并发,使得它们的总带宽需求恰好等于程序组的IABW.2.2新带宽敏感调度策略引入的问题需要解决:为了实现上述调度策略,有两个重要的问题仍(1)为了在执行前计算程序组的IABW,需要获得每个程序独占执行时间和平均带宽需求,这需要实现对程序进行独占测试,或者对程序的执行行为有较为精确的预测,而且程序输入集的变化也使上述信息的获取更加困难.另一方面,受到共享cache的影响,主存访问数量和独占执行相比也会不可预测地增多.(2)为了预测程序在下一时间片的带宽需求,我们需要获得程序的带宽需求历史.然而由于程序并发时存在带宽竞争,通过PMU直接获得的带宽使用量可能远小于它的实际需求(即程序在独占执行时的带宽).历史信息的偏差导致程序带宽需求预测误差较大,误导调度.本文提出了一个对用户透明的跨执行优化方法,来解决上述问题,支持带宽敏感调度策略的进行.2.3相关工作Tian等人对单线程程序跨执行优化技术进行了一系列研究.在文献[10]中首先讨论了如何从不同的程序输入集中抽取特征,使得程序在同一个特征的输入集下行为相似,因此可以收集程序的执行历史决定较好的动态优化选项.但该工作需要为程序收集多个代表性的输入,并通过离线学习的方法获得程序输入特征和优化选项之间的关系.在后续工作[11]中,他们设计了一个对用户透明的跨执行优化框架,自动进行上面的工作,并主要讨论了开销控制、学习算法改进等关键问题.上述工作针对单个程序的跨执行优化,收集的信息是程序的输入特征,用来指导其动态优化选项的确定;而本文针对并发程序的优化,因此收集的信息是程序在时间片粒度上的阶段性行为(phase)和性能信息,优化手段是程序调度,提高吞吐率.在本文的跨执行优化方法中,需要在程序并发执行时,估算出它的独占执行性能.并发程序对共享存储资源的竞争使得它们的并发性能变化不可预测.Mutlu等人在文献[12-13]中对内存控制器做出改动,设计了复杂的硬件计数器和触发条件来估算程序独占执行时的内存阻塞时间.然而存储设备的工作机制非常复杂,并且和处理器的流水线之间存在交互,因此构建一个精确的性能分析模型仍然是非常困难的.在本文中,我们使用了一个基于程序阶段性行为(phase)的估算方法,它不需要特殊硬件支持,并且实验结果表明具有较高的精度.3支持带宽敏感调度的跨执行优化图1给出了本文的跨执行优化系统总体设计.为了预测程序带宽需求以及估算程序整体独占执行时间,都需要在程序并发执行时反推出它的独占执行性能.相关研究表明,程序在执行过程中,其行为会发生阶段性的变化并且重复出现[14-15],表现在片上cache缺失率、主存访问密度以及IPC等多个指标上.本文利用这一规律,提出了一个基于程序阶段性行为(phase)的估算方法.首先在运行时通过phase划分技术把程序在一个时间片内的执行识别为一个phase(赋予它一个唯一的PhaseID),而不Page4同时间片的执行可能属于同一个phase,并且它们的独占执行性能相同或足够相似.只要我们知道一个phase中某个时间片的独占执行性能,我们就获得了该phase的独占执行性能.再结合phase预测技术,就可以预测出程序在下一个时间片的带宽需求.怎样获得属于某特定phase的一个时间片的独占执行性能?直接的方法是通过预测得知该phase即将出现(即程序下一个时间片的执行会是该phase),然后临时修正调度策略让该程序在下个时间片中独占执行,然后通过PMU信息获得它的执行性能.然而,这个方法会造成处理器空闲和开销.实际上我们发现,即使通常情况下共享资源竞争带来的并发性能变化是不可预测的,但至少在3种“低竞争”执行状态下,一个程序的并发执行性能和独占执行性能相同(或足够接近),因此我们可以在避免处理器浪费的前提下获得一个时间片的独占执行性能.基于上述思想,在程序运行时,我们根据PMU获得的基本信息,进行phase识别及其独占执行性能估算工作,并在程序结束时将该信息更新到一个数据库中.同时我们根据程序中每个phase的出现频度及其性能信息估算出整个程序的独占执行时间;而程序的主存访问数也可以在程序执行结果后直接获得,同时还包含了共享cache的影响.对于一个未知的程序组,如果它包含的每个程序在数据库中都有相应的独占执行时间和主存访问量估算,那么就可以估算出程序组的IABW,指导调度.上述几个步骤互相支持,形成了一个持续进化的过程:在数据库中信息的帮助下,带宽敏感调度算法可以更有效的缓解带宽竞争,加速程序组的执行;另一方面,通过程序组的多次执行,更精确的信息被收集到数据库中,从而服务这些程序将来的执行.在下面的章节中,我们分别介绍上述几个关键技术.3.1访存带宽敏感的任务调度在作者本人尚未发表的工作中,我们发现了并发程序的3种“低竞争”执行状态,并利用它估算程序的执行进度,保证系统的性能公平性和服务质量(QoS).在本文中,我们提出了这个观察结论的另一个重要用途:程序的带宽需求预测以及程序整体独占时间估算,以支持带宽敏感调度策略的进行,提高系统吞吐率.在本节中,我们结合实际系统测试对“低竞争”执行状态做简要说明.图2给出了测试系统的框架图,该系统包含两个IntelXeonE5410四核处理器,其中每两个处理器核共享最外层数据cache(LLC).我们使用总线事务速率(BusTransactionRate,BTR)来度量带宽,它指每微秒能够传输的cache行总数.该系统可提供的峰值带宽为120trans./usec.我们使用SPECCPU2006测试集在该系统上进行了随机并发测试.我们称一个程序在一个时间片内的执行部分为一个程序段.测试使用的观察粒度为100ms,和Linux默认的调度时间片长度相同.对于一个程序段q,我们使用PMU获得它的并发执行性能IPCq及它所在时间片的系统级带宽利用率(即所有并发程序的带宽利用率之和),记为BTRq程序段并发执行性能和BTRq关系,我们通过事先的profiling工作,获得该程序段的独占执行性能IPCq相对性能:Speedq=IPCq需要注意的是,上述基于profiling获得程序独占性能的方法只是为了验证3个“低竞争”执行状态的正确性,它不是本文跨执行优化方法中的一部分.在本文的方法中,“低竞争”执行态的识别能在程序并发执行时自动进行,无需任何事先提供的信息制导或其它用户干预.图3给出了在处理器核0,1,4,5私有cache模式下的并发测试结果,数据为随机选择的1000个程序段,具有代表性.实线为这些数据点的趋势线.上面3个子图分别对应了3种“低竞争”状态:Page5(1)一个程序段的自身带宽需求非常低.图3(a)显示出,随着程序段主存访问密度的增加,它的相对性能下降幅度也慢慢增大.在本文中,我们设置一个阈值BTRSelfLow为峰值带宽的4%,如果一个程序段的带宽使用低于这个阈值,我们就可以假设它的并发执行性能和独占执行性能非常相似.(2)一个程序段的自身带宽使用超过了BTRSelfLow,但是它执行过程中的系统级带宽利用率较低,这说明它所处的带宽竞争环境相对宽松.图3(b)表明当系统级带宽利用率上升时,一个程序段遭受的性能下降也有增大的趋势.如果一个时间片的系统带宽利用率小于预设的阈值BTRSyswideLow(实验中设置为峰值带宽的1/3),那么所有并发程序段都满足“低竞争”执行状态.(3)系统级带宽使用超过了BTRSyswideLow,但是在所有并发程序中,最多只有一个程序是访存密集型的(即它的带宽使用超过BTRSelfLow).其动机是,只有当系统中至少存在两个访存密集型程序时,他们之间才会发生激烈的带宽竞争,造成严重的性能下降.图3(c)从所有测试数据点中筛选出了属于这种情况的子集,可以看到,使用这个规则可以有效的选择出那些自身带宽需求很高但是仍然没有遭受严重性能下降的执行情况.在处理器核0,1,2,3上进行的四核共享cache测试也显示了同样的趋势.相关工作[16]也提出共享cache竞争并不是造成并发程序性能下降的主要因素;主存系统部件,如访存总线、预取等竞争才是主要的因素,因此在共享cache条件下上述结论仍然成立.3.2程序phase识别和phase性能信息维护相关工作指出程序的阶段性变化和它所执行的代码区间密切相关[15,17-19],基于基本块向量(BasicBlockVector,BBV)的方法是一种很有效的phase识别方法[14,20-21].在本文中,我们设计和实现了一个基于BBV的运行时phase识别技术.我们把程序的一个执行时间片识别为一个phase.在程序的执行过程中,我们使用PMU对程序执行的指令地址(InstructionPointer,IP)进行采样.BBV的每一个元素对应一个程序基本块.每当一个IP采样落到一个基本块的地址范围中,它在BBV中对应的元素自加1,最后对BBV进行归一化.如果两个时间片形成的BBV的曼哈顿距离小于BBV相似度阈值,则这两个时间片就属于同一个phase.对每一个程序,我们维护一个phase信息表来记录其出现的phase及其历史执行信息.信息表的最终的目的是估算出每个phase的独占执行性能,包括IPCphase和带宽BTRphase.当一个程序在时间片q中执行结束,首先根据其IP采样进行phase识别,同时根据PMU数据计算出它的基本性能信息,包括总线事务速率BTRq以及系统级带宽使用率.然后我们检查该程序段的执行是否满足3种“低竞争”的情况之一.如果满足,则我们称它的执行性能信息为有效,否则称为无效.最后我们把当前时间片内的执行信息更新到phase信息表中.IPCphase和BTRphase的更新方法相同,我们以前者为例.在一个phase得到它第一个有效的执行历史之前,它的IPCphase被估算为所有无效历史的平均值.在该phase获得第一个有效历史信息时,所有旧Page6的信息被抛弃,随后它的IPCphase被估算为所有有效历史的平均值,获得的有效历史信息越多,该阶段的IPCphase估算准确率越高.由此可见,3种“低竞争”执行情况的作用是过滤掉无效的数据,只让有效的历史信息参与到IPCphase的估算过程中来.上述phase信息表在程序执行结束之后更新到数据库中,并在该程序未来的执行中重新载入,持续更新.可以发现,上述过程需要的数据都可以在程序并发时通过PMU实时获得,无需用户干预或事先准备的用户制导信息.3.3数据库信息精度自评价数据库中的信息随着程序的多次执行不断积累和强化.但在程序的前几次执行中,有可能并发程序竞争激烈,“低竞争”执行状态数量有限,导致数据库中信息精度较低.如果此时启动调度优化,这些不精确的信息可能会错误的指导调度,损害系统吞吐率.为了避免上述情况发生,我们制定了数据库信息精度自评价的机制.首先我们定义程序的有效信息覆盖率(或简称为覆盖率)为其获得了有效独占性能估算的时间片数量和它总的执行时间片数量的比率,该值在运行时自动维护.一个程序在多次执行中不断收集有效性能信息,覆盖率不断提高.当一个程序组中包含的所有程序的覆盖率超过一个设定的阈值,我们才会进行调度优化,否则将使用系统原有调度策略.我们称一个程序有效信息覆盖率提高至阈值的过程为数据库的初始化.数据库初始化在该程序的历史执行中自动进行,无需用户干预.同时我们希望该过程越短越好,以尽早地启动调度优化.初始化所要经历的执行次数和程序执行环境密切相关,相关测试将在5.2节中给出.3.4使用数据库信息数据库中收集的信息有两个作用:在执行前估算程序组的平均带宽需求IABW以及在运行时估算下一个时间片程序的带宽需求.程序组IABW估算:为了估算一个给定程序组的平均带宽需求,我们需要知道其中每个程序的独占执行时间以及主存访问数.程序的主存访问数可以在它执行一次后直接由PMU获得,我们使用程序多次执行的平均值进行估计.图4给出了每个SPECCPU2006程序在本文的测试系统上,独占cache执行以及和随机程序共享cache并发执行时的主存访问密度的变化情况(相对于独占执行时间进行了归一化),可以看到,程序在共享cache执行时主存访问密度增大,但和不同程序并发时增加量的变化范围相对较小,因此使用多次执行的平均值可以容忍这种波动,取得较为准确的估算.程序的独占执行时间估算方法如式(4)所示,它由该程序的每个执行时间片(从1~Q)的独占执行时间累加得到,其中Iq是指程序在时间片q内执行的指令数,由PMU模块直接获得;IPCqq的独占执行IPC,它通过所属的phase到数据库中索引获得.Calone=∑Q对于一个给定的程序组,当它所包含的所有程序都获得独占执行时间和主存访问数估算之后,就可以使用我们在2.1节中介绍的方法计算程序组的IABW.运行时程序带宽需求预测:我们使用马尔科夫预测器来预测每个程序在下一个时间片即将出现的阶段.我们把一个阶段看成一个状态,根据程序本次的执行历史维护一个状态转移矩阵.如果马尔科夫预测器没有足够的历史信息而不能产生预测结果(尤其是程序执行的开始阶段),我们推测下一个phase和当前phase相同.最后我们使用预测出的phaseID到数据库中进行索引,获得该phase的带宽需求.3.5不同输入集的影响程序输入集的变化会使程序行为发生改变.和输入密切相关的指标是程序独占执行时间和访存数,因此需要将它们的值和程序输入对应,分开记录.程序的输入包括命令行、输入文件的内容等,最直接的区分方法是对输入内容进行严格匹配,但该方法降低了信息的重用性.Tian等人[10]在相关工作中利用了程序在相似输入集下行为相似的特点,提出了使用“种子特征集合”(seminalbehaviorset)的Page7方法来刻画输入集.种子特征是在程序中挑选出来的执行信息,例如一个循环迭代次数、一个变量的值等,具有相同种子特征集合的输入会产生相似的程序行为.本文主要讨论了跨执行调度优化方法的设计以及信息获取、估算等关键问题,对程序输入特征的刻画在本文的讨论范围之外,但我们可以利用上述方法进行进一步探索,这是我们的未来工作之一.而程序phase的性能信息是可以跨输入部分重用的.这是因为程序的phase代表了程序在特定代码区域的执行,而程序的执行存在和输入无关的路径,或者在不同输入下共有的执行路径.4系统实现为了验证本文方法的有效性,我们在Linux系统上实现了一个用户级的进程调度器.对每一个提交到系统中的程序,调度器通过fork创建出该进程,同时使用perfmon库①为其绑定一个PMU采样上下文,然后读取数据库中该程序的历史信息,并估算当前程序组的IABW.调度器设置了一个计数器来测量调度时间片,当计数器被触发时,调度器将接收到SIGALRM信号,并在信号处理程序中完成下列调度工作:(1)使用ptrace_attach系统调用让程序暂停执行.(2)读取和处理PMU数据,识别phase并进行信息更新,预测程序在下一个时间片的带宽需求.(3)选择程序并发执行,使总带宽需求等于程序组的IABW.(4)对每一个选定的程序,使用ptrace_detach(5)重置调度器的时间片计数器,等待下一个为了验证跨执行优化的效果,我们设计了自定义的文件格式作为数据库存储程序的历史执行信息.5实验评价5.1测试平台和程序组构建测试平台和3.1节中介绍的相同,我们进行8核调度实验.表1给出了系统使用的默认参数.我们使用SPECCPU2006程序构建程序组.为了排除负载平衡对测量的影响,首先要对程序进行长度归一化:我们让每个程序在系统中独立执行10秒,并系统调度使之继续执行.时间片的到来.记录它完成的动态指令数.每当一个程序被选择加入到一个程序组中,与这10秒钟等价的部分将被执行.这部分执行包含了程序的初始化,会产生不规则的阶段变化,这给本文的性能估算和预测技术提出了更大的挑战;同时其长度也可以容忍误差.程序有效信息覆盖率可信阈值为了验证本文方法的通用性,使用的程序组都随机生成.我们定义程序组的并发度为其所包含的程序个数与CPU核数的比值.为了测试调度效果,我们生成了10个并发度在2~4之间的程序组,且具体组成也通过随机数决定.它们的组成和统计特征将在使用时给出.我们测量程序组的执行时间以显示系统吞吐率的变化.对比的对象是Linux2.6默认的调度策略.在该策略下,不同权重的非实时程序被赋予不同长度的时间片,而上述程序组中的程序在系统中默认具有相同的权重,因此该策略和时间片轮训(Round-Robin)策略具有等价的效果.另外,本文的工作是一个为带宽敏感调度策略提供性能数据的方法,因此我们也对比了在相同带宽敏感调度算法下,通过其它方式获取和处理性能数据时的效果.5.2程序phase执行信息收集效率和开销(1)时间开销和默认的Linux调度策略相比,我们的策略需要额外的数据收集和处理工作,时间开销主要包括PMU采样和调度策略本身(包括处理PMU数据、Phase相关工作以及调度算法)两个方面,它们进一步由PMU采样频率和程序静态phase数决定,是程序相关的.我们测试每一个SPEC程序在8个实例并发时在本文方法下的开销,测试结果如图5所示.相比之下,每个程序组在本文方法中的执行时间平均增加了3.19%.其中来自于调度策略的开销可以直接在系统中插入计时函数获得,为2.21%,这部分开销主要来源于PMU数据处理和程序Phase相关处理,这些工作可以很自然地分布在相应的处理器核上,因此随着核数增加而具有扩展性.①ThePerfmon2website.http://perfmon2.sourceforge.net/Page8调度算法本身的开销小于0.2%,它的时间复杂度为O(NC),其中N为并发任务数,C为CPU核数.剩余的时间开销来自于PMU采样,平均为0.98%.表2数据库初始化效率和精度(a)数据库初始化效率和精度(低、中负载)系统负载测量指标BTRphase数437.leslie3d52.1301459.GemsFDTD49.9322433.milc49.6351462.libquantum49.0181410.bwaves47.3272482.sphinx328.0141470.lbm434.zeusmp24.52134450.soplex20.4504429.mcf17.3241436.cactusADM15.7111483.xalancbmk14.1111403.gcc473.astar9.2381471.omnetpp9.1241481.wrf(b)数据库初始化效率和精度(高负载)系统负载测量指标BTRphase数437.leslie3d52.130459.GemsFDTD49.932433.milc462.libquantum49.018410.bwaves47.327482.sphinx328.014470.lbm434.zeusmp24.521337450.soplex20.450429.mcf436.cactusADM15.711483.xalancbmk14.111403.gcc473.astar471.omnetpp9.124481.wrf表2中“执行次数”表示在相应的系统负载下,数据库初始化所需要的执行次数;“估算精度”是数据库初始化完成后,实际独占执行时间和估算独占这部分开销天然分布在不同的程序中,因此也是具有扩展性的.(2)数据库信息收集效率根据3.3节的描述,只有数据库完成初始化(即程序的有效信息覆盖率达到80%)之后,才能在后续的执行中对它进行调度优化.数据库的初始化在该程序的前几次正常执行中自动进行,我们希望该过程越短越好.程序有效执行信息的收集速度和它所处的执行环境密切相关,在本节中,我们通过在不同系统负载(即程序组并发度)下随机向系统提交程序的实验方法来模拟评测数据库的初始化速度.测试结果如表2所示.9799979298989698979193869698999985939586869348978983949699949792执行时间的比值,它是程序每个phase性能估算精度的综合体现.表中的程序按照自身BTR从高到低排列,比481.wrf带宽需求更小的程序没有列出,因Page9为无论在什么负载下,它们仅需要一次执行就可以获得95%以上的覆盖率和信息精度.作为示例,图6给出了在中负载下每个程序的有效数据来源分布(3种“低竞争”状态之一或者没有获得有效数据),它表3测试程序组特征和组成程序组IABW程序数量WL#0110428leslie3d(2),bwaves(2),sphinx3(2),lbm(2),soplex(2),wrf(2),dealII(2),namd(8),gamess(4),WL#029716GemsFDTD(2),soplex(2),astar(2),omnetpp(4),hmmer(2),gromacs(2),namd(2)WL#0312230leslie3d(2),GemsFDTD(2),sphinx3(4),mcf(2),gcc(6),astar(2),omnetpp(2),calculix(2),WL#0411526GemsFDTD(4),milc(2),mcf(2),gcc(4),bzip2(2),hmmer(2),perlbench(2),h264ref(4),gamess(4)WL#059932leslie3d(2),libquantum(2),sphinx3(2),lbm(2),soplex(2),wrf(4),sjeng(4),gobmk(2),dealIIWL#0613118leslie3d(2),GemsFDTD(2),milc(2),calculix(4),sjeng(2),dealII(4),povray(2)WL#0712030leslie3d(2),sphinx3(4),lbm(4),soplex(2),mcf(2),gcc(2),omnetpp(2),wrf(2),gobmk(2),WL#0814228GemsFDTD(4),milc(2),libquantum(2),cactusADM(2),gcc(2),astar(4),sjeng(4),hmmer(2),WL#099729leslie3d(1),GemsFDTD(3),bwaves(1),sphinx3(1),soplex(1),mcf(1),omnetpp(1),wrf(3),WL#1013027图7给出了在数据库初始化结束之后,每个程序组在带宽敏感调度策略下连续40次的执行性能变化情况,包括程序组中所有程序的平均有效信息覆盖率、独占执行时间的平均估算精度和相对于OS默认调度策略的加速比.测试结果表明,每个程序组的平均有效信息覆盖率和估算精度保持稳定,并且有小幅度上升,这显示了本文的跨执行优化方法能在程序执行过程中不断优化获得更精确的数据;信息精度提升幅度不大的原因是,程序组并发度较高,而且其组成固定,因此在每次执行中获得的新的“低竞争”数量有限.在某些执行下,程序组的有效信息覆盖率比上一次执行略有下降,这是因为在该次执行中检测到了新的程序phase.总体来看,每个程序组在40次执行过程中的吞吐率获得了持续稳定的提升,对某些程序显示出计算密集型程序的大部分执行时间片满足“自身BTR低”的条件.而随着程序的带宽需求逐渐增加,它的执行所经历的“低竞争”执行态变少,因此数据库初始化需要更多的执行,并且有效数据的主要来源也变成“系统级BTR低”以及“1访存密集+多计算密集”.而对于同一个程序,随着系统负载增加,他们之间的带宽竞争也更加激烈,因此所需的执行次数也更多.5.3程序组调度效果为了评测本文提出的跨执行调度优化的效果,我们随机生成了10个并发度为2~4的程序组,表3给出了它们的组成和平均带宽需求.app1(x)表示程序组中包含x个并发的app1.组最大加速比可达8.5%,平均最大加速比为5.5%.程序组2(WL#02)在第9次执行时的性能发生了明显波动,加速比从8.10%减小到3.17%.通过统计我们发现前8次执行的程序组主存访问总量变化率在0.76%以内,程序估算精度的累积变化使得在第9次执行时的调度决策发生了变化,由于共享cache竞争度改变,使得程序组的主存访存量比前8次的平均值突增了2.88%,增大了估算的IABW和实际值的差距;在后续5次执行中维持了类似的调度决策,之后由于性能数据的进一步积累以及程序组主存访问量估算对波动的适应,最终程序组获得的加速比趋于稳定.5.4与已有方法效果的比较在本节中我们将本文的方法和其它两种方法进行对比,这些方法都使用了作者在工作[9]中提出的Page10图7程序组40次执行的性能带宽敏感调度算法,区别是程序性能信息的获取和处理方法不同.第1个方法是作者在工作[9]中提出的离线训练的方法,其基本思想是:由于带宽竞争,在运行时通过PMU收集到的程序带宽使用总是比它实际需求低,因此,我们可以将程序组的IABW修正得比实际值低,以维持同样的调度决策.为了确定程序组实际IABW和修正值之间的关系,我们需要事先生成一些具有代表性的程序组,并通过多次测试分别找出其取得最佳性能的修正值,并记录下它和IABW的对应关系,最后通过多项式拟合的方法获得IABW和修正值之间的函数关系式.对于一个新的程序组,我们首先计算出它的IABW,并根据上述关系式计算出修正值.而程序组IABW估算所需的信息仍要由用户提供.第2个对比方法是让每一个程序独立执行并收集详细的profiling信息,由此计算出精确的程序组IABW,并在动态运行时索引profiling取得精确的程序段带宽需求.该方法需要事先进行独占profiling工作,并且不能适应输入集变化,因此不具有实用性,本文只是通过该方法显示出带宽敏感调度策略在信息完全精确时的性能潜力.图8给出在上述方法下多次执行相对于OS默认策略的最大、最小和平均加速比.基于离线训练的方法在一定程度上修正了动态性能信息的偏差问Page11题,因此获得了平均2.89%的加速,最大平均加速为4.08%.对于WL#10,程序组执行性能反而下降,这是因为基于离线训练获得的IABW和修正值的关系是多个数据的拟合结果,并不能对任意程序组都具有最佳效果.对10个程序组中的6个,本文方法的效果超过了基于离线训练的方法,原因是前者能在程序多次执行中获得足够精确的信息,并且容忍了共享cache竞争带来的影响;对于WL#02、WL#05、WL#07和WL#08,本文方法的平均加速比低于基于离线训练的方法,这是因为本文程序独占执行性能估算不能做到100%精确,其误差来源于“低竞争”执行性能的获取和phase识别的步骤;而基于离线训练的方法通过事先训练学习来确定程序共享执行性能和独占执行性能的关系,是另一种容忍性能数据误差的方式.最终本文的方法获得了平均3.73%的性能提升,最大性能提升为8.46%,平均最大提升为5.50%,其效果超越了基于离线训练的方法,更重要的是使得调度策略的进行不再需要任何额外的离线训练过程和用户提供的制导信息.基于精确profiling的方法能获得最精确的程序性能信息,因此获得了最好的性能提升,平均为7.42%,最大平均为7.96%.对于WL#02,基于profiling数据在某次执行下的最大加速比略低于基于跨执行优化的方法,这是因为前者即使获得了精确的性能信息,但在程序phase预测(也就是程序带宽需求预测)过程中仍存在误差,而后者在数据精确略低的情况下仍然有概率在某次执行中产生接近于最佳的调度决策.图8基于profiling、离线训练方法和跨执行优化支持下的6总结和展望访存带宽竞争是共享存储多核系统的重要性能瓶颈.带宽敏感的任务调度可以有效地缓解访存带宽竞争,提高系统吞吐率,但它的进行需要程序性能先验知识,和较为准确的带宽需求预测.在本文中,我们提出了一个低开销、对用户透明的跨执行优化方法,该方法利用了程序执行的阶段性(phase)特点,能在历史执行中识别程序phase,并估算它的执行性能,这些信息又能在程序未来的执行中不断强化,指导调度.基于Intel多核处理器的测试结果表明,本文的方法可以使系统吞吐率平均提升3.7%,对某些程序组可达8.5%.它使得带宽敏感调度策略的进行不再需要任何用户信息制导,同时改善调度效果.本文的工作对于在操作系统中透明的集成带宽敏感调度策略具有重要的意义.作为未来工作,我们准备进一步研究输入特征及其对程序性能的影响,我们还准备使用跨执行优化方法支持更多的动态优化.致谢在此,我们向对本文的工作给予支持和建议的老师和同学表示感谢!美国明尼苏达大学的游本中和中国台湾交通大学的徐慰中教授对本文研究内容给予了启发和指导,研究小组同学对本文提出了宝贵意见,在此一并感谢!
