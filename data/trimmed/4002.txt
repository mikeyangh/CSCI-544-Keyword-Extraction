Page1基于自适应特征融合的自然环境视频行为识别郭梓鑫1)衣杨1)李汉巨2)1)(中山大学信息科学与技术学院广州510006)2)(广东电网公司东莞供电局信息中心广东东莞523008)摘要文中针对无约束环境下现实人体行为识别的难点问题,提出了一种自适应特征融合算法.首先,通过图像特征点跟踪和背景轨迹剪除,获得可靠的特征点轨迹,并从中计算得到一组丰富的动作描述符,以同时保留轨迹的局部运动信息、形状以及静态外观信息;然后,采用词袋(BagofWords,BOW)模型,将视频序列表示为视觉词语频率直方图;最后,为应对摄像头移动和静止情况下,视频中动作特点的剧烈变化,提出自适应的特征融合策略,即根据摄像头的动静情况,选择性地将基于特征点轨迹的描述符与基于3D兴趣点的描述符进行融合.通过在2个现实人体行为数据集上的实验并与当前已有算法的比较,证明了文中算法的有效性.关键词行为识别;特征点轨迹;时空兴趣点;词袋表示;特征融合1引言随着人体行为理解研究的不断发展和深入,当前的研究兴趣已从拍摄环境良好控制下的简单行为识别,转移到故事性电影、体育广播录像和YouTube家庭录像(见图1)等无约束环境(即自然环境)下更为现实的行为识别.然而,由于杂乱背景、摄像机运动、Page2遮挡、显著的类内变化等因素的影响,现实视频中的行为识别仍然是一个复杂且极具挑战性的任务.例如,在一段现场拍摄的滑冰比赛视频中,由于运动员在滑冰场上迅速移动,摄像机也需要同时移动以跟随运动员,捕捉其细节动作.对于这种场景下的行为视频,传统的视觉算法[1-6]很难获得较好的运动识别、分类和定位效果.图1自然环境下的人体行为视频(从左到右:自然环境下视频中的行为识别主要难点在于:(1)自然环境下的行为视频通常面临更大程度的多物体遮挡、阴影、背景杂斑以及光照、尺度、视角上的剧烈变化等现象,导致特征提取成为一个严重的难题,因为必须针对感兴趣的行为计算并选择信息量更大、判别能力更强、鲁棒性更高的视觉特征,同时忽略由背景杂斑和其它运动物体产生的无关特征.(2)由于拍摄过程不加任何限制条件,摄像头可能是静止的,也可能是运动的,这两种状态以难以预料的方式混合出现.特别的,在摄像头相对背景移动的情况下,动作特征将由待识别运动和杂乱背景两者共同产生.这样会明显降低所提取特征的有效性,从而对识别效果产生不良影响.因此,必须采用有效的特征剪除方法,去除对应于背景的无关特征.本文的研究目标在于提出一种新的运动表示方法,从一定程度上解决上述问题.目前,已有部分研究工作尝试对无约束视频中的人体运动进行识别.Liu等人[7]从视频中同时提取局部动作和静态特征,由于这两类原始特征包含噪声,再通过统计学方法获得稳定的动作特征和不含噪声的静态特征.然后,采用PageRank从特征空间中选择最具信息量的静态特征.该算法在YouTube现实行为数据集上取得了很好的识别效果.Laptev等人[8]研究的是故事性电影中的现实人体行为识别.从电影剧本中收集包含待识别动作的录像片段,构成一个复杂的行为数据集,并提出了基于局部时空特征、时空金字塔模型和多通道非线性SVM的运动表示和行为分类算法.相对于YouTube视频,其所用的电影录像质量较好,并且不包含摄像机运动带来的背景变化.另外,这些录像中的大多数行为是非周期性的.Sun等人[9]提出以分级的方式对时空上下文信息进行建模.将时空上下文抽象为3个级别:图像特征点上下文(SIFT描述符)、轨迹内上下文(轨迹状态转移描述符)、轨迹间上下文(轨迹邻近度描述符).对后两个级别,为获得有效的简洁描述,将时空上下文信息集成到马尔科夫过程的转移矩阵中,并提取其稳定分布作为最终的上下文描述符,同样获得了优越的现实行为识别性能.Kovashka等人[10]针对现有词袋(BagofWords,BOW)模型在表达特征时空关系上的缺陷,提出一种基于时空特征邻域形状的运动表示方法.对于给定的一组训练视频集,首先提取局部动作和外观特征描述符,并量化成视觉词汇.然后,形成包含与临近点相关联的视觉词语以及临近点相对于中心兴趣点的方向的候选邻域.接着,将这些大小可变的邻域产生的描述符递归式地映射至高层词汇表,从而产生关于特征时空分布的分层描述.上述行为识别方法均典型地采用了局部图像特征作为运动表示.这些特征可以从时空兴趣点提取得到[6-8,10-11],也可以从特征点轨迹提取得到[9,12-13].时空兴趣点对应于视频序列的局部时空显著区域,从中可以同时获得局部的动作和外观特征.然而,从时空兴趣点获得的特征仅仅捕捉到运动目标在极短时间内的动作信息,具有一定的瞬时性,因此不足以描述长时间的更复杂的人体动作.另一方面,较长时间的动作特征可以从图像特征点轨迹中提取得到.然而,这种方法往往容易受低质感环境的不良影响(特征点数目不够),并且忽略了静态外观信息,导致难以识别动作较为细微的行为类别.因此,基于轨迹的描述符和基于时空兴趣点的描述符所包含的信息是互补的.但是,目前的绝大多数行为识别研究工作仅仅考虑了其中某一类特征,而对于这两类特征的结合却鲜有报道.为此,本文提出了基于自适应特征融合的现实行为识别算法.算法主要包括以下4个步骤:(1)通过图像特征点跟踪和背景轨迹剪除方法,获得前景区域内可靠的特征点轨迹;(2)计算一系列对尺度、平移、旋转等具有不变性的轨迹描述符,以及局部时空描述符,两者结合后可以形成一组用于运动表示的富含信息量的动作描述符;(3)采用典型的词袋表示方法[4,8-10],将视频序列表示为视觉词语频率直方图;(4)运用光流计算的思想自动检测摄像头的状态,并根据检测的结果,对基于轨迹的描述符和基于时空兴趣点的描述符这两者进行自适应融合.算法的整体流程如图2所示.Page3图2本文算法的流程(中间的矩形框表示自适应特征融合的过程)与现有技术相比,本文的贡献在于:(1)采用基于轨迹相异度度量的背景轨迹剪除方法,并通过对特征点轨迹分布的统计学分析进行感兴趣区域(RegionofInterest,ROI)定位;(2)同时考虑了从特征点轨迹和时空兴趣点提取到的两类描述符,形成更具信息量和判别力的运动表示;(3)提出自适应的特征融合策略,有效地增强模型在摄像头运动情况下对特征噪声和背景杂斑的鲁棒性.2特征点轨迹描述符给定一段xyt-空间中的视频序列犞,首先通过图像特征点跟踪和轨迹剪除获得可靠的特征点轨迹集{狋1,…,狋N},作为时空曲线,然后基于这些曲线计算三种轨迹描述符{犗,犉,犛},即方向-模长描述符、傅里叶描绘子和SIFT均值描述符,分别用于捕捉轨迹的局部动作信息以及形状、静态外观信息.算法从空间显著点轨迹提取开始.2.1特征点跟踪特征点跟踪是在当前帧中提取特征点,然后在后续帧中匹配这些特征点以预测和估计特征点的运动轨迹.本文采用两种方法提取特征点的轨迹,分别是PyramidLucas-Kanade-Tomasi(KLT)特征跟踪器[12-13]和SIFT匹配算法[9].这两种跟踪算法独立地应用到视频序列中,以便在分辨率较低的视频中也可以获得尽可能密集的轨迹.轨迹提取的过程是基于连续帧中图像特征点的配对.对于一段k帧的视频序列,记为犞={犳1,…,犳k},在犳i和犳i+1之间建立特征点匹配关系,1ik-1,并由延续多帧的配对构成特征点的动作轨迹.为了减少由错误匹配导致的伪轨迹的产生,对匹配过程加以唯一性约束,并舍弃距离过大的配对,因为大多数现实动作不会进行得非常快.具体来说,对于帧犳i中的任意显著点p,在帧犳i+1中最多只能有一个候选的点p跟它匹配;并且p必须位于p点周围的N×N空间窗口内(在所有的实验中,N=64).在这种约束下,当轨迹到达镜头边界或者伴随有较大遮挡时,便会自动终止,然后算法重新开始跟踪新的轨迹.另外,为进一步移除不可靠的轨迹,并降低较长轨迹与连续动作混淆的可能性,如人起立和行走,在跟踪过程中,将任意有效轨迹的长度L限制为LminLLmax.实验中,设置Lmin=5,Lmax=25.图3显示了运用该算法生成的特征点轨迹.2.2轨迹剪除由上述方法提取得到的轨迹对于行为识别并不总是有用的.从图3可以发现,大量的轨迹来自背景Page4图3特征点轨迹示例(图中的轨迹是通过连接同一轨迹区域,必须将其去除,以保留对人身体动作描述最相关的轨迹(如图4).为此,考虑轨迹剪除过程.本文采用基于特定时间窗口内轨迹相异度度量和感兴趣区域(ROI)检测的轨迹剪除方法.具体如下.假设存在N条以帧f为起点的轨迹:犜={狋i},i=1,…,N.对每条轨迹,定义一个时间窗为5帧的轨迹段狋i={(xi(xif+3,yi犱i={dif+k-yiyi轨迹对应的位移向量犱i和犱j,我们计算其相异度,从而形成一个N×N的矩阵犆:则轨迹狋i的相异度计算为mi=∑N了长度为5帧的时间窗内,该轨迹与所有其他以帧f为起点的轨迹之间的相异度.对于运动目标较小的视频片段,本文认为,当一条轨迹与其他轨迹很相似时,它便很可能是从动态背景中提取得到的,是不可靠的,必须去除.因此,对帧f,计算一个自适应阈值Mf移除所有相异度小于Mf经过上述条件约束后,假设共有Nf条可靠的轨迹穿越帧f,则可以通过对所有可靠轨迹在帧f中的位置坐标求均值,获得ROI的中心:TH=γ其尺寸则由以下式子给出:其中cxx和cyy分别是特征点空间坐标在x方向和y方向上的二阶中心矩.在相对稳定的背景下,该方法可以获得较好的运动定位效果.然后,移除所有位于ROI之外的轨迹.图4显示了经过剪除后保留下的轨迹.可以看到,感兴趣区域的检测结果可以正确地定位到发生明显运动的区域.不同于基于二维兴趣点检测的ROI检测算法[7],本文的运动定位方法是基于特征点轨迹分布的统计学分析,不需要显式的目标检测和跟踪过程,并对摄像机运动情况下的视频具有一定的鲁棒性.2.3轨迹特征提取2.3.1方向-模长描述符给定同一条轨迹上任意两个连续的点P和P,其位移向量为犇=PP---→(见图5).注意到犇的时间分量是确定的(即帧间隔),所以在后续讨论中忽略该分量,并假设犇只包含空间分量,记为犇=(Δx,Δy).考虑分别对向量犇的模长和方向进行量化.对于模长,为了使量化结果具有尺度不变性,先用相同轨迹上最大的位移量犇max标准化犇,然后采用4个均匀的量化级别;对于方向,将上半圆和下半圆等分成8个扇形,每个22.5°,如图5所示.结合模长和方向这两种量化,每条轨迹可以相应的描述为一个32-bin的直方图犗.图5方向-模长描述符:将轨迹量化并转换为直方图2.3.2形状描述符本文利用傅里叶描述子来作为轨迹的形状描述符.首先考虑两种原始的形状特征:中心距离和复坐标.前者用于捕捉形状的整体特征,后者用于捕捉形状边界的局部特征.这两种形状特征已被应用于多数文献中,并显示出了可观的实用性[14].假设轨迹上包含L个特征点{(x0,y0),Page5(x1,y1),…,(xL-1,yL-1)}.复坐标即是由点坐标产生的复数:为使形状描述对平移具有不变性,将坐标函数转化为其中(xc,yc)是该轨迹的中心:中心距离函数则表示为轨迹上特征点与轨迹中心的距离:由于减去了中心的坐标,因此使用中心距离进行形状描述对平移也具有不变性.然后,对上述两组形状特征数据进行傅里叶变换,经变换后得到的傅里叶系数形成轨迹形状的傅里叶描述符.它们从频域的角度表示形状信息,其低频部分描述形状的大体特征,高频部分反映形状的详细特征.给定上述形状特征s(t),t=0,…,L-1,假设它已经被标准化为N个点,则s(t)的离散傅里叶变换为un=1N∑N-1系数un(n=0,1,…,N-1)称为形状的傅里叶描述符,记为FDn(n=0,1,…,N-1).由于上述的两种特征对平移都具有不变性,因此对应的FD也具有平移不变性.同时,通过忽略相位信息,仅考虑FD的幅值,可以使FD对旋转也具有不变性.对于复坐标特征,N设为30.第一个描述符FD0(直流分量)只依赖于轨迹的中心位置,对描述形状不起作用,因此删去.再将其余的N-1个描述符的幅值除以描述符FD1的幅值,以获得尺度具有不变性:犳1=FD2对于中心距离特征,N设为60.由于函数(7)求值结果为实数,傅里叶变换后只有N/2个不同的频率,因此只需一半描述符来表示形状[14].将前半部分FD除以直流分量FD0,以获得尺度不变性:犳2=FD1最终,结合上述两种傅里叶描述符,每条轨迹的形状描述符可以表示为一个58维的向量犉=[犳1,犳2].注意到傅里叶描述符与之前的方向-模长描述符有很大的不同:前者包含轨迹整体动作信息,后者则包含轨迹段局部运动信息,两者互为补充.2.3.3局部外观描述符在场景和物体识别任务中,图像帧中隐含的空间上下文信息可以为出现的场景或物体提供语义类别上关键的判别作用.相似的,对于视频序列中的行为识别,这种信息对限制行为类别也是非常有用的.本文采用Lowe[15]提出的SIFT(ScaleInvariantFeatureTransform)描述符对轨迹上的显著点形成图像局部特征描述,从而捕捉其空间上下文信息.具体来说,给定任意长度为L的轨迹,对该轨迹上所有的特征点提取SIFT特征犛i(i=1,…,L),则局部外观描述符犛计算为注意到犛是128维的特征向量.2.3.4基于词袋模型的轨迹整体表示上述计算为视频空间中的每条轨迹生成了犗、犉、犛3种独立的描述符.为便于视频序列的有效表示和处理,本文采用词袋表示方法.由于这些描述符的数值范围存在较大差异,为克服该差异对后续操作的不良影响,考虑使用Z分数作数据转换,使其平均值变为0,标准差变为1.对每条轨迹,连接标准化后的犗、犉、犛,即形成该轨迹的整体描述向量犌=[犗,犉,犛],犌的维数是218(32+58+128).视频的词袋表示由以下3个步骤完成:(1)通过K-means算法对训练集中所有轨迹的整体描述子犌进行聚类,构造一个规模为500的视觉词典.这里将聚类得到的簇中心作为视觉关键词,并由它们构成视觉词典.然后,将每个描述子犌赋值为与它距离最近的视觉词语,这样,即是给视频空间中的每条轨迹赋予一个簇标号(1~500).(2)为了弥补词袋表示模型在描述轨迹时空分布上的不足,保留轨迹的时空信息,本文采用时空网格(Spatio-temporalGrid)的方法[8]来描述轨迹特征的分布.注意到经过2.2节的轨迹剪除后,所有的轨迹都落在了ROI空间中.将视频序列的ROI时空体划分为4个非重合的空间块和2个部分重合的时间块(重合量为视频长度的1/3),即4×2=8个块,如图6所示.Page6图6时空网格示意图(其中T1、T2分别表示时间块(3)采用直方图量化技术,对落在每个时空块中的轨迹集形成一个500-bin的直方图,用于统计该块中各视觉词语出现的频率.由于有8个时空块,所以最终生成一个500×8=4000维的特征向量(记为犉犞1)来描述整段视频序列中的所有轨迹.3时空兴趣点特征除了轨迹特征,本文还考虑了基于时空兴趣点(Space-timeInterestPoint)的局部特征,因为它包含了与轨迹特征互补的信息.对于时空兴趣点的检测,本文采用文献[11]的算法.相比其他检测器[6],该方法在细微的摄像头移动、背景杂乱、尺度变化、阴影等现实环境下所检测到的兴趣点更为可靠.兴趣点的检测包含两个步骤.首先通过帧间差分法检测运动区域;然后,对帧间差分结果使用一系列不同的2DGabor滤波器进行多方向Gabor滤波,兴趣点便是视频帧中获得滤波响应极大值的位置.另外,为了降低兴趣点错误检测的影响,根据连续4帧中候选兴趣点的空间分布情况来估计其集中分布区域,并剔除远离分布边界的离群点,从而获得可靠的兴趣点集.图7显示了对部分视频序列进行时空兴趣点检测的结果.可以看到,该方法所检测到的时空兴趣点可以正确地定位到视频序列中具有明显运动的区域.采用Dollar等人[6]提出的Cuboid描述符对检测到的兴趣点形成特征描述.首先,对每个兴趣点提取一个3D视频块.然后,为描述该立方块,对块中的每个像素计算梯度值,并将计算结果连接形成一个特征向量.其次,使用主成分分析方法(PrincipalComponentAnalysis,PCA)降低特征描述符的维度.最后,再次使用词袋模型来表示每个视频片段.由于整段视频的兴趣点特征空间通常很大,为了降低时间和存储上的开销,仅仅针对训练数据特征集合的随机子集进行K-means聚类,建立具有500个词汇的视觉词典,再为每个兴趣点赋予视觉词语,并量化成500-bin的直方图,即维度为500的特征向量,记为犉犞2.4自适应特征融合本文同时考虑了基于特征点轨迹的描述符和基于时空兴趣点的描述符.然而,这两类描述符受摄像头移动的影响程度不同.特别的,在摄像头静止的情况下,这两种描述符都可以比较可靠地计算得到;而在摄像头运动的情况下,基于3D时空兴趣点的描述符将显得很不可靠(如图8所示).为解决该问题,本文提出自适应的特征融合策略,根据摄像头的动静情况,选择性地融合这两类描述符.图8摄像头移动情况下时空兴趣点的检测结果(可以看首先检测摄像头的状态(静止或者移动).这里采用光流计算的思想.为了使计算简洁高效,我们重新利用了2.1节的特征点跟踪结果.假设视频序列犞中共有N条轨迹(前景+背景)同时穿越第f帧和第f+1帧:犜={狋j},j=1,…,N.取轨迹狋j上的两点:(xj量分别为Δxj帧f的平均轨迹位移量计算为f,yjf或者My若Mx含整体运动.对序列犞中的所有视频帧重复上述的运算,如果判断得多数帧包含整体运动,则认为该视频片段犞是通过移动摄像头拍摄得到的.实验结果Page7显示,当η=1.3时,该方法在多数情况下可以获得较为准确的镜头运动检测效果.然后,根据摄像头的动静情况,对视频片段的特征描述符作进一步处理.对于镜头静止的视频片段,基于轨迹的描述符和基于兴趣点的描述符都具有较高的可靠性,因此均用于行为识别,构成4500-bin的直方图(也即4500维特征向量)为犎=[犉犞1,犉犞2].相反,如果检测到摄像头移动,则基于兴趣点的描述符意义不大,只选择轨迹描述符,构成4000-bin的直方图犎=犉犞1.如此,将每个视频片段表示成相应的特征向量犎.5实验分析5.1数据集以验证算法对自然环境下人体行为的识别效果.实验使用了两个公开的现实人体行为数据集,图9UCF体育运动数据集和Kisses/Slaps数据集图像示例5.2实验设置(SVM),并选择χ2核[8]作为核函数:对于行为识别和分类,使用非线性支持向量机K(犎i,犎j)=exp-1其中,犎i={hin}V频率直方图,V为直方图的维数,A表示所有训练样本距离的平均值.由于SVM直接求解多类别分类问题比较困难,本文采用One-Against-All方法将多类别分类问题转化为两类问题求解.测试过程采用留一交叉方法(Leave-One-OutCross-Validation,LOOCV).即对于每个数据集的实验,取其中1个视频片段用于测试,其余的片段用于训练.重复这个过程,使得该数据集中每个片段都有一次被作为测试数据.5.3与最新成果的比较表1显示了本文算法在UCF体育运动数据集上获得的平均识别率,同时将识别结果与当前几个最新的算法进行了比较.分类结果的混淆矩阵如图10所示.在该数据集上,本文算法取得了86.67%的识别准确率,逼近于当前已知的最好识别效果(仅相差UCF体育运动数据集[16]包括10个来源于体育广播视频的运动类别:跳水(Diving)、踢球(Kic-king)、举重(Lifting)、骑马、跑步(Run)、滑板(SkateBoarding)、打高尔夫(Golf)、鞍马(体操)(SwingBench)、单双杠(体操)(SwingSideAngle)、走路(Walk),由150段不同帧速、不同像素的视频构成.这些视频片段在多种场景下拍摄得到,目标尺度、视角变化比较明显,并且固定摄像机和移动摄像机经常混合出现.每段视频平均时长5s.Kisses/Slaps数据集由Rodriguez等人[16]从一系列不同类型的经典故事电影中收集得到,包括92段Kissing类别的行为视频和112段Slapping类别的行为视频.这些视频片段中的行为执行者不同,拍摄的场景、视角变化较大,并且由于其电影的特点,摄像头的移动和切换比较频繁.每段视频的帧速和分辨率不同,时长为5s~15s.图9为上述数据集的示例.0.6%),并且明显优于Rodriguez[16]和Yeffet[19]等人的识别结果.图10混淆矩阵(UCF体育运动数据集,仅保留2位小数)Page8表2比较了本文算法和当前已有算法在Kisses/Slaps数据集上的实验结果.相比而言,本文算法获得了更为优越的识别性能,对Kissing和Slapping两类行为的识别结果均优于当前已有的算法,平均识别率达到了89.5%,远远高于当前最好的识别率80.75%.据我们所知,这也是迄今为止在该数据集上已公布的最好成果.Rodriguez等人[16]66.467.266.80Oshin等人[20]82.477.279.80Yeffet等人[19]77.384.280.75本文算法5.4背景轨迹剪除的有效性本文提出了基于轨迹相异度度量的背景轨迹剪除方法,为验证该方法对提高复杂场景下行为识别性能的有效性,本文对背景轨迹剪除前后的识别结果进行对比实验,如图11所示.对于Kisses/Slaps数据集,轨迹剪除前后的平均识别率分别为84.15%和89.5%,提高了大约5%.对于UCF数据集,通过背景轨迹剪除,平均识别率从77.33%提高到86.67%.除了Lifting和SwingBench之外,每个行为类别提高的幅度为4%~23%.这是因为,UCF数据集中Lifting和SwingBench这两类运动视频的拍摄场景多数是静态的,摄像头没有明显的移动,导致背景轨迹的数目很少,并且位移量很小,因此背景轨迹剪除的作用并不明显.而对于其他体育运动类别,特别是SwingSideAngle、Riding和Run,由于视频录制过程中,摄像头需要跟随运动员的位置而频繁快速移动,导致背景变化十分剧烈,产生了大量与前景(人体)运动无关的特征点轨迹,对行为识别造成严重的噪声干扰.利用本文提出的轨迹剪除方法,在一定程度上有效滤除了由动态背景导致的轨迹,从而使特征提取结果减少了噪声的干扰,显著地提高了识别性能.5.5自适应特征融合策略的有效性表3显示了各种轨迹描述符性能和自适应特征融合策略的有效性评估结果.其中各缩写词表示为:犗:方向-模长描述符,犉:傅里叶描述符,犛:SIFT描述符,犐犘:时空兴趣点描述符.从中可以看到,相比单独使用某一种轨迹描述符,犗、犉、犛三者融合后识别性能都获得了不同程度的提高.从表3还可以发现,相对于单独使用轨迹特征(犗+犉+犛),当基于轨迹的特征与基于时空兴趣点的特征以自适应的方式融合时,两个数据集上的行为识别效果均得到了显著的提高.然而,如果在摄像头相对背景移动的情况下,不考虑运动特征的可靠性,而将这两类特征无条件地融合,则反而出现识别性能降级的现象.特别是在摄像机移动更为频繁的UCF体育运动数据集上,这种性能的下降更为明显,达到9%左右,而引入自适应策略后,识别率得到了大幅度的提高(接近12%).表3各描述符性能和自适应特征融合有效性评估犗+犉+犛+犐犘无自适应74.60犗+犉+犛+犐犘自适应86.67分析表明,轨迹描述符和时空兴趣点描述符具有互补性,两者结合可以构成富含信息量的特征集合,有效地捕捉到运动在时间上的全局和瞬时动作信息.除此之外,还需考虑所提取特征的可靠性.对于自然环境下的行为视频,由摄像头的移动导致的背景变换现象较为突出,并且不同类型的局部动作描述符受摄像头运动的影响程度是不同的,因此对它们的组合必须以选择性的方式进行.实验证明,本文提出的自适应特征融合算法可以有效地应对摄像Page9头动态情况下动作特点的剧烈变化,增强模型对特征噪声和杂乱背景的鲁棒性.6结论针对自然环境下人体行为视频中背景杂乱、摄像机运动等难点问题,本文提出了一种新的基于自适应特征融合的现实行为识别算法.该算法同时考虑了基于时空兴趣点的描述符和基于轨迹的描述符,两者组合形成一组富含信息量的特征集,有效地捕捉到运动在时间上的全局和瞬时动作信息.为提高特征描述符在背景变化情况下的可靠性,提出了自适应特征融合的策略,根据镜头运动检测结果,选择性地将两类描述符进行融合.实验结果表明了本文算法在处理无约束环境下人体行为识别问题上的优越性能.
