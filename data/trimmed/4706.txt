Page1数据中心网络高效数据汇聚传输算法方兴1)1)(数学工程与先进计算国家重点实验室江苏无锡214125)2)(解放军信息工程大学国家数字交换系统工程技术研究中心郑州450002)3)(信息系统工程国防科技重点实验室(国防科学技术大学)长沙410073)摘要在数据中心中,类MapReduce的分布式计算系统在数据的混洗阶段产生巨大流量,令数据中心的东西向网络资源成为瓶颈.将这些高度相关的数据流在接收端进行聚合是分布式计算的通用处理方式,为了降低网络通信量并有效利用带宽,文中采用网内关联性流量的汇聚传输策略,将混洗和汇聚并行化,达到进一步降低东西向网络资源消耗、缩短混洗阶段延迟的目的.目前提出的IRS-based算法在适用场景上有一定局限性,为了解决这一问题,文中首先在以服务器为中心的代表结构BCube上建立incast最小树模型,分别提出MIB-based算法和MC-based算法,仅根据已知拓扑结构和发送节点编号即可快速生成一棵近似的最小代价incast树.MIB-based算法针对发送节点强关联的情况,使高层发送节点尽可能汇聚到已有的低层发送节点构建incast树;MC-based算法针对发送节点松散关联的情况,将节点进行最大程度上的聚合,通过增加最少的汇聚点完成incast树的构建.随后将上述两种算法结合起来进一步提出适用于各种场景的M2-based算法,通过推算时间复杂度证明该算法能够满足在线构建incast树的需求.最后,详细分析了M2-based算法对其他数据中心网络结构的适应性以及网内汇聚传输能够减少作业完成时间的原理.小规模实验结果表明,在不同网络规模下,M2-based比IRS-based节省了网络中约3%的数据量,整个作业在混洗和Reduce阶段的等待时间比不采用网内汇聚缩短约2/3;在不同传输节点规模下,M2-based比IRS-based节省了网络中约19%的数据量,整个作业在混洗和Reduce阶段的等待时间比不采用网内汇聚缩短约3/4.关键词数据中心;数据汇聚;网内聚合;混洗传输;incast树1引言随着分布式数据处理技术和云计算的不断发展,大规模数据中心成为分布式计算系统(如Map-Reduce[1]、Dryad[2]、CIEL[3]、Pregel[4]和Spark[5])处理和存储大数据的平台.在数据中心上运行的应用利用分布式计算框架将数据分发到成百上千台服务器上并行执行,从而达到在短时间内处理Tb(Terabyte)级以上海量数据的目的.为了确保高可扩展性,这些应用通常采用分割-汇聚的操作模式:在分割阶段,主节点将作业或用户请求分成若干个子任务,发送到不同的工作节点并行执行,每个工作节点处理一个数据子集,并在本地生成部分中间结果;在汇聚阶段,由各工作节点生成的庞大的中间结果集被分割成不同子集,由一个或多个工作节点通过聚合处理得到最终的输出数据.例如,在Hadoop的MapReduce中,输入数据集被分割并分别发送到不同的mapper上处理,生成一系列中间结果;reducer把具有相同键的中间数据合并从而得到结果数据.可见,汇聚阶段典型地包括一个数据流的混洗(shuffle)传输过程,大量工作节点之间相互通信,产生“多对多”的流量模式,Facebook的数据中心显示在汇聚阶段产生的网络流量占总流量的46%[6],许多研究均表明,这一巨大的网络流量使网络传输成为MapReduce应用的性能瓶颈[7-9].例如,在Facebook的所有包含Reduce阶段的MapReduce作业中,网络传输占总运行时间的33%,在26%的作业中,传输占据50%以上的运行时间,有16%的作业,传输甚至占据其70%以上的运行时间[6].产生上述瓶颈的原因主要有以下几点,首先,现代数据中心的内部流量已从传统的“南北流量”为主演变为“东西流量”为主,而MapReduce作业造成的大量流量令数据中心的东西向网络资源成为瓶颈;其次,传统数据中心网络的带宽收敛比较高,大大限制了混洗过程的数据传输率,如汇聚层通常为51,即在一些通信模式下汇聚层的可用带宽仅为服务器端的20%,而核心层则高达801,甚至2401[7];再者,服务器端有限的可用带宽(通常最多为1Gbps或10Gbps)成为“多对多”通信的瓶颈;最后,普通商业交换机的可用缓存空间较小,当多台服务器同时向一台服务器传输大量数据时往往造成Page3交换机的缓冲区溢出而发生丢包,从而导致网络吞吐量急剧下降,即典型的TCPIncast[10]问题.现存的解决这些问题的方法主要包括采用全二分带宽拓扑结构消除网络的带宽收敛比以增加网络的可用带宽,如VL2[7]、PortLand[11]、BCube[12]、DCell[8]等,或通过数据迁移避免网络热点,如Hedera[9].然而,上述方法都未能减少网络的通信量,使得性能还是受限于终端服务器的可用带宽,尽管目前许多数据中心已经升级到10G网络,但鉴于成本因素和更高的带宽收敛比,全面更新到40G网络仍然需要相当长的一段时间.再者,由于缺乏任务级的调度策略我们仍不能很好地处理流的聚合行为.因为在MapReduce中,通常只有接收到Map阶段处理的所有数据,Reduce阶段的处理过程方可开始,如果将一次传输定义为在任务的两个连续阶段中通过一组数据流,则整个任务的运行时间不是由单个流的持续时间决定,而是依赖于完成整个传输所花费的时间.在分布式计算框架中,任务节点间的唯一通信过程发生在混洗传输阶段.通过网络和计算资源联合实现网内关联性流量的聚合,能够极大降低对稀缺的东西向网络资源的消耗,避免MapReduce作业在混洗阶段产生太长的等待延迟.由于“多对多”混洗传输由一组互不相关的“多对一”incast传输组成,因此,我们通过优化incast传输实现高效混洗传输.本文利用数据中心网络的拓扑特性建立高效的incast数据汇聚树,将混洗和汇聚并行化,在传输数据的同时聚合数据流,从而减少网络中传输的数据量并提高网络性能.然而,最小代价incast树的构造是个NP难问题,最近提出的算法IRS-based[13]在incast成员节点的推算上仍具有一定的随机性,导致各层新增许多不必要的汇聚节点,从而增加了数据流和链路代价.为了解决上述问题,本文首先提出了两种分别适用于不同场景的最小代价incast树构建算法MIB-based和MC-based.MIB-based算法针对发送节点强关联的情况,使高层发送节点尽可能汇聚到已有的低层发送节点构建incast树;MC-based算法针对发送节点松散关联的情况,将节点进行最大程度上的聚合,通过增加最少的汇聚点完成incast树的构建.将上述两种算法结合起来进一步提出了M2-based算法,在任何场景下能够在更大程度上减小网络中传输的数据量,且仍然能够在较短时间内完成数据中心的混洗传输.同时,本文提出的方法同样适用于以交换机为中心的FBFLY和HyperX等基于Generalizedhypercube[14]的网络结构.2研究背景及相关工作2.1数据中心网络结构现有的数据中心网络主要依靠交换机、汇聚交换机、核心交换机/路由器将服务器连接起来构成树形结构,然而树形结构的高带宽收敛比(oversub-scription)使其很难达到数据中心网络所追求的高可扩展、容错性好、高聚集带宽等目标.因为树型结构的高层核心交换机/路由器构成网络的流量瓶颈,所以在扩展系统时,往往需要更换为更高端的交换机;而且树型结构的容错性也不理想,容易出现单点故障;与此同时,许多应用服务,如搜索引擎等对服务器间数据交换的带宽要求越来越高,因此,研究人员针对传统数据中心网络结构的固有缺陷,提出了一些新的架构设计方案.目前提出的数据中心网络结构主要分为两大类,即以交换机为中心的结构和以服务器为中心的结构.在以交换机为中心的结构中,网络连接和路由功能主要由交换机完成.Fat-Tree[15]、VL2和Port-Land中交换机彼此互连成各种树形结构,服务器仅通过一个NIC端口与接入层交换机相连,这种结构通过在树形结构上层横向增加更多的交换机来提供网络冗余,故称之为以交换机为中心的类树结构.为克服类树结构的固有弊端,研究人员又提出了两种以交换机为中心的扁平结构FBFLY[16]和HyperX[17],其基本思想是将高基交换机互连成一个GeneralizedHypercube结构,每个交换机的剩余端口连接一些服务器.由于所有链路和交换机被公平使用,所以与类树结构相比扁平结构本身不存在性能瓶颈.在以服务器为中心的结构中,主要的互连和路由功能由服务器完成.这类拓扑结构中,服务器通常采用多个NIC端口接入并连接网络,使得网络具有大量冗余的链路和平行路径以支持各种类型的流量模式.他们或者仅将交换机作为类似于crossbar的交换功能使用,如DCell、BCube、FiConn[18]和HCN[19];或者不使用任何交换机构建网络,如CamCube[20].在上述两类结构中,FBFLY和BCube最具代表性.FBFLY是一个利用高基数交换机构建的易扩展、低直径的多维直接网络.FBFLY中的所有交换机在各个维上与其他所有交换机互连,剩余端口连接服务器,任何两个服务器之间不直接相连.k元n维的FBFLY同构于k元n立方网络torus,不同的是,torus中各维互连成一个环形结构,而FBFLY中各Page4维全互连.例如,FBFLY(c,k,n)网络中共包括ckn-1台服务器和kn-1个k口交换机,每个交换机连接的服务器个数为c.BCube针对模块化数据中心设计,其典型规模为1K~4K.BCube结构以迭代方式构建,BCube(n,k)同构于n元k+1维的GeneralizedHypercube结构,不同的是,GeneralizedHypercube结构中的所有邻居服务器直接相连,而BCube中的图1BCube(4,1)结构2.2数据中心网内数据汇聚在许多分布式计算应用中,数据在接收端汇聚,输出数据的大小仅是输入数据的一小部分.以MapReduce为例,在混洗传输阶段,从所有发送节点传输到某相应接收节点的数据流是高度相关的,也就是说,对每个incast传输,相同接收节点的key/value对拥有相同的key值,因此,通常在接收端应用一个汇聚函数(sum,maximum,minimum,count,top-k和KNN)把具有相同键的中间数据合并生成结果数据.研究表明在Google的MapReduce中,平均输出数据占中间数据集大小的40.3%[1],同样的应用在Facebook中,从中间数据集到输出数据的数据量减少了81.7%[21],而在Yahoo中,数据量更是减少了90.5%[21].这些数据说明在接收端执行数据汇聚操作能够缩减数据量的规模,受此启发,如果在混洗阶段执行相同的数据汇聚操作,还能够进一步减少网络中的流量,如图2所示.图2Incast传输阶段构建数据汇聚树的示意图邻居服务器之间均通过交换机相连.BCube(n,0)由n个服务器连接一个n口交换机构成,BCube(n,k)由n个BCubek-1连接nk个n口交换机构成.每个BCube(n,k)具有k+1个网络端口.具有多个网络端口的服务器连接到多个层次的小型交换机,任何两个服务器之间没有直接连接.图1所示为BCube(4,1)结构,包括16台服务器和两层交换机.特别是在以服务器为中心的数据中心网络中,服务器可使用集成有交换机芯片的PCI网卡Server-Switch,他不但具备传统交换机的能力而且能通过高速PCI口实现CPU和ServerSwitch[22]的高速互联,能借助服务器强大的计算能力甚至存储能力实现对网络流量的深入分析处理和对数据流的网内存储、聚合等功能.然而当前数据中心的网内关联性流量聚合方面的研究工作依然很欠缺.Costa等人[23]针对CamCube结构设计了一个类MapReduce的系统Camdoop,在Camdoop中,服务器使用getParent函数计算incast树拓扑,给定接收节点R和一个发送节点(或中间节点)S,getParent函数返回S的6个邻居中距离R最近的一个,因此,在一棵incast树中,由每个发送节点传输的数据流各自沿着不相关的路由路径发送,类似于基于单播汇聚树传输的方法,这种方法难以实现中间数据流的有效汇聚,从而很难有效减少网络流量;Guo等人针对incast汇聚传输提出了一个近似的优化算法IRS-based.在incast树中,每一个中间节点都有若干流输入和一个流输出,较少的中间节点则意味着较少的数据流,因此,IRS-based算法试图在每一层中寻找包含最少节点的服务器集合.然而,该算法在逐层推算汇聚点时没有考虑低层已有的发送节点,也就是说,IRS-based算法通过对上一层节点的同一维进行调整来生成下一层节点,没有考虑上一层节点与位于下层的已有节点直接聚合的可能,从而导致新增一些不必要的节点.为此,本文提出了一种新的计算最小代价incast树Page5的高效算法M2-based,该算法充分考虑高层发送节点能够直接汇聚到已有的低层发送节点的情况,仅根据已知拓扑结构和节点编号即可快速生成一棵近似的最小代价incast树.相对于IRS-based算法,M2-based占用更少的数据中心资源,且能够在较短时间内进一步减小网络中传输的数据量.3数据中心关联性流量的网内聚合算法3.1问题描述给定发送节点集{S1,S2,…,Sm}和接收节点R,在incast传输阶段,从各发送节点到R的消息路由本质上形成一棵汇聚树.在富连接数据中心网络(如BCube)中存在许多棵路由成本不尽相同的汇聚树,我们面临的挑战是如何构建一棵使网络内传输的数据量尽可能最小的汇聚树,即最小代价incast树.假设数据中心网络结构以图G=(V,E)表示,其中,V是网络中节点的集合,E是连接节点的边的集合.节点对应数据中心的交换机或服务器,边(u,v)定义一条从u到v的链路,则最小代价incast树的建立问题可形式化描述为在G=(V,E)中寻找一个包括所有发送节点集和接收节点的最小代价连通子图,使得该连通子图上的链路代价之和最小.以BCube结构为例,给出如下定义.定义1.在BCube(n,k)中,如果两台服务器的编号xkxk-1…x1x0和ykyk-1…y1y0仅在第j(j∈[0,k])维不同,则称它们为j维1跳邻居服务器,其中,xi,yi∈{0,1,…,n-1},i∈[0,k].每台服务器在每个维度都有n-1个1跳邻居服务器,且所有j维1跳邻居服务器共同连接到第j层交换机上,因此,将任意两台编号在j个维度都不相同的服务器称为j跳邻居服务器.令接收节点编号为rkrk-1…r1r0,一个发送节点的编号为sksk-1…s1s0,其中ri,si∈{0,1,…,n-1},i∈[0,k],则接收节点和每个发送节点间的海明距离最多为k+1.因此,所有发送节点和接收节点间的最短路径组成一个具有k+2层的多层有向图,如图3所示.第0层仅包括接收节点;第k+1层仅包括发送节点;第j(j∈[1,k+1])层的服务器节点是接收节点的j跳邻居.一般地,只有发送节点和接收节点无法形成一个确定的连通子图,即无法构建一棵incast树,那么,如何在第k层到第1层中适当地选择一些中间节点使incast树的代价最小成为解决问题的关键.文献[13]已经证明在BCube网络中建立最小incast树问题是一个NP难问题,因此,本文的目标是寻找一种高效的近似算法,仅根据已知拓扑结构以及发送节点和接收节点的编号即可快速生成一棵近似的最小代价incast树.图3BCube(4,1)结构中,发送节点11、21、22、3.2MIB-based算法在incast树中,每新增一个汇聚节点,所有经过该节点的输入流将汇聚为一个输出流,那么,汇聚节点数量越少,输出的流的数量则越少,从而网络中传输的数据流的总数越少.因此,一个近似的优化策略为使高层发送节点尽可能汇聚到已有的低层发送节点,即尽量使各层中除发送节点外的新增汇聚节点数最少.首先给出如下定义.定义2.发送节点共有l(l∈[1,k+1])层,节点A位于第l(l∈[1,m))层,则将第l+1层到第l层中与节点A的编号依次相差j(j∈[1,l-l])维的发送节点称为节点A的相关发送节点,其中,位于第l(l∈[l+1,l])层的节点称为节点A的第l-l层相关发送节点.定义3.由节点A及其相关发送节点构成的节点集称为一组incast树分支.定义4.在一组incast树分支节点中,如果除最低层外的其他节点都不能再形成新的incast树分支,则称该incast树分支为最小incast树分支.例如,图3中11和21是01的第1层相关发送节点,32是02的第1层相关发送节点,因此节点集{11,21,01}和{32,30}构成2组incast树分支,且均为最小incast树分支.由上述定义可知,最小incast树分支由一系列相关发送节点构成,位于不同层次的相关发送节点之间其编号相差的位数即为层次Page6差.因此,构建最小incast树分支的过程即为从最低层发送节点开始,自底向上寻找相关发送节点集的过程,如算法1所示.算法1.BuildIncastMinimalBranch函数.输入:接收节点R,第l层第x个发送节点Sl[x]输出:第l层第x个发送节点的最小incast树分支数1.IFSl>02.THENFindMutualNode(R,S)3.FORi←0toIl.lengthDO4.BuildIncastMinimalBranch(R,Il)FindMutualNode(R,S)1.FORi←0toSl.lengthDO2.FORj←0toSl.lengthDO3.IFSl[i]是Sl[j]的(l-l)-跳邻居;4.将发送节点Sl[j]添加到Il[i];在算法1中,从给定发送节点集的最低层起,对每一个节点逐层查找其相关发送节点,这些具有高度相关性的发送节点构成一组incast树分支,并存入相应节点的incast树分支数组,最终将给定发送节点集分割成若干组最小incast树分支.因此,在每一组最小incast树分支中,如何通过增加最少的汇聚节点将这些具有高度相关性的发送节点连接起来,即将最小incast树分支构成一棵最小代价incast树分支成为解决问题的关键.定理1给出最小incast树分支中位于各个层次的发送节点的编号规律和汇聚关系.定理1.在一组最小incast树分支中,假设接收节点为X(0),第l(l∈[1,k+1])层中的服务器节点表示为X(l),有(1)当l<l时,X(l)中有l-l个维度与X(l)不同.(2)当l>l时,若X(l)与X(0)在某一维不同,X(l)与X(l)必然在该维相同.(3)当3la=lbk+1,la,lb>l+1,若X(la)和X(lb)在与X(l)不同的维度上有j(j∈[1,la-l])维相同,则X(la)与X(lb)在第j+l层可以汇聚到同一个节点;否则不能在第l+1~la-1层实现汇聚.(4)当3la≠lbk+1,la>lb>l,若二者在与X(l)不同的维度上有j(j∈[1,lb-l])维相同,则X(la)与X(lb)在第j+l层可以汇聚到同一个节点;否则不能在第l+1~lb层实现汇聚.证明.(1)由定义1、2可知,对一组incast树分支节点,X(l)是X(1)的l-1跳邻居节点,X(l)是X(1)的l-1跳邻居节点,因此,X(l)中有l-l个维度与X(l)不同,得证;(2)已知X(l)中有l个维度与X(0)不同,假设X(l)与X(l)在这l个相应维不同,因为X(l)中共有l-l个维度与X(l)不同,所以除了这l个维度之外,X(l)中还有l-2l个维度与X(l)不同,因此X(l)中共有l-l个维度与X(0)不同,而实际上,X(l)是X(0)的l跳邻居节点,故X(l)中应有l个维度与X(0)不同,与原假设矛盾,故原假设不成立,X(l)与X(1)必然在相应维相同,得证;(3)因为X(la)有la-l个维度与X(l)不同,又X(la)和X(lb)在与X(l)不同的维度上有j维相同,所以X(la)和X(lb)有la-j-l维不同,即X(la)与X(lb)各自需调整la-j-l维可实现汇聚,故在第j+l层可以汇聚到同一个节点.反之,若X(la)和X(lb)在与X(l)不同的维度上均不相同,即X(la)与X(lb)各自需调整la-l维可实现汇聚,故只能在第l层汇聚到X(l),即不能在第l+1~la-1层实现汇聚,得证;(4)因为X(la)有la-l个维度与X(l)不同,X(lb)有lb-l个维度与X(l)不同,又二者在与X(l)不同的维度上有j维相同,所以X(la)与X(lb)分别需调整la-j-l维和lb-j-l维即可实现汇聚,故二者在第j+l层可以汇聚到同一个节点.反之,若X(la)和X(lb)在与X(l)不同的维度上均不相同,即X(la)与X(lb)分别需调整la-l维和lb-l维可实现汇聚,故只能在第l层汇聚到X(l),即不能在第l+1~lb层实现汇聚,得证.证毕.由定理1可以很容易地推算出各层发送节点的下一跳节点,如推论1.推论1.在一组最小incast树分支中,令第l(l∈…le[1,k+1])层节点为X(l)=lklk-1…le1le2X(l)=lklk-1…le1le2le1le2层相关发送节点.若X(l)和X(l)在与X(l)不同的维度上有j(j∈[1,min(l,l)-l])维相同,即le1=le1≠le1烄le2=le2≠le2烅j=lele烆调整除j维的其他任意维度为X(l)的相应维得到其下一跳节点,并在第j+l层汇聚到节点X(j+l)=lklk-1…ae1ae2调整维度le1le2其下一跳节点,将所有节点相连则构成一棵以X(l)为根节点的最小代价incast树分支.推论1给出了将最小incast树分支自顶向下构建最小代价incast树分支的方法.其中下一跳节点Page7的生成方法使得位于高层的发送节点尽可能汇聚到已有的低层发送节点,最大程度上确保各层新增的汇聚节点数最少.假设最高层为H,共有mH个发送节点,距离最高层最近的低层发送节点位于第l(l∈[1,H])层,那么根据上述推论可以构建出以X(l)为根,以mH个发送节点为叶子节点的部分最小代价incast树分支;同理,可以继续构建出以距离X(l)最近的低层发送节点X(l)(l∈[1,l))为根,以X(l)为叶子节点的部分最小代价incast树分支,直到形成一棵完整的最小代价incast树分支.我们将这种方法命名为MIB-based(MinimalIncastBranchbased)汇聚树构建方法.3.3MC-based算法针对多层次、强关联的发送节点,采用自底向上推算相关发送节点,自顶向下建立汇聚路径的方法构建最小代价incast树.那么,对于不能构成一组incast树分支的节点将通过寻找各层节点之间的汇聚关系将节点进行最大程度上的合并,即针对松散关联的发送节点,我们采用自顶向下聚类并建立汇聚路径的方法,通过计算最小聚类节点组(MinimalClusteringNodeGroups,MCNG)使高层节点逐层合并到低层节点构建最小代价incast树.首先给出最小聚类节点组的定义.定义5.将发送节点按照如下要求进行分组,即组内成员之间互为j(j∈[1,k+1])跳邻居且各组之间无交集,使得组合化程度最高而分组数目最小,由此形成的节点组定义为最小聚类节点组.最小聚类问题(MinimalClusteringProblem)也是一个NP难问题,本文提出了一种近似的优化算法,如算法2所示.令图G=(V,E)中V表示所有待聚类发送节点,对任意两个节点u,v∈V,若u和v互为j(j∈[1,k+1])跳邻居,则存在(u,v)∈E.首先,计算图G中所有节点的度并找出具有最大度的节点u,由于u和不同的邻居节点可能在不同的维度上相差j维,因此计算出在相同维度上相差j维的邻居节点的最大集合构成一组;然后,从图G中将该组内的所有节点和相关的边移除,对于剩下的节点重复以上过程直到图G为空.最终,该算法将所有待汇聚的发送节点分成若干无交集的组.算法2.MinClustering函数.输入:图G=(V,E)输出:最小聚类节点组1.Groups={};2.WHILEG≠DO3.计算V中每个节点的度;4.5.6.因此,在由非incast树分支组成的节点集中构建最小代价incast树的方法为,首先,在第k+1层的所有节点中寻找k维相同的MCNG,得到位于第k层的汇聚节点;第二,在第l(lk)层的剩余节点中寻找在与X(0)不同维度上k-1维相同的MCNG,得到位于第k-1层的汇聚节点;在第l(lk-1)层的剩余节点中寻找在与X(0)不同维度上k-2维相同的MCNG,得到位于第k-2层的汇聚节点;以此类推,直到在第l(l2)层的剩余节点中寻找在与X(0)不同维度上1维相同的MCNG,得到位于第2层的汇聚节点;第三,对从第k+1层到第2层的所有不能汇聚的节点逐一调整相应维度为X(0)的相应维得到其下一跳节点,最后,将所有节点相连则构成一棵以X(0)为根节点的最小代价incast树.我们将这种构建最小代价incast树的方法命名为MC-based(MinimalClusteringbased)汇聚树构建方法.3.4M2-based算法在实际应用中,发送节点的分布往往具有随机性,因此,需要把MIB-based和MC-based结合起来共同构建一棵最小代价incast树,我们将这两种方法统称为M2-based.综上所述,给定发送节点集和接收节点,最小代价incast树的构建过程分为以下5步:步骤1.根据发送节点与接收节点的编号计算出每个发送节点在incast树中的层次.即位于第j(j∈[1,k+1])层的发送节点是接收节点的j跳邻居,它们的编号相差j维.步骤2.从最低层发送节点开始,自底向上逐层构建incast树分支.假设最低层发送节点为第l层,节点个数为ml,对第l层中的每一个发送节点,自底向上查找其相关发送节点,构成ml组incast树分支.对余下各层中的剩余发送节点依次采用相同的方法构建incast树分支.步骤3.对每一组incast树分支,再次应用上一步中的方法递归构建incast树分支,直到所有incast树分支均为最小incast树分支.对每一组最小incast树分支,可根据推论1直接构建出最小代价incast树分支.步骤4.对没有构成incast树分支的剩余节点则通过计算MCNG自顶向下构建最小代价incastPage8树分支.步骤5.将位于第1层的发送节点与接收节点相连得到最终的最小代价incast树.例如,令接收节点R=000,发送节点集为{002,003,010,011,031,121,202,211,221,300,301,321,322,323}.根据发送节点与接收节点的编号将发送节点分为3层,分别为l1={002,003,010,300},l2={011,031,202,301},l3={121,211,221,321,322,323}.对第l1层到第l3层的每一个节点,自底向上查找其相关发送节点,构成7组incast树分支,如图4(a)所示.对第一组incast树分支递归构建最小incast树分支,由此形成8组最小incast树分支,如图4(b)所示.对第1、2组最小incast树分支可根据推论1直接构建出其最小代价incast树分支,如图4(c)所示.对没有汇聚到第l1层的剩余节点图4BCube(4,1)结构中最小代价incast树的构建过程{031,121,221}自顶向下逐层计算MCNG并合并构建最小代价incast树,如图4(d)所示.最后将位于第1层的发送节点与接收节点相连得到一棵最小代价incast树.3.5算法复杂性分析假设incast树中所有边的权重为1,树的代价为所有边权重之和,则最小代价incast树的构建问题可等效为Steiner树问题,是一个NP难问题,目前已经提出的在一般图中构建Steiner树的近似算法其时间复杂度为O(mN2),m表示发送节点的个数,N表示网络规模.然而,大规模商用数据中心通常拥有成千上万台服务器,这一较高的复杂度显然无法满足在线构建incast树的需要,而且这些算法也无法高效利用诸如BCube等富连接网络的拓扑特性.本文提出的M2-based算法利用BCube的多等价路径特性构建最小代价incast树大大降低了通用算法的复杂度.定理2.假设incast传输由m个发送节点组成,位于第l层的发送节点的个数为ml(l∈[1,k+1]),则m=m1+m2+…+mk+1,那么最小代价incast树构建算法的时间复杂度为O(m2(lgm+lgN)).证明.最小代价incast树构建算法的计算量主要花费在2个阶段,即自底向上计算最小incast树分支(算法1)和对不能构成incast树分支的剩余节点计算MCNG(算法2).第1阶段中,从第1层到第k+1层逐层递归构建最小incast树分支,形成一棵深度为O(lgm)的递归树,其中,每一层的代价为O(mj×(mj+1+mj+2+…+mk+1))=O(mj×(m-m1-m2-…-mj))=O(m2).因此,第1阶段的时间复杂度为O(m2lgm).第2阶段中,逐层计算MCNG的时间复杂度为O((mk+1+mk+…+mj)2)=O(m2),最多需要计算k次,因此,第2阶段的时间复杂度为O(km2)=O(m2lgN),综上,总的时间复杂度为O(m2(lgm+lgN)).4讨论4.1扩展到其他网络结构在以交换机为中心的结构中,传统交换机不提供可编程数据层,而软件定义的网络技术仅支持控制层的可编程.网内汇聚要求网络设备能够缓存和处理数据流.因此,将用户定义的汇聚函数放在传统交换机上是不可行的,以交换机为中心的结构不能直接支持网内汇聚.近年,新型的思科ASIC和aristaPage9应用交换机提供可编程的数据层,如果数据中心利用这种基于软件的或基于FPGA的新型交换机,在以交换机为中心的结构中实现网内汇聚将成为可能.在以交换机为中心的结构中构建最小incast树的工作将在下一步开展.目前,以服务器为中心的结构已经能够支持数据中心的网内汇聚.尽管本文基于BCube结构进行研究,文中提出的方法仍可应用到其他以服务器为中心的结构中.然而,在不同的网络结构上构建incast树需要利用不同结构的不同拓扑特性,关于在其他以服务器为中心的结构上的汇聚传输策略也将作为我们后续工作之一.如果采用具有可编程数据层的新型交换机,本文提出的方法可直接应用于以交换机为中心的FBFLY和HyperX结构,因为BCube拓扑本质上属于GeneralizedHypercube结构,而FBFLY和HyperX在交换机的互连层面上也同构于GeneralizedHypercube.4.2对作业完成时间的影响给定一个类MapReduce作业,其执行时间取决于3个阶段,即map、混洗和reduce.网内汇聚操作仅影响混洗和reduce阶段的完成时间,而对map阶段无影响.混洗阶段的完成时间主要取决于网络的通信量及可用的网络资源.在数据中心中,类MapReduce的分布式计算框架存在map倾斜问题[24-28],由于不同map任务上负载分布的不均衡性,使得map任务的运行时间高度可变.当这种倾斜发生时,一些map任务会花费更长时间处理输入数据,从而延缓了整个作业的完成时间.同理,reduce任务也存在类似问题.近年来,研究人员提出了许多方法解决或缓解map倾斜问题,这些策略与使用网内汇聚优化传输是正交的,因此,我们采用这些方法使所有map任务尽可能同时完成以更好地支持网内汇聚策略的应用.在上述设定下,incast传输中的每一个发送节点沿着incast树向接收节点传送数据流.当到达某汇聚节点,该流的所有数据包将被缓存,一旦有新的数据流到来,汇聚节点即可执行汇聚操作,即将所有key-value对根据key值分组,并对每组应用汇聚函数.如果汇聚节点上的可用缓存很小或者等待所有流到达的时间超出某个阈值,则生成一个新的数据流并继续沿着incast树向接收节点发送.原理上类似于文献[29]中的MapReduce延迟调度技术.因此,与目前不采用网内汇聚策略的方法相比,网内汇聚直接减少了混洗阶段传输的网络流量,从而缩短了混洗阶段的持续时间.在这种方式下,由于存在拖后腿的map任务,最后到达的数据流可能无法在汇聚节点与其他流聚合,将以cut-through的方式转发,等同于现存的无网内汇聚的方法.因此,采用网内汇聚算法后,混洗阶段的完成时间在最坏情况下与现存方法相同.同时,与现存方法相比,网内汇聚策略使接收端接收的数据量大大减少,从而减少了接收端进行reduce计算的时间.综上,我们的方法不会增加混洗阶段的完成时间,且能够减少reduce计算的时间,因此,与现存方法相比能够减少整个作业的完成时间.5仿真实验本节对M2-based算法与目前不采用网内汇聚的方案(以下简称现存方法)、单播算法以及IRS-based算法进行对比实验,通过调整网络规模和incast传输节点的规模评估如下3个指标:混洗传输的数据量即incast树中所有边流量的总和、汇聚服务器个数、新增服务器个数、incast树中的有效链路个数以及接收端的数据量.由于实验条件所限,我们仅搭建了小型的测试床,在大规模真实网络上的验证工作拟留待下一步完成.5.1实验设计实验平台采用3台服务器通过1GB以太网交换机互连.服务器的硬件配置包括一个2路8核2.50GHzIntelXeonE5420处理器,24GB内存和1TBSATA硬盘.Hadoop分布式实验环境由3台服务器上运行的31台RedHat虚拟机构成,其中,1台服务器上运行11台虚拟机,1台为Hadoop的管理节点,10台为数据节点;其余2台服务器上各运行10台虚拟机均作为Hadoop的数据节点.每个数据节点支持4个map任务和1个reduce任务.每台服务器使用一块Intel网卡,该服务器上的所有虚拟机通过虚拟交换机共享该网卡.修改Hadoop使其支持网内数据包缓存和汇聚功能.MapReduce作业采用Hadoop0.21.0中的样例程序Wordcount,包括120个发送节点(map任务)和1个接收节点(reduce任务).为每个map任务分配10个64M大小的输入文件.在混洗阶段,执行中间结果本地汇聚后从每个发送端传输到接收端的平均数据大小为1M.在实际的数据中心网络中,map任务往往被调度到空闲服务器执行incast传输,从而发送节点呈随机分布,因此,为所有发送节Page10点和接收节点随机分配一个BCube编号,为了实现在BCube(6,k)(2k8)网络上的incast传输,根据相应算法推算出中间节点,则所有节点共同构成一个incast连通子图,相当于一个部分的BCube网络.Incast树中的节点与31台虚拟机的映射关系为:发送节点和接收节点分别映射到30台数据节点.通过在每台虚拟机上设置代理软件模拟中间节点收发和存储数据包,将中间节点也映射到30台数据节点.同时,为了确保执行incast传输时一台数据节点上的多个中间节点之间不发生本地通信,将任图5incast传输中,各评价指标随BCube(6,k)网络的k值的变化趋势何一对在incast树的连续层次出现的邻居节点映射到不同的数据节点.因此,incast树中的每一条边映射为两台虚拟机之间的虚拟链路或服务器之间的物理链路.5.2网络规模的影响给定incast树,在BCube(6,k)子网上部署具有120个发送节点和1个接收节点的incast传输.实验分别采用M2-based算法、现存方法、单播算法和IRS-based算法生成相应的incast树,执行100次取平均结果,如图5所示.Page11图5(a)表示不同网络规模下incast树中传输的流量的变化趋势.与现存方法相比,M2-based算法、单播算法和IRS-based算法均显著节省了网络中传输的流量,证明了关联性流量网内汇聚策略的有效性.M2-based算法和IRS-based算法明显优于单播算法,一个重要原因是前两种算法中的汇聚点个数随k值的增大而增加,而单播算法中的汇聚点个数随着k值的增大而减少,如图5(b)所示.M2-based算法的效果更好,因为M2-based通过增加最少的节点完成incast树的构建,减少了输出的流的数量,从而减少了网络中传输的数据流的总量,如图5(c)所示.此外,M2-based算法占用更少的链路,从而占用更少的服务器和网络设备,节约更多的网络资源,如图5(d)所示.同时,关联性流量的网内汇聚策略均极大地减少了接收端的数据量,如图5(e)所示.表1中的数据说明与现存方法相比,M2-based明显减少了整个作业在混洗和reduce阶段的等待延迟.表1网络规模对混洗和reduce阶段完成时间的影响BCube(6,k)k=2k=3k=4k=5k=6图6BCube(4,8)网络中,incast传输的数据量和有效链路数随发送节点数量的变化趋势表2网络规模对混洗和reduce阶段完成时间的影响1000150020003000综上所述,M2-based算法支持发送节点为120的小规模incast传输,且生成更少的网络流量并占用更少的数据中心资源.5.3Incast传输节点规模的影响在现实的数据中心网络中,类MapReduce作业通常包括数百甚至上千个map任务,由于资源有限,现有测试平台无法运行大规模的词频统计程序,因此,我们通过模拟程序证明M2-based算法的扩展性.令发送节点m∈{100,200,…,3900,4000},通过Hadoop的样例程序RandomTextWriter为每个发送节点提供输入数据.控制从每个发送节点传输到接收节点的平均数据量为1G.图6为BCube(4,8)网络中,incast传输的数据量和有效链路数随发送节点数量的变化趋势.BCube(4,8)网络的规模为262144,能够满足实际数据中心网络的规模要求.结果表明,当发送节点的数量从100增长到4000时,与现存方法相比,M2-based算法和单播算法均显著节省了网络中传输的数据量,证明了关联性流量网内汇聚策略在大规模incast传输中的有效性.且在拥有4000个节点的incast传输中M2-based算法仍然优于其他两种方法.图6(b)表明M2-based算法占用更少的链路,从而占用更少的服务器和网络设备,节约更多的网络资源.表2显示在大规模incast中,与现存方法相比,M2-based仍然能够减少整个作业在混洗和reduce阶段的等待延迟.综上所述,M2-based算法能够支持较大规模的incast传输,且生成更少的网络流量并占用更少的数据中心资源.6结论在大规模分布式计算应用中,混洗传输阶段产Page12生的巨大网络流量给数据中心网络带来巨大压力,严重影响应用性能.在数据传输过程中执行网内关联性流量的聚合能够大大减少网络流量,提高通信性能.数据中心网络的数据汇聚可形式化描述为最小代价incast树的建立问题,然而,最小代价传输汇聚树的构建是一个NP难问题.针对这一问题,本文提出了一种构建最小代价传输汇聚树的算法M2-based,仅根据已知数据中心网络的拓扑结构以及发送节点和接收节点的编号即可快速构建出一棵近似的最小代价incast树,实验结果表明,与现有方法相比,M2-based能够占用更少的数据中心资源、在较短时间内进一步减小网络中传输的数据量.
