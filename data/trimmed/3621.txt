Page1一种多权值神经元覆盖网络的构造方法魏莱1)徐菲菲2)王守觉3)1)(上海海事大学计算机科学系上海201305)2)(上海电力学院计算机科学与技术系上海200090)3)(中国科学院半导体研究所神经网络实验室北京100083)摘要仿生模式识别利用多权值神经元覆盖网络构造模式类的覆盖来进行相应事物的识别.但在构造多权值神经元覆盖网络的过程中,关于构造神经元个数的确定方法没有相关讨论,即需要使用多少个神经元才能完成对模式类的覆盖.较多的神经元在精确的对模式类进行覆盖同时,也增大了网络的复杂度.文中提出了一种多权值神经元覆盖网络的构造方法.在保持神经网络对模式类的覆盖能力的基础上采用尽量少的神经元,从而能有效的降低神经网络构造代价.最后,通过实验作者验证了算法的有效性.关键词模式识别;神经元;单纯形;覆盖;多维尺度变换1引言仿生模式识别[1]是以“认识”而非“区分”事物为目的的,与传统的模式识别中寻找对模式类的“最佳划分”方法不同,仿生模式识别采用“覆盖”的方式完成对不同模式类的分类.仿生模式识别的实现方法是在对数据样本在特征空间中形成的无穷点集的“形状”的分析和认识基础上,通过多权值神经元构造出高维空间中的简单覆盖几何体,再由多个这样的神经元连接而成的神经网络构造出由简单几何体拼接而成的复杂覆盖区域,从而完成对样本Page2集的最佳覆盖.仿生模式识别相比于传统模式识别方法的优势主要体现在两点.首先,传统模式识别克服不了对未训练样本的高误识率问题.传统模式识别的分类是对现有的训练样本集的划分,当有不属于现有训练样本集中任一类的样本进入时,其必定会被分到原训练样本中的某一类,从而造成误识.而仿生模式识别构造了不同类别样本集的不同覆盖,当数据点不属于任意覆盖区域时,则会将其拒识.其次,由传统模式识别建立起来的分类系统,当有新的类别的训练样本增加时,需要对已训练好的系统进行重新训练,即重新划分各种样本,导致系统训练时间加长.而对于仿生模式识别来说,只需要对这类新的样本集训练覆盖网络就行了,并不需要对前面已经学习好的覆盖网络进行重新的训练,因此整个系统训练时间必定能大大地减少.可以说,仿生模式识别方法更接近于人类对事物的认知方式,也正因为这样,仿生模式识别已经在人脸确认[2]、人脸识别[3]、语音识别[4]方面显示了巨大的优越性.示意图见图1.图1仿生模式识别示意图(图中三角形为要识别的样本,圆圈和十字形为与三角形不同类的两类样本,折线为传统BP网络模式识别的划分方式,大圈为RBF网络的划分方式(等同于以模板匹配的识别方式),细长椭圆形构成的曲线代表仿生模式识别的“认识”方式)但是,在构造多权值神经元覆盖网络的过程中,关于构造神经元个数的确定方法没有相关讨论,即使用多少个神经元才能完成对模式类的覆盖.实际上,仿生模式识别是通过数据点组成的单纯形的拼接去拟合数据集的低维“流形”[5-7]结构.每一个单纯形,对应一个多权值神经元,该神经元能覆盖住空间中距离单纯形小于等于一定阈值的区域,这种神经元也叫单纯形神经元[8-9].目前尚没有明确的规则来确定使用多少个单纯形去逼近数据流形是合适的,经常使用的方法是尽量通过近邻数据点来构造尺度小的单纯形,这样就需要使用较多的单纯形去拟合数据流形,因此得到的神经网络也往往具有较多的神经元.对于每一个单纯形神经元,需要存储其相应的单纯形顶点以及一定的阈值来构造相应的覆盖区域[1,9],因此构造每一个多权值神经元的代价是比较高的,故我们就期望有一种合理的方法使得在拟合数据集的几何结构的同时能用最少的多权值神经元将模式类进行完全的覆盖.本文尝试进行这方面的探索,提出一种多权值神经元覆盖网络的构造方法,并通过实验证明本文算法的可行.2仿生模式识别基本原理简介仿生模式识别认为自然界中任何欲被认识的事物(包括事物、图像、声音、语言、状态等等)若存在两个“同源”同类而不完全相等的事物,而这两个事物的差别是可以渐变的或非量子化的,则这两个同类事物之间必至少存在一个渐变过程,在这个渐变过程中间的各事物都是属于同一类的.用数学的语言描述为:特征空间Rn中,设所有属于A类事物的全体所组成的点集为A,若集合A中存在任意两个元素X与Y,则对ε为任意大于0的值时,必定存在集合B,使得B={X1,X2,…,Xn|X1=X,Xn=Y,nN,ρ(Xm,Xm+1)<ε,ε>0,n-1m1,mN},BA.仿生模式识别对事物的这种规律性认识,实际上就是对事物的全体在特征空间中的数据点的分布规律的把握,也即对数据集形成的“流形”的认识.而现实中得到的数据集不可避免地带有“噪声”,所以数据流形并不会非常地“光滑”,数据点会依据流形结构分布在其周围小的区域内,因此要完成对该类数据集的覆盖就需要覆盖住数据流形和n维超球的拓扑乘积(这里的n是数据观测空间维数)形成的一个区域.仿生模式识别通过单纯形的组合来拟合数据流形,并使用单纯形神经元组成的神经网络来构造覆盖区域.我们下面来介绍单纯形和单纯形神经元[8-9]的概念.2.1单纯形单纯形是欧式空间的一类最基本的几何图形.定义如下.定义1(单纯形).在n维空间Rn中,以不同在任何一个k-1维超平面上的k+1个点W0,W1,…,Wk(kn)为顶点,两两连接得到的有限封闭超多面体称为k维单纯形,即Page3θ〈W0W1…Wk〉=X|X=∑k由上述定义可知,当某一αi为0时,θ〈W0W1…Wk〉成为k-1维单纯形,此时称之为顶点Wi所对应的侧面.k维单纯形有k+1个顶点和k+1个侧面;连接两个顶点的线段称为棱,k维单纯形有C2k(k+1)条棱.实际上,k维单纯形是某个k维以上的欧氏空间中的k+1个仿射无关的点的凸包.例如,0-单纯形就是点,1-单纯形就是线段,2-单纯形就是三角形,3-单纯形就是四面体,而4-单纯形是一个五胞体.所有棱长都相等的单纯形称为正则单纯形.例如:正三角形是一个二维正则单纯形,正四面体是一个三维正则单纯形.更高维正则单纯形的几何图形的认识需借助高维抽象的想象.从上面的介绍中知道,单纯形是一种线性的几何体,而数据流形一般是非线性的,因此从全局的角度上不能使用一个单纯形来代替数据流形,但是我们知道流形在局部范围内与欧式空间“同胚”,也即在一个比较小的尺度上,局部数据流形是线性的,故可以用一个单纯形来近似,这样就可以使用与流形同维数的小的单纯形组合去实现对流形的整体拟合.但需要说明的是,在实际应用中,数据集流形的维数通常是未知的,而如何确定数据集流形的维数,一直是机器学习研究领域的一个复杂问题.目前存在的方法包括分形锥方法[10]、PackingNumber方法[11]等等.实际上,文献[6]中不仅仅给出了一个非线性数据约简算法———ISOMAP,也给出了一个利用残差曲线来估计数据集内在维数的方法.通过残差曲线的拐点,可以大致得出数据集流形的维数.本文并不去深入讨论数据流形维数问题,我们这里假设流形维数已知,在实际应用中,可以先通过上述算法确定数据流形维数,然后采用本文算法.2.2单纯形神经元一个人工神经元,其基本运算公式为其中,f为神经元非线性状态转移函数,Th为神经元的阈值,Φ(x1,x2,…,xn)为包含各个权值在内的神经元基本运算函数,权值计算可以参见文献[9].从几何上解释,一个神经元可以对应一个超平面、超曲面或者超球,而所谓的单纯形神经元就是单纯形与Th为半径的n维超球的拓扑乘积,是一类特殊的封闭复杂几何形体.定义2(单纯形神经元)在n维空间Rn中,X到θ〈W0W1…Wk〉的距离为称与θ〈W0W1…Wk〉关联的神经元模型y=f[Φ(W0,W1,…,Wk,X)-Th],为拥有k+1个权值矢量的单纯形神经元.Φ是X到k维单纯形θ〈W0W1…Wk〉的距离函数,Th为覆盖区域距离单纯形的最大距离.由定义2,我们可以得到如下推论.推论1.当k=0时,θ〈W0〉是一个点,对应RBF(单权值)神经元模型,图2(a).推论2.当k=1时,θ〈W0W1〉是一条线段,对应超香肠神经元模型,图2(b).推论3.当k=2时,θ〈W0W1W2〉是一个三角形,对应三角形神经元模型,图2(c).当k3时,把与单纯形θ〈W0W1…Wk〉对应的神经元统称为多维(多自由度)单纯形神经元模型.k=3时的三棱锥神经元如图2(d)所示.通过上述单纯形神经元的组合,我们就能对数据集进行覆盖,训练好覆盖网络后,当数据点落在被单纯形神经元覆盖住的区域时我们就认为数据点为该类数据点,反之则会被拒识.用多少个单纯形神经元能够将其覆盖?现在问题浮现了,对于一个模式类,我们需要采为了方便,先考虑一个简单的情况,即数据流形是一维的,那么由于“噪声”的影响,真实数据点在特征空间的分布可以是一维流形与n维超球的拓扑乘积.可以用连接数据点的一系列小的直线段来逼近这个一维流形,而覆盖神经元采用超香肠神经元.现在的问题就是,对于每一个直线段,应该取多长是合适的,即应该选择哪些数据点作为直线段的端点,从而用来构造超香肠神经元?当然,线段取的越短,对流形的逼近效果也就越好,但是直线段越短,所需要的直线段数量就越多,而每一个直线段都对应一个超香肠神经元,那么构造超香肠神经元的代价较高.Page4因此,实际上我们就是希望在保持直线段对流形的逼近能力基础上用尽量少的直线段来拟合这个一维流形.推而广之,当数据流形维数不是一维的情况下,就是希望能用尽量少的与数据流形维数相同的单纯形来逼近数据流形,从而减少覆盖神经元的数量,降低整个覆盖网络的复杂度.3多权值神经元覆盖网络构造方法由于数据采样技术的限制以及数据集流形几何结构的多变性,使得得到的每一数据点周围的局部密度和空间曲率都会不一样,对于一维数据流形问题来说,就是在不同的区域,直线段的长短不一样.在数据曲率大的地方直线段应该短一点,而在数据曲率小的地方,直线段可以长一点,即在一维尺度上是近似线性的.这样,可以明确直线段长度的确定方法,即直线段可以尽量地长,只要包含在直线段端点范围内的数据点集在一维上是线性的.这样既可以做到尽量地减少直线段的数量,又能对数据流形进行合理的覆盖.同样对于d维流形,d维单纯形也可以尽量地大,只需要在该单纯形区域内的数据集在d维尺度上是线性的,这样该局部数据集构成的几何体可以用一个单纯形来代替.如何判断数据集在特定维数上是线性的,在这里我们采用多维尺度变换(MultidimensionalScaling,MDS)[12]的方法来确定.3.1多维尺度变换MDS算法的原理是基于这样的数学推导.假设数据集为犡={x1,x2,…,xN},如果我们不知道数据点的具体坐标,只知道数据点的距离矩阵犇={dij},dij=xi-xj原点,定义内积矩阵犅犡=犡犡T.那么,如果能得到犅犡的值,相应可以得到数据集坐标(至多与真实坐标相差一个常数).而从距离矩阵犇={dij}出发,令犛={d2ij},对其进行双中心化,即τ(犇)=-犎犛犎{hij},满足hij=δij-1我们发现有犅犡=τ(犇),再对τ(犇)进行奇异值分解,即犅犡=τ(犇)=犝Λ犝T(犝T表示犝的转置),很明显犡的重构坐标就可以表述为犡=犝槡Λ.如果数据分布是由内在低维变量控制,变量个数为d(这里的内在控制变量个数,就是数据集的内在维数,也就是数据集形成的流形的维数),那么矩阵Λ的第d+1个特征值就接近于零,这时就有犡≈犝dΛ槡d,其中Λd为Λ的前d个特征值组成的矩阵,犝d的列为相应的特征向量.从上面的叙述可知,如果数据集的内在维数已知,那么数据集犡在该维数下的线性结构越明显,它的低维重构犣越能精确的表示犡,反之则不是,而这种重构的是否合理可以用犅犡-犅犣L2的值来度量(犅犡、犅犣分别是犡、犣的内积矩阵,犅犡通过双中心化局部距离矩阵犇={dst}(k+1)×(k+1)得到,而犅犣=犣T犣,这里·L2表示矩阵的L2的模),差值越小则重构越精确.3.2单纯形神经元的构造通过多维尺度变换可以判断一个数据集是否是线性的,那么反过来,对于也可以通过多维尺度变换来确定数据流形上的一个小的子集是否是线性的.具体的操作可以表述如下(假设数据集内在维数为d).假设对数据点xi选择一个较大的邻域参数kmax(kmax表示邻域集包含的数据点个数)组成邻域集犡i,然后求得其d维重构犣i,计算犅犡i-犅犣iL2,并设定阈值ε(例如ε=0.1).当犅犡i-犅犣iL2>ε时,减小邻域参数kmax=kmax-1,重新计算新的局部邻域犡i和相应的低维坐标犣i,更新犅犡i,犅犣i,再次计算犅犡i-犅犣iL2,重复上述循环直到犅犡i-犅犣iL2小于ε时,输出合适的邻域大小k(还可以规定一个最小的kmin,使k不能小于kmin).这样的k应该可以认为是犡i所能包含数据点的最大个数,犡i也就是最大的线性数据集.我们把这个方法叫做线性邻域集选择算法[13].1.设定邻域变化范围kmax,kmin和阈值ε和邻域集犡i=WHILE犅犡i-犅犣iL2>εk>kmin{xi0,xi1,xi2,…,xikmax}T,邻域按离xi0的距离排序;2.对犡i做MDS得到犣i,计算犅犡i-犅犣iL2,k=kmax,END3.输出k.通过这个算法,我们能得到数据流形上的最大的线性邻域集,理论上这个邻域集就可以用一个单纯形来代替,对于一维流形来说,直线段就是犡i中距离最长的两点连线,而对于高维的数据流形,单纯形构造稍微复杂,我们在下节给出了构造算法.3.3单纯形神经元覆盖网络的构造算法盖网络的构造算法可以表述如下:假设数据流形维数为d,那么单纯形神经元覆Page51.对任一xi∈犡通过自适应邻域算法选择其合适的邻域大小犡i;2.在邻域集犡i中选择距离最远的两个点xi1,xi2,将这两点相连,然后找到距离线段xi1xi2距离最远的点xi3,将这三点连接,一直这样做,寻找到第d+1与前面d个点构成的d-1维单纯形距离最远的点,将这d+1点连接,构成d维单纯形θ1,构造覆盖区域P1={犡|ρ犡θ1Th,犡∈犡i},然后将犡i中被P1覆盖住的点除去.在犡i剩余点中寻找与θ1距离最远的点xi1,同时将θ1中与xi1距离最近的d-1维单纯形的顶点与xi1链接,构成d维单纯形θ2,构造覆盖区域P2={犡|ρXθ2Th,犡∈犡i\P1}然后在犡i除去被P2覆盖住的点,一直这样做下去构造K个单纯形神经元,直到犡i中所有点都被覆盖住;3.将犡中被前K个单纯形神经元中覆盖住的点及单纯形顶点分别记录在犢和犣中,在K个单纯形顶点中任选一个通过自适应邻域算法求出其的最佳邻域,在邻域集中除去在犢和犣中也存在的点,如果邻域集为空,则取另一个顶点;否则除去也在犢中的点,转步2.注.上述的算法是从数据流形不是一维的情况下出发的,对于覆盖一维流形的超香肠神经元覆盖网络,在步2中只需要将邻域中相互距离最大的数据点链接,组成一维单纯形就可以了.4算法分析实际上算法最重要的部分即是选取数据点的一个线性邻域集,使得可以用线性的单纯形去拟合该邻域数据点形成的几何体.线性邻域选择算法采用的一个迭代方法,我们首先选取一个确定的较大的图3算法实验效果从图3中可知,覆盖神经元只需要使用13个,而目前一般的构造方法所需神经元个数必定远远大于这个数字.如果采用人为观察来确定神经元个数,在数据集简单的情况下可能可行,但风险一定是非常大的,当数据集结构复杂、维数较高时,更是不可能完成的.邻域,然后逐步减小邻域内数据点个数,我们知道,当邻域点个数减小到d+1时(d为流形维数)其一定是线性的,因此算法一定能在有限步内收敛.而覆盖神经元的确定是构造性的,在有限数据点集范畴内,构造性算法一定是收敛的.我们再来看算法的复杂度.邻域选择算法中有一个特征值求解问题,该问题和邻域包含的数据点个数相关,为O(k2)(k为邻域数据点个数),而算法需循环的最大次数为kmax-d-1,故自适应邻域选择算法的时间复杂度为O(k3max),而kmax一般不会取的太大,因此算法运行时间是可以接受的.而每个邻域中覆盖神经元个数最多为k来说,覆盖神经元最多需要k算法的时间复杂度为ON5实验分析5.1人造数据实验1.我们来看一下采用上述算法构造的覆盖网络,图3是一组数据集,其分布在一个等速螺旋曲线上,数据点个数为91,左子图为原始数据集,右子图为覆盖神经网络示意图,黑色线段表示用我们算法确定的一个超香肠神经元.这里ε=0.005,kmax=20,kmin=3.上面实验表明通过提出的算法可以减少覆盖神经元数目.我们再来构造一个识别实验,以证明虽然神经元个数减少了但是覆盖网络的识别能力并不会降低.实验2.实验数据集是分布在3条等速螺旋曲线上的点,每条曲线共取141个点,对其中两条曲Page6线上的点,我们取其中71个点作为训练样本构造覆盖网络(神经元阈值就取做每一邻域内点到直线段图4识别网络构造效果这个实验中测试样本一共281个其中,经过训练的样本类测试点的识别正确率为100%,而没有构造覆盖神经网络的样本集则完全被拒识.可见,算法是有效的,这里我们仍然采用超香肠神经元构造神经网络.如果采用RBF神经元,同样每一段线段对应一个神经元,由于RBF神经元可以看作一个超球,这里线段长度大致等于超球直径,则可以想见其覆盖区域要大于超香肠神经元,那么导致的误识率也将较大.5.2COIL20数据库COIL20数据库①包含20个对象.图5是COIL20数据库的10个对象的样本图片.每个对象在水平上旋转360°,每隔5°拍摄一张照片,因此每个对象共72幅图片.每幅图像为32×32像素大小的灰度图,故可以表示成1024维的向量.从数据集观察,可见每一类对象的样本点构成一个一维流形.我们随机取每个对象图片的60幅图片作为训练样本(其中50幅作为神经网络构造,而余下10幅用来训练神经元的覆盖阈值,阈值设定为所有训练距离的最大值)其余作为测试样本,而最后一类完全作为测试样本,如图4.样本到相应单纯形距离的最大值),其余12幅图片作为测试样本,来测试通过本文算法构造的多权值神经元覆盖网络的识别率.RBF神经网络、支撑向量机(SVM)以及传统方法构造的多权值神经元覆盖网络作为对比算法.整个实验循环10次,平均识别率如表1所示:算法类别识别率/%RBF神经网络87.45尺度参数设为所有样点SVM传统覆盖网络99.7549个超香肠神经元本文算法98.8326个超香肠神经元由表1可知,多权值神经元覆盖网络在COIL20数据库的识别实验中显示出明显的优势.而通过本文算法构造的多权值神经元覆盖网络可以采用较少的神经元构造网络,但识别率没有明显地降低,由此可见,本文算法是有效的.6结论本文讨论了仿生模式识别多权值神经网络的构造方法,主要是希望在保持神经网络对数据流形逼近能力的基础上采用尽量少的神经元来完成覆盖,从而降低网络复杂度和构造代价,通过实验证明我们的算法是比较有效的.但是需要说明的是,本文的实验都是在数据流形成一维情况下进行的,覆盖神经元采用的是超香肠神经元,而当数据流形维数增加时算法复杂度必定也会增加,但在利用多维尺度①COIL20数据库.http://www.cs.columbia.edu/CAVE/Page7变换进行线性邻域选择时,我们并没有限定数据流形的本征维数,同时在单纯形覆盖网络的构造算法第2步中,同样没有对数据流形的限制.因此,文中所述的单纯形神经元覆盖网络的构造算法实际上对于一维以上的数据流形,仍然是有效的.
