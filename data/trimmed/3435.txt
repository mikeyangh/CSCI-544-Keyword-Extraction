Page1自然语言处理中主题模型的发展徐戈王厚峰(北京大学计算语言学研究所,北京大学计算语言学教育部重点实验室北京100871)摘要主题模型在自然语言处理领域受到了越来越多的关注.在该领域中,主题可以看成是词项的概率分布.主题模型通过词项在文档级的共现信息抽取出语义相关的主题集合,并能够将词项空间中的文档变换到主题空间,得到文档在低维空间中的表达.作者从主题模型的起源隐性语义索引出发,对概率隐性语义索引以及LDA等在主题模型发展中的重要阶段性工作进行了介绍和分析,着重描述这些工作之间的关联性.LDA作为一个概率生成模型,很容易被扩展成其它形式的概率模型.作者对由LDA派生出的各种模型作了粗略分类,并选择了各类的代表性模型简单介绍.主题模型中最重要的两组参数分别是各主题下的词项概率分布和各文档的主题概率分布,作者对期望最大化算法在主题模型参数估计中的使用进行了分析,这有助于更深刻理解主题模型发展中各项工作的联系.关键词自然语言处理;主题模型;隐性语义索引;LDA;期望最大化算法;Gibbs采样1引言在自然语言处理中,主题(topic)①可以看成是词项的概率分布.我们使用主题模型对文档的生成过程进行模拟,再通过参数估计得到各个主题.当以词袋(bagofwords)形式表示文档时,其维度可能是数万.若指定主题模型的主题个数为K,通过主题模型的训练,最终形成了K个主题,则可以将词项空间中的文档变换到主题空间,得到文档新的表达.由于通常主题的个数K远小于词项的个数,常使用主题模型进行降维.在以文本为处理对象的领域中,降维后的新坐标(即在K个主题上的分量)往往具有语义上的特征.图1是在人民日报语料上通过LDA(LatentDirichletAllocation)模型训练得到的一部分主题.每个主题中的词项按照在该主题中的概率降序排列.其中主题1表示“国家”相关的概念,主题2表示了“中国人民代表大会”相关的概念等等.法国欧洲德国欧盟巴黎希拉克瑞典主题1主题2主题3主题4主题5图1人民日报语料在LDA模型上的训练结果(部分)主题模型的起源是隐性语义索引(LatentSemanticIndexing,LSI)[1].隐性语义索引并不是概率模型,因此也算不上一个主题模型,但是其基本思想为主题模型的发展奠定了基础.在LSI的基础上,Hofmann[2]提出了概率隐性语义索引(probabi-listicLatentSemanticIndexing,pLSI),该模型被看成是一个真正意义上的主题模型.而Blei等人[3]提出的LDA(LatentDirichletAllocation)又在pLSI的基础上进行了扩展得到一个更为完全的概率生成模型.近几年来,与特定的任务相结合,出现了越来越多的基于LDA的概率模型.本文第2节对主题模型的主要内容进行归纳;第3节简单介绍EM算法;第4节到第8节按照主题模型的发展过程依次介绍LSI,pLSI,LDA以及LDA的扩展模型;最后第9节总结全文并展望下一步的工作.2主题模型的主要内容一个主题模型通常包括5项内容(见2.1节~2.5节).一般,主题模型的输入和基本假设这两部分对大部分主题模型都是相同的,因此针对具体的主题模型分析时一般不再涉及.主题模型的表示、参数估计和新样本推断3个部分在不同的主题模型中有所不同,我们将在具体的主题模型中分别介绍.2.1主题模型的输入主题模型的主要输入是文档集合,由于交换性的假设(见2.2节),等价于词项文档(term-docu-ment)矩阵,图2是词项文档矩阵的一个实例.从该词项文档矩阵可以看出,语料包括6篇文档,整个语料中共有5个词项②,文档d1中ship和ocean,voyage三个词项各出现一次.注意同一个词项在一篇文档中可以出现多次.另外还有一个重要输入就是主题个数K.通常,K的大小需要在模型训练前指定,而且存在一定的经验性.确定最优K的简单方法是用不同的K重复实验,当评价指标如困惑度(perplexity)、语料似然值、分类正确率等最优时认为此时的K是模型的最佳选择[3-6].也有作者用非参数贝叶斯的方法来选择模型的合适主题数目[7-8],该方法假设主题个数为无穷多,实际主题个数可以随着语料的规模而变化,训练结束时的主题个数即K的最佳选择.2.2主题模型中的基本假设主题模型中的一个重要假设是词袋(bagofwords)假设,即一篇文档内的单词可以交换次序而不影响模型的训练结果.可交换(exchangeability)可以简单理解为与顺序无关,和条件独立同分布等价.事实上,通过观察2.4节中的似然函数,我们可以看出文档也是可交换的,即语料中文档的次序也①②Page3不影响模型的训练结果[3].需要指出的是,在LDA的一些派生模型中,一些可交换性会被打破,以便构造相应的模型,读者可以参考本文7.2小节中的有关实例.2.3主题模型的表示生成过程.对LDA模型的表示.以LDA模型为例[3],图3是使用图模型的方法主题模型的表示有两种,分别是使用图模型和方框表示其中的内容进行重复,右下角是重复的次数;灰色节点表示观测值,空心节点表示隐含随机变量或者参数,箭头代表依赖关系.α是θ的超参数,β是K×V的参数集合,每行代表某个主题中的词项概率分布,K是主题个数,V是词项个数;θ表示某文档的主题概率分布,共M个,M为文档个数.w为单词,z为w的主题标号.我们也可以通过生成过程来对主题模型进行描述,即LDA模型是按照如图4所示的方式生成一篇文档,重复M次则生成整个语料.2.4参数估计过程在主题模型中,最重要的两组参数分别是各主题下的词项概率分布和各文档的主题概率分布①.参数估计可以看成是生成过程的逆过程:即在已知文档集(即生成的结果)的情况下,通过参数估计,得到参数值.这些估计值也就是我们整个训练过程的输出结果.针对参数估计我们需要选择最优化的目标函数,在主题模型中通常是整个语料的概率值.以LDA模型为例[3],根据其图模型很容易得到语料概率值p(D|α,β)为d=1∫p(θdα)∏Nd∏M其中,D代表整个语料,也就是所有文档的集合;Nd表示第d篇文档长度;θd表示第d篇文档的主题概率分布;wdn表示第d篇文档的第n个单词;zdn表示wdn主题.该函数以α和β作为参数,通过对目标函数进行最大化来估计α和β的值.2.5新样本的推断主题模型训练完成后,我们便可以使用训练好的主题模型对新的样本进行推断,通过主题模型将以词项空间表达的文档变换到新的主题空间,得到一个以主题为坐标的低维表达,该表达也就是文档的主题概率分布.新样本的推断不仅可以针对新的文档,还可以针对查询,以便应用于信息检索之中.3期望最大化算法和参数估计期望最大化算法(ExpectationMaximization,EM)由Dempster等人[9]于1977年提出,是一种对具有隐变量(缺失数据)的概率模型寻找极大似然估计的一般性方法.该算法通过迭代不断修改模型参数直到达到局部最优点,即每次都用现有的模型推断隐变量的后验概率分布,然后对参数重新估计得到一个新的模型,如此反复直到满足终止条件.由于EM算法不能保证全局最优解,因此有的时候需要变换参数的初始值,或者选择较多的迭代次数,才能得到较为理想的参数估计值.在自然语言处理中,常见的诸如隐马尔可夫模型(HMM)、高斯混合模型(GMM)、k-均值算法(k-means)、主成分分析(PCA)等都可以用EM算法的思想来解释.一般情况下,主题模型中的参数估计问题很难得到精确解,可以使用EM算法来得到近似解.EM算法简介如下[10]:已知一个概率模型,包括:1.隐变量集Z;2.观测值集X;3.参数集θ目标:得到p(X|θ)最大化时的θEM算法过程:初始化θE步骤:以当前θold估计p(Z|X,θold)M步骤:利用前一步的结果,对θ最大化如下式子:∑Z重复E,M步骤直到满足结束条件.在主题模型中,主题通常表示为隐变量,单词为①各个模型在表示这两组参数的时候所用符号可能会不同,Page4观测值,而参数集通常就是各主题下的词项概率分布和各文档的主题概率分布.不同的主题模型中的观测值、隐变量和参数集都不尽相同,辨识这些元素有助于正确和快速理解主题模型.在以下几个主要的主题模型的分析中,我们将以EM算法的框架来理解参数估计过程.4隐性语义索引隐性语义索引中的奇异值分解(SingularValueDecomposition,SVD)与主成分分析(PrincipalComponentAnalysis,PCA)有着紧密的联系,在介绍隐性语义索引之前,有必要先对主成分分析作简单的介绍.4.1主成分分析主成分分析将高维的向量变换到低维空间,而且低维空间中各个维度不相关,基本过程是取协方差矩阵犛(见式(1))的前m个最大的特征值对应的特征向量来构造一个m维的新空间.此处m可以理解为主题模型中的主题个数K,也需要人为指定.对原始样本作近似时,可以证明该方法产生的误差最小[10].其中,N为样本个数;狓i为第i个样本;μ是样本均值.矩阵犛揭示坐标间的相关性,而变换后的样本在新坐标空间中的协方差矩阵是一个以降序特征值为主对角线元素的对角阵,因此在新的空间中各个坐标统计不相关.主成分分析的思想在LSI中有充分的体现,即构造原坐标间的相似度矩阵,通过特征向量对样本进行变换,在新的坐标空间中各个坐标间统计不相关,且新空间维度一般远小于原空间的维度.4.2隐性语义索引隐性语义索引通过奇异值分解构造一个新的隐性语义(LatentSemantic)空间[1].该空间通常比原空间维度低,文档或者单词可以变换到这个新的空间,找到更简单的表达.SVD示意图如图5所示[1].其中,犡是词项文档矩阵;t是词项空间的维度;d是文档个数;犝,犞都是正交单位矩阵;Σ是对角阵且主对角线上的元素值降序排列;m是犡的秩;犝是犡犡T的特征向量集;犞是犡T犡的特征向量集,犡犡T和犡T犡的特征值相同.犡犡T的元素(i,j)代表了词项i和词项j的共现次数(以文档为窗口范围).这个矩阵反映了任意两个词项之间的相似度.犝代表了词项空间到主题空间的转换.在LSI的介绍中一般没有提及参数估计的问题,但通过主成分分析我们仍然可以把LSI与EM算法联系起来.LSI可以看成是对两个相似度矩阵分别做了主成分分析,而主成分分析可以通过EM算法进行解释.我们可以把特征向量看成是待估计的参数,样本在新空间的坐标(隐性语义)看成是隐变量,套用EM算法的框架来迭代求解.这种方法尤其适合相似度矩阵维度很高无法直接处理,或者存在数据缺失的情况[11].在文献[10]中用了一个形象的实例解释主成分分析中EM算法的过程.无论是训练集中的文档,还是一篇新的文档,都可以通过SVD分解后得到的矩阵把文档变换到隐性语义空间,公式如下其中,狓为词项空间的文档;狔为狓在主题空间的表示,均为列向量.在信息检索中,可以把一个查询请求看成是一篇文档,从而将其变换到主题空间,并在该空间寻找与之匹配的文档.类似地,对于犡T犡可以理解为文档间的相似度矩阵,得到它的特征向量集犞后,可以把一个单词变换到新的主题空间.LSI的详细例子可以参考文献[1].5概率隐性语义索引概率隐性语义索引(probabilisticLatentSe-manticIndexing,pLSI)①是Hofmann[2]在1999年提出的一个主题模型.同LSI相似,pLSI寻找一个从词项空间到隐性语义(主题)空间的变换,但pLSI是一个概率生成模型,而且选择了不同的最优化目标函数.5.1模型表示图6中,d代表文档标号,z是主题,w是单词,①在该文中,作者将该模型称之为AspectModel,在不引起混Page5其中只有z是隐含变量,M代表文档数目,N表示文档的长度.观察该模型的生成过程描述(见图7),容易得到模型的两组主要参数:p(w|z)和p(z|d),即各主题下的词项概率分布和各文档的主题概率分布.由于没有指定概率分布的类型,这两组参数其实就是两张二维的参数表,需要通过参数估计确定二维表中每个参数的值.5.2参数估计根据模型的表示,我们可以按照EM的框架找出模型中的各个对应成分,分别是:pLSI模型中的w,d为观测值,z是隐变量,p(w|z)和p(z|d)为待估计的参数.不难看出p(w|z)相当于某主题下的词项概率分布;p(z|d)相当于某文档的主题概率分布.整个语料的概率对数值定义如下:其中,n(d,w)是d文档中w出现的次数;p(d,w)是(d,w)对的概率.参数估计的EM过程如下:E步骤.在当前的参数估计下,隐变量z的后p(zd,w)=p(z)p(dz)p(wz)验概率表示为M步骤.根据上一步的结果对完整数据的期望值进行最大化,得到更新参数的公式p(wz)=∑dp(dz)=∑w其中,R≡∑d,w易算出p(z|d),从而得到了p(w|z)和p(z|d)两组参数.为了防止过拟合,在E步骤中,可以引入控制参数b,且b<1.详细内容可以参考文献[2].5.3新样本的推断在pLSI中,对于新样本的推断仍然采用EM算法完成.不过由于我们只需要得到新样本dnew在主题空间的表达p(z|dnew),而不需要修改p(w|z),因此只在EM算法中M步骤更新p(z|dnew)而保持p(w|z)不变.这和LSI的处理不同,因为LSI在对新样本向低维的隐性语义空间变换的时候只需要作矩阵运算.5.4pLSI和LSI的关系两者的差异是很明显的.LSI不是概率生成模型,因此无法用文档的生成过程来解释LSI,从而也无法将不同类型的语义结构和语法角色引入到LSI中.pLSI作为生成模型,具有概率基础,也容易进行模型扩展.此外,LSI和pLSI最优化的目标函数不同:LSI以最优低秩逼近为优化的目标函数,而pL-SI以观测值的似然值为优化目标函数.另外,LSI的SVD分解得到的是全局最优解,而pLSI得到的是局部最优解.即便如此,pLSI模型仍然取得了比LSI更好的效果[2,12-13].尽管两者存在差别,但是,如果我们仅考虑从词项空间向主题空间转换,那么两者又是十分相似的.我们可以找出以下的对应关系[2],比如:LSI的犝矩阵对应pLSI中的p(wj|zk)j,k;犞对应p(di|zk)i,k;而Σ对应diag(p(zk))k.当然,犝,犞矩阵中的元素取值可以为负,这也是LSI缺乏概率基础的一个表现.而在pLSI中对应的元素是非负的概率值.正如pLSI的命名,它在概率化的LSI,而基本思想却是源自LSI.值得一提的是,在pLSI提出的同年(1999),Lee等人[14-15]提出了非负矩阵分解(Non-NegativeMatrixFactorization,NMF),在某些条件下被证明和pLSI等价[16-17].6LDA模型Blei等人[3]在2003年提出了LDA(LatentDirichletAllocation).他们在pLSI的基础上,用一个服从Dirichlet分布的K维隐含随机变量表示文Page6档的主题概率分布,模拟文档的产生过程(见图3).Griffiths等人[4]又对β参数施加Dirichlet先验分布,使得LDA模型成为一个完整的生成模型(见图8).LDA主题模型及其扩展正被越来越多地应用于图像处理、自然语言处理等领域.近些年出现的主题模型或多或少与LDA模型存在联系,因此,理解LDA模型对于把握主题模型的发展是十分有意义的.6.1模型表示图8中,φk表示主题k中的词项概率分布;θm表示第m篇文档的主题概率分布.θm,φk又作为多项式分布的参数分别用于生成主题和单词.K代表主题数目,M代表文档数目,Nm表示第m篇文档的长度,wm,n和zm,n分别表示第m篇文档中第n个单词及其主题.α和β是Dirichlet分布的参数,通常是固定值且对称分布(symmetric)①,因此用标量表示.θm,φk均服从Dirichlet分布,该分布函数如下式所示其中,0μk1,∑kμk=1;α0=∑K函数.LDA的生成过程如图9所示.对于多项式分布函数而言,Dirichlet是其共轭先验分布,可以简化模型中的计算.其中Dirichlet的先验α和β的经验值取值一般为α=50/K,β=0.01,起到平滑数据的作用.在一些情况下,也可以使用语料对α和β进行经验贝叶斯估计.根据Dirichlet分布函数的性质可知,先验变大表示概率密度越集中于K-1维Simplex的中间区域,可以得到更为均匀的概率分布[5].本节中选用的模型表示参照文献[18],与Blei提出的LDA模型表示略有差别,但实际需要估计的参数相同,并无本质差异.6.2参数估计LDA的参数估计方法有变分贝叶斯推断(Var-iationalBayesianInference,VB)[3]、期望传播(Expectation-Propagation,EP)[19]和CollapsedGibbsSampling[4]等.此外,Teh等人[20]提出了CollapsedVariationalBayesian(CVB)方法,结合了CollapsedGibbsSampling和VariationalInference两种方法.每种参数估计方法都各有利弊,选择一个合适的近似算法要在效率、复杂性、准确性和概念简洁性之间综合考虑[20-21].无论哪种方法,我们所要处理的任务是相同的,即根据给定的最优化目标函数,得到对参数的估计值.由于Gibbs方法描述简单且更容易实现,成为主题模型中最常采用的参数估计方法.本文选择CollapsedGibbsSampling方法进行介绍[18].所谓“Collapsed”的含义是指通过积分避开了实际待估计的参数θm和φk,转而对每个单词w的主题z进行采样,一旦每个w的z确定下来,θm和φk的值可以在统计频次后计算出来.因此,问题转为计算单词序列下主题序列的条件概率,然后进行主题序列的采样,公式如下其中,狑表示所有文档首尾相接而成的单词向量;狕是其对应主题向量.由于狕的序列通常较长,可能取值随向量长度指数增长,一般无法直接计算.这时我们可以考虑使用Gibbs采样把问题进行分解,每次对一个隐变量(主题)进行采样.Gibbs采样是马尔可夫链蒙特卡洛方法(Markov-chainMonteCarlo,MCMC)的特例,每次对联合分布的一个分量进行采样,而保持其它分量的值不变[10].对于联合分布维度较高的情况使用Gibbs采样可以产生比较简单的算法.①注意此处的β与图3中的β含义不同,图3中的β对应图8Page7经过推导,最终的采样公式如下p(zi=k狕i,狑)∝其中,假设wi=t;zi表示第i个单词对应的主题变量;i表示剔除其中的第i项;n(v)现词项v的次数;βv是词项v的Dirichlet先验;n(z)表示文档m中出现主题z的次数;αz是主题z的Dirichlet先验.一旦获得每个单词w的主题z的标号,我们需要的参数计算公式表示如下式(11):其中,φk,t表示主题k中词项t的概率;θm,k表示文档m中主题k的概率.因此,只要知道了每个单词的主题标号,那么我们就可以通过简单计数的方式对参数进行估计.6.3新样本的推断单词的隐含主题采样公式如下已知训练好的模型M,任给新文档狑~,其中每个p(z~i=kw~n(t)其中,狕~代表新文档狑~对应的主题向量,其余符号含义请参考式(9)~(11)的解释.通过前面提到的Gibbs采样方法,最终我们可以得到每个单词的主题标号,然后套用公式计算出该文档在各个主题分量上的值后,一篇词项空间的文档就获得了在主题空间中的表示.6.4LDA参数估计与EM算法联系LDA的参数估计方法有多种,我们也可以套用EM算法的框架来进行理解.在Collapsed的Gibbs采样中,由于将参数θm和φk通过积分消去,所以上述EM算法中每次迭代的M步骤被省去,只需要对主题序列进行采样,等采样结束再根据式(11)计算参数,作为最终的参数估计结果.我们可以这样理解:在E步骤中得到一个后验分布p(狕|狑)的采样,用来近似计算似然期望值,并供M步骤最大化使用.需要指出的是,用EM框架理解Collapsed的Gibbs方法时,M步骤的参数估计结果在E步骤中没有用到,所以不需要重复多余的M步骤,只需在最后进行一次M步骤,得到所需要的参数θm和φk即可.这种在E步骤中用后验分布的采样代替后验分布并用于近似数据似然值的处理称为随机(stochastic)EM,是蒙特卡洛EM的一个特例[10].使用变分贝叶斯推断[3]或者期望传播[19]方法来对LDA的参数进行估计时也采用了EM算法的框架,详细内容请参考文献[3,19].6.5LDA和pLSI的关系LDA模型可以看成是对pLSI进行了贝叶斯化,使得参数具备了概率分布,变成了随机变量.事实上,在图3中去掉α,或者在图8中去掉α、β,得到的就是pLSI模型.也就是说,pLSI是对参数作最大似然估计,而LDA是在参数有先验分布的情况下对参数作最大后验估计.针对图3表示的主题模型,Girolami[22]称pLSI是LDA模型在α先验为1的情况下的最大后验或者最大似然估计.因为对于Dirichlet分布,α为1时先验失效,所以此时最大后验估计和最大似然估计等价.这样,pLSI可以纳入LDA的框架.之所以把LDA看成是比pLSI更为彻底的生成模型,就是因为在LDA中把p(z|d)和p(w|z)看成了随机向量(见图8中θ和φ),指定了先验概率分布;而在pLSI中仅把它们当做参数来估计.这样来看,主题模型从pLSI发展到LDA是非常自然的.6.6LDA模型的直接应用首先,LDA模型可以作为一种降维的工具.由于LDA模型训练完成后,能够得到一个文档在主题空间的表示,一些在词项空间中的文档处理可以通过LDA模型转而在主题空间中完成,比如文档分类[3]、聚类等.此外,利用主题模型中的参数估计值,可以完成协同过滤(collaborativefiltering)[3]、单词或文档相似度计算[5]、文本分段[8]等任务.一般而言,直接使用LDA模型只是作为具体任务的一个环节,究竟如何使用LDA模型还要结Page8合实际情况,本文不再详述.7LDA模型的扩展目前,主题模型相关的工作大多是对LDA模型进行修改,或者是将LDA模型作为整个概率模型的一个部件.虽然也存在一些和LDA模型无直接关系的主题模型,但作为词项概率分布的主题贯穿所有的主题模型,而这和LDA中的主题并无实质差异.因此,本节以LDA模型为线索,通过介绍其扩展来反映主题模型在近年的发展.由于针对LDA扩展的研究工作非常多,本文中难以全面涉及.我们对这些扩展作了粗略分类,简单介绍每类中一些具有代表性的工作.7.1对参数的扩展我们知道,在主题模型中最重要的两组参数就是各主题下的词项概率分布和各文档的主题概率分布,通过对它们进行扩展,使得模型更接近数据的真实情况.在LDA模型中,假设每个文档的主题概率分布θ服从Dirichlet分布,并没有对不同主题之间相关性进行刻画.然而,在真实的语料中,不同主题之间存在相关性的现象很普遍.在2004年,Blei等人[23]提出了主题间为树结构的层级LDA(Hierar-chicalLDA).在该模型中,树中的每个节点代表一个主题.其生成过程如下:首先针对文档选择一条从根到叶节点的路径;然后按照各层的比重,选择路径中一个节点作为主题,以该主题的词项概率分布生成单词,重复直到生成整篇文档.该模型还有一个特点是可以从语料中估计出主题的个数,并与使用LDA模型在不同主题数下重复实验得到的最佳主题个数一致[7].Blei等人[24-25]于2006年又在LDA的基础上提出了相关主题模型(CorrelatedTopicModel,CTM).与LDA不同的是,CTM从对数正态分布中对主题概率分布θ进行采样,先验参数包括一个协方差矩阵,描述每对主题之间的相关性.Li等人[26]针对CTM只考虑两个主题间关系的不足,提出了PAM模型(PachinkoAllocationModel),该模型的特点是把主题之间的关系表示成一个有向无环图,其中叶子节点是单词,而非叶节点(主题)可以看成是由所包含的子节点(主题或单词)构成.PAM和层级LDA模型的一个区别是:前者的主题可能是词项的概率分布,也可能是其它(子)主题的概率分布,而层级LDA中的每个主题都是词项概率分布.之后,Mimno等人[27]又在PAM的工作上提出了层级PAM模型(hierarchicalPAM),该模型可以看成是把层级LDA和PAM结合起来,使得PAM模型中的非叶节点也具有词项的概率分布.Wang等人[28]向LDA模型中添加了一个作为观测值的时间随机变量后得到了主题随时间变化的主题模型(TopicOverTime,TOT),该模型认为主题概率分布受到时间信息的影响,而时间变量服从beta分布,归一化到[0,1]之间.Blei等人[29]在2006年提出了动态主题模型(DynamicTopicModels,DTM),他们认为主题会随着时间变化,且满足一阶马尔可夫假设,图模型如图10所示.可以看到,主题概率分布θ的超参数α以及主题中词项的概率分布参数β随时间变化,且依赖于前一个时间片的状态.7.2引入上下文信息通常主题模型假设单词序列中的单词是可交换的,即单词的顺序和模型的训练结果无关.然而,有时需要引入一些上下文信息,考虑当前节点和其它节点的关系,这就破坏了LDA的可交换性假设.Griffiths等人[30]认为可以通过HMM来捕捉句法结构信息,而通过LDA来揭示语义关系,并将两者结合在一起提出了HMM-LDA模型,见图11.该模型把主题分成两类:一类是功能主题,比如代词主题、介词主题等;一类是概念词汇,主要是名词和动词等有具体语义的主题.实验的结果证明该模型是有效的,能够把两类主题分开,并可以计算出主题之间的转移概率.Wallach[31]认为,在生成过程中,一个单词除了依赖于其对应的主题外,还与前一个单词有关,提出超越词袋(BeyondBag-of-Words)的主题模型.这个模型可以看成是LDA模型和单词二元组模型的结合.Wang等人[32-33]将搭配引入到主题模型中提出了TNG模型(Topicaln-gramModel),作者认为两个相邻单词之间是否搭配不仅与前一个单Page9词有关而且受前一个单词的主题影响,比如whitehouse在white的主题是政治时应该搭配,而如果white主题是颜色,那么应该分成两个单词.该模型的最明显的特征是:主题不再只是词项的概率分布,还可以是词组(词项的搭配)的概率分布.Gruber等人[34]提出了隐性主题马尔可夫模型(HiddenTopicMarkovModel,HTMM),与许多对每个单词指定一个主题的模型不同,该模型以句子为单位分配主题.即同一句话内所有的单词共享同一个主题,当句子切换时,按照Bernoulli分布对句子重新选择主题.Boyd-Graber等人[35]提出了句法主题模型(Syn-tacticTopicModel,STM),该模型的特色是在选择主题时不仅要考虑整个文档的主题概率分布,而且还要考虑句法树中父节点的主题类型.为了使用该模型,要先对语料进行依存句法分析得到语法树.模型训练完成后所获得的主题同时呈现语义和句法上的相关性,并且主题之间的转移概率也被估计出来.7.3面向特定任务本小节对基于LDA模型的面向特定任务的研究工作进行介绍,涉及分类、作者主题模型、词义消歧、引用链接分析、人名消歧、情感分析等问题.Blei等人[36]针对文本分类问题提出了有监督LDA模型(supervisedLatentDirichletAllocation,sLDA),该模型将训练语料中的文档类别标记作为观测值加入LDA模型,且类别标号服从一个与文档主题概率分布有关的正态线性分布.对于新文档,可以通过该模型判定新文档的类别标号.李文波等人[37]在2008年提出Labeled-LDA模型,该模型将参数按照类别细化,并应用于文本分类任务.Steyvers等人[38]提出作者主题模型(Author-Topic,AT),认为每个作者有一个主题概率分布.文档的生成过程是:随机选择一个作者,根据这个作者的主题概率分布,生成一个词,重复该过程直到生成整个文档.注意一篇文档可以由多个作者共同完成.McCallum等人[39]又在AT模型的基础上,提出了作者接受者主题模型(Author-Recipient-Topic,ART),如图12所示.该模型针对具有方向性的文档(比如电子邮件),将发送者和接受者对(pair)看成是一篇文档的主题概率分布的决定因素.通过积分或求和可以分别得到同一个人在接受者和发送者两个角色时的主题概率分布.进而,我们还可以使用这些主题概率分布进行聚类,判定哪些人具有相同的社会角色.比如说:如果一些人作为接受者时总是收到诸如要求复印、旅行预约、安排会议室等信息,那么我们认为他们具有“行政助理”这样的社会角色,即便这些人所处的社会关系完全不同.Boyd-Graber等人[40]提出了一个基于Wordnet的LDA模型(LatentDirichletAllocationwithWORDNET,LDAWN).通常,我们用一个词项概率分布来表示一个主题,但是在LDAWN模型中,作者针对每一个主题,定义了一个同义词集(synset)的转移概率矩阵.在生成过程中,首先选择一个主题概率分布,然后根据该主题概率分布选择一个主题,到此都与LDA的做法相同.接下来,LDAWN选择了一条以entity为根节点,不断游历(walk)直到碰到一个由单词构成的叶节点,输出该单词.我们可以这样认为,即便是相同的一个单词,由于其语义(主题)的不同,在生成该单词时可能在Wordnet中选择一条不同路径.Nallapati等人[41]在2008年提出了Link-PL-SA-LDA模型,对于任给的测试集中的文档可以预测其引用其它文档的概率.该模型分两部分,一部分针对所有被引用文档构造一个pLSI模型;另一部分则针对所有引用文档的一个Link-LDA模型,对于一篇文档而言,该模型不仅生成其中的所有单词,而且生成所有的Link,而Link所指向(引用)的文Page10档就是那些在pLSI模型训练时使用的文档.由于重名以及同一个人的姓名有多种写法,存在人名的消歧问题.Bhattacharya等人[42]提出了一个基于LDA的无监督的实体消解模型来处理人名的消歧问题.该模型使用书目(bibliography)信息作为输入进行训练,完成训练后,可以推测一条书目中实体引用(作者姓名)对应的真实实体(作者实体).该模型把一条书目的所有作者姓名看成是文档中的单词,构建了称为组(group,相当于LDA模型中的主题)的隐含变量,每个组代表一个作者实体的概率分布,而作者引用的生成是作者实体的属性(attribute,可以理解为作者的全名)经过噪声变形得到的(如中间名缩写,甚至省略等).该方法还能从数据中推断出真实实体的数目.Song等人[43]提出了一个和AT模型类似的LDA扩展模型,用于无监督的人名消解,该方法把文档的每个作者姓名看成是文档生成的单词,添加了一个作者姓名变量作为观测值.这样不仅可以得到某个主题下词项的概率分布,还可以得到该主题下作者姓名的概率分布.实际作者的个数可以通过聚类后的聚类个数进行判定.需要注意的是,Song的方法考虑到了文档的内容,而Bhattacharya的方法只是关注作者姓名的共现.Mei等人[44]提出了一个主题情感混合模型(Topic-SentimentMixture,TSM),该模型把单词分成两大类,一类是与主题无关的普通单词(如the,a,of),另一类和主题有关的单词又分为中性(可细分为k类)、正面和负面三大类.单词的生成过程是依照概率分布在四个大类之间选择类,进而在类内选择单词.EM算法被用来估计每个类中的词项概率分布.此外,还对情感随时间的动态变换进行了分析,判断出某些单词随时间变化出现情感极性的波动和爆发(burst).Titov等人[45]提出了一个文本和特征(aspect)评价的混合模型,认为一篇文档可以由滑动窗口(slidingwindow)的集合构成,而每个滑动窗口又由连续的若干句子组成.在一个滑动窗口中存在局部主题的概率分布,而整篇文档对应一个全局主题的概率分布.单词可以从局部主题的概率分布中生成,也可以从全局主题的概率分布中产生.在有关旅游评价的语料中,全局主题对应于实体,如Londonhotels,seasideresorts,而局部主题对应于特征,比如location,service,room等.作者还将每个特征的评分作为观测值加入到模型,并假定对特征讨论的文本是对该特征评分的预测信息,这样将所需要的特征和主题关联起来,避免了LDA模型这种无监督学习中出现的主题含义无法显式确定的问题.Doyle等人[46]提出DCMLDA模型来检测文档中单词的爆发(burstiness)现象(即某单词突然大量出现).和标准LDA模型相比,DCMLDA模型中每个文档都有特定的K个主题,K为全局主题个数.训练结束后,对于一个文档,可以检查K个主题中哪些单词出现了burstiness现象.在delicious网站中,每个页面对应若干个标记(tag).但是,在应用这些标记的时候,所采用的标准并不一定完全一致.为了将文档中的单词和标记进行关联,Ramage等人[47]提出了一个多标记文本分类器,称为LabeledLDA模型.作者考虑文档集合中所有可能的标记,让每个标记(tag)对应一个主题.在训练时,一个文档的主题的个数就是文档中标记的个数.训练结束后,我们就能对每个单词知道其对应的主题,从而知道其标记.基于此,作者还进行了文本片段(snippet)抽取和多标记文本分类等任务.Gerrish与Blei[48]提出了DIM模型(DocumentInfluenceModel)来识别文档集合中最有影响力的文档.该方法基于Blei等人在2006年提出的DTM模型,把文档集合按照时间进行切片,并对每个文档附加一个影响力的隐变量.计算文档的影响力并不是一个新的任务,DIM模型的最大特色是没有使用文档间的引用关系.该模型假设:一篇文档的影响力越大,则后续时间片中的主题越受到这个文档的影响.实验结果表明,DIM模型得到的文档影响力和引用率有着很强的相关性.针对具体任务的主题模型十分丰富,即便是相同或者类似的任务,都会存在多个主题模型.其区别可能是结构不同,隐变量不同,甚至是边的方向不同.在此不一一列举.本节汇总见表1.8主题模型发展的一些趋势总体上来说,主题模型的大部分工作集中在面向特定的任务之中.对参数的扩展和引入上下文的信息的工作相对而言较少,主要原因是后两种类型的工作是针对主题模型的整体修改,可以入手的研究点不多.除此之外,尤其是在近几年,我们也注意到了一些新的趋势.首先,出现了许多关注主题模型性能的工作.这Page11说明主题模型已经不局限于理论研究阶段,它的实用性得到认可,因此呼唤更加高效的训练算法.Nallapati等人[49]提出了并行的变分EM(Varia-tionalEM)算法来对训练过程进行加速,以便应用于多处理器和分布式环境.Asuncion等人[50]给表1主题模型扩展中所介绍的主题模型汇总2004年,Blei等人[23]2006年,Blei等人[24]2006年,Li等人[26]2007年,Mimno等人[27]hierarchicalPAM2006年,Wang等人[28]2006年,Blei等人[29]2004年,Griffiths等人[30]HMM-LDA2005年,Wang等人[32]2007年,Gruber等人[34]HTMM(HiddenTopicMarkovModel)以句子为单位分配主题.2009年,Boyd-Graber等人[35]STM(SyntacticTopicModel)2008年,Blei等人[36]2004年,Steyvers等人[38]AT(Author-Topic)2004年,McCallum等人[39]ART(Author-Recipient-Topic)2007年,Boyd-Graber等人[40]LDAWN(LatentDirichletAllocation2008年,Nallapati等人[41]Link-PLSA-LDA2006年,Bhattacharya等人[42]LDA-ER2007年,Song等人[43]2007年,Mei等人[44]sLDA(supervisedLatentDirichletAllocation)文档有类标号.2008年,Titov等人[45]2009年,Doyle等人[46]2009年,Ramage等人[47]LabeledLDA2010年,Gerrish等人[48]DIM(DocumentInfluenceModel)另外一个较为明显的趋势是主题模型和跨语言的结合.其中一个原因是机器翻译本身是自然语言处理的热点,积累了大量的跨语言的语料,可供主题模型使用.Ni等人[56]针对Wikipedia提出了一个ML-LDA模型来从跨语言的语料中抽取主题.每一个主题都对应多种语言.这样,不同语言的新文档都能够用统一的主题来表示,适合跨语言的网络应用.Mimno等人[57]提出的PLTM与此非常相似.以上两个工作所使用的语料是文档级对齐的,因此在语料上还有所限制.Jagarlamudi等人[58]提出了JointLDA模型,用来同时对西班牙语和英语语料进行采样.该模型使用了一个双语词典.模型训练结束后,每个主题可以是不同语言的混合主题.作者将该模型应用到跨语言的信息检索中取得了较好的效LDA模型和HDP模型提出了分布式算法,在保证全局正确性的前提下,各个处理单元能够独立进行Gibbs采样.Hoffman等人[51]提出了LDA模型的在线(online)变分贝叶斯方法(variationalBayesian).其它关注主题模型性能的工作还有文献[52-55]等.果.该模型的最大特点是不需要对齐的文档.类似的工作还有Boyd-Graber等人[59]提出的MuTo模型.除此之外,还有一些工作并没有归入上面的分类.比如:Zhu等人[60]提出了CTRF模型,该模型能够融合单词的外部特征和单词间主题的依赖关系,是一种通用的机器学习方法,而非针对某个具体的任务.这种趋势值得我们关注.文献[61-62]假设文档之间是有关的,破坏了原始LDA模型中关于文档独立的假设.新的假设是相似的文档具有相似的主题分布.各种有新意的工作还有很多,不一一列举.本节列出的这些工作,在某种意义上说明主题模型在深度和广度上仍在进行着渗透,体现了主题模型的生命力.Page129总结和展望从主题模型的发展脉络来看,各个工作之间都有着紧密的联系和延续性.在LSI中,出现了隐性语义,而这实际就是现在主题模型中的主题.LSI通过对相似度矩阵计算特征向量构造了一个线性变换,将词项空间的文档变换到了隐性语义空间(主题空间).从词项空间到了隐性语义(主题)空间变换这一点来看,LSI,pLSI一直到LDA是一致的.它们的区别在于最优化的时候使用的目标函数不同,或者主题模型表示上有所差别.LDA主题模型作为概率生成模型,被直接或扩展使用在自然语言处理的众多任务中.对于主题模型而言,最重要的两组参数是各主题下的词项概率分布和各文档的主题概率分布.由于通常无法求得精确解,EM算法经常被应用在主题模型的参数估计中,而理解EM算法在主题模型各个阶段的具体使用,也有助于了解主题模型的发展中各项工作之间的关联.当然,关于人类语言的生成本质,学术界还存在争议.作为概率生成模型,主题模型也有其局限性.在今后的主题模型发展中,人们需要对语言和问题的本质进行更为深入的分析和观察,以便构造出符合实际问题的主题模型.
