Page1基于深度支撑值学习网络的遥感图像融合2)(西安电子科技大学智能感知与图像理解教育部重点实验室、智能感知与计算国际联合研究中心、1)(西安电子科技大学计算机学院西安710071)智能感知与计算国际合作联合实验室西安710071)摘要该文将深度学习用于遥感图像融合,在训练深度网络时加入了结构风险最小化的损失函数,提出了一种基于深度支撑值学习网络的融合方法.为了避免图像融合过程中的信息损失,在传统卷积神经网络的基础上,取消了特征映射层的下采样过程,构建了深度支撑值学习网络(DeepSupportValueLearningNetworks,DSVLNets),DSVLNets网络模型包含5个隐藏层,每一层的基本结构由卷积层和线性层构成,该基本单元提供了一种多尺度、多方向、各向异性、非下采样的冗余变换,该模型在网络训练完毕之后,取出各卷积层和第5个隐藏层的线性层作为网络模型的输出层.输出层的各卷积层图像融合采用绝对值取大法,得到融合后的各卷积层图像;另外,将线性层图像分别在过完备字典上进行稀疏表示,并对稀疏系数采用绝对值取大法进行融合,得到融合后的线性层图像;最后将融合后的各卷积层和线性层图像重构得到结果图像.文中使用QuickBird和Geoeye卫星数据验证本文所提方法的有效性,实验结果表明,与PCA、AWLP、PN-TSSC和SVT算法相比较,该文所提方法的融合结果无论在主观视觉还是客观评价指标上均优于对比算法,较好地保持了图像的光谱信息和空间信息.关键词深度学习;卷积神经网络;深度支撑值学习网络;过完备字典;遥感图像融合;机器学习1引言由于传感器技术的限制,地球观测卫星很难获得同时具有高空间分辨率和高光谱分辨率的遥感图像.例如QuickBird卫星和IKONOS卫星获得的单通道的全色图像具有较高的空间分辨率,可详尽描述地物的细节特征;而多光谱图像虽具有较多的光谱信息,有利于对地物进行识别、分类和解译,但其空间分辨率较低.将具有高空间分辨率的全色图像与光谱信息丰富的多光谱图像融合,能够获得高分辨的多光谱图像,从而为后期的目标检测与识别等处理提供基础[1].传统的全色图像与多光谱图像融合技术可以分为:成分替换法,多分辨分析法和迭代复原法.成分替换法原理简单、易实现,常用的成分替换算法有主成分分析(PrincipalComponentAnalysis,PCA)、强度色彩饱和度(Intensity-Hue-Saturation,IHS)和施密特正交变化(Gram-Schmidt,GS)等[2-7],该类融合方法能够获得较高的空间信息,但通常会产生严重的光谱扭曲.基于多分辨分析的融合方法利用Mallat提出的多尺度逼近思想,该方法首先对原始图像进行多尺度分解以得到图像在不同尺度分解层上的高频与低频系数,根据不同的融合策略分别对高频和低频系数进行融合,最后经过逆变换得到融合结果.多分辨分析方法在保持光谱信息的同时,较好地注入了全色图像的高频信息,它有效地解决了频谱失真的问题.常见的多分辨分析融合方法有小波变换[8-9]、曲线波变换[10]、轮廓波变换[11-12]和支撑值变换(SupportValueTransform,SVT)[13-15]等.由于小波变换只能捕获点奇异性,且缺乏平移不变性,难以真实反映图像中的边缘和轮廓等空间特征,它对图像并不是“最稀疏”的表示方法.随之出现的曲线波和轮廓波等多尺度分析工具较好地解决了二维或更高维奇异性,在图像处理方面得到了广泛且成功的应用.迭代复原法通过建立全色图像与多光谱图像的观测模型,将融合问题转化为一个复原问题,近年来受到了众多学者的关注.Li等人[16]首先提出了基于压缩感知的全色图像与多光谱图像融合方法,该方法对于不同卫星的多光谱图像和全色图像进行融合时,需要生成不同卫星的、规模较大的字典,实验结果表明,该方法取得了较为满意的融合结果,然而用来生成字典所用到的高分辨多光谱图像通常不易获得;文献[17]将全色图像和上采样后的低分辨多光谱图像作为训练图像集用以构造联合字典,这种方法训练字典时不需要高分辨率多光谱图像,但却需要大量的训练图像;2013年,Li等人[18]又提出了在不需要训练图像集的基础上采用字典学习的方法构造所需字典;同年,Zhu等人[19]提出了SparseFI融合方法,此方法在不需要大量训练图像集的情况下,仅由全色图像即可得到所需要的对偶HR/LR字典,其中HR字典是将全色图像进行分块而得到,对全色图像进行下采样,再取其相对应的块作为LR字典,这种构造字典的方法简单,但存在的缺点是如果图像块的结构信息较弱或没有结构信息,则从字典中选择适当的原子是一件比较困难的事情;Two-Step方法的提出,有效利用了低分辨多光谱图像和高分辨全色图像的局部结构相似性[20].近年来,深度学习在图像处理领域显示出巨大的成功.加拿大多伦多大学的Hinton教授以及他的学生Salakhutdinov[21]于2006年在《Science》杂志上发表了一篇关于深度学习的文章,文章指出深度学习网络结构具备以下两个特征:第一,包含多个隐藏层的人工神经网络具有优异的特征学习能力,经过网络学习得到的特征对数据本身有着本质的刻Page3画;第二,可以通过“逐层初始化”(layer-wisepre-training)来降低训练深度神经网络的难度,通过无监督学习实现“逐层初始化”.深度学习能够模拟视觉感知系统的层次结构,通过建立包含有多个隐藏层结构的机器学习模型,对大量数据的训练得到更为有用的特征;它起源于神经网络,是一种深层次的非线性网络结构,通过这一非线性结构更优地逼近复杂函数,深层的结构使深度学习具有极强的表达能力和学习能力,当它用于处理图像时,能够学习到图像的“部分-整体”的分解关系,它在人脸识别、字符识别、图像去噪[22-23]等方面取得了远优于传统浅层神经网络的结果.卷积神经网络(ConvolutionalNeuralNetworks,CNNs)是一种典型的深度学习模型,它利用空间相对关系减少参数数目以提高训练性能,是第一个真正获得成功的深度架构[24].CNNs融合了局部感受野、共享权值和下采样来实现位移缩放和扭曲不变性,如LeNet5是一个典型的CNNs,它有2个卷积层、2个采样层和2个全连接层[25].首先,局域感受野指的是每一个卷积层的神经元都只与它上一层的一个小邻域内的神经单元相连接,以提取初级的视觉特征;第二,权值共享指的是同一个特征图中的神经元可以共享网络权值,它的优点是可以减少网络结构中的参数;第三,下采样降低了特征图的分辨率,降低了位移缩放和扭曲的敏感度;由此可见,CNNs的3个特征保证了卷积神经网络的平移不变性,减小了对位移缩放和扭曲的敏感度以及对提取出的特征出现位置的不敏感性.在卷积神经网络之后,也出现了许多其他的深度学习模型,如去噪自动编码[26-27]、DCN[28]和Sumproduct[29]等.尽管卷积神经网络可以识别有变换的模式,具有一定的鲁棒性,并已成功应用于人脸识别、车牌识别[30]、行为识别[24]、语音识别[31]和图像分类[32]等计算机视觉的研究领域,目前很少有将它用于图像融合的工作.本文将深度学习用于全色图像与多光谱图像融合,在传统卷积神经网络(ConvolutionalNeuralNetworks,CNNs)的基础上,构建了深度支撑值学习网络(DeepSupportValueLearningNetworks,DSVLNets)模型,该模型每一层的基本结构包括卷积层和线性层,该基本单元提供了一种对输入图像的多尺度、多方向、各向异性、非下采样的冗余变换.同时该模型去掉了卷积神经网络中的下采样过程,使得输入图像在进行各级滤波时,自适应地学习出不同的支撑值滤波器,以获得对图像的最优表示.本文所提方法用于全色图像与多光谱图像融合时突显出了3个优点:(1)DSVLNets模型在学习过程中不仅仅依赖于经验风险,它能够自适应地学习各级分解时的最优支撑值滤波器;(2)多尺度支撑值滤波器能够最优发现图像的本质特征,有效提取各级图像的高频信息;(3)将全色图像的高频信息注入到光谱图像中,减少了图像的空间与光谱扭曲,与已有融合算法相比较,本文所提方法融合图像质量较高.本文在第2节中详细介绍深度支撑值学习网络的构建模型;在第3节中给出深度支撑值学习网络的计算过程以及全色图像与多光谱图像的融合步骤;将深度支撑值学习网络用于全色图像与多光谱图像融合,并将融合结果与经典的融合方法作比较;在第4节中给出具体的实验结果,并对实验结果进行分析.2深度支撑值学习网络2.1深度卷积神经网络(CNNs)深度学习网络模型可被视为包含了多个隐藏层的人工神经网络模型,为了得到有效的特征以提高预测与分类的准确性,需要利用海量的训练数据来训练网络结构.在深度神经网络中,通过对上一层输出的非线性变换来实现复杂函数的逼近性能.CNNs模型是一个典型的深度学习模型,它的优势在于可以将图像直接作为网络的输入,它的权值共享网络结构降低了网络模型的复杂度,减少了权值的数量.卷积神经网络结构图如图1所示,从图1可以看出,多层的结构组成了CNNs模型,其中网络的每一层都由数个二维平面组成,每个二维平面又都由数个独立神经元构成,网络中的任一中间级都由C层和S层串接而成,其中C层为卷积层,S层为特征映射层,通过映射面上神经元的权值共享,降低选择参数的复杂度和减少参数的个数.2.2深度支撑值学习网络在深层神经网络的各模型包括上述CNNs中,各层中的滤波器大都通过最小化经验风险获得系数.然而,最小化经验风险的准则难以保证训练出的网络具有好的推广性能,类似于支撑向量机(SupportVectorMachine,SVM).在本节中我们使用如图2所示的网络基本单元.Page4图1卷积神经网络结构示意该基本网络包括卷积层和线性层,令输入X∈Rd(y∈R),Rd为输入空间,d为维数,通过卷积操作C后,再经过隐藏层神经元的传递函数进行映射(X):Rd→Rq,q为特征空间的维数,其输出经过线性层,通过权值W和偏置b后得到输出结果.在基本单元的训练中,输入训练数据{xi,yi}N估计函数为f(C,W,xi)=犠T(CX)+b.定义目标函数:R(C,W)=∑其中L[yi,f(C,W,xi)]=[yi-犠T(CX)-b]2.基于深度神经网络中表征学习的思路,令期望输出等于输入.类似于SVM中求解权值的思路,首先将基本网络的估计函数写为这里K(CX,CXi)=(CX)T(CXi),i=1,…,N是核函数,αi是支撑矢量犡i的支撑值.将估计函数写成矩阵形式:其中:Ωij=Kij+Iij/γ;Kij=K(CXi,CXj);犢=yi,…,y[用交替迭代算法优化网络参数,更新过程如下:首先固定C,计算α,b的解析解:下一步,令A=Ω-1,犅T=一步表示为定义犙=A(I-1犅T),犙是一个(N×N)的矩阵,假设各像素点的支撑值与以该点为中心的映射领域的中心点的支撑值相似,则取犙的中央行向量组成方阵,获得卷积层中的权值C.在网络训练完毕之后,取出卷积层的输出,可以得到一系列的支撑值图像{I1,I2,…,Ik,…,Il}和序列近似图像,序列近似图像则由原始图像P与它的支持值图像的差得到其中:k(k=1,2,…,l)为网络模型的隐含层层数;F1,F2,…,Fk,…,F{为卷积运算.则图像的重构过程为3基于DSVLNets的融合算法本文将深度学习理论用于遥感图像融合,基Page5于卷积神经网络结构构建了深度支撑值学习网络(DeepSupportValueLearningNetwork,DSVLNets),该网络模型在使用深度卷积神经网络时,去掉了隐藏层的下采样操作.DSVLNets网络模型可以有效选取输入图像在各级分解时使用的最优支撑值滤波器,使其能够获取图像的本质特征.经DSVLNets学习得到的序列卷积层图像和线性层图像,对序列卷积层图像采用绝对值取大法进行融合,得到融合后的序列卷积层图像;对线性层图像在过完备DCT字典下进行分块稀疏表示得到块稀疏系数,对稀疏系数使用绝对值取大法进行融合,进而得到融合后的线性层图像;最后,将融合后的序列卷积层图像与线性层图像按照式(7)进行重构,得到融合后的高分辨多光谱图像.3.1DSVLNets模型全色图像与多光谱图像的融合要尽量保持光谱图像的光谱信息和全色图像的空间信息,因此本文对传统卷积神经网络结构(图1)进行了修改,取消每一单元池化层S的下采样过程,将其修改为前层输出图像与卷积层图像C的差值层,修改后的网络结构保持了隐含层所有图像的大小与输入图像大小相同的特点,并将修改后的网络结构用于全色图像与多光谱图像的融合,网络计算过程如图3所示.除去输入层和输出层,中间隐含层共5层,即由5对C-S串接构成隐含层,其中C层为卷积层,S层为线性层;首先对低分辨的多光谱图像进行上采样,使其大小与全色图像大小一致,将上采样后多光谱图像的4个波段与全色图像相拼接,构成网络模型的输入层;输出层输出的是网络结构的各卷积层图像以及最后一个隐藏层的线性层图像.(1)C1层为卷积层.C1得到的图像为原始输入图像与学习得到的滤波器组卷积操作后的图像I1图3DSVLNets网络计算过程(I1={I1式(6)中的I1.(2)S2为线性层.获取对目标的局部平移不变性的描述,传统的卷积神经网络结构是对C1层的图像进行下采样,进而得到S2层的图像.本文去掉了对S2层的下采样过程,这一层被视为线性层,它得到的是原始输入图像P与C1层的卷积图像I1的差值图像P2(P2={P1的P2.(3)C3,C5,C7和C9为卷积层.实现原理与C1层的实现原理相似,即将一组可训练滤波器组分别与S2,S4,S6和S8线性层图像进行卷积,分别得到卷积层图像C3,C5,C7和C9,它们分别对应于式(6)中的Ik,k(k=2,3,…,l)为网络模型的隐藏层的层数.本文DSVLNets网络结构包含5个隐藏层,故k的最大值为5.(4)S4,S6,S8和S10为线性层.实现原理与S2层的实现原理相似,将上一隐藏层的线性层图像视为该层的输入图像,将其与该层的卷积层图像相减,得到的差值即为该层的线性层图像,即将S2,S4,S6和S8层的图像分别与C3,C5,C7和C9层的图像相减,得到线性层的差值图像S4,S6,S8和S10,它们分别对应于式(6)中的Pk+1.(5)输出层输出各卷积层图像以及S10层的线性层图像.文中采用自编码方式训练DSVLNets网络模型.自编码器可以通过无标签学习得到一个能更好地表示输入数据的特征模型.它使用了反向传播算法,并让目标值等于输入值,自编码神经网络尝试学习一个hw,b(x)≈x的函数,它试图逼近一个恒等函数,从而使得输出x^接近于输入x.网络训练过程为:依次训练网络的每一层,进而预训练整个深度神Page6经网络.首先利用原始输入数据训练仅包含有一个隐藏层的网络结构,训练结束之后扩展网络结构为包含有两个隐藏层的深度支撑值学习网络,再次进行训练;每一次训练过程都是采用自编码方式进行,即固定前面t-1层,增加第t层,以此类推直到将整个网络训练完毕.这些各层单独训练所得到的权重被用来初始化最终的深度网络的权重,再对整个网络进行“微调”.3.2图像融合由3.1节可知,网络结构输出层输出的是多光谱图像R、G、B、N和全色图像PAN这5个波段图像的C1,C3,C5,C7,C9和S10层图像,卷积层图像主要包含图像的边缘和轮廓等高频信息,对其采用绝对值取大法进行融合,得到融合后的各卷积层图像;线性层主要包含图像的低频信息,对其采用分块稀疏表示再融合的方法.文中构造了过完备离散余弦变换(DiscreteCosineTransform,DCT)字典DDCT,将线性层图像分块后在DDCT字典下使用正交匹配追踪(OrthogonalMatchingPursuit,OMP)算法进行稀疏表示,对稀疏系数采用绝对值取大法进行融合,得到融合后线性层图像;最后,将融合后的各卷积层图像和线性层图像按式(7)进行重构,得到融合后的高分辨多光谱图像.过完备DCT字典由DCT变换获得,给定序号x(n)(n=0,1,…,N-1),有Xc(k)=将其写成矩阵的形式:犆N是N×N变换矩阵,其行向量为余弦基.离散余弦变换后得到一个完备字典,再将其扩展成为过完备离散余弦字典,图4所示为已经构造好的过完备DCT字典.全色图像与多光谱图像的融合过程具体描述如下:(1)由图3的DSVLNets网络计算过程可以得到多光谱图像各波段的序列卷积层图像和线性层图像,即Cd表示为多光谱图像的波段数.同时得到全色图像的序列卷积层图像和线性层图像,即CPCP9和SP(2)将多光谱图像各波段的Cd9图像与全色图像的CPCd别采用绝对值取大法进行融合,得到融合后高分辨多光谱图像的CFu表示为各波段融合后的图像;(3)将多光谱图像各波段的Sd像的SP块,将图像块在DDCT字典下稀疏表示,对稀疏系数按绝对值取大法进行融合,得到融合后高分辨多光谱图像的S(4)将(2)和(3)得到的融合后的图像CFuFu5,C7,CC融合后的高分辨多光谱图像.其中(1)~(4)过程中和各卷积层图像C表示式(7)中图像I,线性层图像S表示式(7)中的图像P.4实验结果与分析文中使用QuickBird和Geoeye数据集,与经典的主成分分析方法(PrincipalComponentAnalysis,PCA)[2]、AWLP[9]方法、PN-TSSC方法[20]和单尺度支撑值变换方法(SupportValueTransform,SVT)[14]进行比较,以验证本文所提方法的有效性,各对比算法的参数设置与文章中的参数设置相同.在对融合结果进行分析时采用了主观评价法与客观评价指标相比较的方法,评价指标包括有相关系数(CC),通用图像质量指数(UIQI),均方根误差(RMSE),全局融合指标(Q4)[33],光谱角映射(SAM)[34]和全局相对光谱损失(ERGAS)[35].(1)相关系数(CC)相关系数(CorrelationCoefficient,CC)反映的Page7是融合图像与参考图像对应谱带的相似程度,理想值为1,值越接近于1表示融合后图像的光谱损失越少,与参考图像越相近.其表示形式为CC=其中:F(i,j)和X(i,j)分别表示图像大小均为M×N的融合后图像和参考图像;μF和X(i,j)的均值.(2)通用图像质量指数(UIQI)通用图像质量指数(UIQI)用于评价融合图像与参考图像的结构失真程度,其理想值为1,越接近于1说明图像融合质量越好,UIQI定义为其中:σFX表示F(i,j)和X(i,j)的标准差.(3)均方根误差(RMSE)均方根误差(RootMeanSquareError,RMSE)表示融合图像与参考图像的差异程度,其理想值为0,即RMSE值越小则表示融合结果越理想.RMSE定义为RMSE=(4)全局融合指标(Q4)全局融合指标(Q4)反映了融合图像与参考图像在空间和谱间的整体相似程度,融合结果越好,Q4的值越接近于1.首先应对图像进行分块(D×D),对每一块按式(14)计算Q4.其中,Q4(D×D),x和y由式(15)计算得到烄Q4(D×D)=烅x=X1(i,j)+αX2(i,j)+βX3(i,j)+γX4(i,j)y=F1(i,j)+αF2(i,j)+βF3(i,j)+γF4(i,j烆(5)光谱角映射(SAM)光谱角映射(SpectralAngleMapper,SAM)表示融合图像与参考图像的光谱相近程度,其理想值为0,即光谱越相近,SAM越小.SAM定义为其中,狌F和狌X分别表示F(i,j)和X(i,j)的谱向量.(6)全局相对光谱损失(ERGAS)全局相对光谱损失(ErreurRelativeGlobalAdimensionnelledeSynthèse,ERGAS)表示对融合图像各个谱带进行总体的性能评价,其理想值为0,ERGAS值越小,表示融合结果越好.ERGAS定义为其中:hl表示多光谱图像的谱带数,x-4.1QuickBird图像集实验QuickBird卫星数据来源于印度的孙德尔本斯(拍摄于2002年11月21日)和中国的西安(拍摄于2008年9月30日),该卫星获取的全色图像的分辨率为0.6-m,多光谱图像的分辨率为2.4-m.实验中将原始图像作为参考图像,将其进行采样因子为4的下采样,下采样后的图像作为实验图像,下采样后的全色图像的分辨为2.4-m,多光谱图像的分辨率为9.6-m.文中所用的多光谱图像和全色图像的大小均为64×64和256×256,参考图像大小为256×256;图5所示为10组测试图像的多光谱图像,表1为这10组图像融合后的结果图像的平均数值指标;图6和图7所示为单幅图像的融合结果,其客观评价指标如表2和表3所示,表中最优指标均用加粗字体表示.表1为来自QuickBird卫星的10组图像的平均数值指标,从该表中可以看出本文所提出的基于深度支撑值学习网络的图像融合方法得到的平均融合指标均大于对比算法中相应的数值指标.图6所示为单幅图像的融合结果对比图,其中图6(a)和(b)分别为多光谱图像和全色图像,(c)为参考图像,从图中可以看出PCA方法融合后的图像较暗,光谱扭曲较严重;AWLP方法融合后的结果有较强的空间扭曲和光谱扭曲;PN-TSSC方法的融合结果空间保持较好,但光谱信息有失真;单尺度SVT方法融合后的结果图像较模糊,且空间失真较严重;相较于对比算法,本文所提方法得到的融合图像较好地保持了图像光谱信息和空间信息.表2为图6融合后的数值指标,从表2可以看出,本文方法的CC,UIQI,RMSE,Q4,SAM和ERGAS这6个指标均优于其它对比算法的相应数值指标.Page8图5测试图像集图6图像融合结果Page9表1图5的平均数值指标表2图6融合结果的数值指标PN-TSSC方法图7图像融合结果表3图7融合结果的数值指标PN-TSSC方法UIQIRMSESAMERGASUIQIRMSESAMERGASUIQIRMSESAMERGAS从图7可以看到,基于PCA方法融合后的图像较暗,光谱扭曲较严重;基于AWLP方法融合后的结果有边界模糊;单尺度SVT方法的方法融合图像模糊,空间扭曲较严重;而PN-TSSC方法和本文方法的结果图像光谱信息较接近于参考图像,融合效果较好.表3为图7融合后的数值指标,从表3可以看出,PN-TSSC方法0.93380.93150.07550.82886.07212.45070.94880.94800.06040.89442.84071.14750.93770.93760.08210.88137.20612.9522本文方法的CC,UIQI,RMSE,Q4,SAM和ERGAS这6个指标均优于其它对比算法的相应数值指标.4.2Geoeye图像集实验Geoeye卫星数据来源于澳大利亚的霍巴特(拍摄于2009年2月24日),此卫星获取的全色图像的分辨为0.5-m,多光谱图像的分辨率为2-m.实验中Page10将原始图像作为参考图像,将其进行采样因子为4的下采样,下采样后的图像作为实验图像,下采样后的全色图像的分辨率为2-m,多光谱图像的分辨率为8-m.文中所用的多光谱图像大小均为64×64,全色图8测试图像集图9图像融合结果图像大小均为256×256,参考图像大小为256×256;图8所示为10组测试图像的多光谱图像,表4为这10组图像融合后的结果图像的平均数值指标;图9和图10所示为单幅图像的融合结果,其客观评价指Page11表4图8的平均数值指标表5图9融合结果的数值指标图10图像融合结果表6图10融合结果的数值指标UIQIRMSESAMERGASUIQIRMSESAMERGASUIQIRMSESAMERGAS标如表5和表6所示,表中最优指标均用加粗字体表示.表4为来自Geoeye卫星的10组图像的平均数值指标,从该表中可以看出本文所提出的基于深度支撑值学习网络的图像融合方法中的CC,UIQI,PN-TSSC方法0.94010.93710.07750.84936.69552.1792PN-TSSC方法0.97070.97040.05970.90584.27391.4816PN-TSSC方法0.92050.91760.08840.777011.92602.8988RMSE,SAM和ERGAS这5个指标均优于对比方法的相应指标,而PN-TSSC方法的Q4指标最优,本文方法的Q4指标次之.图9所示为单幅图像的融合结果对比图,其中图9(a)和(b)分别为多光谱图像和全色图像,(c)为参考图像,从图中可以看出Page12PCA方法融合后的图像较暗,光谱扭曲较严重;AWLP方法融合后的图像边界较模糊,有较少的空间扭曲和光谱扭曲;PN-TSSC方法的结果空间保持较好,但图像较暗;单尺度SVT方法融合后的结果图像较模糊,空间失真较严重;而本文方法的结果比较接近于参考图像,它较好地保持了光谱信息和空间信息.表5为图8中各对比算法的数值指标,从表5中可以看出,本文方法的CC、UIQI、RMSE、SAM和ERGAS这5个指标均优于对比方法的相应指标,而PN-TSSC方法的Q4指标最优,本文方法的Q4指标次优,但从视觉效果上看本文方法所得到的结果图像光谱信息和空间信息保持最优.从图10可以看出,基于PCA方法融合后的图像较暗,光谱扭曲较严重;单尺度SVT方法融合后的结果图像较模糊,空间失真较严重;AWLP方法的光谱信息较接近于参考图像,但图像边界较模糊;从视觉效果上看PN-TSSC方法和本文方法的结果空间保持较好,且光谱信息较接近于参考图像.但对图像从客观指标上整体评价,可以看出本文方法的CC,UIQI,RMSE,Q4和ERGAS这5个指标均优于对比方法的相应指标,而单尺度SVT方法的Q4指标最优,本文方法的Q4指标次优,但从视觉效果上看本文方法所得到的融合结果图像空间信息最优,光谱信息最接近于参考图像.5结论本文提出了基于深度支撑值学习网络(DSVLNets)模型的全色图像与多光谱图像融合方法,网络模型各层中的滤波器通过最小化经验风险获得系数.DSVLNets网络模型取消了传统卷积神经网络结构中特征映射层的下采样过程,构建了由5个隐藏层构成的适合图像融合的新网络模型,每一层的基本结构包括卷积层和线性层,这样的基本单元提供了一种对输入图像的多尺度、多方向、各向异性、非下采样的冗余变换.为了验证本文方法的有效性,文中使用了QuickBird和Geoeye卫星数据集,实验结果表明,与PCA,AWLP,PN-TSSC和SVT算法相比较,本文所提方法的融合结果无论在主观视觉还是客观评价指标上均优于对比算法,较好地保持了图像的光谱信息和空间信息.致谢感谢陕西省智能感知与大数据协同创新中心、教育部创新团队和首批陕西省科技创新团队计划以及国家“九七三”重点基础研究发展计划项目和国家自然科学基金委重大研究计划的支持.计算机学报编辑部和评委老师给出了宝贵意见,在此一并感谢!
