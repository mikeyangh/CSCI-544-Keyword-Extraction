Page1基于内存混合复制方式的虚拟机在线迁移机制陈阳怀进鹏胡春明(北京航空航天大学计算机学院北京100191)摘要虚拟机VM(VirtualMachine)在线迁移技术(LiveMigration)充分利用虚拟化技术的灵活性和封装性,有利于实现大规模虚拟化环境中负载的动态调整以及应用的灵活部署.已有的在线迁移机制存在内存迭代收敛、迁移数据冗余以及客户操作系统不透明等问题.文中结合内存推送复制以及按需复制两种方式,提出了基于内存混合复制方式的VM在线迁移机制HybMEC,以实现对VM运行状态的快速迁移.基于KVM虚拟机监控器,实现了HybMEC原型系统.多种不同类型应用负载下的实验结果表明,HybMEC能够高效地支持虚拟化环境中低开销、低延时的VM在线迁移,并在总迁移时间、内存同步数据量以及VM停机时间等方面具有显著的性能提升.关键词虚拟机;虚拟机监控器;在线迁移;按需复制;预取页1引言渐成为科学界和工业界研究的热点主题.云计算实现了资源、平台以及软件的服务化,并通过计算机网络向用户提供按需、灵活、可伸缩的计算和存储资源.其中,基础设施即服务IaaS(Infrastructureasa当前云计算作为一种新型网络应用模式,已逐Page2Service),例如AmazonEC2①、Eucalyptus[1],更被认为是云计算的先行者[2].IaaS以“按需付费”的模式向用户提供虚拟化基础设施,有效地提高了物理资源利用率以及应用程序的可用性,同时简化了用户对于IT基础设施配置和运维管理,大幅度地降低了硬件投资成本.在IaaS高灵活性和高可扩展性的背后,虚拟机VM技术(VirtualMachineTech-nology)[3-4]是其实现的核心技术.后者通过软件抽象将单个物理机资源划分成多个相互隔离的虚拟资源,实现了多个操作系统实例对单个物理资源的复用.然而,VM在线迁移技术更是VM技术灵活性和可用性的重要应用和体现.在线迁移技术实现了VM运行状态(即操作系统运行状态)在分布、异构的物理运行环境中快速的平移.整个迁移过程中,VM的运行状态只在极小的时间间隔内被中断,这使得VM应用对外表现出“不间断”的可访问性.目前,VM在线迁移技术广泛地应用于云计算虚拟化数据中心(virtualdatacenter)环境中的运维管理,例如,动态负载均衡和资源管理、物理设备能耗调整以及在线的设备维护(onlinemaintenance)等.实现VM内存状态在源、目的宿主机(hostma-chine)之间的高效复制是VM在线迁移技术的关键所在.目前,主流虚拟化平台均提供VM在线迁移功能,并且在实现上普遍采用了基于内存预拷贝Pre-copy机制[5-6].但是,Pre-copy机制受其自身特性的约束,在一些情况下并不能取得理想的迁移效率.例如,在低带宽网络环境中或对于大内存VM,Pre-copy机制很可能会由于迭代收敛性问题,导致迁移过程中VM应用性能出现明显的下降.而对于访存密集型应用,由于VM内存更新频繁,Pre-copy机制需要经历低效的内存迭代同步才能完成内存一致性状态的复制.过长的迭代过程导致迁移总时间被延长,造成迁移过程对物理资源的长时间占用.此外,程序访存的局部性还会造成Pre-copy机制在内存迭代复制过程中一些内存数据被多次重复地复制.这种迁移数据的高度冗余性从另一方面导致了迁移过程对物理资源不合理的消耗.云计算系统服务于大量企业或个人用户,而系统中有限的物理资源需要在众多用户之间实现共享,因此单用户往往不允许以独占的方式使用系统分配的物理资源.但是,现有的VM在线迁移机制在资源受限的环境中,常表现出低效的性能甚至出现迁移失败的情况.另外,在物理资源共享的环境中,当前用户所执行迁移的操作,可能会损害其他用户对物理资源的需求;或者迁移操作受限于被其他用户占用的物理资源.因此,在线迁移机制在实现VM执行环境快速切换的同时,更需要考虑迁移过程本身对于物理资源的占用和消耗.针对现有在线迁移机制性能和实现上的不足,以及云计算环境中用户物理资源共享且受限的特点,本文提出基于内存混合复制方式的VM在线迁移机制HybMEC(HybridMEmoryCopy).即HybMEC在迁移过程中充分结合内存推送复制和按需复制两种方式实现VM内存状态快速、高效的同步.其中,推送复制是指源宿主机主动向目的宿主机复制内存数据;而按需复制是指目的宿主机适时地向源宿主机请求某一部分内存数据.HybMEC以缩短总迁移时间,避免迁移过程对物理宿主机的长时间依赖为首要目标.同时,HybMEC还尽可能地减少迁移过程对物理资源的消耗,同时降低VM应用在迁移过程中的性能损失.总体上看,HybMEC将内存同步的过程严格划分为全内存同步、内存位图同步以及脏内存同步3个阶段.其中,全内存同步阶段,Hyb-MEC在保持VM运行的前提下,利用源宿主机主动推送的方式将VM完整的内存数据复制到迁移目的端.该阶段保证了在VM切换到目的宿主机上运行时,绝大多数内存数据经过一次复制已经驻留在在目的宿主机本地.在全内存同步结束后,Hyb-MEC不陷入脏内存的迭代复制,而是迅速地复制脏内存位图数据并在目的宿主机上恢复VM执行,随后真正进入脏内存同步阶段.一方面利用虚拟机监控器VMM(VirtualMachineMonitor)在目的宿主机端捕获VM对脏页面的访问,HybMEC通过网络缺页处理从源宿主机端按需地复制某一部分未同步的脏页面数据;另一方面为了加速脏页面的同步,HybMEC定时地从源宿主机上推送剩余的脏页面数据.因此,从迁移过程上看,HybMEC分阶段地利用推送复制和按需复制实现了内存状态的高效同步;而从脏内存数据的角度,HybMEC充分结合按需复制及定时推送的优势,实现了对脏内存数据的快速复制.由于脏页面的按需复制要求中断当前VM的正常运行后才能开始执行网络缺页处理.因此,频繁的按需复制势必导致迁移过程中VM运行性能出现剧烈抖动,并延长VM应用的执行延时.为了避①AmazonElasticComputeCloud.http://aws.amazon.Page3免频繁地陷入由按需复制引起的缺页中断处理,根据访存的局部性和顺序性原理,HybMEC在该类型缺页中断处理中采用了预取页(Pre-paging)机制.即从源宿主机端按需地复制一个脏页面时,HybMEC会“捎带”地预先复制该缺页页面的邻居脏页面.这样,在很大程度上HybMEC保证了VM后续访问邻居脏页面时,这些页面已经驻留在目的宿主机本地,而避免再一次引发高延时的网络缺页处理.另外,为进一步提高内存同步的效率,一方面,HybMEC主动避免对空闲页面的按页面大小(4KB)进行复制;另一方面,HybMEC禁止空闲脏页面引起按需复制处理,转而通过VMM为空闲的脏页面直接分配本地内存页.我们在KVM虚拟化平台上实现了HybMEC原型系统,并通过多种不同类型的应用负载验证了HybMEC机制的高效性和可用性.实验结果表明,相比于Pre-copy机制,HybMEC不仅减少了迁移过程的同步数据量、降低了迁移过程造成的VM运行性能损失,而且缩短了VM停机时间以及总迁移时间.特别是对具有访存密集型应用的VM,HybMEC在上述的各方面性能上均有显著的提升.本文第2节将详细介绍VM在线迁移机制基本背景以及HybMEC机制的原理和方法;第3节介绍HybMEC在KVM上的具体实现细节;第4节介绍原型系统的实验结果及分析;第5节介绍相关工作对比;最后第6节小结全文.2HybMEC实现原理2.1VM在线迁移原理本质上,在线迁移机制实现了VM运行状态通过计算机网络在物理宿主机之间的高效复制.其中,运行状态包括VM的虚拟处理器VCPU寄存器、内存以及外部资源设备状态(即外部磁盘文件系统和图1Pre-copy与HybMEC内存同步过程对比网络连接状态).在局域网LAN环境中,VM外部资源设备状态一般可通过配置网络共享存储设备(NAS、NFS等)以及主动汇报网络拓扑变化,解决迁移过程中VM外部设备的状态一致性问题[5-6].因此,解决内存状态的高效同步成为VM在线迁移的关键问题.通过计算机网络,将VM内存状态从一台物理宿主机复制到另一台物理宿主机可通过以下3种方式[6-7]:(1)停机拷贝.源宿主机暂停VM,将VM内存状态完整地复制到目的宿主机;目的宿主机正确加载内存状态后启动VM.(2)目的端按需复制.源宿主机暂停VM运行,VM内存状态驻留在源宿主机上;随后VM在目的宿主机上恢复运行,并按需地从源宿主机获取内存页面.(3)源端推送复制.源宿主机保持VM持续运行,并主动将VM内存状态复制到目的宿主机.现有的迁移机制中,基于内存预拷贝(Pre-copy)的迁移机制结合了源端推送复制和停机拷贝两种方式;而基于内存后拷贝(Post-copy)的迁移机制[8]结合停机拷贝和目的端按需同步两种方式.以Pre-copy机制为例,其完整的迁移流程如图1(a)所示.迁移初始,VMM将所有的VM内存页面标记为“脏页面”;接着,迁移过程开始进入多轮的内存迭代同步过程.值得注意的是,由于所有页面都被置为“脏页面”,第一轮内存同步需要复制全部的VM内存数据;而后续的每一轮同步只需要复制上一轮同步过程中被VM修改的脏页面数据.经过若干次迭代后,如果剩余脏页面数小于预设最小值或迭代次数大于预设最大值,Pre-copy机制终止迭代,并主动暂停源宿主机上的VM,将剩余的脏页面以及其它虚拟设备状态推送至目的宿主机.最后,目的宿主机接收并加载最后的状态数据后立即恢复VM执行,同时向源宿主机确认迁移完成.Page4在迁移过程中,为了获得VM在某一时刻的完整运行状态(VCPU寄存器、内存以及其它虚拟设备状态),VM正常运行需要在一段时间内被中断.如果该时间间隔足够小,迁移机制就能保证VM与外部的网络连接在此期间不发生超时,并避免VM应用对外部表现出明显的不可访问.例如,Pre-copy机制在结束内存迭代同步后,立即暂停源宿主机上VM的运行,并在随后的某一时刻恢复VM在目的宿主机上的运行.理想情况下,Pre-copy机制能够保证该时间间隔介于几十毫秒到1秒之间.我们将该时间间隔定义为VM停机时间,它等于源宿主机暂停VM时刻与目的宿主机恢复VM执行时刻的间隔,即VM分别在源和目的宿主机上切换运行状态的时间间隔.对于Pre-copy机制,VM停机时间主要由停机时刻剩余状态数据的大小及可用物理网络带宽决定.因此,如果剩余脏页面数据过大或者物理网络带宽过小,那么VM停机时间将受此影响而被延长.在实现上,VM在线迁移机制首先需要保证透明性,即迁移过程对VM以及其内部应用透明.同时,迁移机制还需要权衡各方面的性能以及迁移过程本身对物理资源的占用.(1)VM停机时间.VM被暂停执行的时间间隔;直接反映了VM不可访问的时间.(2)总迁移时间.从迁移开始到结束的持续时间;决定了迁移过程对物理资源的依赖时间,其中物理资源主要包括物理网络带宽、宿主机CPU资源.(3)内存同步数据量.整个迁移过程中复制的内存页面数量;直接反映了迁移过程对物理网络带宽的占用情况.(4)VM应用性能损失.VM内部应用执行延时或对外表现出的性能抖动;直接反映了迁移过程对VM应用性能的影响.虽然Xen[3]、VMware[5]及KVM[9]等虚拟化平台均提供了基于上述Pre-copy机制的在线迁移功能,但是由于依赖基于源端推送复制的内存迭代同步,Pre-copy机制并不能保证稳定的迁移性能.例如,潜在的迭代收敛性问题,即当网络传输速率小于VM更新其内存的速率时,迁移过程经过多轮迭代复制后剩余的脏页面仍无法收敛于预设的最少脏页面数的问题.在该情况下Pre-copy机制需要被迫终止内存迭代同步,立即暂停VM运行并进入停机拷贝阶段.但此时,大量未同步脏页面数据驻留源宿主机上等待停机时间内的复制,造成VM停机时间被延长.其次,内存迭代同步方式直接造成迁移性能对剩余脏内存收敛速率的过分依赖.而后者在不同的VM应用负载下往往表现出强烈的不确定性.最后,访存的局部性使得Pre-copy机制在上一次迭代中复制的内存页面很可能在后续的迭代中被再次修改,并且修改只局限于该页面上的某一小部分数据.因此,内存迭代同步造成了迁移数据具有很高的冗余性.上述数据的冗余性必定会导致迁移过程对物理网络带宽不必要的消耗和占用.2.2HybMEC原理针对现有迁移机制的不足,如图1(b)所示,HybMEC在迁移过程中分阶段地利用源宿主机推送复制以及目的宿主机按需复制两种方式,实现对VM内存状态的高效同步.HybMEC将整个内存同步过程划分为3个阶段:(1)全内存同步.HybMEC按页为单位将整个VM内存数据从源宿主机复制到目的宿主机,并在结束时刻暂停VM运行.由于该过程中VM在源宿主机上仍保持运行,因此HybMEC需要主动地标记所有被更新的脏页面.(2)内存位图同步.源宿主机推送上一阶段里记录“脏页面”的位图数据dirty_bitmap;目的端宿主机根据接收到的dirty_bitmap数据设置VM相应页面的状态.(3)脏内存同步.目的宿主机恢复VM执行,HybMEC利用VM访问未同步的脏页面产生的缺页异常,从源宿主机端按需地复制脏页面数据;同时,源宿主机定时向目的宿主机端推送未同步的脏页面.当所有脏页面复制完成后,在线迁移过程结束.相对于Pre-copy机制,HybMEC从根本上避免了内存迭代同步.同时,在VM停机时间内,Hyb-MEC只复制内存位图数据dirty_bitmap,减少了VM停机时间内复制的数据量,缩短了VM运行被中断的持续时间.其次,由于在整个迁移过程中只有脏页面数据需要额外的一次复制,而绝大多数的内存数据只需要经过一次的复制即完成同步.因此,HybMEC大幅地减少迁移过程中同步数据量,同时降低对于宿主机物理资源以及物理网络带宽的占用.最后,虽然每一次的脏页面按需复制需要强制中断VM的运行,但是相对于Post-copy机制全内存按需复制的方式,HybMEC只按需复制部分脏页面数据,这在很大程度上降低了缺页处理造成的性能损失,避免了VM应用性能出现剧烈抖动,同时也Page5缩短了迁移持续时间.完整的HybMEC迁移过程如图2所示.其中,步骤VI-VIII将循环执行直至所有VM内存同步完毕.步骤I中,源宿主机发起迁移操作并向目的宿主机复制完整的VM内存数据,同时在dirty_bitmap中标记复制过程中被修改的页面;随后的步骤II中,源宿主机暂停VM执行.此时VM的内存、VCPU寄存器以及其它虚拟设备状态在源宿主机上被冻结.在步骤III里,源宿主机向目的宿主机复制dirty_bitmap、VCPU寄存器以及其它虚拟设备状态.另一端,目的宿主机执行步骤IV,根据图2HybMEC迁移流程在上述的迁移过程中,步骤I-II对应于全内存同步阶段,完成了大部分内存数据的一次性复制;而步骤III-IV对应内存位图同步阶段,除了dirty_bitmap数据,源宿主机同时向目的宿主机复制VCPU寄存器以及除内存外的其它虚拟设备状态数据.最后,步骤VI-VIII对应脏内存同步阶段.当所有脏页面完成同步后,目的宿主机上VM的运行不再依赖源宿主机,HybMEC断开两端宿主机之间用于内存同步的网络连接,并释放源宿主机上为迁移VM分配的物理内存.2.3脏页面同步VM在目的宿主机端恢复运行,HybMEC正式进入脏内存同步阶段后,由于程序访存的不确定性[10],VM可能在一段时间内不再访问某一些脏页面.这将造成依赖于“脏页面访问”驱动的按需复制不能快速完成所有脏页面的同步,同时也导致总迁移时间的不确定性.因此,在该阶段里HybMEC利用脏页面定时推送来辅助脏页面按需复制,以避免脏页面的同步过程受限于VM对内存访问模式.在进入脏页面同步阶段后,源宿主机上HybMEC按dirty_bitmap设置脏页面的“缺页”状态,同时加载VCPU寄存器以及其它虚拟设备状态;最后恢复VM执行.随后,VM在目的宿主机上正常运行的过程中访问“缺页”状态的脏页面,产生缺页异常被VMM捕获(步骤VI)并通过目的宿主机向源宿主机发送缺页请求(步骤VII).步骤VIII中,源宿主机根据请求消息中缺页信息以及本地dirty_bitmap定位脏页面,并以该页面数据响应缺页请求;而目的宿主机接收该缺页数据后,将其写入缺页地址指示的内存页,并立即恢复VM执行.预设的时间间隔,定时地向目的宿主机推送未同步的脏页面数据.这样,即使在最坏情况里,VM不再访问任一脏页面或者访问任一脏页面之前该页面已经被预先复制,HybMEC仍可以通过有限次的定时推送完成所有脏页面的同步.每一次的脏页面按需复制都需要借助于网络缺页异常处理从源宿主机端复制指定的脏页面.在该过程中,VM需要被无条件地挂起,直至脏页面数据被加载后才能恢复运行.因此,简单的“一次一页面”的响应模式将会恶化脏内存同步的效率,同时造成VM应用性能的损失.因此,HybMEC在源宿主机上维护一个同步窗口,适时地用脏内存数据填充该窗口,并以窗口大小为基本单位执行脏页面复制.然而,频繁的脏页面按需复制势必会频繁地打断VM正常运行,导致VM应用性能出现抖动.同时,由于按需复制的处理依赖于高延时的网络处理以及大开销的VM状态切换,频繁的脏内存按需复制势必还会影响VM应用的执行延时.所以,为了避免频繁的缺页中断处理,缩短VM应用的执行延时,并加速脏内存的同步过程,HybMEC在按需复制的缺页Page6处理中采用预取页机制.预取页机制最早由Denning提出,它利用程序的历史运行信息预测程序最近的工作集(workingset)[11].该机制常被应用于程序内存页面的预先加载,例如磁盘数据文件的预先加载.根据应用访存的局部性和顺序性原理[10],当前被访问且发生缺页异常的脏页面的邻居页面很可能在后续的运行过程中被访问.如果该邻居也是脏页面且未被同步,那么VM在访问该邻居脏页面时必然会再一次引起网络缺页处理.但是,如果HybMEC在当前缺页响应时“捎带”上该邻居脏页面,那么就能直接地避免上述潜在的网络缺页处理.因此,当源宿主机在收到一次缺页请求后,HybMEC利用缺页信息以及dirty_bitmap数据定位当前缺页页面,有选择地用“邻居”脏页面填充上述的同步窗口,并迅速响应该缺页请求.随后,当VM访问被“捎带”的脏页面时不再发生缺页异常,而转向对本地内存的直接访问.2.4内存缺页设置在脏内存同步阶段,HybMEC需要及时地捕获VM对任一脏页面的访问.如若不然,HybMEC将错过了复制该页面数据的时机,很可能造成VM由于读写了错误的内存数据而出错甚至导致VM宕机.考虑到透明性需求,HybMEC不能依靠VM访问脏页面时的主动通知来执行按需页面复制.因此,为了感知VM对任一脏页面的访问,HybMEC需要根据内存虚拟化原理,在目的宿主机恢复VM运行之前预先为脏页面设置相应的缺页状态.同时,由于设置缺页状态的操作在VM停机期间执行,因此该操作的执行延时直接影响了迁移过程中VM停机时间.所以,HybMEC需通过透明、高效的方式控制脏页面状态.系统虚拟化中的地址转换相比于传统操作系统,额外引入了一层地址映射转换的工作.其中,客户机操作系统(简称,客户OS)负责VM内部的线性地址到客户OS物理地址的映射;而VMM中内存虚拟化模块负责客户OS物理地址到真实机器地址的映射.VM运行时,VMM负责将由其维护的页表数据结构写入物理MMU(MemoryManagementUnit)用于VM访存时的地址转换.因此,HybMEC可通过控制该页表数据结构中相应表项的属性以实现对脏页面缺页状态的设置.在迁移步骤IV中,目的宿主机在接收到dirty_bitmap数据后,HybMEC根据dirty_bitmap中记录的脏页面客户OS物理地址对相应的页面设置缺页状态.虽然,利用内存虚拟化的纯软件实现影子页表技术SPT(ShadowPageTable)[12],HybMEC亦能实现对于任一VM内存页状态的控制,但是考虑到执行的效率及实现的复杂性,HybMEC选择利用EPT(ExtendingPageTable)硬件辅助功能[13-14]实现对脏内存页透明、高效的控制.这样,为一个脏页面设置缺页状态的操作,HybMEC只需要根据该页面的客户OS物理地址查找EPT,并将相应的表项上设置为“不存在”(non-present).随后在脏内存同步阶段,VM访问“不存在”脏页面时,将产生缺页异常并自然地陷入VMM,而被HybMEC捕获.2.5内存页复制优化通常情况,VM运行时并不会使用物理宿主机分配的全部内存,因此VM内存页中往往含有大量未使用的空闲页面(freepage),并且这些页面一般被初始化为全零页(zeropage).如果迁移过程中HybMEC完整地复制这些空闲页面势必会不必要地占用物理CPU及网络带宽资源.最近,Hines等人[15]提出利用动态内存气泡机制(dyanmicself-ballooning)避免在Post-copy迁移过程中复制空闲的内存页面.其思想是,迁移过程中通过定时的气泡回收机制,将VM空闲页面换出主内存,从而一定程度上避免由于VM访问空闲页面引起网络缺页处理.但是,在VM在线迁移机制中引入动态内存气泡回收机制,一方面使得VM在线迁移与机制复杂的VM内存虚拟化紧耦合;另一方面要求客户OS主动支持内存气泡回收机制.为了避免迁移过程中按照4KB大小复制空闲内存页,HybMEC采用了简单而高效的方法.即在复制一个页面之前,HybMEC首先判断该页面是否为空闲页.如果是空闲页面,HybMEC只需要在迁移数据流中设置一个标志位指示该页面为空闲页;反之,HybMEC向数据流中写入完整的页面数据.相比于复杂的内存压缩算法[16],上述的方法只针对空闲页面处理,在内存同步数据量和物理CPU资源消耗上做出较好的折衷.实验表明,上述主动避免空闲页面复制的方法能够有效地节省迁移过程占用网络带宽资源.另外,为一个空闲脏页面执行按需复制的做法显然是低效而不可取的.这是因为空闲脏页面的按需复制不但造成VM发生不必要的状态切换,而且还使得迁移过程占用不必要的物理网络带宽.因此,迁移步骤I中,HybMEC在dirty_bitmap数据结构中还标注了所有的空闲脏页面;迁移步骤VI中,当VM访问一个空闲脏页面而陷入缺页处理时,目的Page7宿主机上HybMEC不向源宿主机发送缺页处理请求,转而通过VMM直接从本地分配一个页面,并立即恢复VM运行.3HybMEC实现我们在KVM虚拟化平台(版本KVM-86/88)上实现了HybMEC原型系统.KVM以驱动模块的形式加载进入Linux内核,将Linux操作系统转换成为一个高效的虚拟机监控器.同时,KVM通过系统调用ioctl及内存共享实现与用户态进程QEMU交互.后者负责VM初始化配置及关键虚拟设备模图3HybMEC原型实现3.1迁移数据流处理在迁移过程中,VM内存、VCPU寄存器、内存以及其它虚拟设备状态数据以迁移数据流的形式从源宿主机传输到目的宿主机.为了保证VM状态同步的正确性,HybMEC需要准确控制数据流在源和目的宿主机上的发送和接收.同时,这也要求HybMEC明确定义不同虚拟设备在数据流中的格式,并正确区分同一虚拟设备在不同迁移阶段中的数据内容.如图4所示,HybMEC定义了4种类型的数据段格式,其中WPAGES、BITMAP和DPAGE3种数据段格式分别对应3个内存同步阶段中内存状态数据,剩余的DEVICE类型数据段格式则对应VM虚拟设备状态数据.其中,WPAGES类型数据段用于全内存同步;BITMAP和DEVICE类型数据段则用于内存位图同步;DPAGE类型数据段则用于脏内存同步.每一种数据段由首字节标识其类型,后跟一段不定长的设备标识字段指明该数据段所对应的VM虚拟设备;设备标识字段后跟随真正的状态数据.其中,内存状态数据以页为单位,每个页面数据拟,例如PCI总线、网卡NIC、IDE硬盘等.原型系统的实现如图3所示.在用户态进程QEMU中,我们用HybMEC迁移控制器替换了KVM原有的基于Pre-copy迁移机制的迁移控制模块,同时增加了HybMEC脏页面同步模块,即源和目的宿主机QEMU进程中的缺页响应控制器和缺页请求控制器.此外,我们还主要修改了KVM内核驱动中的内存虚拟化模块KVM_MMU.修改后的KVM_MMU通过ioctl向用户态QEMU提供设置VM内存页的访问控制的接口.同时,KVM_MMU还将捕获VM对脏页面访问,并通过dpage_pull回调函数向QEMU提交网络缺页处理请求.page_data的前8字节被用于记录该页面信息标记page_flag:包括该页面首地址(客户OS物理地址)以及是否为空闲页等重要信息.如果page_flag指示一个内存页面为空闲页,那么page_data字段长度为1字节,并且其值等于零;反之,page_data字段包含了该页面完整的数据内容.值得注意的是,DEVICE类型数据段包含了除内存外的所有虚拟设备状态数据,不同设备状态数据之间以虚拟设备标识字段区分.HybMEC原型系统中,源、目的宿主机上迁移控制器与缺页控制器分阶段共享数据流通道(socket连接)用于传输数据流.其中,迁移控制器负责前两个迁移阶段的数据流传输,而脏页面同步阶段数据流由源、目的宿主机上的缺页控制器控制.Page83.2脏页面按需复制目的宿主机上HybMEC除对脏页面设置缺页状态之外,还需要在内核态KVM模块中维护dirty_bitmap数据结构.这样做的直接好处是,避免了在KVM_MMU确认当前缺页异常页是否是脏页面时,从内核态切换至用户态查询dirty_bitmap.但这也要求HybMEC在每一次收到到脏页面数据后,主动地更新内核态中的dirty_bitmap.因此,目的宿主机迁移控制器接收到dirty_bitmap数据后,HybMEC通过ioctl系统调用访问KVM_MMU接口,对所有脏页面设置缺页状态,并在KVM创建一个dirty_bitmap的复本.由于HybMEC执行上述操作时,VM在目的宿主机上仍处于暂停状态,因此KVM_MMU还未为一些页面建立有效的EPT表项.因此,HybMEC根据脏页面地址查找对应的EPT表项的过程中,还需要适时地为该地址建立各级EPT表项,并将最低级EPT表项设置non-present属性.当VM访问一个未同步的脏页面时,将产生缺页异常而陷入KVM异常处理入口函数handle_excep-toin.后者根据异常类型,确定是缺页异常后则调用缺页异常处理函数kvm_mmu_page_fault.kvm_mmu_page_fault将首先确认产生该缺页异常的原因是否是VM对未同步的脏页面的访问,并且当前缺页页面不是空闲页面.如果上述确认通过,kvm_mmu_page_fault则调用dpage_pull函数向用户态QEMU进程转发脏页面缺页事件.QEMU进程中缺页请求控制器模块接收到该缺页事件后,立即执行脏页面的按需复制,并通过数据流通道向源宿主机上的QEMU进程提交“缺页”请求.需要注意的是,如果kvm_mmu_page_fault函数确认缺页脏页面为空闲页面,那么该函数直接为该页分配一个零页面,并调用vm_lanch恢复VM运行.对于其它情况的缺页异常,kvm_mmu_page_fault将按正常缺页异常处理逻辑执行.3.3脏页面预取源宿主机上的QEMU进程在进入脏页面同步阶段时,缺页响应控制器模块将自动开启同步定时器,并初始化同步窗口.其中,定时器间隔长短以及同步窗口大小都会对脏页面同步效率以及VM应用性能造成影响.例如,窗口过小或定时间隔过长,可能延长脏页面同步阶段的持续时间,并导致过多的网络缺页处理而造成VM应用性能的抖动;过大的窗口或过短的超时间隔,可能会导致脏页面同步过程占用过多的物理资源而造成VM应用性能的明显下降.目前HybMEC原型实现中,我们将定时器间隔设定为10ms,同步窗口大小等于64页(256KB).通过定量的实验分析,明确上述两个参数对HybMEC迁移性能的影响将是我们下一步工作内容之一.当源宿主机上同步定时器发生超时时,缺页响应控制器会将同步窗口首地址移置窗口上一次结束位置的下一个脏页面处,并依次将页框号(framenumber)大于窗口当前起始页面的剩余脏页面填入同步窗口,直至窗口剩余空间或剩余脏页面数为零.随后,缺页响应控制器开始推送同步窗口中的脏页面数据,并在本次推送结束时更新本地dirty_bitmap数据以及同步定时器,等待同步定时器下一次超时的到来.如果源宿主机QEMU进程接收到缺页请求,那么缺页响应控制器立刻结束当前的定时器等待而进入缺页响应处理.此时,缺页响应控制器立即将同步窗口首地址调整至当前缺页页面处,并将该页面剩余邻居脏页面填入窗口,直至窗口剩余空间或剩余脏页面数为零;接着,缺页响应控制器立即以窗口中的页面数据响应缺页请求.此外,在源宿主机上,缺页响应控制器真正传输一个页面数据前同样需要确认该页面是否为空闲页,避免空闲的脏页面通过“捎带”或者定时推送以完整页数据的形式被复制.在目的宿主机上,QEMU进程中的缺页请求控制器除了控制向源宿主机QEMU进程提交缺页请求,还负责接收脏页面数据流.在接收到脏页面数据后,缺页请求控制器通过ioctl调用通知内核态KVM_MMU更新VM内存页面映射关系以及dirty_bitmap数据.4实验评估前文介绍了HybMEC原型系统的实现细节,本节将通过具体实验验证HybMEC的有效性和可用性,并说明各项关键技术对于迁移性能参数的影响.在多种不同类型的应用负载下,我们将对比KVMPre-copy机制,并关注总迁移时间、VM停机时间、内存同步数据量以及VM应用性能损失等性能参数.最终的实验结果表明,相比于Pre-copy机制,HybMEC不仅减少了迁移数据量、降低了迁移过程造成的VM运行性能损失,而且缩短了VM停机时间以及总迁移时间.特别是对具有访存密集型Page9应用的VM,HybMEC表现出明显的稳定性和高效性.4.1实验环境我们在DELLT1500工作站上完成所有迁移实验.各物理机配置:Intel(R)Core(TM)i7860@2.80GHzCPU,8GB内存,320GBSATA硬盘以及BroadcomBCM57780Gigabit网卡.物理机之间通过H3CS5120-28C-EI千兆交换机连接;所有的VM磁盘镜像文件存放在iSCSI共享存储服务器.KVM宿主机软件环境为UbutunServer10.04TLS,内核版本2.6.32-24,KVM版本kvm-88.实验中,VM均配置了一个VCPU,并安装UbuntuDesktop10.04LTS版本操作系统,内存大小介于256MB~2GB之间.此外,所有VM通过桥接模式接入物理网络.如果没有特别指出,实验中VM虚拟内存大小默认设置为512MB.另外,同一组应用负载下VM迁移实验被重复10次,本节中实验结果为10次结果均值.4.2内存压力负载为了验证访存密集型负载条件下,HybMEC迁移的高效性和可用性,我们以一个内部持续运行内存更新程序的VM作为迁移对象.其中,内存更新程序模拟一个访存密集型应用,以页面为单位,按照指定的速率循环更新其线性地址空间内的一块内存区域;每次访问一个内存页时,该程序将向内存页首字节写入一个随机数.每一组迁移实验中,当内存更新程序运行15s后,VM开始执行在线迁移如图5所示,当内存更新速率由0逐渐上升至10240页/s,Pre-copy机制迁移性能急剧下降.其中,总迁移时间从最初的8.3901s上升至188.7537s.实验中,我们发现当内存更新速率大于16348页/s,Pre-copy迁移即出现无法正常结束的情况.因此,Pre-copy机制的内存压力负载实验中,我们只给出了内存更新速率小于10240页/s的实验结果.最图5Pre-copy迁移总时间随内存更新速率的变化后,我们还发现即使在相同内存负载条件下,Pre-copy机制的总迁移时间具有明显的不稳定性.例如,当内存更新速率设定为10240页/s,总迁移时间的最大差值接近75s.上述的实验结果表明,VM内存更新速率明显地制约着Pre-copy机制迁移性能.随着内存更新速率上升,迁移性能出现大幅度下降同时还伴随着强烈的不稳定性.通过分析KVM(版本kvm-86~kvm-0.12.4)的Pre-copy机制源代码,我们发现造成上述结果的原因在于KVM仅以最小脏内存数作为内存迭代同步的终止条件,而未考虑通过设置最大迭代次数或最大数据传输量限制迭代同步.因此,当VM内存更新速率大于物理网络传输速率时,Pre-copy机制因为剩余脏页面数不能收敛于最小脏内存数而导致迁移失败.但是,由于最小剩余脏内存数的限制,Pre-copy机制在不同的负载下停机时间内复制的数据量大致相等,这使得VM停机时间反而不随着内存更新速率的上升而表现出明显变化,如图6所示.但是,这样的稳定性以剩余脏内存数可收敛为前提,以延长总迁移时间为代价.虽然,不同的虚拟化平台对于Pre-copy机制在实现细节上不尽相同,但是通过上述的实验及分析,我们有理由相信由于依赖于剩余脏内存收敛速率,Pre-copy机制在访存密集型负载下不能保证理想的迁移性能.相反,因为不依赖于迭代同步,所以即使VM内存更新速率远大于物理网络传输速率,HybMEC亦能高效地完成迁移任务.如图7所示,当VM内存更新速率由0增加至65536页/s,总迁移时间只是缓慢增加至最初值的10.2倍.即便是内存更新速率上升为65536页/s,HybMEC迁移平均持续时间为32s,平均复制了203100个内存页面,其中大约有67930个页面(约265MB)经过脏内存同步的二次复制.图6对比了两种迁移机制的VMPage10停机时间随内存更新速率的上升的变化情况.我们发现,当内存更新速率较小的情况下,HybMEC的VM停机时间明显低于Pre-copy机制.这是因为在停机时间内,HybMEC只需要复制脏页面位图数据,并且脏页面缺页状态设置执行延时小于Pre-copy的剩余脏内存复制延时,因此HybMEC能够保证相对更小的VM停机时间.但是,随着内存更新速率的进一步上升,停机时刻剩余脏页面数增加,HybMEC执行脏页面缺页状态设置的延时增大.因此,当内存更新速率大致为5600页/s(约22MB/s)时,HybMEC的VM停机时间超过Pre-copy机制.然而,实验中我们也发现即便内存更新速率上升至65536页/s,HybMEC也只造成293.024ms的停机时间.在图8中,我们给出了相同内存更新速率的条件下,两种机制在迁移过程中内存同步数据量对比情况.从图中我们看到,随着内存更新速率的增大,两种机制在同步数据量上的差距被迅速拉大.其原因在于随着内存更新速率的增大,迁移过程中Pre-copy机制重复复制的内存页面数急剧上升,而HybMEC只有部分的脏页面执行额外的一次复制.此外,HybMEC对于空闲页面的处理进一步减小了被复制的内存页数量.图7HybMEC迁移总时间随内存更新速率的变化图8Pre-copy与HybMEC迁移内存同步数据量对比上述3方面的迁移性能对比结果说明,由于依赖于剩余脏内存收敛速率,Pre-copy机制在访存密集型应用负载下,并不能保证理想的迁移性能,甚至出现迁移失败的情况.然而,HybMEC通过推送和按需复制相结合的方式,减缓了内存更新对迁移性能的影响,加速迁移过程的同时,大幅度地降低同步数据量,减少了迁移过程对物理资源的消耗.4.3并发Web负载为了研究HybMEC在迁移过程中对VM应用I/O性能造成的影响,我们以内部运行Apache2服务器的VM为迁移对象,跟踪并分析了迁移过程中Apache2服务器在并发访问下输出带宽的变化情况.实验中,我们在第3台物理机上模拟了50个并发客户端持续从Apache2服务器下载512KB静态文件的行为.图9和图10分别给出了Pre-copy和HybMEC迁移过程Apache2服务器输出带宽的变化曲线.其中,两次迁移实验分别从图中的10.5s和1.5s时刻开始.图中我们可以看出,两种迁移实验中输出带宽曲线均出现了瞬时下降和恢复.但是,HybMEC曲线中Apache2输出带宽出现明显波动,即从最小带宽值恢复至正常波动范围的持续时间要小于Pre-copy机制.出现上述实验结果的原因在于HybMEC在目的宿主机上恢复VM执行后,通过“定时推送”以及“预取页”加速了脏内存页数据的同步过程,避免了频繁的脏页面按需复制对VM运行性能的影响.此外,将VM切换到目的宿主机上运行之前,Hyb-MEC通过为脏页面设置缺页状态的操作,预先为脏页面在EPT表中建立了地址映射关系.这样,当VM访问上述脏页面时不再因为地址映射关系未建图9Pre-copy迁移过程中Apache2输出带宽变化图10HybMEC迁移过程中Apache2输出带宽变化Page11立而通过缺页异常陷入内核态KVM_MMU处理地址映射关系的建立.相反,Pre-copy机制由于在切换VM运行的时刻,目的宿主机上EPT基本为空,KVM需要在后续的运行过程中通过缺页异常处理动态地为VM访问的页面建立地址映射关系.因此,如图9所示,Pre-copy机制造成VM性能在迁移结束后的5s时间里才恢复正常.此外,在实验中我们也发现与内存压力负载实验类似的结果.在并发Web负载下,HybMEC的迁移持续时间仍明显小于Pre-copy机制.上述实验结果说明,即使是高并发I/O负载条件下,HybMEC在脏内存同步阶段,采用定时推送复制辅助按需复制的方式,保证了VM应用在迁移过程中更为稳定的I/O性能.4.4综合负载对比实验的最后,我们对比了4种不同类型负载下HybMEC与KVMPre-copy机制迁移的性能,包括总迁移时间、VM停机时间以及内存同步数据量,实验结果分别如图11~图13所示.实验中,除了上述内存压力负载(Mem_stress)和并发Web负载(Apache2_web),我们还选取Linux内核编译(Liux-kbuild)以及空闲系统(idle)作为迁移实验的应用负载.其中,Linux-kbuild是一种系统调用密集型应用负载,其对于CPU、内存以及磁盘文件读写的操作需求大致均衡;而idle负载除了必要的系统程序随客户OS启动后,无额外的应用程序运行,近似为低负载应用场景.从图11中我们不难看出4种负载情况下,相对于Pre-copy机制,HybMEC均大幅地缩短了迁移总时间.其中,在内存更新速率最大的Mem_stress负载下(内存更新速率被设定为4096页/s,即16MB/s),两种机制的迁移持续时间差距最大.在该实验中,HybMEC平均复制了76684个内存页面,其中大约有5200个页面(约20.3MB)经过脏页面同步二次复制;而Pre-copy在完成第一轮内存复制后,即近入了迭代同步过程直至剩余脏页面数小于预设值10,导致总迁移时间依赖于剩余脏页面收敛速率而被延长.而即使在最理想的idle负载实验里,Hyb-MEC迁移持续时间仍明显小于Pre-copy机制.在图12中,我们看到在相同的负载情况下,HybMEC均取得了更小的停机时间.其中,在idle负载条件下HybMEC取得了最小的VM停机时间.其原因如前文所分析的,由于idle负载的内存更新速率小于其它负载情况,在停机阶段剩余的脏页面数也少于其它负载.这也使得HybMEC在停机时间内设置脏页面缺页状态的执行延时相对于其它应用负载要小.相反,Pre-copy机制在停机时间内需要复制剩余脏内存页数据,并且复制的页面数不大于预设的最小剩余脏页面数.因此,虽然Pre-copy在4种负载下的停机时间大致相当,约为30ms,但是仍大于HybMEC4种负载实验中的12ms平均时间.最后,在图13中,我们对比两种迁移机制在4种类型负载下的内存同步数据量.即使是理想的idle负载,Pre-copy迁移的内存同步数据量仍然是HybMEC的3.72倍.通过上述4种不同类型负载下的迁移实验对比,我们不难发现HybMEC在减少迁移同步数据量、缩短总迁移时间以及减小VM应用性能损失等方面明显好于现有Pre-copy机制.同时,实验结果从另一个侧面验证了Pre-copy机制对于访存密集Page12型负载的低效性和不适用性.同比于其它3种类型负载,Pre-copy机制在内存压力负载的实验中各方面的迁移性能均取得最差结果.对剩余脏内存收敛速率的过分依赖是造成该结果的根本原因.但是,Hyb-MEC通过内存混合复制方式以及页面数据复制优化,大幅地减小了由于VM内存频繁更新所造成的各方面迁移性能下降.同时,HybMEC在复制页面时对于空闲页的优化处理,大幅度地削减了内存同步数据量,并提升了VM状态迁移的整体速度.5相关工作最早由Clark等人[5-6]提出并实现的基于内存预拷贝(Pre-copy)方式的VM在线迁移机制,将恢复VM在目的宿主机上的运行推迟在内存一致性状态同步完成之后.其中,内存状态需要通过多轮的脏内存复制实现同步.目前,Xen[3]、VMware[5]及KVM[9]等主流系统虚拟化平台上均提供了对于该机制的实现,并广泛应用于局域网环境中跨物理机的VM快速迁移.由于需要经过多轮迭代复制才能实现迁移目的端对VM内存状态的一致性备份,而机制本身不能控制或预测迁移过程中VM内存更新速率,因此Pre-copy机制存在内存同步的收敛性问题[15].该问题是指当网络传输速率小于VM修改其内存的速率时,内存状态的迭代复制过程由于不满足最少剩余脏页面数而不能主动停止.目前,解决上述问题的普遍方法是通过预设定最大迭代次数或数据传输量,约束迁移过程中内存状态的迭代复制.但是,对于具有大内存或访存密集型应用的VM,在停机阶段里大量剩余脏内存数据等待复制,延缓了VM在目的宿主机上的恢复运行,这势必延长了VM停机时间,并影响VM在迁移过程中的外部可访问性.类似的,在低速网络环境中执行VM在线迁移时,Pre-copy机制也可能遇到上述的收敛性问题.另外,Pre-copy机制的内存迭代复制还造成了迁移数据的高冗余性,导致了迁移过程对物理资源的不合理消耗.最近,在一些研究中提出利用内存压缩[16-17]或消除内存冗余数据[18]等方法减小迁移数据量.但是,这些方法仍以Pre-copy迁移机制作为性能优化前提,本质上没有脱离两阶段迁移基本模式,也没有从根本上消除迁移过程对内存迭代同步的依赖.此外,压缩算法的引入势必会造成迁移过程对宿主机CPU资源额外的消耗.基于内存后拷贝(Post-copy)方式的VM在线迁移机制[15,19],将内存的同步过程推迟到VM在目的宿主机上恢复运行之后.迁移过程首先在两端宿主机之间同步VM的VCPU以及除内存外的虚拟设备的状态,接着立即在目的宿主机上恢复VM执行.随后,VM内存数据则通过目的宿主机按需取页的方式实现同步.Post-copy机制中VM在目的宿主机上的恢复执行先于内存状态同步,所以在VM恢复执行的时刻,只有源宿主机拥有完整的VM内存状态.Post-copy机制能够保证在迁移过程中每一个内存页至多被复制一次.因此,该机制从根本上避免了冗余数据对网络带宽的不必要的占用.然而,VM切换至目的宿主机上运行,并访问一个未同步的内存页面时,Post-copy机制被迫中断VM正常运行,通过网络缺页处理及时地解决该页面的加载问题.虽然,类似HybMEC采用的预取页机制能够帮助Post-copy机制减少网络缺页处理的次数,但是,由于驻留在源宿主机上的内存页面数远大于HybMEC中的脏页面数,Post-copy机制仍无法避免频繁的按需加载内存页所造成的VM性能下降以及内部应用执行延时的上升.此外,上述工作在实现上未能很好地满足在线迁移机制的透明性要求.Hines等人[15]的工作需要分别修改客户OS以及宿主机OS的内核以实现动态内存气泡回收和内存页面的按需加载.因此,该方法中预先正确配置客户OS以及宿主机OS是VM能够执行Post-copy迁移的前提.Hirofuchi[19]在工作中,网络缺页处理需要通过除QEMU进程以及KVM内核模块外的辅助用户态进程和内核模块完成.由于辅助进程在启动时即严格绑定与VM内存映射关系,因此在VM执行在线迁移的前后,该辅助进程不能由KVM控制启动或停止,而需要通过手动控制.最近,VM动态克隆系统SnowFlock[20]利用了与Post-copy机制类似的VM内存加载方式.当开始执行克隆操作时,原VM的VCPU及虚拟设备的状态被迅速复制到目标宿主机上;接着,原VM和克隆VM各自恢复执行,并且分别按照“写时复制”和“按需加载”方式处理内存访问.当克隆VM访问未加载的内存页时,VMM通过网络缺页处理从原VM复制页面数据.但是,通过HybMEC的工作我们有理由相信,仅依靠“按需加载”处理克隆VM对内存页的缺页访问,SnowFlock很难保证克隆VM内部应用的执行性能和效率.此外,SnowFlock在实现上同样需要修改Xen及Linux客户OS以实现VMM对VM访存的捕获和内存页面的网络加载.另Page13外,与VM在线迁移工作类似,VM快照恢复同样需要解决VM内存页加载的问题,区别在于后者的内存数据从本地或网络磁盘上的快照文件中加载.Zhang等人[21]提出利用“懒恢复”(lazilyrestoring)机制实现对VM内存快照的快速加载,即VM快照恢复的初始阶段,预先加载内存工作集(workingset)中的内存页面.该工作更多地关注执行VM快照过程对内存工作集的跟踪以及内存工作集中页面数据的存储.与HybMEC类似,该工作中利用了内存硬件辅助虚拟化技术,以实现VM内存访问跟踪.6结论与下一步工作虚拟机技术充分利用复用单个物理机资源的优势,结合其灵活性、封装性,实现了物理资源的高度共享、资源利用率的显著提升以及软件应用的灵活部署.而VM在线迁移技术更是充分利用VM技术的封装性和灵活性,实现了VM运行状态通过计算机网络在物理机之间的快速迁移.通过结合源宿主机复制推送和目的宿主机按需复制两种方式,本文提出了基于内存混合复制方式的VM在线迁移机制HybMEC.在实现上,HybMEC不需要修改或重新编译客户机操作系统内核,对客户机操作系统及其应用程序完全透明.本文的基本思想是:利用源宿主机主动推送的方式完成绝大多数的内存数据的一次性复制;而将少数状态不一致的脏内存数据推迟到VM在目的宿主机上恢复执行之后,通过按需复制或定时推送的方式实现同步.相对于已有的迁移机制,由于不依赖于内存轮迭代同步,HybMEC能够大幅度地减少迁移过程中的同步数据量,并缩短总迁移时间.同时,在VM停机时间内,HybMEC不需要复制剩余脏页面数据,从而有效地缩短了VM停机造成的不可访问时间.为了进一步优化迁移性能,本文还对内存页数据的复制做了重要的优化,例如采用预取页机制以减少脏内存按需复制造成的VM性能损失,采用定时推送机制以缩短脏内存同步过程的持续时间,以及禁止空闲脏页面引发网络缺页处理以避免VM运行状态发生不必要的状态切换.基于KVM上,我们设计并实现了HybMEC原型系统,并通过多种不同类型的应用负载验证了HybMEC机制的高效性和可用性.实验结果表明,即使对于访存密集型应用,HybMEC也能够保证更短的VM停机时间以及总迁移时间,缩短迁移过程对源宿主机的长时间依赖.同时,HybMEC极大地降低了迁移过程同步数据量,减少了迁移过程本身对物理资源的占用.当VM在目的宿主机上恢复执行后,HybMEC迁移过程开始进入脏页面同步阶段.在此阶段中,VM的完整状态需要由源和目的宿主机共同维护:源宿主机拥有正确的脏内存状态;而目的宿主机拥有VM当前VCPU、部分内存以及其它虚拟设备的状态.因此,在该阶段中源或目的宿主机的宕机不但会导致VM迁移过程失败,而且还会造成VM运行状态的丢失.因此,HybMEC如何保证上述阶段中的迁移可靠性将成为我们下一步即将开展的重要工作.
