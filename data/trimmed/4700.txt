Page1一种基于双通道LDA模型的汉语词义表示与归纳方法1)(中国科学院自动化研究所模式识别国家重点实验室北京100190)2)(中国科学院脑科学与智能技术卓越创新中心北京100190)摘要语义记忆是人类理解自然语言的基础.人类理解语言的过程可以看作是对词义进行编码、对语义记忆进行检索,进而对词义进行解码的过程.因此,对词义进行合理地表示是计算机理解语言的关键步骤.该文总结分析了已有的词义表示方法与人脑词义表征的关系,针对汉语词汇的歧义现象,重点阐述了如何从歧义词所处的上下文中最大限度地自动获取关于歧义词的词义信息,并将这些信息整合,通过一系列的特征集合表示歧义词的词义.具体地说,该文将出现在歧义词上下文语境中有明确含义的实词作为模型的输入,同时在上下文中获取可以表示歧义词词义的其他特征,最终将这两种信息通过贝叶斯概率模型整合在一起,共同实现歧义词的词义表示和归纳.实验表明,该文提出的方法可以得到更好的词义表示和归纳效果.关键词词义表示;词义归纳;词义消歧;主题模型;双通道主题模型1引言随着互联网技术的快速发展,爆炸式的信息增长模式凸显了计算机自动理解和处理自然语言文本的重要性.目前自然语言处理的计算模型大多采用有监督的机器学习方法,以分类为手段,以获得较高的准确率为目标,无法处理训练样本中没有出现过的词汇和字符.少数计算模型,如主题模型(topicmodel)、词向量(Word2Vec)模型等,试图利用大规模无标注数据来对文本进行向量化表示,以解决传统计算模型泛化能力差和特征表示困难的问题,但即使这些模型对文本进行了编码,与真正实现文本语义理解的目标还是有较大的距离.反观我们人类自身,人脑可以毫不费力地理解一句话的含义,即使在阅读含有歧义词的句子时,人脑甚至都感觉不到歧义的存在.人脑处理语言的过程十分迅速,但由于其认知理解的生物过程尚不清楚,要想完全模拟人脑处理语言的认知过程和建立计算模型是十分困难的.幸运的是,认知心理学家已经对人脑处理语言的过程和表现进行了深入的研究,并得出了一些重要的结论,我们可以借鉴这些结论来建立更好的自然语言理解模型.根据认知心理学家研究的结果,人类理解语言是建立在概念存储基础之上的,只有在记忆中存储了相关的世界概念,才可能理解一句话的含义.认知心理学家将语言理解看作是一个增量式的动态系统,他们认为,读者增量式地理解句子的含义,即阅读到一个词汇时会自动将其概念含义融入到已经出现过的上文的含义之中[1].这样,人脑理解一个句子的过程就可以看作是概念信息的检索和语义整合的过程.大脑首先从记忆中检索单词对应的语义概念及相关信息,然后通过句子中的语法和上下文信息来整合句子的含义.所以,要想让计算机理解一个自然语言的句子,首先需要对词义进行合理地表示.相关研究表明,人类与外部世界的互动影响了概念的形成,因此概念中包含了丰富的信息.语言文字可以看作是人类表达、传递和交流大脑思维的一套符号系统,人类遣词造句从一定程度上也反映了人类对于外部世界的表达.概念的形成受到了视觉、Page3知觉和语言上的影响,本文仅关注如何从语言文字中对概念进行表示和归纳.根据语义的分布式假设,即相似的词出现在相似的上下文中,概念可以表示为统计特征的集合.在不利用知识库的前提下,我们可以从文本中利用词汇共现信息对词义进行某种程度上的表示,并将其看作词汇概念的一个原型.这样,计算机对词义进行表示相当于人脑对单词进行编码.对于新的刺激,人需要检索记忆和提取相关编码才能理解其含义.类似地,计算机需要将新的刺激与已有词义表示进行比较才能对其进行归类.词义消歧和词义归纳是计算语言学领域研究的两个基本任务.一般将词义消歧看作是有监督的分类问题,将歧义词上下文窗口范围内的多个单词及词性等当作可以区分词义的特征,在训练集中训练特征的权重并利用分类器对测试语料中的歧义词进行区分[2].而词义归纳被看作是无监督的聚类问题,通常利用歧义词上下文的相似程度对包含歧义词的句子进行聚类,从而实现对歧义词的词义归纳.不同于上述两种任务和已有的方法,本文根据上下文对歧义词的不同含义建立不同的向量表示,并利用CLP2010词义归纳评测数据集对词义表示的效果进行检验.与以往工作相比,本文的最大贡献在于提出了一种基于双通道隐含狄利克雷分布(LatentDirichletAllocation,LDA)模型的汉语词义表示与归纳方法,该方法将上下文中获取到的可以表示歧义词词义的信息与歧义词所在上下文中的实词进行整合,同时作为双通道的贝叶斯概率模型的输入,从而获得了更好的词义表示和归纳效果.2相关工作与对比2.1相关研究人类如何对概念进行表征是认知科学的基础问题.具身认知(embodiedcognition)理论认为,语言中包含的语义是人类与外部世界的交互和体验,语义由大脑的构造、神经的结构、感官和运动系统的活动方式共同决定.具身认知理论的观点极大地推动了认知心理学和认知神经科学研究的发展.与此同时,语义的分布式模型也取得了很多进步,语义的分布式假设认为从语音或者书面文字的统计分布中可以建立语义的计算模型.最近的研究趋向于认为,文本的统计数据和人对外部世界的感知都会影响人脑对于语义的表征[3].另外,一些认知心理学研究通过行为学实验验证了语义的分布式假设的正确性.比如,McDonald等人[4]设计了两个实验,通过分别操控被试对于“略微熟悉的词汇①”和“非词②”的语意信息③”和“非词④”的语境信息⑤,发现了语境中包含的信息对词汇相似度测试的结果造成了影响.该实验说明了词汇的分布式信息的确会影响人脑对于语义的表征.最近,一些研究通过融合词汇的分布式信息和人的感知信息,来建立词义表示的计算模型.实验证明,这种方法比单独利用上述信息可以在词汇相似度等测量指标上得到更好的效果[5-8].为了从大规模文本中抽取语义信息,计算语言学家将词义看作是单词在文本中统计分布的结果,建立了很多计算模型.向量空间模型(VectorSpaceModel,VSM)是表示文本词义重要的模型之一,它将词义看作高维的空间向量,利用词汇的共现信息对词汇含义进行表示.其中,最受关注的一种向量空间模型是潜藏语义分析(LatentSemanticAnalysis,LSA)方法,该方法通过奇异值分解(SingularValueDecomposition,SVD)对词项文档矩阵进行分解,使用降维后的矩阵构建潜在语义空间,试图找出词在文档中的潜在语义.但是,LSA无法解决一词多义的问题,因而其表达词义的能力有限.为了解决LSA模型对于词义表示的限制,Hofmann在LSA的基础上提出了一种概率隐含语义模型(probabilisticLatentSemanticAnalysis,pLSA),用概率的方式解释了文档的生成过程.之后,Blei等人在pLSA的基础上又提出了基于隐含狄利克雷分布(LatentDirichletAllocation,LDA)的主题模型,为pLSA模型的概率分布加上了先验知识,利用贝叶斯推断方法求解参数,解决了pLSA容易过拟合的问题.另一种可以对文本词汇进行向量化表示的方法是近年来非常流行的基于神经网络的词向量(Word2Vec)模型.这种方法通过对词汇上下文进行预测来学习词汇的向量化表示.Bengio等人[9](2003)提出的方法可以在神经网络语言模型(NeuralNetworkLanguageModel,NNLM)训练的过程中得到词汇的向量化表示.由于利用整个神经网络结构来学习词汇的向量化表示效率很低,Mikolov等人[10](2013)提出了词向量(Word2Vec)模型,可以①②③④⑤Page4利用单层的神经网络结构直接学习词向量,他们在神经网络语言模型的基础上进行了大量优化.由于Word2Vev模型可以快速地处理大规模无标注语料,且在词汇相似度比较(wordsimilaritytask)、词汇类比(wordanalogytask)等任务中表现良好.但是,Word2Vec模型生成的词向量维度无法解释其物理意义,且在词义归纳任务中不如贝叶斯模型的可扩展性好.另外,与本文相关的还有以下利用贝叶斯概率模型来完成词义归纳任务的工作.Brody和Lapata等人[11]利用主题模型对词义进行归纳,在模型学习过程中可以得到歧义词的词义表示.具体地,他们提出的方法将传统的LDA模型的输入变为包含歧义词的句子,这样学习到的文档(句子)主题即为歧义词在句子中的含义.他们还提出了一种多层的LDA模型来解决不同的特征分布问题,分别将不同的特征输入到不同的LDA层中,并在文档主题分布处加以融合.但是,模型仅仅考虑了词袋信息和常用的词义消歧特征(n元语法、共现信息、词性和依存句法信息),且多层次的LDA模型效果相比单层的LDA模型效果没有明显的提升.另外,为了简化实验过程,他们对不同种类的信息赋予了相同的权重,没有体现对不同特征分布的输入信息分别进行建模的优越性.上述工作的基本出发点是对歧义词的词义进行聚类,而词义表示只是模型学习过程中的副产物,没有专门探讨词义的表示问题.我们认为,只有合理地表示词义才是解决词汇理解问题的关键,因此文本主要关注如何从文本中获取更加丰富的词义表示.我们的实验证明,用统计方法得到的词义表示可以作为人脑中的词义表征的近似表示.2.2对比分析基于统计方法的词义表示研究是根据词义的分布式假设,通过词汇间的共现关系对词义进行表示.而大量认知心理学的相关研究表明,相似的词在大脑中有相似的词义表征.那么,基于统计方法的词义表示是否反映了人脑判断的这种相似性呢?下面我们分别计算从认知心理学角度得到的词汇相似度和从统计模型得到的词汇相似度,并分析其相关性,验证统计方法得到的词义表示与大脑的词义表征是否显著相关.由于认知心理学研究中公开的实验语料大多为英文,因此本节使用英文语料.实验中采用的McRae①数据集[12]是目前心理学研究中使用最广泛的关于(英语)语义特征的数据集.我们首先利用语义特征产生实验范式得到的词汇属性值计算不同词汇间的余弦相似度,得到McRae数据集中词汇间的相似度信息,将其看作人脑对词汇相似度的编码.为了直观展示McRae数据集中的词汇相似度信息,我们选取了38个基本单词,根据余弦相似度计算得到的结果绘制了图1.这38个单词分别属于不同的类别,包括餐具、水果、厨房用品、交通工具、动物、衣物和家具,见图1横坐标或纵坐标轴上的词汇.图中每一个矩形颜色小方块代表对应横坐标上的词汇与纵坐标上的词汇之间的相似度,颜色越深表示其相似度越高.其中,图中对角线为词汇与其自身的相似度,因此相似度值最高,为1.由图1可以看出,McRae数据集中的信息可以很好地捕捉词汇间的相似度信息.之后,我们再从统计模型的角度,利用目前广泛使用的3种词义表示模型:基于矩阵分解的模型、神经网络模型和贝叶斯概率模型,分别计算这38个词的词义表示效果,并对这3种模型得到的词义表示效果与人脑的词义表征进行对比分析.本文选择了LSA模型、Word2Vec模型和LDA模型分别作为上述3种模型的代表,在BNC(BritishNationalCorpus)②数据集上对词汇进行向量化表示.下面分别对LSA模型和Word2Vec(Skip-gram)模型的原理进行简要叙述.关于LDA模型的原理介绍见3.1节.①②Page5在LSA模型中,假设有N个文档,词典大小为M,将M个单词表示为特征矩阵犡=[X1,…,XM]∈RM×M,其中,Xij为词典中第i个单词在文档j中出现的频次.假设有D个不相关的隐变量U1,…,UD,其中Ud∈RN为单位长度.由于隐变量之间互不相关,可以得到犝T犝=犐,其中犐为单位矩阵.假设矩阵犡j可以表示为隐变量U1,…,UD的线性组合,即其中,犃=[adj]∈RD×M为特征空间到隐空间的映射矩阵,ε是均值为零的噪声.LSA模型的目标是求解映射矩阵犃.通过将问题转化为下述优化问题[13],通过最小化秩为D的构造矩阵犝犃与特征矩阵犡的误差,即可求解映射矩阵犃.如式(2)所示:在Word2Vec模型中,目前有两种目标函数:一种是利用目标词汇的上下文来预测目标词汇,称为连续的词袋模型(ContinuousBag-Of-Word,CBOW);另一种是利用目标词汇来预测其上下文的词汇,称为Skip-gram模型.一般认为Skip-gram方法在小数据集上的效果优于CBOW模型①,因此本文实验采用Skip-gram模型.Skip-gram模型的目标函数是最大化所有单词预测其上下文固定窗口c内单词的对数概率,即J(θ)=其中:狏′和狏分别是单词w的输入和输出向量;θ为模型参数.利用LSA、Word2Vec和LDA这3个模型均可以得到McRae数据集所有词汇的向量化表示,然后我们通过余弦相似度计算,可以分别得到3种情况下McRae数据集中的词汇间相似度值.然后将这些结果与认知心理学家得到的人脑对这些词汇的相似度进行相关度计算,从而验证统计方法得到的词义表示与大脑的词义表征是否显著相关.在具体的实验中我们选择的BNC数据集中句子的长度范围为200~300(单词数),且每个句子中包括至少一个McRae中的单词(参照文献[6]).去除停用词和低频词等预处理后共得到17526个句子,词汇量为90459.表1给出的是上述3种模型分别得到的词义相似度与认知心理学家得到的词义相似度之间的相关性结果.表1统计模型与人脑对于词汇相似度的相关性模型LSAWord2VecLDA表中,“皮尔逊”和“斯皮尔曼”是度量两个变量间相关程度的方法,皮尔逊值和斯皮尔曼值越大,说明两个变量越相关.“P值”是结果出现的可能性大小.P值越小,说明效果越显著.从表1可以看出,LSA模型、Word2Vec模型和LDA模型下得到的P值均近似为0,因此我们可以认为3个模型计算出的词义相似度与认知心理学研究得出的词汇相似度均有较大的相似性.换句话说,统计方法得到的文本表示所包含的词汇相似度信息可以较好地反映认知心理学研究的(人脑)类似的词汇相似度信息.从表1中的结果还可以看出,以BNC数据为训练集,LSA方法和LDA方法与人脑对于词汇相似度的相关性高于Word2Vec,这说明在中等规模训练数据的情况下,LSA方法和LDA方法得到的词义表示能够更好地描述词汇相似度信息.上述3种模型都是利用分布式假设对词义表示进行的建模,那么,这3种模型得到的词汇相似度信息是否类似呢?以下对这3个模型在上述38个基本单词上得到的词汇相似度与人脑反映出的词汇相似度(图1)分别给出更加直观的比较.图2是利用LSA词汇表示模型②和余弦相似度计算方法得到的词汇间相似度信息.原始矩阵为词汇-文档共现矩阵,即统计词典中每个单词在文档中出现的频次.图3为利用Word2Vec词汇表示模型③和余弦相似度计算方法得到的结果.图4为利用LDA④词汇表示模型和余弦相似度计算方法得到的结果.由图2、图3和图4可以看出,相比LSA和Word2Vec模型,LDA模型得到的词汇相似度边界①②③④Page6图2LSA模型处理BNC数据得到的词汇相似度图图3Word2Vec模型处理BNC数据得到的词汇相似度图图4LDA模型处理BNC数据得到的词汇相似度图比较清晰.相比LSA和LDA模型,Wod2Vec模型捕捉了较好的属于动物和交通工具类别的词汇间的相似性,但是对于不属于同一类别的词汇赋予了较高的相似性,导致了在McRae所有词汇的相似度比较上效果较差.总体来说,LSA、Word2Vec和LDA这3种表示模型从BNC语料中计算出的词汇之间的相似度比较类似,3个模型都捕获了部分与人脑类似的词汇相似度信息(如属于交通工具、动物和衣物类别的词汇).尽管与人的认知有一定的差距,但是通过表1可以看出,LSA模型和LDA模型得到的词义表示能更好地描述词汇相似度信息,并且LDA模型是一种生成式的贝叶斯概率模型,其可扩展性和可解释性要优于LSA模型和Word2Vec.LDA模型结构比较灵活,利用概率图模型来表示词义可以融合其它语义和句法结构信息[14],所以概率图模型受到了计算语言学家[15-18]和认知语言学家[19-22]的共同关注.针对词义表示和归纳任务,由于贝叶斯概率模型具备从大规模无标注语料中学习词义表示的能力,类似人学习概念的过程.因此,我们认为完全可以利用贝叶斯概率模型来提取文本中有用的信息以建立词义表示,并将其看作词汇概念的一个原型,利用相似度原则对标准数据集进行词义归纳.3模型描述3.1词义归纳模型词义归纳任务的目的是自动地从无标注语料中获取单词的含义.传统的归纳方法假设上下文信息可以对歧义词的含义进行区分,因此一般将词义归纳看作是无监督的聚类问题,利用上下文信息将歧义词划分为不同的类别.Brody等人[11]解决词义归纳问题采用生成式模型,并利用贝叶斯方法进行求解,利用包含歧义词的句子作为LDA主题模型的输入,将模型求解得到的文档主题看作句子中歧义词的词义.本文提出的模型建立在该模型的基础之上,因此以下首先对该基准模型做简要介绍.LDA主题模型如图5所示.在词义归纳任务中LDA模型的输入为包含歧义词的句子.假设语料中共有M个包含给定歧义词的句子,每个句子中包含Nm个单词,wmn表示第m个句子中的第n个词,smn表示wmn的词义.假设歧义词共有K个词义,则上下文中词wmn的分布为Page7p(wmn)=∑令φk=p(wmn|smn=k)为LDA模型的主题词项分布,即某个主题下单词出现的概率矩阵,矩阵每一行是一个词典大小为V维的向量,由狄利克雷分布产生,即φk~Dir(β),图中α和β为狄利克雷分布的参数.令θm=p(smn=k|d=m)为句子主题分布,即某个句子中主题出现的概率矩阵,矩阵每一行是一个主题个数K维的向量,由狄利克雷分布产生,即θm~Dir(α).由LDA主题模型描述的句子中词的产生由如下过程组成:对于主题k∈(1,…,K):采样主题词项向量:φk~Dir(β)对于包含歧义词的句子m∈(1,…,M):采样文档主题向量:θm~Dir(α)对于包含歧义词的句子m中的每个词n∈(1,…,Nm):采样词项n的主题:smn~Mult(θm)采样词项n:wmn~Mult(φs利用吉布斯采样(Gibbssampling)方法对模型进行推断[23],每一次循环需要从条件概率分布函数式(6)中采样当前词的词义si.狊i表示除了当前词的主题以外其他所有词项的主题;狑i表示除了当前词以外其他所有的词项;nk外句子m中属于主题k的词项个数;nt当前词以外主题k中属于词项t的个数.p(si=k|狊i,狑)∝p(si=k,wi=t|狊i,狑i)LDA主题模型中参数β控制主题词项分布的平滑程度,β越大,某个主题下的词项概率分布越平滑.参数α控制句子主题分布的平滑程度,α越大,某个句子中的主题概率分布越平滑.3.2我们的模型3.2.1基本思路贝叶斯概率模型将词义表示与词义归纳同步进行,即在实现词义归纳的同时,生成了关于词义的主题分布,使其具有了词义表示的功能.借鉴Brody等人[11]的方法,我们首先利用LDA主题模型在大规模无标注数据集上学习词义的表示,在词义表示的基础上对测试集数据进行词义归纳.模型流程见图6.考虑到不同词类的词需要不同的特征来表达[24],本文仅以名词歧义词的词义表达为实验目标.为了更好地捕捉句子中关于歧义词的词义信息,本文建立了双通道的狄利克雷分布(DualLatentDirichletAllocation,Dual-LDA)主题模型,在基准LDA模型的基础上加入可以表达歧义词词义的信息作为另一个通道的输入.下面对该双通道模型进行介绍.3.2.2Dual-LDA主题模型假设不同种类的数据分布不同,因此需要利用不同的输入通道来处理不同分布的数据.图7描述了执行词义归纳任务的Dual-LDA模型,图中符号含义同图5,下标1和2表示两个通道.对于不同种Page8类的数据赋予一个主题词项分布,在文档主题分布中进行融合.利用双通道模型融合不同种类的数据,得到的有关句子的主题词项分布可以更好地对歧义词的词义进行表示.考虑到人一般利用实词对概念进行描述,因此本文只提取文本中有明确语义的实词作为模型的输入.根据宾州中文树库中的词性标注,我们认为具有如下词性标签的词具有明确的词义信息:其他动词(VV)、形容词(VA)、专有名词(NR)、普通名词(NN)、除名词之外修饰名词的词(JJ)、外来词(FW)和量词(M).在我们的Dual-LDA模型中,从句子中提取的这类实词作为模型一个通道的输入,记作“通道一”.如歧义词“黄牛”出现在如下句子中:“刘翔博尔特,480的票100了!”卖力的吆喝,无法阻止黄牛迎来赔钱的夜晚,上海八万人体育场,上座人数只怕还不到八千.群众不再对“飞人”抱有信心,他们很明智,刘翔只得了第三,甚至没跑赢史冬鹏.词性标注①后抽取出了该句子中所有上文所述词性标签的词:刘翔,博尔特,票,卖力,吆喝,阻止,黄牛,迎来,赔,钱,夜晚,上海,人,体育场,上,座,人数,怕,到,群众,飞人,抱有,信心,明智,刘翔,得,跑,赢,史,冬鹏.另外,我们认为按上述方法抽取出的句子中的实词只能捕捉歧义词的主题或者部分上下文语境信息,并不能完全表示歧义词的词义.通过对大量歧义词文本的观察,本文认为包含歧义词的基本单元中,与歧义词具有并列关系的词、与歧义词紧邻的形容词、名词及动词②,也可以表达歧义词的含义.对于并列关系,本文利用“顿号”、“和”、“或”、“与”、“并”、“及”、“甚至”等关键字作为并列规则进行抽取.对于基本单元的切分边界,本文利用标点符号“,”、“.”、“?”、“!”、“:”等作为边界信息,从句子中抽取包含歧义词的基本单元.对于上述包含歧义词‘黄牛’的句子,得到基本单元为:无法阻止黄牛迎来赔钱的夜晚.对于歧义词的局部紧邻实词,为了避免语法分析工具带来的错误,本文抽取基本单元中歧义词前的一个形容词、动词和名词,以及歧义词后的一个动词和名词,来近似表示句子中歧义词的局部紧邻实词.对于上述包含歧义词‘黄牛’的句子,抽取出的词为:阻止,迎来,钱.为了缓解数据稀疏的问题,对于抽取的非歧义单词,本文在词义表示中加入了同义词词林中的词类信息.这些抽取出来的词和词类信息一起作为Dual-LDA主题模型的另一个通道的输入,记作“通道二”.同LDA模型,Dual-LDA模型同样利用吉布斯采样方法对模型进行推断,每一次循环需要从条件概率分布函数公式(7)中采样当前词的词义si.与LDA模型不同的是,更新nk的词项个数)时需要利用两个通道的信息,因为两个通道的数据共同影响文本的主题词项分布.其中λ1和λ2为两个通道的权重.p(si=k狊i,狑)∝p(si=k,wi=t狊i,狑i)其中,nk{2}表示通道二,其他符号含义同式(6).我们提出的Dual-LDA模型与传统的LDA模型有以下几处不同:Dual-LDA模型的输入为从包含歧义的句子中抽取的不同特征,训练得到主题的词汇分布即为歧义词的词义表示,测试得到的文档主题即为歧义词的词义.传统的LDA模型的输入为整个文档,训练得到的主题词汇的分布为整个文档的主题词汇分布,测试得到的主题为整个文档的主题.图8示意性地描述了执行词义归纳任务的Dual-LDA模型的工作原理.图8执行词义归纳任务的Dual-LDA的图模型表示图8中通道一和通道二的输入为从句子中抽取的特征,通过Dual-LDA模型训练可以得到φ1φ2主题下单词出现的概率矩阵,可以看作是歧义词不,分别为通道一和通道二的主题词项分布,即某个同词义的表示.这里需要强调的是,Dual-LDA模型的两个通道共享θ,即句子的主题分布,这样会影响①②Page9和φ2φ1义的编码.利用训练阶段得到的φ1Dual-LDA模型就可以得到测试语料中句子内歧义词的词义编号.4实验与分析实验中我们利用K-Means聚类模型①和单通道的基本LDA模型作为基线模型,一方面验证基于词义表示的方法②有利于词义归纳任务;另一方面验证本文提出的双通道的贝叶斯概率模型可以更好地获取词义表示和词义归纳的效果.4.1实验数据我们选择的歧义词为CLP2010词义归纳评测任务③中的部分名词.从大规模无标注训练语料中分别抽取包含歧义词的句子,筛选出29个出现句子数大于1000但小于10000的歧义名词.利用搜狗新闻数据作为词义表示实验的训练数据,大小为5.6GB.为了满足实验需求,首先从训练语料中抽取包含歧义词的句子约13万条.然后对句子进行清洗(去重和乱码过滤).最后对句子进行分词和词性标注,删除句子中词数少于10的句子,并去除停用词.最终训练集包含句子约12万个.测试语料中每个歧义词包含50个句子,每个歧义词的词义分布均匀.4.2评价方法由于词义归纳可以被看作是对歧义词实例的聚类问题,因此一般利用标准聚类方法的评价指标来对词义归纳效果进行评价.类别中实例的正确率是正确分类的样例数与总体样例数目的百分比,实例的召回率为返回的样例数目与相关样例(标准答案)数目的比值.参考文献[11]对歧义词词义的评价方法,我们利用有监督模型的评价方法对实验结果进行评价.由于本文提出的模型得出的主题个数与测试集中的歧义词的词义个数不同,我们需要将模型得到的主题词集合映射到相应的词义上才能对结果进行评价.对于主题1~K和词义1~S,我们用下面的式(8)计算KS(topic-sense)值,将主题空间映射到词义空间中:这样,给定某个文档的主题分布,相比于将概率最大的主题作为歧义词的词义,式(9)可以更好地预测歧义词的词义:其中,θjk为测试集文档的主题分布,即包含歧义词句子中歧义词的词义分布.有监督模型的评价方法包括准确率(P,precision)、召回率(R,recall)和F值.由于词义归纳时会对每个句子的歧义词赋予词义,因此本文用准确率作为实验最终的评价指标.令标准词义类别Sr的大小为nr,令经过KS计算映射到词义空间的类别hj的大小为nj,nr,j为上述两个类别相同实例的个数,则准确率可由式(10)计算得出4.3模型选择在模型选择时我们首先利用高频词义(Most-Frequent-Sense,MFS)模型对歧义词赋予常见的词义,作为一种基准模型,用来评定本文所选测试数据集的复杂度.然后利用K-Means聚类模型对测试数据集中的歧义词进行自动聚类,聚类中心个数与LDA模型相同,也作为基线模型之一,用来比较基于词义表示的模型(LDA模型和Dual-LDA模型)和聚类模型在词义归纳任务上的效果.最后,本文选择LDA模型作为基准模型,用来与本文提出的Dual-LDA模型比较词义归纳效果,验证合理的词义表示是否可以提升模型的性能.为了调试贝叶斯概率模型的参数,我们选择3个歧义词作为开发集分别对LDA模型和Dual-LDA模型进行调参.在LDA模型中,需要确定Dirichlet分布的先验α,β和主题个数K.同文献[11],本文固定参数β=0.1,调节α和主题数目K,α调节范围为0.005~1,K调节范围为2~9.其中在开发集中,主题数目K对模型效果的影响如图9所示.超参数α对模型效果的影响如图10所示.通过开发集进行调参,最终确定参数为α=0.32,K=3.Dual-LDA模型的狄利克雷分布的先验参数α,β及主题个数K选择和LDA模型参数相同.为了简化调参过程,我们没有分别调节β1和β2,令β1=β2=β.我们在开发集上对两个通道的权重λ1和λ2进行了调节实验,λ1和λ2的调节值分别为[0.8,1.0,1.2].①②③Page10图10超参数α对模型正确率的影响示意图经过收敛性验证,上述LDA模型和Dual-LDA模型设置吉布斯采样的迭代次数均为1000.由于模型中涉及随机采样过程,我们对每个模型都进行了3次实验,最后对结果取平均作为最终的实验结果.4.4实验结果表2展示了3个基准模型、Dual-LDA模型和调节了权重的LDA模型在词义归纳标准测试数据集上的效果.根据CLP2010标准测试集中歧义词词义分布均匀的原则,可以得到MFS模型的平均正确率.其中,每个歧义词的词义个数见附录1.由表2可以看到,LDA模型和Dual-LDA模型的正确率高于K-Means模型,这说明基于词义表示的方法可以提升词义归纳任务的效果,但是效果提升并不十分明显,我们会在结果分析中解释原因.由表2还可以看出,再加入歧义词的词义信息并对通道的权重进行调节后,Dual-LDA模型在这个测试集上取得了最好的效果.由于词义表示的好坏直接影响了模型的正确率,从而验证了本文的假设,即在句子中抽取出的可以表示歧义词词义的信息与句子的部分实词信息作为不同的输入通道,并通过贝叶斯模型进行整合,可以更好地表示歧义词的词义.另外,我们对Dual-LDA模型的不同通道赋予了不同的权值后,观察到改变主题和词义的权重对词义表示效果的影响.由表2可以看出,Dual-LDA(1.2{1}+0.8{2}①)平均正确率最高,这说明在本文的实验数据集上,通道一的特征比通道二的特征对词义归纳任务的效果要好.据此推断,如果对影响词义的重要信息赋予较大的权值,应该可以学习到更好的词义表示.因此,我们在LDA模型的基础上,利用词频×逆文档频率(TermFrequency×InverseDocumentFrequency,TF×IDF)和正值点互信息(PositivePointwiseMutualInformation,PPMI)指标对输入信息赋予不同的权值进行了进一步实验,但结果与预期相差较大,TF×IDF和PPMI指标的引入相反地降低了模型的平均正确率.对此我们也会在4.5.3节中解释其中的原因.Dual-LDA(0.8{1}+1{2})Dual-LDA(1{1}+0.8{2})Dual-LDA(1.2{1}+0.8{2})详细的实验结果见附录1,词语正确率分布图见附录2.4.5实例与结果分析下面以“光环、结晶、黄牛、春秋”4个词作为实例,对模型的效果进行进一步分析和说明.如表3所示,因为“光环”和“结晶”在K-Means模型和基于词义表示的模型上的正确率相差较大,所以选择这两个歧义词来比较K-Means模型与基于词义表示的模型之间的差异.而“黄牛”和“春秋”在LDA模型和Dual-LDA模型上的正确率相差较大,所以选择“黄牛”和“春秋”来比较LDA模型和Dual-LDA模型词义表示的差异.表4~表7分别展示了我们所选示例及贝叶斯模型得到的歧义词的词义表示结果.完整的歧义词测试正确率结果见附录2.表4和表5中S1~S3表示包含歧义词3个含义的测试集实例,由于K-Means模型根据实例间词汇的重合程度对句子进行聚类,因此,表4和表5中用测试集实例来反应K-Means算法的聚类效果.①1.2{1}+0.8{2}表示通道一的特征权重为1.2,通道二的Page11歧义词光环结晶黄牛春秋4.5.1实例(光环)“光环”一词在汉语词典①中有3个含义:(1)某些行星周围明亮的环状物,由冰和铁等构成,如土星、天王星等都有数量不等的光环;(2)发光的环子,如象征奥运会的五彩光环;(3)特指神像或者圣象头部周围的环形光辉.词义归纳任务测试集中只给出了含义(1)和(3).对于“光环”这个词,简单的K-Means聚类模型比LDA模型正确率高.表4比较了语料中的“光环”和LDA模型对“光环”的词义表示.由于测试集中包含“光环”的句子含义比较明确,用词比较一致,而利用大规模无标注语料学习到的词义表示得到的结果和词义归纳标准测试集的领域差异较大,因而导致了基于词义表示模型正确率的减少.S1天文学家认为,他们将很快会直接观测到黑洞,并且能够观测到这些〈head〉光环〈/head〉.S1太阳缓缓落山时,云层中的小冰珠将太阳光折射成一圈圈不同的〈head〉光环〈/head〉,继而又连缀成一根直达天顶的太阳光柱.月亮光柱则更为罕见.S2佛像头后表现出的圆形〈head〉光环〈/head〉.S2当时多钦哲在他处见到多智钦的光蕴身在虚空中,〈head〉光环〈/head〉围绕,赫赫放光.T1光环冠军比赛场赛季球员世界球队中国顶T2光环中国市场公司品牌企业基金顶元国际T3光环明星生活想头身上笼罩奥运女父亲4.5.2实例(结晶)汉语词典给“结晶”一词定义了3个含义:(1)物质从液态(溶液或熔化状态)或气态形成晶体;(2)原子、离子或分子按一定空间次序排列而成的固体,具有规则的外形.如食盐、石英、云母、明矾等;(3)比喻珍贵的成果,如劳动的结晶.词义归纳任务测试集中将(1)和(2)归为一个含义,加上含义(3),共两个含义.表5中比较了K-Means模型和LDA模型对于“结晶”的效果.由表5可以看出LDA模型学习到“结晶”的词义表示与测试集标准词义一致.对比实例“光环”并结合表3可以看出,LDA模型和Dual-LDA模型在这种词义表示与测试集一致的实例上能得到更好的词义归纳效果.S1现对外承接各类中草药的〈head〉结晶〈/head〉加工服务及技术支持.S1日本研究人员称,他们制造出氧化钛的一种新的〈head〉结晶〈/head〉形式,可以用于制造“超级”蓝光光盘,这种光盘不仅价格更加低廉,而且其数据存储能力是DVD的几千倍.S2一片绿叶不仅是大自然的恩赐,更是人类辛勤劳作的〈head〉结晶〈/head〉.S2所以汉字是汉民族文化的〈head〉结晶〈/head〉,是民族文化历经数千年凝练而成的精华,值得善待珍视,而不应随便对汉字动“手术”.T1结晶过程研究工艺结构作用影响水技术温度T2结晶爱情孩子爱岁儿子生活女儿想妻子T3结晶中国智慧文化发展技术思想国精神公司4.5.3实例(黄牛)表6中T1~T4表示模型对主题通道的表示,ST1~ST3表示模型对词义通道的表示.在双通道模型中,由于主题相同的约束,T1和ST1表示相同的词义,其他T和ST的含义与上述描述一致.词义表示实例中的字母,如“Di02”,是为了改善词义通道数据的稀疏性引入的同义词信息,由同义词词林中得到.T1黄牛元票门票买钱名球迷球票价格T2黄牛车二手元市场车牌价格头牛工作T3老黄牛头牛公牛工作人民村民岁长T1黄牛元票价格门票车市场二手买钱T2黄牛头牛发展产业改良县元企业经济T3老黄牛工作足球成都精神影片小罗村长党员ST1黄牛买票手中倒卖告诉钱门票车牌黑车ST2黄牛改良养殖县产业基地发展Di02品种Di18ST3老拿出黄牛体育恳恳求票奔走遍地勤勤精神T1黄牛倒卖拍牌假票兜售赠票车票挂号号源售票处T2黄牛黄牛票锵肉用倒号鲁力冻配返现驱使空子T3黄牛黄牛党倒票返T1黄牛倒票黄牛党炒到假票售票处卖票球票返利水牛T2黄牛倒卖车牌拍牌黄牛党返T3黄牛兜售贩子赠票卖完票贩万荣二郎山育肥黄牛票“黄牛”一词在汉语词典中有两个意思:(1)牛的一种,角短,皮毛黄褐色,或黑色,也有杂色的,毛短,用来耕地或拉车,肉供食用,皮可以制革;(2)指恃力气或利用不正当手法抢购物资以及车票、门票后高价出售而从中取利的人.由表6可以看出,LDA模型的词义表示结果捕①http://xh.5156edu.com/Page12捉到了歧义词的常用含义.表中T1表示含义(2),T2和T3表示含义(1).在Dual-LDA中,T1和ST1表示含义(2),T2、T3、ST2和ST3表示含义(1).可以看出,ST1~ST3对“黄牛”的词义做出了正确的表示,可以帮助提高模型的词义表示效果.另外,从LDA模型和Dual-LDA模型的T3中可以看出,模型还捕捉到了“黄牛”的比喻义.标准测试集中没有给出这个含义,这也会影响基于词义表示模型的效果.LDA-TF×IDF和LDA-PPMI模型对输入信息按TF×IDF和PPMI指标赋予了不同的权重.Wilson和Chew[25]利用这个方法在多语言的检索任务上得到了很好的效果.但是,在我们的实验中(见表6)该模型只捕捉到了“黄牛”的含义(2).我们认为,这是因为训练语料是搜狗网络新闻文本,大部分包含“黄牛”的句子都是表示“黄牛”的含义(2),而且模型的输入单词已经做过筛选,再对单词赋予权值可能会影响单词权重的真实表达.因而TF×IDF和PPMI指标将“黄牛”含义(2)的相关词赋予了较高的权重,导致词义表示部分含义的缺失.4.5.4实例(春秋)“春秋”一词在汉语词典中有5个含义:(1)年岁;光阴:苦度春秋|他在讲台上耕耘了40个春秋;(2)泛指历史:甘洒热血写春秋;(3)时代名,因鲁国编年史《春秋》得名,一般指前770年~前476年这个时期;(4)儒家经典之一,编年体春秋史;(5)古代史书的通称.词义归纳任务测试集中只给出了3个含义,分别是“春夏秋冬、春秋两个季度”和含义(3)、(1),如表7所示.T1春秋中国时期文化历史国战国淹城荆州时代T2航空春秋公司旅客元服务航班上海黑名单延误T3春秋墓传文物发现时代岁考古世界长ST1航空春秋公司元旅客服务航班上海黑名单延误ST2春秋中国发展社会活动世界孔子文化传生活ST3春秋时期国文化历史战国淹城荆州中国时代表7比较了LDA模型和Dual-LDA模型在“春秋”一词上的词义表示效果.可以看到,LDA模型和Dual-LDA模型捕捉到的词义与测试集给出的标准词义不符,这是由于语料中大量出现“春秋航空”和“春秋儒家经典”的原因,导致了贝叶斯模型得到“春秋”一词其他的含义.由表7可以看到,Dual-LDA模型的正确率并没有单通道LDA模型的正确率高,一方面由于贝叶斯模型捕捉到的词义与测试集不同,导致了在语料中捕捉到的表示词义的信息并不能帮助提高模型的正确率.另一方面,“春秋”是抽象词,本文抽取的可以表示词义的特征并不适用.比如包含歧义词“春秋”的句子:他在职业拳坛上奋战了25个春秋,一共打了202场职业赛,抽取的特征为:Dd05,Di19,奋战,拳坛,职业.显然,这些特征对于词义归纳并没有帮助.针对这个问题,由于本文针对词义归纳任务中的每个歧义词都训练一个模型,因此,在实际应用中选择模型融合的方式可以得到更好的效果,即对于每个歧义词,选择LDA模型和Dual-LDA模型在这个歧义词上测试正确率高的那个模型.4.5.5领域差异性分析大规模无标注语料与标准测试集的领域差异性影响了基于词义表示的模型在词义归纳任务上的正确率.实际上,训练集与测试集的差异性影响模型性能是所有统计方法存在的普遍问题.针对这一问题,我们手工将测试样例中与基于词义表示模型得到的词义不一致的样例排除,最后利用剩余的测试样例重新评估模型的效果.结果显示,排除了部分大规模无标注语料与测试集数据主题不一致的样例后,除了K-Means以外(K-Means模型不需要训练),LDA模型和Dual-LDA模型的平均正确率均有了一定幅度的提高(见表8).具体实验结果见附录1.由表8可以看出,与K-Means模型相比,LDA模型和Dual-LDA模型利用在大规模无标注语料中获取的词义表示信息对提升词义归纳任务的效果有明显的帮助,并且本文提出的Dual-LDA模型比LDA模型得到更好的词义表示和词义归纳效果.5总结与展望本文尝试换一个角度来研究词义表示和归纳任务.首先利用两种数据的相似度矩阵说明文本的词义表示可以捕捉大脑词义表征的部分信息,然后总结分析了已有的词义表示方法与人脑词义表征的关系,最后提出了一种新的汉语词义表示与归纳的方法,利用双通道的贝叶斯概率模型在大规模无标注数据集上学习歧义词的共现信息,建立词义的向量Page13化表示,并在此基础上,对测试集中的歧义词自动地进行词义归纳.实验表明,本文提出的方法可以提升词义归纳模型的性能.为了更好地表示单词的含义,在下一步的工作中,我们将从以下几点进一步开展相关研究:(1)人在进行词义归纳时,经常会从多个角度进行考虑,因此词义的表达应在多个维度上(主题、句法、语用、属性等)进行,词义类别可以由选择的维度确定.(2)不同的上下文会影响词义的表达,因此应该建立更加灵活的模型以捕捉上下文对于词义的影响.另外,仅从无结构的文本中挖掘词义是十分局限的,大规模知识库的出现为词义表示提供了良好的条件.如果词义表示可以将知识库中的概念作为一般的世界知识,上下文可以对一般世界知识进行调整,那么应该可以得到更好的词义表示效果.(3)针对训练语料与测试语料的领域不一致性和新义项问题,研究领域自适应方法,提高词义归纳模型的效果.(4)研究人类学习概念、表征概念和整合不同种类信息的认知机制,建立计算模型以组合不同类型的信息,从而获得更好的词义表示结果.致谢审稿专家对本文提出了深刻而到位的修改意见和建议,在此表示衷心地感谢!通过对这些审稿意见的学习和对本文的多次修改,我们感到受益匪浅.
