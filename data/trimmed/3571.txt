Page1针对高速数据流的大规模数据实时处理方法亓开元1),2)赵卓峰1),3)房俊1),3)马强1),2)1)(中国科学院计算技术研究所北京100190)2)(中国科学院研究生院北京100190)3)(北方工业大学信息工程学院北京100144)摘要以实时传感数据和历史感知数据为基础的各类计算需求逐渐成为当前物联网应用建设中的关键,如何实现基于高速数据流和大规模历史数据的实时计算成为数据处理领域的新挑战.现有批处理方式的MapReduce大规模数据处理技术难以满足此类计算的实时要求.文中结合城市车辆数据的实时采集与处理应用,在理论和实践分析的基础上,提出了一种针对高速数据流的大规模数据实时处理方法,并对方法中的本地阶段化流水线、中间结果缓存等关键技术瓶颈进行了改进.其中,根据系统参数控制阶段化流水线,使CPU得到了充分、有效利用;通过改造内外存数据结构、读写策略和替换算法,优化了本地中间结果的高并发读写性能.实验表明,上述方法可以显著提升大规模历史数据上数据流处理的实时性和可伸缩性.关键词数据流处理;大规模数据处理;MapReduce;物联网;大数据;云计算1引言随着物联网的发展,以实时传感数据为基础的各类数据流处理逐渐成为当前物联网应用构造的关键.面对持续到达的数据流,数据流处理系统必须快速对其进行响应并即时输出结果.由于数据流的连续性和无限性,有限的处理机不可能处理数据流的完整信息,因而采用窗口机制(时间、数据量)来划定处理边界,窗口边界范围内已积累的数据称之为历史数据.传统数据流处理[1-3]受数据采集速度、传输带宽和内存容量等因素的限制,侧重于针对一个相对小的历史数据规模进行.随着数据采集和传输技术的进步,使得短时间内积累大量历史数据成为可能.同时,当前物联网环境下数据流处理应用的长期性、全面性和准确性需求也要求扩大历史数据规模.以一个物联网环境下的车辆监管应用为例.该应用通过传感设备对城市车辆实时数据(包括道路运行车辆和停车场静止车辆)进行收集,并在已收集数据的基础上进行套牌、超速、限行等多种违法车辆的自动识别计算.应用面对的数据一方面是高速到达的数据流,另一方面是持久化的历史数据,要求实时的完成数据流同历史数据的分析、比较等计算.类似的应用还包括网络入侵检测、Web个性化搜索等.在这类应用中,由于窗口范围的不断变大,数据处理对象(如车牌)数量的急速增加,以及每个数据对象数据量(如车辆监控信息)的迅速增加,造成了历史数据规模不断扩大.在上述趋势下,面向大规模历史数据的数据流实时处理需求同计算、存储能力不足之间的矛盾成为云计算和物联网领域的新挑战.本文将此问题定义为数据流处理的可伸缩性问题.现有数据流处理的可伸缩性研究可分为集中式和分布式两类.在集中式环境下,受计算和存储(主要是内存)资源限制,主要通过概要数据[4]、准入控制[1]、QoS降阶[2]等方法,以牺牲服务质量为代价保障伸缩性.在分布式环境下,针对由多个处理算子组成的数据流处理网络,主要通过在多个节点上平衡算子的分布来保障伸缩性[2-3],但处理能力仍局限于单个算子所在节点所能处理的数据窗口,在面对大规模历史数据的情况下伸缩能力不足.面向大规模历史数据的数据流处理需要突破单个节点的内存和计算能力限制.计算、存储设备性能的提高、成本的降低和大规模数据处理技术的不断成熟,为采用无共享集群架构解决此类数据处理问题创造了条件.为了支撑大规模数据的存储和计算,当前往往采用具有多个CPU的多核集群计算架构,以及Cache、内存、外存和分布式存储四层存储结构,如图1.在这种架构中,节点上的多核CPU构成了本地计算资源;相比于分布式存储,节点上的内存和外存组成了高速的本地存储.在无共享集群架构下,利用MapReduce[5]编程模型解决大规模数据处理需求同计算、存储能力不足之间的矛盾是云计算的核心技术,MapReduce通过简单的编程接口为并行处理可划分的大规模数据提供了支持,向程序员屏蔽了任务调度、数据存储和传输等细节,非常适合解决数据处理问题的伸缩性需求.然而,现有的MapReduce方法,如Hadoop①、Phoenix[6]等,属于对持久化数据的批处理方式,在每次处理时,都需要初始化运行环境,重复载入、处理大规模数据,同步执行Map和Reduce阶段,并在节点间传递大量数据.以批处理方式处理持续到达的数据流,若每次处理小规模批的数据,则系统开销太大,实时性受到限制,若等待批达到一定规模又增加了处理延迟,同样无法满足实时需求.因此,针对高速数据流下的大规模数据实时处理需求,如何利用MapReduce模型是需要考虑的问题.为增强MapReduce的数据流处理能力,可以通过预处理、分布缓存和复用中间结果的方法避免每次数据流到达时的历史数据重复处理开销,并使得数据流处理本地化,减少节点间的数据传输开销.针对本地化的数据流处理,可以采用事件驱动的阶段化处理架构[7](StagedEventDrivenArchitecture,①ApacheHadoop[EB/OL].http://hadoop.apache.org/Page3SEDA),利用线程池技术减少每次处理的初始化开销,并通过划分阶段和在阶段间异步传递数据,消除阶段之间的数据同步.基于以上思想,本文首先通过理论和实践分析证明了采用MapReduce模型解决此类问题的适应性,并在此基础上提出了一种支持高速数据流下大规模数据实时处理的方法RTMR(Real-TimeMapReduce).RTMR的处理过程为预处理历史数据并将中间结果分布缓存到各个节点上,在节点上基于SEDA构造从Map阶段到Reduce阶段的本地阶段化流水线,充分利用本地计算和存储资源实现数据流同历史数据的实时计算.在上述方法中,还存在以下挑战性问题:(1)Esper①公布的数据表明,CPU是影响数据流处理性能的关键资源.在大规模历史数据情况下,计算量骤增,使得有效利用CPU更为重要.因此,如何充分、有效地利用CPU(包括CPUCache)提高实时处理能力,是本地阶段化处理面临的关键问题.(2)在本地阶段化架构下,针对Map和Reduce线程对中间结果的频繁读写,应该如何支持对中间结果本地存储的高并发访问也是需要解决的问题.针对上述问题,RTMR方法包括了一种基于系统参数的本地阶段化处理优化方法和支持高并发读写的本地存储方法.2针对高速数据流的大规模数据实时处理方法批处理方式的MapReduce无法满足物联网环境下数据流的实时处理需求.本节在理论和实践分析的基础上,基于MapReduce模型提出了支持高速数据流下大规模数据实时处理的方法RTMR.2.1RTMR方法MapReduce模型的定义[5]为其处理过程是,Map方法将[k1,v1]键值对转换为[k2,v2]键值对,Reduce方法针对每个k2的值列表list(v2)做list操作.将上述模型改造为函数形式,若待处理数据为D,Map阶段中间结果为I,MapReduce过程可以表示为其中,M表示Map方法,R表示Reduce方法,list表示Reduce方法所做的操作.下面分析MapRe-duce模型的性质.定义1.对于函数F:I→O,若存在函数P:O×O→O,使得F(D+Δ)=P(F(D),F(Δ)),则称F为可合并的.定义2.对于数据集D的n个数据子集D1,D2,…,Dn,若D1∩D2∩…∩Dn=并且D1∪D2∪…∪Dn=D,则称D1,D2,…,Dn为D上的一个划分.定义3.对于键值对数据集合D,键集合K,称集合{d|d.key∈K,d∈D}为D在K上的投影,记为σK(D).由MapReduce模型和上述定义可知,MapReduce具有以下性质:(1)Map方法满足分配率.即两个数据集合并集上的Map等于分别对两个集合Map的并集(2)Reduce方法具有可合并性.即(3)Reduce方法满足分配率,即若K1,K2,…,Kn为中间结果I键集合的一个划分,则list(I)=list(σK1(I))+list(σK2(I))+…+list(σKn(I)).定理1.MapReduce为可合并的.证明.根据MapReduce的性质,对于数据D和增量Δ有因此,由定义1可知MapReduce是可合并的.定理1表明MapReduce模型可通过预处理历史数据并缓存中间结果的方法降低每次数据流到达时的重复处理开销,提高实时处理能力.将上述过程用函数表示为MR(D+Δ)=list(ID+IΔ)=MR(Δ|ID).定理2.K1,K2,…,Kn为MapReduce中间结果I键集合的一个划分,对于数据增量Δ在I上的MapReduce,有MR(Δ|I)=MR(Δ|σK1(I))+MR(Δ|σK2(I))+…+①Esperperformance[EB/OL].http://docs.codehaus.org/Page4证明.由MapReduce的性质,对于中间结果I和数据增量Δ,有MR(Δ|I)=list(I+IΔ)=list(σK1(I+IΔ))+list(σK2(I+IΔ))+…+list(σKn(I+IΔ)).对于Reduce方法来说,在中间结果的K1投影上处理的数据增量仅与K1有关,即MR(Δ|σK1(I))=list(IΔ+σK1(I))同理,=list(σK1(IΔ+σK1(I)))=list(σK1(IΔ+I)).因此,MR(Δ|I)=MR(Δ|σK1(I))+MR(Δ|σK2(I))+…+定理2表明MapReduce模型通过分布缓存中间结果,可以使数据流同中间结果的计算仅发生在节点本地.由于避免了节点间的数据传输,因此合理的分布中间结果能够保障整个架构的可伸缩性.从实践角度,以套牌车计算为例,在2.93GHzCPU,2GB内存和1Gbps以太网连接的服务器集群上处理数据流,在已有100MB历史数据的情况下,针对每条200B的数据流,从接收到完成Hash分组操作的平均耗时是4(±0.5μs),完成数据流同历史数据之间比较操作的平均耗时是1(±0.2ms),在服务器之间完成数据传输的平均延迟为40(±4μs).上述实验数据表明一颗2.93GHzCPU能够以超过50MB/s的速度接收和分组数据流,而在实际应用中,受采集端带宽等限制,数据流远达不到这个速度.因此,对于多核系统,接收和分组数据流(Map阶段)只需占用很少一部分CPU资源,是套牌计算这类应用所要面对的次要矛盾,主要矛盾是数据流同大规模数据之间的统计、比较等计算(Reduce阶段)以及节点之间的数据传输.为了降低处理主要矛盾的系统开销,也就是减少历史数据重复处理和避免节点间数据传输,可以采用前述理论分析论证的技术路线:将预处理的历史数据中间结果分布缓存于各节点;每个节点冗余地接收数据流,通过Map阶段过滤出本节点负责处理的数据并在本地缓存上进行Reduce计算.当已有节点的本地计算和存储资源不能满足实时性需求时,可以在新增节点上通过重新划分和移动缓存数据进行扩展.有鉴于此,我们设计了大规模历史数据上的数据流处理方法RTMR,如图2.RTMR的工作过程如下.过程1.RTMR过程.1.中间结果缓存.预处理相关历史数据生成中间结果,根据k2的Hash值划分区间,分布缓存到各个工作节点的本地存储上.2.本地阶段化处理.Map阶段通过作用于k2的Hash函数分组数据流,并按照区间划分过滤出与本地中间结果相关的数据,异步的传递给Reduce阶段进行与中间结果的计算.3.数据同步.将本地计算结果同步到分布式存储.在RTMR方法中,工作节点负责维护本地中间结果缓存和阶段化流水线.控制节点负责RTMR任务的生命周期管理、可靠性和可伸缩性保障.本文主要对中间结果本地存储、本地阶段化流水线等技术进行讨论,对于在控制节点中需要解决的关键问题,将在后续工作介绍.2.2编程接口根据RTMR的处理过程,RTMR编程接口除map和reduce方法外,还包括预加载方法load和数据同步方法update,如代码1.load方法默认以map和reduce方法预处理相关持久化历史数据生成中间结果,程序员还可以在load方法中实现其他相关数据的预加载,如车辆监管应用中的套牌阈值、布控黑名单列表等.update默认将中间结果同步到分布式存储,为了增强适应性,RTMR支持定时和即时两种同步机制.Page5代码1.RTMR编程接口.publicclassFakeLicenseCarJobimplementsJob{publicvoidload(){…}publicvoidupdate(){…}publicstaticclassFakeLicenseCarMapTaskimplementsMapTask{publicvoidmap(){…}}publicstaticclassFakeLicenseCarReduceTaskimplementsReduceTask{publicvoidreduce(){…}}}3支持高并发读写的中间结果本地存储为减少每次数据流到达时的历史数据重复计算开销,RTMR支持中间结果缓存.在本地阶段化架构下,Map和Reduce工作线程将对中间结果频繁地进行读写,优化中间结果的高并发读写性能是提高数据流处理能力的关键.本节在介绍中间结果内存数据结构和外存文件结构基础上,提出了一种支持高并发读写的本地存储优化方法.3.1中间结果存储结构定义4.在MapReduce模型中,将[k2,list(v2)]以及list(v2)称为中间结果.借鉴Metis[8]的思路,中间结果在内存中采用HashB+树结构存储,如图3.在这种结构中,具有相同Hash值的k2在Hash表的同一项中用B+树组织,[k2,list(v2)]在B+树的叶节点用链表组织,list(v2)存储在B+树的叶节点.HashB+树结构具有很高的读写性能.如果k2可预测并且具有唯一的Hash值,则可以通过为Hash表分配足够的项来避免冲突和B+树查找,插入和查找操作都只有O(1)的复杂度.如果k2没有唯一的Hash值或不可预测,则在Hash表项中维护一个B+树,插入和查找操作也只有O(1)+O(logn)的复杂度.为了扩大中间结果的本地存储容量,在外存构造SSTable文件[9]存储中间结果.SSTable文件结构包括一个索引块和多个64KB的数据块(如图4),以块为单位为Hash表项分配外存空间.在数据流处理过程中,如果所需的中间结果Hash表项不在内存而在外存并且内存已无空间,将发生内外存替换.针对大规模的历史数据,为在集群环境下保障可伸缩,RTMR为工作节点划分其所负责的中间结果Hash区间.如图5所示,k2的Hash区间分布在n个工作节点上(P1,P2,…,Pn).由RTMR过程可知,Map阶段包括一个对于k2的Hash分组操作,每个节点只负责区间内数据流的处理.如果Hash区间划分采用对节点数取余的方法,那么在伸缩时(增减节点)会引发大量数据的移动.为了减少伸缩时的数据移动规模,RTMR采用一致性Hash算法[10]在节点上划分中间结果.例如,原有P1、P2和P33个分区,当增加一个节点时,只需要将P1和P3的一部分划分为P4即可,如图6.在上述存储结构基础上,阶段化流水线中Map和Reduce线程对中间结果内存结构的并发读写同步,以及对中间结果外存文件的并发读写开销制约Page6了数据流实时处理能力.为了提高本地中间结果的并发读写性能,还应从两方面进行考虑:(1)建立内存缓冲区减少并发线程之间的同步;(2)改造外存文件读写策略和替换算法降低外存的并发读写开销.3.2内存缓冲区由上节可知,工作节点在内存中维护HashB+树存放中间结果.在本地阶段化架构下,Map和Reduce线程都会对中间结果频繁地进行读写,Map线程写入处理结果,Reduce线程读取Map结果,处理图7本地存储结构3.3外存读写策略现有基于SSTable结构的文件读写策略是写优化的,如BigTable[9]在将内存缓存数据写(dump)到磁盘时采用直接写入一个新文件的追加写(minorcompaction)方式,而在读时需要将缓存数据和若干个小文件进行合并(mergecompaction),开销较大.对于中间结果本地存储文件来说,读写操作都比较频繁并且比例均衡,不能盲目地只优化写操作.为提高并发读写性能,可以根据开销选择读写方式.下面给出各种读写开销的估算方法.后写入或更新中间结果.因此,为了提高内存并发读写性能,关键是减少Map和Reduce工作线程之间对同一块内存区域的并发读写同步开销.通过建立Map和Reduce阶段之间的缓冲区能够避免对中间结果的读写同步.为了避免多个Map线程之间对缓冲区的写同步,每个Map线程独占一个缓冲区.为了避免Map和Reduce线程之间对缓冲区的读写同步,缓冲区设计成一个FIFO队列,Map线程向队列尾部写入,Reduce线程从队列头部读出.根据Reduce的分配率,每个Reduce线程负责一个Hash区间,Reduce线程之间不存在同步关系.Map和Reduce线程之间对应Hash区间建立缓冲区,Map线程在数据流中过滤出本节点负责的数据并将其写入相应区间的缓冲区,Reduce线程处理负责区间内的缓冲数据并将计算结果写到HashB+树中.按照上述设计,根据Map和Reduce线程数量,在内存中的缓冲数据形成一个M×N的缓冲区矩阵,如图7.若寻道时间为常数Cs,数据读写开销函数为Cr和Cw,数据合并开销函数为Cm.追加写(dumpbyminorcompaction)数据d包括寻道和写数据开销,即合并读(readbymergecompaction)已有数据d和新增数据d包括两次寻道、两次读数据以及数据合并开销,即Costrmc=2Cs+Cr(d)+Cr(d)+Cm(d,d).合并写(dumpbymergecompaction)数据d包Page7括寻道和写数据开销,即随机读(readrandomly)数据d包括寻道和读数据开销,即基于上述计算方法,在出现内外存替换时,对于要替换的Hash表项,应该首先利用Map和Reduce阶段间的缓冲区查看该表项是否即将被访问.若此表项不会很快被访问,采用写开销较小的追加写方式;若此表项很快被访问,则比较若Cost1开销大,选择合并写和随机读方式,若Cost2开销大,选择追加写和合并读方式.此外,针对追加写方式产生的小文件,通过管理线程进行合并以优化读操作,尤其在系统由于负载低而CPU空闲时,可以增加用于合并文件的管理线程.3.4替换算法提高替换算法的命中率也是降低外存读写开销的重要方法.在传统的操作系统页面替换算法中,最优算法[11]根据将要访问的页信息确定替换页,所以命中率最高,但在实际系统中,由于无法预知将要访问的页,该方法较少使用.在RTMR的本地阶段化流水线中,Map和Reduce阶段之间的缓冲区包含着将要访问的Hash表项信息,所以可以利用最优算法的思想.此外,数据访问的局部性和替换代价也是需要考虑的因素.因此,RTMR内外存替换算法按照是否即将访问、是否最近访问、替换代价最小的顺序确定被替换的Hash表项.其中,检索缓冲区数据确定表项是否将被访问,基于LRU算法记录表项的最近访问时间,按照数据量选择能容纳替入表项的最小表项.4基于系统参数的阶段化处理优化为了提高数据流处理能力,RTMR在每个工作节点构造阶段化流水线(如图8),利用线程池减少每次处理时的初始化开销,并通过异步的传递数据消除Map和Reduce阶段间的同步.在阶段划分时,为减少阶段化的开销,将数据接收阶段和Map阶段合并,每个阶段由工作线程池、输入缓冲区和阶段内控制器组成,阶段之间通过控制器调整资源分配.在RTMR本地阶段化流水线中,Map和Reduce阶段各占用一部分工作线程,关键的共享资源是CPU.如何充分、有效地利用CPU(包括CPUCache)是提高数据流处理能力的关键问题.阶段化流水线可以通过阶段内的批调整和阶段间的线程池调整优化对CPU的利用.4.1阶段内控制批控制能提高阶段处理能力的原因在于:工作线程每次从缓冲区取一批数据的时间相当于取一条数据的时间,减少了访问缓冲区的次数;根据局部性原理,工作线程对一批数据的处理过程共享一个代码段和数据结构,可以有效提高CPUCache的命中率,从而提高阶段处理速度.在SEDA设计中,由于无法精确刻画批大小同阶段处理能力的关系,所以批控制一般基于启发式的反馈控制方式,通过观察系统参数调整批大小.现有的SEDA批控制方法通过观察阶段吞吐量调整批,当吞吐量降低时增大批,当吞吐量增大时减小批.为了说明这种方法的不足,给出如下定理.定理3.阶段吞吐量不能决定处理速度.证明.设某阶段在时间T内处理了n条数据,吞吐量观察周期为W,则阶段处理速度为吞吐量为根据排队稳态理论,当处理速度低于数据到达速度时,系统吞吐量由处理速度决定,此时吞吐量η提高(降低)说明处理速度μ提高(降低).当处理速度不低于数据到达速度时,系统吞吐量由数据到达速度决定.此时吞吐量的变化与数据处理速度无关.定理3说明以阶段吞吐量作为参数控制批大小会造成控制不准确,从而制约阶段处理能力提升.为了充分利用阶段内的优化机制(主要是CPUCache),应该以阶段处理速度作为控制参数,在没有达到最大处理能力之前,尤其是在数据处理速度跟上数据到达速度后,仍然可以增大批.当处理速度开Page8始下降时,则应该减小批.阶段内批调整算法如下.算法1.批控制算法.输入:数据到达速度λ输出:阶段处理速度μ1.λ=0,μ=0,β=1;2.每批处理结束后,计算数据到达速度λ、阶段处理速3.ifμ<μ4.elseifμ>μ∧μ<λ5.elseifμ>μ∧μ>λ6.λ=λ,μ=μ;7.转步2;在算法1中,首先初始化数据到达速度、阶段处理速度和批大小.之后在每批处理结束后,观察数据到达速度、阶段处理速度,若处理速度开始下降,则减小批;若处理速度增加但仍小于数据到达速度,则根据已接收数据增大批;若处理速度增加且已经跟上数据接收速率,则根据调整因子增大批,批的调整因子设为5%.4.2阶段间控制在阶段控制的基础上,阶段间控制器通过调整各阶段的工作线程以充分利用CPU、提高整体吞吐量.阶段间线程资源调整同样采用反馈控制方式,现有的SEDA方法通过观察阶段输入缓冲区内的数据规模调整线程池大小,当缓冲区数据超过阈值时增加线程,反之,减少线程.这种方法没有综合考虑系统CPU使用信息进行各阶段间的全局控制,造成CPU利用率的提高无法转化为全局性能优化.根据全局的CPU信息,造成CPU空闲(利用率低)的原因一是系统负载低,二是频繁发生读写文件等阻塞操作.区别这两种情况的方法是判断数据处理速度是否跟上数据到达速度,若处理速度不低于数据到达速度,则属于前者.反之,属于后者.阶段间的工作线程调整主要针对第二种情况.下面首先给出阶段过载的定义.定义5.若阶段的数据处理速度低于数据到达速度,则称此阶段过载.在阶段过载时,若CPU利用率不足,应该通过增加线程来提高CPU利用率,从而提高阶段处理速度.然而,当CPU利用率到达一定程度后,如果继续增加线程,会因为频繁的线程切换导致系统开销增加,处理速度降低.文献[7]指出,CPU利用率在75%以下时线程的切换开销成线性增长,而在75%以上时则成指数增长.因此,为了减少多线程的系统开销,将75%作为判断是否可以增加线程的标准.在一个阶段过载而另一个阶段非过载时,若CPU利用率小于75%,增加过载阶段的线程数,若CPU利用率大于75%,则移动非过载阶段的线程到过载阶段.如果两个阶段都过载,由于系统吞吐量是由Reduce阶段决定的,所以应该优先向Reduce阶段增加或移动线程.在确定了调整方法后,还应该确定调整目标,即系统何时达到吞吐量最大,下面定义了平衡系统的概念.定义6.Map和Reduce阶段的数据到达速度分别为λM,λR,处理速度分别为μM,μR,CPU利用率为u.若系统存在下列情况之一,则称系统是平衡的.(1)λRμR,λMμM.(2)u0.75,λM>μM,λR=μR.定理4.平衡系统的吞吐量最大.证明.RTMR阶段化流水线可以看作一个由数据流接收、Map和Reduce三个组件组成的串联系统,系统吞吐量由最小输出速度的组件决定.若λRμR并且λMμM,即Map阶段和Reduce阶段的处理速度都大于数据到达速度,此时的系统吞吐量由数据流速度决定.当CPU不低于75%时,若λM>μM并且λR=μR,即Map阶段的数据处理速度低于到达速度,输出速度(Reduce阶段的数据到达速度)与Reduce阶段的处理速度相等.此时由于CPU繁忙,阶段间控制不能再增加线程,只能通过移动线程进行调整.若移动Map阶段的工作线程到Reduce阶段,由于Map阶段的处理速度降低,造成λR降低,系统的吞吐量也就随之降低;若移动Reduce阶段的工作线程到Map阶段,虽然λR增加,但由于Reduce阶段处理能力下降造成系统吞吐量降低.实际上,此时的系统处于过载平衡状态.定理4表明阶段间线程调整的目标是达到平衡状态.阶段间工作线程调整算法如下.算法2.线程池调整算法.输入:Map和Reduce阶段的数据到达速度λM,λR,输出:Map和Reduce阶段的处理速度μM,μR1.每5秒观察Map和Reduce阶段的数据到达速度λM,λR,处理速度μM,μR以及全局CPU利用率u;2.ifλR=μR∧u0.753.elseifλRμR∧λMμMPage94.elseifλR>μR5.elseifλM>μM6.转步1.在算法2中,首先向过载阶段增加或移动工作线程,当两个阶段都过载时,优先考虑向Reduce阶段增加或移动工作线程,直至达到平衡状态.算法的调整周期设定为5s,这样能够保证阶段内的批控制方法能够有充足的时间适应线程池的变化.在系统达到平衡状态后,若系统非过载且CPU利用率小于75%,说明CPU没有被充分利用就已经能够跟上数据流速度,则线程太多只会增加开销;若CPU利用率高于75%,则会由于线程太多造成开销急剧增加;在系统过载情况下,CPU利用率肯定大于75%,此时也会由于线程太多导致开销增加.因此,在系统达到平衡状态后,无论系统是否过载,都可以试图通过减少线程降低系统开销.具体的调整方法为依次减少Map和Reduce阶段的线程直到阶段处理速度开始降低或重新达到不平衡状态.5评价本节以物联网环境下城市车辆实时数据处理应用中具有代表性的套牌车计算作为基准测试验证RTMR方法.5.1基准测试套牌车主要根据车辆时空矛盾来判定,针对每一条出现在特定监控地点的车辆实时数据,检索该车牌出现在所有其他监控点且在最大套牌时间阈值内的历史数据,如果二者的时间差小于该两点之间的套牌时间阈值,则认为该车辆有套牌嫌疑.根据前期物联网建设工作统计数据,若在某大型城市全面捕获车辆数据,数据流速度将会达到1MB/s(每条数据按200B计,约5000条/s),同时,在车辆数据保存一个月的情况下,历史数据的存储量将达到1TB.套牌车计算可以使用如下RTMR算法:针对某车牌号,Map阶段在所有车牌号的Hash表中找到其所在分组表项,Reduce阶段在B+树中找到其链表所在位置,依次与每条历史数据比较套牌时间阈值并更新链表.在某城市的车牌数量达到107的情况下,可以采用输出为20位二进制数的Hash函数进行分组,存储中间结果的Hash表共有220个表项,平均每个Hash表项存储107/220≈10个车牌的数据.因为在持久化历史数据中可能只有部分数据与计算相关,例如在套牌车计算中只有套牌阈值范围内的历史数据相关,所以以预处理的历史数据中间结果规模来衡量流处理架构处理能力.数据流处理集群搭建在2×6核2.0GHzCPU、32GB内存、250GB硬盘的服务器集群上;使用Oracle10g作为持久化存储,搭建在一台2×4核2.4GHzCPU,16GB内存服务器和20TBRAID5磁盘阵列上;网络连接采用1Gbps以太网光纤和交换机;在1台双核3.0GHzCPU、4GB内存服务器上使用LoadRunner9.0模拟数据流.为了测试数据流处理架构的伸缩能力,在集群节点上均匀划分历史数据区间,并在车辆数据流随机性和局部性特点基础上,模拟在集群节点上均匀分布的数据流,具体方法为:以十进制区间(0,107]内的数模拟车牌号,若存在n个节点,在各节点的历史数据划分区间中选取一个子集P1,P2,…,Pn,使得|P1|+|P2|+…+|Pn|=105,循环的为n个节点产生负载,对于节点i,在Pi中选取一个随机表项t,在区间(0,10)内选取一个随机数x,以220x+t作为该条模拟数据的车牌号,随机设定监控点,记录系统时间添加时间戳并控制数据流速度.本文在测试单个方法优化效果时,采用面向方面编程(Aspect-OrientedProgramming,AOP)思想,禁用其他优化方法,将测试代码插入测试目标方法,作为比较对象的方法使用同样的集群配置同时接收和处理数据流负载,分别计算性能指标.5.2中间结果存储性能分析首先验证RTMR对中间结果本地存储的优化效果.实验使用单个节点,数据流速度固定为1MB/s(每条数据200B,约5000条/s),中间结果数据规模为50GB,每项测试进行10次,每次10min,取平均值计算实验结果.表1显示了对中间结果本地存储进行优化前后的性能对比,可以看到,RTMR通过建立缓冲区消除读写同步,使中间结果的内存读写Page10性能提高了12.1%,RTMR通过利用缓冲区信息指导读写策略和替换算法,使外存读写性能和内外存命中率分别提高了15.5%和9.3%.综合3种方法,将中间结果本地读写性能(单位时间内读写次数)提升了近1/4.性能指标测试方法实验结果效果/%内存读写性能读写同步75385.2外存读写性能BigTable4425.5次/s内存命中率LRU整体读写性能改进前73901.4次/s在上述实验的基础上,保持数据规模不变,逐步提高数据流速度,测试RTMR方法外存读写性能和命中率的变化情况.由图9和图10可知,随着数据流速度的提高,由于缓冲区队列不断变大,能够为外存读写策略和替换算法提供更为充足的将要访问表项信息,因而读写性能和命中率不断提高.但当数据流速度提高到一定程度时,缓冲区数据规模的扩大将不会再引起读写性能和命中率的提高,反而因为检索开销加大,以及占用过多内存资源引发了额外的内外替换和读写操作,造成读写性能和命中率急剧下降.针对上述规律,在RTMR内外存结构、读写策略和替换算法的基础上,下一步的工作是对数据流速度、缓冲队列大小与读写性能、命中率的关系进行进一步分析,以指导缓冲区大小的动态变化以及负载丢弃策略.5.3阶段化流水线性能分析与5.2节实验类似,验证阶段化流水线优化效果的实验使用单个节点,数据流速度固定为1MB/s,中间结果数据规模为50GB,每项测试进行10次,每次10min,取平均值计算实验结果.表2显示了对本地阶段化流水线进行优化前后的性能对比.可以看到,由于RTMR利用阶段数据到达速度和处理速度进行控制,比利用吞吐量进行控制更能充分利用CPUCache,因此Reduce阶段处理速度提高了14.8%;由于RTMR根据CPU信息进行各阶段间的全局控制,更能将CPU利用率充分转化为性能提升,因此系统吞吐量提高了10.7%.综合两种方法,将阶段化流水线的整体性能(吞吐量)提升了近1/5.性能指标测试方法实验结果/(条·s-1)效果/%阶段处理速度SEDA7848.8吞吐量SEDA4804.8流水线性能SEDA4920.7在上述实验基础上,保持数据规模不变,分别以Reduce阶段处理速度和系统吞吐量为测试目标,采用变化的数据流比较RTMR与SEDA的批控制和线程池控制效果.实验进行10次,记录60s内阶段处理速度和系统吞吐量的变化,取平均值计算实验结果.SEDA控制方法的配置取实验最优值,RTMR线程池控制周期配置为与SEDA相同的500ms.图10和图11中黑线所示为实际数据流负载对应的阶段处理速度和吞吐量参考值.由图11可知,SEDA批控制方法在吞吐量降低的情况下增大批(阶段处理速度提高),在吞吐量增大的情况下减小批(阶段处理速度降低),造成在数据流速度提高的情况下需要较长的调整时间,而在数据流速度降低情况下衰减又太快,并且由于未能Page11充分利用CPUCache等阶段优化机制,制约了阶段处理速度.而RTMR方法通过观察阶段数据到达速度和处理速度持续增大批,能够在短时间内达到更高的阶段处理速度并在数据流速度降低的情况下缓慢衰减.由图12可知,SEDA方法根据缓冲数据规模变化调整线程池,需要较多次调整才能适应数据流,并且由于没有考虑整个系统的瓶颈,单个阶段线程池的调整无法转化为全局吞吐量提升,而RTMR方法根据CPU信息进行全局调整,使得CPU得到了充分有效利用,因而在调整稳定后吞吐量更高.5.4实时性分析在实时性分析实验中对比S4、HOP和RTMR3种处理架构.由于S4、HOP不支持预处理,每次处理数据流时都要重新进行历史数据加载和处理,大量无谓开销制约了系统吞吐量.因此,为了比较面向大规模数据的数据流处理能力,在基准测试的S4和HOP实现中加入了预处理逻辑,也就是先将相关历史数据进行一次流处理.各种数据流处理架构都搭建在两个节点上,数据流速度固定为1MB/s,每种数据规模测试10次,每次10min,取平均值计算实验结果.由图13和图14可知,当中间结果规模小于32GB时,因为一个节点的内存就可以容纳全部中间结果,所以HOP和S4也具有很高的吞吐量(大于4500条/s),但RTMR由于采用了本地阶段化流水线和内存读写优化,吞吐量更高;当中间结果规模超过32GB时,中间结果分布到两个节点的内存中,HOP和S4由于节点间的数据传输和同步开销增加造成吞吐量下降较快,而RTMR由于采用本地化技术避免了数据传输,吞吐量依然很高;当中间结果规模超过64GB时,由于已经超出了S4和HOP的中间结果缓存能力,其吞吐量趋于稳定,但错误率随数据规模增加而变大,而RTMR由于利用本地外存对中间结果缓存能力进行了扩展和优化,能够在降低错误率的同时(低于5%)保持较高的吞吐量(大于4300条/s).5.5可伸缩性分析在伸缩性分析中进行两组实验,分别测试RTMR针对历史数据规模和数据流速度的伸缩能力.在第1组实验中固定数据流速度为1MB/s,测试在节点数量增加情况下RTMR所能处理的历史Page12数据规模.由图15左y轴曲线可知,节点增加时RTMR处理能力的提升是近似线性的,这是由于RTMR通过划分历史数据中间结果和本地化处理,使节点之间不会产生制约并行吞吐量提升的数据传输和同步开销.而之所以未能达到线性伸缩,是因为在历史数据规模扩大的情况下本地存储文件读写开销增加.在第2组实验中固定每个节点中间结果数据规模为50GB,测试在节点数量增加情况下RTMR所能处理的数据流.由图15右y轴曲线可知,在数据流速度低于15MB/s的情况下,节点增加时RTMR处理能力的提升是近似线性的,在数据流速度超过15MB/s的情况下,节点增加时RTMR处理能力的提升变缓.这是因为随着数据流速度的提高,RTMR每个节点接收冗余数据流和进行Map阶段的CPU开销增加,同时RTMR方法的中间结果读写性能和替换命中率开始下降,所以影响了吞吐量.从利用RTMR方法解决车辆监管问题的经验来看,现阶段物联网环境下的数据流处理应用,受采集端带宽等因素影响,数据流速度远达不到15MB/s,只需占用节点多核CPU的很小一部分就可以完成接收和分组操作,并且RTMR方法能够有效提高中间结果的并发读写性能,在历史数据规模不断扩大的情况下,RTMR具有很好的伸缩性.6相关工作编程模型是连接特定问题与特定硬件实现的桥梁,并行编程模型更是建立大规模数据处理环境的重要基础.典型的并行编程模型有OpenMP、MPI、MapReduce和Dryad[12]等,其中OpenMP和MPI是抽象层次比较低的模型,需要程序员显式处理任务管理和数据管理等细节;Dryad侧重于构造一个完整的计算流程;而MapReduce则侧重于构造一个计算算子.对于可划分的大规模数据处理任务,MapRe-duce能够提供充分的并行计算语义,因而基于批处理方式的MapReduce得到了广泛应用,已有在多核系统、多核集群系统上的多种实现.然而,以批处理方式处理持续到达的数据流,无法满足实时需求.针对MapReduce的实时性改进已经成为研究热点,增量处理Percolator[13]、DryadInc[14]和迭代处理Twister[15]、Spark[16]等工作通过随机访问存储、缓存和复用中间结果提高了大规模数据处理的实时性.然而,上述方法仍属对静态数据增量的批处理方式.HOP[17]和S4[18]分别利用流水线和分布处理单元技术扩展了MapReduce的实时处理能力,但S4和HOP依然不是以面向大规模历史数据的数据流处理为目标的,表现在编程模型中缺乏对历史数据预处理和同步方法的支持,也没有充分利用中间结果缓存和本地阶段化处理等技术优化MapRe-duce的数据流处理能力.本文基于MapReduce模型提出了一种支持此类数据处理的方法RTMR,并对其中的关键技术瓶颈进行了改进.虽然S4和HOP都采用了流水线技术,但S4需要在节点间传输大量中间结果,HOP需要在Reduce阶段进行前完成多个Map节点结果的同步合并,都没有充分利用多核计算资源本地化MapReduce处理,实时处理能力受限于节点之间的数据传输和同步.Phoenix[6]和Metis[8]实现了多核架构上的MapReduce,但仍属于批处理方式.文献[7]提出的阶段化事件驱动架构SEDA能够利用线程池技术减少每次处理的初始化开销,并通过在阶段间异步传递数据消除阶段间的同步;还可以通过控制阶段内每次处理的批大小和阶段间的资源调整提高实时处理能力.本文基于SEDA构建了RTMR本地阶段化流水线.然而,现有的SEDA控制器[7]依赖于经验或实验方式,没有充分利用各种系统参数进行自动控制,往往花废大量时间仍不能达到最优的控制效果,造成CPU(包括CPUCache)无法得到有效利用,从而制约了数据流处理能力.系统控制方法可以分为准入控制和反馈控制两种,准入控制方法简单,但需要较多的人工参与并且控制策略固定、调整速度慢.现有的SEDA线程池控制方法就属于此类.反馈控制的优势在于自动调整能力,文献[19]通过PI控制适应性的调整线程池,以提高资源利用率、优化性能,文献[20]基于自动agent,通过附加调整参数对MIMO系统进行优化控制.本文以反馈控制方式改造了SEDA控制方法,与上述Page13工作相比,通过分析系统输入输出参数与控制参数的关系,确定控制目标和控制方式,使得对批大小和线程池的控制更为准确.此外,本工作在线程池控制中利用CPU使用信息进行各个阶段的全局控制,使CPU得到了充分有效利用,提高了数据流处理能力.数据流处理系统S4,迭代系统Twister、Spark和增量处理系统DryadInc将中间结果以对象或键值形式缓存在本地内存中,通过复用提高实时处理能力.然而,上述工作都没有着重解决中间结果内存结构的高并发读写性能问题.Metis采用HashB+树结构存储中间结果.与对象、Hash表和树等结构相比,HashB+树针对各种负载类型都有很高的读写性能.本文在HashB+树基础上,通过消除阶段化流水线中Map和Reduce线程间的读写同步进一步优化中间结果的并发读写性能.此外,上述工作的中间结果规模受制于内存容量,没有充分利用本地存储(包括外存)的数据存储能力,文献[21]在外存上建立了历史数据中间结果存储,但由于采用抽样方法丢失了部分历史数据,所以主要用于针对历史数据的聚集查询,不支持数据流同历史数据实时计算所需的随机查询.本文基于SSTable结构建立了键值类型历史数据中间结果的外存文件.然而,现有基于SSTable的文件读写策略[9]只是对写操作进行优化,读开销较大,应对频繁发生并且比例均衡的中间结果读取和写入操作,具有一定盲目性.另外,从提高内外存替换命中率角度,现有的LRU、clock等替换算法[11]没有充分利用阶段化流水线缓冲区中包含的将要访问的表项信息,制约了替换命中率.本文利用读写开销估算和缓冲区信息改造外存文件读写策略和内外存替换算法,进一步优化了中间结果的高并发读写性能.7结束语物联网环境下针对高速数据流的大规模数据处理难点在于伸缩性和实时性两方面的需求,本文提出了一种支持此类数据处理的RTMR方法,其创新性主要表现在:(1)通过中间结果分布缓存和本地阶段化流水线技术,改进了MapReudce的数据流实时处理能力.(2)根据系统参数控制本地阶段化流水线,使CPU得到了充分、有效利用,提高了实时处理能力.(3)通过改造内外存数据结构、读写策略和替换算法,优化了本地中间结果的高并发读写性能.基于实际物联网环境的基准测试表明RTMR方法能够保障大规模历史数据上数据流处理的实时性和可伸缩性,具有很高的应用价值.在RTMR方法中,单个节点处理能力的提升是集群可伸缩性的基础,因此本文主要解决本地优化问题.下一步工作主要针对RTMR可伸缩性保障中的关键问题———负载均衡,包括异构工作节点上的中间结果数据分布和动态负载调度.
