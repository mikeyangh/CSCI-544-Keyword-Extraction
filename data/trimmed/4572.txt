Page1面向复杂三维场景的高质量纹理映射姜翰青王博胜章国锋鲍虎军(浙江大学计算机辅助设计与图形学国家重点实验室杭州310058)摘要为三维模型创建纹理贴图是真实感图形学领域的一个重要课题,然而现今针对大数据的复杂结构的自然场景的纹理映射方法还很少.该文提出了一种新的多视图纹理映射方法,能够利用多角度拍摄的图像序列为三维模型进行高真实感的纹理贴图.相比现有的方法,该文的方法在针对模型中存在复杂遮挡的纹理映射以及大纹理数据的融合这两方面有着明显的优势.首先将多视图纹理映射转化为一个马尔可夫随机场问题,并用能量优化的方法获得模型每个面片最优的纹理图像对应关系;然后将映射的纹理数据进行精简化,并对部分不可见区域的无效纹理进行修复;最后,利用梯度融合的方法消除纹理拼接边界的颜色差异,最终获得无缝隙的纹理贴图效果.一系列复杂场景的实例证明了该文方法的有效性和鲁棒性.关键词多视图纹理映射;纹理映射最优化;无缝隙纹理融合1引言纹理映射是建立三维物体表面和二维图像空间Page2单幅或者少数几幅照片往往不能涵盖场景的完整信息,往往需要更多角度的图像数据才能够实现完整的纹理映射.本文的研究目标便是如何利用多角度的图像序列来实现高质量的三维模型纹理映射.近年来,随着三维重建技术的迅速发展[1],从多视图图像序列或者视频中重建高精度的自然场景的三维模型已经越来越成熟.然而,要实现对自然三维场景的逼真展示,仅恢复其三维几何模型是不够的,还需要恢复其表面的真实纹理.因此如何有效地为重建的三维模型贴上真实的纹理同样是一个值得关注的课题.虽然已有一些工作[2-3]尝试利用多视图图像序列来实现自然场景三维模型的纹理映射,并且有些方法[4-5]已经能够创建高真实感的纹理贴图,但是这些方法往往只针对于结构简单的三维物体模型,难以很好地处理复杂的大尺度自然场景数据.本文提出了一种纹理拼接优化的方法来实现三维模型的高真实感纹理映射.如图1所示,输入一段多视角拍摄并且每帧摄像机参数和深度已知的纹理图像序列,以及与该序列场景世界坐标系对齐的三维几何模型,我们的目标是为三维模型恢复完整的真实感纹理贴图.与文献[4-5]类似,我们的方法将纹理拼接优化问题转换为一个马尔可夫随机场问题,并用能量优化的方法对其进行求解,如图1(c)所示.然而,我们方法的不同之处在于考虑了遮挡和大尺度场景等问题的处理,因此能够很好地解决结构复杂的大尺度场景的纹理贴图,这是现有的方法难以做到的.我们首先对三维模型中的每个三角面片选择最佳的纹理图像,从而实现纹理映射的整体最优化;然后我们对纹理数据进行分块和有效区域裁剪,以最小化整体纹理数据;接下来,我们对纹理图像的颜色进行调整,从而消除来自不同纹理图像帧的纹理色差,实现三维模型的无缝隙纹理映射.我们方法的主要创新点如下:(1)现有的纹理映射方法对于自遮挡结构的纹理贴图存在多义性,而我们的方法能够利用深度信息来有效地处理自遮挡,因此适用于结构复杂的三维模型的纹理贴图;(2)由于内存的限制,现有的大多数方法仅能够处理结构简单的小规模三维模型,缺乏一种面向大尺度复杂三维场景的纹理映射方法,而我们的方法通过迭代求解来分片处理模型的纹理,因此适用于大尺度场景三维模型的纹理贴图应用.2相关工作视频的每帧图像仅仅能捕获到场景的一个局部区域,因此需要结合视频的多帧图像来创建三维模型的完整纹理贴图.Wang等人[2]的方法和Baumberg的方法用加权混合的方式结合多帧纹理图像,为每帧图像求解合适的权重以获得最佳的混合效果.然而,如果纹理图像之间存在偏移或者颜色差异,加权混合后会产生明显的走样.Niem和Broszio[6]的方法为每个三角面片选择其可见的某帧纹理图像进行映射,并用贪心的方式调整每个三角面片使得相邻的三角面片尽可能来自同一幅纹理图像,该方法的结果很难完全保证相邻三角面片的纹理一致性.与文献[6]不同,Lempitsky和Ivanov[4]提出将Page3三维模型的多视图纹理映射视为一个图像拼接问题:整体模型的纹理映射即为每个三角面片选择合适的纹理图像,并且保持相邻三角面片纹理接缝颜色的一致性.该问题可转换为一个马尔可夫随机场问题进行求解.Gal等人[7]对于文献[4]的方法做了改进,对于三角面片在每个帧纹理图像上的映射考虑了局部位移,从而能够处理摄像机参数和三维模型存在的误差偏移问题,但是求解空间扩大会影响该方法的运行效率.文献[4,7]提出的方法计算时间和空间复杂度均很高,因此仅能够处理形状简单的小规模模型.此外,上述这些方法均没有考虑三维模型的自遮挡情况.我们的方法与文献[4,7]类似,不同之处在于能够处理模型自遮挡导致的歧义性,并且能够处理大尺度复杂场景三维模型的纹理映射,这是上述方法所做不到的.除了文献[7]之外,近年来还有一些纹理映射方法也能够较好地处理误差偏移问题,从而实现高质量的真实感纹理贴图效果.例如,Eisemann等人[8]利用多帧纹理图像之间的光流来实现图像对齐以解决误差偏移问题,其运行效率依赖于GPU的加速.Goldluecke等人[5]则提出了超分辨率的纹理映射方法.Zhou和Koltun[9]提出了利用颜色一致性调整摄像机参数并且对图像帧进行形变来解决误差偏移问题.由于计算时间和空间上的复杂性,这些方法都难以处理大尺度复杂场景的纹理贴图.还有一些结合用户交互的方法[10-12]提供了半自动的工具允许用户在模型面片和图像区域之间指定匹配点,从而将纹理图像贴至三维模型表面上.与这些交互工具不同,我们的方法能够全自动地为三维模型的面片选取合适的纹理贴图区域,无需繁琐的用户交互.3算法目标和系统概述给定一段包含n帧的自然场景图像序列I={It|t=1,…,n}以及视频每帧对应的摄像机参数和深度图,同时输入该场景的三维几何模型M={Fi|i=1,…,m}(其中m为模型中三角面片的总数,Fi为第i个三角面片,其包含3个顶点{Vk顶点的位置与该序列场景世界坐标系对齐),我们的目标是为场景模型恢复完整的真实感纹理贴图以及纹理坐标数据.视频第t帧的摄像机参数表示为Ct={Kt,犚t,犜t},其中Kt为内参,犚t为旋转矩阵,犜t为平移向量,第t帧的深度图表示为Dt.我们分别用It(x)和Dt(x)=1/Zt(x)来表示第t帧的像素点x对应的颜色和视差(即深度的倒数).这里的三维模型既可以通过深度相机(如Kinect或ToF相机)直接扫描得到(如图2),也可以通过多视图立体方法(如文献[13]或PMVS2[14])来重建,甚至可以通过人工交互建模的方式得到(如图3的家具模型).采用多视图立体方法来重建,一般先恢复每帧的摄像机参数和深度图(在实验里,我们采Page4用ACTS软件①来恢复相机的参数和每帧的深度图像).而对于人工交互建模的模型,可能没有深度图,可以通过将三维模型在每帧图像对应的视角下进行深度绘制来得到每帧的深度图像.图4展示了系统的整体框架.我们首先用能量优化方法为三维模型中的每个三角面片选择最佳的纹理图像,从而实现纹理映射的整体最优化(见图4系统框架4纹理映射优化我们输入的纹理数据是多视角拍摄的图像序列,因此对于三维模型的每个三角面片,可映射的纹理图像不止一幅,我们所要做的是为每个三角面片选择一幅最优的纹理图像进行映射.换句话说,对于每个三角面片Fi,从输入图像序列I={It|t=1,…,n}中寻找一幅最优的纹理图像Il(i),其帧号l(i)∈{1,…,n}可视为关于i的标记函数.最优化的纹理映射结果即为所有三角面片最优纹理帧标记的集合L={l(i)|i=1,…,m}.由此,我们将纹理映射最优化问题转化为求解L标记问题.此标记问题有两方面的目标:(1)选择纹理映射信息最丰富的纹理图像;(2)保证来自不同纹理图像的相邻三角面片之间的纹理缝隙颜色一致性.类似于文献[4,7],我们可以将此标记问题视为一个马尔可夫随机场问题,并用能量最小化的方法进行求解.我们的能量方程表示如下:E(L)=∑其中:N(Fi)表示与Fi相邻的三角面片集合;数据项Ed衡量纹理图像Il(i)在三角面片Fi上映射的纹理信息丰富程度,平滑项Es衡量模型表面不同纹理图像缝隙的颜色一致性.第4节);然后,我们按照纹理映射结果对模型的三角面片进行分块,同时对纹理图像的有效区域进行裁剪,以此精简化模型整体的纹理数据(见第5节);接下来我们对纹理图像的颜色进行调整,从而消除来自不同纹理图像帧的纹理接缝色差,得到三维模型最终无缝隙的纹理图像和纹理坐标(见第6节).与文献[7]类似,我们从两方面来衡量纹理映射信息的丰富程度:(1)三角面片映射纹理图像的分辨率(可以用三角面片到纹理图像摄像机位置的距离来衡量);(2)纹理图像视线方向与三角面片法向一致性(正视角度的纹理信息最丰富).结合上述两方面标准,Ed可以按照以下公式定义:Ed(l(i))=-∑i,l(i))是关于顶点Vkd(Vk距离的函数,用来衡量三角面片映射纹理图像的分辨率,其定义如下:d(Vki,l(i))=烄z(Pl(i)(Vk烅0,烆其中,Pl(i)(Vk的坐标,pl(i)(Vkz(·)表示取三维点的z轴坐标.可以看到,z(Pl(i)(Vk越近,纹理映射的分辨率越高,d(Vk距离的计算来代替纹理分辨率的统计比文献[7]中提出的颜色梯度积分法更为简单直观,而且计算效率更高.此外,与文献[4,7]不同的是,我们对d(Vk①ACTS2.0:AutomaticCameraTrackingSystem.http://Page5的定义引入了遮挡的判断:如果z(Pl(i)(Vk知的投影位置pl(i)(Vk在该帧被遮挡,该帧的纹理图像对于Vkd(Vki,l(i))默认为0.在能量优化的过程中,为了使目标能量降至最低,优化器会避免选择无效的纹理信息.a(Vk线方向夹角的函数,其定义如下:a(Vki,l(i))=烄烅0,烆v^(Vki,l(i))=-犚l(i)犜l(i)-Vkni=∑F其中,〈·|·〉表示两个法向的点积,N(Fi)表示Fi的3个顶点的一环邻域所包含的三角面片集合.如果该帧摄像机视线方向与ni的夹角大于90°,则三角面片Fi在该视角下被遮挡不可见(通常处于三维模型的背面),该帧的纹理图像对于Vka(Vki,l(i))默认为0,同样可以避免无效纹理信息的影响.值得注意的是,这里我们利用三角面片邻域的表面法向平均值ni来替代该三角面片的真实法向i.平均法向在邻域范围内具有一致性,因而可以避n免表面法向不规整所产生的影响.Ed中将两类函数相乘,可以同时考虑到上述的两方面标准,从而有效地衡量纹理信息的丰富程度,来作为最优帧选择的参考.图5(d)展示了利用Ed获得的最优帧映射结果.相比于利用真实法向计算的结果,邻域平均法向获得的映射结果更为规整.Es统计相邻三角面片边缘的纹理颜色一致性,从而保证无缝隙的纹理拼接.其定义如下:Es(l(i),l(j))=[l(i)≠l(j)]r∑k=1Il(i)Il(j)(k-1)pl(j)(V1(其中(V1的采样点数.可以看到,仅当两个相邻三角面片映射自同一幅纹理图像时,Es为0,从而鼓励相邻的三角面片尽可能映射自相同的纹理图像,减少纹理缝隙的出现.对于处在纹理缝隙上的相邻三角面片,Es衡Page6量其公共边在各自纹理图像上的投影颜色差异(在公共边的投影位置均匀采样r个点进行颜色比较),从而保证纹理缝隙处的颜色尽可能一致,实现最大程度的无缝纹理拼接.我们利用GraphCuts算法[15]求解能量方程最小化.对于面片数目较多的大场景模型,输入纹理帧数过多会导致候选标记数目过多,从而导致内存过大.为解决此问题,我们在求解过程中采用了标记压缩法,对于每个三角面片仅考虑数据项最小的5个纹理帧,GraphCuts算法则在压缩后的纹理帧标记范围内求解,这样便可在内存允许的情况下获得近似的最优标记结果.最优化后的纹理映射标记结果如图5(e)所示,比图5(d)中的无平滑项约束的映射结果更为规整化.此外,如图6所示,由于函数d(·)和a(·)均考虑了遮挡的情况,因此我们的纹理映射最优化方法能够很好地处理三维模型中存在的复杂结构,这是文献[4,7]的方法难以做到的.纹理映射最优化计算的时间和空间瓶颈在于GraphCuts求解,其中,数据项的计算时间复杂度为O(mn),能量方程的求解根据文献[15]中的算法描述,其时间复杂度为O(m)(理论上为O(L2m),由于候选标记数L在我们实验中取常量5而省略),空间复杂度为O(m),因此该环节的总时间复杂度为O(mn),空间复杂度为O(m).5纹理数据精简化对于长序列的自然场景视频来说,纹理图像集合的数据量很大.例如,图7展示的“Dionysus”实例拍摄了雅典卫城的“Dionysus”歌剧院遗址内景,其中包含343帧图像的长序列实例,需要107MB大小的源纹理图像数据,这会导致真实感三维模型渲染时的纹理数据载入过程负担较大.幸运的是,根据第4节的纹理映射最优化算法获得的结果可以发现,三维模型并没有利用每帧纹理图像的全部数据,而是仅仅映射了一些局部区域,因此我们可以对纹理图像进行裁剪,只保留有效区域的数据,同时更新三角面片的纹理坐标,便能够在不影响纹理映射结果的前提下对纹理数据进行精简化.首先对三维模型上的三角面片进行分块,使得每一块包含所有相邻的并且纹理映射图像相同的三角面片.为此我们构造一个无向无权图G=〈中的每个顶点代表一个三角面片,相邻的并且映射自同一帧纹理图像的三角面片对.然后,我们寻找G中的所有连通分量1,…,q},每个连通分量接的整体,并且映射自同一幅纹理图像f(i).我们i包含的三角面片集合投影至相应的纹理图像将If(i)上,计算所有三角面片投影的包围盒区域Bi,将包围盒所在的纹理图像块If(i)(Bi)作为的有效数据单元,我们称之为“纹理块”,同时依据Bi的坐标位置重新计算所有三角面片的纹理坐标.所有连通分量对应的纹理块集合{If(i)(Bi)|i=1,…,q}即为精简的纹理数据.利用上述方法处理后,“Dionysus”实例的纹理数据精简至55MB.该环节的运行时间主要与模型面片数量有关,复杂度为O(m).5.1不可见纹理的修复由于输入纹理视频存在遮挡情况,三维模型的某些局部区域在纹理图像序列中是不可见的,无法获得这些区域的纹理映射信息.这些区域的纹理贴图结果会呈现出“空洞”,如图8(a)所示,这会对模型的真实感视觉效果产生很大的影响.因此,需要对Page7这些区域的纹理进行修复.我们注意到,不可见的模型区域往往是小片区域,通常不包含丰富的纹理结构信息.对于这些区域可以用简单的扩散填补方式进行修补.对于模型中某个不可见区域Ω,首先获得Ω中面片法向的平均值n;然后,将Ω邻域(Ω中三角面片顶点的一环邻域)的纹理颜色正交投影到与n垂直的平面上获得一张正投纹理图;接下来,迭代地扩散填补正投纹理图中空洞区域的边界像素点颜色,对于每一个空洞边界上的像素点p,利用以p为中心3×3窗口内已知像素点颜色的平均值对其进行填补,直到空洞区域的所有像素点都被填补完毕;最后将该纹理图作为新的纹理块加入模型的纹理块集合中,并计算Ω中三角面片的纹理坐标.扩散填补方法的修补结果如图8(b)所示.6纹理边界无缝隙融合由于光照变化、拍摄角度的不同、表面高光和阴影等多种因素的影响,模型的局部区域在多个纹理块图像中的颜色可能会不一致.尽管纹理映射优化过程中的平滑项尽可能地保持纹理边界的颜色一致性,然而映射到三维模型上的纹理边界不可避免地会存在一些色差缝隙,如图9(a)所示.因此,还需要对精简后的纹理块颜色进行调整以消除这些缝隙,即对纹理边界的颜色进行无缝隙融合.纹理边界融合需要满足3方面条件:(1)只是做微小的调整,调整后的纹理块颜色尽可能和源图像的颜色一致,不要有太大的变动;(2)调整后的纹理颜色梯度和源图像的颜色梯度尽可能一致;(3)调整后的纹理块在相邻的三角面片边界上尽可能保持颜色一致.类似于文献[16],上述条件下的纹理边界无缝隙融合问题可以通过求解一组带约束的泊松方程组来解决.该方程组求解纹理块集合{If(i)(Bi)|i=Page81,…,q}中包含的所有像素点的颜色,每个像素点的颜色对应一个变量(我们对RGB的3个通道独立求解).方程组包括以下3个部分方程式,分别对应上述3方面条件:(1)对于任意一个纹理块中的每个像素点x∈If(i)(Bi),我们有其中:If(i)(x)是待求解的x点颜色变量,约束其与源图像的颜色If(i)(x)接近,ws是一个权重值(在实验中取0.01,表示第1类条件仅起到很小的约束作用);(2)对于任意一个纹理块中的每个像素点x∈If(i)(Bi),我们有其中N(x)表示x的四邻域像素点集合(仅在包围盒Bi范围内的有效),这部分方程式约束x点颜色变量的拉普拉斯梯度和源图像保持一致;(3)任意一对在三维模型上相邻的纹理块If(i)(Bi)和If(j)(Bj)(相应的三角面片集合存在相邻关系),将If(i)(Bi)和If(j)(Bj)相邻纹理边界中的每一条边(V1并对二维投影边(pf(i)(V1pf(j)(V2们有ij))均匀采样r个点,对于每一对采样点我If(i)If(j)其中k∈{1,…,r},这部分方程式约束纹理边界两侧的颜色变量保持一致.由于以上方程组是过约束的稀疏线性方程组(方程数大于变量数),用共轭梯度法对其进行求解,并将求解获得的颜色写回相应的纹理块中,便可获得纹理边界无缝融合的效果,如图9(b)所示.6.1大数据纹理的迭代融合对于长序列的大数据纹理图像,稀疏线性方程组的规模会很大,求解方程容易造成内存溢出.例如,图1所示的“桌面”实例的方程组规模达到20649637×20649637之大.对于此类大纹理数据场景,无法一次性求解所有的纹理块,需要对纹理块集合进行分批求解,每次求解纹理块的像素点总数不超过10000000.为此,我们对纹理融合算法进行改进,采用一种迭代融合的策略.每次迭代选取若干个纹理块用上述方法进行融合求解,并将求解完成的纹理块标记为“已求解”.在下一次迭代过程中,用“已求解”的纹理块颜色作为除了上述3类约束条件之外的附加约束条件:如果待求解的纹理块If(i)(Bi)的某个相邻纹理块If(j)(Bj)标记为“已求解”,则我们将其相邻纹理边界中的每一条边(V1到两幅纹理图像If(i)和If(j)上,并在二维投影边(pf(i)(V1均匀采样r个点,对于每一对采样点我们有以下方程式:Page9If(i)((^If(j)其中^If(j)表示修改后的第f(j)帧的纹理颜色.如此便可约束纹理边界一侧带求解的颜色变量和另一侧已求解的颜色常值保持一致,以保证每次迭代求解的颜色结果可以融合到接下来的迭代运算中.我们按照广度优先顺序遍历选取纹理块作为每次迭代的求解对象,以保证下一次迭代选取的纹理块和当前迭代选取的纹理块有相邻,从而保证求解结果能够不断地传递扩散遍整个三维模型,保证整体模型纹理边界实现无缝隙融合,如图10(d)所示.迭代融合的算法伪代码如算法1所示.该算法的时间瓶颈在于线性方程组的共轭梯度求解环节,其时间复杂度为(O∑qi=1If(i)(Bi)为需要迭代融合的纹理块像素点数∑总和.该迭代融合算法能够在有限的内存下处理大规模的纹理数据,这是现有的算法难以做到的.算法1.纹理数据迭代融合算法.遍历纹理数据中的每一个纹理块If(i)(Bi):如果If(i)(Bi)未被标记为“已访问”:1.创建队列Q={If(i)(Bi)},并创建纹理块集合S=,初始为空,并将If(i)(Bi)标记为“已访问”;2.迭代执行以下操作:2.1.取出Q队首的纹理块If(h)(Bh):2.2.遍历每个与其相邻的纹理块If(n)(Bn)∈N(If(h)(Bh)):最后,我们用Zhou等人[17]提出的网格参数化方法(该算法已经在Direct3D9中实现)将融合后的所有纹理块信息展开至一幅完整的纹理图像上(如图10(e)所示),并重新计算三角面片的纹理坐标.7实验结果我们实验了几组视频数据及其三维模型.所有Page10实验都是在一台主频为3.20GHzCPU的台式机上运行的.对于图1的“桌面”实例,包含211帧分辨率为960×540的序列,纹理映射的时间需要14min,实际内存占用仅770MB.表1给出了本文主要实例的运行时间和内存占用情况.“桌面”、“仕女”、“Dionysus”三组实例以及图11和图12展示的两组实例的输入三维模型均是用多视图三维重建的方法[14]生成的.其中,“龙龟”实例的三维模型包含一些复杂的自遮挡结构,并且纹理视频颜色还包含非朗伯的光照变化,而我们的方法能够很好地处理这些问题并获得无缝隙的纹理贴图结果,如图11(e)表1本文所有实例运行时间和内存占用情况实例(分辨率960×540)桌面仕女Dionysus龙龟Dionysus22008结论和未来工作展望本文提出了一种三维模型的纹理映射方法,能够利用多视角拍摄图像序列实现高真实感的纹理贴图.我们将纹理拼接最优化问题转换为一个马尔可所示.图12展示的大规模场景模型的实例拍摄了“Dionysus”歌剧院遗址外景,我们的方法能够精确地还原场景模型的完整纹理贴图以及局部的纹理细节.上文图3展示的家具三维模型是通过手工建模获得的,跟实际家具的三维结构难免会有细小的出入,而我们的方法能够很好地处理三维模型的结构误差并获得真实感的纹理贴图效果.上文图2还展示了一组Kinect捕获的室内场景实例,我们的方法同样能够实现高质量的纹理映射.我们的方法对于各类复杂的自然场景模型均能够实现高质量的完整纹理映射,这是现有多视图纹理映射方法较难做到的.所需时间/min1451529夫随机场问题,并通过求解能量方程实现最优视图的纹理映射.我们的方法能够很好地处理自然场景的自遮挡等复杂结构的纹理贴图.对于不同视图纹理拼接边界产生的色差,我们提出了一种迭代融合的算法以消除颜色差异,能够实现大数据的场景的无缝纹理拼接.本文的实验数据输入的三维模型的坐标位置以Page11及在每帧纹理上的投影均较为精确,然而实际应用中可能存在三维模型无法精确对齐多个视角的纹理数据的情况,如何处理模型自身的误差使得纹理贴图的算法更为鲁棒是我们接下来将要解决的问题.如果输入的模型存在局部破损结构,现有的方法无法恢复这些破损区域的纹理,如何利用多视图的纹理信息来指导模型结构的修补,从而实现完整的纹理修复以满足实际工业应用要求,也是未来我们的研究方向.此外,我们在纹理图像的选择中仅考虑了图像的分辨率和视线方向,实际应用中光照条件等拍摄环境因素同样会影响视图中纹理的颜色和信息丰富度,物体本身的非朗伯材质导致的高光效果也会掩盖物体本身真实的纹理色彩,如何恢复环境光照模型以及物体表面材质系数,从而还原场景真实的纹理颜色信息,这些都是我们未来的研究方向.致谢审稿专家提出了宝贵意见,新加坡新科劲力有限公司的王英杰博士和SohLingMin博士对此工作给出了建议,杭州深粉象数字科技有限公司提供了家具模型和照片,在此一并致谢!
