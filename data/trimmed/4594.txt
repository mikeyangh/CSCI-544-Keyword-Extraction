Page1HiBase:一种基于分层式索引的高效HBase查询技术与系统1)(南京大学计算机软件新技术国家重点实验室南京210046)2)(中兴通讯股份有限公司南京210012)3)(江苏省软件新技术与产业化协同创新中心南京210046)4)(清华大学计算机科学与技术系北京100084)摘要大数据时代,众多应用领域的数据量爆炸式增长,迫切需要研究和寻找有效的大数据存储管理方法,提供实时或准实时的大数据查询分析能力.HadoopHBase系统为大数据的存储管理提供了一种具有高可扩展性的技术方法和系统平台.然而HBase只有主键索引,不支持非主键索引,这导致HBase的数据查询效率较低,难以满足数据实时或准实时查询需求.为此,在HBase基础上提供面向非主键的快速查询能力,是目前Hadoop环境下急需研究和解决的一个重要问题.该文研究提出了一种基于分层式HBase非主键索引的查询模型和方法,该模型和方法首先建立基于HBase的持久性索引.然后,为了利用内存提升查询性能,该文进一步提出了一种索引热点数据缓存技术和一种高效的热度累积缓存替换策略,以降低对HBase索引表的磁盘访问开销.热度累积缓存替换策略克服了最近最少使用(LRU)算法的局限性,考虑数据访问的累积热度和时间局部特性,从而更准确地捕获数据访问的特征.为了使索引热点数据缓存内存层具有良好的可扩展性,HiBase设计了基于一致性哈希的分布式内存缓存,支持高效的基于非主键的单点查询和范围查询.最终,该文设计实现了完整的分层式索引和查询系统HiBase.在千万至十亿条记录规模数据集上的测试结果表明,HiBase冷查询响应时间比标准HBase快65倍(大结果集)到3000多倍(小结果集);而引入基于查询热度累积算法的内存索引缓存方法后,热查询性能可在HiBase冷查询基础上再提升5~15倍,使得总体查询性能比标准HBase快300多倍(大结果集)到1.7万倍(小结果集),比开源的Hindex系统快5~20倍.关键词HBase;非主键索引;查询处理;分层式索引;缓存替换策略;大数据1引言继Google在2006年发表论文阐述BigTable[1]数据模型后,为了有效应对海量数据的存储与查询管理,人们越来越多地用NoSQL分布式数据存储系统替代关系数据库管理系统(RDBMS).在迫切的企业大数据应用需求推动下,目前出现了一些NoSQL非关系数据存储系统,例如,Apache社区的HBase①,Facebook的Cassandra[2],Amazon的Dynamo[3]以及支持高效数据查询的内存数据存储系统Redis②等等.这些数据存储都采用了key-value数据模型.在key-value数据存储系统中,HBase的使用最为广泛.HBase在主键上建立了类B+树索引,可以高效地支持基于主键的快速数据查询.然而,由于HBase缺少非主键索引能力,在面对以非主键为条件的查询请求时,需要对全表进行扫描,导致查询效率低下,难以满足需要快速响应的数据查询和统计分析场景.在传统关系数据库领域,面对这样的问题,通常会借助索引来解决.为了优化非主键查询的响应时间,降低查询代价,本文研究提出了一种基于HBase的非主键索引和查询模型与技术.随着内存性能的持续提升和价格的不断下降,内存在支持实时大数据处理的商业需求中扮演着越来越重要的角色.目前,用内存计算提升大数据处理性能已成为普遍公认的发展方向[4].在大数据查询分析过程中,数据访问通常具有一定的80/20分布特征:即20%的数据通常会被频繁访问,我们把这些数据称之为“热数据”;而80%的数据很少被访问,我们称之为“冷数据”.为了提高查询性能,低延迟、高带宽的内存被普遍用做热数据缓存来填补CPU和磁盘之间的性能鸿沟.为了优化HBase上非主键属性查询的性能,本文研究实现了基于HBase的分层式索引和查询系统HiBase(Hierarchical-indexedHBase).在索引存储模型上,HiBase分为两层:①基于HBase的持久化存储层.用来存储HBase用户表对应的所有非主键索引数据;②基于分布式内存的缓存层.为了进一步提高索引访问的速度,本文研究提出了一种基于分布式内存的索引热点数据缓存替换策略和技术,将部分索引热点数据缓存在内存中,大幅提高数据查询访问的效率.在HiBase中,我们进一步研究①②Page3实现了热度累积的缓存替换策略,它克服了最近最少使用算法(LRU)只关注数据最近一次访问时间、不关注数据访问频繁程度不同的局限性,考虑数据访问的累积热度,并对早前时间的热度累积进行指数衰减,从而更准确地捕获数据访问的特征.同时在缓存空间未满的时候,采用“访问即插入”的缓存插入策略,保证缓存空间得到充分利用,缓存命中率在数据加载阶段可以得到快速提升并趋于稳定.而当缓存充满以后,热度累积的缓存替换策略会根据记录的热度累积评分选择“牺牲者”淘汰出内存,选择获得热度高分的记录保存在缓存中.通过实验对比算法的命中率和系统的查询响应时间,验证了热度累积的缓存替换策略可以有效地优化非主键索引上的查询性能.本文第2节介绍相关工作;第3节给出HiBase的非主键索引及其存储模型;第4节提出HiBase索引热点数据缓存替换策略;第5节阐述HiBase的系统设计与实现;第6节对实验结果进行分析评估;第7节是总结与展望.2相关工作通常,检索HBase表中的数据有3种方式:指定单个行键查询、指定行键的范围查询以及扫描(Scan).HBase以字节数组的字典序对行键进行排序(这种存储格式更适合于记录的顺序读取,充分利用磁盘传输带宽),支持高效的指定行键的单点查询和指定行键范围的范围查询.扫描操作主要用于对非主键属性的查询,它允许指定扫描的数据范围,可以在查询过程中指定限定条件来过滤目标结果集.扫描操作允许对任意一个数据属性进行查询,因而比基于行键的查询更灵活,但是对非主键属性的扫描操作代价很大.基于行键检索的时间复杂性是O(logN),甚至如果使用Bloomfilter可以达到O(1),而扫描操作的时间复杂性是O(N).大数据应用的数据记录规模在千万至亿级以上,对全表进行非主键扫描的时间开销是不可接受的,这导致了HBase在很多实际应用中难以满足高实时性的查询需求.为此,一些研究工作致力于改善HBase非主键查询的性能.这些工作按照研究目标可以分为如下几个方面:(1)面向选择查询优化的非主键索引方法针对大数据上基于非主键属性的选择查询(包括单点查询和范围查询)需求,华为公司研发并开源了非主键索引查询系统Hindex①.它采用基于Region的局部索引模型,通过对HBase表的每个Region建立单独的索引表来提高非主键查询的效率.查询请求发送到每个Region服务器,然后通过Region上的索引表将结果集过滤并返回.Hindex查询需要访问所有的Region,由于绝大多数检索任务的目标记录数量相对较少,在分布式集群中并行地执行该任务往往造成很多未存储任何目标记录的计算节点也触发了检索过程,而最终却返回空集.在检索任务频繁的情况下,这一并行执行过程会耗费大量不必要的计算资源,最终将降低系统的并发查询吞吐量和系统的查询效率.NGDATA公司的HBase-indexer②也提供了HBase上的非主键索引.HBase-indexer将HBase的更新数据异步发送到索引服务器上,在索引服务器上分析数据并生成对应的索引数据.索引服务器会定期将索引数据推送到SolrCloud③服务集群上,查询则通过访问Solr服务来定位HBase上的内容.这种索引机制周期性地对索引进行异步更新,索引的时效性稍差,在面向实时应用时难以有效地满足需求.(2)面向范围查询优化的非主键索引方法IntervalIndex[5]是希腊Patras大学提出的基于HBase的非主键范围查询索引,它通过MapReduce构造SegmentTree[6]索引结构并保存在内存中,同时将索引树中存储的每段范围的上界保存在HBase表中,以支持有效的范围查询.IntervalIndex的可扩展性好,但是对单点查询,线段树退化成二叉树,索引的空间开销和维护代价都很大.中国科学院研究团队提出在分布式有序表上提供高性能、低空间开销和高可用的基于非主键的多维范围查询索引方案CCIndex,并分别在HBase和Cassandra上实现了原型系统[7-8].其主要思想是:充分利用数据的多个副本,在每个副本上分别建立基于不同非主键属性的聚簇索引,将非主键查询中大量的随机读转换成基于索引表主键的顺序扫描.CCIndex在多维非主键范围查询上取得了显著的性能提升,同时优化了索引的空间开销.Bloomfilter是Bloom在1970年提出的二进制向量数据结构,它具有很好的空间和时间效率,被用来检测一个元素是不是集合中的一个成员.Bloomfilter的局限性是它仅可以应用于单值查询(point①②③Page4query).在文献[9]中,微软提出了一种新的数据结构AdaptiveRangeFilter(ARF).从本质上来说,ARF是适用于范围查询的Bloomfilter,可以判定出一个集合中的全部元素都不在特定的域值空间内.ARF继承了Bloomfilter的优点:快速、节省空间,它可以为具有可排序特性的属性列建立索引,并可以探测一个属性列上是否有满足范围查询需求的记录,以此来优化范围查询的性能.(3)采用内存缓存的查询优化方法随着内存和固态硬盘性价比的持续提升,大数据上的缓存算法成为近几年的研究热点.TBF[10]在HBase数据存储上提出了基于固态硬盘的缓存替换策略.TBF算法结合CLOCK和Bloomfilter的优点,充分利用HBase数据存储行键有序的特点,有效地降低缓存替换算法的两个元数据(“数据记录是否被缓存”和“最近访问信息”)的空间开销.TBF算法的空间效率高,但它无法避免CLOCK算法固有的缺陷,不能量化数据最近被访问的频率,并且不能保存一个CLOCK周期以外的数据访问信息.高效、准确地区分冷热数据是缓存算法的基础.将频繁访问的热数据缓存,可以大幅度提高查询的性能.微软在文献[11]中提出了如何区分持续数据访问中的冷热数据的方法.通过指数平滑算法将数据记录的访问频率量化,并动态调整冷热数据的门限至最终收敛.由于针对的是离线日志的数据访问分析,所以微软在这篇文章里提出了回溯(backward)算法和并行回溯算法,回溯算法只适用于日志数据的离线分析,因为只有这样才可以高效地从新时间到旧时间逆向分析日志,使冷热门限提前收敛.本文研究提出了一种分层式非主键索引方案:采用全局索引模型的非主键持久化索引层,为索引机制提供良好的可扩展性和可靠性;内存热点索引数据缓存层通过热度累积的缓存替换策略为非主键查询提供实时、准确的热点数据缓存;同时,基于一致性哈希的分布式内存缓存结构为HiBase缓存层提供了良好的可扩展性和容错性.3分层式索引存储机制3.1非主键持久化索引存储模型为了避免对HBase非主键查询时的全表扫描,提供快速的非主键查询能力,HiBase为保存在HBase用户表中的非主键属性建立索引表,并将索引表保存在HBase中,借助HBase获得良好的可扩展性和容错性.每个HiBase索引表用来存储管理用户表中的某个待查询非主键属性的索引.我们为用户表中待建立索引的非主键属性定义如下格式的索引表主键:〈用户表索引列名(简短别名),用户表索引列值,用户表主键〉其中,用户表索引列名是用户表中被索引属性的名称.通过将列名映射到一个简短的别名,如“Age”映射到“a”,可减少索引表主键存储空间的开销.在索引表主键中存储用户表主键有两个作用:一是保证了索引表主键的唯一性;二是提供HBase用户表中被索引记录的地址,通过用户表主键,可快速获得用户表中被索引的记录.图1给出了一个HBase用户表示例以及以Age属性为索引主键的索引模型和索引表示例.该示例中,索引表的主键形如“a,12,Tom”,其中,a为Age属性的简短别名;12是用户表数据记录Tom的Age值,也就是索引列值;Tom是该记录在用户表中对应的主键.与关系数据库中的部分索引(partialindex)一样,HiBase以查询属性为索引表主键,索引表包含用户表的部分字段,通常只将查询中可能需要辅助性访问的字段存放在索引表的非主键属性中.这是为了方便对查询中需要访问的辅助性字段提供快读访问,避免再次访问用户表而导致的二次磁盘访问.本例中,索引表中的value部分所保存的如{Income:500},表示查询Age时可能需要访问Income列.以上描述了单个非主键属性索引的情形.在实际应用中存在多个非主键属性组合查询的需求,为此,类似于数据库中的多字段索引,需要构建多个非主键属性列的组合索引.对于多个非主键属性列的组合查询情况,HiBase会基于多个查询属性列建立索引.举例来说,当查询请求的条件中同时包含“Age”和“Income”时,我们建立以这两个非主键列为主键的索引表.索引表的主键构成形如“a,12,i,500,Tom”.其中a是列名“Age”的别名,i是列名“Income”的别名.在这里,我们使用逗号做分隔符(HiBase的分隔符是可配置的,对于整数、浮点数类Page5型和固定长度字符串类型,系统指定使用‘\0’作为分隔符.对于变长字符串类型,用户需自定义分隔符).在多个属性列上建立了索引后,组合查询就转换成基于索引表主键的查询,索引过程和查询过程与单属性查询基本相同.3.2分层式索引存储模型与索引缓存上述索引表将为HBase表实现索引数据的持久化存储.由于索引数据是存放在HBase中,每次查询访问HBase表会涉及到很多磁盘访问,我们进一步考虑把索引中那些访问频率高的索引数据作为热点数据缓存在内存中,形成基于HBase和分布式内存的分层式索引存储和查询机制,进一步提高索引的查询速度.HiBase的分层式索引存储模型如图2所示.HiBase中的内存热点索引数据缓存格式不同于持久化存储中的索引格式,内存缓存索引的主键格式为〈用户表索引列名(简短别名),用户表索引列值〉其中,用户表索引列名和用户表索引列值的含义与持久化索引存储层中的相同.内存索引构建的基本思路类似于倒排索引,内存索引缓存层中的每个索引主键对应着一个具有相同索引列值的索引记录集合,该集合包含了与该索引值对应的所有索引表数据记录.与持久化索引存储层一样,集合中也包含了可能需要访问的其他非主键属性.因此,完整的内存索引数据格式如下:索引主键:〈用户表索引列名(简短别名),用户索引集合:{〈用户表主键,{〈频繁访问列名,频图3是内存缓存层的索引结构示例,该例中缓存了Age属性下的两个年龄值“21”和“30”,分别对应于索引内存缓存层的主键“a,21”和“a,30”,其中,a是Age的简短别名.当以年龄为30进行查询时,根据索引内存缓存层主键“a,30”的哈希值找到对应索引在内存中的存储地址,可以得到该年龄值所对应的索引集合{〈Jerry,{Income:10000}〉,〈Ron,{Income:20000}〉}.由该集合对应的用户表主键到用户表中可获得相应的用户数据记录.在实现上索引内存缓存层数据存储在Redis内存数据库中,并由Redis自动完成上述哈希和快速查询过程,这一层全内存化的查询相当高效.分层式索引存储模型的查询过程是:首先到索引内存缓存层查询热点索引数据,若缓存中没有命中,则将查询转发到索引持久化存储层进行检索.可以看出,通过将索引热点数据缓存在内存中,部分查询可以直接在内存中命中结果集,从而降低了磁盘访问开销,提高整体查询性能,这对于具有倾斜的数据访问分布特性的应用来说尤为有效.3.3基于一致性哈希的分布式内存缓存HiBase利用HBase服务器节点上的分布式内存来存储管理所有的索引热点数据.我们引入了一致性哈希[12]来完成索引热点数据在分布式内存中的存储管理.一致性哈希是1997年由麻省理工学院的DavidKarger提出,目的是为了修正简单哈希算法在分布式环境中存在的节点更新带来的巨大开销等问题,使得哈希算法可以在分布式环境中真正得到应用.一致性哈希在许多分布式系统中得到了广泛的应用,如Dynamo[4]、Cassandra[3].一致性哈希为HiBase的索引内存缓存层提供了良好的可扩展性.索引内存缓存层的可扩展性是指当索引缓存层的内存使用率偏高时,通过加入新的服务节点即能够实现索引缓存层容量的动态增加.在动态变化的内存环境中,单调性是可扩展性的可靠保证[12].单调性是指如果已经有一些内容通过哈希方法分配到对应节点的缓存,此后又有新的节Page6点加入到系统中,那么哈希的结果应能够保证原有已分配的内容要么被映射到新节点的缓存中,要么仍然被映射到原先所在节点的缓存中,以尽量减少数据迁移带来的开销.例如,对于最简单的线性哈希:其中,N表示全部缓存区的个数,在本文的分层式索引系统中即表示索引缓存层的节点数量.倘若不使用一致性哈希管理分布式的索引缓存,由于服务进程的加入和退出而使得缓存节点个数N发生变化时,原来所有的哈希结果均会发生变化,意味着所有的映射关系需要在系统内全部更新.节点的加入和退出将带来极大的计算和传输开销.在HiBase分布式内存缓存中,使用一致性哈希来确定数据所在的服务器节点.一致性哈希的基本原理如下:使用某种哈希函数将所有数据映射到圆环边上的某一点上(如果使用32位地址空间,那么圆环上总共存在232个点).同时使用另一个相同或不同的哈希函数将每个存储节点映射到该圆环边上的伪随机分布点上.当查找数据所在的节点时,一致性哈希算法会从圆环上该数据的映射点开始,沿顺时钟方向进行搜索,找到的第1个存储节点即为数据所在的节点.在节点发生变化时(如节点失效或节点加入),只有和变化节点相邻的节点数据需要迁移,从而可减少节点的加入和退出带来的计算和数据传输的开销.例如,当图4(a)中存储节点2出现故障时,原先映射到该节点的数据会映射到顺时钟图4基于一致性哈希的分布式索引内存缓存存储机制方向遇到的第1个存储节点上,如图4(b)所示部分数据被重新映射到存储节点3.而增加存储节点的时候,数据映射关系的变化与上述的过程正好相反,新节点将会负责管理其哈希地址到它逆时针方向上第一个存储节点之间的所有数据.可以看出,当存储节点发生变化时,只需要迁移小部分的数据即可.而且通过将存储节点映射到圆环的伪随机分布点上,能够有效地保证各存储节点之间的负载均衡.HiBase的内存缓存需要通过两次哈希来找到对应索引记录的真实位置:第1次通过图4所示的一致性哈希找到索引数据所在的服务器节点;第2次则通过Redis的哈希机制找到节点内的索引数据地址.3.4支持基于内存缓存范围查询的值表HiBase在索引持久化存储层中存储值表,用来有序地记录和存储索引属性所有取值的集合,以支持高效的基于索引内存缓存的范围查询,如图5(a)所示.通常情况下,一个索引属性值经常会对应多条用户表记录.在持久化存储层的索引表中,HiBase在索引表主键中加入用户表主键以维护其唯一性(如3.1节所示).在值表中,我们只保存索引列值,因此值表的记录条数会比用户表小得多.用户对分层式索引存储系统进行范围查询时,值表会被频繁访问,因此值表会被底层文件系统缓存在内存中,可以大大提高值表的查询效率.这个缓存由文件系统管理.当索引列的基数(即索引列取值的实例个数)不大时,上述机制保证值表能有效地工作.当索引列取值很多或无限多(比如取值为范围变化大的整数、字Page7符串或浮点数)时,值表会带来大量的空间开销.因此我们提出粒度值表优化方案:将索引列取值进行粒度分段,每个分段范围内的多个索引列值对应一条值表记录,如图5(b)所示.HiBase将一条值表记录对应的索引列取值范围内的索引记录存储在同一个内存缓存索引集合(如3.2节所述的内存缓存索引结构)中.因此,采用粒度值表优化方案后,仍然是一条值表记录对应一个Redis集合.HiBase在处理范围查询时,首先会查询值表,得到落在查询范围内、满足查询条件的所有值表记录.然后以值表记录值为请求发起批量的单值查询,并汇总查询结果.如前所述,粒度值表优化方案中仍然是一条值表记录对应一个Redis集合,因此不影响范围查询过程.只是查询结果可能是Redis集合的部分子集,需要过滤筛选.依赖于Redis高效的查询机制,以及可配置的值表粒度,过滤筛选是非常高效的.详细的查询执行过程见5.3节数据查询过程中的范围查询.4索引热点数据缓存替换策略缓存是数据查询优化常用和行之有效的方法,缓存技术一直是数据存储和查询的研究热点,通过将部分数据保存在内存中,可以提高随后对数据重复访问的效率.通常缓存的容量远远小于保存全部数据的磁盘数据库的容量,所以当缓存满了之后需要选择合适的牺牲者淘汰出缓存,这就是缓存替换策略.HBase也有自己的缓存策略,即HBase的块缓存(BlockCache).HBase在读数据时会以块(Block)为单位进行缓存,用来提升读性能,块的默认大小是64KB.读请求先到写缓存(MemStore)中查询,查询失败则继续到块缓存中查询,再查询失败就会到磁盘上的HBase表中读取,同时会把读取的结果放入块缓存.在配置文件中会指定块缓存的容量,当块缓存的容量已经被使用85%以上的时候开始采用缓存替换策略淘汰数据,淘汰过程在块缓存容量占用比为75%的时候停止.然而,HBase的缓存策略是面向系统的读写性能提升的,而不是面向特定的数据表的.HBase采用的是最近最少使用(LeastRecentlyUsed)的缓存替换策略来淘汰块.LRU算法是基于如下假定:最近被访问的数据在近期最有可能被重复访问,而如果数据很长时间未被使用,则在最近的未来被访问的概率很低.LRU算法只关注数据项最近一次访问时间的差异,没有关注数据项访问的频繁程度不同,也就是说,LRU算法没有考虑数据的累积热度.4.1HiBase索引热点数据缓存基本思想在HiBase中,我们提出了热度累积的缓存替换策略.它基于与LRU算法相同的假设:最近被访问的数据在最近的未来最有可能被重复访问.其基本设计思想是周期性地累积缓存索引集合被访问的次数,并将访问次数周期性地累积成热度保存在缓存元数据中.进而,对所有记录的累积热度排序,选择累积热度TOP-K的索引记录缓存到内存中,这就是热度累积的缓存替换策略.图6给出了HiBase的分层式数据存储结构.用户表、索引表和值表全部存储在基于HBase的索引持久化存储层中.在内存缓存中,我们保存索引热点数据.索引缓存存储结构是基于Redis的集合(Set),RedisSet也是以〈key,value〉格式来组织数据.热点数据的索引主键(如3.2节描述)做RedisSet的key,而索引集合(如3.2节描述)作为RedisSet的value保存在内存缓存中.显然,具有相同索引列值的记录被绑定在同一个集合中,基于索引列值的查询命中是以集合为单位的.同时,它们也是热度累积的基本单位,每个集合都会累积它在一个计算周期内的访问次数.4.2热度累积的缓存替换策略在HiBase中,热度累积缓存替换策略的热度Page8计算公式如下:scoren=α×其中0α1.式中的countPeriod即热度计算周期,visitCount指当前热度计算周期中,该索引集合被访问的次数.历史热度scoren-1则反映集合累积的历史热度.参数α是衰减系数,用来确定当前周期累积的热度和历史热度在scoren中各自所占的权重.α越大,则最近的访问在数据访问热度中所占的权重越大,历史访问记录对数据热度的影响越小,反之亦然.集合的历史热度在本计算周期内以系数(1-α)的速率衰减,经过多次迭代,更早计算周期的累积热度经过了更多次衰减,所以早期的累积热度对数据热度的影响不断降低.为了降低热度计算带来的计算和更新开销,在执行查询请求时,内存缓存的服务进程将会对访问到的每条索引数据记录本周期内的访问次数,此时并不对内存缓存的数据进行替换.直到查询请求次数达到countPeriod,即到达热度计算周期时,服务进程触发缓存的更新替换.按照热度累积公式对所有的记录计算热度,根据热度排序,将热度排序TOP-K的集合记录缓存到内存中,集合中包含的记录条数是不固定的,所以选择TOP-K时,根据缓存空间能够容纳的记录条数限制计算出热度门限,高于门限的集合被缓存到内存中.然而,在系统初始阶段,缓存是大量空闲的.LRU算法在系统初始阶段的命中率提升很快,这是由于LRU算法中数据记录是访问即插入缓存,最长时间没有被访问的数据记录会在缓存充满后被淘汰.所以LRU可以快速地进入稳定状态.而热度累积的访问如果在系统初始阶段通过周期性地计算热度,等被访问数据记录的热度累积到门限时才可以插入缓存的话,那么初始预热阶段的缓存利用率低.所以我们的热度累积算法在缓存空闲阶段做了优化,只要缓存有空闲,我们就采用“访问即插入”的策略,将所有访问到的记录都插入缓存.而当缓存充满以后,热度累积的缓存替换策略根据记录的热度累积评分选择“牺牲者”淘汰出内存,选择获得热度高分的记录保存在缓存中.热度累积的缓存替换策略不仅考虑了数据的访问时间远近,同时考虑了数据的访问频率,所以比LRU更准确.从实验结果看出,热度累积的缓存替换策略明显优于LRU算法,和不使用内存缓存策略相比,可以提升5~15倍的查询性能.5HiBase的系统设计与实现5.1系统总体架构基于以上的非主键索引模型和技术方法,我们设计并实现了一个基于HBase的分层式非主键索引查询系统HiBase,该系统实现了基于HBase的持久化索引存储,基于内存的索引热点数据缓存,以及热度累积的缓存替换策略.HiBase提供了基于分层式非主键索引的数据查询方法,并对范围查询进行了并行化优化.HiBase将整个分层式索引存储系统划分为以下几个模块,系统功能模块划分如图7所示.(1)索引构建管理模块.管理索引的元数据(记录用户表对应的索引表名称、索引列等信息),并实现针对HBase的流式数据和静态数据两种不同特性数据的索引构建方法,包括支持索引表和值表的插入、删除、更新操作.(2)持久化存储管理模块.提供索引表和值表的持久化存储,HBase为持久化存储数据提供可扩展性和容错性.(3)索引内存缓存模块.管理索引热点数据的缓存存储、更新和地址映射,实现热度累积缓存替换策略,使得最近频繁访问的数据能缓存到内存中.(4)查询执行引擎.将用户的查询请求翻译成系统识别的命令,调用对应的方法执行查询,并将查Page9询结果汇总返回给客户端.为了提供索引内存缓存的高可用性,HiBase使用了Hadoop环境中的分布式协调管理系统ZooKeeper来可靠地检测分布式内存节点上服务进程的存活状态.索引内存缓存层每个内存节点服务进程会分别建立与ZooKeeper的会话,并创建临时Znode来表示自身的存活状态.每个内存节点服务进程从ZooKeeper系统镜像中可以观察到其他节点进程的存活状态.系统通过对分布内存节点状态的监测实现内存节点的失效检测和上线处理,从而实现索引内存缓存层的高可用性.5.2索引构建过程根据数据输入方式的不同,索引构建可分为面向流式数据和面向批处理数据的索引构建.索引构建过程都是读取用户表的一条记录,在非主键属性上生成一条索引记录,如果满足缓存条件,同时生成内存缓存层的索引数据.最后将索引数据分别更新到持久化存储层与内存缓存层,并更新值表.对于流式数据索引构建方法,HiBase利用Observer类型的Coprocessor来构建相关的索引.具体来说是使用HBase提供的RegionObserver接口的回调函数prePut,插入一条数据之前会被触发调用.prePut方法首先根据索引信息对用户发起的Put操作进行分析,如果Put操作的数据包含有索引列,即包含要索引的数据,则触发索引数据的插入.由于静态数据一般相对较大,为了能够加快静态数据索引的构建速度,本文利用HadoopMapReduce来并行化执行静态数据索引构建.MapReduce任务首先得到输入〈Row,Result〉,其中Row为用户表的行键,Result是通过Row获得的HBase记录.然后根据索引信息生成其对应的索引数据,并将索引数据插入到分层式索引中.整个过程不需要Reduce阶段即可完成,同时由于HBase用户表记录之间是相互独立的,所以可以充分利用MapReduce提供的并行化处理能力来加速索引构建过程.5.3数据查询过程(1)单值查询HiBase通过建立非主键属性上的索引,支持高效的非主键列上的单值查询(pointquery)和范围查询(rangequery).本文所说的单值查询,即查询请求中条件属性被限定取值唯一的查询.例如,对于图1中给出的实例,查询年龄为30岁的所有记录.对于单值查询,客户端的基本流程如下:①从配置文件获取ZooKeeper的地址,建立与ZooKeeper的连接,获取注册的所有服务进程,确定当前提供内存缓存的所有服务进程的位置信息.②向内存缓存层的服务进程发起查询请求.若内存缓存层命中,则返回内存缓存层给出的结果,结束查询.③若内存缓存层未命中,则向HBase的索引表发起查询请求.获取结果后,返回查询得到的结果,结束查询.可以看出,如果在内存缓存层命中,整个查询流程都不会访问到磁盘,减少了磁盘访问开销,能够大幅度提高响应速度.另外,对ZooKeeper访问获取的内存缓存层服务进程信息会被缓存在客户端,在后续的访问中会进一步减少数据查询的通信开销.(2)范围查询本文所说的范围查询,即查询请求中条件属性的取值是范围的查询.例如,对于图1中给出的示例,查询年龄在15~35岁的所有记录.对于范围查询,内存缓存层通过一致性哈希的方法将数据分布到各个存储节点上来提高查询性能,而哈希破坏了索引列的有序性,所以我们需要记录索引表主键的所有取值,保存在值表中.通过该值表,我们可以获得索引主键某个范围内所有存在的列值.对于范围查询,客户端的基本流程如下:①从配置文件获取ZooKeeper的地址,建立与ZooKeeper的连接,获取注册的所有服务进程,确定当前提供内存缓存的所有服务进程位置信息.②从HBase中的值表获取查询范围内所有的查询列值.③依次发起单值查询请求.将查询结果汇总并返回.为了进一步提升查询效率,HiBase对范围查询进行如下的优化和改进:(i)如果范围查询中的某些索引主键列值是由同一节点管理的,则将这些列值汇总,然后向该节点发起一次批量查询请求;(ii)向范围查询涉及到的多个节点并发地发起查询请求;(iii)当索引缓存层的数据未命中时,节点上的内存索引服务进程并不返回未命中标志,而是将查询请求转发给基于HBase的持久存储层,并将持久存储层的查询结果返回给客户端.优化的范围查询具体流程如下:①从配置文件获取ZooKeeper的地址,建立与ZooKeeper的连接,获取注册的所有服务进程,确定当前提供内存缓存的所有服务进程位置信息.Page10②根据范围查询的条件,客户端从HBase值表中获取范围之间存在的所有索引列值.③对于所有存在的索引列值,根据一致性哈希算法计算出存储节点地址,从而将所有存在的索引列值与相关的节点地址一一对应起来.④并发地对相关节点发起查询请求,其中,对同一节点发起的多个查询请求将会合并成一个批量请求.⑤各节点上的内存索引服务进程对查询请求进行响应,如果查询的内容在内存中,则直接返回内存中的数据;否则,服务进程将发起对持久存储层的查询,并返回查询结果.⑥客户端汇总从各服务节点返回的查询结果.下面给出一个范围查询的实例:查询年龄在15~35岁的所有记录,来完整地描述整个基于分层式索引的范围查询流程,查询过程如图8所示.①图8(a)是客户端根据查询条件向值表发起查询的过程.首先客户端发起值表查询,HBase返回值表中存在的满足查询条件的3个索引列值,表明用户表的年龄属性列中在15~35之间的值有21与30.②客户端根据一致性哈希算法计算返回的索引列值与索引缓存层中节点之间的映射关系,如图8(b)所示,21映射到内存索引服务进程存储2上,30映射到内存索引服务进程存储3上.③客户端并发地向服务进程存储2与存储3发起查询.如图8(c)所示,客户端同时向存储2与存储3发起了对21与30的查询.在存储3中,内存缓存直接命中“a,30”,于是直接返回结果.而在存储2中,内存缓存并未存储索引记录“a,21”,此时存储2将此查询请求转发到基于HBase的持久存储层,最终将HBase的结果返回给客户端.④最后,客户端汇总来自存储2与存储3的查询结果,完成此次范围查询的流程.与单值查询一样,缓存命中率的提高会降低范围查询的响应时间.范围查询中对值表的访问是HiBase为哈希索引付出的代价,但是由于每次范围查询只需要访问一次值表,其访问代价占整个范围查询的比重是较小的.6实验与性能分析评估6.1实验环境与数据集我们从数据查询性能、数据扩展性、集群规模扩展性分别测试了HiBase的性能.我们的集群测试环境共有10个节点,集群的节点配置参见表1.CPUMemoryDiskNetworkOSJVMVersionHadoopVersionHBaseVersionZooKeeperVersion实验采用的数据集包括布朗大学的大数据基准测试和国内电信运营商的电话用户上网记录数据.首先,我们用电话用户上网记录数据测试不同系统的查询响应时间,该数据集共1072868115条记录,每条数据记录不等长,平均约是200个字节,Page1110个属性.查询需求包括下面4个:①根据用户号码MDN查询某一个时间段内的详单列表.②根据IP查询某个时间段内使用该IP进行上网的用户列表.③根据IP+PORT查询某个时间段内使用该IP+PORT进行上网的用户列表.④根据URL查询某个时间段内登录该URL的用户列表.以上查询需求都是时间段上的范围查询,其中,需求1是在用户表主键上查询,不用建立索引;需求2和4是单个属性上的非主键查询,建立查询属性上的索引;需求3是两个属性上的组合查询,建立组合索引.针对以上查询需求,我们一共设计了10个测试用例,每个查询测试用例分别测试缓存未命中的冷查询响应时间和缓存命中的热查询响应时间.然后,我们再进行布朗大学的大数据基准测试实验.布朗大学的大数据基准测试①是布朗大学在文献[13]中为了对比关系数据库和Hadoop在大数据上的查询性能而提出的基准测试.我们用该基准测试在4个节点上对数据集进行10000次符合齐夫分布的持续查询(包括点查询和范围查询),测试在不同的数据缓存比例下缓存的命中率和查询响应时间.6.2实验结果及其分析(1)非主键索引查询性能对比实验与结果分析我们用10亿条电话用户上网记录数据对HiBase实现的非主键索引的性能进行对比测试.对比如下3种情况:①不带任何非主键索引的标准HBase表上进行扫描的查询性能;②华为公司的开源HBase非主键索引系统Hindex的查询性能;③HiBase基于热度累积缓存算法的非主键索引的查询性能.图9给出了4个测试用例的查询响应时间性能对比.图9中的横坐标分别是查询获得的结果集大小,纵坐标是查询响应时间,由于HBase和HiBase的查询响应时间数量级相差太大,无法用等比例坐标,在这里使用指数指标.实验结果可以看出,HiBase的查询响应时间大大优于无非主键索引的标准HBase和开源的Hindex非主键索引系统的查询性能.对于结果集很小(极端情况下,结果集大小为0)的查询,HiBase和无非主键索引的标准HBase的扫描方法相比,冷查询的性能提升达到116912倍,热查询的性能提升可以达到131732倍.对于结果集较小(结果集为1155)的查询,冷查询的性能提升达到3082倍,热查询的性能提升达到17751倍.对于结果集较大(结果集为97515)的查询,冷查询的性能提升是65倍,热查询的性能提升是343倍.和开源的Hindex非主键索引系统相比,HiBase热查询的性能提升可以达到5~20倍.从HiBase和标准HBase性能对比来看,结果集小的查询性能提升效果更明显,这是因为对于标准HBase,不论结果集大小,非主键属性上的查询都需要遍历全部数据,而HiBase上的非主键索引可以直接定位到记录,查询响应时间随着结果集的大小而增长.(2)缓存命中率查询性能对比实验与结果分析缓存命中率是指在批量查询中,在缓存中获得结果集的数据查询个数占全部数据查询个数的比率.提高缓存命中率会减少访问磁盘的次数,有效降低查询响应时间,因此命中率是缓存算法最重要的性能评价指标.命中率越高,缓存系统的查询性能越好.我们用布朗大学的大数据基准测试对HiBase进行10000次符合齐夫分布的持续查询来测试缓存性能,数据规模为10000000条记录.图10给出了HiBase的热度累积缓存算法和LRU算法的命中率对比,图11给出了查询响应时间对比.图10热度累积缓存算法和LRU的命中率对比①BenchmarkofBrownUniversity.http://database.cs.Page12从图10中可以看出,缓存空间增加,命中率提高.热度累积缓存算法的命中率高于LRU,尤其是在数据缓存比例较低的情况下.如数据缓存比例为0.2时,热度累积缓存算法的命中率大约比LRU的命中率高16%,因为热度累积缓存算法的热度累积机制能够更精确地记录数据的冷热程度,将热点数据缓存到内存中.对于大数据应用,缓存空间受内存限制,热度累积缓存算法在缓存比例不高的时候正好可以充分发挥优势,提高大数据下的查询性能.HiBase是为了实现大数据上的高效非主键索引而实现的系统,我们在HiBase上做了无内存缓存、LRU缓存替换策略、热度累积的缓存替换策略的10000次符合齐夫分布的持续查询,并记录查询响应时间,如图11所示.可以看出,随着缓存空间的增大,命中率不断提升,使用两种替换策略的索引查询时间都在随之降低.由于热度累积的缓存替换策略在命中率上的优势,查询响应时间明显优于使用LRU替换策略的查询性能,提升幅度从缓存比例为0.2时的30%到缓存比例为0.8时的67%.(3)冷热查询对比实验与结果分析我们在10亿条电话用户上网记录数据上做了10个测试用例的冷热查询性能对比实验.每个测试用例测试了第一次冷查询和随后的缓存命中热查询的查询响应时间.性能对比实验结果如图12所示,图12电话用户上网记录数据查询响应时间这里我们的查询响应时间是单次查询的性能指标,横坐标是查询得到的结果集大小(结果集包含的记录条数).可以看出,每个用例的第1次冷查询的时间都是最长的,第2次热查询由于缓存命中,所以查询响应时间缩短5~15倍(注:图12中的查询响应时间是指数坐标).(4)扩展性实验与结果分析我们通过布朗大学的大数据基准测试验证了HiBase在不同数据规模下的查询性能,如图13所示.可以看到,部署在10个节点上的HiBase在数据规模从10000000扩展到50000000条数据记录时,查询时间保持线性增长,这验证了HiBase在数据可扩展性方面有着良好的表现.查询响应时间和结果集大小成正比,因为在查询响应时间中,主要开销是对查询结果集的访问.在数据集中,数据是符合均匀分布的,随着数据行总数的增长,查询结果集也随之增加,因此查询响应时间会与数据行总数的大小成正比.同时,我们在20000000条数据记录上进行了10000次范围长度为10的范围查询可扩展性测试,如图14所示.本实验不使用单值查询的原因是:在进行单值查询时,没有并发的过程,所以节点的变化几乎不影响查询时间.可以看出,随着节点数量的增Page13加,查询响应时间逐渐降低.在范围查询中,查询会根据范围内存在的索引列值向各节点发送查询请求,查询被并行执行后汇总结果,所以查询响应时间会随着节点的增加而降低.另一方面,索引列值是通过一致性哈希算法映射到各个节点上的,我们的测试实例范围固定为10,所以当节点个数增加到一定程度之后,查询响应时间趋于平缓.(5)索引构建的时间和空间开销分析HiBase非主键索引系统是在数据导入阶段创建索引,我们在电话用户上网记录数据上测试HiBase系统的数据导入时间来衡量系统的索引构建时间开销.同时我们测试了HBase的数据导入时间作为对比.数据导入时间的对比实验结果如表2所示.数据规模(记录行数)65013111072868115在电话用户上网记录实验数据集上我们分别做1.4GB部分数据集(6501311条记录)和225GB全部数据集(1072868115条记录)的数据导入时间对比测试.可以看出,HiBase的数据导入时间分别是HBase的1.03和1.12倍,因为HiBase在将用户表插入到HBase的同时,触发HBaseCoprocessor并行地构建索引记录并插入到索引表中.实验结果表明,HiBase为建立索引所带来的时间开销是完全可以接受的.空间开销上,索引数据与原始用户表数据的占比不是绝对固定的,而是依赖于用户表的结构以及索引属性列值的占用空间.在本文的电话用户测试数据中,225GB全部数据集的用户表和索引表空间开销总和为575GB,其中225GB文本数据导入到HBase后的用户表空间开销为238GB.针对6.1节中提到的查询需求1~4(其中查询需求1是用户表主键查询),我们在HiBase中为查询需求2~4建立非主键索引.索引的空间开销总和为337GB,单个索引的平均开销约为112.3GB,故单个非主键索引表的空间开销约为用户表的47%.对于采用廉价的分布式存储、具有优异的存储空间可扩展性的Hadoop和HBase系统来说,构建一个十多个节点的Hadoop集群即可获得数十TB的存储容量.因此,通过为每个非主键索引提供约一半用户表的空间开销而达到数十至数百倍以上的查询性能提升是完全值得的.7总结和展望为了提高HBase上非主键查询的效率,本文提出了一种基于HBase的分层式非主键索引存储模型,并设计实现了分层式索引查询系统HiBase.HiBase在HBase上持久化存储用户表和索引表,并将索引表的热点数据缓存在内存中.同时我们研究提出并实现了一种基于热度累积的缓存替换策略,通过高效的缓存替换策略,HiBase获得了优于LRU的缓存命中率,进一步提升了HBase上非主键查询的性能.下一步,我们会针对非主键属性上的聚集查询和连接查询进行研究并提出高效的解决方案.同时,引入SSD尝试多层热点缓存存储方案.另外,在HiBase中为了支持基于内存缓存的范围查询我们引入值表,下一步我们会针对值表做自适应粒度的存储优化,保证值表中的记录对应的集合大小基本均衡,从而提供性能稳定的范围查询.
