Page1SimHPC:一种基于执行驱动的大规模并行系统模拟器刘轶1)支予哲1)张昕2)李鹤2)焦林2)张鹏2)苏阳明1)倪泽辉1)钱德沛1)1)(北京航空航天大学中德联合软件研究所北京100191)2)(西安交通大学计算机系西安710049)摘要模拟实验方法对高性能计算机系统的性能评价和优化设计有着重要的意义,然而由于目标系统规模庞大,传统的体系结构模拟器难以满足模拟性能方面的要求.文中提出了一种专门用于高性能计算系统的模拟器———SimHPC,该模拟器采用执行驱动的全系统模拟方法,支持操作系统和应用程序的模拟运行.通过采用与目标系统同构的节点作为宿主节点以及并行模拟的方法,使得模拟性能相比传统的体系结构模拟器大幅提高,与现有的几种大规模并行系统模拟器相比,SimHPC在通用性和模拟性能方面也具有一定的优势.关键词模拟器;高性能计算;并行系统;执行驱动;性能评价1引言高性能计算系统以超高的计算性能和数据处理能力广泛应用于气象、航空航天、材料科学、新药研制等国民经济各个领域,已成为体现一个国家综合实力的象征.高性能计算机结构复杂、规模庞大且造价高昂,其实际性能受到处理器、节点结构、互连网络等多方面因素的影响,在系统设计阶段,通过模拟实验的方法对系统方案及其各种参数进行性能评价,对于优化和改进系统设计有着重要的意义.然而由于高性能计算机系统规模庞大,传统的体系结构模拟器难以满足模拟性能方面的要求.本文提出了一种专门用于高性能计算系统的模拟器———SimHPC,该模拟器采用执行驱动的全系统模拟方法,支持操作系统和应用程序的模拟运行.通过采用与目标系统同构的节点作为宿主节点以及并行模拟的方法,模拟性能相比传统的体系结构模拟器大幅提高,与现有的几种大规模并行系统模拟器相比,SimHPC在通用性和模拟性能方面也具有一定的优势.本文首先讨论模拟器的设计思路,提出保证模拟性能的方法,之后分析基于同构节点的执行驱动模拟实现方法、应用进程时间轴模型和建立方法以及互连网络和存储系统模拟方法,在此基础上,给出了模拟器的系统结构,最后对系统的模拟精度和性能进行测试,给出针对不同规模目标系统的模拟实验结果并进行分析讨论.2设计思路2.1模拟方法选择目前的体系结构模拟方法主要有踪迹驱动(trace-driven)、执行驱动(execution-driven)和数学建模分析等.踪迹驱动模拟方法首先获取程序的访存地址序列、执行指令序列等执行踪迹数据,将其作为模拟器的输入,通过模拟目标系统的功能和行为,获取程序在目标系统中的性能数据,此种方法常用于cache等部件级模拟;数学建模分析方法则通过数学模型描述目标系统特征,通过变换参数获取性能数据,此种方法较为简单,但误差往往较大;执行驱动模拟方法则是建立目标系统的功能和性能模型,并通过应用程序的执行获取性能数据,此种方法可以模拟应用程序在目标系统中的执行,获得的性能数据相对误差较小,且可用做前期的软件开发调试平台,在体系结构模拟中占有重要地位.由于执行驱动模拟方法可以模拟程序在目标系统中的实际执行,近年来进一步发展出全系统模拟(full-systemsimulation),它能够模拟操作系统和应用程序在目标系统中的执行.评价高性能计算机的最重要指标是其综合应用性能,通常量化为以Linpack为代表的基准测试程序性能,这也是设计阶段人们最关心的指标之一.因此,面向高性能计算机的模拟器需采用执行驱动模拟方法,并且支持全系统模拟,以实现Linpack等应用程序在目标系统中的模拟执行.2.2保证模拟性能的方法由于执行驱动需要模拟程序在目标系统中的实际执行,而高性能计算机系统规模庞大,因此模拟性能成为模拟器设计的最大挑战.在传统的体系结构模拟器中,需要进行目标处理器内部各种部件及指令系统的建模,导致程序的模拟执行效率低下,无法满足规模庞大的高性能计算系统的模拟需要.采用以下两种措施:为了大幅提升模拟性能,SimHPC的系统设计首先,采用与目标系统同构的处理节点作为模拟器宿主节点.高性能计算机通常采用商用处理器甚至商用主板,因此,可以采用相同的处理器和处理节点作为模拟器的宿主机平台.采用这一方法后,由于宿主机和目标机的处理器和节点结构相同,同一机器代码序列的执行时间也一致,因此可以避免在模拟器中进行处理器内部各种部件和指令系统的建模,一方面大幅提高了程序模拟执行的性能,另一方面也大大简化了模拟器设计.Page3其次,采用多机并行的方式模拟高性能计算系统,即采用小规模的并行宿主机模拟更大规模的目标机.这一方法使得模拟系统可根据要模拟的目标系统规模调整宿主机节点个数,使得大规模并行系统的模拟成为可能.3模拟器关键问题分析3.1基于同构节点的执行驱动模拟方法模拟器的重要目标之一是尽可能准确地模拟出应用程序在目标系统中的执行时间.这一执行时间的模拟方法分析如下.在目标系统中,应用进程在操作系统调度下运行,其执行时间的构成如式(1)所示.其中Trun为该进程在处理器上实际执行的时间;TIO为执行I/O操作的阻塞等待时间;Tready为按时间片轮转调度时在就绪队列中的等待时间.由于宿主机系统采用与目标机同构的节点,因此同一机器代码序列在宿主机和目标机中的执行时间相等,即同一应用进程在宿主机和目标机中的Trun相等;而由于宿主机和目标机的互连网络及外存系统不同,I/O阻塞时间TIO需通过对互连网络和外存系统的建模模拟得到;至于就绪等待时间Tready,由于宿主机规模远小于目标机,一个宿主机节点运行的应用进程个数要比目标机节点多得多,因此应用进程在目标机中的就绪等待时间Tready需要根据目标机节点进程个数、操作系统调度策略和进程执行步调重新计算.基于以上分析,为了较为准确地模拟出应用进程在目标系统中的执行时间,并获取通信、I/O等重要事件信息,模拟器需要完成的核心工作如下:(1)事件捕获.捕获应用进程执行过程中的重要事件,包括进程调度、通信操作、文件访问操作等.通过捕获进程调度事件,可以计算出应用进程的实际执行时间Trun;而捕获通信和文件访问事件后,可以通过互连网络和外存模拟模块计算获得I/O阻塞时间TIO.(2)互连网络及外存系统模拟.在高性能计算机中,节点间的消息通过互连网络传输,因此互连网络是影响高性能计算系统性能的重要因素,通过对互连网络和外存系统的数学建模和模拟,可以在每一次通信和文件访问事件发生时,计算出消息传递以及文件读/写操作的时间延迟,进而计算出应用进程在目标系统中的阻塞时间TIO.(3)建立应用进程时间轴.由于宿主机节点中的进程个数远大于目标机,且两个系统中的I/O操作时延也不相同.为了准确刻画应用进程在目标系统中的执行步调,需要为每个应用进程建立其在目标系统中的运行时间轴,以明确各种事件在目标系统中的发生时刻,进而计算出应用进程的就绪等待时间Tready.3.2节对进程时间轴的建立方法进行较为深入的介绍.3.2应用进程时间轴本系统使用时间轴记录和描述应用进程的动态执行轨迹.应用进程时间轴包含了改变该进程状态的各种事件及其发生时刻的信息,定义如下:设应用进程集合P={p0,p1,…,pn-1},其中应用进程pi(i=0,1,…,n-1)的时间轴由该进程的事件记录序列构成Ai={Ei0,Ei1,…,Eim-1},该序列中每个事件记录对应一个二元组,包含事件及其发生时刻,即事件记录Eij=〈eij,tij〉(i=0,1,…,n-1;j=0,1,…,m-1).事件记录在时间轴集合中按发生时间顺序先后排列,即时间轴Ai上存在时间偏序关系.在宿主机系统中,通过应用程序执行时进行事件捕获,可以得到每一个应用进程在宿主机上的时间轴HAi,模拟器需要根据宿主机时间轴、应用进程在目标系统中的分配信息以及进程调度策略建立应用进程在目标机中的时间轴TAi.一个程序的多个应用进程之间需要相互通信和同步,这会影响进程的执行步调,因此要建立一个进程在目标机上的时间轴,不但要参照宿主机上该进程的时间轴,还要参照其它相关进程的时间轴,以计算进程间的通信等待时间.在建立时间轴的过程中,模拟器按捕获时间顺序逐个处理捕获到的所有进程事件,根据事件类型、宿主机发生时刻以及相关模拟模块的返回值计算并推进各进程时间轴.为了记录时间轴推进的当前位置,为每个应用进程pi维护一个逻辑时钟CLKi(初始值为0),随着各事件在目标机中的发生时刻逐个被确定,CLKi的值也逐步递增.另外,为了实现进程间消息发送和接收事件的匹配,为每个进程维护一个通信事件缓冲队列.对各种类型的事件进行处理并推进进程时间轴的方法见表1.Page4事件类别进程调度通信表1进程事件处理方法(设事件记录为Eij)推进时间轴:进程逻辑时钟CLKi←CLKi+R;推进时间轴:进程逻辑时钟CLKi←CLKi+(tij-tsi)推进时间轴:进程逻辑时钟CLKi←CLKi+(tij-tsi)if(本消息目的进程pk没有匹配的接收操作事件)then将事件暂时放入缓冲队列,待匹配后处理;调用互连网络模拟得出消息传递时延D;累计进程执行时间:Truni←Truni+(tij-tsi)推进源进程时间轴:CLKi←CLKi+(tij-tsi)记录源进程的状态开始时间:tsi←tij目的进程时间轴与源进程同步:CLKk←CLKi其它进程间通信处理流程与MPI发送/接收消息类似,但无需通过互连网络模拟消息传递时延文件访问读/写文件注1:关于MPI消息传递事件,为简化讨论,只给出MPI标准模式的处理方法,其它模式类同;注2:处理通信或文件访问事件时,根据互连网络或存储模拟得出时延D,仅当后续该进程被阻塞时,该值才被计入I/O阻塞时间,这主要是考虑到进程可能采用非阻塞方式的消息传递和异步文件访问.图1给出了3个应用进程时间轴的简单示例.在宿主机上模拟运行时,进程0和进程1运行在同一处理器(核)上,而在目标机中,3个进程各自独占一个处理器(核).图1(a)为在宿主机上模拟运行时,通过事件捕获得到的宿主机时间轴,其中进程0向进程1发送了一条MPI消息,并进行了1次文件图1应用进程时间轴示例该进程在目标机中时间轴计算方法访问,进程1则从进程0接收了消息,并向进程2发送了一条消息;图1(b)为根据宿主机时间轴建立的应用进程在目标机上的时间轴,由于在目标机中每个进程各自独占处理器,不再像宿主机那样多进程在一个处理器上分时交替运行,因此应用进程的目标机时间轴与宿主机时间轴相比有较大差异.Page53.3互连网络与存储系统模拟互连网络与存储系统模拟的作用是,针对系统捕获到的消息传递和文件访问事件,计算得出相应操作的时间开销长度,为建立应用进程时间轴提供依据.(1)互连网络建模与模拟系统在已有的互连网络数学模型基础上,针对近年来高性能计算领域应用较多的Infiniband(IB)技术建立了IB交换机、节点、路径和网络延时模型以及Fat-tree、2D/3D-mesh和2D/3D-Torus互连网络模型.数据包的网络延时包括在每个节点上的交换机处理时间Tproc(nodei)和交换机排队等待时间Twait(nodei),还包括在各个信道上的传输时间Ttrans(c),c∈path={inj,c1,c2,…,cn-1,ej},包括包头传输时间Th(c)和数据载荷的传输时间Td(c).在虚切通交换方式下,每发生一次切通,数据包的延时便小于存储转发交换方式下的延时,两者之间的差值为Td(c)+Twait(nodei).基于以上的路径模型,长度为L的数据包(包头长度所占比例为α)从源端到目的端经历的跳数为h,即经过n=h+1个节点,将IB信道的带宽标记为W,则该包的网络延时为latency=[n×Tproc(Nodei)+∑nc×∑j∈CT其中,CT表示发生切通的节点的编号集合,交换机上的处理时间近似为一个常数τ,故有Tproc(nodei)=τ.另外,由于各个IB信道的带宽相等,故包在各个信道上的传输时间相等,均为Ttrans(c)=L将式(3)代入式(2),有latency=n×τ+∑n假设互连网络中数据包流呈泊松分布,应用排队论,得出所有源和目的相同的数据包的平均网络时延为Li,j=n×τ+∑n=n×τ+∑n(1-α)×L其中,μ为排队系统中服务台的平均服务速率,此处与信道平均包传输速率相等;λci为信道上的数据包到达率;ρci为信道的服务率ρci=λci/μ.由式(5)可以得知,对于一个源、目的和长度确定的数据包,其在规则拓扑且使用虚切通流控制机制的互连网络上的延时由信道带宽、跳数、每条信道上的数据包到达率等参数决定.因此,对于Fat-tree、2D/3D-mesh等不同拓扑结构和路由算法的互连网络,只需推导出数据包的跳数和其路径中每条信道上的数据包到达率λci,即可得出该数据包的网络延时.(2)存储系统建模与模拟高性能计算机通常设置一定数量的专用存储节点,并采用并行文件系统,其文件访问时延除了与设备参数相关外,还与节点操作系统中的文件管理和存储节点中的管理调度机制、多级的文件缓冲管理、文件访问历史等密切相关,对其进行精确的数学建模有很大难度.因此,系统采用了功能模拟和数学建模相结合的方法,通过对文件预取和缓冲管理进行功能模拟,区分出缓冲命中和不命中这两类处理流程完全不同且性能差异较大的文件访问请求,对需要访问磁盘阵列的请求,结合数学建模和目标设备平均访问时间等参数,计算其访问时延.对计算节点内缓冲未命中的文件访问请求,其时延模型为Tstorage-system=Testablish-connection+Tsend-request+其中,Testablish-connection表示计算节点发送通用服务管理数据包与存储节点建立连接所用的时间,主要为互连网络的延时;Tsend-request表示处理节点发送读写请求至存储节点的时间,即读写请求的网络延时;Treturn为返回信息的网络延时,返回信息包括完成状态,所读数据等;Tstorage-node为读写请求在存储节点上花费的时间,即对存储节点的访问时延.由上可知,式(6)中除Tstorage-node外,其余时间均可通过调用互连网络模拟模块获得.对存储节点内缓冲未命中的文件访问请求,根据文件访问数据量和目Page6标系统磁盘阵列平均读/写时延计算其访问时延.4模拟器系统结构图2为模拟器运行示意图,如前所述,系统采用图2模拟器运行示意图模拟器的组成结构如图3所示.系统由内核模块、运行控制模块、事件分析处理模块、互连网络与存储系统模拟模块构成.其中内核模块运行在每个宿主机节点操作系统的内核空间,负责在应用进程运行过程中捕获进程调度、通信操作、文件访问操作等事件,并将其传输给事件分析处理模块;运行控制模块负责整个模拟系统的运行控制,它是用户与模拟系统进行交互的接口,提供参数配置、启动、停止图3模拟器系统结构系统对事件采取分布式捕获、集中式处理的方式,这是因为应用进程时间轴的建立需要对所有进程的事件进行协同处理,无法在各宿主机节点上独立完成,因此系统设置专门的模拟控制和事件处理节点,运行除内核模块之外的事件分析处理、互连网络和存储系统模拟以及运行控制模块.5评价与分析5.1实验环境与实验方法系统的测试评价包含两部分,首先是模拟精度并行模拟方式,使用多个与目标机同构的宿主机节点作为模拟平台,通过目标应用进程的分配和映射,在宿主机上模拟应用程序在大规模目标机上的运行.多个宿主机节点之间通过网络互连,网络类型与目标系统可以不同.等控制命令,并在模拟结束后向用户返回模拟结果;事件分析处理模块对内核模块捕获的各种事件数据进行收集、处理,并通过分析处理建立各应用进程在目标系统中的时间轴,最终获得应用程序在目标系统的执行信息;互连网络和存储系统模拟模块分别实现目标系统中互连网络和存储系统的建模,这两个模块由事件分析处理模块调用,根据目标系统的配置参数,计算节点间消息传递时延和文件访问时延.和模拟性能的测试,在此基础上,对不同规模的目标并行系统进行模拟实验.成,其硬软件参数配置如表2.测试使用的宿主机由4台双路刀片服务器构项目刀片型号及配置节点间互连操作系统RedhatEnterpriseLinux5.0,内核版本:2.6.18测试用程序LinpackHPL+MPICH2+GOTOBLASPage75.2模拟精度与性能测试为了测试系统的模拟精度,在模拟器中对较小规模的目标系统(节点个数1~32)进行模拟实验,获取目标系统的Linpack性能数据,并将其与真实物理系统中获取的数据进行对比,得到模拟误差如表3所示.目标节点个数124128.8188273.09516522.98824768.08632988.637从表3可以看出,模拟器得到的性能数据与真实系统相比的误差比例随节点个数增加有增大趋势,但在16节点后趋于稳定,这主要是由于模拟实验在4个节点的宿主机上进行,每个宿主节点上的进程个数可能多于目标系统,而模拟器忽略了进程间cache共享对性能的影响,因此当目标节点个数大于8时,模拟器得到的性能值比真实数据偏小,这也是模拟器今后的改进方向之一.总体而言,目标系统达到一定规模后模拟误差比例趋于稳定,且保持在9%左右.模拟器的减速比(slowdown)是评价模拟性能的重要指标,它是同一工作负载在模拟器中执行时间与真实系统中执行时间的比值,能够较好地反映模拟行为的开销,该值越小,说明模拟器性能越高.本系统减速比的测试仍然采用Linpack程序作为工作负载,针对不同规模的目标系统测量Linpack程序模拟执行时间(含模拟器数据分析处理时间),然后在不启动模拟器的情况下,在4节点宿主机上测量相同个数Linpack进程的执行时间,将两者相比得到减速比如图4所示.从图4可以看出,随着节点规模的增大,减速比从十几倍到一百多倍逐渐升高.与之相比,通用的体系结构模拟器减速比一般在千倍以上甚至上万倍;与同类的大规模并行系统模拟器相比,本系统的性能也表现较优,例如,BGLsim[1]使用4节点模拟1024节点时减速比最小为220,而本系统为126.9.5.3目标系统模拟实验在如前所述的宿主机平台上对不同系统规模和不同互连网络结构的目标系统进行了模拟实验,并获取系统的Linpack性能及通信行为等数据.实验中目标机节点个数32~1024,互连网络拓扑分别选用基于Infiniband的fat-tree、3D-mesh和3D-Torus,Infiniband带宽为20Gbps;Linpack任务规模N=30000,NB分块为168,应用程序及所用程序库均未做专门优化.图5和图6分别给出了不同系统规模下Linpack性能及其增长情况.图53种互连网络拓扑下Linpack峰值性能图6Fat-tree互连拓扑下Linpack程序性能及增长比例从图5可以看出,3种互连拓扑下的Linpack性能由高到低依次为Fat-tree、3D-Torus、3D-mesh,这主要是因为相对于mesh/torus结构,fat-tree网络具有更小的网络直径和平均距离,因此平均通信时延相对较小.当然,随着系统规模增大,构建fat-tree网络需要的交换机及光纤数量也比torus/Page8mesh结构多得多.从图6可以看出,Linpack性能并没有随节点个数成倍增长,而是增长幅度越来越小,例如,节点个数由32增加到64时,Linpack性能基本都能同步获得100%的增长率,但当节点个数由512增长到1024时,仅获得47.9%的性能增长.主要原因是随着节点数量增加,互连网络规模增大导致通信延迟越来越大,进而影响到程序性能.由此可以看出,在大规模并行系统中,如何提高应用程序的并行效率,尽量降低不必要的网络通信开销,是非常重要的环节.6相关工作在计算机体系结构的研究中,实验工具多采用通用的体系结构模拟器,典型的有较早期的SimpleScalar[2]和SimOS[3]以及近年来应用较多的Simics[4]、GEMS[5]、M5[6]等.这些系统一般支持执行驱动模拟方式,多数支持全系统模拟,在体系结构建模方面,支持处理器微体系结构和内存系统的细粒度建模和模拟,如处理器内的流水线、Cache访问等.这些功能可以为新型处理器结构和内存系统的研究提供很好的支持,但这种细粒度的建模和模拟也大大降低了模拟性能,因此通用的体系结构模拟器通常用于处理器个数较少的系统的模拟,而难以满足大规模并行系统的模拟需求.有鉴于此,近年来也出现了一些专门针对大规模并行系统的模拟器,典型的有BigSim[7]、BGL-Sim[1]、MPI-SIM[8]、Hypersim[9]等.BigSim由美国伊利诺依州立大学香槟分校(UIUC)开发,能通过执行真实应用来预测高性能计算机的性能,并且支持较为精细的网络性能模拟,然而该系统依赖于UIUC的CHARM++[10]并行编程和运行环境,这在一定程度上限制了它的使用范围;BGLsim是IBM公司为BlueGene/L系统专门开发的模拟器,该系统可模拟BlueGene/L几乎所有的硬件特性,且可模拟运行MPI应用程序,其主要局限性是目标机限定于BlueGene/L体系结构和PowerPC处理器;MPI-SIM是一个MPI程序库,可用于测试和预测MPI并行程序在各种体系结构下的性能表现,可设定的目标系统参数包括处理器个数和通信延迟,但不支持互连网络及外设的建模和模拟;Hypersim由中国科学院计算技术研究所设计,专门用于模拟其超并行体系结构(HPP),该系统改进了传统体系结构模拟器按时钟逐步推进的模拟方式,采用N步推进的超步执行,以此提高模拟效率,但由于需要支持节点内的部件级建模,其模拟性能依然受限.与已有的大规模并行系统模拟器相比,本文工作所采用的基于与目标系统同构节点进行模拟的方法与MPI-SIM类似,但MPI-SIM不支持互连网络和I/O的建模和模拟,而互连网络是影响大规模并行系统性能的重要因素,历来受到设计人员高度重视;在并行模拟的时间同步方法上,已有模拟器大多沿用传统体系结构模拟器中所使用的方法,即各进程(节点)维护各自的逻辑时钟,并通过复杂的同步机制确定事件的先后顺序,某些策略在发生误判时还需进行回滚,这可能会大大降低并行模拟的效率,与这些方法相比,本文针对大规模并行系统模拟中所关注事件的特点,采用了执行中捕获事件,执行后重建时间轴的串行时间同步方法,可在保证正确性的前提下使进程全速运行,模拟效率更高;另外,本文提出的SimHPC不局限于某种硬件体系结构或软件编程运行环境,具有更好的适用性.7结论和未来的工作模拟实验方法对于高性能计算机系统的研究和设计具有重要的意义,然而由于目标系统规模庞大,传统的体系结构模拟器难以在模拟性能方面满足要求.本文提出的专门用于大规模并行系统模拟的模拟器———SimHPC,采用执行驱动的全系统模拟方法,支持操作系统和包括Linpack在内的应用程序的模拟运行.SimHPC通过采用与目标系统同构的节点作为宿主节点,以及并行模拟的方法,使得模拟性能相比传统的体系结构模拟器大幅提高,与现有的几种大规模并行系统模拟器相比,SimHPC在通用性和模拟性能方面也具有一定的优势.为保证模拟性能,目前SimHPC要求宿主节点与目标节点同构.今后的努力方向是,在满足高性能计算机系统模拟性能的前提下,使系统能够模拟与宿主节点不同结构甚至不同种类处理器的目标系统,目前系统正在研究和实验基于性能比例因子和基于性能计数器的异种处理器及节点模拟方法;另外,需考虑宿主节点中多进程共享cache带来的性能影响,以进一步提高模拟精度.
