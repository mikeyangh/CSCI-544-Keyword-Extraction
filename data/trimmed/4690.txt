Page1基于最大化交叉互信息的对称IB算法娄铮铮叶阳东(郑州大学信息工程学院郑州450001)摘要对称IB(SymmetricInformationBottleneck)通过行、列压缩变量之间的相互协作来挖掘数据中的双向压缩模式.由于行、列压缩变量不能完全承载行、列基层变量中所蕴含的特征信息,从而导致对称IB所得的数据双向压缩模式与基层变量所蕴含的内在模式之间存在一定的偏离.针对该问题,通过最大化地保存压缩变量与基层变量交叉之间的互信息,将基层变量引入到数据的双向压缩中,使它们协助压缩变量共同来学习联合分布中的双向压缩模式,提出交叉对称IB:ICSIB(Inter-CorrelatedSymmetricInformationBottleneck).ICSIB算法采用交错的顺序“抽取-合并”迭代过程来优化压缩变量与基层变量交叉之间的互信息,可保证得到目标函数的一个局部优解.实验结果表明,在基层特征变量的协助下,ICSIB算法得到的数据双向压缩模式更接近于数据中真实的内在模式,并可有效地应用于数据的联合聚类中.关键词IB方法;多变量IB;对称IB;双向压缩;联合聚类;数据挖掘1引言IB方法(InformationBottleneckmethod)是Tishby等人[1]于1999年提出的一种基于信息论的数据分析方法.该方法在做数据分析时,将数据模式的提取视为一个数据压缩的过程,如图1所示,其中X表示待分析的数据对象,Y表示描述数据对象的特征变量,珦X为X的压缩“瓶颈”变量.变量X到珦X的压缩编码p(x~|x)即为IB方法所获得的数据压缩模式,若某些数据对象被压缩到同一个簇x~中,那么它们被视为具有相同的模式特征.为使压缩编码p(x~|x)尽可能真实地反映数据中所蕴含的内在模式,IB方法在对数据进行压缩的同时,要求“瓶颈”变量珦X尽可能最大化地保存特征变量Y中所载有的信息量.变量Y客观地描述了数据对象的特征,是IB方法数据压缩的依据.IB方法具备良好的理论基础,在众多领域中均取得了成功的应用[2-13].多变量IB方法(MultivariateInformationBot-tleneck)[14-15]是对IB方法的拓展,采用更多的变量来抽象现实问题,让更多的信息参与到数据的压缩中,通过多种信息之间的相互协作来完成更具挑战性的数据分析任务.多变量IB方法为多元数据分析问题提供了理论框架,其协作模型刻画了变量之间的协作关系,为数据分析任务目标函数的确定提供了依据.多变量IB方法在处理多元数据分析问题时具有独特的优势[14-21],然而在应用中需结合具体问题来实体化多变量IB方法的协作模型并设计相应的优化算法.对称IB[15]是多变量IB方法的一个实体化协作模型,将IB方法的数据单向压缩拓展到双向压缩中,即对联合分布p(X,Y)同时做X到珦X及Y到珟Y的压缩.对称IB一方面可获取变量X的压缩模式p(x~|x)及变量Y的压缩模式p(y~|y),另一方面可获取联合分布p(X,Y)的高度压缩模式p(珦X,珟Y).例如:在对文档数据进行分析时,对称IB可同时对文档和单词进行压缩,将具有相似主题的文档压缩到同一个文档簇中,得到文档的压缩模式,同时也将具有描述相似语义能力的单词压缩到同一个单词簇中,得到单词的压缩模式;另外,对称IB还可获得文档簇与单词簇之间的高层统计规律.若将一个文档簇视为一个主题,一个单词簇视为一个语义单位,那么该高层统计规律则反映出每个主题与每个语义单位之间所对应的关联关系.对称IB的数据双向压缩可用于解决机器学习领域中的联合聚类问题[22-26].在此值得一提的是,当对称IB将重点放在压缩变量珦X与珟Y之间信息量的保存上时,对称IB的目标函数可简化为Dhillon等人[23]提出的信息论联合聚类(Information-TheoreticCo-Clustering,ITCC)的目标函数,此时信息论联合聚类可视为对称IB的一个特例.本文在2.3节将对此做详细分析;另外在4.3节中的实验结果表明,对称IB的顺序“抽取-合并”优化算法在做联合聚类时性能优于ITCC的“矩阵逼近”优化算法,因此可以说对称IB算法是一个更高效的联合聚类算法.对称IB和ITCC算法所得到的压缩变量之间的统计规律p(珦X,珟Y)是对联合分布p(X,Y)的高度压缩模式,揭示了联合分布p(X,Y)中行、列变量压缩模式之间所蕴含的高层特征模式.该高度压缩的模式结构对数据分析同样有着重要的意义,在文档数据分析[27]、图像场景建模[28]、视频数据分析[29]、自适应学习[30-31]中均取得了成功的应用.对称IB通过行、列压缩变量珦X与珟Y之间的相互协作来学习联合分布p(X,Y)中的双向压缩模式,即在迭代学习过程中,X到珦X的压缩p(x~|x)是依据珟Y所提供的特征信息来进行的,Y到珟Y的压缩p(y~|y)是依据珦X所提供的信息来进行的.珦X和珟Y分别是X与Y被压缩之后的变量,它们并不能完全承载基层变量X与Y中所固有的特征信息.因此,仅考虑压缩变量珦X与珟Y之间协作关系的对称IB在一定程度上存在着特征信息损失的问题,进而可能导致所得的双向压缩模式偏离数据中所蕴含的内在模式.例如:对称IB在对文档数据进行双向压缩时,文档簇和单词簇分别是单词与文档的压缩依据.尽管一个单词簇中的单词具有描述相同语义的能力,可被视为一个语义单位,但就单词个体而言,它们往往还具备表达出多种语义信息的能力,而作为多个单词合体的单词簇则损失了单词本身的多义性信息.因此,在以单词簇为依据所得的文档压缩模式与以单词为依据所得的文档压缩模式之间可能存在着一定的偏离.另外,一个文档有时可归属于多个主题,因此在以文档簇为依据的单词压缩过程中也存在着同样的上述问题.针对对称IB在对数据进行双向压缩时存在特征信息损失的问题,在多变量IB方法的基础上,提出Page3一个交叉对称IB协作模型ICSIB(Inter-CorrelatedSymmetricIB).在ICSIB的协作模型中,除了关注高层压缩变量珦X与珟Y之间的协作关系外,还将基层变量X与Y引入到数据的双向压缩中,使得它们协助压缩变量珦X与珟Y一起来学习联合分布中的双向压缩模式.ICSIB算法采用交错的“抽取-合并”顺序迭代过程对目标函数进行优化,具有较低时间和空间复杂度,且可保证得到目标函数的一个局部优化解.在文档联合聚类及图像无监督模式识别上的实验结果表明,在基层变量的协助下,ICSIB算法所得数据双向压缩模式更接近于数据中所蕴含的真实内在模式,其性能优于k-means算法、NormalizedCuts算法[32]、信息论联合聚类ITCC算法[23]、对称IB算法[15]、aIBCC算法[19]及基于非负矩阵分解的DRCC联合聚类算法(DualRegularizedCo-clustering)[24].另外,在基层特征变量的协助下,ICSIB算法得到的压缩模式p(珦X,珟Y)中压缩变量珦X与珟Y之间的关联程度同样优于ITCC算法[23]和对称IB算法[15].本文的主要贡献可总结如下:(1)提出一个交叉对称IB协作模型ICSIB.该模型将压缩之前的基层特征变量X与Y引入到对称IB的双向压缩中,更充分地利用X与Y所提供的基层特征信息,使其协助压缩变量珦X与珟Y更好地学习数据中的双向压缩模式,从而解决对称IB中特征信息损失问题.(2)ICSIB算法采用交错的“抽取-合并”顺序迭代过程对目标函数进行优化,理论上保证收敛到目标函数的一个局部优解,具有较低时间和空间复杂度.2背景知识本文中,符号X、Y表示离散型随机变量,其值域分别为珦X、珟Y表示X、Y的压缩变量,值域分别为2,…,x~x~变量的集合,即犡={X1,…,Xn}.2.1IB方法IB方法[1]起源于著名的率失真理论.该方法在做数据分析时,将数据模式的提取视为一个数据压缩的过程,即将源变量X压缩到一个“瓶颈”变量珦X中,同时使压缩变量珦X最大化地保存相关变量Y中所蕴含的信息量,其中X到珦X的压缩编码p(x~|x)便为IB方法所得的数据压缩模式.IB方法可形式化地描述为其中,I(X;珦X)为变量X与珦X之间的互信息[33],计算方法为I(X;珦X)=∑x∑x~式(1)表明,IB方法是在满足信息保存限制的条件下,即压缩变量珦X所保存特征变量Y中的信息量I(珦X;Y)满足I(珦X;Y)D,在所有可能编码方案中选择使压缩信息I(X;珦X)最小的一个编码方案p(x~|x).为求最优压缩编码方案p(x~|x),文献[1]采用拉格朗日乘子法将式(1)改为如下的IB目标函数:其中,β是一个大于等于0的拉格朗日因子,用于平衡源信息的压缩和相关信息的保存.对IB方法的目标函数式(3)求关于p(x~|x)的导数,可得到如下IB方法的形式化解,p(x~)=∑x,y其中,DKL[p(y|x)‖p(y|x~)]是条件分布p(y|x)与p(y|x~)之间的KL(KullbackLeibler)距离[33],Z(x,β)=∑x~是归一化函数.2.2多变量IB方法多变量IB方法[14-15]是对IB方法的拓展,在处理更复杂的多元数据分析问题时具有独特的优势.该方法使用一组随机变量犡={X1,…,Xn}来表示领域问题,将相关问题的领域知识抽象为一个多元联合分布p(犡)的形式.给定一组随机变量犡={X1,…,Xn},多变量IB方法力图求解{犝1,犝2,…,犝k}(犝j犡)到珟犡={珦X1,…,珦Xk}的一组压缩表示,其中珦Xj是犝j的压缩变量.类似于原IB方法,多变量IB方法在对变量进行压缩的同时,为使压缩变量有效地获取数据中所蕴含的某种内在模式,压缩变量珟犡={珦X1,…,珦Xk}应该最大化地保存变量集合犡={X1,…,Xn}中某些变量所提供的相关信息.变量之间的压缩关系称为期望压缩关系,变量之间的相关模式保存关系称为期望模式保存关系,它们共同组成多变量IB方法的协作模型.多变量IB采用贝叶斯网Gin来刻画变量之间的期望压缩关系,采用Gout来刻画变量之间的期望模式保存关系.贝叶斯网是一个有向无环图G=(V,E),Page4其中V为节点集合,由一组随机变量{X1,…,Xn}组成,E为边的集合,描述变量间的相互依赖关系.如果(Xi,Xj)∈E,则节点Xi到Xj存在有向边,Xi为Xj的父节点,Xj为Xi的子节点.记变量节点Xi在贝叶斯网G中的父节点集为犘犪G依赖关系的随机变量集合{X1,…,Xn}的联合分布可分解为[15]一旦确定了Gin中犡到珟犡的期望压缩关系,则犡与珟犡之间的联合分布可由式(8)求得[15]其中犘犪来确定变量犝j到珦Xj的压缩关系.多变量IB方法分别采用多信息IGin和IGout来度量Gin中变量之间的压缩程度和Gout中的相关模式的保存程度.文献[15]给出如式(9)的多变量IB目标函数,其中β为大于0的平衡因子.min=IGin[p(犡,珟犡)]-βIGout[p(犡,珟犡)](9)为更有效地计算Gin和Gout中联合分布p(犡,珟犡)内多个变量之间的信息量,文献[15]给出如下定义.定义1.如果犡={X1,…,Xn}服从联合分布p(犡),并且犡中的节点服从贝叶斯网G所刻画的变量之间的依赖关系,则服从贝叶斯网G的联合分布p(犡)中的多信息可分解为其中每一个互信息I(Xi;犘犪G犘犪GXi2.3对称IB)可通过p(犡)的边缘分布计算得到.对称IB[15]将IB方法的数据单向压缩拓展到数据的双向压缩中,其协作模型如图2所示.图2(a)中的贝叶斯网Gin描述了变量之间的期望压缩关系,其中变量X到变量Y之间的箭头表示变量X与Y之间服从联合分布p(X,Y),该箭头也可由变量Y到变量X;变量X到变量珦X、变量Y到变量珟Y之间的箭头表示对称IB力图做变量X到变量珦X、变量Y到变量珟Y的双向压缩,其中X为珦X的父节点、珦X为X的子节点,变量Y与珟Y之间也具备类似的关系.图2(b)中的Gout描述了压缩变量之间的期望模式保存关系.从该图中可以看出,对称IB力图保存压缩变量珦X与珟Y之间的信息量,变量珦X与珟Y之间的箭头方向可互换.为求得变量X到珦X的压缩模式p(x~|x)及变量Y到珟Y的压缩模式p(y~|y),在期望模式保存关系图Gout中,压缩变量珦X与珟Y相互为对方提供特征模式,即压缩变量珟Y中所承载的信息为变量X到珦X的压缩提供了依据,压缩变量珦X中所承载的信息为变量Y到珟Y的压缩提供了依据.在学习过程中,变量珦X与珟Y之间相互协作,共同挖掘联合分布p(X,Y)中所蕴含的内在模式.式(11)为对称IB的目标函数,其中I(X;珦X)+I(Y;珟Y)度量了Gin中X到珦X、Y到珟Y的压缩程度,而I(珦X;珟Y)度量了Gout期望模式的保留程度,通过β调节二者的平衡程度.max=I(珦X;珟Y)-β对称IB可用于解决机器学习领域中的联合聚类问题,这里给出对称IB与经典的信息论联合聚类ITCC[23]之间的关联.ITCC在做联合聚类时,力图使压缩前后联合分布p(X,Y)和p(珦X,珟Y)中所包含的信息损失最少,采用互信息来度量聚类前后p(X,Y)与p(珦X,珟Y)内的信息量,其目标函数如式(12)所示.在该目标函数中,给定联合分布p(X,Y),I(X;Y)的值不变,为一个确定的常量.此时,目标函数(12)等同于式(13).如果在对称IB目标函数(11)中β取值为(无穷大)时,则对称IB的目标函数与信息论联合聚类的目标函数一致.因此,文献[23]中的信息论联合聚类可视为对称IB的一个特例.文献[19]将IB方法应用到联合聚类中.该文献虽然在期望压缩模式保存关系图中将联合分布p(X,Y)中X和Y之间相互蕴含的基层特征信息引入到联合聚类中,但是该算法采用层次凝聚的方法来优化目标函数,即通过不断合并最相似的两个簇(行变量的簇或列变量的簇)来实现联合聚类.该优化过程时空复杂度高,且不能保证得到目标函数的一个局部优化解.另外,文献[19]仅关注双向压缩中的聚类结果,而对双向压缩的另一个结果,压缩变量之间的高层统计规律,却没有做深入的研究,该模式从更高层面反映了簇与簇之间的关联性,具有较高的应用价值[27-31].Page53交叉对称IB图2中对称IB的协作模型Gin描述了变量X到珦X与变量Y到珟Y的数据双向压缩关系,Gout刻画了压缩过程中所期望保存的特征模式.Gin和Gout表明,在压缩学习过程中,X到珦X的压缩编码p(x~|x)是依据珟Y所提供的特征信息确定的,Y到珟Y的压缩p(y~|y)是依据珦X所提供的信息确定的.然而,珦X和珟Y分别是X与Y被压缩之后的变量,它们并不能完全承载基层变量X与Y中所固有特征信息.因此,仅考虑压缩变量珦X与珟Y之间协作关系的对称IB存在着特征信息损失的问题,进而可能导致所得的双向压缩模式偏离数据中所蕴含的内在模式.本节在多变量IB的基础上,重点研究数据的双向压缩,提出一个交叉对称IB协作模型ICSIB.ICSIB算法在做双向压缩时,不仅关注压缩变量珦X与珟Y高层特征之间的关联模式,同时还将变量X与Y中所蕴含的基层特征信息引入到数据的双向压缩中,使它们协助珦X与珟Y共同来挖掘数据中的双向压缩模式,从而更充分地利用数据中所蕴含的有价值信息.3.1ICSIB协作模型ICSIB的协作模型如图3所示,该协作模型的Gin描述了X到珦X及Y到珟Y的压缩关系,Gout描述了压缩变量所期望的相关模式保存关系.与图2中的对称IB协作模型相比,在图3的Gout中多了两条交叉的珦X→Y与珟Y→X,以使压缩变量珦X与珟Y分别交叉地保存变量Y与X所提供的基层特征信息,从而使基层特征变量协助压缩变量珦X与珟Y共同学习数据中所蕴含的双向压缩模式.因此,ICSIB所得的期望压缩编码p(x~|x)和p(y~|y)不仅反映了压缩变量珦X与珟Y之间相互蕴含的高层特征信息,同时还交叉地反映了承载在变量X与Y中数据原有的基层特征信息,从而解决了对称IB特征信息损失的问题.图3中的贝叶斯网Gin和Gout分别描述了变量之间的期望压缩关系与期望模式保存关系.在Gin中,变量X的父节点集合为空,即犘犪珦X的父节点集合犘犪GinY={X},变量珟Y的父节点结合犘犪犘犪据定义1中的式(10)可将图3(a)中Gin的期望压缩信息IGin分解为在图3(b)的Gout中,犘犪Gin珦X=,犘犪犘犪将图3(b)中Gout的期望模式保存信息IGout分解为将式(14)与式(15)中的IGin和IGout代入多变量IB目标函数(9)中,并在等式两边同时除以-β,可得式(16)所示交叉对称IB的目标函数.由于p(X,Y)已知,在式(14)中的I(X;Y)为一个固定的值.在不影响数据分析性能的情况下,为简化目标函数,在式(16)中省去I(X;Y)这一项.max[p(x~|x),p(y~|y)]=I(珦X;珟Y)+I(珦X;Y)+I(X;珟Y)-β下面给出目标函数(16)中联合分布p(x,x~)、p(y,y~)、p(x~,y~)、p(x~,y)及p(x,y~)的计算公式.首先,根据式(8)可对Gin中变量之间的联合分布进行分解,得到p(X,Y,珦X,珟Y)=p(珦X|X)p(珟Y|Y)p(X,Y)(17)其中p(X,Y)已知,由此可得p(x,x~)=p(x)p(x~|x),p(y,y~)=p(y)p(y~|y),p(x~,y~)=∑x∑yp(x~,y)=∑x∑p(x,y~)=∑y在双向压缩的应用中,由于珦XX,珟YY,X到珦X及Y到珟Y本身就是很大的压缩.此时我们可将重点放在相关信息的保存上,β可取值为无穷大,交叉对称IB的目标函数可简化为max[p(x~|x),p(y~|y)]=I(珦X;珟Y)+I(珦X;Y)+I(X;珟Y)在此,我们将其称为基于最大化交叉互信息的对称IB.Page63.2ICSIB算法ICSIB算法在做双向压缩时,采用交错的“抽取-合并”顺序迭代过程对目标函数(18)进行优化,来求解X、Y到珦X、珟Y的最优压缩表示p(x~|x)和p(y~|y).在寻求X到珦X的最优压缩p(x~|x)时,p(y~|y)保持不变,而在求p(y~|y)时,p(x~|x)保持不变.通过交错地“抽取-合并”顺序迭代过程来优化ICSIB目标函数(18).本文仅考虑“硬划分”,即p(x~|x)和p(y~|y)取值仅为0或1,此时,双向压缩即为将p(y~|y)保持不变的情况下,首先介绍“抽取-合并”顺序优化p(x~|x)的过程.在已知p(x~|x)上,记为珦Xold,迭代的将顺序地从其所在的簇x~old:p(x~old|x)=1中“抽取”出来,将其视为单独的一个簇,此时共有珦X+1个簇,我们将此时的珦X+1个簇表示为珦Xmed.为确保最终结果为珦X个簇,再将x“合并”到满足x~new=dL({x},x~)的新簇x~new中,其中dL表示将xargminx~合并到x~时目标函数的减少量.在p(x~|x)保持不变的情况下,“抽取-合并”顺序优化p(y~|y)的过程同上.整个ICSIB算法交错“抽取-合并”顺序迭代的过程如算法1所示.算法1.ICSIB算法.输入:联合分布p(X,Y);输出:X到珦X的压缩表示p(x~|x);Y到珟Y的压缩表示1.p(x~|x)←X的初始划分;2.p(y~|y)←Y的初始划分;3.REPEAT4.FOReveryx∈5.将x从当前簇x~(p(x~|x)=1)中抽取出来;6.将{x}合并到簇x~new中,7.END8.FOReveryx∈9.将y从当前簇y~(p(y~|y)=1)中抽取出来;10.将{y}合并到簇y~new中,11.END12.UNTIL划分p(x~|x)与p(y~|y)不再有新的变化13.根据公式p(x~,y~)=∑x∑y为使ICSIB算法快速收敛到目标函数的一个局部优值,在每次合并时都要确保目标函数中信息量损失最少,即新簇x~new和y~new应满足x~new=argminx~)和y~new=argmin变,将x从x~old:p(x~old|x)=1中“抽取”出来,将其视为单独的一个簇{x},并将其合并到簇x~中,形成新簇x~new,则p(y|x~new)=p(y~|x~new)=当p(y~|y)固定不变时,p(x~|x)的优化过程仅涉及到目标函数(18)中I(珦X;Y)与I(珦X;珟Y)两项的值,而I(X;珟Y)不变.因此,目标函数的减少量dL({x},x~)可计算为dL((x),x~)=Δ=其中ΔI1=I(珦Xmed;Y)-I(珦Xnew;Y)将式(19)、(20)代入上式得ΔI1=p(x)∑y=p(x)∑y=p(x)DKL[p(Y|x)‖p(Y|x~new)]+=[p(x)+p(x~)]·JS[p(Y|x),p(Y|x~)],Page7其中JS[p(Y|x),p(Y|x~)]为条件分布p(Y|x)与p(Y|x~)之间的JS(JensenShannon)距离[33].由此类推可得ΔI2=[p(x)+p(x~)]·JS[p(珟Y|x),p(珟Y|x~)].因此,在将{x}合并到簇x~中时,目标函数的信息损失量dL({x},x~)为dL({x},x~)=[p(x)+p(x~)]·{JS[p(Y|x),p(Y|x~)]+同理,当p(x~|x)固定不变时,在优化p(y~|y)的过程中,将y从y~old:p(y~old|y)=1中“抽取”出来,将其视为单独的一个簇{y},并将其合并到簇y~中,形成新簇y~new,此时,目标函数的信息损失量dL({y},y~)为dL({y},y~)=[p(y)+p(y~)]·{JS[p(X|y),p(X|y~)]+3.3算法分析3.3.1算法收敛性分析定理1.在ICSIB算法中,采用表示将一个元素从当前簇中“抽取”出来之前的目标函数值,采用素重新“合并”到某个簇中的目标函数值,则有证明.首先证明当p(y~|y)固定不变时,将x从当前簇x~old:p(x~old|x)=1中“抽取”出来,然后将其“合并”到簇x~new=argmin“抽取-合并”前后的目标函数值满足(珦Xold,珟Yold).记将x从当前簇x~old:p(x~old|x)=1中“抽取”出来后的目标函数值为过程中,当x~new=x~old时,p(x~|x)没有改变,则有(珦Xold,珟Yold)=(珦Xold,珟Yold)=(珦Xnew,珟Yold)=而x~new=argmindL({x},x~new),从而此当p(y~|y)固定不变时每次“抽取-合并”前后的目标函数值满足同理可得,当p(x~|x)固定不变,将y从当前簇y~old:p(y~old|y)=1中“抽取”出来,并将其合并到簇y~new=argmin(珦Xold,珟Yold).因此,在ICSIB算法的每一步“抽取”前与“合并”后,都有推论1.ICSIB算法可在有限步骤内收敛到交叉对称IB目标函数(18)的一个局部优解.证明.由定理1可知,ICSIB算法的每一步“抽取-合并”过程都能增加交叉对称IB目标函数(18)的值,而I(珦X;珟Y)I(X;Y),I(珦X;Y)I(X;Y),I(X;珟Y)I(X;Y),因此该目标函数是有界的,3·I(X;Y).由此可得出,ICSIB算法可在有即限步骤内收敛到交叉对称IB目标函数的一个局部优解.3.3.2算法时间复杂度分析ICSIB算法的第5步将数据元素从当前簇中抽取出来,该过程可在单位时间内完成,其时间复杂度为O(1);第6步为算法的优化合并过程,该过程需要计算当前数据对象到每一簇质心分布的JS的距离,该步骤的时间复杂度分别为O(k||),因此第4步到第7步的时间复杂度为O(k||||).类似地,第8步到第11步的时间复杂度为O(l||||).整个算法的时间复杂度为O(L(k+l)||||),其中L是ICSIB算法找到局部优化解迭代循环的次数.综上所述,ICSIB算法的时间复杂度和数据的规模线性相关.ICSIB算法是对对称IB算法[15]及ITCC算法[23]的拓展,后两者的时间复杂同为O(L(k+l)||||).由此可见,增加基层特征变量之间的交叉项并没有增加算法的时间复杂度.aIBCC算法在做联合聚类时同样引入基层特征变量之间所蕴含的特征信息,但该算法采用层次凝聚的过程来优化目标函数,其时间复杂度为O(||||(||2+||2)).由此可见,ICSIB算法的时间复杂度远低于aIBCC算法.4实验与性能分析本文从文档联合聚类、图像无监督分类和高层模式提取3个方面来评估ICSIB算法数据分析的性能.4.1数据集选择及预处理(1)文档数据20Newsgroups①包含大约20000篇新闻文档,可划分为20个新闻话题组.本文从该数据集中抽取9个数据子集作为实验数据,其中每个数据集由500篇文档组成,它们是随机地从20Newsgroups中的一些新闻话题文档中选出的.对选出的文档进行预处理,主要包括大写字母转化为小写字母;所有阿拉伯数字合并为单一的数字符号;删除非希腊数字符号、停用词(stopwords)①http://people.csail.mit.edu/jrennie/20Newsgroups/Page8和仅出现一次的单词;选择对互信息贡献最大的2000个单词作为文档的相关变量集[2].(2)图像数据本文采用文献[34]中所使用的amazon,dslr,webcam①三个图像数据集来验证ICSIB算法对图像无监督模式识别的性能,其中amazon中的图像是网络上的商标图像,dslr中的图像是由数码相机拍摄得到,webcam域中的图像是由网络摄像头拍摄得到,每个图像数据集均包含有31个类别的图像.原始的图像数据以像素为单位,故而学习算法无法直接对其进行模式分析,因此需要事先对图像数据进行一些预处理,将其转换为算法1可分析的数据形式.本文采用Bag-of-Words的方法[11,34-35]将图像数据集转化为共现矩阵.该方法主要分为4个步骤:(1)采用SURF算法[36]从每幅图像中抽取局部特征,并用64维的SURF描述符来描述每一个局部特征;(2)采用k-means算法量化SURF描述符,构建一个规模为800的码本,每一个码本可视为一个视觉单词(visualword);(3)把第一步从图像中抽取的每一个SURF描述符映射到上述码本中,将局部特征转化为视觉单词;(4)统计每个视觉单词在每幅图像中出现的次数,将图像数据集转化为共现矩阵.本文所使用的文档和图像数据集具体描述如表1所示.数据集名称Binary_1,2,3Multi5_1,2,3Multi10_1,2,3dslrwebcamamazonICSIB算法、对称IB算法[15]、ITCC算法[23]、aIBCC算法[19]均需要事先给出数据的联合分布.通过上述方法对文档及图像数据集预处理之后,可得到文档与单词、图像与视觉单词之间的共现矩阵.通过式(25)便可得到上述数据集的联合分布,其中n(x,y)为单词(视觉单词)y在文档(图像)x中出现的次数.4.2实验设计4.2.1对比方法为验证ICSIB算法对数据进行双向压缩的性能,本文给出该算法与以下算法的对比实验结果.(1)k-means算法.首先对数据集进行L2规范化(L2norm)预处理,然后采用k-means算法对数据对象进行聚类.(2)NormalizedCuts(NCuts)算法[32].该算法为基于图分割的聚类算法,以数据间的相似度矩阵作为输入数据.实验中首先对数据集进行L2规范化预处理,数据间的相似度矩阵为数据对象间欧几里得距离的负值.(3)对称IB(Sy_IB)[15].采用顺序迭代的方法对对称IB目标函数进行优化.(4)信息论联合聚类算法(ITCC)[23].采用矩阵逼近(MatrixApproximations)的方法对目标函数进行优化.(5)凝聚IB联合聚类算法(aIBCC)[19].数据对象与特征之间的联合分布作为输入数据,采用凝聚迭代的过程对目标函数进行优化.(6)DRCC(DualRegularizedCo-Clustering)联合聚类算法[24].原始数据作为输入数据.k-means算法和NormalizedCuts算法是传统的聚类算法,仅对数据对象进行聚类.ICSIB算法、ITCC算法、对称IB算法、aIBCC算法及DRCC算法则同时对矩阵的行和列进行聚类.4.2.2实验评估方法本文采用准确度(Accuracy)[25]和标准化互信息(Normalizedmutualinformation)[25]来度量上述算法所得的数据模式与已知数据类标签的拟合程度,从而来评估不同算法的性能.对于数据对象xi,分别采用x~签,则准确度可定义如下:其中m是数据集中数据对象的总数目;δ(x,y)为狄垃克函数,当x=y时该值为1,否则该值为0;map(x~射函数,最佳的映射关系可通过Kuhn-Munkres算法[25]得到.采用珦X和C分别表示簇标签和真实类标签,则压缩结果和已知类标签之间的互信息I(珦X;C)可由式(2)求得.本文采用标准化互信息来度量不同算法所得结果和已知簇标签之间的拟合程度,其定义为①http://www.icsi.berkeley.edu/~saenko/projects.html#Page94.2.3实验细节上述6种对比算法及本文所提出的ICSIB算法在运行时均需要事先指定相应的参数.表2给出了表2算法的参数设置、随机初始化及最终结果选择方法k-meansNCutsDRCCaIBCC对称IBITCCICSIB为了发现数据中所蕴含的模式结构,所有的算法在运行时均需要事先指定数据中的簇数目.本文实验中,行簇数设置为数据集内所固有的类别数.k-means算法与NormalizedCuts算法均为单向聚类算法,仅需指定行簇数目即可.这两个算法均运行10次,从中选择准确度最高的作为最终结果.ICSIB算法、ITCC算法[23]、对称IB算法[15]这3个双向压缩算法在文档数据集上的列簇数目为100,在图像数据集上列簇数目为50.另外这3个算法均需要事先给定一个初始划分,不同的初始划分可得到不同的聚类结果.为了确保实验对比的公平性,避免有些算法在较好的初始划分上得到较好的聚类结果,在本文的实验中,首先对数据做10次随机初始划分,然后将这10个初始划分作为这3个算法的初始划分,分别运行10次,每个算法得到10组不同的压缩结果,从中选择目标函数最优的作为最终结果.本文按照文献[24]中的参数选取方法来运行DRCC算法,列聚类簇数目与行聚类簇数目相同,均为数据集中真实类别数,并且采用k-means算法来初始行与列的划分.另外该算法需要指定近邻的数目k及规则化参数λ和μ.为了选取适合的参数值,实验中测试不同的k取值及λ和μ的取值来运行DRCC算法,其中k的取值范围为{1,2,3,…,10},λ和μ的取值范围为{0.1,1,10,100,500,1000},且λ=μ.因此针对不同的参数,在同一个数据集上DRCC算法运行60次,从中选择准确度最大的作为最终结果.aIBCC算法[19]为凝聚层次联合聚类算法,不需要事先给定初始划分.在该算法中,仅需给出平衡参这些算法在运行时参数的选择,随机初始化及最终结果选择方法的详细信息,其中NTCs(theNumberofTrueClusters)表示数据中集固有的簇数目,D代表文档,I代表图像.另外,在本文实验中,行代表数据对象,列代表特征变量.初始化随机随机k-means×相同的随机初始化数γ即可,实验中该参数取值为1.另外对称IB算法[15]与ICSIB算法中的平衡参数β=,此时对称IB算法的目标函数等同于ITCC算法[23]的目标函数.4.3实验结果及性能分析4.3.1文档联合聚类在文档数据集中,行数据代表文档,列代表单词.由于实验中仅知道每篇文档所归属的主题类别,而无法确定单词的分类模式.因此,本文实验也仅给出文档,即行数据的实验评估结果,如表3所示.图4中的(a)和(b)分别给出了算法在所有文档数据集上联合聚类的准确度和标准化互信息的平均值.DataSetsBinary_159.076.452.480.881.479.492.4Binary_255.674.653.069.476.085.688.8Binary_360.884.054.674.885.084.291.4Multi5_147.244.030.031.638.856.690.2Multi5_247.247.027.034.042.673.489.6Multi5_349.442.231.441.045.870.894.0Multi10_133.836.818.424.223.843.656.0Multi10_229.636.419.822.227.243.054.6Multi10_331.431.614.023.626.841.259.8Binary_15.623.90.529.730.732.261.3Binary_21.418.20.911.220.940.749.4Binary_36.236.71.519.039.737.758.1Multi5_124.218.06.26.712.734.974.2Multi5_220.820.93.67.715.044.673.1Multi5_324.614.211.516.619.244.383.2Multi10_123.924.38.79.913.531.146.2Multi10_223.223.78.98.815.131.945.1Multi10_323.120.54.610.216.033.447.2Page10图4文档联合聚类准确度及标准化互信息对比结果表3和图4中的实验结果表明:(1)本文所提出的ICSIB算法在做文档联合聚类时,性能远优于单向聚类的k-means算法和NormalizedCuts算法.(2)ITCC算法和对称IB算法在做联合聚类时,仅关注行、列压缩变量之间的协作关系,却忽略了行、列基层变量之间相互为对方提供的特征信息,而ICSIB算法则从更多角度更充分地利用数据中所蕴含的有价值信息,其性能明显优于上述两种算法.(3)aIBCC算法的层次凝聚优化过程不能充分地优化目标函数,而ICSIB算法的交错的顺序迭代方法则更充分地优化了目标函数,其性能优于aIBCC算法.4.3.2图像无监督模式识别表4给出了各算法在dslr、webcam、amazon这3个图像数据集上的实验结果.该组实验结果表明:相对于其他6个对比算法,ICSIB算法在图像模式识别中同样具有较强的识别能力.DataSetsdslrwebcam40.335.734.739.743.847.348.2amazon23.122.218.820.321.923.225.6dslrwebcam55.652.050.555.759.762.863.6amazon26.124.622.524.526.627.728.44.3.3高层模式提取双向压缩除了得到X到珦X压缩表示p(x~|x)及Y到珟Y的压缩表示p(y~|y)外,也可获取珦X与珟Y之间的高层统计规律p(珦X,珟Y),而传统的单向聚类算法是不能获得该高层统计规律的.在p(珦X,珟Y)中,压缩变量珦X与珟Y之间的关联程度同样反映了双向压缩算法性能的优越程度.ITCC算法和对称IB算法在做联合聚类时目标一致,均力图使行簇与列簇之间高度关联,并采用互信息I(珦X;珟Y)作为双向压缩的目标函数.本文提出的ICSIB算法不单力图使压缩变量之间高度关联,还力图使压缩变量中保留数据集内原有的模式结构.为评估压缩变量之间的关联程度,本文采用互信息I(珦X;珟Y)作为评估标准.表5给出了ICSIB、ITCC及对称IB聚类算法所得双向压缩结果p(珦X,珟Y)中互信息I(珦X;珟Y)的值,该值度量了簇与簇之间的关联程度.从该表中可以看出,比较这3个算法得到的簇与簇之间的关联程度,ICSIB算法最优,其次为对称IB算法.该组实验结果表明:(1)同样的目标函数,对称IB算法中的顺序迭代方法所得到的目标函数值优于ITCC算法中的矩阵逼近方法所得的目标函数值.(2)ICSIB算法在12个数据集中的8个数据集上取得了最优结果,并且ICSIB算法所得I(珦X;珟Y)的平均值最优.(3)ICSIB算法在对称IB算法的目标函数的基DataSetsBinary_1Binary_2Binary_3Multi5_1Multi5_2Multi5_3Multi10_1Multi10_2Multi10_3dslrwebcamamazonaveragePage11础上,将数据中的原模式引入双向压缩中,在关注数据中原有模式的同时,同样可确保压缩变量之间高度关联.4.3.4算法收敛性实验图5给出了ICSIB算法在Multi10_1数据集上每次迭代时ICSIB的目标函数值.该图表明,ICSIB算法的每次迭代都确保了目标函数值的增加,最终得到目标函数的一个局部优值.在Multi10_1数据集上,ICSIB算法经过19次迭代便得到了目标函数的一个局部优值,该算法具有很好的收敛性.在本文其他数据集上的实验中,ICSIB算法同样表现出很好的收敛性,本文在此并不全部给出.5总结对称IB在做双向压缩时,仅关注行、列压缩变量之间相互蕴含的特征模式,却忽略了压缩之前行、列基层变量相互为对方提供的特征信息.行、列压缩变量相互为对方提供的特征信息是被压缩之后的信息,伴随着变量的压缩,数据中必有信息的损失,因此它们并不能完全承载数据中原有的特征信息.针对该问题,在多变量IB方法的基础上,提出交叉对称IB:ICSIB.ICSIB除了关注压缩变量之间相互蕴含的特征模式外,还将压缩之前的原变量引入到对称IB中,使它们协助压缩变量来学习联合分布中的双向压缩模式.ICSIB算法采用交错的“抽取-合并”顺序迭代过程对目标函数进行优化,理论上保证收敛到目标函数的一个局部优解,具有较低时间复杂度.实验结果表明,ICSIB算法得到的数据双向压缩模式更接近于数据中真实的内在模式,并可有效地应用于数据的联合聚类中.
