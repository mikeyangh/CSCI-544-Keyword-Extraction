Page1一种面向数据流程序的软件流水并行化方法魏海涛1)于俊清1),2)余华飞1)秦明康1)1)(华中科技大学计算机科学与技术学院武汉430074)2)(华中科技大学网络与计算中心武汉430074)摘要数据流编程被广泛应用于多媒体、图像处理和信号处理等领域.多核处理器为数据流程序提供了强大并行计算资源,如何利用多核处理器的并行性以提高数据流程序性能,对满足媒体处理等实时性需求具有重要意义.基于多核处理器提出了一种面向数据流程序的软件流水并行化方法,利用整数线性规划理论对软件流水中的计算、通信资源和流水线执行阶段等进行统一的形式化建模,在最大化流水线计算速率的同时最小化通信开销;同时对存储资源进行了形式化建模,提高存储访问的性能.通过设计数据流编程语言DFBrook,在Cell处理器实现了上述方法.实验结果表明,该软件流水并行方法比其它方法在提高数据流程序性能的同时,降低了通信开销.关键词数据流程序;多核处理器;软件流水;并行1引言多核处理器已经成为主流和工业标准,如Sony、Toshiba和IBM联合开发的Cell[1]多核处理器集成9个核;Nvidia公司开发的GeForce8800GPU[2]集成16个流处理器,每个流处理器包含8个处理单元;Sun公司开发的Niagara[3]集成8个核;Intel公司和AMD公司将推出16核的x86多核处理器.多核处理器为应用提供了强大的并行计算能力,但同Page2时将其复杂的数据划分、存储结构和通信机制等底层设计暴露给了编程人员,从而给编程带来了巨大的挑战.传统的编程模型,如C、C++和FOR-TRAN已经无法很好地适应多核处理器的架构.数据流编程(DataFlowProgramming)模型将媒体应用特性与程序设计语言相结合,在简化编程的同时,为编译器在多核处理器下的优化提供了大量的并行性,目前受到广泛的关注.数据流编程语言如StreamIt[4]、CUDA[2]和SPUR[5]等一般都是基于同步数据流模型(Syn-chronousDataFlow,SDF)[6].在该模型中,每个actor是一个运算过程,代表一系列的指令,数据从输入队列进入,经过actor的处理后到输出队列.actor的每次运行由输入队列上的数据到达速率来决定,只要输入数据队列中有数据,且输出队列有空闲的存储,actor就可以运行.一般来说,一个数据流程序由多个actor和连接actor的数据队列组成.actor一般被表示为一个函数,数据队列采用先进先出方式组织.每个actor有相应的触发规则,当规则满足时,该actor被触发,读取输入队列上的数据,产生输出数据.同步数据流模型的这种机制为编译器提供了并行优化的机会.软件流水是一种开发数据流程序并行性的有效方法.通过将数据流程序看作一个循环,不同迭代中的actor能够在流水线中实现重叠执行.然而,获得的性能常会被处理器核间的通信和同步开销所抵消.同时,处理器核、通信带宽和片上存储等系统资源限制将会带来流水线的停滞.软件流水的性能由每次启动迭代的时间来衡量,一个具有最小启动时间的调度称为最优计算速率的软件流水调度.因此,如何设计软件流水调度方法,在满足系统资源受限的情况下,获得最优计算速率的同时最小化通信开销是本文研究的主要问题.本文基于多核处理器提出了一种面向数据流程序的软件流水并行化方法,利用整数线性规划理论对软件流水中的计算、通信资源、存储资源以及流水线执行的阶段等进行统一的形式化建模,在满足最大化流水线计算速率的同时最小化通信开销.通过设计数据流编程语言DFBrook,在Cell处理器上现了上述方法.实验结果表明,本文提出的方法比现有的其他方法在较大地提高性能的同时减小了开销.本文第2节介绍相关性研究工作;第3节介绍DFBrook数据流编程语言;第4节详细讨论数据流程序的软件流水调度方法;第5节给出相应的实验结果;第6节对论文进行总结.2相关工作软件流水最初在超标量和超长指令字计算机中被提出来,用于开发指令级的并行[7-8].作为一种开发并行性的方法,软件流水开始逐渐被用于开发数据流程序的并行性.Kudlur等人采用了的软件流水线调度方法实现了StreamIt在多核处理器下的并行优化[9].该方法首先对计算节点进行划分,然后采用阶段赋值算法来实现流水线阶段调度,但是该方法只对计算资源的调度进行了研究,没有考虑到通信和存储资源.Choi等人在Kudlur的基础上提出了在嵌入式系统下的存储受限软件流水调度[10],但是该方法对解的集合采用保守估计,即假设了所有相连的计算节点都被调度到不同的处理器,因此过多地计算量存储的开销,同时,该方法也没有考虑通信开销.Govindarajan等人采用线性规划研究了数据流程序在GPU上的调度框架[11],但是该模型只是针对共享存储结构的处理器,针对分布式存储结构无法使用.针对上述问题,本文基于多核处理器提出了一种面向数据流程序的软件流水并行化方法,利用整数线性规划理论对软件流水中的计算、通信资源、存储资源以及流水线执行的阶段等进行统一的形式化建模,在取得最优化流水线计算速率的同时最小化处理器间的通信开销.同时对存储受限的多核处理器进行了形式化建模,以提高存储访问的性能.3DFBrook数据流语言与编译DFBrook在语义上对Brook[12]语言进行了数据流扩展.DFBrook程序由普通的C代码和数据流代码组成新的数据类型,表达了数据流模型中的数据队列的语义.数据流类型只能限制在核函数中使用.流操作符则是一组对流类型数据进行特定运算操作的集合,用于与其它数据类型之间的转换.图1给出了一个DFBrook的例子程序和相应的SDF图.核函数KA具有2个输入流和1个输出流,每次执行消耗从输入流vec_a和vec_b中各消耗8个数据单位,向输出流vec_result中生成8个数据单位,其中vec_a和vec_b是只读的,vec_result是只写的.在main函数中,streamfor语句块对应DFBrook程序的数据流代码,流类型数据由引用参Page3数从一个kernel函数传递到另一个kernel函数.streamRead和streamWrite操作符是两个特殊的流操作符,streamRead用于将外部C代码的数据“读入”流,指明一个流的起始点,因此streamRead只有第3个参数用于指定输出码率;streamWrite将流“写回”外部C代码,指明流的终止,因此stre-amWrite只有输入数据流.在例子中,streamRead将数据从外部数组data_matrix和data_vector读入数据流中,该数据流作为kernel函数KA的输入进行处理后结果输出到tempmv流,tempmv流被kernel函数KB消耗产生result流,最后result流被写回到数组data_result中.这些流在kernel函数间以队列的方式进行传输,队列的长度由编译器指定.kernelvoidKA(streamfloatvec_a〈8〉,streamfloatvec_b〈8〉,outstreamfloatvec_result〈8〉){…}kernelvoidKB(streamfloatvec〈8〉,outstreamfloatresult〈1〉){…}main(){…floatdata_matrix[64];floatdata_vector[8];streamfloatmatrix〈〉,vector〈〉;streamfloattempmv〈〉,result〈〉;streamfor(…){streamRead(matrix,data_matrix,64);streamRead(vector,data_vector,8);KA(matrix,vector,tempmv);KB(tempmv,result);streamWrite(result,data_result,8);}}图1数据流编程语言DFBrook及其对应的SDF编译器首先对DFBrook程序进行语法和语义检查,然后采用数据流分析理论,建立各个核函数和流操作符之间的依赖关系,构造中间表示———SDF;同步数据流图的每个节点代表一个核函数或者流操作符,图的每条有向边代表核函数之间的数据流通路,边上的两个权值代表起点核函数的输出流速率和终点的输入流速率,即生产和消耗数据的速率;流水线调度模块对同步数据流图进行分析,将图中的节点调度到目标处理器上的处理核上,同时为调度到不同处理器核上的节点间生产DMA(DirectedMemoryAccess),实现软件流水调度;最后生成多线程目标代码.本文采用的目标结构为基于加速器的多核处理器结构如Cell,每个处理器核只能访问自己私有的片上局部存储,通过异步DMA来实现数据的通信.4数据流程序的软件流水并行4.1多核处理器下的软件流水调度在基于多核处理器的软件流水中,一次循环迭代作为一个任务,被分成若干阶段,各个阶段在不同的处理器上执行,当一个处理器完成了它负责的阶段后,结果就作为输入传送到流水线中的下一个处理器.图2给出了一个基于多核处理器的软件流水调度的例子,图左边为一个数据依赖图,K1执行后利用DMA,将数据传给K2执行,图右边为对应的流水线的调度表.计算任务被分为3个阶段,第0位置的处理器核完成处理后,结果用DMA传输到第1位置的处理器核,同时第0位置的处理器核开始处理第2个任务,即下一次循环迭代.按照这种方式,流水线被逐渐填满,处理器逐渐进入忙状态.在流水线满的状态下,不同的循环迭代在流水线中并行执行.在软件流水线中,相邻两次循环迭代(任务)开始进入流水线的时间间隔称为启动间隔(InitiationInterval,II),启动间隔的长短代表了流水线的速度,启动间隔越小,流水线的执行速度越快.流水线进入充满状态后,单位时间内流水线完成循环迭代(任务)的次数称为吞吐率(Throughput,T),吞吐率代表了流水线的响应速率,吞吐率越大,流水线的响应时间越短,吞吐率T和启动间隔II满足以下公式:II=1/T.4.2通信最小化的最优化软件流水调度本节设计的软件流水调度的目的是根据任务的依赖关系和当前系统中资源的情况,构造一个软件流水调度,在最小化启动间隔II的基础上,最小化通信开销.图3给出了两种调度方法,方法1不考虑通信Page4开销,首先将节点A0和A1分别调度到处理器SPE0和SPE1,节点B0、B1和B2被调度到处理器SPE2;然后检查每一条边,如果边的两个节点被调度到不同处理器,那么将该边调度到对应的DMA;最后对每个实例和边赋值阶段(Stage),节点A0和A1被赋值阶段Stage0,节点B0、B1和B2被赋值阶段Stage2,4个DMA被赋值阶段Stage1.低通信开销软件流水调度方法将处理器调度、DMA和阶段赋值作为一个整体来考虑.如图3(b)所示,A0和B0被调度到处理器SPE0,B1被调度到处理器SPE1,A1和B2被调度到处理器SPE2.由于调度到相同的处理器上的实例共享存储,无需通信开销,因此,A0和B0之间以及A1和B2之间的通信开销被消除.图3(c)和(d)比较了以上两种软件流水调度方法的执行结果.方法1得到的启动间隔II为46个单位,总通信开销为46个单位;低通信开销软件流水调度方法得到的启动间隔II为30个单位,总通信开销为22个单位,因此,该方法在取得高计算速度的同时降低了通信的开销.以上例子阐述的软件流水调度问题可以被形式化为一个整数线性规划问题.考虑一个数据流程序的数据流图中间表达G=(V,E),V为节点集,E为边集.对于每条边,为了满足产生的数据个数与消耗的数据个数相等,起点和终点所需要执行的最小次数,称为该节点的重复运行次数,所有节点的重复执行次数构成了重复执行向量狉G.通过将G中的节点进行狉G次展开,可以得到数据流图对应的数据依赖图Gd=(Vd,Ed),具体方法可参见文献[13].以下将对数据依赖图Gd进行形式化建模,Gd中的每个节点由v∈Vd表示,每条边由(u,v)∈Ed表示,其中u∈Vd.令P为数据处理器的集合,P={0,1,…,Pmax-1},Pmax为数据处理器的个数.为每个节点v定义0-1变量av,p,表示节点v是否被调度到处理器p上.处理器资源的限制被形式化为等式(1),该限制条件确保了每个节点只能被调度到一个处理器上.令()workv表示函数节点v的执行时间,II表示软件流水的启动间隔,不等式(2)形式化给出了以下限制条件:调度到一个处理器上的总工作负载必须在给定的启动间隔II内完成.∑v∈Vd以上两个限制条件给出了处理器的调度限制,没有考虑数据的依赖限制,即没有考虑图中边的限制条件.如果两个相连接的节点被调度到不同的处理器上,将需要DMA传输数据的操作.DMA传输是双向的,既可以由源处理器发起,也可以由目的处理器发起.考虑大多系统处理器的性能设计,采用由目的处理器发起DMA.对于边(u,v)∈Ed,定义0-1变量du,v,p来表示边(u,v)∈Ed是否被调度到DMAp上.变量du,v,p为1当且仅当v被调度到处理器DPp,同时u被调度到其他不同的处理器上.不等式组(3)确保了当两个相互连接的节点被调度到同一个处理器上,将不需要DMA进行数据传输.此处约定一对相连接的节点之间的DMA传Page5输总是由目的节点所调度到的处理器发起.令Commun(u,v)表示两个节点u和v之间的数据传输负载,流水线的计算速率同样也受所有的DMA的数据传输负载的限制.限制不等式(4)确保了在给定一个DMA上的数据传输总负载不大于给定的启动间隔II.不等式(1)~(4)只对节点在各个处理器上的调度以及相应的数据传输在DMA的调度给出了限制条件,即只在空间维上对软件流水调度进行了形式化限制.为了在时间维上调度节点和边,引入Stage概念来描述节点和边在时间上的调度.定义整数变量svv表示节点v被赋值的阶段号;定义整数变量seu,v表示边(u,v)∈Ed被赋值的阶段号.不等式(5)和(6)给出了软件流水调度在时间上的限制条件.对于给定的边(u,v),目的节点的阶段号应该在源节点后面,因为在时间上,目的节点必须在源节点运行后运行,即svvsvu表示了源节点和目的节点之间的数据依赖关系.如果u和v被调度到不同处理器上,两者需要DMA操作进行数据传输,那么从u到v的数据传输必须分配一个单独的阶段号seu,v,节点u、v和DMA必须满足不等式svu<seu,v<svv.由于每个节点只能调度到一个处理器,即av,p只能对于某一个处理器p取到1,从不等式(3)可以得知:对于给定的一条边,要么只被赋值给一个DMA,要么不被赋值给任何DMA.当u和v被调度到不同处理器上,求和式∑Pmax-1v被调度到相同处理器上,值为0.当源节点u和目的节点v被调度到不同处理器上时,不等式(5)确保了DMA传输的阶段号至少在源节点u后面一个阶段;类似的,不等式(6)确保了目的节点v的阶段号至少在DMA后面一个阶段.当节点u和节点v被调度到相同处理器上时,不等式svvsvu保持,此时du,v,p无意义.min∑Pmax-1函数(7)描述了所有节点之间的总通信开销,规划的目标函数是最小化通信开销.式(1)~式(7)为通信最小化的最优化流水调度(CommunicationMinimizedRate-Optimalscheduling,CMRO)问题提供了精确的整数线性规划形式化描述.该模型精确描述了软件流水调度中的计算资源和通信资源,在保证高计算速率的同时,最小化通信开销,提高软件流水的性能.因为最小化通信是在最优化流水的前提下,因此首先必须求得满足限制条件(1)~(6)的最小化IImin,将式(7)用minII代替,可以得到最优化流水调度RO(RateOptimal).通过求解RO问题,可以得到IImin,以此作为CMRO问题的输入,从而求出最小化通信开销.4.3内存限制的流水调度4.3.1存储分配机制这里采用文献[10]的方法来阐述节点对(u,v)的两种缓存分配机制.如图4(a)所示,u,v两个节点被调度到同一处理器P0上,被赋值的阶段分别为0和1,为了保证流水线的重叠执行,此时分配的缓存个数为3;在图4(b)中,u,v两个节调度不同处理器上,被赋值的阶段分别为0和4,此时,u所在的处理器P0和v所在的处理器P1分别分配3个缓存.这是因为调度到不同处理器上的节点对需要DMA进行数据传输,在软件流水的执行中,u,v的执行以及两者间DMA的传输在时间上是重叠的,因此都需要缓存进行保存中间结果.4.3.2形式化建模定义生产者-消费者对为普通的相连的计算节点对或者DMA-计算节点对;定义缓存组为在软件流水调度中,生产者-消费者对所分配的缓存队列.针对以上两种缓存分配机制有:(1)当两个相连计算节点对(u,v)被调度到同一处理器上,缓存组被共享,节点u的输出数据存入缓存组作为节点v的输入数据,缓存组的大小计算如下:svv-svu+1;Page6(2)当两个相连计算节点对(u,v)被调度到不同处理器P0和P1上,分配在处理器P0的存储上u的输出数据,需要DMA传输到处理器DP1的存储上,作为v的输入数据.在DP0上用于缓存u输出数据的缓存组大小由seu,v-svu+1计算;在P1上用于缓存v输入数据的缓存组的大小由svv-seu,v+1计算.不等式(8)给出了处理器p的内存限制的形式化描述,不等式左边是处理器p上的存储消耗总和.其中,Buffer(u,v)为程序在一次迭代执行中,边(u,v)所需要的缓存大小,可以由边上的数据传输个数和相应的数据类型大小的乘积得到.其中,M1=seu,v-svu+1,M2=svv-seu,v+1,M3=svv-svu+1,表示各种缓存组大小所关联的阶段差.∑(u,v)∈EdM3)du,v,p]×Buffer(u,v)Memp,通过把式(8)中的M1、M2和M3用数值常量替换,限制条件则从非线性转换为线性.从3.2节可知当相连两个节点u和v被调度到不同的处理器上,那么DMA和u的阶段差至少为1,同样,v和DMA的阶段差至少也为1,即seu,v-svu和svv-seu,v的最小值都为1;如果节点u和v被调度到相同的处理器上,那么两者至少是在同一个阶段,即svv-svu的最小值为0.因此,得到估计值:M1=2,M2=2,M3=1,限制条件(8)可以被简化为(9).∑(u,v)∈Ed模型的目标函数为最优化流水线吞吐率,即最小化启动时间,如式子(10)所示.[2au,p-av,p+3du,v,p]×Buffer(u,v)Memp,限制条件(1)~(7)、(9)和目标函数(10)给出了数据流程序在内存限制多核处理器下的软件流水调度模型的整数规划问题形式化表达.因此可以采用经典的分支界定法或割平面法进行求解.虽然整数线性规划求解是NP的,但是当前已经有许多高效的产品级的求解器,如CPLEX[14]混合整数规划求解器(MixedIntegerProgrammingSolver,MIPSolver)可以较快地求解整数线性规划问题,该MIP采用分支界定算法来求解,并且可以通过设定最优解的精度和求解的时间限制来加速求解的过程.由于限制条件(9)是对内存消耗的一个最小估计,因此,真实的内存消耗可能会大于限制条件.我们通过对DMA的阶段号进行动态调整,从而平衡各个存储之间的内存使用量,以消除内存溢出.首先,计算每个处理器的真实存储消耗,针对存储是否溢出将处理器分为溢出和非溢出两类;其次对调度到溢出处理器上的节点的DMA进行查询,如果有DMA可以调整,使得该处理器的存储消耗减小,则调整DMA阶段,直到消除所有存储溢出或者没有可以调整的DMA.5实验结果与分析在第3节中描述的DFBrook数据流编译系统中,实现了上述的软件流水调度方法.本节通过实验对该软件流水调度方法进行性能评价,同时通过与其它方法的比较来验证方法的有效性.5.1测试平台和实验方法编译前端对DFBrook语言进行语法和语义分析后,采用数据流分析,生成数据流图,调度程序以数据流图为输入,按照上述的模型进行建模,求解出调度结果,用于进一步的优化和代码生成.实验的硬件测试平台为PlayStation3,配有一个Cell处理器(6个SPE可用)和256MB的主存.Cell处理器采用主从式的组织结构,集成了一个控制处理器单元(PowerPCProcessorElement,PPE)和8个数据处理器单元(SynergisticProcessorElement,SPE),每个SPE都配有一个256K的局部存储器LS和DMA数据传输引擎,PPE和SPE通过片上环形网络互连,共享片外存储,PPE和SPE可以通过消息同步.实验采用IBM的CellSDK3.0作为DFBrook编译器软件支持,采用的本地编译器为ppe-gcc和spe-gcc.整数线性规划采用的是CPLEX的混合整数规划求解器MIPSolver.DFBrook程序中存在3种通信模式:streamRead到kernel函数、kernel函数之间以及kernel函数到streamWrite,为了测试模型中的通信量参数,使用1个PPE和2个SPE测量每个kernel函数、数据流操作符以及每个DMA传输量.实验采用表1中的测试程序集来评价调度模型的性能.表中对各个测试程序的特征:如kernel节点个数、读写节点个数以及通信的边条数进行了详细描述.大多数的测试程序来自于多媒体处理领域,如:Gausslap测试程序实现了高斯-拉普拉斯算子进Page7行边缘检测算法;histogram实现了并行化的直方图算法;shortEnergy实现了声音短时能量特征算法的DFBrook程序;averageMotion实现了视频处理中的运动向量提取算法.测试程序DCTFFTGausslap28654高斯拉普拉斯算子histogram11215图像直方图imagesmooth28654图像平滑MatrixMult24337分块矩阵乘mergesort17224归并排序shortEnergy20226音频短时能量averageMotion28343视频平均运动向量5.2可扩展性实验首先通过加速比来评价模型的可扩展性,加速比采用以下方法计算:当前程序Programi在p个SPE下取得的加速比由当前程序在1个SPE调度获得的启动间隔II与当前p个SPE调度获得的启动间隔II相比得到.图5给出了表1各个测试程序在不同个数SPE下的加速比.结果显示,调度方法对绝大多数程序取得了近似线性加速比.FFT和histogram在SPE个数大于4和5时,程序的加速比不随处理器增加而提高,这是由于这两个程序只有有限个数的kernel函数,即问题的规模较小,无法带来更多的加速比提升.如表1所示,FFT只有10个kernel节点(表中kernel节点数+读写节点数=总节点数),histo-gram只有9个kernel节点.通过循环展开数据流图来增加并行性(问题的规模)可以进一步地提高加速比.与上述两个程序相比,DCT和MatrixMult程序分别有33个和21个kernel节点,因此,取得了近似线性的加速比.同理,Gausslap、imagesmooth和av-erageMotion也取得近似线性加速比.图5表1中测试程序的加速比(以II的比率计算)5.3性能比较与分析本节将本文的通信最小化的最优化流水调度方法CMRO与经典的List表调度方法[7-8]、周期可行的并行调度算法[6](PeriodicAdmissibleParallelSchedule,PAPS)和最优计算速率调度[9]3种方法(Rate-OptimalSchedule,RO)进行比较.表调度算法是广泛应用于传统指令级软件流水调度中的一种方法[7-8].实验中,通过修改经典的基于优先级的表调度算法[15],实现了基于List方法的软件流水调度.具体方法如下:首先采用List调度构造节点到处理器上的调度,实现空间上的调度;然后采用Kudlur和Mahlke提出的阶段赋值算法[9],对每个节点和数据传输进行阶段赋值.PAPS算法不考虑实例在不同迭代中的重叠执行,循环的每次迭代为一个基本调度块,一旦调度结果确定以后,后面的迭代采用相同的调度方法.RO的目标函数是最优化计算速率,即最小化II.实验中,通过修改MCRO整数线性规划调度模型中的目标函数来实现RO调度.图6给出了本文的通信最小化的最优化软件流水调度方法和表调度及PAPS调度在启动间隔II上的比较结果.从图中可以看出,低通信开销调度方法比表调度算法有较大的性能提高.对于具有大计算量的测试程序,如DCT、shortEnergy和average-Motion,MCRO调度方法比表调度在软件流水线的计算速率上取得了17.8%~24.7%的提高.MCRO调度方法比PAPS调度算法在软件流水计算速率上,平均有47%较大的性能提高.特别,对于FFT和mergesort两个程序,MCRO调度比PAPS调度在流水线计算速率上,分别提高了58.9%和53.9%.对于计算速率提高最小的imagesmooth程序.MCRO调度的解是通过逐步增加II,求解规划问题,直到使得模型具有最优解停止,MCRO方法得到的II是最小的II.因此,MCRO调度和RO调度具有相同的计算速率.图6本文调度方法与其它方法在II上的比较Page8图7给出了本文的通信最小化的最优化软件流水调度与表调度、PAPS调度和RO调度在通信开销上的比较.MCRO调度比List调度在通信开销上平均降低了约23%,而对于具有高通信量的程序如FFT和mergesort,MCRO调度方法比List调度在通信开销上降低了31.8%~40.7%;MCRO调度对于大部分程序比PAPS调度减小了10%~20%;MCRO调度方法比RO调度算法在通信开销上,有7%~40.9%的较大降低,平均减小了21.7%的通信开销.图7本文调度与其它方法在通信开销上的比较5.4存储性能分析与比较以下将分析存储限制对流水线性能的影响.存储限制软件流水调度的目的是最大化地利用片上局表2本文的存储限制调度与文献[10]在求得解上的比较测试程序DCTFFTGausslap√histogram√imagesmooth√MatrixMult√mergesort√shortEnergy√averageMotion√另一方面,将从求得解上来说明本文的存储限制软件流水调度的有效性.表2给出了本文的存储限制调度与文献[10]在每个不同的片上存储大小情况下求得解的情况比较.在表中,符号“”表示本文的调度方法与文献[10]的方法都能得到解;符号“√”表示本文的调度方法可以求得解,但文献[10]的方法无法求得解.从表中可以看出,文本的调度方法可以求得文献[10]无法求得的解.这是因为文献[10]做了假设:数据流图中相连的节点调度到不同的处理器上,因此过多的计算了存储的开销.本文的部存储来减小程序对主存的访问延迟.对图8给出了本文4.3节中存储限制的软件流水调度对程序性能的改进.这里,处理器的个数为固定值,通过逐渐减小片上存储的容量来记录流水线的启动时间.片上存储从能够容纳全部程序数据的MaxMem开始,逐渐减小,程序的性能也随之下降.这是因为随着存储减小,调度器将尽可能地把节点调度到相同的处理器上来减小存储开销,从而导致处理器上的工作负载加大.另一方面当片上存储耗尽,将引入新的DMA将数据放到主存,从而带来DMA的开销.图8本文的存储限制调度对程序性能的改进片上存储容量MinMem+3(MaxMem-方法避免了这个问题,从而能求得更有效的解.6总结数据流程序为多核处理器提供了并行优化的机会,但处理器间的通信和同步给程序带来较大的性能开销.本文基于多核处理器提出了一种面向数据流程序的软件流水并行方法,在实现最大化流水线吞吐率最小化通信开销,同时针对内存受限系统提出了存储限制的流水线调度方法来改进内存的访问Page9性能.实验结果表明了方法的有效性.本文提出的方法主要针对分布式存储结构的多核处理器架构,如何设计面向共享和层次性存储结构的软件流水并行方法是将来需要进一步研究的工作.
