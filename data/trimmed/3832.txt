Page1面向通用HPC的高性能DSP设计权衡张凯陈书明王耀华宁希(国防科学技术大学计算机学院长沙410073)摘要GPU由于其计算能力高达数TFLOPS,被高性能计算领域用于加速并行运算.但GPU较低的峰值性能利用率和功耗效率,已经成为了系统性能进一步提升的瓶颈.为了解决这个问题,作者开始研究将高性能DSP用于通用高性能计算领域.为了高效支撑通用高性能计算,文中提出了高性能DSP的结构框架,并通过映射GotoBLAS库到该结构上,建立了GEMM在该结构上的性能模型.作者研究了影响GEMM效率的主要因素,包括性能、存储层次、核的大小以及核的数量.文中总结了一些有指导意义的结论用于构建面向通用高性能计算的高效DSP.实验结果表明,通过尽可能少的硬件代价,可以在TFLOPSDSP上获得接近峰值的性能.关键词高性能计算;矩阵乘法;数字信号处理器;模型;设计权衡1引言超级计算机的性能已经达到了每秒万万亿次的量级①,使得单片万亿次浮点操作(TFLOPS)级的高性能微处理器在HPC领域得到了广泛的应用,并随着高性能计算(HPC)应用需求的不断增长,且对其需求呈现持续增长的趋势.以计算为核心任Page2务的图形处理器(GPU)性能已经超过单片TFLOPS,但大量GPU堆叠构成的系统效率不高.在Top500中排名前10的机器,以GPU为加速结点构成的HPC系统,其峰值性能利用率不超过60%,系统功耗效率低于1GFLOPS/W.随着系统性能需求的持续增长,功耗问题已经逐渐成为了一个显著的拦路石.Esmaeilzadeh等人[1]认为,未来芯片不得不伴随着“darksilicon”现象,即在22nm工艺下,芯片的21%必须被关掉,在8nm工艺库下则必须关掉50%.因此,要在适度功耗约束下进一步提高HPC系统的性能,关键是要提高节点芯片的峰值性能利用率和功耗效率.数字信号处理器(DSP)一直是嵌入式系统的核心组成部分.它的主要优势是高功耗效率、低价格和易编程性,这些特点是HPC领域非常青睐的,易编程性是指相对于GPU而言的.基于GPU加速的CPU-GPU异构系统中,CPU作为主设备,将需要加速计算的数据传递给GPU来完成加速计算,属于一种主从式的编程模型.而DSP可以直接作为主设备独立完成任务,在DSP上可以轻松实现类CPU式的编程.高功耗效率的优势给DSP迈向HPC领域奠定了良好的基础,但将其用于HPC的一个重要问题是:能否在高功耗效率的优势下提供比较高的计算性能,并在执行通用HPC任务时有较高的计算效率.因此,本文开始探索能够很好地用于通用HPC领域的高性能DSP的设计权衡.本文采用多核和单指令流多数据流(SIMD)技术作为高性能DSP的基本框架.这两种技术已经被广泛应用于高性能低功耗的DSP设计中[2-4].多核技术是在遇到“频率墙”和“功耗墙”问题时通过开发有效的线程级并行来提高处理器性能的重要手段.SIMD技术通过一条指令同时处理多个数据来有效开发数据级并行性[4].由于控制逻辑简单,它能够在较低的功耗下提供较高的性能.因此,我们采取SIMD和多核技术作为构建用于支撑高效HPC的高性能DSP的基本框架.HPC领域包含很多应用.稠密线性代数库是通用HPC领域的一个重要应用,很多计算问题的求解最终都转化为稠密线性代数问题,如果一个结构无法有效地支撑稠密线性代数库,就不可能有效支撑其它高性能应用[3].通用矩阵乘法(GEMM)是稠密线性代数库最重要的库函数之一.稠密线性代数库的很多问题又被投射到GEMM上[5].GEMM是决定其它代数库函数能否很好地映射到一个处理器的代表[6-7].因此,如何在SIMD和多核框架构成的DSP上高效支撑GEMM,是决定DSP能否用于通用HPC的关键.要解决好这个问题,必须对DSP的体系结构进行仔细的设计权衡.本文构建了基于SIMD和多核技术框架构成的高性能DSP上GEMM的性能模型,研究SIMD宽度、功能单元的流水线深度、多核数目以及存储容量和存储带宽等因素对GEMM的性能和效率的影响,得到了一些有指导意义的结论,为构建面向通用HPC高效的高性能DSP提供有效的设计指导.2相关工作近年来,GPU成为了加速密集型运算的流行选择,很多超级计算机通过GPU来加速GEMM.GPU通过大量SIMD单元可以有效加速GEMM[8].Nath等人[8]优化的GEMM算法在TeslaC2050GPU上双精度GEMM(DGEMM)性能达到300GFLOPS,峰值性能利用率为58%.当大量GPU作为加速通用HPC的节点构成系统时,峰值性能利用率会进一步降低.通过进一步优化,Tan等人[9]将DGEMM性能提升到362GFLOPS,峰值性能利用率提升到70%.Li等人[10]采用了分离的写回操作和双缓存算法等技术,更好地隐藏了写回操作延迟,通过将4级软件流水细化为5级,在RadeonHD5970GPU上获得了758GFLOPS的性能,峰值性能利用率高达82%,他们还提出了面向GPU的体系结构改进建议.面向GEMM的专用硬件加速结构研究主要包括systolic阵列结构和FPGA.研究人员最开始研究使用systolic阵列结构加速矩阵乘法[11-13].由于systolic阵列的数据通路专用性,在灵活性和通用性上有很大损失.随着应用对灵活性和可扩展性需求的不断提高,研究者们开始用FPGA加速矩阵运算.Dou等人[14]提出了一种并行分块矩阵乘算法和相应的可扩展线性阵列结构,该结构通过其良好的可扩展性能够加速任意规模的矩阵乘.Lopes等人[15]提出了一种适合小到中规模的矩阵乘法的并行结构,性能达到35GFLOPS,高于当时的高端CPU.还有一些研究主要关注面积、存储容量、计算延迟和带宽需求与能耗和性能之间的相互影响和折中[7,16-17].Goto等人[18]分析了在通用CPU上影响GEMM性能的主要因素,为通用CPU的设计提供Page3了一定的设计指导.分块计算和分块存储是Goto等人的核心思想,分块后的计算使用高效的Kernel函数来提高性能.本文分析了在SIMD和多核框架的DSP上影响GEMM性能的主要因素,将Goto等人的思想映射到该结构框架中,并在该框架上建立了DGEMM的性能模型,从而为面向通用HPC高效的高性能DSP提供有效的设计指导.3高性能DSP上的GEMM性能模型高效的支撑BLAS库通常是检验一个新的HPC结构的关键.GEMM是level-3BLAS的一个关键程序[5].GotoBLAS是当今BLAS最流行和高效的实现之一,被广泛地应用于HPC领域.下文将根据Goto等人[18]的基本思想,分析GEMM在高性能DSP上的映射和性能模型.3.1GEMM到高性能DSP上的映射3.1.1高性能DSP的基本框架SIMD技术可以使多个运算单元在一条指令的控制下对多个数据并行操作,由于其硬件代价低,控制简单,可以在较低的功耗下获得较高的性能,被广泛地应用于高性能低功耗处理器的设计,如AnySP[4]、SODA[19]、Cell[20]、VT[21]等,它们都由一个简单的标量核用于控制和通信等任务,向量SIMD单元用于加速并行计算.本文提出如图1所示的结构,采用SIMD和多核技术来构建TFLOPS级DSP.每个单核由一个SPE和多个VPE组成,SPE用于指令流的控制、对向量单元的配置以及通信任务,多个VPE组成的向量SIMD单元用于加速并行运算,AdderTree和ShuffleNetwork分别用于VPE间的数据规约加和重排.VPE为多功能单元组成的VLIW结构.我们将多核DSP的存储层次抽象为3层,每个核拥有自己的Local-Memory(LM)作为L1,多个核共享一个Shared-Memory(SM)作为L2.片外主存(MM)作为L3.LM为CacheRAM结构.考虑到大容量Cache的设计代价非常高,我们将SM设计为便签存储器.可以通过预取技术实现数据从SM到LM中的自动搬移.DMA用于控制片外数据到SM中的搬移.程序员可以通过SPE配置DMA来自动完成数据搬移操作,以隐藏通信开销.3.1.2GEMM到TFLOPSDSP上的算法映射GotoBLAS库考虑了不同存储层次的大小和效率对GEMM性能的影响.GEMM通常采取分块计算的方法,分块方法有3种.根据Goto等人的思想,考虑运算犆=犃犅+犆,最高效的分块方法是将犃和犅分别按列和按行划分成多个Panel(其中一个维度比较小),分块之后的计算过程为在实现犃p犅p时,犃p通常被划分为多个block(犃0,p,犃1,p,…,犃M-1,p),block是指两个维度都比较小,每个block分别和犅p相乘,每个犃i,p的行数目为m,列数目为k.此时,在实现犃i,p犅p时,犅p被划分为多个Panel(犅p,0,犅p,1,…,犅p,N-1),以保证犃i,p和犅p,j能放在最近一级的RAM中.犆i,j+=犃i,p犅p,j就作为实现GEMM的最内层循环,这个过程被称为GEBP,直接决定GEMM的性能.根据以上分析,我们在多核DSP上对GEMM进行算法映射.如图2所示,每个核所需的犃i,p和犅p,j存放在LM中,然后搬移到本地寄存Page4器中完成犆i,j+=犃i,p犅p,j.SM中存放多个核需要的犃i,p和犅p,j的副本,每一块犅p,j被所有核共享使用并且只被每个核使用一次.犃和犅存放在片外主存中.访问主存的数据带宽为BW3,访问SM的数据带宽为BW2.LM与本地寄存器之间的数据带宽BW1假设为足够大.3.2GEBP的实现在每个核上高效实现犆i,j=犃i,p犅p,j+犆i,j,关键是充分利用SIMD单元的并行计算能力.下面考虑运算犆=犃犅,以SIMD方式实现犆=犃犅主要包括如图3所示的两种算法.在一个包含4个VPE的SIMD处理器上,以犃和犅均为4×4为例,算法1如图3(a)所示,犃的一行数据的4个元素和犅的一列数据的4个元素分别在4个VPE上相乘,4个VPE产生的乘法结果通过向量单元间的加法规约树进行相加,得到犆矩阵的一个元素(ci,j).算法2如图3(b)所示,犃的第i行数据的第1个元素被广播到4个VPE中,该元素与犅的第1行数据分别在4个VPE上相乘,每个VPE上的乘法结果保存在MAC部件的累加器中.然后犃的第i行数据的下一个元素再被广播到4个VPE中,分别与犅的下一行数据在4个VPE上相乘,每个VPE的乘法结果与MAC部件累加器中的数据进行累加操作,再将累加的结果写回累加器.如此递推进行,直至犃的第i行数据全部用完,4个VPE的MAC部件的累加器中存放的结果即为ci,1、ci,2、ci,3、ci,4.在DSP上实现以上两种算法时各有优缺点.算法1的优点是算法的执行过程与运算的数学定义一致,编程直观、方便.缺点是对数据的访存效率不高,主要是由于算法1要对犅矩阵的数据进行列访问.如果在访问DDR时实现对数据的列访问,访问DDR的效率不高.如果在访问片内RAM时实现列访问,由于片上RAM无法提供过多的访问端口,则造成严重的RAM体冲突,导致访问RAM的效率不高.软件流水技术通过充分利用功能单元来开发循环遍之间的并行性,是提高DSP的程序执行效率的一个常用方法.软件流水需要增加额外的指令来进行循环填充,这会增加外层循环的开销.对算法1进行软件流水后增加的外层循环开销和浮点乘法以及浮点规约加法的延迟时钟周期数有关.而浮点规约加法的延迟时钟周期数一般比较大,这就导致了内层循环软件流水后增加的外层循环开销大,在寄存器资源有限时,内层循环的软件流水效率也不高.算法2的缺点是编程不直观,但是对矩阵犃和矩阵犅的数据访问均为行访问,因此访存效率高.算法2的软件流水效率和浮点乘法以及浮点加法部件的延迟时钟周期数有关.浮点加法的延迟时钟周期数小于浮点规约加法,算法2的内层循环软件流水后带给外层循环的额外开销小,需要的寄存器资源少,容易达到高效的软件流水.下面的讨论中,我们将采用算法2.3.3GEMM的性能模型如图2所示的GEMM算法映射中,一共有P个核,每个核完成一个犆i=犃i,p犅p+犆i的计算过程.犅p被划分成很多个Panel犅p,j,每次GEBP的过程中每个核的LM中存放犃i,p和犅p,j,根据图3所示的算法,犅p,j的列数应为SIMD宽度(SIMD单元的VPE个数)的整数倍.为了占用尽可能少的存储空间,犅p,j的大小取为k×S.每一个犅p,j分别与犃i,p进行一个GEBP的计算,犃i,p的大小取为m×k.我们假设:(1)BW1足够大,不会成为计算的瓶颈;(2)GEBP的每次循环的执行不会发生VLIW的功能单元冲突;(3)不考虑指令cache失效引起的性能损失.对于犃、犅、犆的所有维度均为N的GEMM计算犆+=犃犅,考虑每个VPE每拍最多发射一条双精度浮点乘累加(FMAC)指令,则峰值计算时间tp为Page5每一次GEBP处理一个犃i,p犅p,j+犆i,j.作为内层循环的GEBP被执行的总次数ci为为了提高计算效率,应将当前犆i,j的计算时间与下次计算所需数据块的传输时间尽可能重叠,以隐藏通信开销.由于犃i,p具有重用性,LM中至少要存放犃i,p的mk个元素和犅p,j以及犅p,j+1的2kS个元素,即要存放(mk+2kS)个元素.这种情况下隐藏了犅的传输时间,但不能隐藏犃的传输时间.犃i,p保存在LM中直至与犅p的所有子块犅p,j都进行了运算.犆的每一个元素在计算过程中存放在累加寄存器中,需要写回时,可以复用犃的存储空间.在没有功能单元冲突和寄存器资源充足的情况下,GEBP采取软件流水后,理想情况下可以每拍流出1个计算结果.但浮点乘累加运算一般为多周期指令,而且指令的目的操作数同时也是源操作数,因此无法实现每拍流出1个乘累加计算结果.但是通过循环展开技术优化后,可以实现(dF+1)拍流出(dF+1)个乘累加结果,dF为FMAC单元的延迟时钟槽.因此,每一次GEBP的计算时间为mkS/S.在这个过程中需要读写犆i,j和搬移犅p,j+1的所有元素,共(2mS+kS)个元素.因此,内层循环GEBP的执行总时间tie为GEBP循环展开和软件流水后会给外层循环增加时间开销.GEBP的核心计算过程为每一拍从LM中Load犃和犅的数据,然后再进行乘累加操作.因此,每一次GEBP过程给外层循环增加的开销仅由Load/Store指令和FMAC指令的延迟时钟槽决定,且增加的时钟周期为dLS(dF-1),dLS为Load/Store单元的延迟时钟槽.因此,给外层循环增加的总时间开销tee为第1次GEBP所需数据的传输时间tft是无法隐藏的.在tft的时间内,从MM中共传输了P个核所需的犃子块和犅子块,共(Pmk+kS)个元素,则由于犃没有缓冲区,其传输时间无法隐藏.每一块犃i,p可以重用N/S次,之后需要传输P个核所需的下一犃子块.因此,犃的总传输时间为综上,完成GEMM的总时间T为计算效率,即峰值性能利用率U为SM到LM之间的数据传输时间要隐藏在计算时间中,就必须在tie/ci的时间内给P个核传完它们所需的犅p,j+1,共PkS个元素.因此,SM与LM之间的带宽BW2为如果要将犃的传输时间也隐藏在计算时间中,LM中就必须能够存放犃的下一个子块.即LM的大小必须为(2mk+2kS)个元素.此时,完成GEMM的总时间T为此时,需在每次GEBP的执行时间内读写犆i,j和读入犅p,j+1,共(2mS+kS)个元素,还需在N/S次GEBP中读入P个核分别需要的下一块犃i,p,共Pmk个元素,则tie为tie=cimaxmk,2m+()kS+Pmk/N/S在N/S次GEBP计算时间内,需要从SM传输P个犃的子块和将每一个犅的子块复制到P个核上.因此,BW2为如果在SM中只有犅子块的缓冲区,SM中需存放P个核需要的犃子块和P个核当前计算用的犅子块以及下次计算用的犅子块,共(Pmk+2kS)个元素.如果在SM中同时有犃子块和犅子块的缓冲区,则SM需存放(2Pmk+2kS)个元素.4TFLOPSDSP的设计权衡要使DSP高效地处理GEMM,需对体系结构设计进行很好的优化,并在达到双精度浮点TFLOPS的峰值性能约束条件下,尽可能减少设计代价,在核的数目、核的大小、存储容量和存储带宽等方面进行仔细的设计权衡.图1所示结构中,每个核的大小主要由每个SIMD单元的大小和SIMD宽度决定.我们假设每个SIMD单元每拍可以执行一条双精度FMAC指令,当SP=512且主频为1GHz时,单片峰值性能为1TFLOPS.随着VLSI工艺水平的不断进步,很Page6多高性能DSP的主频都超过了1GHz,因此1GHz的主频假设是合理的.Galal等人[22]给出了45nm工艺库下高速FMAC单元的详细参数,在1GHz主频和45nm工艺库下,FMAC的流水线深度为6,因此我们取dF为5.根据设计经验,dLS取为4.我们根据第3节建立的性能模型设计了体系结构性能推演器,该性能推演器基于我们已有的多核DSP模拟器[23]框架搭建,主要包括参数配置模块、算法映射参数分析模块、执行推演模块和结果输出模块.参数配置模块用以配置系统参数,包括输入矩阵的大小、核的数目、SIMD宽度、流水线深度、各数据通路的带宽以及存储层次的容量.算法映射模块分析矩阵分块及各子块在不同存储层次中的分配方案.执行模块根据参数配置模块和算法映射模块的输入及本文第3节建立的模型,推演GEMM的执行过程并记录执行时的各种参数,并将执行结果传递给输出模块.我们在该性能推演器上对高性能DSP进行了设计空间探索.4.1Local-Memory容量对性能的影响图4给出了在片外带宽BW3一定的情况下,LM容量对DGEMM性能的影响.图中PO代表LM容量为(mk+2kS)个元素时,则犅子块有缓冲区,犃子块无缓冲区,犃的通信时间无法隐藏在计算时间中,即通信时间与计算时间部分重叠.FO代表LM容量为(2mk+2kS)个元素时,犃和犅的通信时间都可以和计算时间重叠,即完全重叠.BW3的单位为双字/拍.图4Local-Memory容量对DGEMM性能的影响可以看出,在给定BW3时,当LM容量比较小时,DGEMM的性能成指数降低.当LM容量增加到一定程度后,性能趋于稳定.m和k的选择应当使得犃和犅的通信时间能与计算时间完全重叠,以使峰值性能利用率更易达到90%以上.片外带宽越小,要达到峰值性能所需要的LM容量越大.当片外带宽为2时,LM的容量非常小(64KB)时,就可以获得峰值性能的90%以上.一个800MHz的64位DDR3接口可以提供102.4Gbps的数据传输率.当内核的频率为1GHz时,相当于102.4bit/cycle,只能满足每拍提供一个双字的请求.当片外带宽过小时,需要很大的LM才可以获得峰值性能的90%以上.一个令人满意的现象是,BW3=1和BW3=2时,要达到峰值性能所需的LM容量相差不大,给片外带宽设计带来了设计权衡空间.4.2Local-Memory容量和犅犠2的关系如图4所示,当片外带宽为64bit/cycle(BW3=1)时,LM容量很小时就可以获得很高的性能.但并非LM的容量越小越好.图5给出了在90%的峰值性能利用率下,LM容量和BW2之间的关系.图中分别给出了双精度浮点峰值性能为2TFLOPS和1TFLOPS的情况.当LM容量很小时,BW2的需求呈指数增长.在LM容量不变的情况下,当峰值性能提升1倍时,BW2的需求也增长1倍.即单片峰值性能的提升与BW2需求增长呈线性关系.从图5还可以看出,在峰值性能一定的情况下,大量的小核(SIMD宽度比较小)和少量的大核(SIMD宽度比较大)对BW2的需求基本一致.图5Local-Memory容量、核数目、核大小对BW2的结合图4和图5,可以在LM容量、LM和SM之间的数据带宽以及峰值性能的利用率之间进行设计权衡.小容量的LM减小了存储器的代价,但大大增加了片上数据传输电路的代价.线性增加LM容量,可以在保证性能的前提下,使片上数据传输电路的设计代价成指数减小.4.3Shared-Memory容量和犅犠3的关系图6给出了SM容量和BW3需求之间的关系.图中的数据是在达到峰值性能90%的情况下得到Page7的.当SM容量非常小时,对BW3的需求成指数增长.当SM的容量到一定程度时,对BW3的需求趋于稳定,并且处于硬件代价可接受的范围.线性增加SM容量,可以在保证性能的前提下,使BW3的设计代价成指数减小.如图6所示,大量的小核比少量的大核需要的片外带宽小.因为当SIMD宽度较小时,可以选择比较大的m和k,使每个GEBP的计算时间增长,从而减小了带宽的需求.图6Shared-Memory容量对BW3的影响(N=4096)从图5和图6中都可看出,当存储器容量太小时,对数据带宽的需求将呈指数增长.因为GEMM的计算量为O(N3),而通信量为O(N2),计算通信比为O(N),片上存储器可以存放的N越大,需要的数据带宽就越小.LM和SM的容量决定了m和k的大小,当LM或SM的容量非常小时,m和k将会非常小,从而使带宽需求成指数增长.4.4流水线深度对性能的影响我们知道,在频率一定的情况下,减小功能单元流水线深度通常是提高功能单元效率的有效手段.功能单元的效率经常成为影响应用性能的关键.然而,要缩短流水线深度通常要耗费一定的硬件资源.SIMD和多核结构的TFLOPSDSP中要对通用功能单元进行大量复制,例如ALU、Load/Store和FMAC单元等.因此,这些通用功能单元的设计代价直接影响到全芯片的设计代价.图7给出了不同规模的DGEMM中,tee、tie和tft各自的归一化执行时间.可以看出,tft所占比例非常小,因此式(1)和(2)中可以去除tft以简化性能模型.如图7所示,当矩阵规模比较小时,例如N<256,tee占总执行时间的一半,甚至更多.当矩阵规模比较大时,例如N>1024,tee所占比例可以忽略不计.因此,在分析大规模GEMM问题时,可去除式(1)和(2)中的tee.tee和FMAC单元以及Load/Store单元的延迟时钟槽成正比.这就意味着,在处理大矩阵的GEMM时,访存带宽对性能影响大于访存延时对性能的影响.在处理小矩阵GEMM时,访存延时会对性能产生较大的影响.因此,可以根据问题规模对Load/Store单元的设计进行一定的权衡.同时,当矩阵规模较小时,设计高效的FMAC部件对于提升GEMM的性能很有意义.当矩阵规模比较大时,可对FMAC单元的硬件代价进行一定的权衡.图7tee、tie和tft的归一化执行时间(S=32,P=16)根据上文的讨论,我们经过一定的权衡,取了一组TFLOPSDSP的合适设计参数,并分析了DGEMM在该DSP上的性能.该DSP主频为1GHz,拥有16个32宽度的SIMD核,每个核拥有256KB的LM,16个核共享4MB的SM.片外带宽可以提供1个双字/拍的传输能力,SM和LM之间的数据带宽为4个双字/拍.FMAC的流水线深度为6,Load/Store每次访问LM的数据需要5拍.图8给出了DGEMM在该DSP上的性能,该DSP的双精度浮点峰值性能为1TFLOPS,可以看出,DGEMM的性能可以接近峰值性能.Page85结论本文第一次提出了一个面向通用高性能计算的高性能DSP的基本框架,并且将GotoBLAS库映射到该结构中,构建了GEMM在该结构上的性能模型.我们综合考虑了性能、存储层次、核大小及核数量等因素,在SIMD和多核结构的DSP上高效实现了GEMM,通过建立的性能模型得到了一些有指导意义的参数和结论,能够指导设计者进行很好的设计空间探索和设计权衡,构建面向通用高性能计算高效的DSP结构.经过一定权衡后,可以在TFLOPSDSP上获得接近峰值的性能.本文还说明了将DSP用于通用高性能计算的优势和可行性,它具有高功耗效率、低价格、易编程等优势.未来的工作将考虑多核之间的同步开销对性能的影响以及在模型中引入功耗的评价指标.致谢在此,我们向国防科技大学高性能计算联合博导组科研基金委员会表示感谢,有了你们的高瞻远瞩和大力支持,本文从思想上和细节上都得到了很大的帮助!
