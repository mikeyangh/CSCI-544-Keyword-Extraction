Page1正则化稀疏模型1)(中国石油大学(北京)自动化研究所北京102249)2)(中国科学院软件研究所基础软件国家工程研究中心北京100190)摘要正则化稀疏模型在机器学习和图像处理等领域发挥着越来越重要的作用,它具有变量选择功能,可以解决建模中的过拟合等问题.Tibshirani提出的Lasso使得正则化稀疏模型真正开始流行.文中总结了各种正则化稀疏模型,指出了各个稀疏模型被提出的原因、所具有的优点、适宜解决的问题及其模型的具体形式.最后,文中还指出了正则化稀疏模型未来的研究方向.关键词正则化;稀疏;变量选择;套索;无偏估计;组稀疏;融合套索1引言机器学习和生物信息学中常常遇到高维小样本数据,高维小样本数据的变量空间维数很高而样本空间维数却很低,因此往往会为建模带来一系列的问题,例如,过少的训练样本会导致过拟合问题,模型的泛化能力很差;训练样本数小于变量数会导致Page2好的解释性,便于数据可视化、减少计算量和传输存储.1996年Tibshirani[1]把岭回归估计的L2范数罚正则化项替换为L1范数罚正则化项得到了Lasso(LeastAbsoluteShrinkageandSelectionOperator,Lasso).L1范数罚具有产生稀疏模型的能力,使用L1范数罚作为正则化项的Lasso具有变量选择功能和变量空间降维功能.实际上在Lasso之前已有能够产生稀疏解的非负绞刑估计(nonnegativegarrote图1正则化稀疏模型分类图本文第2节介绍各种正则化稀疏模型,其中首先介绍Lasso及其起源,然后介绍近似无偏稀疏模型、自动组效应稀疏模型、预设组效应稀疏模型以及其他一些稀疏模型;第3节介绍正则化稀疏模型对应的贝叶斯模型;第4节介绍正则化稀疏模型的常见求解算法及其软件包;第5节通过一些实验直观地展示几种主要稀疏模型的变量选择效果;第6节介绍了稀疏模型在实际中的一些应用;第7节指出了正则化稀疏模型未来的发展方向;第8节对全文进行总结.2正则化稀疏模型2.1Lasso及其起源已知线性回归模型其中狔∈犚N叫做响应向量,犡∈犚N×P叫做设计矩阵,β∈犚P叫做回归系数向量,ε∈犚N叫做误差向量且全部误差变量独立同分布εn~N(0,σ2),n∈{1,2,…,N}.在Lasso被提出之前,最佳子集选择法和逐步estimator)[2]和桥回归(bridgeregression)模型[3]被提出,但由于缺少高效的求解算法因而没有引起足够的重视,而直到Lasso这种正则化稀疏模型以及可对其有效求解的LAR算法(LeastAngleRegression,LAR)[4]被提出后,正则化稀疏模型才得到了广泛深入的研究,并在机器学习、数理统计和生物信息学等领域逐渐流行起来.正则化稀疏模型的分类图如图1所示.回归方法是统计学上传统的变量选择方法,但是这些变量选择方法都是离散的过程,所谓离散的过程指的是初始选择某一个变量或全部变量,然后由某种选择标准依次添加或删除变量的个数,直至选择标准的函数值不再改变为止,最终得到的变量子集参与到模型的拟合过程中.逐步回归法这种离散的变量选择过程抗干扰能力差,数据中微小的扰动都有可能导致完全不同的变量选择结果,并且其变量选择过程与参数估计过程是相互独立的.另外,基于式(1)中线性回归模型的岭回归(ridgeregression)是一种典型的正则化模型岭回归估计对回归系数向量会进行了一定程度的压缩,但并不能将其压缩为零,因此不能产生稀疏解.Breiman提出的基于式(1)中线性回归模型的非负绞刑估计(non-negativegarroteestimator)[2,5-6]为其中犅=diag(βPage3二乘估计的解.非负绞刑估计的解比最佳子集选择法和逐步回归方法稳定且是稀疏的,它的提出比Lasso还早.另外,在Lasso被提出之前还有一种叫做桥回归(bridgeregression)[3,7-9]的稀疏模型被提出:其中γ∈(0,+).Lasso是受到上述子集选择法、岭回归估计、非负绞刑估计和桥回归模型等的启发而被提出的,其通过对回归系数的绝对值之和(即回归系数向量的L1范数)进行惩罚来压缩回归系数的大小,使绝对值较小的回归系数自动被压缩为0,从而产生稀疏解和实现变量选择,Tibshirani提出的基于式(1)中线性回归模型的Lasso为其中λ0为可调参数(tuningparameter),·2表示L2范数,β1=∑法和岭回归不同,Lasso不具有显式解,其解可通过坐标下降算法(coordinatedescentalgrithm)[10]进行不断迭代求得,求解Lasso的坐标下降算法关于回归系数βp的更新公式为^OLS其中βsquare)的解,sign(·)为符号函数,(β示当β下降算法每次只更新一个回归系数,同时固定其他^OLS回归系数不变.整个坐标下降算法关于全部回归系数{1,…,P}循环迭代上述更新过程直到收敛.由式(6)可以看出,落在[-λ,λ]区间内的回归系数向量分量均被置零0,因此实现了回归系数向量的稀疏化,而回归系数向量的稀疏化使得与回归系数向量分量为零所对应的变量不参与模型的拟合,实现了变量选择效果.实际上,Lasso是桥回归模型在q=1时的一个特例.Lasso有着诸多优点,其能够同时实现变量选择和参数估计,并且对应的优化问题是凸优化问题.由于Lasso这种正则化稀疏模型具有上述诸多优点,所以逐渐流行起来,被广泛应用到机器学习、图像处理、生物信息学和信号处理等多个领域.2.2近似无偏稀疏模型在变量选择中,将与响应变量密切相关的变量叫做目标变量(即期望选择出来的变量),而将其他与响应变量无关的变量叫做噪声变量(即冗余变量).由于Lasso对回归系数向量的全部分量都进行相同程度的惩罚,因此除了将噪声变量对应的回归系数压缩为0外,还会对目标变量对应的回归系数进行一定程度的压缩,导致了对目标变量回归系数的有偏估计.针对Lasso的有偏估计这个缺点,一系列的近似无偏稀疏模型陆续被提出.所谓近似无偏的稀疏模型指的是与Lasso相比减弱甚至消除了对于目标变量回归系数压缩的稀疏模型,这些近似无偏的稀疏模型有:自适应Lasso[11-12]、松弛Lasso[13]、SCAD模型[14-15]、MCP模型[16]和桥回归模型[3,7-10],其中自适应Lasso和松弛Lasso是凸的,而SCAD、MCP和桥回归是非凸的.(1)自适应Lasso(adaptiveLasso).自适应Lasso[11-12]的提出是为了克服Lasso估计有偏的缺点.若要令Lasso具有估计无偏的特性,一个自然的想法就是令各个变量对应的回归系数所受到的惩罚程度具有自适应的性质,将罚函数中目标变量的回归系数的惩罚权重设定地比较大,而将罚函数中噪声变量的回归系数的惩罚权重设定地比较小,这就是自适应Lasso的思想.基于式(1)中线性回归模型的自适应Lasso为^=argminβ其中λ0,w^p=1/β值,γ>0,β自适应Lasso稀疏模型构造过程分为两步:①先做一次普通最小二乘估计,将得到的系数估计值的绝对值的γ次方的倒数作为第p个变量对应的权值;②将权值代入式(7)中,求解式(7).自适应组Lasso克服了Lasso估计有偏的缺点.(2)松弛Lasso(relaxedLasso).另外一种针对Lasso有偏估计而被提出的稀疏模型为松弛Lasso[13],其思想为将变量选择过程和参数估计过程分为两步:第1步利用Lasso进行变量选择步骤得到变量子集,第2步在第1步中得到的变量子集的基础上进行参数估计,但第2步中的可调参数不像第1步中的可调参数那样大,因此减小了对目标变量的回归系数的压缩,但仍有可能将某些冗余变量的回归系数置零.令Mλ表示由求解Lasso得到的回归系数集:则基于式(1)中线性回归模型的松弛Lasso[31]为^=argminβPage4其中λ0,∈(0,1],p∈{1,…,P},1M集Mλ{1,…,P}的指示函数,即当=1时,松弛Lasso就退化为Lasso.松弛Lasso对回归系数向量进行了两次压缩过程,因此比Lasso的解更加稀疏;但松弛Lasso与Lasso相比减小了对目标变量对应的回归系数的压缩程度,从而提高了Lasso的预测准确性.(3)SCAD(SmoothlyClippedAbsoluteDeviationpenalty).SCAD模型[14]与也为一种近似无偏稀疏模型,其具有如下的形式:^=argminβ其中p∈{1,…,P},φλ,γ(·)为SCAD罚φλ,γ(θ)=其中γ>2,λ0.SCAD模型对绝对值小于λ的回归系数(即噪声变量的回归系数)的压缩作用与Lasso相同,倾向于将这一部分回归系数压缩为零;对于绝对值位于区间[λ,γλ]内的回归系数(即目标变量的回归系数),随着回归系数绝对值的增大而减小压缩的程度;对于绝对值大于γλ的回归系数(即目标变量的回归系数),不再对其进行压缩.由于减小甚至避免了对目标变量对应的回归系数的压缩,故SCAD模型克服了Lasso有偏估计的缺点,改善了其参数估计一致性和变量选择一致性.另外,文献[15]中将损失函数由最小二乘损失函数替换为铰链损失函数,但基于铰链损失函数的SCAD与基于最小二乘损失函数的SCAD的变量选择效果相同,因为其变量选择效果取决于SCAD罚而与损失函数无关.(4)MCP(MinimaxConcavePenalty).MCP模型[16]也是一种近似无偏稀疏模型,其形式为^=argminβ其中φλ,γ(·)为MCP罚λ0,γ>1,I(·)为指示函数.当γ→时,MCP罚逐渐趋向于L1范数罚,解得的回归系数向量的稀疏性越来越小;当γ→1时,MCP罚逐渐趋向于L0范数罚,解得的回归系数向量的稀疏性越来越大.MCP模型对绝对值小于γλ的回归系数(噪声变量的回归系数)进行压缩,而对绝对值大于γλ的回归系数(目标变量的回归系数)不再进行压缩.通过减小对于目标变量回归系数的压缩来实现近似无偏估计.(5)桥回归模型.在第2.1节中已提到的桥回归[3,7-9],实际上是最早被提出的近似无偏稀疏模型,但由于求解算法困难导致一直没有流行起来.其形式为其中βSCAD模型和MCP模型类似,桥回归模型也是通过随着回归系数的绝对值的增大而减小压缩的程度来克服Lasso有偏估计缺点的.(6)小结与分析.虽然Lasso能够得到稀疏回归系数向量,但其有一个显著缺点:Lasso是有偏估计,只在不可表示条件[17](irrepresentablecondition)、稀疏Riesz条件[18](sparseRieszcondition)和限制特征值条件[19](restrictedeigenvaluecondition)等非常强的附加条件下的估计值才是近似无偏的,此时其参数估计一致性和变量选择一致性才存在.所谓变量选择一致性是指稀疏模型所选出的变量与实际上对响应变量有影响的变量一致,即Lasso估计有偏的主要原因在于其不仅对噪声变量对应的回归系数进行压缩,而且对目标变量对应的回归系数也进行压缩,即它对全部变量对应的回归系数向量都进行压缩.针对Lasso估计有偏的问题,提出的正则化稀疏模型大都从调整回归系数向量各分量被惩罚的程度入手,例如自适应Lasso、松弛Lasso、SCAD模型和MCP模型,而其中SCAD模型和MCP模型的罚函数为非凸的SCAD罚和MCP罚,非凸罚较之凸罚(例如Lasso的罚函数就是凸罚)来说在某些方面具有优势,比如利用非凸罚函数得到的正则化稀疏模型往往变量选择一致性不错.但非凸的罚函数也有弊端,例如非凸性会导致全局最优解不存在,因此在求解优化过程时往往比凸的罚函数难度大.2.3自动组效应稀疏模型Lasso在进行变量选择时不具有组效应.所谓Page5组效应指的是某些变量作为一个整体被同时选中进而参与模型的构造,或同时从模型中移除进而不参与模型的构造,即具有变量组选择的效果.自动组效应在文献[20]中首先被提出,其含义为某种估计方法令那些彼此之间高度相关的变量的回归系数的绝对值(几乎)相等,从而倾向于将全部高度相关的变量作为一个组同时选中或同时移除.但自动组效应只能实现对高度相关变量的组选择效果.自动组效应稀疏模型可被分为三类:一类为通过岭罚实现自动组效应的稀疏模型,包括弹性网[20]、弹性SCAD[21-22]、两两弹性网[23-24]和迹Lasso[25];另一类为通过对回归系数之差施加L1范数罚从而实现自动组效应,包括融合Lasso[26-31]、两两融合Lasso[32]、HORSES模型①、加权两两融合Lasso[33]和弹性相关网[34];第三类为利用两两无穷范数罚实现自动组效应,包括OSCAR模型[35-36].2.3.1通过岭罚实现自动组效应通过岭罚实现自动组效应的稀疏模型有弹性网、弹性SCAD、两两弹性网和迹Lasso.(1)弹性网.弹性网(elasticnet)为第一种被提出的具有自动组效应的稀疏模型,其分为朴素弹性网(naiveelasticnet)和弹性网.朴素弹性网形式为^=argminβ其罚函数λ1β1+λ2β(即岭回归的罚函数)β数(elasticnetpenalty).L1范数罚使得朴素弹性网具有稀疏性,岭罚使得弹性网具有自动组效应的特性.但由于岭罚和L1范数罚都具有压缩回归系数的作用,因此朴素弹性网对回归系数进行了两次压缩,从而导致估计的偏差较大,因此需要对朴素弹性网进行改进.Zou等人[20]提出的将朴素的弹性网进行一次大小为(1+λ2)的比例变换的方法即可解决两次压缩增加的偏差问题,并将比例变换后的朴素弹性网称作弹性网.令β性网与朴素弹性网之间的关系为β(2)弹性SCAD.弹性网由于使用了L1范数罚,因此其缺点为有偏估计,Zeng等人[21]针对此问题提出了弹性SCAD,其形式为^=argminβ其中φλ,γ(·)为SCAD罚,λ1>0,λ2>0,γ>2.弹性SCAD克服了弹性网有偏估计的缺点,而且由于使用了岭罚,故其具有自动组效应.(3)两两弹性网.弹性网忽略了不同变量之间相关程度大小各不相同的事实:有些变量之间具有相关性,因此需要施加岭罚;有些变量之间的相关程度较低,故施加岭罚的程度要较小;有些变量之间的相关程度较高,故施加岭罚的程度要较大;甚至有些变量之间并不存在相关性,因此根本不需要施加岭罚,因而弹性网“一刀切”地施加同等程度岭罚的方法是不恰当的.针对该问题,文献[23-24]提出两两弹性网(pairwiseelasticnet),其形式如下:β其中λ0,β表示向量β的全部元素取绝对之后形成的新的向量,犘为元素非负的对称半正定矩阵两两弹性网中的罚函数β由式(21)可知关于相关系数矩阵犚中元素犚ij的罚函数为犚ij(β2相关系数犚ij越小则式(22)就越接近于β于稀疏性;相关系数犚ij越大则式(22)就越接近于2,更倾向于组效应.因而可以看出两两弹性网β的自动组效应本质上也是通过岭罚实现的.而且,两两弹性网的显著特点为利用相关系数矩阵犚来确定矩阵犘,从而使得两两弹性网能够根据变量之间相关系数的大小自适应地决定是否施加岭罚以及施加多大程度的岭罚,从而克服了弹性网死板地施加岭罚的缺点.(4)迹Lasso.另外一种利用岭罚实现自动组效应的稀疏模型为迹Lasso(traceLasso),其形式为^=argminβ其中λ0,diag(β)表示由向量β的全部分量作为对角元素构成的对角阵,tr(·)表示矩阵迹范数罚(tracenormpenalty),含义为矩阵的全部特征值之和.迹Lasso具有根据变量之间相关系数大小不同而自适应变化的功能.当全部变量两两都不相关且设计矩阵被单位化时,迹范数退化为L1范数罚:tr(犡diag(β))=∑①Regressionshrinkageandgroupingofhighlycorrelatedpre-Page6此时倾向于实现解的稀疏性.当全部变量两两完全相关且设计矩阵被单位化时,迹范数罚退化为L2范数罚:tr(犡diag(β))=狓1β=狓12β2(25)此时倾向于实现自动组效应.显然,迹Lasso的自动组效应本质上也是通过岭罚实现的.另外,迹Lasso较之于弹性网的优点在于:施加的L1范数罚随变量之间的相关性减小而增大,施加的岭罚随着变量之间的相关性增大而增大,因而其自动组效应性质具有自适应特点.(5)小结与分析.我们知道,岭回归适合处理变量之间存在高度相关性的数据集,Zou等人[20]受此启发,将岭回归的罚函数(岭罚)引入到Lasso的目标函数中,构造出新的稀疏模型弹性网,使得弹性网兼具Lasso的稀疏性和岭回归处理相关性数据的优越性,能够将高度相关的变量作为一个整体同时选中或移除.本节介绍的弹性网、弹性SCAD、两两弹性网、自适应弹性网和迹Lasso这些稀疏模型虽然形式完全不同,但是本质上都是利用岭罚而实现的自动组效应的.另外,弹性网、弹性SCAD和自适应弹性网的自动组效应比较死板,对全部变量均施加同等程度的岭罚.而两两弹性网和迹Lasso根据变量间相关系数值的变化而自适应地改变所施加的岭罚的大小.2.3.2通过惩罚回归系数之差(和)实现自动组效应通过惩罚回归系数之差实现自动组效应的稀疏模型有融合Lasso、两两融合Lasso、HORSES模型、加权两两融合Lasso和弹性相关网.(1)融合Lasso.融合Lasso(fusedLasso)[26-31]假设全部变量是有序的,它不仅对回归系数进行惩罚,还对相邻变量的回归系数之差的绝对值进行惩罚,因此不仅会使解稀疏化,还会使相邻回归系数平坦变化,即得到的解具有分段常数化(piecewiseconstantsolution)的特点.基于式(1)中线性回归模型的融合Lasso为^=argminββ∈犚其中λ10,λ20,∑penalty),融合罚的作用即为对相邻变量对应的回归系数之差的绝对值进行惩罚,从而使得融合Lasso的解具有分段常数化的特点,而L1范数罚的作用为实现解的稀疏性.显然,由于融合Lasso的解具有“分段常数化”性质,它的作用是使相邻回归系数的绝对值几乎相等,所以融合Lasso可以被看做具有自动组效应性质.但融合Lasso的这种自动组效应性质仅仅局限于前后相邻的变量,为最简单的自动组效应性质.(2)两两融合Lasso与HORSES模型.融合Lasso的应用具有局限性,其只适用于一维有序的变量,只能在前后相邻变量间实现组效应,两两融合Lasso(pairwisefusedLasso)[32]将融合Lasso推广到变量无序的情形,其思想为对任意两两变量的回归系数之差的绝对值都进行惩罚,因而可以令不相邻变量的回归系数的绝对值趋向于相等,从而将不相邻但具有共性的一些变量成组地选择出来,实现不相邻变量间的自动组效应.两两融合Lasso的形式为^=argminββ∈犚其中λ10,λ20.罚函数∑融合罚,两两融合罚的作用为对任意两两变量的回归系数之差的绝对值进行惩罚.另外,HORSES模型本质上属于两两融合Lasso,其与两两融合Lasso唯一的不同之处在于其要求调节参数λ1大于某个人为选定的正数.另外,两两融合Lasso和HORSES只实现关于正相关变量的自动组效应.(3)加权两两融合Lasso.与两两融合Lasso只能实现正相关变量的自动组效应不同,加权两两融合Lasso[33]将相关系数值引入罚函数中,能够同时实现关于正相关变量和负相关变量的自动组效应,即将正相关变量和负相关变量作为一个整体同时选中或移除.加权两两融合Lasso的形式为^=argminβ其中ρij表示变量狓i和变量狓j之间的相关系数,且sign(ρij)有两方面的作用:当变量为正相关时对回归系数之差的绝对值进行惩罚;当变量为负相关时对回归系数之和的绝对值进行惩罚,因而能够实现关于正相关和负相关变量的自动组效应.另外,权重1/(1-ρij)随着相关系数绝对值的增大(减小)而增大(减小),这样可以保证当变量之间的相关性越大(小)时对它们之间回归系数之差的惩罚力度越大Page7(小),从而令两个变量对应的回归系数的绝对值越接近(不接近),即加权两两融合Lasso的自动组效应具有随相关系数值自适应变化的特点.特别地,当ρij=0时,则弱化为两两融合Lasso.(4)弹性相关网.另外一种能同时实现正相关和负相关变量自动组效应的稀疏模型为弹性相关网(elasticcorr-net)[34],其形式为^=argminβ其中λ1>0,λ2>0,(β)为基于相关系数的罚(corre-lationbasedpenalty):(β)=∑其实现自动组效应的原理为:当ρij→1(趋向于完全正相关)时,式(31)中括号里的第一项的权重变得很大,此时主要对回归系数之差进行惩罚;当ρij→-1(趋向于完全负相关)时,式(31)中括号里的第2项的权重变得很大,此时主要对回归系数之和进行惩罚.总之,弹性相关网通过将变量之间的相关系数引入罚函数中,使得到的解中正相关的变量具有几乎相等的回归系数大小,同时负相关的变量具有符号相反而绝对值几乎相等的回归系数,既实现了关于正相关变量的组效应,也实现了关于负相关变量的组效应.(5)小结与分析.本节介绍的稀疏模型本质上都是通过惩罚回归系数之差(和)来实现自动组效应的,其中融合Lasso只对有序变量中前后相邻的两个施加融合罚,因此只适用于一维有序变量的情形.但融合Lasso是本小结所介绍的其他全部模型的鼻祖,其余模型的提出均受到其融合罚的启发;两两融合Lasso和HORSES模型对原始的融合Lasso进行了拓展,其对任意两两变量之间均施加融合罚,但只能实现关于正相关变量的自动组效应;加权融合Lasso和弹性相关网中均引入了数据集中变量之间的具体相关系数值,因而对变量施加的融合罚随相关系数值不同而自适应变化,且能够实现关于正相关和负相关变量的自动组效应(即将正相关和负相关变量作为一个整体同时选中或移除).2.3.3通过两两无穷范数实现自动组效应Bondell等人提出的OSCAR(OctagonalShrinkageandClusteringAlgorithmforRegression)[35-36]模型也具有自动组效应,其通过无穷范数约束任意两两变量的回归系数的最大值,从而令该两个变量的回归系数趋于相等而实现自动组效应.基于式(1)中线性回归模型的OSCAR模型求解问题为β^=argmin其中犮>0,λ0.L1范数罚的作用为实现稀疏性,罚函数中的第2项叫做两两无穷范数罚(pairwiseLnormpenalty),其作用为约束任意两两变量的回归系数的最大值进而使得OSCAR模型具有组效应.OSCAR模型既能实现关于正相关变量的自动组效应,也能实现关于负相关变量的自动组效应.2.3.4自动组效应稀疏模型的总结岭罚具有组效应性质,弹性网、两两弹性网和基于相关系数的弹性网在本质上都是使用了岭罚才具有组效应性质的.迹Lasso虽然没有直接使用岭罚而是使用的迹范数罚,但当变量间存在的相关性增大时迹范数罚朝着岭罚的方向变化,所以本质上也可将迹Lasso归为使用岭罚而具有组效应性质的那一类.而融合Lasso、两两融合Lasso、基于相关系数的两两融合Lasso和HORSES模型本质上是对回归系数之差或和的绝对值进行惩罚,从而促使回归系数相等,实现组效应.OSCAR模型通过限制回归系数向量的两两无穷范数罚而实现组效应.但必须指出的是,自动组效应稀疏模型与下文中的预设组效应稀疏模型不同,自动组效应稀疏模型只是对强相关的变量实现组选择,且其组选择是自动实现的,而预设组效应稀疏模型的组选择特性是通过人为事先定义分组而实现的且可对任意变量组实现组选择.2.4预设组效应稀疏模型具有代表性的预设组效应稀疏模型有组Lasso(GroupLasso)、组SCAD(GroupSCAD)模型、组MCP(GroupMCP)模型和稀疏组Lasso(SparseGroupLasso,SGL)等,三者均只具有组稀疏性,而稀疏组Lasso既具有变量稀疏性又具有变量组稀疏性.预设组效应稀疏模型需要预先设定分组,分组情况完全由人为决定,被设定为同一个组的变量的回归系数将同时为零或非零,即同时被预设组效应稀疏模型选中或移除.(1)组Lasso.实践中常常遇到变量之间具有组结构的情形,例如在多因子方差分析中因子对应的多个哑变量可被视为一个变量组,针对该问题组Lasso[37]被提出.假设存在P个变量,预先人为地将它们划分成J个变量组,则组Lasso为Page8其中β2,1=∑归系数向量.组结构还有一些特殊情形,例如不同变量组所包含的变量出现重复的情形[38-40]和变量组之间的关系为树结构[41-43]的情形,组Lasso正在从简单的组结构向着复杂的重复组(overlapgroup)结构和树组(tree-guidedgroup)结构等方向发展.(2)组SCAD与组MCP.与Lasso类似,组Lasso的估计也是有偏的,针对该问题,文献[44]利用SCAD罚构造出组SCAD模型,文献[45]利用MCP罚构造出组MCP模型.组SCAD与组MCP具有如下统一的形式:^=argminβφλ,γ(·)为式(12)中的SCAD罚或式(14)中的MCP罚.因为利用了非凸的SCAD罚和MCP罚,所以组SCAD与组MCP克服了组Lasso估计有偏的缺点.(3)稀疏组Lasso.Simon等人[46]将Lasso与组Lasso的罚函数结合到一起提出了稀疏组Lasso.稀疏组Lasso的形式为^=argminβ其中λ1β2,1使得稀疏组Lasso具有组稀疏性,而λ2β1使得稀疏组Lasso的某些分组内的变量也具有稀疏性.(4)小结与分析.预设组效应的实现必须预先将变量进行分组,并且分组情况完全由实验者自主决定,因而其与自动组效应的最大区别在于可实现关于任意分组的组选择效果,而自动组效应稀疏模型只能实现关于高度相关变量的组选择.值得指出的是,同时结合了Lasso罚函数和组Lasso罚函数的稀疏组Lasso既能实现变量选择也能实现变量组选择.另外,组SCAD和组MCP克服了组Lasso的有偏估计缺点.2.5其他稀疏模型(1)Dantzig选择器(DantzigSelector).基于式(1)中线性回归模型的Dantzig选择器[47-53]为如下的凸规划问题:其中λD>0,且犡T(狔-犡β)=supDantzig选择器也可以被写作如下无约束的形式:~其中λ之处在于Lasso的目标函数是最小化输出误差的L2范数的平方,而Dantzig选择器的目标函数是最小化输出误差向量与设计矩阵乘积的L范数,两者共同之处在于均使用了L1范数罚β1,因此Dantzig选择器也能实现稀疏解,具有类似于Lasso的变量选择功能.文献[49]指出当PN(参数空间维数小于样本空间维数)时,Dantzig选择器和Lasso的解路径相同的充分条件为矩阵(犡T犡)-1满足对角占优条件:其中犕=(犡T犡)-1,i,j∈{1,…,P}.特别地,在二维的参数空间中(即P=2时)对角占优条件总是成立的,因此Dantzig选择器与Lasso得到的解在二维的参数空间中总是等价的.但是文献[49]中给出的Lasso与Dantzig选择器两者产生的解等价的条件只局限于PN的情形,为此文献[53]给出了在任意情形下(无论是PN还是P>N)Lasso与Dantzig选择器等价的条件:对于Lasso令IL表示由Lasso得到的解β索引集,令珟犡L∈犚N×|IL|表示由索引集IL对应的变量组成的矩阵,令犡L∈犚N×|IL|表示将珟犡L中每个变量乘以β设设计矩阵犡是满秩矩阵且秩为IL,在式(36)中的可调参数λD与式(40)中Lasso的可调参数λL满足λD=λL时,若成立,则β素都为1的向量,狌=(犡T犡)-110表示向量狌中的元素全部都大于等于0.此外,文献[53]还指出,当令λD=λL时,式(40)中的Lasso的解总是式(36)中的Dantzig选择器的一个可行解(虽然不一定是最优解),因此当Lasso和Dantzig选择器的解不同时,Dantzig选择器的解要比Lasso的解更具稀疏性.文献[49]指出Dantzig选择器解路径的波动性比较严重,Lasso的解路径比Dantzig选择器的解路径要平滑地多,并且Lasso关于β的均方误差^2β-β择器相比差不多,有时候还明显小于Dantzig选择器的均方误差,尤其当信噪比较高或变量间的相关Page9性较高时Dantzig选择器的均方误差要比Lasso的均方误差大很多.但Dantzig选择器与Lasso相比最大的优点在于其要求解的优化问题是一个线性规划问题,求解非常方便.(2)核Lasso.将式(1)中的线性模型推广为如下的基于核函数的非线性模型:其中ε为噪声向量,k(狓,狓n)为核函数.已知设计矩阵犡=[狓1,狓2,…,狓N]∈犚N×P,其中狓1,狓2,…,狓N∈犚P,N为样本数,P为变量数,假设ε~N(0,σ2犐),则基于式(42)中非线性模型的核Lasso(kernelLasso)[54-56]为^=argminββ∈犚=argminβ∈犚其中犓为由核函数k(狓n,狓m)作为元素组成的核矩阵犓=[k(狓n,狓m)]NN}.核Lasso的提出是因为Lasso只能处理线性回归模型下的变量选择问题,存在一定的局限性,在处理非线性问题时线性Lasso的预测效果较差.而引入核函数是处理非线性问题的典型方法,对于线性表1各正则化稀疏模型的特点归纳模型非负绞刑估计Lasso自适应Lasso松弛LassoSCAD模型MCP模型桥回归弹性网弹性SCAD两两弹性网迹Lasso融合Lasso两两融合LassoHORSES加权两两融合Lasso√OSCAR弹性相关网组Lasso组SCAD组MCP稀疏组Lasso核Lasso图LassoDantzig选择器不可分的情形,核函数利用非线性变换将数据从低维空间映射到高维空间,在高维空间中实现分类.核函数的引入使得核Lasso具有处理非线性情形下变量选择问题的能力,克服了Lasso不适用于处理非线性情形下变量选择问题的缺点.(3)图Lasso.已知则基于式(44)的图Lasso(GraphicalLasso)[57-60]为Σ^-1=argmax其中Σ-1近年来将稀疏方法应用于概率图模型的结构和参数学习中是研究的热点,很多学者还将图Lasso推广到有向无环图[61-62]、Ising模型[63-64]、半参数图模型[65-67]、Poisson图模型[68-69]、节点不能观图模型[70-71]和时变图模型[72-74]等其他概率图模型中,图Lasso的应用领域正在快速扩展.2.6各稀疏模型的对比各正则化稀疏模型的特点如表1所示,其中“稀疏性”指变量水平上的稀疏性,“预设组稀疏性”和“自动组稀疏性”指变量组水平上的组稀疏性.提出比Lasso早,具有稀疏性具有开创性的意义,使得正则化稀疏模型真正开始广泛流行近似无偏估计,克服了Lasso有偏估计的缺点近似无偏估计,克服了Lasso有偏估计的缺点近似无偏估计,克服了Lasso有偏估计的缺点近似无偏估计,克服了Lasso有偏估计的缺点近似无偏估计,克服了Lasso有偏估计的缺点克服Lasso不适合处理共线性数据集的缺点克服Lasso不适合处理共线性数据集的缺点且具有估计无偏性引入相关性信息,克服Lasso不适合处理共线性数据集的缺点罚函数中引入设计矩阵,能根据相关性信息自适应地施加岭罚对相邻变量的系数进行融合,其解具有分段常数化的特点与融合Lasso区别在于其对全部变量两两之间的系数都进行融合对全部变量两两之间的系数都进行融合在两两融合Lasso的基础上引入了数据集中的相关性信息通过两两无穷范数罚同时实现对正负相关变量的组效应引入相关性信息,克服Lasso不适合处理共线性数据集的缺点克服了Lasso不能进行变量组选择的缺点克服组Lasso估计有偏的缺点克服组Lasso估计有偏的缺点同时具有稀疏性和组稀疏性克服Lasso不适合处理非线性情形的缺点简化概率图模型的结构优化问题为线性规划问题,求解方便Page103贝叶斯模型早在1996年Tibshirani[1]就指出基于式(1)中线性回归模型狔=犡β+ε的Lasso可以被表示为一个最大后验估计,其中Lasso的正则化项对应最大后验估计中的先验分布,Lasso的损失函数项对应最大后验估计中的似然函数,文献[75-76]根据这一思想进一步具体地提出了贝叶斯Lasso(BayesianLasso).贝叶斯Lasso实质上是关于参数向量β的最大后验估计,其似然函数和先验分布分别如式(47)和(48)所示:其中狓n和yn表示第n个样本,且各样本间是相互独立的,yn服从均值为狓T差σ2即为式(1)中误差变量εn~N(0,σ2)的方差.需注意的是,式(48)中参数向量β的先验分布是条件Laplace分布,而且取σ2所服从的分布为均匀分布,如下文中式(51e)所示,文献[46]指出这样做能够保证后验分布的单峰性,若后验分布不具有单峰性会导致吉布斯抽样算法收敛太慢.由式(47)和(48)便可以求得参数向量β的后验概率分布根据贝叶斯理论,在贝叶斯估计中有了参数向量β的后验概率分布后,若给定某可信水平1-α,则计算回归系数βp的贝叶斯可信区间(Bayesiancredibleinterval)是非常直接的,因此贝叶斯Lasso能够提供全部回归系数β1,…,βP的贝叶斯可信区间,这是贝叶斯Lasso的显著优点,因为Lasso这种非贝叶斯形式的稀疏模型只能给出回归系数的点估计值而不能给出其区间估计.另外,贝叶斯Lasso通过吉布斯抽样等算法可以求得式(1)中误差变量εn的方差σ2的估计值,这也是Lasso所不能做到的.文献[77]中进一步指出,将式(48)中的Laplace分布表示为如下Gaussianscalemixture分布的形式会有效降低计算的复杂度:P(βσ2)=∏Pp=1∫=∏其中犖(βp0,σ2τ2σ2τ2p的正态分布,Gammaτ2数为1和λ2布进一步转化成了两层结构:顶层分布为正态分布,底层分布为伽马分布.因此整个贝叶斯Lasso的最大后验估计形式可以被表示为如下的层次贝叶斯模型(HierarchicalBayesianModel):狔犡,β,σ2~犖N(犡β,σ2犐N)βσ2,τ2犇τ=diag(τ2σ2,τ2π(σ2)=1/σ2σ2,τ2其中式(51e)表示σ2服从均匀分布,犐N为N×N阶的单位矩阵.可以看出,关于β的整个最大后验估计形式被表示成了一个具有3层分布结构的层次模型:第1层分布为响应向量所服从的均值为犡β且协方差矩阵为σ2犐n的正态分布,第2层分布为回归系数向量所服从的均值为零向量且协方差矩阵为σ2犇τ的正态分布,第3层分布为τ21和λ221/σ2.文献[75]中利用吉布斯抽样方法求解贝叶斯Lasso中的参数,但吉布斯抽样方法对于高维数据集来说计算复杂度很高,期望最大化算法(ExpectationMaximation,EM)的计算复杂度往往比吉布斯抽样方法低,因此未来探索如何利用EM算法求解贝叶斯Lasso是一个值得研究的方向.实际上,很多正则化稀疏模型均有其对应的贝叶斯模型,均可以被表示为一个最大后验估计形式,例如桥回归模型的最大后验估计———贝叶斯桥(BayesianBridge)[75]、自适应Lasso的最大后验估计形式———贝叶斯自适应Lasso(BayesianAdaptiveLasso)[78]、弹性网的最大后验估计形式———贝叶斯弹性网(BayesianElasticNet)[79]、组Lasso的最大后验估计形式———贝叶斯组Lasso(BayesianGroupLasso)[77,80]以及图Lasso的最大后验估计形式———贝叶斯图Lasso(BayesianGraphicalLasso)[81]等,虽然各个正则化稀疏模型的最大后验估计形式不同(主要是先验分布不同),但其原理是相同的,即正则化稀疏模型的损失函数项对应最大后验估计形式中的似然函数.正则化稀疏模型的正则化项对应最大后验估计形式中的先验分布,各正则化稀疏模型的最大后验估计形式均可以用类似于式(51a)~(51f)的层次贝叶斯模型并利用吉布斯抽样算法求解,具Page11体形式请参考相应的文献,在此不再赘述.另外,贝叶斯模型推理中的先验信息多种多样,远不止目前已经构造的稀疏模型中的正则化项的形式.从贝叶斯角度设计新的先验信息从而得到新的正则化稀疏模型是一个重要的研究方向.4正则化稀疏模型的求解算法正则化稀疏模型本质上为一个最优化问题.在Lasso被提出的前几年,由于缺少对其高效求解的算法,所以一直没有广泛流行.直到LAR算法(LeastAngleRegression,LAR)的提出,使得Lasso的求解方便而快捷,Lasso等一系列正则化稀疏模型开始被广泛地研究.文献[4]中指出,在一定条件下LAR算法的解路径与Lasso的解路径一致,因而可以通过LAR算法来求解Lasso的解.另外,LAR算法的变体组LAR算法(GroupLeastAngleRegression)[37]可用来求解组Lasso.坐标下降[10,82](coordinatedescent)及其变体组坐标下降[37](Blockcoordinatedescent)可用来求解Lasso、自适应Lasso、非负绞刑估计、弹性网、组Lasso和稀疏组Lasso等问题,但不同的稀疏模型的坐标下降算法的具体形式不同,例如Lasso和组Lasso等稀疏模型直接利用坐标下降算法或组坐标下降算法求解即可,而求解稀疏组Lasso需要利用两层迭代的结构才行,其中外层迭代为针对L2,1范数罚的组坐标下降算法,内层迭代为针对L1范数罚的坐标下降算法.对于SCAD模型与MCP模型这类非凸的稀疏模型来说,无法直接应用凸优化方法求解,Fan等人提出的LQA(LocalQuadraticApproximation)[14]方法可有效解决优化问题目标函数中的非凸非光滑难题,但LQA方法往往较为耗时.ADMM方法(AlternatingDirectionMethodofMultipliers)[83]可以被用来求解Lasso和组Lasso等诸多稀疏模型问题.已知优化问题:其中g(·)和h(·)均为凸的,狓∈犚P,狕∈犚Q,犃∈犚N×P,犅∈犚N×Q,犮∈犚N.对应的增广Lagrange函数为Lρ(狓,狕,狔)=f(狓)+g(狕)+狔T(犃狓+犅狕-犮)+其中ρ>0,狔∈犚N,ADMM算法由如下的迭代组成:另外一类求解稀疏模型的算法为近似梯度方法(proximalgradientmethod)及其变种,近似梯度方法又叫做广义梯度方法(generalizedgradientmethod)或近似算子方法(proximaloperatormethod).近似梯度方法一般要求解的优化问题的形式为其中g(狓)为可微的凸函数,h(狓)为任意的凸函数,不要求h(狓)为可微的.求解上式中优化问题的近似梯度算法的迭代公式为其中tk为步长,prox(·)为近似算子:当h(狓)=0时,近似梯度算法就是经典的梯度方法;当h(狓)为指示函数h(狓)=IC(狓)时,近似梯度算法退化为投影梯度法;当h(狓)为L1范数罚h(狓)=狓1时,近似梯度算法特化为所谓的ISTA算法(IterativeSoft-thresholdingAlgorithm,ISTA)[84].Nesterov[85-87]对近似梯度方法的收敛速度进行了改善.近似梯度算法及其变种形式广泛应用于稀疏优化问题的求解中,例如文献[88]利用近似梯度方法的变种求解以融合罚为罚函数,文献[89]利用近似梯度方法的变种求解以迹范数罚为罚函数的稀疏模型,文献[90]利用近似梯度方法的变种求解重叠组Lasso.另外,著名的稀疏优化问题求解软件包SLEP(SparseLearningwithEfficientProjections)中的算法基本上都采用了近似梯度方法及其变种形式来求解稀疏模型,更详细的说明请参考该软件包提供的说明书①.另外,更多关于稀疏模型求解方法的研究综述,读者可参考文献[91].5实验本节通过高维小样本不相关数据集实验和高维小样本相关数据集实验来展示Lasso、SCAD、组Lasso、稀疏组Lasso和弹性SCAD这5种具有代表性的正则化稀疏模型的变量选择效果进行对比,其中Lasso和SCAD代表了只在变量水平上实现稀疏性的稀疏模型,其他的稀疏模型例如自适应Lasso、①http://www.public.asu.edu/~jye02/Software/SLEP/Page12松弛Lasso和MCP模型等的变量选择效果与Lasso和SCAD的变量选择效果类似;组Lasso代表了在组水平上实现稀疏性的预设组效应稀疏模型,其他的稀疏模型像组SCAD模型和组MCP模型的变量选择效果与组Lasso类似;稀疏组Lasso代表了同时实现变量水平上稀疏性和组变量水平上稀疏性的稀疏模型;弹性SCAD代表了具有关于高度相关变量的组选择能力的自动组效应稀疏模型;其他的稀疏模型像弹性网、OSCAR模型、HORSES模型、两两弹性网和两两融合Lasso等与弹性SCAD的变量选择效果一致.(1)高维小样本不相关数据集实验.首先通过不相关数据集实验来对比基于最小二乘损失函数的3种稀疏模型:Lasso(使用了L1范数罚)、GroupLasso(使用了L2,1范数罚)与SparseGroupLasso(同时使用了L1范数罚和L2,1范数罚)的变量选择效果.当今稀疏模型能够实现的稀疏效果有3种:普通的稀疏性、组稀疏性和双稀疏性,其中普通的稀疏性是指稀疏效果只在变量水平上实现,具有代表性的稀疏模型为Lasso,组稀疏性是指在变量组的水平上实现稀疏效果,具有代表性的稀疏模型为GroupLasso;双稀疏性是指既在变量组水平上实现变量组选择,又在组内的变量中实现变量选择,即既有稀疏性又具有组稀疏性,具有代表性的稀疏模型为SparseGroupLasso.因此,本实验中我们选择Lasso、GroupLasso和SparseGroupLasso这3种具有代表性的模型来说明稀疏模型的变量选择效果.首先生成一模拟数据集,该数据集包含20个样本和100个变量:变量1,变量2,…,变量10,…,变量100,且各个变量之间不具有相关性,其真实的系数向量为(-2,-1,0,1,0,0,1,1,0,0,0,…,0),表2Lasso拟合出的回归系数向量表3GroupLasso与SGL拟合出的回归系数向量模型Lasso0.803-0.724模型GLSGL0.916-7.062(2)高维小样本相关数据集实验.稀疏模型是否具有自动组效应由罚函数部分决定,而与损失函数无关,下面通过高度相关数据集实验来对比基于即变量1、变量2、变量4、变量7和变量8的回归系数非零,为想要选择出来的重要变量,而其余变量均为不期望选择出来的噪声变量.对于GroupLasso和SparseGroupLasso来说,其分组情况为分组1=(变量1,变量2,变量3,变量4),分组2=(变量5,变量6),分组3=(变量7,变量8),分组4=(变量9,变量10),分组5=(变量11,变量12,…,变量100).对于Lasso来说不进行任何分组.3种稀疏模型所拟合出的回归系数向量如表2和表3所示,但由于空间有限,我们只列出前10个变量变量1,变量2,…,变量10的回归系数,从前10个变量的回归系数就足以展示3种稀疏模型的变量选择功能了.从表2和表3的对比可以看出,GroupLasso具有组稀疏性,因为其各个分组中的回归系数同时为零或同时非零,尤其从分组1中的回归系数可以看出虽然变量3为不期望选择出来的噪声变量,但由于分组时将变量3(噪声变量)与期望选择出来的变量1、变量2和变量4分在了同一组,所以其也不得不被选择出来,所以这4个变量的回归系数同时不为零.再观察表3,其中的SparseGroupLasso,SparseGroupLasso与GroupLasso的分组情况完全一致,但得到的稀疏性效果却不同:SparseGroupLasso得到的回归系数向量中变量3(噪声变量)的系数为零,而GroupLasso却不能将分组1中变量3的回归系数置零,这说明SparseGroupLasso的优点为具有组内变量水平上的稀疏性.本实验中利用了求解Lasso的软件包glmnet①;求解GroupLasso利用了软件包gglasso②;求解SparseGroupLasso利用了软件包SGL③.总之,glmnet、gglasso和SGL软件包均来自CRAN网站(TheComprehensiveRArchiveNet-work,CRAN).变量5-0.016分组2变量5变量600①②③Page13铰链损失函数的Lasso、SCAD和弹性SCAD的变量选择效果,三者分别使用了L1范数罚、SCAD罚和弹性SCAD罚,而损失函数均为铰损失函数.首先生成一个含有若干高度相关变量的模拟数据集,该模拟数据集包含n=50个样本和p=300个变量,其中前5个变量两两之间具有高度的相关性且相关系数ρ=0.9.进行60次实验,将60次实验结论的平均值列入表4中,其中tol、sig和unsig分别表示基于铰链损失函数的Lasso、SCAD、弹性SCAD选中的变量数、选中的重要变量数、选中的噪声变量数.通过表4中的实验结果可以看出Lasso与SCAD具有明显的变量选择能力,但面对5个高度相关的变量却只能选择出其中的一小部分;弹性SCAD也具有明显的变量选择能力,而且几乎将高度相关的重要变量都选出来了.因此可以得出结论:弹性SCAD具有关于高度相关变量的变量组选择能力,而Lasso和SCAD具有变量选择能力但却不具有变量组选择能力.与上一节中的实验一样,本实验中求解各稀疏模型的软件包来自网站CRAN,求解SCAD和弹性网的软件包在CRAN网站①上有许多.表4Lasso、SCAD和弹性SCAD的变量选择效果对比模型LassoSCAD弹性SCAD6正则化稀疏模型的应用总体来说,GroupLasso适用于具有组稀疏性的数据集的变量选择,例如在基因微阵列分析中属于同一个生物学路径的基因可被归类为一个基因组,在基因关联研究中某基因的全部基因标记可被视为一个基因标记组;弹性网、两两弹性网、迹Lasso、OSCAR模型和HORSES模型等适用于具有高度相关变量的数据集的变量选择;融合Lasso适用于处理变量的回归系数具有光滑性的数据集,例如在微阵列比较基因组杂交(Array-basedComparativegenomichybridization,arrayCGH)分析中,相邻基因往往被认为具有共性,往往将相邻基因的回归系数近似相等作为先验信息;核Lasso适合处理非线性情形下的变量选择问题;图Lasso适合处理具有网络结构的数据集.下面介绍一些稀疏模型的代表性的应用情况.6.1在生物信息学和医药学中的应用稀疏模型在生物信息学中有大量的应用.随着科学技术的发展,生物医学领域中数据的规模、多样性以及复杂性快速增长,形成了海量数据的状况.然而,虽然生物医药数据是海量的,但往往只有一小部分是有意义的数据,即生物医药数据可以被稀疏化.在微阵列比较基因组杂交(Array-basedCompara-tivegenomichybridization,arrayCGH)中,相邻基因往往被认为具有共性,因此将相邻基因的回归系数近似相等作为先验信息是合理的.Liu等人[88]将融合罚应用于arrayCGH中对膀胱癌的级别分类问题(tumorgradeclassification),实验证明利用融合罚的分类准确性比利用L1范数罚的分类准确性要高出6个百分点.科学家通常通过脑部图像来诊断阿尔茨海默(Alzheimer’sDisease)疾病,一种合理的假设是阿尔茨海默疾病的早期损害集中于脑部的某个区域,然而传统方法在诊断阿尔茨海默病时往往忽略了立体像素之间的关联性,针对上述问题,Xin等人[92]利用基于两两融合罚的逻辑斯蒂回归方法对阿尔茨海默疾病的诊断并且通过实验证明收到了良好的效果,诊断的准确性要比L1逻辑斯蒂回归、支持向量机以及文献[93]和文献[94]中的方法都高.Allen等人[95]将图Lasso推广到随机变量服从泊松分布的情形,并且将其应用到乳腺癌微核苷酸(microRNA)的网络结构的学习中;Zhong等人[96]将Lasso、OSCAR模型、融合Lasso和弹性网应用于乳腺癌数据的基因选择中.实验证明OSCAR模型的测试准确性最高.另外,还有很多文献将稀疏模型应用于生物信息学与医药学中[97-98].6.2在信号去噪中的应用文献[10]中提出了FLSA(fusedlassosignalapproximator)方法,该方法利用了融合Lasso的罚函数,其与经典的全变差(totalvariation)去噪方法的区别在于多了一项L1范数罚,FLSA方法可对一维的信号进行去噪.文献[10]中还提出了二维的FLSA方法,该方法假设对象为一个网格,对网格结构的横向和纵向均施加融合Lasso罚,因而能够对二维的图像进行去噪,文献[99]中应用二维的融合Lasso进行图像去噪获得了良好的效果.Wang等人[100]将Lasso和自适应Lasso应用到图像去噪领域:传统去噪方法考虑的都是去除高斯噪声的影响,而Wang等人考虑了一种脉冲噪声(impulse①http://cran.r-project.org/Page14noise),脉冲噪声是数字系统中常见的一种噪声,常常由相机中传感器发生故障、模数转换错误以及硬件中存储位置错误等造成.Wang等人通过实验对最小二乘法、Lasso和自适应Lasso的去噪效果进行了对比,实验结果表明自适应Lasso的去噪效果最好;Selesnick等人[101]在经典的全变差去噪基础上进一步假设相邻变量的回归系数之差具有重叠的组结构,将该重叠组结构作为先验信息引入去噪方法中,实验结果表明得到的去噪信号比全变差方法更加平滑.6.3在信号重建中的应用压缩感知无疑是稀疏模型应用的前沿阵地,例如利用了L1范数罚的基追踪方法为压缩感知中图像重建的重要方法.然而,很多情况下信号的稀疏结构是组稀疏的,因此文献[102]将L1范数罚替换为组Lasso的罚函数、稀疏组Lasso的罚函数和重叠组Lasso的罚函数,将组稀疏结构、双水平稀疏结构(组稀疏而且组内元素也稀疏)和重叠组稀疏结构引入到图像重建中.实验结果证明基于双水平稀疏结构的信号重建方法和基于重叠组稀疏结构的信号重建方法均优于基于单纯组稀疏结构的信号重建方法.Rao等人①指出当信号的具有重叠组稀疏结构时,将该重叠组稀疏结构作为先验信息进行信号重建所需的测量值比不利用重叠稀疏结构作为先验的标准压缩感知方法更少,并且给出了所需测量值的边界.以往的压缩感知方法均假设信号是时不变的,而文献[103]将组Lasso的罚函数和融合Lasso的罚函数应用到时变信号的重建当中.6.4在人脸与语音识别中的应用文献[104]中提出基于稀疏表示的分类算法(SparseRepresentation-basedClassification,SRC)应用于人脸识别中,该方法利用了L1范数罚;而文献[105]则将组稀疏结构引入人脸识别中;进一步地,文献[106]将更复杂的数组稀疏结构引入SRC中实现人脸识别,并且通过实验证明该方法取得了比SRC更佳的识别效果;文献[107]将迹范数罚(tracenormpenalty)应用于人脸识别中,提出了一种叫做监督迹Lasso(SupervisedTraceLasso,SSL)的方法,但值得指出的是该方法中迹范数罚的构造与上文中迹Lasso中的迹范数罚的构造方式不完全相同.Tan等人[108]将组Lasso的组稀疏思想应用于语音识别领域,将组稀疏思想与文献[109]中的稀疏贝叶斯学习(SparseBayesianLearning)方法结合,提出了一种新的组稀疏贝叶斯学习(GroupSparseBayesianLearning)方法,并通过实验证明组稀疏结构的语音识别方法(组稀疏贝叶斯学习方法)准确率明显高于不具有组稀结构的语音识别方法(稀疏贝叶斯学习方法);另外,他们还提出了一种叫做组弹性网(GroupElasticNet)的新稀疏模型,该模型是将弹性网中的L1范数罚替换为组Lasso的L2,1范数罚而得到的,他们的实验还表明组弹性网模型的识别准确率高于弹性网的识别准确率,而且弹性网和Lasso的识别准确率都比稀疏贝叶斯方法高.7未来研究方向7.1拓展正则化稀疏模型尽管已经提出了很多正则化稀疏模型,但它们仍然存在各自的缺点,如何克服这些正则化稀疏模型的缺点?一种方法为改进已有的罚函数,例如Zou等人将L1范数罚和L2范数罚结合到一起提出的弹性网兼具L1范数罚的稀疏性和L2范数罚的组效应性质,类似的,未来通过改进已有的正则化稀疏模型也许可以得到大大优于已有正则化稀疏模型的新的正则化稀疏模型.例如,将非凸的capped-L1罚函数[110-111]、对数罚[112-114](logpenalty)、对数和罚函数(Log-SumPenalty,LSP)[115]、对数指数和罚[116](Log-Exp-Sumpenalty)、Geman罚函数[117](GemanPenalty,GP)等非凸罚函数推广到变量组选择情形下从而得到相应的组稀疏模型的问题值得研究.当然,另一种方法为针对已有正则化稀疏模型存在的缺点直接设计新的罚函数并将其应用到正则化稀疏模型中.7.2其他回归模型上的推广正则化稀疏模型在线性回归模型中已经得到了极其广泛的应用,并且其中一些在广义线性回归模型中得到了一定程度的应用,但还有很多正则化稀疏模型在其他某些回归模型中的应用有待于更深入更广泛的研究,未来我们可以将已有的正则化稀疏模型推广到其他回归模型中,以便处理在相应回归模型下的变量选择问题.例如,将融合Lasso推广到COX比例风险回归模型的情形和将MCP模型推广到负二项回归模型的情形等问题有待于研究.研究损失函数的改变,对稀疏模型统计特性和变量选择特性的影响也是值得研究的问题.①TightmeasurementboundsforexactrecoveryofstructuredPage157.3正则化稀疏模型的一致性很多正则化稀疏模型的一致性尚未被研究,例如,基于相关系数的弹性网和两两弹性网的变量选择一致性和参数估计一致性等的研究仍为空白.另外,正则化稀疏模型的一致性大都在不可表示条件和限制特征值条件等假设条件下进行探讨,然而这些假设条件在实际应用中指导性不大,而且目前主要使用人工产生的数据集对变量选择一致性进行验证,对变量选择一致性进行验证的基准实际数据集仍没有建立,这些方面还需要继续深入地研究.7.4贝叶斯观点重新审视正则化稀疏模型根据贝叶斯的观点,用损失函数+正则化罚项构造模型即相当于用似然函数+正则化罚项构造模型,而正则化罚函数本质上对应于贝叶斯模型推理中的先验信息.贝叶斯模型推理中的先验信息多种多样,远不止目前已经构造的稀疏模型中的罚函数形式,而且针对不同的问题应使用不同的先验信息,到底使用哪种先验信息,怎样判别先验信息形式的好坏,都是值得探讨的问题.7.5稀疏模型问题的再思考最原始的稀疏模型或变量选择的问题本质上应该是组合优化问题,即给定P维变量的采样样例,如何从P维变量中选择k维变量子集参与到回归系数向量的求解当中去,使得这k维变量子集是真正与输出最相关的变量,且在求得的回归系数向量尽可能低的维数情况下,对输出的预测尽可能的准确.这里牵涉到到底选择多大的变量子集,选择的变量子集是否是真正产生输出变量的原因变量,选择的变量是否能够产生足够准确的回归系数向量的问题,这些问题的权衡有时候可能是矛盾的,有的时候变量选择正确并不一定能够使得回归系数向量对输出的预测更准确.真的是在损失函数或似然函数上增加一个罚项就能实现上述所有目标了吗?如何在目标函数中引入更加精细化的约束项,体现上述所有要求,是值得认真思考的问题.而且损失函数或似然函数与稀疏罚项之间存在一个比例分配系数(即可调参数λ),使得整个问题的求解需要遍历整个比例分配系数取值范围的解路径,才能确定这个比例分配系数适当的值,除了某些特殊情况外,这使得参数选择算法复杂性太高,在实际应用中不太可行.如何解决这些问题仍需要研究.7.6稀疏方法在支持向量机中的应用稀疏方法已被应用到支持向量机(supportvectormachine)领域用来同时进行分类和变量选择,例如使用了L1范数罚的L1SVM[118]和使用了SCAD罚的SCADSVM[15]不仅仅为分类器,其还具有稀疏性,具有变量选择功能;而使用了“L1范数罚+L2范数罚的”DrSVM[119-120](doublyregularizedsupportvectormachine)同时具有3种能力:分类、变量选择和组效应.未来可以探讨融合Lasso的融合罚、两两融合罚、迹范数罚和L2,1范数罚等其他罚函数应用于支持向量机领域中得到的分类器具有何种特性.7.7模型稀疏化的质疑模型稀疏化有许多优点,但是到底是否应该稀疏化,稀疏化到什么程度才合适?最近有学者[121]对稀疏模型提出质疑,他们认为稀疏性必然带来模型不稳定,模型不稳定使得留一误差估计值不准确,最终使得模型的泛化误差不好,因此模型的稀疏化和稳定性是一对不可调和的矛盾,在使用时必须在两者之间做出恰当的权衡.另外,我们不禁要问,除了牺牲算法的稳定性以外,得到模型稀疏化的同时是否还会付出更多其他方面的代价?8结论与展望本文对各种正则化稀疏模型进行了综述,指出了各个模型提出的原因、所具有的优缺点和应用中应该注意的问题.纵观正则化稀疏模型的发展历程,其大都是根据人们期望的新特性,合理地在已有模型的目标函数基础上进行改动或直接设计新的目标函数,大致思想为或将先验信息加入到罚函数中,或将不同的罚函数进行整合,或将其推广到其他的回归模型情形下等.正则化稀疏模型为进行变量选择的有效方法,可以解决建模过程中由于高维数据集造成的过拟合问题和数值计算病态等问题,其在机器学习和图像处理等领域势必将发挥越来越重要的作用.
