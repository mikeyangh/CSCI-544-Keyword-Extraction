Page1面向近似近邻查询的分布式哈希学习方法文庆福1)王建民1),2)朱晗1)曹越1)龙明盛1),2)1)(清华大学软件学院北京100084)2)(清华大学信息科学与技术国家实验室(筹)北京100084)摘要近似近邻查询是信息检索领域中的一项重要技术.随着文本、图像、视频等非结构化数据规模的迅速增长,如何对海量高维数据进行快速、准确的查询是处理大规模数据所必须面对的问题.哈希作为近似近邻查询的关键方法之一,能够在保持数据相似性的条件下对高维数据进行大比例压缩.以往所提出的哈希方法往往都是应对集中式存储的数据,因而难以处理分布式存储的数据.该文提出了一种基于乘积量化的分布式哈希学习方法SparkPQ,并在Spark分布式计算框架下实现算法.在传统的乘积量化方法的基础上,该文首先给出了分布式乘积量化模型的形式化定义.然后,作者设计了一种按行列划分的分布式矩阵,采用分布式K-Means算法实现模型求解和码本训练,利用训练出的码本模型对分布式数据进行编码和索引.最终,该文构建了一套完整的近似近邻查询系统,不仅可以大幅降低存储和计算开销,而且在保证高检索准确率的条件下加速查询效率.在较大规模的图像检索数据集上进行的实验验证了方法的正确性和可扩展性.关键词近似近邻查询;哈希学习;高维索引;分布式计算;Spark1引言在信息技术迅速发展的今天,非结构化数据如文本、图像、视频以及音频等都呈现出指数级的增长.如何从海量的互联网数据中快速、准确地获取用户想要的信息,是非结构化大数据管理与检索中的一个重要技术问题.谷歌、百度等互联网公司所提供的文本、图像等搜索服务为我们获取信息带来了极大的便利.而在这些搜索服务的背后,都需要近似近邻查询(ApproximateNearestNeighborSearch)技术的支持.在大规模高维数据的应用场景下,精确的近邻查询需要耗费大量存储和计算资源,且查询时间太长、索引系统吞吐量过低,实际应用价值偏低.近似近邻查询技术可以大幅度缩短查询时间、降低存储和计算开销,同时保证查询结果与精确查询结果近似,因此具有更高的实用性.除了信息检索以外,近似近邻查询技术被广泛应用于机器学习、数据挖掘、多媒体管理等领域.近年来,近似近邻查询技术一直是相当活跃的研究方向,新的实现方法不断出现,但是该技术面临的挑战却没有改变.一方面,随着互联网上的数据越来越多,需要处理的数据量也越来越大,传统的树索引结构一般都是面向小规模数据而设计的单机结构.大规模数据一般无法做到单机存储,这些数据往往存储于分布式系统中,同时也需要一种分布式的索引结构来支持查询和检索.海量的数据不仅给数据存储带来了压力,同时也给实时数据查询带来了挑战.另一方面,在图像、视频、音频等非结构化数据处理过程中,往往都会对数据进行特征提取.针对特定的任务,为了获得更高的准确度,往往提取出的特征维度都比较高.例如,在图像和视频数据处理过程中,常用的SIFT、SURF特征有128维,GIST特征有960维,近年来取得突破的深度卷积网络特征为4096维,而BOW(BagofWords)词袋特征的维度更是高达成千上万维.怎样对如此高维度的数据进行快速、高效的检索是一个十分具有挑战性的问题.哈希方法可以对高维数据进行保持相似性的编码压缩,从而减少了近似近邻查询的存储空间和计算时间.在数据规模不断增长的今天,越来越多的应用都是基于存储在分布式系统中的大规模数据,例如互联网文本、图像和视频检索等.现有的很多哈希算法都是在单机的环境下实现的,而在分布式环境下,现有大部分哈希算法都要求将所有数据迁移到同一台机器中进行集中式学习,但这违背了数据的分布式存储方式,带来了很高的数据迁移代价,并且单机能够处理的数据总是很有限的,因此一种有效的分布式哈希方法是非常有必要的.为有效管理和分析分布式数据,Hadoop、Storm、Spark等大数据分布式处理系统相继涌现.本文基于Spark分布式计算平台设计并实现了一种分布式哈希学习方法,从而实现对分布式高维数据进行快速准确的检索.利用Spark分布式计算框架的特点构建的系统能够更为高效地应对大规模数据的近似近邻查询任务.本文第2节分别对基于树结构和基于哈希的两大类近似近邻查询方法进行综述;第3节介绍向量量化和乘积量化的哈希方法以及Spark弹性分布式数据集模型;第4节阐述分布式哈希学习方法及其在Spark上的设计与实现;第5节分析讨论了分布式哈希算法和近似近邻查询系统在大规模图像检索数据集上的实验结果;最终,第6节对本文工作进行了总结和展望.2相关工作对于近似近邻查询问题,在不考虑时间效率的情况下,这一问题可以直接通过线性扫描的方式来解决.比如,可以直接计算查询数据q与数据集合S中每一条数据的距离,并根据距离大小选取出距离最近的前r个数据组成一个结果列表.但由于数据集合S的规模非常大,这种朴素的查询方法的查询时间太长、计算代价太大而无法实用.但是,如果首Page3先从规模比较大的数据集合S上筛选出一个显著小的候选集合S,然后在集合S上进行朴素的线性扫描选取出前r个近邻数据,这时的线性扫描时间效率是可以接受的,整个查询过程的时间效率和准确率就取决于筛选出候选集合S的过程.候选集合S大小与查询准确率有着密切关系,一般而言,集合S越大查询准确率越高,但后续线性扫描阶段的时间就会越长;反之,集合S越小则查询准确率会越低,但后续线性扫描阶段的时间就会越短.近邻查询问题的关键就在于如何选取出一个近邻候选集合S.为了快速地筛选出近邻候选集合,就需要将待检索数据库索引起来.依据不同的索引结构分类,近邻查询的方法一般可以分为基于树结构的索引和基于哈希的索引两大类.2.1基于树结构的索引传统的树结构索引方法有很多,比如R树、KD树、Ball树等.下面介绍FLANN[1]近似近邻搜索算法库中用到的两种树结构索引方法.经典的KD树会将原始的数据空间按数据的每个维度划分成一棵二叉树.它在低维的空间上检索效率非常高,但随着维度不断增加,KD树的检索效率会迅速降低并退化为线性扫描.因此,许多改进KD树的工作涌现出来.其中,Silpa-Anan等人[2]提出了一种随机化KD树的改进方法.原始的KD树方法在空间划分时会选取数据方差最大的维度,在该维度上将空间一分为二,而随机化KD树在数据空间划分的时候,并不是固定选择数据方差最大的维度,而是从数据方差比较大的前几个维度中随机地选取一个维度进行空间划分.此外,FLANN中的层次K-Means树[3]是在数据需要划分时,使用K-Means聚类的方法将数据划分成K份.依照这一方法,在每一层中都使用K-Means聚类,当数据量少于K个时,就可以直接将这K个节点作为叶子节点.这样,层次K-Means树就可以看作是一棵K叉树.利用层次K-Means树作近似近邻查询的时候,当遍历到某个父亲节点,首先在其子节点当中选取出一个距离查询数据最近的子节点,然后再优先遍历这个子节点.显然,这样的查询效率是非常高的,能够快速地找到近邻候选集合.但是在层次K-Means树的建树过程中,在每个非叶子节点都要执行一次K-Means聚类,这种算法不管在时间效率上还是空间效率上代价都是非常大的.2.2基于哈希的索引传统树结构索引方法最大的不足就是存储空间占用过大,随着维度的不断增长,空间代价成倍增长.因此,我们需要对原始数据通过哈希进行编码压缩以节省空间.现有哈希方法主要分为数据无关哈希和数据驱动哈希.数据无关哈希方法以局部敏感哈希(LocalitySensitiveHashing,LSH)[4]为代表,其变种之一是随机投影法,该方法在不考虑数据分布的情况下将原始空间中的数据随机投影到超平面获取相应编码.数据驱动哈希方法主要通过判别数据结构及分布信息来自动学习哈希函数,代表性的方法主要有谱哈希(SpectralHashing,SH)[5]、迭代量化(IterativeQuantization,ITQ)[6]、乘积量化(ProductQuantization,PQ)[7]、笛卡尔K均值(CartesianK-Means)[8]以及组合量化(CompositeQuantization)[9]等.局部敏感哈希(LSH)的基本思想是保持相似性的空间转换,对于原始空间中相似的两个数据点,经过相同的哈希函数映射后,这两个数据点在映射后的空间中依然是相似的;反之,如果两个点在原始空间中不相似,那么映射后的两个点也是不相似的.局部敏感哈希算法首先将原始数据嵌入到汉明空间,然后在汉明空间中选取多个位置的值进行组合,作为哈希映射.最后,将上述经过哈希映射得到的序列进一步通过哈希函数转化成一个实数.这样,就实现了原始向量到哈希桶的转换.由于局部敏感特性的存在,原始空间中越相似的两个数据点,经过哈希之后,越有可能出现在同一个哈希桶中.谱哈希(SH)的基本思想与局部敏感哈希类似,但对数据的分布特点进行建模,在保证原始空间的向量相似性条件下,它将整个编码过程转化为一个图分割的过程.它首先对原始空间的高维数据进行谱分析,通过松弛约束条件转化为求解一个拉普拉斯矩阵的特征值分解问题.谱哈希的求解过程为:首先根据数据对的相似度构造一个K近邻图,该图上每个顶点代表一个数据点,该顶点仅与其最相似的K个数据点有边相连,边的权重由数据点之间的相似度确定;其次由K近邻图的邻接矩阵得到拉普拉斯矩阵,计算拉普拉斯矩阵的特征值和特征向量(图分割算法);最后选取前若干个最小特征值对应的特征向量,通过对其进行二值化得到哈希编码.原始数据空间进行降维,将原始的p维数据狓∈降成d维向量狏∈迭代量化(ITQ)首先通过主成分分析(PCA)对Page4迭代如下两个步骤直到收敛:(1)通过正交变换矩阵犚将任一向量狏旋转得到犚狏;(2)将犚狏通过符号函数sgn进行二值化,得到对应的二进制编码犫=sgn(犚狏)∈-1,{}1d.ITQ的学习目标是使整个数据集的量化误差最小,即要求旋转后的向量犚狏与编码后的哈希码犫之间的均方误差犫-犚狏2最小.3背景知识3.1向量量化向量量化(VectorQuantization)[10]就是对原始向量进行量化压缩,维度为p的原始向量狓∈通过量化函数q被映射为q()狓∈C=犮{}i,其中集合C被称为码本,C中的每个元素犮i被称为码字,映射q()狓就是将向量狓用码本C中的某个码字来表示.在向量量化中,对一个包含n个p维数据点的数据集=狓{}j据点聚成k类簇,同时用聚类中心来代表每一个类簇的数据.记矩阵犆∈聚类中心构成,每一列都是一个聚类中心,即犆=[犮1,犮2,…,犮k].向量量化模型简单有效,使用最朴素的枚举方法就可以将数据点映射到相应聚类中心.这种映射过程将每条原始数据的大小压缩到log2kbit,所需要消耗的存储空间会随着k的增长而呈现出对数级增长.3.2乘积量化乘积量化(ProductQuantization,PQ)[7]是比向量量化更有效的一种量化方法.假设需要量化压缩p维的向量到64bit,如果采用向量量化方法,则需要有264个聚类中心,这样不管是从K-Means聚类所需要的时间还是从存储聚类中心所占用的空间来看,都是不可行的.为了解决上述聚类中心(即码字)数量膨胀问题,在乘积量化算法中,首先将原始的数据空间划分为m个不相交的子空间,也就是将p维的向量切成m个长度为p/m的子向量;在每个子空间里,分别对其中的子向量集合进行K-Means聚类,聚类中心数量为h.这样就可以用聚类中心编号1~h对子向量进行编码,m个子向量的编码串接在一起就构成了原始向量的哈希编码.这样,原始空间的p维向量就可以压缩为mlog2hbit的哈希编码,从而大大节省了存储空间.乘积量化(PQ)学习方法的目标函数形式化如下:其中狓i是p维原始向量;犫j子空间中聚类后所属的聚类中心编号(即在码本中字编号),每条数据在每个子空间中仅能属于一个聚类中心,因此犫j中,整体的码本C就可以用多个子空间中码本的笛卡尔积的形式表示,C=C1×C2×…×Cm.码本的大小就是所有子空间中聚类中心数量的乘积,根据前面的假设,共有m个子空间,每个子空间聚类个数为h,所以码本大小就是k=hm.求解过程其实并不复杂,正如前文提到,在每个子空间中做K-Means聚类就可以求解出码本,这样我们就可以利用码本对每个子空间中的子向量进行编码,从而对原始向量进行编码表示.整个算法的空间复杂度就和向量维度p、子空间数量m、子空间聚类中心数量h有关.存储码本所需要的空间为O(mhp).从表1中乘积量化算法和向量量化算法的空间占用对比可知,当m=1时,乘积量化就退化成普通的K-Means向量量化了.此外,h取值越大,不仅计算时间复杂度越大,而且空间复杂度也越大,进而也会使得在查询时的时间复杂度变大.因此,选择合适的m和h的参数值是非常重要的.文献[6]中指出,为了能够用一个字节表示bj256;而对于128维的向量数据,在采用64bit进行哈希编码时,m=8是比较合适的取值.向量量化klog2kO(kp)乘积量化hmmlog2hO(mhp)3.3Spark与弹性分布式数据集Spark是一个实现了MapReduce编程范式的通用的大数据分布式计算框架,最初由UCBerkeleyAMPLab开发完成.Spark继承了MapReduce编程简单的优点,并增加了对分布式内存计算的支持.MapReduce将计算过程的中间数据存储在磁盘上,而Spark一般是用内存来存储中间数据,从而提高计算效率.在Spark数据处理过程中,数据来源不仅可以是本地文件系统或者HDFS上各种格式的数据,还可以是HBase、Cassandra等数据库中的数据.在Spark内核的基础上,还集成了SparkStreaming、SparkSQL、MLlib、GraphX等数据处理的组件,形Page5成一栈式的生态系统.图1是Spark集群系统架构图.驱动程序(Driver)会和集群的管理器(ClusterManager)相连接,驱动管理器为集群其他节点分配资源.在分配完毕以后,驱动程序会将应用程序发送到各个节点的执行进程(Executor).之后驱动程序会调配任务给各个执行进程执行任务.弹性分布式数据集(ResilientDistributedData-sets,RDD)[11]是Spark中的分布式内存的抽象.相比于MapReduce的计算过程,RDD可以被缓存在内存中,每一次的计算产生的结果都可以保留在内存中,从而避免了大量的磁盘读写操作,大大节省了计算时间.在Spark程序中,RDD的创建是通过静态类SparkContext来实现,主要包含有两种创建来源:一是从指定的文件系统(或指定的数据库)读取数据来创建;二是从内存数据集合直接生成.不同于MapReduce中仅有map和reduce两种操作,RDD还支持多种丰富的常用操作,主要分为转换操作、控制操作和行为操作3类.转换操作顾名思义,就是将一个RDD操作之后转换为另一个RDD,包括map、flatMap、filter等操作.控制操作主要是将RDD缓存到内存中或者磁盘上,比如cache、persist、check-point等操作.行为操作主要分为两类:一类是变成集合或标量的操作;另一类是将RDD存储到外部文件系统或数据库的操作.Spark的所有对RDD的操作,只有当执行行为操作时,才会执行之前的转换或控制操作.例如,我们先对RDD执行map操作,然后执行reduce操作,在map操作时,Spark并不会真正执行,只是记录,只有执行reduce操作时才会真正一起计算.这一特性称为惰性计算(lazycomputing).4分布式哈希学习及其Spark实现4.1算法整体设计算法的总体流程设计如图2所示,首先在训练数据集上进行码本的训练,得到码本模型后将其应用在原始数据集上进行编码压缩,从而可以将原始数据进行编码表示,将编码后的数据存储起来.最终,对于任意一个查询向量,通过近似近邻查询算法在编码数据集上找出近邻候选集合.在近似近邻查询过程中,主要分为两步:首先是通过索引找出候选集合;然后在候选集合上进行重新排序.4.2数据结构设计在Spark上,分布式程序的编写必须依赖分布式数据结构RDD,RDD分布式地存储在不同节点上,这样才能使得程序分布式地执行.因此,在编写Spark程序过程中如何合理设计程序的RDD数据结构非常重要.在SparkMLlib库中提供了一种自带的RDD数据结构BlockMatrix.BlockMatrix是用RDD构建的分布式矩阵,其中RDD的类型是((Int,Int),Matrix).(Int,Int)是Matrix的下标索引,BlockMatrix的每一个元素都是一个带下标索引的矩阵.BlockMatrix还提供了一些自带的函数可供调用,如add、multiply等.图3是一个示例的BlockMatrix的划分方式,图中的8×10的矩阵被按行划分成4个子空间,每个子空间上有10个子向量,具体实验过程中的参数大小与此不同.那么我们需要一个4×10的BlockMatrix来存储,图中深色框中的向量就对应了一个matrix,相应matrix的下标索引标示在图中了.在上一章节的算法中已经说明,我们需要划分m个子空间,每个子空间中有n个子向量,因此我们用m×n的BlockMatrix数据结构来表示数据是比较合适的.具体而言,我们算法中的训练集数据、原始数据、编码后的数据等都是用BlockMatrix来存储.Page64.3训练码本首先,我们将乘积量化模型的目标函数进行分布式表示,把式(1)改写成弗罗贝尼乌斯范数(Frobeniusnorm)的形式:其中犡=狓1,狓2,…,狓[证明如何从式(1)推导到式(2).证明.由弗罗贝尼乌斯范数的定义可知而犡-犆1犫1熿犆m犫m燀狓1=狓m故犡-因此式(1)可以改写为式(2).证毕.在分布式的系统中,数据是分布式地存储在拥有S个节点的计算集群上.假设第t个节点上存储的nt个数据,原来的数据矩阵犡就可以被划分成S个小的矩阵进行分布式存储,即犡=[犡1,犡2,…,犡S],其中犡i∈也可以用犅=犅1,犅2,…,犅[布式这一特点,由式(2),我们可以写出如下形式的分布式乘积量化的目标函数:从式(3)中,我们可以看出,在每一个子空间中需要求解的等式形式都是相同的.以第一个子空间为例,对于∑S犆1和犅1t,只需要对犡1t进行分布式K-Means聚类就可以得到结果.在具体训练码本的过程中,如前文中所介绍的一样,首先将训练集中的数据划分到m个子空间.然后在每个子空间中,对所有的子向量数据进行K-Means聚类,可以得到h个聚类中心,也就得到每个子空间的码本.图4表示了对训练数据集犡进行划分为m个子空间,然后在子空间中分别聚类得到码本模型犆.训练集的选取对整个算法是非常关键的,最终近邻查询的准确率一定程度上取决于训练集的好坏.在选择训练集上有两点需要注意:一是训练集的规模大小;二是训练集的代表性.一般而言,模型确定以后,训练集的规模不宜过大也不能太小.此外,训练集还应该尽可能得有代表性,尽可能广泛地分布于整个数据空间,这样才能使得训练出来的码本更好地量化原始数据空间,更准确地对原始数据集进行编码.在后面的实验过程中,我们采用对原始数据集随机采样的方法来构建训练集.码本的训练算法描述如算法1所示.算法1.训练码本.输入:训练集犡矩阵,子空间聚类数量h,聚类算法最Page7输出:码本模型数组model1.uniformlysplit犡byrowsintomsubspaces2.FOREACHsubspacei=1:minparallelDO3.[]modeli←kmeans_train犡[i],h,(4.ENDFOREACH5.RETURNmodel4.4编码压缩训练得出每个子空间中的码本之后,将其应用到原始数据集犣上进行编码压缩.首先,同样也是将原始数据集划分到m个子空间.然后在每个子空间中,对每一个数据的子向量,分别用训练出来的码本进行编码,也就是用训练好的K-Means模型进行预测,可以计算出每个子向量的所属聚类中心,从而可以使用对应的聚类中心序号对该子向量进行编码.这样,整个数据集中的向量数据都可以用编码来进行表示.完成编码压缩之后,编码后的数据集相比于原始数据集,存储空间成倍减少.图5中编码压缩过程的示意图,将码本模型犆广播到原始数据集犣的每个子空间,在子空间中对应编码压缩形成编码后的矩阵犅.具体算法如算法2所示.算法2.编码压缩.输入:码本模型数组model,原始数据集犣矩阵输出:编码后的矩阵犅1.uniformlysplit犣byrowsintomsubspaces2.broadcastmodeltoallnodes3.n←numofcolumnsof犣4.FOREACHsubspacei=1:m,columnj=1:nin5.犅i,[]j←[]modeli.predict犣i,[]()j6.ENDFOREACH7.RETURN犅4.5近邻查询在近似近邻查询的过程中,对于任意一个查询向量狇,计算狇和任意的数据库中向量狓i之间的距离时,使用非对称距离度量(Asymmetricdistancecomputation)[7]方式进行距离计算.哈希方法的出发点就是避免直接计算狇和狓i之间的欧式距离D(狇,狓i),因此如果狇与数据库中的每一个向量都计算一次距离,查询的时间代价太大.在近似近邻查询过程中,使用狇和狓i之间的非对称距离AD狇,狓()i近似表示原始距离D狇,狓()i,其中AD狇,狓()i=D(狇,狓i),狓i是狓i所属的聚类中心.D狇,狓()i可以先计算出来存储在查找表中,在之后查找比较时,用查找表中的非对称距离近似表示原始距离.在算法具体流程上,我们首先计算出狇在子空间中对应子向量和子空间中聚类中心之间的距离,将计算出的距离用一个查找表存储好.现在我们计算查询向量狇和数据库中每个数据之间的距离.在每个子空间中,因为子向量与聚类中心之间距离已经存储在查找表中,可以查找出每个数据与向量狇之间的近似距离.最后,将不同子空间中同一向量距离求和.这样就得到了查询向量狇和数据库中每个向量之间的距离.通过线性扫描一遍距离数组,我们就可以快速获取出前k个近邻向量.图6是近似近邻查询阶段的流程图.算法如算法3所示.算法3.近邻查询.输入:码本模型数组model,编码数据集犅,查询向量狇输出:近邻集合result数组1.FORi=1→mDO2.FORj=1→model[i].numOfCenters3.犇[i,j]←computeDist([]modelsi.center[j],狇)4.ENDFOR5.ENDFOR6.n←numofcolumnsof犅7.broadcastdisttoallnodes8.FOREACHsubspacei=1:m,columnj=1:nin9.dist[i]←dist[i]+犇[i,犅[i,j]]10.ENDFOREACH11.result←getTopK()dist12.RETURNresult上述的查询过程中,非对称距离与原始距离之间的误差可以用AD狇,狓()i-D狇,狓()i表示.Page8定理1.狓i-犆犫i2是非对称距离AD狇,狓()i与原始距离D狇,狓()i之间误差的上界.证明.AD狇,狓()i-D狇,狓()i由定理1可知,非对称距离与原始距离之间的误差可以被式(1)中目标函数所约束,最小化目标函数的同时,也最小化了非对称距离与原始距离之间的误差,从而保证检索的准确性.4.6系统优化集群系统上的分布式程序一般都会需要考虑节点间数据通信的问题,Spark中对RDD已经进行了较完备的封装,让开发者可以不用直接进行底层的数据管理(通信、容错),只需通过操作上层的一些接口即可,但是数据通信的问题在Spark中仍是一个不能忽视的问题,只有了解底层的通信机制才能利用API编写出高效的程序.程序优化的原则是尽可能地减少数据的通信,特别是数据量比较大的数据通信.以上是对系统的第1点优化.第2点优化是利用SparkRDD的持久化机制.SparkRDD的持久化可以将RDD数据缓存到内存或者磁盘上,以后用到RDD的时候不必多次重复计算,从而节省时间效率,这一机制特别适合在迭代算法中使用.在我们的算法中,我们选择将一些反复用到RDD缓存在内存当中.第3点优化是对Spark程序参数的调优.Spark程序需要设置一些如executor数量、每个executor的核数、executor内存大小、RDD分区数等在内的系列参数.一般而言,executor的数量乘以每个executor的核数应该与集群的总核数相同;executor的内存大小应该与集群中每台机器的内存大小除以每台机器分配的executor数相同;RDD的分区数会直接影响程序并行度,合适的分区数才能保证程序充分并行执行,这一参数设置应该与executor数量相关.5实验结果与分析在本节中,我们通过在4个数据集上进行大量对比试验,以此来观察我们在Spark上所实现的分布式乘积量化方法SparkPQ的性能和可扩展性.一方面,我们通过和单机版的PQ、ITQ、SH、LSH这4个算法在不同数据集上的对比来观测算法的性能好坏;另一方面,我们通过在Spark集群上,改变集群的节点数量、Spark参数配置、数据集大小、算法参数等来观测的SparkPQ的可扩展性和对参数变化的敏感程度.5.1实验环境和数据集实验环境.本次实验中Spark部分的实验是在由4台机器构成的SparkonYARN集群系统上完成的.其中每台机器配置相同,如表2所示.操作系统CPU核数Scala版本Spark版本非分布式的PQ、ITQ、SH、LSH算法的单机实验是在一台机器上完成,配置信息如表3所示.CPU核数MATLAB版本数据集.实验过程中,共使用了4个数据集:SIFT1M、GIST1M、CIFAR-10和SIFT100M,这4个数据集近年来被广泛用于衡量近似近邻查询方法的有效性[7-9,12-13].向量,其原始图片来源于图片分享网站Flickr.SIFT1M中的每条数据为128维的SIFT特征GIST1M中的每条数据是960维的GIST特征向量.CIFAR-10数据集是一个8×108千万极小图像数据集[14]的子集,被广泛应用于计算机视觉领域测试目标识别[15]、图像分类[16]等任务的效果,由60000张32×32大小的彩色图像组成,分属于10个类别,每个类别包含6000张图片.在本次实验中,我们对CIFAR-10数据集中的每张图片提取出320维的GIST特征,并从中随机选出1000张图片作为测试数据集,其余59000张作为训练数据集和待检索数据集.SIFT100M是SIFT1B[7]数据集的一个子集,包含数据集中的1/10数据,该数据集中的待检索集由Page9108个SIFT特征向量构成,用来验证我们提出的方法在大规模数据集上的可扩展性.整个实验部分使用的所有数据集规模如表4所示.实验部分包括算法性能对比实验和算法可扩展数据集维度训练集检索集测试集SIFT1M128105106104GIST1M9605×105106103CIFAR103205.9×1045.9×104103SIFT100M128107108104表4中维度是表示向量的维数,训练集、检索集和测试集表示的是其中向量的数量大小.训练集是用于训练哈希模型的数据集,检索集包含所有可以被编码检索到的数据库数据,测试集是指用于查询的所有数据.5.2实验设置性实验,下文将详述各部分实验的具体设置.在性能对比实验部分,我们参照文献[6],使用召回率(Recall)、查准率(Precision)、平均准确率(MeanAveragePrecision)等评价指标.通过与PQ、ITQ、SH、LSH等方法在不同数据集上的召回率、查准率和平均准确率的对比来观测算法的性能.下面列举出实验中对比的所有算法:(1)SparkPQ.Spark上分布式乘积量化方法.(2)PQ.单机版的乘积量化方法.(3)ITQ.第2.2节中介绍的迭代量化方法.(4)SH.第2.2节中介绍的谱哈希方法.(5)LSH.第2.2节中介绍的局部敏感哈希方法.在验证算法可扩展性的实验中,我们通过在表5Spark与MATLAB上实验召回率Spark0.224MATLAB0.226此外,我们还将分布式乘积量化算法与ITQ、SH、LSH等经典算法的性能对比,图7是改变查询检出相似向量的数量R,不同算法的实验召回率的变化曲线图.从图7中可以看出,我们提出的分布式乘积量化方法的召回率要比其它几种经典方法的召回率都要高,证明了我们方法的有效性.除此之外我们发现检出数量越大,召回率越高,这与召回率的定义相符.5.3.2CIFAR-10数据集实验结果在CIFAR-10数据集上的实验中,Spark集群Spark集群中改变集群的节点数量、Spark中分配的executor数量大小、训练集数据大小和子空间数量m等参数来验证算法的可扩展性和对参数的敏感程度.集群节点数量为实验中使用的机器数量,在实验中默认总节点数量为4.Executor数量为Spark分配给程序的任务执行进程个数,实验中默认设置为32.训练集大小是指哈希学习过程中训练集数据的多少.子空间数量m和子空间聚类中心数量h会影响整体数据压缩的编码长度,默认m=8,h=256,使用8log256=64bit长度编码原始向量数据.5.3算法性能对比实验5.3.1SIFT1M数据集实验结果首先我们在SIFT1M数据集上进行对比实验,通过比较Spark集群上实现的分布式乘积量化方法和单机MATLAB实现的乘积量化方法的查询召回率,以验证算法的正确性.在Spark集群系统上的程序参数设置,我们采用yarn-client模式在集群上运行程序,所有参数如5.2节所述,均采用默认设置,编码长度为64bit.在本次实验中,SIFT1M数据中有104个查询向量,对于任意一个查询向量,通过整个近似近邻查询计算,我们在数据库向量中找到前100个近邻向量,并分别取检出数量R为1、2、5、10、20、50、100,计算出前R个向量中出现最近邻向量的次数s,那么衡量标准召回率=s/104.表5中是Spark集群上分布式乘积量化方法和MATLAB上实现的乘积量化方法的召回率对比.从表中可以看出,Spark上分布式算法的召回率与MATLAB单机版本的相差不大,从而证明了算法正确有效.0.5930.607系统的配置以及程序的参数设置和SIFT1M实验中完全相同.对于CIFAR-10中60000张图片,随机产生1000张图片作为测试数据,其他59000张图片用于训练和检索,对于图片,我们提取出320维的GIST特征,使用欧氏距离先计算出1000个测试数据的与剩余待检索数据的近邻关系.使用此近邻关系作为衡量基准,对于每个算法,我们分别计算检索的查准率、召回率以及平均准确率,以此对比不同算法的性能好坏.在图8中,我们可以看到不同算法的查准率变Page10化,检出数量R从0增加到1000,随着检出数量增大,所有算法的查准率都在不断降低.然而不管检出数量如何变化,分布式乘积量化算法的查准率均比其它算法高.图9则反映的是检出数量R从0变化到1000,不同算法的召回率变化情况.同样,分布式乘积量化方法的召回率高于其他所有算法.图10展现了不同算法在不同编码长度下的平均准确率变化情况.在编码长度分别为8、16、32、64、128情况下,我们可以看到,对于所有算法,编码比特数越大,算法的平均准确率越高,即平均准确率随着比特数的增大而升高.然而,不同算法的增长幅度是不同的.我们可以比较SparkPQ和ITQ,ITQ在从8bit到16bit长度情况下,平均准确率增长幅度较大,之后逐渐趋缓;SparkPQ在8bit长度的平均准确率较高,最初的增长幅度也不大,但是在64bit变化到128bit长度时,平均准确率增长的幅度较大.由此可以看出,SparkPQ在编码长度较长时,增加编码长度的时仍能保持较好的平均准确率增幅.图10不同算法CIFAR-10上的平均准确率对比图11是我们从实验中选取出的3个相似图像检索的示例,对于每个查询请求的图片,选取出前36个最相似的图片.同样,在这个实验中对图片提取320维的GIST特征,采用特征向量的欧氏距离近邻关系作为参考标准.图中深色框标识出的图片与查询图片并不近邻.从图中可以看出,对于这3个查询请求,其中分布式乘积量化的返回结果是最精确的,迭代量化方法次之,局部敏感哈希和谱哈希的准确度较差.5.4算法可扩展性实验5.4.1SIFT1M数据集实验结果本实验通过比较不同节点数量的Spark集群上的乘积量化的近似近邻查询方法实验的召回率和时间消耗的对比,用于验证算法的可扩展性.本次实验分布式乘积量化算法部分采用默认配置,通过改变集群中节点数量,观察不同检出数量的召回率变化与计算时间随集群节点数量的变化.表6和表7分别显示的是在不同节点数量的Spark集群系统上实验召回率对比以及所用时间对比.Page11图11CIFAR-10数据集上64位编码长度图像检索示例表6不同节点数量Spark集群系统上的实验召回率节点数召回率(R=1)召回率(R=2)召回率(R=5)召回率(R=10)召回率(R=20)召回率(R=50)召回率(R=100)10.22720.23130.22040.224表7不同节点数量Spark集群系统上的实验时间节点数训练时间/s编码时间/s单次查询/s1234仅从表6来看,不同节点数量的Spark集群系统上实验的召回率相差不大.在保证召回率变化不大的情况下,从表7中我们可以看出在训练、编码和查询3个阶段的时间消耗上,随着节点数量的增多,训练时间迅速下降并逐渐趋缓,并没有保持理想的线性下降趋势,这是由于节点增多时,Spark集群的额外计算开销增多,包括网络通信时间等.整体而言,时间消耗随着节点增加而下降说明该系统的可扩展性良好.此外,我们还在SIFT1M数据集上进行训练集0.6040.6020.6000.593大小与训练时间关系的实验.通过改变不同训练数据集的大小,从而对比实验的召回率和时间消耗情况,用以验证算法可扩展性.在实验参数设置方面,我们采用与上一实验中的4节点Spark集群实验相同的配置.同样我们采用召回率作为查询结果好坏的衡量标准.不同之处,我们分别取训练集大小为0.1M、0.2M、0.5M、1M进行实验,观察实验召回率和时间消耗的变化.从表8来看,改变实验中的训练集合大小,随着训练集增大,召回率呈现出增长的趋势,但是并不明显.从表9中我们可以看出在随着训练集的增大,训练时间迅速地增大,随后逐渐趋于缓慢增长.编码时间和单次查询之所以时间变化不大,是因为并不受训练数据集大小变化的影响.因此,实验表明系统的可扩展性较好.Page12训练集大小/M召回率(R=1)召回率(R=2)召回率(R=5)召回率(R=10)召回率(R=20)召回率(R=50)召回率(R=100)表8Spark集群系统上不同训练集大小实验召回率0.10.20.51.0表9Spark集群系统上不同训练集大小实验时间训练集/M训练时间/s编码时间/s单次查询/s0.10.21235.75.221.480.52601.55.181.471.05616.34.931.475.4.2GIST1M数据集实验结果对GIST1M数据集的实验都在Spark集群系统上完成,Spark集群系统的配置与SIFT1M的实验相同,在yarn-client模式运行程序,executor的数量为32.在本次实验中,我们分别采用32bit、64bit、96bit、128bit、256bit这些不同编码长度进行实验,观察实验中压缩编码长度对召回率的影响,如图12所示.在同等检出数量下,编码长度越长,召回率越高.图12GIST1M上不同编码长度下的召回率图13中是记录了在GIST1M上不同编码长度的算法运行时间.从图13可以看出,随着编码长度的增大,算法的训练时间、编码时间、单次查询时间都是在不断增长的,但是增长的幅度较小.时间不断增长是因为编码长度mlogh增大,子空间m数量就会增大,从而需要在更多子空间中训练码本、压缩编码和检索近邻,增加了计算时间.5.4.3CIFAR-10数据集实验结果与上一个实验配置相同,我们在CIFAR-10数据集上的分别对不同长度的编码进行比较,采用准图13GIST1M上不同编码长度的算法运行时间确率(Precision)来衡量检出效果.在图14中,R是检出数据的数量,我们可以看到在同样的压缩编码下,准确率随着检出数量增大而不断降低.在相同的检出数量条件下,编码长度越长,检出的准确率越高.图14CIFAR-10上不同编码长度下的查准率图15是不同编码长度下的算法运行时间变化情况,从图15中可以得出与图13中实验相似的结论,编码长度分别为32bit、64bit、96bit、128bit、256bit,算法的训练时间、编码时间、单次查询时间都是在不断增长的,运行时间增长的速度与编码长度的增长幅度有关.图16中是在压缩编码长度分别为32bit、64bit、Page13图15CIFAR-10上不同编码长度的算法运行时间96bit、128bit、256bit情况下,训练时间随executor数量的变化.从图16中可以看出,大体上executor的数量越多,训练时间就越短,但是也并非executor数量越多越好,与Spark集群系统的总处理器核数有关.当executor数量超过集群的总核数时,训练时间反而会增加.实验中集群的总核数为32,因此当executor数量为48时,训练时间略大于executor数量为32的情况.图16CIFAR-10上训练时间随executor数量的变化5.4.4SIFT100M数据集实验结果使用SIFT100M数据集进行实验主要是为了在一个更大规模数据集上验证算法的可扩展性.由于输入数据规模变大,因此Spark集群相关参数需要有所调整.集群节点数仍为4,executor的数目为15,每个executor分配内存大小为12GB,其它参数不变.在此我们比较了不同编码长度下召回率的变化以及计算时间的变化.图17是SIFT100M上不同编码长度下召回率的变化图.由于数据集较大,当编码长度为16时,召回率几乎接近于0,随着编码长度增大,召回率也不断提高.图18是不同编码长度的算法计算时间变化图,随着编码长度增大,计算时间呈现出缓慢增长.以上观察与小数据集时的情况相似,从而证明了在较大规模的数据集上,我们的方法依然具有良好的可扩展性.图17SIFT100M上不同编码长度下的召回率图18SIFT100M上不同编码长度的算法运行时间6结束语本文针对大规模高维数据的近似近邻查询问题,通过对乘积量化的哈希方法的深入研究,我们在Spark平台上实现了一套基于乘积量化分布式哈希方法的近似近邻查询系统.该系统一方面对数据进行了编码压缩,从而可以大幅度降低空间占用;另一方面在保证查询准确率的同时,通过Spark集群系统并行计算的方式可以大大提高查询的效率.Page14本文已经实现了一套基于Spark的近似近邻查询系统,采用乘积量化的方法进行分布式哈希编码,但并没有在Spark上实现一套高效的索引方法.在接下来的研究中,我们准备实现一套高效索引方法[17]以提高查询的性能.致谢感谢清华大学信息科学与技术国家实验室大数据科学与技术专项、国家自然科学基金项目、中国博士后基金特别资助项目的支持.感谢《计算机学报》编辑部和审稿专家的宝贵意见!
