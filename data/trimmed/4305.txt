Page1面向异构多核架构的自适应编译框架白秀秀董小社刘超曹海军李亮(西安交通大学计算机科学与技术系西安710049)摘要针对应用在移植到异构多核高性能计算机系统中所面临的可移植性差以及性能优化难度大的问题,文中提出一种面向异构多核架构的自适应编译框架.通过源到源编译解决传统并行编程模型应用向异构多核架构的映射问题;同时利用动态剖分信息,自适应地调整插桩并配置优化策略,形成迭代式的自动优化过程.文中自适应编译框架将软硬件映射机制与优化策略结合,有效地解决了同构并行应用向异构多核架构的移植问题并提高了应用的整体性能.实验结果表明,文中基于Cell架构实现的原型系统,很好地解决了异构多核架构下应用移植性等问题,同时应用性能有所提高.关键词异构多核;源到源编译;插桩;迭代式优化1引言目前,功耗和散热问题限制了处理器主频的提高,多核已成为提高系统性能的有效方法,其中异构多核更是高性能计算技术的发展趋势.异构多核技术通过在片上简化单个核设计,增加了专用处理核的个数,以较小的成本和功耗获得更高的计算性能.Page22010年11月在世界超级计算机top500中位列第一的“天河一号”超级计算机就是采用CPU+GPU的混合架构,IBM的Roadrunner采用Cell架构,而目前异构多核系统面临的最大挑战是编程模式.Cell架构是目前异构多核系统架构的典型代表之一,基于CellBE(CellBroadbandEngine)架构的Cell[1-2]提供了强大的浮点计算能力,并可用于科学计算.Cell处理器内置一个通用PPE(PowerPCProcessingElement)和8个SPE(SynergisticPro-cessingElement),PPE和SPE具有不同的ISA(InstructionSetArchitecture).Cell处理器的计算能力主要来自SPE,每个SPE有256KB可直接寻址的片上本地存储LS(LocalStorage).SPE只能直接访问LS中的数据和代码,当SPE上运行的计算代码和数据大小超过256KB限制时,需将部分数据和代码放在片外主存,必要时通过DMA(DirectMemoryAccess)操作从片外主存中获取.由于SPE的LS存储容量受限,因此在CellBE架构下的编程需要管理消息编制,才能使得SPE执行时接近峰值吞吐量.Cell的另外一个显著特征是SPE中有128个128位的向量寄存器,因此在软件开发时需要合理编排和组织数据,以便最大限度地发挥SPE的计算能力.从目前应用软件的编程模式来看,大多数编程工具与应用仍是基于x86架构,所以要想充分发挥Cell架构的性能,首先需要解决已有应用程序的移植问题.异构多核的出现为高效能计算系统的构造提供了契机,但其显式的并行模式及多级的存储层次给编程带来困难,如何屏蔽异构多核底层软硬件环境,充分挖掘异构多核环境下应用的可移植性及性能成为目前亟待解决的问题.本文提出了一种面向异构多核架构的自适应编译框架,通过源到源编译解决同构并行程序向异构多核架构的移植问题;通过自适应调整插桩并配置优化策略,形成迭代式的自动优化过程,将软硬件映射机制与优化策略结合,充分利用架构提供的软件显式管理资源能力,以提高应用性能.本文第2节介绍相关领域的研究工作;第3节介绍自适应编译框架;第4节对原型系统进行测试验证;最后在第5节中进行总结并探讨下一步工作.2相关工作异构多核架构下的应用可移植性与性能优化问题,受到业界广泛关注.一种研究方法是在较高抽象层次使用制导语句和代码分析技术,通过源到源编译将原应用程序代码映射到通用处理器与协处理器.BarcelonaSupercomputingCenter的CellSs[3]采用一种在函数级支持类似于OpenMP制导语句的方式,实现软件代码向CellBE硬件的自动编译映射;随IBM的CBESDK一起发布的ALF支持SPMD(SingleProgramMultipleData)并行模式,用于解决数据并行问题;UniversityofToronto的hiCUDA[4]在串行程序中加入制导语句标明可并行执行的代码,通过源到源自动映射机制将hiCUDA程序转换为GPU的CUDA程序.针对具体协处理器的编程模型,如Sequoia[5-7]、Merge[8]等.Sequoia是Stanford面向CellBE处理器推出的一种基于内存层次感知(MemoryHierarchyAwareParallelProgramming),直接针对CellBE处理器存储层次进行程序设计的语言;Stanford的Merge是基于集成现有处理器编程语言能兼容多种处理器的编程框架,支持在运行时根据任务的特征将任务映射到协处理器或通用处理器上;文献[9]针对RAxML应用,基于CellBE平台通过在任务并行级与循环级挖掘并行性,优化存储器访问;Ohara等人[10]提出的MPIMicrotask通过将任务分解为可在SPE中运行的微任务并最终转换为流处理器模型[11]达到充分挖掘Cell计算能力的目的.微软提出的Accelerator[12]基于GPU平台利用数据并行性达到通用目的计算;文献[13]最大利用GPU计算单元以及隐藏内存访问带宽延迟,通过采用数据分块以及数据预取等技术充分利用GPU片上资源及全局内存带宽,使总体应用性能获得了提升.上述编程模型和映射机制主要还存在以下问题:(1)类似OpenMP加制导语句的编程模式具有较好的通用性和扩展性,但缺乏对协处理器底层硬件特征支持的性能优化策略,应用执行效率差,不能有效利用和发挥协处理器的性能;(2)针对底层具体协处理硬件架构的编程模型可获得较好的性能,但缺乏对传统编程模型的支持,传统应用移植困难.因此,本文提出了一种基于插桩分析的自适应编译框架,该框架通过插桩收集信息,并根据程序在新型系统架构下运行时的反馈信息,自适应调整插桩并配置优化策略,形成迭代式的优化过程,将软硬件映射机制与优化策略有效结合,解决了异构多核架构下应用的移植性差问题并提高了整体性能.Page33自适应编译框架3.1统一架构抽象异构多核架构的主要计算能力来自协处理单元,对应协处理单元都拥有各自的小容量局部存储,数据可在局部存储与主存之间传输.这类架构不同处理单元的编程环境也不同,从目前应用软件的编程模式来看,多数编程工具与应用仍然是基于x86架构,所以要充分发挥混合架构的性能,首先需要解决已有应用程序的移植问题.另外,因协处理单元的局部存储容量受限,将应用映射到这类异构多核架构时,需考虑消息的编制与数据布局等问题.基于这类异构多核架构,软硬件映射机制需要解决的问题如下:(1)如何确定应用中的计算核心;(2)若将计算核心下载到协处理单元执行时,如何解决计算核心在协处理单元的编程问题;(3)由于协处理单元的局部存储容量受限,如何管理数据通信问题.为解决上述问题,本文在传统多核存储模型的基础上建立以协处理器为中心的层次化存储模型,对异构多核架构进行了抽象,并在此基础上研究软件向硬件的映射机制及性能优化方法,提出了面向异构多核架构的自适应编译框架,使用源到源编译转化时迭代式的优化方法将软硬件映射机制与优化策略相结合.如图1所示,本文建立以协处理单元为中心的层次化存储模型,包括通用处理单元的主存,协处理图2以协处理单元为中心的程序执行模型3.2总体框架为了解决异构多核环境下应用可移植性与性能优化问题,本文提出一种面向异构多核的自适应编译框架如图3所示,由源到源编译、自适应插桩与策略分析3部分组成.该框架通过源到源编译将同构架构下应用的计算核心移植到协处理单元,并负责单元的局部存储和寄存器.其中通用处理单元可以自由访问主存,每个协处理单元可以自由高速访问其本地的局部存储和寄存器,主存和协处理单元局部存储之间采用数据总线进行连接.以协处理单元为中心的层次存储模型淡化了主存在存储模型中的地位,将数据尽可能多的置于协处理单元的局部存储中,从而减少访存延迟.图1以协处理单元为中心的层次化存储模型基于上述层次化存储模型,本文建立以协处理单元为中心的程序执行模型,如图2所示,该执行模型在于淡化通用处理单元在计算中的地位,将应用计算核心下载到协处理单元上进行计算,同时对局部存储的管理交由协处理单元完成,而将系统资源管理与控制部分保留在通用处理单元,此外从应用逻辑中分离出针对性能优化的配置框架,与优化库结合进一步地挖掘应用性能.局部存储的管理,同时从应用逻辑中分离出与性能优化相关的配置框架,解决了异构架构下应用程序的可移植问题,但应用性能并没有得到充分发挥,要进一步挖掘应用性能,则需分析动态剖分得到的运行时信息,合理管理局部存储并利用数据布局对生成代码进行优化.Page4各模块作用如下所述:(1)源到源编译.在自适应插桩与优化策略的配合下,完成传统编程模型的应用向异构多核架构的映射转换,实现新型系统下应用可移植性.在映射的过程中,涉及到确定计算核心以及将其移植到协处理单元的编程问题、对一些支持库的显式调用进行封装以及调度管理插桩模块.(2)自适应插桩.通过源到源编译调用该部件为生成代码进行插桩,收集相关信息与运行时反馈信息.需要的信息内容是由上一级配置框架中的决策来决定.(3)策略分析.通过分析收集到的相关信息与运行时的反馈信息,获取相关决策,如计算核心与优化策略,间接地指导下次的源到源编译优化.图4所示总体框架的工作流程如下:象信息写入到寄存文件中.译调用与统计对象相关的函数进行预插桩.(1)在对输入源程序首次编译时,由源到源编(2)应用程序在运行的过程中,将收集到的对(3)寄存文件中信息经策略分析后得到相应的决策.(4)通过决策判断应用是否需要进一步优化,若已达到满意程度则输出最终生成优化代码,否则转到第(5)步.(5)配置框架根据反馈调用自适应插桩模块,结合到下次源到源编译过程.(6)回到步(2)继续执行.这是一个迭代式的自动优化过程:编译运行、收集信息、分析信息、自适应插桩、编译运行、收集信息继续优化,最终将优化策略“固化”在最终生成的代码中.输入源程序在此过程中不断地演变,生成的最终优化代码在对应的异构多核架构下具有良好的性能.3.3源到源编译自适应编译框架采用源到源编译解决传统的消息传递模型MPI或Pthread编程模型的应用向异构多核架构映射转换问题.如图5所示,在应用映射转换的过程中,源到源编译模块主要负责完成以下功能:(1)将计算核心下载到协处理单元,应用的其他部分保留在通用处理单元,由主控部分调用执行计算核心,同时将数据拷入到协处理单元的局部存储.在启动计算核心后,协处理单元访问局部存储的数据,当出现数据中断时,由消息管理完成与通用处理单元数据通信任务;(2)源到源编译将配置框架与具体应用逻辑分离.配置框架为策略分析模块与自适应插桩之间的接口,分别从任务层、数据层与存储层辅助性能优化;(3)调度管理插桩在相邻两次映射过程起到承接的作用,从而使整个系统形成自适应编译过程.插桩管理部件由signal(i)启动,具体如式(1)所示.Page5signal(i)=1,表示启动相应插桩其中i=1为计数桩获取应用中计算核心,i=2为获取数据块结点的工具,i=3为获取存储层特征的工具.后两级插桩是否启动由上级插桩的程序运行后反馈信息决定.3.4自适应插桩在解决了应用程序向异构架构下移植问题的基础上,需要进一步挖掘新型架构下的应用性能,自适应编译框架通过分析应用自身特征与具体架构结构寻找相关优化措施.由于优化应用性能时需要分析运行时信息,因此提升性能是一个迭代式优化过程.如图6所示系统以配置框架为接口,通过自适应插桩模块对应用进行不同层次剖分得到运行时信息,进而由策略分析做出相关决策,指导下一级优化,依次此循环生成最终优化代码.自适应插桩分别从不同层次对应用进行剖分,包含任务层、数据层以及存储层.前两层与应用自身特征相关,后者与具体架构结构相关.任务层通过分析应用子任务执行时间及其占总执行时间的比例得到应用中的计算核心.对于大规模应用,系统可用相应较小规模数据进行任务层移植,从而降低开销.数据层通过分析计算核心中对应数据块上下界、大小以及访问频度等信息确定数据块是否需要进一步剖分.存储层通过分析计算核心在对应内存块上的数据访问行为以及具体架构存储结构特征确定应用中计算性能瓶颈并给出优化策略.为了准确分析存储层所需要的数据访问行为特征,本文定义流为一段内存访问元素序列的子序列.该子序列中任意两个相邻元素的距离称为步长.根据流中步长的概念可以分析流的访存行为特征并对其进行划分.设一个流stream中出现的步长为离散型随机变量S,所有步长可能的不同取值为sk(k=1,2,…,n),出现不同步长{S=sk}的概率表示为P{S=sk}=pk(k=1,2,…,n),满足条件∑n系统根据概率分布不同对访存行为特征通过下列公式定义不同的流,其中ε为任意小的正数.定义1.定义1中命题P表示一个流的步长序列中出现一个非常高频率的非零步长sd,说明此流的步长稳定趋于此步长,此流称为规则流,其中sd称为热点步长.命题Q表示此流中热点步长sd为1,这种流称为顺序流.下列用命题逻辑公式定义不同流.Rregseq=P∧Q,Rregseq为真表示stream为规则顺序流烄Rregnon=P∧Q,Rregnon为真表示stream为规则非顺序流烅Rirreg=P,Rirreg为真表示stream烆定义2.在二级插桩中主要是为了检测是否存在热点步长sd,并判断sd是否为1.系统根据定义2确定数据访存行为特征,并结合具体架构存储结构特征,通过策略分析模块得到相应优化策略,深入优化下载到协处理单元上的计算核心.3.5策略分析策略分析需结合底层优化库进行论述,本文基于Cell架构层次化存储特点与协处理器存储访问方法,提供了一个访存库[14],其中包括4种访存方式,针对应用的不同访存特征选取最有效的方式以提高应用性能.这4种访存方式:批量访存、单缓冲、四路缓冲和组相联,后3种统称为按需访存.与按需访存相关的缓冲区参数有组数、路数和块的大小.面向Cell架构的迭代式优化过程中,系统首先确定计算核心中访存变量采用哪类访存方式性能最优,判定过程中采用命题演算方法;若采用按需访存,则根据基于离线的经验搜索制定的规则确定其缓冲区的参数,完成存储层优化,进一步提高应用性能.3.5.1数据层优化系统根据数据层剖分信息对计算核心的访存变量确定其访存方式.一个计算核心的每个数据块大小表示为Size(k)(k=1,2,…,m),其中m表示数据块的块数,数据块总大小为AllSize=∑m处理器SPE上局部存储空间用LS表示,其中LS=256KB,除去计算任务代码所占空间LSc,剩余空Page6间记为LSd.访存策略用MemStrategy(i)(i=1,2,3,4)表示,式(4)中具体指定不同的访存方式.式(5)中命题T表示访存区域小于除去代码所占空间的局部存储空间,若计算核心中的访存区域不确定,则表示为Tunk.规则1.策略分析模块根据规则1采用命题演算选择合适的访存方式,如式(6)所示.批量访存适用于访存区域小于协处理单元LSd;按需访存适用于访存区域大于LSd或无法确定的情况.对于按需访存,则需进一步判断参数的访问特征,若是规则顺序流,则采用单缓冲方式;若是规则非顺序流,且在访存区域很大的情况下采用四路缓冲方式,否则采用组相联方式;若是非规则流,则选取组相联方式.T→MemStrategy(1)烄(T∧Rregseq)∨(Tunk∧Rregseq)→MemStrategy(2)(T2∧Rregnon)→MemStrategy(3)烅(T1∧Rirreg)∨(Tunk∧Rirreg)∨(Tunk∧Rregnon)→MemStrategy(4烆3.5.2存储层优化对于按需访存,若将代码和数据直接放入LS可能会导致程序运行崩溃,只能将计算函数分为多个计算单元Cu,调用过程中按照Cu对数据的需求通过DMA操作按需若干次将数据读入LS的数据缓冲区中.策略分析模块对其缓冲区的组数、路数、块大小进行配置,分别用参数set、way、line表示.由于Cell架构中一次DMA操作的传输上限为16KB,且要求至少是16字节对齐.因此,可得到如式(7)的初始条件.bufferSize=(1set)×(1way)×(1line)烄bufferSize<256KB烅lineSize=1line烆16BlineSize由式(7)可得,组相联方式通过使用软件的方法在LS中建立一个组相联的缓存,可以提高细粒度的非规则应用的数据复用机会.在我们提供的组相联缓存中,设定每组有4路(way=2),缓存查找表中一个组的4路放到一个矢量中,这样主存中的一块可以映像到缓存的这4路中.单缓冲方法在LS中设定一个缓冲区,在数据访问点检查所需的数据是否在缓冲区中,若不在则使用DMA操作从主存读取一个缓冲区的数据放入缓冲区.对于规则非顺序访存时,在单缓冲区的基础上建立4个缓冲区,当SPE使用其中一个缓冲区的数据时,其余缓冲区利用异步DMA操作读取将要使用的数据.根据以上要求,系统按照规则2对缓冲区参数进行预设.由此确定这3种按需访存参数(set,way,line)的取值域,如式(9)、(10).MemStrategy(i)→CacheConf(i,set,way,line)CacheConf()=规则2.i=2,set=0,way=0,4line14烄i=3,set=0,way=2,4line12烅i=4,0set11,way=2,4line14,烆set+line协处理器SPE启动时,将一个计算核心代码段与相关数据段调入SPE的局部存储LS中.因此,在需要缓存策略配置时,需要综合考虑该计算函数的几个数据块访问区域,设一个计算核心有varNo个访存参数,那么须满足varNo×bufferSize<256KB.在符合上述条件的前提下,缓冲区设置的越大性能越好,因此单缓冲与四路缓冲参数便可设定.针对组相联的情况,系统还需考虑组数与块大小之间的平衡关系.本文采用了基于离线的经验搜索,由源到源编译生成与具体应用逻辑分离的策略配置文件,针对不同的参数配置,形成一个最佳策略的搜索空间,对其进行系统化观测,找到最优配置方案,并制定出相应的规则.应用程序移植过程中根据该规则进行存储层优化.如图7所示,在Cell平台上,512×512的单精度浮点矩阵乘法中对分块B使用组相联方式,通过调整组数与块大小来调整缓冲区总大小.经过分析获得以下3条原则:(1)缓冲区容量越大,应用的计算性能越好;(2)对于不同分块大小的计算,当缓冲区的容量小于分块大小的四分之一,组数为1(set=0)时即可得到较优性能;(3)当缓冲区的容量达到分块大小的四分之一或二分之一时,调整组数计算性能得到很大的改善;当缓冲区的容量超过分块大小的二分之一时,调整组数对计算Page7性能影响不大.本文使用的四路组相联,当缓冲区容量达到分块大小的四分之一,可以得到很好的性能.根据以上原则制定出指导存储层优化的规则3如式(11)~(13)所示,其中mostStep(k)(k=1,2,…,m)为数据块中出现概率最大的步长,系统根据数据流的特征调节缓存大小以及组内块大小以利用空间局部性,从而有效降低缓存的失效率,提高性能.通常情况下,当一个组内块总大小达到数据流最大概率步长大小时,性能达到最优.在应用移植的过程中,系统根据规则2与规则3对存储层进行优化,进一步提高应用性能.规则3.bufferSize=1(ceil(log2Size(k)/4))烄2blockNo-1<mostStep(k)2blockNo烅lineSize×2way=2blockNo×sizeof(type_data烆4验证与测试4.1实验环境本文基于PS3(PlayStation3)平台对提出的面向异构多核架构的自适应编译框架进行了测试验证.PS3配置了工作主频为3.2GHzCell处理器,包含一个PPE与6个SPE,256MB主存,每个SPE配备256KB本地LS;操作系统为FedoraCore6,软件环境为CBESDK2.0.4.2实验结果本文测试结合已有访存库验证自适应编译框架的有效性和通用性.实验中采用了6类测试用例包含基数排序(radixSort)、基于距离的迭代聚类算法(k-means)、分块矩阵相乘(BMM)、分块稀疏矩阵LU分解、n-body问题(采用PP算法)、MolecularDynamics分子动力学模拟(MD).这些程序在不同领域有所应用,如科学计算、数据挖掘、物理学及流体动力学等.表1给出了这6类测试用例的问题规模.实验采用程序执行时间为评价标准.测试用例radixSortk-meansBMMLUn-bodyMD4.2.1系统性能测试与有效性分析本文在Cell架构实验平台下对自适应编译过程的3个阶段进行评测,最终优化结果与采用IBM提供的CBESDK默认方式的结果对比,从而验证本文提出的自适应编译框架的有效性.自适应编译框架中任务层移植将程序中计算核心移植到协处理单元,解决应用面向异构多核架构的移植问题;数据层优化根据应用特征对应用进行优化;存储层优化结合具体架构的存储特征对应用进行优化.因此,输入源程序最多需要迭代三次便可生成最终的优化代码.如图8性能评测中,前2类应用radixSort与k-means采用小规模数据集进行测试,完成任务层移植及数据层优化后,经系统的策略分析模块根据Page8相应规则判断出不需要进行存储层优化,因为小规模数据集可一次性存入局部存储,系统不需对存储层进行分析.从图8中可以看出,radixSort与k-means经数据层优化后的性能与采用IBM默认设定的访存方式相比分别提高25%与27%.测试用例BMM、LU、n-body及MD采用大规模数据集进行测试,系统完成任务层移植与数据层优化后,经策略分析模块做出相应的决策,即应用性能可通过存储层优化进一步提高.这是因为对于大规模数据需分批存入局部存储,系统可根据数据流特征重新组织映像方式的结构,可有效降低缓存失效率,从而提高性能.从图8中可以看出,应用通过存储层优化后与只进行数据层优化后性能相比平均提高21%,从而验证了存储层优化对采用大规模数据的应用是有效的.这4类应用最终优化后的性能较IBM默认设定的访存方式平均提高32%.系统中数据层优化根据计算核心访存变量的特征,选取合适访存方式,充分利用协处理单元中有限的局部存储空间,提高计算性能.从图8中可以看出,前5类应用通过数据层优化后性能与采用IBM默认设定的访存方式相比平均提高24%,从而验证了数据层优化的有效性.而MD通过数据层优化后性能不如IBM默认设定的访存方式的性能,这是因为MD相对其他5类应用程序中存在大量3.4节定义的非规则流导致的.系统根据数据层剖分信息分析出,MD程序在计算过程中内存访问步长很不稳定,根据规则1判断出这些访问变量需使用组相联映像方式,而对于大量非规则流使用固定规模的组相联方式,不能很好的降低缓存失效率,从而导致MD程序经过数据层优化后性能改进不明显.存储层优化对于组相联映像方式可根据非规则流的特征通过调节缓存大小以及组内块大小,利用空间局部性有效降低缓存的失效率进一步提高性能.因此对于这类应用可通过文中存储层优化更有效地提高性能.从图8中可以看出,MD经存储层优化后性能较采用IBM默认访存方式相比提高29%,进一步验证了存储层优化的必要性和有效性.综上,自适应编译框架中数据层优化对移植到异构多核架构的应用进行优化时,可有效地提高性能.而存储层优化对大规模数据的应用以及存在大量非规则流的应用能进一步地提高性能.如图9所示,本文自适应编译优化后程序性能随SPE个数增加表现出的平均加速比优于采用IBM默认访存方式.4.2.2系统开销针对同一个应用,其计算核心是确定的,任务层剖分只需执行一次,因此对于大规模应用,系统通过输入较小规模数据进行任务层剖分,将计算核心移植到协处理单元,进一步对该应用进行数据层及存储层优化.对于系统中数据层与存储层剖分开销计算如式(14).设定应用中每个计算核心的执行时间为kernelTime(n)(n=1,…,N),其中N为计算核心个数,优化后计算核心的执行时间为kernelTime1(n)(n=1,…,N),每个计算核心的迭代次数为iteration(n)(n=1,…,N),应用中非计算核心的执行时间为otherTime,决策所需时间为decideTime,数据层与存储层剖分开销时间为overhead,应用优化后执行时间为runTime.overhead=otherTime+∑N烄decideTime烅runTime=otherTime+∑NkernelTime1(n烆当问题规模增大时,应用中计算核心的迭代次数将增大,根据式(14)overhead/runTime趋势将变小,说明系统中剖分开销随着问题规模增大而减小.如图10所示,前2类测试用例radixSort与k-means采用小规模数据集测试,其开销时间占总执行时间的21%;后4类测试用例采用大规模数据集测试,Page9其开销时间占总执行时间的3%.当应用通过自适应编译生成最终优化代码后,即优化策略“固化”在生成代码中,应用在以后的执行过程中则不再需要额外的开销.4.2.3自适应编译过程测试以分块稀疏矩阵LU分解为例进行测试,其中分块大小设为256×256,规模大小分别使用1K×1K、2K×2K、3K×3K及4K×4K单精度浮点数.采用本文提出的自适应编译框架,渐进获取应用中的计算核心,数据块信息与访存行为特征,从而对应用进行数据层优化与存储层优化,有效地改善应用计算性能.其中函数fwd()用于对角线子块及与它同行的其他子块进行计算;函数bdiv()用于对角线子块与它同列的其他子块进行计算;函数bmod()用于处理上述子块外的其他子块.以1K×1K的矩阵规模为例验证自适应编译过程.表2所示LU分解中的相关函数执行情况,得到LU分解的计算核心为bmod()、fwd()、bdiv().函数bmodfwdbdiv在任务层剖分得到计算核心的基础上,进一步剖分得到计算核心中参数的访存行为特征.由表3中剖分信息与3.5.1节中规则1结合可知,函数bmod()中访存数组row和inner热点步长是1,为规则顺序流,设定为单缓冲方式,数组col热点步长是256,为规则非顺序流,设定为组相联;函数fwd()中访存数组col与diag为规则非顺序流,设定其访存方式为组相联;函数bdiv()中访存数组diag与row都为规则顺序流,设定其访存方式为单缓冲.函数参数热点步长热点步长比例/%数据块大小/Bytebmodinner199.61fwdcol25696.12bdivdiag199.22根据系统提供的数据层优化策略,进一步对应用进行存储层优化.根据规则2与规则3设定单缓冲参数配置为(set=0,way=0,line=13),组相联参数配置为(set=6,way=2,line=8),LU分解的计算性能达到最佳.如图11所示,LU分解通过数据层优化后较采用IBM默认的访存方式性能平均提高42%,进一步通过存储层优化后性能平均提高到48%.图11自适应编译过程中不同阶段性能测试如图12所示,LU分解剖分开销占总执行时间比例随问题规模增大而减小.当规模增大到4K×4K时,自适应编译过程中开销时间所占总执行时间的比例降为9%.当生成最终优化代码后,应用在执行过程中则不再需要额外的开销.图12问题规模对自适应编译过程中系统开销的影响附录A中给出了一段计算核心源代码与自适应编译后生成的最终优化代码.4.2.4不同问题规模性能对比测试针对Cell平台的编程模型CellSs与ALF都要求计算核心的数据大小完全容纳在SPE的LS中,因此当访存数据量增大到一定程度,即超过SPE中LS容量时,CellSs与ALF编程模型已不再适用.对测试用例BMM分别使用CellSs、ALF方法与本文方法进行实验对比.对测试用例BMM,设定矩阵大小为1024×1024,分块子矩阵大小为32×32,矩阵分块可完全容纳在LS中,经过自适应编译后BMM只需进行数据层优化.如图13所示,本文系统的性能较CellSs与ALF分别提高57%和36%.当计算核心访存数据量超过SPE中LS容量时,CellSs与ALF编程模型已不再适用,测试中与IBM提供的CBESDK方法进行对比.Page10如图14中,设定BMM子矩阵大小为256×256,规模大小分别设定为1K×1K、2K×2K、3K×3K及4K×4K.此时分块大小为256×256×4B=256KB,超出LS容量.n-body问题描述了具有相互引力作用的n个粒子运行轨迹的计算过程,n-body问题最直接的算法是PP算法.本文采用PP算法对n为8192、16384、32768及65536个粒子进行测试.当n=8192时,需要320KB的存储空间,已超过LS的容量.如图14所示,BMM及n-body经过系统数据层优化后性能较IBM默认设定的访存方式平均提高13%,进一步根据规则2与规则3进行存储层优化后,性能较IBM默认的访存方式平均提高到25%.优化后的规则应用性能提高比例随问题规模变化的趋势是稳定的.针对复杂非规则应用,由于程序中有很多非规则流,访存空间不确定,CellSs与ALF无法直接支持此类应用.通过使用本文中自适应编译框架提供的存储层分析,可对程序进一步优化.以分子动力学Gromacs的kernel010程序(MD)为例,其中循环执行步数设定为1000步,粒子数分别设定为8000(203),15625(253),27000(303).如图15所示,通过本文中自适应编译优化后应用性能较IBM提供的CBESDK访存方式提高29%.优化后的非规则应用性能提高比例随问题规模变化的趋势是稳定的.图15不同问题规模的非规则应用性能测试5结论与后续工作本文提出了一种面向异构多核架构的自适应编译框架.该框架通过源到源编译技术解决传统并行编程模型应用向异构多核架构的映射问题,并利用动态剖分信息,自适应地调整插桩并配置优化策略,形成迭代式的自动优化过程,将软硬件映射机制与优化策略相结合,有效解决了异构多核架构的移植问题并提高了应用的整体性能.实验结果表明,本文基于Cell架构实现的原型系统,在不增加程序员负担的前提下,应用性能相比同类技术有显著的提高.我们下一步工作将原型系统扩展到CPU+GPU以及CPU+MIC异构架构,基于这两种异构架构已做了基本的研究,包含底层的优化方法等.
