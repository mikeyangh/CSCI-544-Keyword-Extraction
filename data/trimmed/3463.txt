Page1基于中心宏块的视频目标跟踪算法肖国强康勤江健民张贝贝(西南大学计算机与信息科学学院重庆400715)摘要目前的视频目标跟踪算法对目标的不精确分割十分敏感,从而影响目标跟踪的性能.文中提出一种新的视频目标跟踪算法,该算法对目标的过分割或欠分割有较强的鲁棒性.文中提出的跟踪算法中引入了一个中心宏块的概念,通过两个层次的相似性度量,以建立相邻帧之间目标的对应关系.同时利用MPEG的运动估计技术和Kalman滤波技术来提高目标跟踪的性能.第一个层次的相似性度量是通过SAD值在中心宏块之间进行局部纹理匹配;第二个层次是用描述目标内部结构的方向矢量建立目标间的对应关系.实验结果表明,文中提出的算法对于不精确分割的目标能够成功地进行跟踪,同时,对于目标的遮挡、形变、出现、消失以及光线的影响有较强的鲁棒性.关键词目标跟踪;视频处理;视频分割;Kalman滤波;中心宏块1引言基于内容的视频处理需要提取视频运动目标,它对视觉内容、事件和相关知识的说明和理解起到至关重要的作用.视频目标提取主要通过目标的分割和跟踪来完成,以保证运动目标在相邻的帧中被准确地提取出来.由于目标分割存在许多尚未解决的问题,目前还没有可靠的方法能够实现精确的视频目标分割.因此,研究非精确分割的视频目标跟踪算法,并提高其目标跟踪的鲁棒性,这对于基于内容的视频处理、分析、诠释和应用有重要的意义.2相关研究视频目标跟踪的方法可以分为4类:基于模型[1]、基于外观[2]、基于轮廓[3]和基于特征[4-6]的目标跟踪方法.基于模型的跟踪方法是利用给定场景中典型物体的先验知识[1,5],这种方法计算量大,且缺乏泛化性.基于外观的方法依赖于视频目标的二维形状区域提供的信息[7-9],如运动、颜色和纹理等信息,来实现目标的跟踪.由于这些信息都是低层次的特征信息,因此,这些方法通常不能解决运动目标复杂的形变问题.基于轮廓或网格(meshes)的方法依赖于目标的轮廓或二维网格.这种方法利用运动信息映射出轮廓,然后将其应用到下一帧的目标分割中,从而完成目标的检测[10].但这种方法计算复杂,运算量大,不能处理非刚性物体的大范围运动.对基于轮廓方法的改进是采用主动轮廓模型,如snakes[11]和meshes[3,12].基于特征的方法也有一些报道,但它们并非针对视频目标跟踪,如文献[13]提出的视频对象跟踪算法.这些算法的主要问题是如何提取特征并确定特征与目标之间的关系.因此,这些算法可靠性不高,容易出现跟踪错误.另一方面,国际上已发布了一系列的视频编码标准,如从MPEG-1到MPEG-4,从H.261到H.264等,来应对日益增长的视频数据.早期的视频编码标准缺乏对高层次视频内容的描述,因此,在最近发布的标准中引入了视频对象层(VOL)的概念来支持基于内容的视频功能.面向对象的多媒体内容的表示为用户提供了基于内容的访问和管理的灵活性,从而使基于运动目标的视频处理引起了业界的广泛关注,如基于对象的视频编码,基于对象的视频内容分析、检索和视频目标跟踪[1,2,4-5,7-11].因此,视频对象分割在基于内容的视频应用中,如视频对象跟踪、基于内容的视频检索、视频标注等,起到关键的作用.然而,目标分割目前仍是一个尚未解决的问题,文献报道的目标分割算法都存在过分割或欠分割的问题[7-10,14-15],因此,不能够提供可靠的视频目标分割.本文提出一种在复杂背景下自动跟踪视频目标的算法,它通过目标分割、区域划分、中心点提取、中心宏块构建和方向矢量确定来实现目标跟踪.该算法利用区域中心点和方向矢量来解决目标之间的匹配问题,从而避免了由于不精确的目标分割所带来的问题.该算法的优势得益于在目标跟踪过程中不需要将整个目标区域投影到下一帧,而仅需处理一个16×16的中心宏块,因此,避免了运算复杂的运动模型.3算法描述给定输入视频序列{I0,I1,…,Ii-1,Ii,…,Im},用文献[14]中提出的通过检测相邻两帧像素的变化和利用运动信息进行视频目标分割.目标分割过程分为3步:第1步是利用Canny算子产生3个边缘图,包括当前帧的边缘图En,差分帧|In-1-In|的边缘图DEn和背景帧的边缘图Eb;第2步,通过分别比较En和DEn以及Eb和DEn之间的边缘像素产生两个运动边缘图MEchange所有的当前运动边缘像素,MEstill运动边缘像素信息中得到的所有静止边缘像素;第3步是通过选择MEchangen)的所有边缘像素产生分割的视频对象MEstill(VO).文献[15]对文献[14]的算法进行了改进,通过区域增长使VO的分割更加精确,同时也提高了算法的鲁棒性.给定当前帧Ii,其中的第j个目标用Oi,j(j=0,1,…,Ni总数.由于目标分割不能做到百分之百的精确[15],特别是一个目标消失,其它的目标又同时进入画面,这时将产生不可避免的遮挡,出现过分割或欠分割的问题,两个重叠的目标可能被分割成一个目标或目标的某些部分被丢失.为了克服这些因素在目标跟踪时带来的负面影响,我们进一步把一个目标分割成纹理一致的不同区域,这些区域之间相互独立,然后分别对每个区域进行跟踪,通过综合利用区域的跟踪信息来实现目标的跟踪,以弥补由于目标分Page3割的不精确所带来的影响.因此,给定视频帧的目标,对第j个目标,利用区域增长方法[16]进一步分割成Nji个互不重叠的区域,用Rk个目标的第k个区域,k=0,1,2,…,Nj由于部分区域可能位于被分割的目标之外,我们仅利用一个区域的代表部分进行跟踪,这个代表部分应位于区域的中心.为此,需要从区域中提取一个中心点.区域Rki,j的中心点犆ki,j按如下的方法提取[17].首先计算其中,Pk的第l个区域边界像素亮度值,μki,j,l的亮度平均值,M表示区域的边界像素点总Pk数.把μk图像gk(x,y):其中,P(x,y)表示视频帧I(x,y)中的像素值.最后,由下列两式得到一个区域的中心点犆kmk以区域中心点为中心,构建一个16×16的中心宏块来代表该区域.宏块大小的选择是鉴于MPEG中运动估计和补偿技术[18],同时也便于利用它的原则来计算SAD(SumofAbsoluteDifferences)值,以达到对中心宏块跟踪的目的.因此,通过中心宏块来建立相邻帧之间区域的对应关系,同时利用Kalman滤波技术和从MPEG中获得的运动矢量来提高目标的跟踪精度,这样,不仅提高了处理速度,还改善了跟踪的性能.由于目标的跟踪是建立在中心宏块之间对应关系的基础上的,为了保证视频目标的正确跟踪,我们将忽略那些不能够包含一个完整宏块的区域,这些区域可通过下式的条件进行判决:其中,d(Pboundary,犆k间的Euclidean距离,η为参数,取值范围1η2.设置该条件的思想是使得选定区域的大小应大于中心宏块,中心宏块的边界点与中心点之间最大的Euclidean距离为82+8槡2槡=128.η用来控制跟踪目标的最小区域面积,η=1对应中心宏块大小的区域.为了实现从第(i-1)帧到第i帧对第k个目标的跟踪,我们用区域的对应关系来建立目标之间的对应关系,利用Kalman滤波来实现区域的跟踪[19-20].众所周知,Kalman滤波利用状态方程和观测方程来描述动态估计和预测系统[19],我们定义每个中心宏块的状态为其中,(xt,yt)表示在时刻t中心宏块的位置,(vtx,vt表示在时刻t分别沿x和y方向的运动速度.对于相邻帧之间的状态连续估计,有vtx=xt-xt-1=Δxy=yt-yt-1=Δy,(Δx,Δy)是第t帧中心宏块的和vt运动矢量.由于MPEG中采用从上到下,从左到右的方式扫描产生宏块,因此,中心宏块可能与MPEG中产生的宏块不重合,我们采用对与中心宏块所重叠的MPEG宏块的运动矢量进行加权来获得(Δx,Δy).假定中心宏块与NMPEG个MPEG宏块重叠,NMPEG最大值为4.(Δx,Δy)由下式确定:其中,wk=1中心宏块和MPEG宏块,(Δxk,Δyk)为对应的第k个MPEG宏块的运动矢量.对于只有帧内编码的帧,其(Δx,Δy)设为(0,0).Kalman滤波的状态方程定义为其中,犉为状态转移矩阵,表示为η(t)=(ηx(t),ηy(t),ηΔx(t),ηΔy(t))T是在时刻t的随机噪声矢量,包括位置噪声和运动噪声.随机噪声通常为相互独立的零均值高斯白噪声,因此,其协方差矩阵为一对角矩阵,即E[η(t)η(t)T]=Qt.t的状态预测为在时刻t的状态更新用下式:狊^(t|t)=狊^(t|t-1)+K(t)犣(t)-犎狊^(t|t-1根据标准的Kalman滤波,在时刻t-1对时刻这里,犎=1000(Page4犣(t)=犎s(t)+犠(t)确定,其中,犣(t)为观测量,犠(t)为观测噪声.Kalman增益犓(t)=犘(t|t-1)·犎T[犎犘(t|t-1)犎T-R(t)]-1,其中犘(t|t-1)=犉犘(t-1|t-1)犉T+犙(t)是t-1时刻的预测协方差矩阵,它的更新方程为犘(t|t)=犘(t|t-1)-犓(t)犎犘(t|t-1).这是一个基于帧的迭代过程,当t=0时,犘(0|0)=E[狊(0)狊(0)T].由方程(11)可知,跟踪的状态位置由Kalman预测部分和校正部分确定.状态预测利用MPEG运动估计技术,中心宏块的位置观测值犣(t)则利用下述方法得到.给定第i-1帧的第j个目标Oi-1,j,它与第i帧的第j-个目标Oi,j-的区域差分用下述矩阵描述,其中区域差分值用SAD(SumofAbsoluteDiffer-ences)表示(j,j-)=烄SAD0,0SAD0,1…SAD0,Nj-SAD1,0SAD1,1…SAD1,Nj-SADNj烆其中,SAD=1为两个区域的中心宏块的差分值,Pi(x,y)为第i帧中区域的中心宏块像素值.由式(12)可知,(j,j-)的第一行对应目标Oi-1,j的第一个区域与目标Oi,j-的所有区域的差分值,即(j,j-)的每一行表示目标Oi-1,j的一个区域与目标Oi,j-的所有区域的差分值,因此,其每一行的最小SAD值代表了目标Oi-1,j的一个区域在目标Oi,j-的区域中的最佳匹配.因此,矩阵(j,j-)度量了第i-1帧的第j个目标Oi-1,j的所有区域到第i帧的第j-个目标Oi,j-的区域的过度过程,其区域的对应关系可表示为min(j,j-)=(MAC0,MAC1,…,MACNj其中,MACk=min{SADk,0,SADk,1,…,SADk,Nj-k∈[0,Nj中的最佳匹配宏块,对应第i-1帧的第j个目标的第k个中心宏块与第i帧的第j-个目标的所有中心宏块之间SAD的最小值.因此,第k个中心宏块的观测值犣(t)由对应MACk宏块的位置确定.由于第i帧中有Ni每个目标都将产生这样的观测量,因此,对于第i帧将有Ni测量.实际上,我们的策略是通过中心宏块的对应关系建立相邻两帧区域之间的对应关系,从而建立相邻两帧目标之间的对应关系.为了综合考虑区域间的对应关系,并把它们进一步转换为目标间的对应关系,把式(13)中对应的最小SAD值全部加起来,构造一个目标之间建立对应关系的代价测度,如下式所示:Costj,j-表示了第i-1帧的第j个目标与第i帧的第j-个目标建立对应关系的综合代价.由于第i-1帧中有Ni-1F个目标,因此,相邻两帧目标之间的对应关系用Ni下面的代价矩阵来度量:Correspondencei-1,i=烄Cost0,0Cost0,1…Cost0,NiCost1,0Cost1,1…Cost1,NiCostNi-1烆其中,第1行表示第i-1帧中的第0个目标跟踪到第i帧中的所有目标的代价.也就是每一行对应第i-1帧中的目标,而每一列对应第i帧中的目标.式(15)说明,第k行中最小的代价值MCostk,l=min{Costk,0,Costk,1,…,Costk,Ni中的第k个目标最有可能跟踪到第i帧中的第l个目标.然而,上述的目标对应关系是建立在SAD值基础上的,它仅仅说明了两个目标之间的纹理相似性.为了进一步优化每个中心宏块跟踪位置的观测量,我们提出两个目标之间的结构相似性度量,具体方法如下.为了表示每个目标的内部结构,首先对每个目标内的区域中心点按从上到下,从左到右的顺序进行标记,然后从第一个区域中心点开始直到最后一个,把区域中心点两两相连,从而构造一个方向矢量的序列.对第i-1帧的第j个目标,这个序列可用下式表示:i-1=犇0DVj={(C0→C1),(C1→C2),…,(CNjPage5其中,(Cm→Cn)表示目标中第m个区域中心宏块到第n个区域中心宏块连线的方向.这样,任意两个目标的结构相似性可用每个矢量的方向来度量.考虑到目标跟踪过程中可能产生旋转,我们把矢量方向仅量化成8个方向,即{[0°-45°),[45°-90°),…,[315°-360°)}.虽然,较少的量化方向可以为被跟踪目标的结构变化提供更大的宽容度,如旋转等,但较多的量化方向能够提供更精确的结构相似性度量.我们通过实验确定了45°的旋转容限.给定两个目标的方向矢量序列,比较相对应的方向矢量,看其是否属于同一个量化的方向,据此来调整它们对应的式(15)中的代价值.具体的调整过程如下:for(珔k=0,珔k<Nki-1,珔k++烅Costk,l=Costk,l-α,∠犇珔k其中,∠犇k-α为步长,用来降低代价值,其取值范围为1~10.如果第i帧中的一个目标的方向矢量数少于或多于第i-1帧中的任一目标的方向矢量数,则相应的代价值增加α.通过式(17)的结构相似性度量对式(15)的代价调整完后,找出式(15)中每一行的最小值,即min踪的目标.也就是第i-1帧中的第h个目标跟踪到第i帧中的第g个目标.因此,第k个中心宏块最后的观测量是在第i帧中对应MACg综上所述,本文提出的视频目标跟踪算法通过两个层次实现.首先,我们建立中心宏块之间的对应关系,以度量目标内区域之间的局部相似性;第二,利用Kalman滤波进行基于中心宏块的目标跟踪,同时,综合纹理和结构相似性信息以保证在目标之间建立正确的目标对应关系,从而实现对视频目标的跟踪.4实验结果与分析为了对本文提出的算法进行评估,我们在WindowsXP环境下,用VC++编程实现了本算法,并进行了大量的视频目标跟踪实验.图1为对视频序列HallMonitor进行实验的结果.该序列共300帧,每帧大小352×240.图1中,(a)说明目标A(用白色方框表示)在第17帧开始出现,并被跟踪到;(b)显示在第80帧中出现目标B(用黑色方框表示),并被跟踪到;从(c)中看出,在第111帧中,目标A弯下腰拿东西时,形体发生了变化,然而算法依然能够对目标A发生形变后实现正确的跟踪;(d)说明在第249帧,目标A消失,只剩下目标B,而此时算法能够正确地跟踪目标B.图1所示的实验结果说明本文提出的算法在以下几种情况下都能够正确地跟踪目标的运动:(1)从第17帧到第249帧目标A的形体由小到大,再由大到小,这代表了视频中的渐变过程;(2)一个目标消失或出现,同时另一个目标仍停留在画面中;(3)目标发生形变.图1HallMonitor视频序列目标跟踪实验结果图1中显示的视频帧的目标分割结果如图2所示.从图2可以看出,目标的分割并不精确,同时在每一帧中都存在过分割的现象.例如,第17帧中,人物目标A仅仅部分地出现在场景中,在目标的分割过程中产生了一些过分割的区域,但这些过分割的区域在目标跟踪的过程中被算法所抑制.如前所述,这些过分割的区域可利用式(5)的条件进行剔出.当Page6一些背景区域作为被分割目标区域的一部分时,如图2中的第80帧和第111帧,中心宏块的方案将把这些背景区域对目标跟踪的影响降低到最小的程度,特别是人物目标与背景的颜色或纹理相似的情况下,如第111帧中人物目标的腿部与背景的颜色相似.为了进一步说明本文提出的视频目标跟踪算法的鲁棒性,我们下载了大量主要用于目标跟踪的视频序列进行实验,部分实验结果如图3和图4所示.图3给出了对PETS2001监控视频序列的实验结果,在第1368帧出现了汽车和行人两个目标,行人比汽车要小得多,但我们的算法仍能够对目标进行正确的跟踪.图3的结果显示,本文算法在全景视图中能够正确地跟踪较小的多视频运动目标.图4给出的是对PETS2006视频序列的实验结果,该图说明本文算法在夜晚场景并有强光影响的条件下,也能够正确地跟踪视频目标.5总结本文提出一种新的基于中心宏块的视频目标跟踪算法,我们的研究工作的新颖性主要体现在以下3方面:(1)在区域层次上引入基于中心点的宏块来进行目标跟踪,克服了由于目标分割的不精确所带来的影响;(2)通过在目标中引入方向矢量,在目标层次上度量目标之间的结构相似性;(3)利用MPEG的运动估计技术,用Kalman滤波综合区域和目标对应关系,利用代价矩阵在相邻两帧的目标之间建立全局相似性度量.大量的实验证明,本文提出的算法在各种复杂背景的情形下,都能够正确地跟踪目标,并对目标的不精确分割有很好的鲁棒性.
