Page1面向InfiniBand数据中心的区别化传输层带宽划分机制张子文吕高锋孙志刚王?臖卢锡城(国防科学技术大学计算机学院长沙410073)摘要数据中心已经成为提供资源和服务的重要平台,在虚拟化基础上部署的多种应用对网络性能提出了更加严格和多样化的需求,从而促使改变以传统TCP为中心的传输层结构.文中从数据中心环境的特性出发,归纳得到改进数据中心网络传输层设计的3个方向.在深入剖析InfiniBand(IB)互联传输优化相关技术的基础上,文中提出了一种更好的支持网络可感知、应用可区分、策略可管理的数据中心网络新型传输控制模型.文中对IB传输控制核心的IB拥塞控制算法及其参数进行了深入的分析,证明了在流级通过区别化的拥塞控制参数配置能够实现成比例的带宽划分.最后,在真实系统中的实验验证了通过参数的动态配置能够保持租户级的公平带宽共享,更好支持云数据中心的带宽隔离需求.关键词数据中心网络;InfiniBand;拥塞控制;带宽隔离;传输策略1引言数据中心通过把计算、存储和网络设备整合成虚拟资源池为用户提供集成服务环境.国内外服务提供商建立的云平台发展迅猛,云数据中心规模日益扩大,已经达到10K~100K台服务器的量级[1].数据中心网络主要设计目标为高带宽、低延时和高可靠性.其它如扩展性、安全性、管理配置、故障诊断等方面也面临着诸多亟待解决的问题.传统TCP协议在解决低延时数据中心网络拥塞控制、满足多样化用户需求、实现可控可管的传输等方面仍存在较多问题,改进传输控制必须考虑数据中心环境的内在特性和需求.首先,传统TCP只能实现流级的公平性,而数据中心内应用规模相对有限,需要以较细的粒度将各自的QoS需求传达给网络管理者,从而实现相应的传输控制策略和灵活的网络资源划分,同时促进网络定价机制的发展.第二,不同于TCP基于端到端RTT和丢包实现网络探测,由于网络和服务器处在同一管理域,数据中心能够直接将网络信息通告给传输层,如交换机接口的统计计数器[2]等,网络对传输层的接口能够优化传输控制.第三,TCP应用于自组织的Internet环境,集中管理的数据中心需要更好的可控性,网络管理者能够为用户配置不同的策略,同时监控传输质量,从而启动流量调度等传输优化机制.IB是诞生于1999年的一种高带宽低延时互连交换技术[2],其与以太网相比具有更强的异构互联能力,在延时、带宽、虚拟化和管理配置等方面其性能都有显著的提高.2011年底最新的top500高性能计算机排名中,全球最快的500台计算机中已经有42.6%使用IB互连.近年来,IB在数据中心网络中的应用逐渐得到关注.2010年,Oracle通过在其数据仓库和数据处理平台Exadata中使用IB交换,处理性能提升10倍[3].2010年,Google在其研究工作中采用IB和蝶形网络拓扑构建数据中心,网络功耗节约85%[4].2011年,Microsoft基于IB组建高性能数据中心支持其BingMaps应用,与传统基于以太网的架构相比节约80%的功耗和50%的成本[5].Stanford课题组通过IB互联分布式DRAM实现高性能数据中心存储系统[6].2012年初,Intel通过收购试图将IB技术与其至强处理器绑定,进一步扩大在服务器市场的优势地位,推动其在数据中心集群的应用.IB的传输控制较之传统TCP加以太网模式更加适应数据中心环境,本文对基于IB的数据中心传输层设计展开深入讨论.本文的创新点在于:(1)将目前数据中心网络的主要研究点归纳映射为如何扩展和改进传输层功能以满足数据中心网络传输的设计需求;(2)基于IB提供的传输优化技术,创新性地提出新型传输控制机制,通过定义各功能模块和接口实现网络可感知、应用可区分、策略可管理的数据中心网络传输;(3)基于IB的拥塞控制机制,通过对其相关参数的研究和真实环境的实验证明通过不同的参数配置能够达到区别化的端到端网络传输能力,从而实现管理者的传输控制策略.本文第2节综述目前改进传输层设计的相关研究;第3节提出数据中心网络新型传输控制模型;第4节深入研究IB拥塞控制算法及其参数配置对流传输能力的影响;第5节通过实验验证IB拥塞控制的公平性及区别化带宽划分的有效性.2数据中心网络传输层设计相关研究早期数据中心网络研究普遍关注于从体系结构的拓扑、编址、路由等角度解决面临的问题,而近期的很多工作都是围绕着如何设计传输控制机制来满足数据中心传输层的新应用需求.针对数据中心的应用环境,其传输层设计的主要需求包括:(1)新型拥塞控制算法完全基于端到端的TCP方法面临着在低延时数据中心环境下对突发拥塞响应不及时、窗口限速能力不足[7]等问题.目前数据中心应用具有很强的流量突发性,如大数据领域的Hadoop会产生all-to-all的流量模式[1],many-to-one的同步流量场景会造成Incast[8-9]问题.出于成本的考虑,数据中心网络通常采用超额认购(Oversubscription)[1]的方式,从而导致核心网络成为瓶颈.交换网络的报文缓冲区比较小也导致更容易出现拥塞[10].目前的很多研究通过基于网络拥塞信息加源限速的方式进行传输拥塞控制.DCTCP[10]针对数据中心的需求修改ECN(ExplicitCongestionNotification)机制,采用一个多比特的反馈,同时改进TCP发送接收端的AIMD速率调整算法,从而保证高的突发容忍性和短流的低延时,同时能够解决Incast问题.以太网通过IEEE802.1Qau的标准实现了QCN(Quan-tizedCN),Jiang等人[11]对数据中心内QCN的稳定性和收敛性进行建模分析,结果显示QCN能够改进吞吐量和延时,同时得到比例公平性,但是网络抖动问题比较严重.Alizadeh等人[12]采用了一种当Page3出现延时增加时能够使得拥塞控制环回稳定的算法AveragingPrinciple,证明该策略在QCN以太网能够达到高稳定性.(2)传输层安全和性能隔离由于传统TCP协议能够保证流级的公平性,故数据中心内的应用和租户能够通过建立大量TCP连接抢占有限的带宽资源.虚拟机提供带宽设限机制解决性能隔离,当速率超过分配的带宽时丢弃报文,从而导致TCP的降速,但存在的问题是固定的带宽分配阈值导致带宽浪费,同时应用需要假设租户使用的是合理行为的TCP.SecondNet[13]通过集中式的VDC分配算法确定虚拟机到物理机的匹配、路由以及服务器管理程序对带宽的预留状态,从而在数据中心网络保证了虚拟机对之间的流量.Seawall[14]根据每台虚拟机在当前链路发送数据的权重,通过控制边缘网络服务器虚拟层的分布式限速器为每条链路提供带宽划分,可扩展性好.但是这两种机制提供虚拟机对之间的带宽更有利于虚拟机对数多的租户;且数据中心通信模式动态性强,很难指定虚拟机对之间的带宽需求.NetShare[15]实现了各个租户之间的加权层次带宽划分,将问题简化为端点的服务器链路在各个共享的虚拟机之间的分配,但是集中的带宽分配器难适应大规模的数据中心、处理快速的速率变化和虚拟机的迁移.(3)传输服务的应用区分能力在传输层面除了传统的公平带宽性能隔离,目前越来越多的研究关注于如何为应用提供更细粒度的区别化带宽划分,尤其是在资源竞争的场景下.近两年来的典型研究包括,AF-QCN[16]在维持QCN简单性、实时响应能力和稳定性特点的基础上,实现了灵活的流或者流量类的可编程带宽划分.Fang等人[17]提出数据中心网络的ECN主动队列管理和基于AIMD的限速器,通过区别的AIMD参数设置能够实现不同优先级的拥塞控制区分,从而达到区别的带宽利用率控制.Wilson等人[18]认为流的公平性会损害应用的性能,提出了一种在数据中心环境下deadline感知的控制协议D3.D3使用显式速率控制为尽快能够结束的任务提供更高的带宽,从而最终优化在deadline之前完成的总任务数.Vattikonda等人[19]采用时分多路复用(TDMA)来解决区分服务,通过在商业交换机硬件实现的MAC层,以TDMA的方式切分流的链路带宽和交换机缓存空间,使得主机能够不需要TCP的可靠性和拥塞控制就能够满足带宽、延时等不同的应用需求.此外,传统的TCP协议无法利用数据中心网络多路径所提供的负载均衡能力,且应用传输对于网络状态的变化缺乏感知能力.在IB网络传输控制方面,Santos等人[20]最早对其ECN机制进行研究,提出异步的报文标记行为能够改进公平性,放宽的速率增加条件能够改进静态和动态流量场景下的吞吐量.Pfister等人[21]通过模拟验证IB拥塞控制如何在胖树结构下解决热点链路的拥塞问题.Yan等人[22]提出在IB中改进ECN报文标记机制以及新的源响应幂增幂减函数,通过带窗口限制的速率控制减少IB网络中有多种优先级类流量的拥塞.Yan等人[23]又提出基于作业阈值的拥塞控制方法,一方面能够减少延时、阻塞概率和平均队列长度,同时提高IB网络的QoS.Lugones等人[24]通过自适应路由算法将流量分配到IB中的多条可选路径以解决拥塞问题,实验验证了该机制同时改进了延时和吞吐量.Gran等人[7]对硬件实现的IB拥塞控制进行分析实验,通过调整参数能够有效地解决拥塞,提高benchmark性能,并通过解决停车位问题改进公平性.Gran等人[25]还研究了在不丢包网络中拥塞控制、交换机仲裁和公平性之间的关系.Escudero-Sahuquillo等人[26]通过限制流量注入和隔离拥塞流这两种方法的结合实现对拥塞的快速反应,该机制可扩展性好且对拥塞流公平.3IB网络传输控制模型我们认为除了网络本身性能的优势,IB中已有的关键技术对数据中心网络扩展传输功能已经提供了各种机制上的支持,满足数据中心网络传输对新型拥塞控制算法、传输性能隔离和传输服务的应用区分能力等要求.其中,QP(QueuePair)是传输层承上启下的核心,IB面向每个单向连接分配发送/接收者对QP作为虚拟化的通信端口.与TCP/IP协议相比,其传输层是基于硬件实现的,且完全在用户模式下运行,从而减少了通信协议本身和操作系统引入的开销.QP在建立时和QP号、CA(ChannelAdapters)源/目的端口地址LID(LocalIDenti-fiers)、SL(ServiceLevel)和MTU等信息绑定,QP在通信结束后销毁.与TCP连接不同,IB采用基于目的LID的确定性路由,QP在建立时事实上已经确定了一条源和目的节点之间的路径.如图1所示,基于IB实现的传输控制模型可以分为用户和网络管理两大部分,用户根据应用需求建立、绑定和销毁QP,网络管理负责QP的参数及Page4其它区分化传输服务的配置接口,同时对QP传输质量进行监控,以动态地触发QP拥塞控制和应用的多QP流量调度.我们根据目前IB中已有的传输图1IB网络传输控制模型(1)感知多路径和网络状态的传输.首先,IB网络的每个端口能够虚拟化出2LMC(LIDMaskControl)个不同的LID,在转发的时候对应2LMC条不同的路径,如图1中p1,p2分别对应A、B之间数据传输两个QP连接的不同路径.IB的子网建立过程,集中控制器发送探测数据报文,通过节点和端口逐步的自我链式反应发现网络拓扑并分配LID地址,这使得应用能够通过管理者的接口获得网络拓扑及路由信息,如应用n能够感知两条传输路径.多路径提供了更加灵活的流量调度,每个应用能够利用端口的多个QP进行传输,如p1发生拥塞时,ECN反馈会使得p1传输流量下降,而更多地切换到p2上,通过同时多路径的动态负载均衡实现了带宽分配下的可靠网络传输.显式拥塞通告ECN为端到端传输本身提供了感知网络拥塞的能力,通过对报文的标记和拥塞反馈使得源发送者能够了解拥塞的发生及程度,根据相应的机制降速以避免拥塞瓶颈点.此外,IB中还提供了大量关于网络状态的信息,如接口流量的报文计数器,主要是通过SA(SubnetAdministration)建立的子网数据库存储子网的状态信息.SA还通过路径记录的数据结构维护路径信息.当端点试图建立一个QP时需向SA发送请求获得该路径信息,还可以通过SA与QoS管理器的交互得知路径的QoS是否能够满足需求.(2)新型传输模式和拥塞控制机制.数据中心应用的性能要求越来越高,端节点开销已经成为通信的瓶颈,IB通过无连接(数据报)/面向连接的传输和可靠/不可靠传输的4种组合定机制提出了其传输控制模型,从网络、传输和应用三者的角度扩展原有机制,该模型提供对以下3个方面的支持:义发送和接受QP这4种不同交互模式.应用在建立QP之前首先选择适合的传输模式,IB中硬件支持的传输模式与TCP和UDP相比能够提供更高的传输性能.IB拥塞控制采用显式拥塞标记的ECN机制,IB交换机通过监测是否超过阈值感知拥塞,避免了传统TCP协议在IB这种极低延时网络中对延时反应不及时的问题.当报文到达目的节点的路径上遇到拥塞(如图1IB交换机R),其报文FECN(ForwardECN)位会进行标记,在接收端端口感知拥塞标记之后会通过ACK或者CNP报文中的BECN(BackwardECN)位通知相应的源端,使其能够发现到前面发送的报文在该VL下遇到了拥塞.IB采用PFC(PriorityFlowControl),在每个优先级,如果上游的邻居交换机没有接收缓冲区会发送控制帧停止本地交换机数据发送,从而避免丢包对于应用性能的严重影响.但是链路层信用量的耗尽会造成头阻塞问题,进而造成拥塞传播,基于ECN的拥塞控制机制在数据中心环境下实现传统公平、收敛的拥塞控制同时能够避免拥塞传播对于无辜流的影响[7,25].(3)基于QoS管理的区别化传输服务策略配置.目前,IB中尚未定义如何支持区分的应用服务策略及传输配置,IB提供集中的子网管理结构,能够通过在其中增加资源分配功能实现高级的应用优化目标.从数据中心应用自身流量特性和不同的QoS需求出发要求传输层协议提供区别的服务策略.从用户之间的竞争关系出发,它们共享同一个数据中心网络资源,某些用户可能通过建立大量连接Page5等方式恶意侵占网络带宽,传输层需要实现用户之间的性能隔离.从应用的相对优先级出发,数据中心通过虚拟化为多个租户提供服务平台,区别收费需要传输层为租户提供不同质量的网络服务.如图1所示,IB协议中已经定义的QoS管理器实现了针对SA中定义的网络和路径监控信息的主动控制.当服务质量达不到时,QoS管理调整配置和策略以满足所需要的服务,如通过与子网管理器的交互调整某个端口的VL(VirtualLanes)仲裁表.这种主动的调整将网络状态与资源分配结合起来,决定最终的带宽分配策略并配置到拥塞控制和流量调度模块,形成了一个闭环的动态控制系统.IB提供了两种传输质量的配置接口,首先在网络层面,每个物理链路上虚拟化多个VL,代表链路级的发送接收缓冲区对,每个VL有各自的流控策略和加权优先级,采用高低两级抢占式调度,而同一优先级的VL能够进一步区分调度,从而获得不同的带宽性能.如图1所示,链路级的性能隔离是通过SL到VL的匹配策略以及VL基于不同加权值的仲裁调度来实现的.在用户节点的端到端传输层面,IB的QP为应用实现了抽象的虚拟通信接口,是应用竞争网络资源以及进行拥塞控制的实体,每个QP的CCT(CongestionControlTable)是拥塞控制的核心,如何通过CCT表相关参数的配置实现区分的应用服务策略是下一节的研究重点.综上,IB协议定义了比传统以太网加TCP/IP模式更加强大的传输层功能,对实现网络可感知、应用可区分、策略可管理的新型传输控制模型供了强大的支持.4基于拥塞控制的区别化IB带宽划分4.1IB拥塞控制算法源端的限速策略依赖于每个QP的CCT表,其中可以配置的相关参数如表1所示,CCT的表项为IRD(InjectionRateDelay),它定义了该QP向网络中注入报文的时间间隔.表项IRD在CCT表中按照由小至大进行排列,当发送端收到BECN时,增加CCT表的指针CCTI(CCTIndex)指向更大的CCT表项,从而降低报文发送速率.每次索引CCTI增加的单位值是CCTI_Increase,该值事实上实现了类似TCP在面对拥塞时的快速回退策略,且通过配置该参数能够控制QP发送速率回退的快慢程度.为了限制速率变化范围,CCTI定义了下界CCTI_Min和上界CCTI_Limit,CCTI初始化之后指向CCTI_Min,CCTI_Min的缺省值为0,即不对报文发送速率做任何限制,而CCTI_Limit指向的表项是报文发送的最大间隔即速率的最小值.CCT参数CCTItemCCTI_IncreaseCCTI每次增加的单位量1CCTI_LimitCCTI的上界CCTI_MinCCTI的下界CCTI_Timer恢复速率的超时定时器150拥塞控制的另一个问题是在拥塞缓解之后如何通过提高发送速率来充分利用网络带宽资源,其中发送速率的增加是基于超时机制,当在CCTI_Timer时间内没有收到拥塞控制报文,那么相应QP流的CCTI值减少1,相应的IRD值得到增加,提高了源发送速率,直到CCTI减少到0不限制报文注入速率.CCTI_Timer参数的设置事实上决定了流量的波动快慢,过小的CCTI_Timer会造成流量的波动,反之如果偏大则使得拥塞控制流量快速下降之后难以及时恢复,CCT管理配置算法如下.算法1.CCT管理.输入:CCTConfigurationMAD,PacketwithBECN输出:UpdatedIRD1.Initialize:ManagerdefinesCCconfigurationfile2.SMparsesitandsendMADtoconfigureCC3.StartTimer=04.ifreceiveBECN&&CCTI<CCTI_Limitthen5.CCTI=CCTI+CCTI_Increase6.IRD=CCT[CCTI]7.Timer=08.endif9.ifTimer=CCTI_Timer&&CCTI>CCTI_Minthen10.CCTI=CCTI-111.IRD=CCT[CCTI]12.Timer=013.endif14.foreverytimeunit15.Timer=Timer+1源端的限速是累积一定数量的报文(如IB规范中定义4个[2])进行报文间隔计算,如果报文的间隔小于CCT[CCTI],则会在判定后等待发送,从而保证源的发送速率不超过CCT表的定义.Page6算法2.源发送限速.输入:IRD输出:Controlledpackettransmission1.Initialize:arb_pkt_num=02.last_time=03.While(packet_to_send)4.arb_pkt_num++5:if(arb_pkt_num=aggregate)6.if(CCT[CCTI]<(current_time-last_time))7.nextaggregatepacketstransmissionstarts8.else9.WaitforCCT[CCTI]-(current_time-last_10.last_time=current_time11.arb_pkt_num=012.endif13.endwhile对于FECN标记,交换机主要有两个参数,一个是标记的阈值,另一个是标记报文的比率.根据IB规范的定义,IB交换机的拥塞阈值从0~15,0表示没有拥塞标识,1表示非常高的阈值,拥塞容易发生传播,而15代表非常低的阈值,代表即使互联拥塞程度不高也可能发生标记降速,降低拥塞传播的可能性.其中给出的参考是15表示256Bytes的报文,报文之间的间隔为176Bytes,从而达到60%的带宽占用率.权重从15~1表示从平均60%的带宽占用以上的线性增加,阈值设置与交换机体系结构相关.4.2区别化IB带宽划分相关研究[7,25]证明在IB网络中若采用相同的CCT参数配置,各个QP之间能够达到很好的流级公平性,即各个QP应当处于相同的CCTI值.而通过对每个QP配置不同的CCT相关参数能够实现区别化的传输服务质量.定理1.基于ECN的IB拥塞机制,对于任意qpi和qpj,通过设置CCTI_Increasei=N×CCTI_Increasej,发生拥塞时两条流稳定之后的发送窗口wi,k:wj,k=1:N.证明.对于当前热点链路上的各条流i(i=1,2,3,…,n)在时间周期k上的报文最小发送间隔限制为CCT[CCTIi,k]=IRDi,k.设Δt周期足够短使得可以认为带宽为某个固定值,第k个周期的时间tk=k×Δt(k=0,1,…,m-1).设平均报文长度为l,qpi在时间k的发送窗口大小是wi,k=l/CCT[CCTIi,k].设α为交换机采取FECN的标记报文比例,对于接受端的CA网卡,每C个报文采用ACK或者发送CN实现BECN携带拥塞标记信息,则可得ΔCCTIi,k=CCTI_Increasei×BECNi,k,BECNi,k=(α×wi,k)/(C×l),BECNi,k代表QP源收到的BECN报文数,ΔCCTIi,k是在时间周期k之后CCTI的变化量.下一周期的发送窗口wi,k+1=l/CCT[CCTIi,k+ΔCCTIi,k].对于qpi,qpj,如果wj,k<N×wi,k,由于CCTI_Increasei=N×CCTI_Increasej,ΔCCTIj,k=CCTI_Increasei×BECNi,kΔCCTIi,k满足l/CCT[a]-l/CCT[a+x]>l/CCT[b]-l/CCT[b+x](a<b)表示CCT表前端的表项CCT[a]导致的限速变化幅度大于后端的表项CCT[b]导致的限速变化,由于CCTIi,k<CCTIj,k,则l/CCT[CCTIi,k]-l/CCT[CCTIi,k+ΔCCTIi,k]>l/CCT[CCTIj,k]-l/CCT[CCTIj,k+ΔCCTIi,k],由于式(1)中ΔCCTIi,k>ΔCCTIj,k,可得CCT[CCTIi,k]-lCCT[CCTIj,k]-lΔwi,k>Δwj,k,对于wj,k>N×wi,k的情况,与式(1)同理ΔCCTIi,k/ΔCCTIj,k<1,可证Δwi,k<Δwj,k.这使得qpi和qpj有不同的降速快慢直到它们稳定在汇聚点wj,k=N×wi,k.定理1说明各个流通过呈比例的CCTI_Increase配置能够达到成比例的带宽划分.如果链路上的总流量小于带宽B,那么对于流i,每经过Ti超时周期之后流i的CCTIi会减1.S为拥塞控制的阈值,而s表示超时增速开始时的缓冲区长度.超时的速率恢复过程如下式所示:i=1∑T/Ti-1∑n该速率增加过程使得两条流i,j的报文间隔比从CCTI[CCTIi]/CCTI[CCTIj]变为CCTI[CCTIi-(T/Ti-1)]/CCTI[CCTIj-(T/Tj-1)],当为CCTI较大流设定短的超时时间能够较好地维护原有的带宽比例关系.能够通过式(2)求出经过的超时增速时间周期.除了CCTI_Increase外,CCT表指针CCTI_Min和CCTI_Limit之间的值表示了报文间隔的变化范围,是实现区别化配置的重要参数.CCT[CCTI_Min]事实上代表报文之间的最小间隔,如CCT[0]默认Page7CCTI_Minj.CCTI_Limitj.值为0,说明当前没有任何限速,其它情况下发送的最高速率为l/CCT[CCTI_Min],而发送的最低速率控制在l/CCT[CCTI_Limit].CCTI_Min和CCTI_Limit为流设定的阈值,导致流的降速和升速受到限制.通过设定该值能够限制当前QP吞吐量的变化范围,从而达到区分化的带宽分配.如果CCT表的表项配置相同,可以得到以下结论.对于qpi,qpj,若其它参数相同则满足以下条件:(1)如果CCTIi<=CCTI_Minj则CCTIj=(2)如果CCTIi>=CCTI_Limitj则CCTIj=(3)其它,则CCTIj=CCTIi.CCTI_Min实现对每个网卡的限速阈值,如CCTI_Min设置为1能够使得流初始的发送间隔指向CCT表第2项,这样即使不发生拥塞,报文发送的间隔也被固定地限制在最小,为CCT[1],从而得到网卡限速的大小.分区段的CCT配置事实上实现了静态的报文发送速率范围.以上分析了为每个QP配置不同的拥塞控制参数(如CCTI_Increase)能够达到区别化的端到端网络带宽资源划分,从而实现传输性能隔离和基于应用层需求的传输机制等目标,如根据各个租户的加权进行成比例的分配,即某个租户无论建立多少流,它所获得的带宽应当是对于当前链路的合理加权划分.如下所示,对于瓶颈链路上存在的租户t,其获得的带宽Bt为Bw×wt∑t当前该链路上租户t所有流的带宽求和应当满足其带宽的分配.5初步实验结果为了对IB在数据中心网络中应用进行研究,验证基于拥塞控制的区别化网络带宽分配,我们搭建了真实的IB实验环境.IB交换机IBS108Q和HCA网卡是由国防科技大学课题组基于Mellanox芯片SwitchX和ConnectX自主设计研发的,端口支持最高速率QDR(40Gbps),设备均支持拥塞控制.实验节点包括6台宝德PowerLeaderPR1760T服务器,服务器配置为IntelXeonE5540双核CPU、16GBECCRAM、2个8xPCIe2.0插槽、400GB硬盘,所有服务器均运行RedHatLinux6.1x8664位操作系统,其上采用开源OFED(OpenFabricsEnterpriseDistribution)1.5.4驱动和管理HCA设备.OFED的最新诊断包infiniband-diags-1.6.0提供了ibccquery和ibccconfig接口来读写拥塞控制参数.我们主要利用ibccconfig来根据不同QP连接配置各自的CCTI_Increase参数.IB拥塞控制实验场景如图2所示,我们设计实现了一个集中控制的流量行为发生器,能够控制其它分布式的客户端建立TCP连接,并根据来自集中控制器的命令向目的端发送报文,类似iperf以尽可能大的速率发送报文.由于TCP反应较慢,在IB拥塞机制工作的情况下,TCP拥塞控制事实上不发挥作用[7].集中控制器通过一个配置文件,设置客户端执行的命令,如连接开始/持续的时间,被控客户端/连接目的端的地址.链路实际工作速率均设置为10Gbps.在第1个实验中,为了验证QP的公平性,由4台服务器向服务器C发送报文,最后一跳成为瓶颈,该模式符合目前在数据中心网络中常见的Incast[8-9]问题场景.QP流每隔1s建立连接,持续5s时间,按照100ms的粒度统计吞吐量.如图3(a),如果拥塞控制关闭,由于著名的停车位问题[27-28],从3s~5s,流量A→C加B→C的和等于D→C或者E→C的流量.直到A→C传输结束才达到公平共享.如图3(b),拥塞控制打开后虽然导致吞吐量的小幅抖动,但是使得各条流能够很好地同步增速和减速以达到带宽的公平共享.为了测试IB中CCT表配置对于QP连接限速的影响,实验中将某条QP连接的CCTI指向1,通过设置固定的不同CCT表项IRD值,实现了不同速率的限制,由于每次发送端聚合4个1024Bytes的报文进行IRD比较,如20.48μs的表项值,带宽限制在(1024×8)/(20.48×4×10-6)=100MB,同理IRD=204.8μs将带宽限制在10MB,而IRD=2.048μs的带宽为1GB.如图4所示,CCT对于发送速率限制的效果比较明显.Page8图3相同配置下QP带宽的公平性第2个实验的目标是根据第4.2节的定理和分析,通过为各个QP配置不同的CCTI_Increase参数实现租户级的公平带宽共享,即对于S1和S2之间的瓶颈链路,两个租户无论建立多少条流都应当达到公平的带宽占用.实验中,租户1有一个稳定的从服务器C→A的QP连接,而租户2的QP流顺序地加入和退出网络(租户2只有D→B的流存在于整个实验的时间区间).实验中CCT表项采用参考设计值CCT(i)=14×i2/1062(μs),可证该值满足第4.2节定理中CCT表前端的表项导致的限速变化幅度大于后端的表项.我们在集中控制器上定义了参数配置的时间点和值,初始化时所有流的CCTI_Increase=1,当租户2有两条流时,CCTI_Increase=2,当流的数目增加为3时,CCTI_Increase=3,随着流的离开再次修改该参数.如图5(a)所示,从C到A的带宽基本稳定在3.4Gbps,没有受到租户2流加入和退出的影响.随着流E和F的加入,租户1应得的带宽在2和3条流之间公平的共享,该流降速收敛过程非常迅速的达到稳定.而随着流的离开,租户2当前的活跃流相对缓慢的增速,包括从5s~7s和7s~8s两个阶段,但是最终该过程还是能够达到租户内流的公平共享.图5基于CCTI_Increase的流级区别化带宽划分我们利用OFED中的ibdump工具在服务器D上抓取了BECN报文,D→B的QP连接源收到的BECN数目如图5(b)所示,除了在流加入和退出的快速突变,BECN保持高度稳定.在1条、2条和3条流的情况下BECN数目分别为790,480和320.所达到的532的比例几乎与带宽分配的目标11/21/3相同,从而验证了第4.2节中分析的BECN数目与发送窗口的正比关系.Page9传统的IB拥塞控制机制只能够提供流级的公平性,因而在数据中心网络存在用户通过建立大量连接抢占带宽的情况.在以太网领域已经提出静态预约和动态调整的多种算法[13-15],但是目前尚没有如何在IB从端到端的传输角度保证带宽和提供隔离的机制.我们利用IB拥塞控制算法的区别化参数配置实现成比例的带宽分配,能够避免限速器划分的带宽浪费问题,实验中我们以租户作为流聚合的单位,动态调整每条拥塞流的少量拥塞控制参数,已较小的开销实现租户级的带宽保证和性能隔离,从而达到本文提出的数据中心对于新型传输控制的要求.6下一步工作基于IB实现的数据中心网络传输层控制以QP为核心,通过拥塞控制相关参数为用户提供了可配置的传输服务,同时利用ECN显式拥塞标记能够更及时的对网络拥塞做出反应.我们下一步的主要工作首先是研究针对IB这种极低延时网络如何配置和改进拥塞控制算法,以适应数据中心存在的突发拥塞抖动,避免拥塞传播的同时减少带宽的浪费.此外,IB提供了QoS传输质量监控,我们需要研究在不能满足QoS需求的情况下,如何通过自适应路由等方式在多路径网络进行灵活的流量调度,从而配合拥塞控制机制缓解网络热点链路的拥塞.
