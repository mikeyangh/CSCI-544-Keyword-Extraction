Page1TSS-BQS系统的GracefulDegradation机制王文韬1),2)林瞡锵1)荆继武1)罗勃3)1)(中国科学院信息工程研究所信息安全国家重点实验室北京100195)3)(堪萨斯大学电子工程与计算机科学系堪萨斯州KS66045美国)2)(中国科学院研究生院北京100049)摘要将门限签名方案分别应用于两种类型的BQS系统(MaskingBQS系统和DisseminationBQS系统),可以得到两种TSS-BQS系统(文中称为TSS-mBQS系统和TSS-dBQS系统).TSS-mBQS系统的性能优于TSS-dBQS系统.由此,作者提出了TSS-BQS系统的GracefulDegradation机制:系统由n=3fd+1台服务器组成,在初始阶段以TSS-mBQS状态运行,容忍fm=fd2台Byzantine失效服务器;随着系统运行,可能失效的服务器数量增大,则以降低性能为代价,切换到TSS-dBQS状态,容忍fd台Byzantine失效服务器.在不影响容错能力的前提下,GracefulDegradation机制提高了已有TSS-BQS系统的平均性能.文中完成的GracefulDegradation机制能够在不中断存储服务、不影响客户端的前提下完成状态切换,客户端也不需要知道系统的运行状态(处于TSS-mBQS或TSS-dBQS状态).关键词拜占庭容错技术;拜占庭选举系统;优雅降级;门限签名方案1引言ByzantineQuorumSystem(简称BQS系统)[1]是指利用冗余复制技术、容忍服务器Byzantine失效[2](即任意失效)、运行于异步网络环境的存储系统.BQS系统由n台独立服务器组成,每次读/写操作都在一定数量的服务器上进行.写操作时,将数据写入qw台服务器;读操作时,从任意qr台服务器读出数据,再从中选出正确结果.在BQS系统中,通过设定合适的n、qw和qr,使读/写操作的服务器群有足够大的交集,从而排除失效服务器的影响,保证读出结果为最近一次写入的数据.当存储不同类型数据时,BQS系统由数量不等的服务器组成[1]:为了容忍f台Byzantine失效服务器,存储普通数据GenericData的MaskingBQS系统需要4f+1台服务器,存储自验证数据Self-VerifyingData(如带有数字签名的证书[3])的DisseminationBQS系统需要3f+1台服务器.所以,当使用n台服务器时,MaskingBQS系统只能容忍n-1能容忍n-1额外操作把普通数据转为自验证数据后再写入服务器,其性能不如MaskingBQS系统.基于上述分析,我们设计了相应的GracefulDegradation机制.GracefulDegradation是指[4]:当运行环境恶化、超出预先估计时,调整系统状态或参数,保证提供服务、但是服务质量有所降低.在本文中,BQS系统的GracefulDegradation机制表现为:在初始阶段,系统以MaskingBQS状态运行;当运行了一定时间、怀疑或检测到失效服务器已达到一定数量时,切换到DisseminationBQS状态.也就是,当失效服务器数量可能将要超出预先估计、使系统无法再提供正常的存储服务时,以降低性能为代价、继续提供存储服务.在我们的设计和实现中,借鉴了TSS-BQS系统[3,5-7](基于门限签名方案ThresholdSignatureScheme的BQS系统)的思想,利用门限签名方案[8-9]来设计协议,使得客户端不需要知道系统的运行状态,同时还保留了TSS-BQS系统的优点.综合以上,本文完成的容错存储服务具备如下特点:(1)系统由n=3fd+1台服务器组成,可作为MaskingBQS系统运行(本文称为TSS-mBQS状态)或者DisseminationBQS系统运行(本文称为TSS-dBQS状态);TSS-mBQS状态的性能优于TSS-dBQS状态.(2)在初始阶段,系统处于TSS-mBQS状态,容忍fm=fd2台失效服务器(注意:并不是fm=3fd4、不同于普通的MaskingBQS系统,详见4.3节的参数分析).当估计失效服务器数量可能将要超过fm时,切换到TSS-dBQS状态,降低性能、容忍fd台失效服务器,继续提供容错服务.(3)系统能在不影响客户端的情况下执行GracefulDegradation.不论系统处于哪一种运行状态,客户端都使用完全相同的协议来进行读/写操作.(4)系统同时保留了TSS-BQS系统支持Pro-activeRecovery[3,10-11]、简化客户端密钥管理和客户端通信的特点.本文第2节先简要介绍背景知识;第3节描述TSS-BQS系统的工作协议;第4节给出TSS-BQS系统的GracefulDegradation机制以及性能分析和实验结果;第5节是相关问题讨论;第6节介绍相关研究工作;最后是全文总结.2背景2.1BQS系统BQS系统[1]由n台独立的服务器组成,每台服务器可能是正确的或Byzantine失效.系统存储的每个数据可记为[x,v,t].其中,x是变量名,v和t分别是变量的值和时戳.时戳集合是满足全序(TotalOrder)性质的集合,例如递增序列号;时戳用来区分值的新旧版本,新版本的值具有更大的时戳.执行读/写操作的服务器集合(称为读Quorum和写Quorum),分别包含qr和qw台服务器.考虑到失效服务器的影响,各服务器Si存储的[x,vi,ti]有可能会不同.为了容忍f台失效服务器,BQS系统的可用性要求[1]:任何情况下,都能找到由正确服务器组成的读/写Quorum来完成操作;有qrn-f和qwn-f.BQS系统的一致性要求[1]:从读Quorum中获得的结果是最近一次被写入的值和时戳.对于存储普通数据的MaskingBQS系统,失效服务器会任意篡改数据,为了排除失效服务器的影响,正确结果Page3在读Quorum中应出现至少f+1次,任意读/写Quorum的交集不小于2f+1台,所以需要有qr+qw-n2f+1;在读出的qr个结果中,去掉出现次数少于f+1的结果,剩余结果中时戳最大的就是正确结果.对于存储自验证数据的DisseminationBQS系统,失效服务器无法篡改数据,只能回复旧版本数据,只需保证正确结果在读Quorum中出现1次,任意读/写Quorum的交集不小于f+1台,所以需要有qr+qw-nf+1;在读出的qr个结果中,去掉验证无效的结果,剩余结果中时戳最大的就是正确结果.2.2TSS-BQS系统TSS-BQS系统[3,5-7]基于门限签名方案来实现BQS系统:系统有一对公私密钥对(称为ServiceKey),其私钥由n台服务器拆分、任意f+1台服务器能合作利用ServiceKey私钥来计算数字签名.每次读/写操作,客户端只需收到带有ServiceKey数字签名的响应消息,即保证完成操作[3,7]:因为ServiceKey门限签名和TSS-BQS系统的服务器协议保证了读/写操作已在qr/qw台服务器上执行、且返回正确结果.相比普通的BQS系统,TSS-BQS系统的重要优点是支持ProactiveRecovery[3,10-11].考虑到MobileAdversary的影响,各服务器应该周期性地恢复到正确状态;否则,攻击者可以逐一地攻占服务器,直至失效服务器数量超过f、破坏存储服务[10].TSS-BQS系统的客户端只配置ServiceKey公钥,而ServiceKey在ProactiveRecovery过程中不会发生变化、只是重新拆分,所以ProactiveRecovery不影响客户端.此外,TSS-BQS系统还具有简化客户端密钥管理和客户端通信的特点[3].3TSS-BQS系统模型和工作协议下面我们先给出TSS-BQS系统模型和协议,这是GracefulDegradation机制的运行基础.下文描述的系统模型和协议,与已有的TSS-BQS系统[3,5-7]基本相同,不同之处在于我们将其从DisseminationBQS系统推广到MaskingBQS系统.限于篇幅,我们只是简要说明了两种状态的服务器协议;关于TSS-BQS系统服务器协议的详细安全性分析,请参考文献[3,5-7].此外,因为系统可能处于不同的运行状态(TSS-mBQS或者TSS-dBQS状态),每一台服务器还需要有独立的state变量,记录自己的当前运行状态.3.1系统模型系统由n=3fd+1台服务器(记为Si,ni1)组成;最多有fd台Byzantine失效服务器,失效服务器可以合谋攻击.系统向不定数量的客户端提供存储服务.服务器和客户端通过异步FairLink信道[3,5]通信:信道可能丢失消息,攻击者可以篡改、延迟、截获或删除消息;但当发送者无限地发送消息时,接收者能收到消息;信道是异步的,没有最大延迟假设.系统有一对ServiceKey密钥对,其私钥由n台服务器拆分、任意fd+1台能合作利用ServiceKey私钥来进行门限签名.所有服务器和客户端都知道ServiceKey公钥,客户端只接受带有ServiceKey数字签名的响应消息.此外,各服务器有自己的公私密钥对(不同于ServiceKey),称为ServerKey,专用于服务器之间的安全通信.各服务器知道其它服务器的ServerKey公钥,但是客户端不需要知道ServerKey的任何信息.每次读/写操作,客户端c周期性地向任意fd+1台服务器发送请求消息,直至收到带有ServiceKey数字签名的响应消息.由于最多fd台服务器失效,所以至少1台服务器(称为此次读/写操作的Delegate,记为Sd)会正确地处理客户端的请求,与其它服务器合作执行服务器协议,然后将正确的响应消息发送给客户端.服务器协议的功能是[3,7]:在qr/qw台服务器上读取/写入数据,生成响应消息,联合其它服务器对响应消息进行ServiceKey门限签名.需要指出,Delegate不是额外的专门服务器,每台服务器都同时具有Delegate功能:当正确服务器从客户端收到读/写请求消息时,就自动转为Delegate、发起服务器协议.变量的时戳t按如下规则产生[3]:时戳由两部分组成,高位是连续递增的整数序号,记为seq(t),低位由相应的客户端写请求消息ReqW确定,有t=seq(t)|Hash(ReqW).其中,Hash()是单向散列函数.容易验证,上述方式产生的时戳集合满足全序性质.在下文的协议描述中,我们使用了如下记号:[·]SK:带有ServiceKey数字签名的消息;[·]c:由客户端c进行数字签名后的消息;[·]d或[·]i:由服务器Sd或Si使用ServerKeyPSi(·):服务器Si计算的部分签名,fd+1台进行数字签名后的消息;Page4READA.客户端c发送读请求消息ReqR=[ReqRead,c,x,p]c给任意fd+1台服务器;其中,c是客户端的唯一标识,x是变量的唯一标识,p是防重放攻击的一次性随机数、防止攻击者重放以前的响应消息.B.客户端c周期性地发送ReqR给fd+1台服务器,直至收到读响应消息RespR=[RespRead,c,x,vR,tR,p]SK;其中vR和tR是读出的值和时戳.WRITEA.客户端c首先执行1次READ操作,得到RespR,其中包含变量x的当前时戳tR.B.客户端c发送写请求消息ReqW=[ReqWrite,c,x,vW,p,RespR]c给任意fd+1台服务器;其中vW是待写入的值.C.客户端c周期性地发送ReqW给fd+1台服务器,直至收到写响应消息RespW=[RespWrite,c,x,vW,tW,p]SK;其中tW=(seq(tR)+1)|Hash(ReqW).3.3TSS-dBQS状态的服务器协议TSS-dBQS状态能容忍fd台失效服务器,读/写Quorum包含的服务器数量分别记为qdr和qdw.当Sd的state变量表明当前是TSS-dBQS状态时,使用如下协议.在协议描述中,我们先假定各服务器的state变量相同;然后,在4.2节讨论了当各服务器state变量不同的处理方法.不同服务器产生的部分签名可合成有效的ServiceKey数字签名.3.2客户端协议客户端c使用如下协议来读/写变量x.注意:不论系统处于TSS-mBQS或者TSS-dBQS状态,都使用相同的客户端协议.READA.当Sd收到ReqR=[ReqRead,c,x,p]c时,验证有效后,发送[Read,state,d,ReqR]d给所有服务器.其中,state是Sd的当前运行状态,设定为TSS-dBQS;d是Sd的唯一标识.B.当Si从Sd收到Read消息时,验证其中的ReqR有效后,回复[Replica,state,i,[xi],ReqR]i.其中,i是Si的唯一标识;[xi]是Si存储的数据,可能是自验证数据形式[x,vi,ti]SK或者是普通数据形式[x,vi,ti].如果系统在切换到TSS-dBQS状态之后,还尚未执行过写操作,则Si存储[x,vi,ti];否则就存储[x,vi,ti]SK.C.Sd周期性地重复执行步骤A,直至收到qdr台服务器(包括Sd)的Replica消息.然后Sd从中选出正确结果,记为[x,vR,tR].选出正确结果的方法,详见4.3节的说明.D.Sd向所有服务器发送门限签名请求[Sign-Read,state,d,MR,ΣR,ReqR]d,其中,MR=[RespRead,c,x,vR,tR,p],ΣR是在步骤C从qdr台服务器收到的Replica消息.E.当Si从Sd收到SignRead消息时,检查:(a)ΣR是否是qdr台服务器针对ReqR的Replica消息;(b)MR是否包含ΣR中的正确结果.检查通过后,Si计算部分签名PSi(MR),回复[PSRead,state,i,PSi(MR),ReqR]i.F.Sd周期性地重复执行步骤D,直至收到fd+1台服务器(包括Sd)的PSRead消息.然后Sd将fd+1个部分签名合成为[RespRead,c,x,vR,tR,p]SK,发送给客户端c.WRITEA.当Sd收到ReqW=[ReqWrite,c,x,vW,p,RespR]c时,验证有效后、发送[SignReplica,state,d,ReqW]d给所有服务器.B.当Si从Sd收到SignReplica消息时,验证其中的ReqW有效后,计算部分签名PSi(x,vW,tW),回复[PSReplica,state,i,PSi(x,vW,tW),ReqW]i.其中,tW=(seq(tR)+1)|Hash(ReqW),tR是包含在RespR中的原有时戳.C.Sd周期性地重复执行步骤A,直至收到fd+1台服务器(包括Sd)的PSReplica消息.然后Sd将fd+1个部分签名合成为[x,vW,tW]SK.D.Sd发送[Write,state,d,[x,vW,tW]SK,ReqW]d给所有服务器.E.当Si从Sd收到Write消息时,验证[x,vW,tW]SK是有效的自验证数据后,回复[Ack,state,i,[x,vW,tW]SK,ReqW]i.然后Si比较[x,vW,tW]SK和自己存储的[x,vi,ti]SK或[x,vi,ti],如果tW>ti,就更新自己的数据.F.Sd周期性地重复执行步骤D,直至收到qdw台服务器(包括Sd)的Ack消息.G.Sd向所有服务器发送门限签名请求[Sign-Write,state,d,MW,ΣW,ReqW]d,其中,MW=[RespWrite,c,x,vW,tW,p];ΣW是在步骤F从qdw台服务器收到的Ack消息.H.当Si从Sd收到SignWrite消息时,检查:(a)ΣW是否是qdw台服务器针对ReqW的Ack消息;(b)MW是否是ReqW对应的写响应消息.检查通过Page5后,Si计算部分签名PSi(MW),回复[PSWrite,state,i,PSi(MW),ReqW]i.I.Sd周期性地重复执行步骤G,直至收到fd+1台服务器(包括Sd)的PSWrite消息.然后Sd将fd+1个部分签名合成[RespWrite,c,x,vW,tW,p]SK,发送给客户端c.3.4TSS-mBQS状态的服务器协议TSS-mBQS状态能容忍fm台失效服务器,读/写Quorum包含的服务器数量分别记为qmr和qmw.当Sd的state变量表明当前是TSS-mBQS状态时,使用如下服务器协议.相比TSS-dBQS状态,TSS-mBQS状态的服务器协议有如下变化:(1)各消息中的state设定为TSS-mBQS;相应地,读/写Quorum的服务器数量变为qmr和qmw.(2)因为TSS-mBQS状态存储普通数据,WRITE操作不需要步骤B、C和D对数据进行门限签名.在步骤A,Sd直接发送[Write,state,d,[x,vW,tW],ReqW]d给所有服务器.所以,在步骤E,当Si从Sd收到Write消息时,验证其中的ReqW有效、[x,vW,tW]是对应的普通数据后,回复Ack消息.4TSS-BQS系统的GracefulDegradation机制3.3节和3.4节描述了系统的两种运行状态.TSS-mBQS状态容忍的失效服务器数量小于TSS-dBQS状态(见4.3节的分析),但是读/写操作性能更高(见4.6节的详细性能分析和实验结果).基于上述观察和分析,我们设计了TSS-BQS系统的GracefulDegradation机制:在初始阶段,系统以TSS-mBQS状态运行,容忍fm=fd2台失效服务器;当运行一定时间或者出现安全事件后(关于状态切换的发起,详见5.2节讨论),切换到TSS-dBQS状态,降低性能、容忍fd台失效服务器.需要指出,在状态切换期间,最多只能有fm台失效服务器;只有状态切换完成后,TSS-BQS系统才能容忍fd台失效服务器.相比现有的TSS-BQS系统[3,5-7](由3fd+1台服务器组成、容忍fd台失效服务器),在不影响容错能力、容忍fd台失效服务器的前提下,我们的系统在部分时间里运行在性能更高的TSS-mBQS状态,所以GracefulDegradation机制能带来更高的平均性能.在我们的系统中,GracefulDegradation机制表现为由服务器合作执行的状态切换协议,状态切换协议使各服务器的state变量从TSS-mBQS变为TSS-dBQS.下面,我们先列出切换协议应满足的条件:(1)容错性.状态切换应由多台服务器共同发起或者经过多台服务器同意才能完成;否则,失效服务器就可以在不必要的时候(例如,系统刚开始运行时),恶意地切换到TSS-dBQS状态,就相当于系统始终运行在TSS-dBQS状态,不能提高性能.(2)完整性.在fm台失效服务器的情况下,切换协议能够顺利地执行.而且,在状态切换协议结束之后,所有读/写操作都必须在TSS-dBQS状态下进行,不可能再执行TSS-mBQS状态的读/写操作.(3)兼容性.切换协议能与读/写操作协议并行执行,可在不中断存储服务、不影响客户端的情况下完成.(4)一致性.切换协议与客户端/服务器的读/写操作协议使用一致的系统模型.(5)高效性.切换协议应能够快速地执行;否则,如果切换协议耗时太长,甚至不如恢复失效服务器的速度,状态切换就没有意义.4.1状态切换协议在下面的状态切换协议描述中,我们先假定至少会有1台正确服务器来发起切换协议(也称为Delegate、记为Sd).在5.2节的讨论中,我们将说明如何保证有正确服务器作为Delegate发起切换协议.此外,需要指出,状态切换操作、读/写操作的Delegate相互之间没有联系,任意服务器都可以作为Delegate、各操作的Delegate可以不同.A.Sd发送[SignTicket,d,MG,reason]d给所有服务器.其中,MG=[Ticket,Ig],reason是发起本次切换协议的原因(详见5.2节的说明),Ig=Hash(reason)是本次状态切换的唯一标识.B.当Si收到SignTicket消息时,验证其中的reason有效后,计算部分签名PSi(MG),回复[PSTicket,i,PSi(MG),Ig]i.C.Sd周期性地重复执行步骤A,直至收到fd+1台服务器(包括Sd)的PSTicket消息.然后Sd将fd+1个部分签名合成为[Ticket,Ig]SK.D.Sd发送[Degrade,d,[Ticket,Ig]SK]d给所有服务器.E.当Si收到Degrade消息时,验证其中的[Ticket,Ig]SK的Servicekey数字签名有效后,存储Page6[Ticket,Ig]SK、将自己的state变量设定为TSS-dBQS,回复[Echo,i,Ig]i.F.Sd周期性地重复执行步骤D,直至收到n-fm台服务器(包括Sd)的Echo消息.4.2并发执行情况的处理在切换协议执行过程中,如果同时有并发执行的读/写操作,则参加读/写操作的服务器的state变量可能不同:因为在异步网络环境中,有些服务器已经切换到TSS-dBQS状态、有些仍然处于TSS-mBQS状态.我们采取如下方式处理:(1)Si只处理与自己state变量相同的读/写操作消息.当Si处于TSS-mBQS状态,只处理state是TSS-mBQS的消息;当Si处于TSS-dBQS状态,只处理state是TSS-dBQS的消息.(2)当Si处于TSS-dBQS状态,如果收到TSS-mBQS状态的读/写操作消息,则直接回复自己存储的[Ticket,Ig]SK,要求Sd切换到TSS-dBQS状态,然后重新执行读/写操作的服务器协议.因为Ticket消息带有ServiceKey数字签名,任何服务器都可以验证.(3)当Si处于TSS-mBQS状态,如果收到TSS-dBQS状态的读/写操作消息,则Si请求Sd先发送[Ticket,Ig]SK.在Si验证[Ticket,Ig]SK有效、切换到TSS-mBQS状态后,继续执行读/写操作的服务器协议.(4)当读/写操作的Sd处于TSS-dBQS状态,如果收到其它服务器的请求,则回复[Ticket,Ig]SK.(5)当读/写操作的Sd处于TSS-mBQS状态,如果收到有效的[Ticket,Ig]SK(无论是在切换协议的Degrade消息中、或者是读/写操作的服务器Si回复消息中),就先切换到TSS-dBQS状态,然后重新开始执行读/写操作.4.3参数分析首先,系统参数需要满足BQS系统[1]要求,才能够保证读/写操作的服务器协议的顺利执行和正确性:(1)DisseminationBQS系统要求,qdrn-fd,qdwn-fd,qdr+qdw-nfd+1.当n=3fd+1时,有qdr=qdw=2fd+1.(2)MaskingBQS系统要求,qmrn-fm,qmwn-fm,qmr+qmw-n2fm+1.其次,因为引入了状态切换,我们还需要考虑状态切换时的特殊情况.在切换到TSS-dBQS状态之后,如果尚未执行过TSS-dBQS状态的写操作,则各服务器仍存储普通数据[x,vi,ti].所以,此时执行TSS-dBQS状态的读操作,不但要考虑自验证数据,还需要考虑普通数据的情况:(1)因为状态切换已完成、系统处于TSS-dBQS状态,可能已有fd台失效服务器.所以,在最差情况下,会有fd+n-qmw台服务器向Sd回复错误的普通数据(其中,有fd台失效服务器,又有n-qmw台没有参加上一次TSS-mBQS状态的写操作).所以,只要Sd得到fd+n-qmw+1个相同数据,就可以确定是正确结果.(2)为了保证在各种失效情况下,Sd都能够得到:fd+n-qmw+1个相同数据,则TSS-mBQS状态的写操作至少写入fd+(fd+n-qmw+1)台服务器,才能容忍fd台服务器在写入数据后失效,不参加TSS-dBQS状态的读操作;所以,有qmwfd+fd+n-qmw+1qmw(n+2fd+1)/2.(3)MaskingBQS系统要求qmwn-fm,可得到:n-fm(n+2fd+1)/2fmfd/2.仅在qmw=n-fm时,不等式中的等号才成立,此时fm取最大值fd2,使系统尽可能长时间处于TSS-mBQS状态以提高平均性能.(4)由qmw=n-fm,容易推导得到:当n-fmqmr3fm+1时,满足MaskingBQS系统的可用性和一致性要求.基于上述分析,在TSS-dBQS状态,从qdr=2fd+1个数据中选出正确结果的方法:先尝试寻找出现次数不少于fd+fm+1的结果,作为正确结果;否则,将验证有效的、时戳最大的自验证数据作为正确结果.在TSS-mBQS状态,从qmr=3fm+1个数据中选出正确结果的方法:去掉出现次数少于fm+1的结果,剩余结果中时戳最大的就是正确结果.注意:在TSS-dBQS状态,应该优先尝试寻找出现次数不少于fd+fm+1的结果,以避免失效服务器回复上一个ProactiveRecovery周期的、旧的自验证数据.4.4安全性分析出的各项要求:下面,我们分析状态切换协议是否满足上文提(1)容错性.Ticket消息需要fd+1台服务器合作进行ServiceKey门限签名,其中必定有正确服务器参加.所以,即使fd台失效服务器合谋,也不能在不必要的时候切换状态.Page7(2)完整性.首先,状态切换协议需要fd+1台服务器进行Ticket消息的ServiceKey门限签名、n-fm台服务器回复Echo消息,因为在状态切换过程中最多有fm台失效服务器,所以肯定能顺利执行结束.其次,在状态切换完成后,至少有n-fm台服务器已收到Ticket消息(其中可能包含有fd台失效服务器).在此之后,如果要成功执行TSS-mBQS状态的写操作,则至少要有qmw台服务器参加,易验证有qmw+(n-fm)-n>fd成立;也就是说,在TSS-mBQS状态的写操作过程中,必然有已收到Ticket消息、处于TSS-dBQS状态的正确服务器参加,该服务器会使Sd切换到TSS-dBQS状态.最后,对于TSS-mBQS状态的读操作,则略有不同.根据上一节分析,要求n-fmqmr3fm+1,同时还要满足qmr+(n-fm)-n>fd,才会使得在此之后的TSS-mBQS状态的读操作不能执行结束,保证有正确服务器会使Sd切换到TSS-dBQS状态,并重新进行TSS-dBQS状态的读操作.容易计算,满足上述条件的qmr最小值是fm+fd+1,使得在切换协议结束之后,所有读/写操作都在TSS-dBQS状态下进行.综合以上分析,表1列出了各系统参数的表达式.签名门限fm=fdfd2317564341107954521391175621611148(3)兼容性.首先,客户端发出的读/写请求消息中不包含系统的状态信息,在TSS-mBQS状态或者TSS-dBQS状态下,读/写操作的Sd都可以用来发起服务器协议.其次,根据4.2节的并行执行情况处理,在状态切换过程中开始的读/写操作,也会转为TSS-dBQS状态的读/写操作并顺利执行.因为切换协议中使用了带有ServiceKey数字签名的Ticket消息,各服务器都可以独立地验证是否有效、分别切换状态,所以Sd可以直接要求其它服务器切换状态,配合执行服务器协议.第三,根据4.3节的参数分析,在切换到TSS-dBQS状态后,虽然各服务器存储普通数据,读操作仍可以顺利执行并得到正确结果;状态切换后TSS-dBQS状态的写操作,也可以顺利执行.(4)一致性.上述的安全性分析,使用了与客户端/服务器协议完全一致的系统模型.4.5性能分析和实验结果本节先从通信量和计算量分析了两种状态下的读/写操作、状态切换的性能;然后给出了原型系统的实验结果.理论分析和实验结果都表明:TSS-mBQS状态的性能优于TSS-dBQS状态,状态切换的资源消耗与1次读/写操作相当、能够快速完成.4.5.1性能分析图1给出了两种状态下的读/写操作、状态切换的通信和计算情况.其中,在分析写操作的性能时,不包括发送写请求消息ReqW之前的读操作(用来获得当前时戳);对于TSS-dBQS状态的读操作,只考虑自验证数据的情况,因为在大部分情况下,TSS-dBQS状态的读操作都是得到自验证数据.在图1中,箭头表示通信过程、方框表示计算过程.并行执行的通信过程,在图中表示为1次通信.例如,Sd同时向多台服务器发送Read消息、多台服务器向Sd同时回复Replica消息,所需通信时间与服务器数量基本无关、取决于其中耗时最长的某一次通信.可以看出,TSS-mBQS状态的读/写操作需要3轮通信;TSS-dBQS状态的读操作需要3轮通信、写操作需要4次通信;状态切换需要2轮通信.COCA[3]和CODEX[5]的实验测试结果表明:在TSS-BQS系统中,主要计算消耗是公钥密码计算(ServiceKey门限签名和验证、ServerKey签名和验证),其它的计算(如Hash计算等)可忽略不计.图1中,Ts和Tp分别表示计算1次ServerKey数字签名和1次ServiceKey部分签名的时间,Tv表示1次数字签名验证的时间,Tc表示合成ServiceKey签名的时间.同样地,不同服务器并行执行的计算过程,计为1次计算;例如,多台服务器同时执行ServiceKey门限签名、多台服务器分别用自己的ServerKey签名消息,所需计算时间与服务器数量基本无关、取决于其中计算最慢的某1台服务器.但是,由同一台服务器执行的连续多次计算,则计为多次;例如,Sd验证qmr个Replica消息上的ServerKey数字签名.表2列出了各操作的计算时间公式以及当fd=2时的值(在原型系统中,fd=2).4.5.2原型系统和实验结果按照上文描述的协议,我们使用C语言实现了原型系统.在原型系统中,fd=2、签名门限fd+1=3,其它参数按照表1来设定.ServiceKey和ServerKey都是1024-bitRSA密钥对,使用Shoup门限签名方案[9]来拆分ServiceKey私钥.每一个变量有Page8图1TSS-BQS系统的通信量和计算量表2TSS-BQS系统的计算量状态切换操作32-bit唯一标识和512-bit值,时戳是192-bit(包括32-bit的整数序号和160-bit的SHA-1散列值).系统由7台服务器和1台客户端组成,它们之间用100M以太网连接、UDP通信,配置都是IntelPentium4(2.4GHz)CPU、256MRAM、Windows2000Server(SP2)操作系统.表3列出了原型系统的实验结果.其中,读操作的测量时间是从客户端生成ReqR到验证RespR,写操作的测量时间是从客户端生成ReqW到验证RespW、不包括发送ReqW之前的读操作,协议切换操作的测量时间是从Sd生成SignTicket消息到验证qmw个Echo消息.读操作写操作协议切换操作5讨论5.1与ProactiveRecovery的结合TSS-BQS系统的重要优点是支持ProactiveRecovery.对于TSS-BQS系统的存储服务,每一次的ProactiveRecovery都需要如下处理[3,12-13]:(1)每一台服务器从可信的只读介质重新启Page9动,恢复到运行代码无错误的状态.(2)对于每一个变量,每一台服务器要重新获取正确的值和时戳,恢复到数据无错误的状态.(3)每一台服务器更新ServerKey并向其它服务器分发ServerKey公钥、重新拆分ServiceKey私钥,保证攻击者不能获得超过f台服务器的ServerKey私钥或者恢复出ServiceKey私钥.完成上述ProactiveRecovery需要消耗大量的服务器资源;尤其当存储大量数据时,需要繁重的通信过程才能保证恢复到数据无错误的状态.所以,在TSS-BQS系统中,应尽量减少ProactiveRecovery次数.另一方面,对于给定的容错系统和运行环境,执行ProactiveRecovery的周期与系统容忍的失效服务器数量成正比:容忍的失效服务器数量越少,需要越频繁地定期执行ProactiveRecovery[10].结合本文设计的GracefulDegradation机制,TSS-BQS系统能以如下方式来执行ProactiveRecovery:(1)在系统初始启动或者每一次ProactiveRecovery之后,各服务器以TSS-mBQS状态运行.(2)以TSS-mBQS状态运行fm到TSS-dBQS状态.其中,TPR是根据运行环境的特性,估计不超过fd台服务器失效的最长时间;也就是,已有TSS-BQS系统[3,5-7]的ProactiveRecovery周期.(3)以TSS-dBQS状态运行1-fm执行ProactiveRecovery.下比较优势和灵活性:本文设计的GracefulDegradation机制,具有如(1)如果n台服务器组成纯粹的TSS-mBQS系统(始终以TSS-mBQS状态运行),只能容忍n-14台失效服务器.相比而言,我们的系统能容忍fd=n-1延长了约1/3,减少了ProactiveRecovery次数;代价则是只有约一半时间具备TSS-mBQS系统的性能、其余时间的性能只相当于TSS-dBQS系统.(2)相比纯粹的TSS-dBQS系统,我们的Pro-activeRecovery周期相同,但是有约一半时间以TSS-mBQS状态运行、性能更高,提高了系统的平均性能.5.2状态切换协议的发起状态切换协议可以在如下情况下发起:(1)结合ProactiveRecovery,在预定时间定期发起.任何正确服务器,在预定时间到达时,自动转为Delegate,发起切换协议;或者,也可以采取PBFT[12]所使用的轮换方式:各服务器按顺序轮流担任Delegate,如果发现当前Delegate未完成状态切换协议,则认为该Delegate已失效;由其余服务器合作确定下一台服务器来担任Delegate.切换协议中的reason=[Schedule,time],其中Schedule表示是按预定时间执行,time则是预定的时间.Si验证reason是否有效:time是否与Si设定的时间一致、time与Si本地时钟的误差在允许范围内.(2)发生某些预料之外的重大安全事件,对运行环境的原有估计不足,则需要在预定时间之前就发起切换协议.例如,爆发全新的严重病毒和木马、可能会影响服务器状态,或者发现了服务器的系统漏洞,但相应补丁尚未发布等.此时,发现安全事件的正确服务器会转为Delegate,发起切换协议,相应的reason=[Event,description],其中Event表示是因为安全事件而切换状态,description给出安全事件的描述(例如,发布病毒、木马或系统漏洞的可信网站链接).Si验证reason需要人工干预,由管理员阅读description,判断是否有必要进行状态切换.6相关研究文献[4]最早提出了GracefulDegradation的概念:运行环境恶化、超出预先估计时,系统调整运行状态,继续提供服务但是服务质量有所降低.此后的研究工作将GracefulDegradation概念应用于集群服务[14]、存储阵列[15]等领域,设计了以降低性能[14,16]、减少可访问文件数量[15]、降低响应结果全面性[16]和准确性[17]等为代价的GracefulDegradation机制,保证失效情况下的服务可用性.针对状态机复制StateMachineReplication系统,文献[18-19]分别讨论了当失效服务器数量超过预先估计时,通过限制失效服务器的恶意操作[18]和支持部分操作命令[19]来实现GracefulDegradation.但是,上述研究的GracefulDegradation机制都不是针对BQS系统的也无法直接应用于BQS系统.COCA[3]首次结合了门限签名方案和Dissemi-nationBQS系统,完成了TSS-BQS系统(也相当于本文的TSS-dBQS状态).使用与COCA相同的服务器协议,CODEX[5]实现了容错的机密性数据存储Page10服务.SP-II设计了一种新的TSS-BQS系统服务器协议[6].文献[7]证明:TSS-BQS系统只存在两类有效的服务器协议,且两类协议的代表分别就是COCA[3]和SP-II[6].类似地,PBFT-BC[20]利用门限签名方案来扩展了BFT-BC协议[21],完成了容忍失效客户端的BQS系统,同时支持ProactiveRecovery.在设计思想上,TSS-BQS系统的ProactiveRecovery[3,10-11]和DynamicBQS系统[22],与本文的GracefulDegradation机制有一定的相似之处:在失效服务器数量有可能要超过预先估计时,提前采取措施,使系统能继续提供容错服务.ProactiveRecovery是定期性地将所有服务器恢复到初始的正确状态,避免失效服务器数量超过预先估计;而本文GracefulDegradation机制是改变存储数据的类型(也就是改变BQS系统的类型),使得能够容忍更多的失效服务器.此外,需要注意,本文提出的GracefulDegradation机制完全可以和ProactiveRecovery同时结合使用.DynamicBQS系统[22]是由4f+1台服务器组成的MaskingBQS系统,最多能容忍f(f>1)台失效服务器:在初始阶段,设定适当的qr/qw,只容忍1台失效服务器;随着系统运行,不断地动态改变qr/qw,降低性能、容忍2、3、…、f台失效服务器.本文的GracefulDegradation机制与DynamicBQS系统的区别主要在于:(1)DynamicBQS系统的容错参数变化针对同一类型的BQS系统,而GracefulDegradation机制是在两种不同类型的BQS系统之间切换;(2)DynamicBQS系统只是变化了每次读/写操作的服务器数量,通信轮数和计算时间没有明显变化,性能提高有限,而我们的状态切换改变了通信轮数和计算时间,性能提高更为明显;(3)DynamicBQS系统的客户端需要自己处理容错参数,而我们的机制基于TSS-BQS系统,客户端不需要知道系统的运行状态.7总结本文设计了用于TSS-BQS系统的GracefulDegradation机制:系统由n=3fd+1台服务器组成,在初始阶段以TSS-mBQS状态运行,能容忍fm=fd2台Byzantine失效服务器;随着系统运行,失效的服务器数量增大、有可能将要超过fm,以降低性能为代价,切换到TSS-dBQS状态,能容忍fd台Byzantine失效服务器.在我们的GracefulDeg-radation机制中,客户端并不需要知道系统的运行状态(TSS-mBQS或TSS-dBQS状态),系统可以在不影响客户端、不中断存储服务的前提下完成状态切换.本文设计的状态切换协议,使得相比纯粹的TSS-dBQS系统[3,5-7](由n=3fd+1台服务器组成、容忍fd台Byzantine失效服务器),能以更高的平均性能运行;在失效服务器数量达到fm、丧失容错能力时,通过快速的状态切换,继续提供容错的存储服务,相比纯粹的TSS-mBQS系统,减少了ProactiveRecovery次数.在不影响TSS-BQS系统容错效果的前提下,GracefulDegradation机制带来了更高的平均性能,是已有TSS-BQS系统ProactiveRecovery的重要补充.
