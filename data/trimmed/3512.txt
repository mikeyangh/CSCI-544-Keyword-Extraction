Page1一种新的局部判别投影方法谢钧1)刘剑2)1)(解放军理工大学指挥自动化学院南京210007)2)(解放军理工大学通信工程学院南京210007)摘要为解决多模数据的分类问题,局部化思想被引入到判别分析中,称为局部判别分析.该文以人工数据为例深入分析了近年来提出的较为成功的两种局部线性判别分析方法:LFDA(LocalFisherDiscriminantAnalysis)和MFA(MarginalFisherAnalysis)的不足.为克服这两种方法中没有充分考虑异类样本近邻关系的缺点,文中提出了一种新的局部判别投影方法.该方法采用与LFDA和MFA不同的局部化方法,其基本思想是寻找投影方向使同类近邻样本在投影后尽量紧凑,而异类近邻样本在投影后尽量分开.针对该思想,文中提出了两种优化目标(一种用样本间距离平方和来表示,另一种用样本类内与类间散度来表示)并做了分析和比较.实验结果表明,该文方法有效地克服了LFDA和MFA存在的固有问题,在人工数据集、UCI、USPS手写数字标准数据集和IDA标准数据集上均取得较好效果.关键词分类;降维;线性判别分析;局部保持;多模1引言降维是通过某种变换把高维空间中的数据变换到低维空间,在降低数据维数的同时尽可能地保留原始数据的本质信息.降维技术是高维数据压缩、可视化和分类的重要预处理手段,在计算机视觉、机器学习和模式识别等领域应用广泛.降维方法可分为线性降维(采用线性变换)和非线性降维(采用非线性变换).以ISOMap[1]、Laplacian特征映射[2]、LLE(LocallyLinearEmbedding)[3]等为代表的非线性降维方法虽然在处理特定的人工数据时表现出很好的性能,但面对实际数据时却并没有明显优势[4].此外,一些基于流形学习的非线性降维方法通常不能得到一个明确的变换,因而无法处理新出现的样本,使得此类方法在分类中的应用受到限制.在分类中更常用的是各种线性降维方法,包括传统的PCA(PrincipleComponentAnalysis)[5]、LDA(LinearDiscriminantAnalysis)[5]及近年来提出的LPP(LocalPreservingProjections)[6]、NPE(NeighborhoodPreservingEmbedding)[7]、LEA(LocallyLinearEmbedding)[8]、ISO投影[9]、SPP(SparsityPreservingProjections)[10]等等.此类方法通过训练得到的投影矩阵可直接将未见样本投影到低维空间.降维又可分为无监督降维和有监督降维,前者关注如何更好地表示高维数据,通常不借助样本的类别信息;而后者是为了有效地实现分类,将类别信息用于降维过程中.两种降维也分别称作表示性降维和判别性降维.在线性判别降维方法中LDA较为经典,它以最小化类内散度和最大化类间散度为优化目标.LDA在模式识别领域得到广泛应用,但对于多模数据(同一类样本包含几个不同分布的簇)的分类却并不理想[11].要解决多模数据的分类问题,需要在降维过程中借助局部信息,尽可能地保留数据的流形特性.为此,一些研究聚焦在如何将局部化思想引入到判别性降维方法中,例如近年提出的LFDA(LocalFisherDiscriminantAnalysis)[11]和MFA(MarginalFisherAnalysis)[12].本文深入分析了LFDA和MFA的不足,并在此基础上提出了一种新的局部线性判别降维方法,采用与LFDA和MFA不同的局部化方法,有效地克服了两种方法存在的固有问题,在人工数据、USPS、UCI手写数字标准数据集和IDA标准数据集上的实验中均取得较好效果.2局部化线性判别降维方法由n个样本组成的样本集记作犡=[狓1,狓2,…,狓n]d×n,狓i是第i个样本,类标号为ci,维数为d.线性降维是按照某种优化准则求得一个d×m(m<d)的变换矩阵犠,将样本集投影到低维空间得到犢=犠T犡=[狔1,狔2,…,狔n]m×n,其中狔i是狓i在低维空间的投影,维数为m.2.1LDA和LPP经典线性判别降维方法LDA的基本思想是最小化类内散度和最大化类间散度,并以散布矩阵行列式作为样本散度的度量.其优化目标为其中c是样本类别总数,μ是所有样本的均值,μl是第l类样本的均值,犛(w)称为总类内样本散布矩阵,犛(b)称为总类间散布矩阵.为便于与LFDA和MFA比较,把LDA的犛(w)和犛(b)写成下面的等价形式[11]:犛(w)=1犛(b)=1其中犘=[pij]n×n和珟犘=[p~珟犇是n×n的对角阵,对角元素di=∑j∑jij,以下均采用这种记号.从式(2)不难看出,权p~值pij和p~否同类有关,然而pij和p~权值与狓i和狓j是否相邻无关.LPP[6]是一种旨在保留局部信息的表示性降维方法,在降维过程中能较好地保持流形特性,是Laplacian特征映射[2]的线性近似.LPP的基本思想是希望在高维空间中相距较近的样本点在投影后的Page3低维空间中仍比较近.其优化目标是其中Nk(j)表示样本狓j的k近邻集合,Aij是样本狓i和狓j之间的一种相似性度量,如果狓i和狓j差距大则Aij小.例如Aij可定义为Aij=exp-‖狓i-狓j‖2,以σi,σj为参数.可见LPP中的权值pij体现了局部信息,与狓i和狓j是否相邻有关.2.2LFDA和MFALFDA和MFA有效结合了LDA和LPP的思想,在LDA权值的定义中融入局部信息,在数据表示和分类中均取得较好效果.两者可与LDA纳入到同一个优化框架中在LFDA中权值定义为其中Aij表示狓i和狓j之间的一种相似性度量,通常在两个样本不是近邻时取0.在MFA中权值定义为pij=1,狓i∈N+ij=1(i,j)∈Pk2(ci)或(i,j)∈Pk2(cj)p~其中N+是集合{(i,j)|i∈πc,jπc}中前k2个最近的样本对,πc表示类别号为c的所有样本的标号集合.2.3LFDA和MFA的不足从式(5)和(6)不难看出,LFDA和MFA都在同类样本间的权值定义中引入了局部信息,同类近邻样本之间权值非0,同类非近邻样本之间权值为0.两种方法的主要区别在于对异类样本权值的选择上.LFDA不考虑异类样本之间的近邻关系,只要两个样本异类,两者之间的权值p~会带来两个缺点:一是对野值敏感,例如存在一个野值狓o,则所有与野值异类的样本狓i,差值项(狓i-狓o)(狓i-狓o)T都将出现在目标函数中,这样优化得到的投影矩阵犠将受狓o的影响过大.第二,不考虑异类样本之间的近邻关系,会导致相距较远的异类样本在目标函数中占据较大比重,以致在处理某些多模问题时出现错误.例如后节图1所示的多模数据分类问题,“o”和“+”代表的两类数据分别有左和右两个模态,LFDA选择的投影直线接近水平,导致不同类的数据重叠在一起.这是因为“o”类的左(右)模态和“+”类的右(左)模态数据是异类样本,为了使这些异类样本在投影之后距离较远,优化过程选择了投影到水平方向.其根本原因是LFDA在定义异类样本权值时没有考虑近邻关系.MFA排列出每类样本到异类样本之间的前k2个距离最近的样本对,并将这些样本对之间的权值设为1,这相当于仅考虑了部分位于边界的样本与异类样本之间的关系.这将带来两个问题:一是参数k2较难选取,因为它依赖于样本个数和数据维数,样本越多、维数越大,位于边界的样本点就越多,就要选择更大的k2才能充分描述边界样本和异类样本之间的关系.二是在多模问题中可能会出现一类中某一个模态的样本比本类其它模态的样本到异类样本之间的距离要远,那么该模态的样本与异类样本之间的距离将被忽视,导致出现错误的结果.例如图3所示的“o”类样本有左上和右下两个模态,因为右下模态与“+”类样本较近,因而异类样本距离的前k2个距离最近的样本对中就没有左上模态的样本,也就是说“o”类样本左上模态与“+”类样本之间的距离完全不在优化目标考虑的范围内,那么MFA在选择投影方向时除了同类近邻之间的距离以外,只考虑到“o”类样本右下模态与“+”类样本之间的距离要远,以至于选择了向水平方向投影,导致“o”类样本左上模态与“+”类样本完全混叠.3一种新的局部判别投影方法为解决多模数据的分类问题,LFDA和MFA将局部化思想融入到LDA当中,但它们对异类样本间权值的设定方法使它们不能正确处理某些多模数据的分类问题.本文提出了一种新的线性局部判别投影方法LDP(LocalDiscriminantProjection),有效地解决了LFDA和MFA存在的问题.LDP同时考虑了同类样本和异类样本间的近邻关系,其基本思想是希望同类近邻样本在投影后尽量紧凑,而异类近邻样本在投影后尽量分开.这样Page4的思想可以通过两种目标函数来刻画,第一种是用样本间距离平方和来刻画;第二种是用样本类内和类间加权散度来刻画.本文分别给出了这两种目标函数的分析和实验结果的比较.设样本集记作犡=[狓1,狓2,…,狓n]d×n,d×m变换矩阵犠,变换后样本犢=犠T犡记作犢=[y1,y2,…,yn]m×n.第1种目标函数是最小化同类近邻样本之间的距离,同时最大化异类近邻样本之间的距离.同类近邻样本之间的距离平方和为f1(犠)=∑n其中N+f1(犠)变形得到f1(犠)=∑n=∑n=tr∑n=tr犠T∑n=tr犠T犡(犇-犘)犡T其中犘=[pij]n×n是权值矩阵,tr表示矩阵的迹,犇与犘的关系同式(2).类似地,异类样本之间的距离平方和为f2(犠)=∑n其中N-可得按照最小化同类近邻样本间距离,最大化异类近邻样本之间距离的思路,我们可把目标函数定义为min犠∈Rd×mpij=1狓j∈N+ij=1狓j∈N-p~其中犘=[pij]n×n和珟犘=[p~的关系同式(2).这个优化问题可以用文献[13]提出的迭代算法求得唯一最优解.第二种目标函数是最小化类内加权散度,最大化类间加权散度.散度反映了样本散布的离散程度,散度的一个简单度量就是散布矩阵行列式.类内和类间散布矩阵可写成式(2)中犛(w)和犛(b)的形式,修改其中的系数pij和p~布矩阵.本文采用式(11)中定义的pij和p~权散布矩阵,并采用加权散布矩阵的行列式作为散度的度量,那么优化目标就可纳入到框架式(4)中,写成其中N+表示样本狓j的异类k近邻集合,犘=[pij]n×n和珟犘=[p~ij]n×n是权值矩阵,犇与犘的关系同式(2).比较目标函数(11)和(12)不难看出,前者是类内加权散布矩阵和类间加权散布矩阵的迹的比值,而后者是两个矩阵行列式的比值.散布矩阵的各特征值正比于样本散布超椭球体的各轴长,考虑到矩阵的迹是各特征值之和,而矩阵的行列式是各特征值之积,因此散布矩阵的迹正比于超椭球体各轴长之和,而散布矩阵的行列式正比于各轴长之积(超椭球体的体积).尽管这两个指标都一定程度上反映出样本散布的程度,但相对而言还是超椭球体的体积反映得更加准确,实验结果也证实了这一点,在多数情况下,目标函数(12)获得了比(11)更好的识别结果.4实验结果与讨论为评估各种算法的性能,本文完成了3类实验.第1类是为了说明各种算法在原理上的区别而构造的3种多模人工数据;第2类是标准手写数字库USPS和UCI,我们把10个数字分成两类,每类都是多模数据;第3类是IDA标准数据库共13个数据集,其中有3个数据集的数据呈现多模分布.3类实验中我们分别比较了LDA、LPP(drtoolbox[4])、LFDA(文献[11]提供代码)、MFA以及本文算法LDP的工作原理.LDP中选择近邻个数k=8,MFA中同类近邻个数k1=8,异类距离最近的样本对个Page5数取k2=10,LPP、LFDA选用源码中的默认参数.实验结果表明无论是在人工数据还是标准数据库,本文算法都具有优势.4.1人工数据每个人工数据实验随机产生200组数据(3个实验中分别有一组数据显示在图1、图2和图3中,图例中的“LDP”是以式(12)为目标函数,在各表中记为“LDP2”),每组数据包含两类各100个样本.每次实验取一组作为训练,另一组作为测试,共进行100次.对每种方法(LDA,LPP,LFDA,MFA,本文算法LDP2)计算了100次实验识别率的均值,如表1所示.3个人工数据实验中的数据均产生自二元正态分布,实验1中二元数据的协方差阵为二阶单位阵,两类共计4个模态的数据均值分别为(-7,3)(7,3)(-7,-3)(7,-3).实验2中,数据的协方差阵是对角元素为1和36的对角阵,中间一类数据的均值为(0,1),两边一类的两个模态均值分别为(-5,0)和(5,0).实验3中数据协方差同实验2,右下角一类数据的均值为(-3,-5),另一类两个模态数据的均值为(-3,3)和(3,-5).LDA0.9880.5200.9870.832LPP0.5190.5060.7120.579LFDA0.5120.5140.6860.571MFA0.9530.9720.7800.902LDP20.9950.9810.8890.955在给出定量的实验结果之前,我们首先详细地分析3种人工数据实验数据的特点,并讨论每种方法在处理此类数据时的行为以及各自成败的原因.图1所示的多模数据分类问题中,“o”和“+”代表的两类数据分别有左和右两个模态,并且同类两个模态样本之间的距离大于不同类别相同模态样本之间的距离.显然正确的投影方向是垂直方向,实验结果显示LDA、MFA和LDP获得了正确结果,而LFDA和LPP没有获得正确结果.LDA的目标是最小化类内散度,同时最大化类间散度,显然向垂直方向投影,把同类的两个模态样本投影到一起同时满足这两个需求.MFA要使到处于边界的样本点与异类样本之间的距离较远,本文算法LDP要使异类近邻样本之间的距离较远,因此在这两种方法的目标函数中都会出现部分“o”类的左(右)模态和“+”类的左(右)模态样本之间的距离.这些项的存在使优化过程选择了向垂直方向投影(如果向水平方向投影会使这些异类样本之间的距离变小).“o”类的左(右)模态和“+”类的右(左)模态数据是异类样本,LFDA没有考虑异类样本之间的近邻关系,因此为了使所有这些异类样本在投影之后的距离平方和较远而选择了向水平方向投影.LPP的优化目标是在投影后样本范数的加权平方和为常数的条件下,希望相距较近的样本点在投影后仍比较近,因此投影到数据方差较大的方向(本例中是水平方向)能Page6获得更优的目标函数.图2所示的多模数据分类问题可简记做“夹心问题”,“o”类数据被夹在“+”类数据之间,而且每个模态分布都呈现为狭长的流型.显然正确的投影方向是水平方向,实验结果显示MFA和LDP获得了正确结果,而LDA、LFDA和LPP没有获得正确结果.同实验1类似,在MFA和LDP的目标函数中分别出现了“o”类数据和“+”类数据左和右两个模态之间样本的距离,这些项的存在导致了优化过程选择向水平方向投影,以防止这些异类样本之间距离变近.本例中两类样本各自的均值位置以及所有样本的均值位置十分接近,由于“o”类样本稍微偏上,因此LDA的投影方向为垂直方向.LPP属于无监督投影,对它而言样本3个模态的数据就是一个狭长分布的数据团,而向数据方差较大的方向投影可以获得更大的样本范数的加权平方和,因此LPP会选择向垂直方向投影.如果数据不是这样狭长并且靠得很近,LFDA和LPP是可以获得正确结果的(如文献[11]中Figure1(c)所示),而对于狭长的数据LFDA之所以出错还是因为这种狭长数据中相距较远的异类样本的距离和在目标函数中占据了过大的比重,是LFDA在异类中不考虑近邻关系的结果.图3所示的多模分类问题中,“+”类数据和“o”类数据左上和右下两个模态成犄角之势.为了使两类数据在投影后分开,正确的投影方向应该是近似45°直线或135°直线.只有LDA和本文算法LDP获得了正确结果.LDP能获得正确结果仍然是得益于目标函数中分别出现了“+”类数据样本和“o”类数据左上和右下两个模态之间样本的距离,这些项导致优化过程选择了正确的投影方向.LDA倾向于将同类不同模态的样本投影到一起以获得较小的类内散度,因而选择接近45°的直线.LPP选择向垂直方向投影是因为垂直方向数据的方差比其它方向更大,而且“+”类数据样本和“o”类右下数据之间存在近邻关系,要求投影后保持近邻关系.MFA出现错误结果是因为异类样本距离的前k2个距离最近的样本对中没有“o”类数据左上模态的样本,也就是说“o”类样本左上模态与“+”类样本之间的距离完全不在优化目标考虑的范围内,以至于选择了向水平方向投影,以保证同类近邻样本之间距离近以及“+”类样本与“o”类样本右下模态之间的距离较远.LFDA选择了向垂直方向投影是为了保持“+”类样本与“o”类样本左上模态之间这些异类样本距离较远.表1是各种方法在3个人工数据实验(每个实验进行了100组)中的平均识别率(投影后用最近邻分类器进行分类),从以上人工数据的分析以及表1不难看出,本文算法LDP有效地克服了LFDA和MFA在处理一些特殊多模问题时暴露出的缺点,实验结果具有明显优势.4.2标准手写数字库4.2.1标准库简介UCIMultipleFeaturesDataSet手写数字库是UCI机器学习数据集①(UniversityofCalifornia,IrvineMachineLearningRepository)中的一个标准数据库,包含了‘0’~‘9’10类数字的每类200个共计2000个样本.每个样本提取出包括付立叶系数、轮廓相关函数、KL系数、像素值均值、泽尔尼克(Zernike)矩、形态特征6类共计649个特征.美国国家邮政局USPS手写数字库②包括‘0’~‘9’10类数字的共计7291个训练样本和2007个测试样本,每个样本用256个灰度值来表示.4.2.2在标准库上的实验在UCI和USPS手写数字库上我们对LDP1(目标函数(11))、LDP2(目标函数(12))、LDA、LFDA和MFA进行了比较.从标准库中每个数字取100个样本作为训练样本,取100个样本作为测试样本.为公平比较,每个样本先用PCA降到相同维数(对于UCI,每个样本先用PCA降维到150维;对于USPS,每个样本先用PCA降维到100维),再用各算法降到10维,最后用最近邻分类器进行分类.实验结果见表2第2列和第3列,本文算法LDP2均获得次优解,并与最优结果十分接近.LDP10.9670.8610.9710.9870.9650.9010.8980.915LDP20.9830.8930.9900.9980.9860.9430.9500.939LDA0.9850.8760.9560.9500.9450.8020.8970.817LFDA0.9820.8950.9830.9950.9780.8770.9060.900MFA0.9830.8700.8960.9900.8610.8340.8510.824为比较在多模数据上的性能,实验中用多类手写数字构造两类多模数据.实验UCI1和USPS1中的两类是数字[01234]和[56789](实验结果见表2第4列和第7列),实验UCI2和USPS2中的两类是数字[13579]和[02468](实验结果见①②Page7表2第5列和第8列),实验UCI3和USPS3中的两类是[01269]和[34578](实验结果见表2第6列和第9列).实验结果显示本文算法LDP2在多模数据上有较明显的性能优势.同时实验结果还表明,目标函数(12)要明显优于目标函数(11),这也验证了我们此前对于两种目标函数的分析.4.3IDA标准数据库4.3.1标准库简介IDA标准库①最早在文献[14]中使用,后来成为模式识别中广泛使用的标准数据.它包含13个标准数据集以及每个数据集训练样本和测试样本的多个标准划分.每个数据集都是两类数据,其中表3中用“”标记的3个数据集中每类数据呈现多模分布.表3给出了各数据集的基本信息.数据集样本bananabreast-cancer920077100diabetisflare-solar9666400100germanheartimageringnorm204007000100splicethyroidtitanictwonorm204007000100waveform2140046001004.3.2在IDA数据库上的实验为公平比较,本文实验中将13个标准集上的数据都降至2维,然后采用最近邻分类器进行分类.在表4中最后一列给出了不降维直接用最近邻分类器banana0.86350.86370.86250.86360.86370.8636breast-cancer0.66080.64640.65340.63390.65000.6730diabetis0.68900.63920.66300.61720.67840.6988flare-solar0.60740.60260.61130.59350.60950.6078german0.68800.61440.67920.60660.65940.7054heart0.77840.76410.78000.63160.77170.7684image0.78090.82750.83420.76110.84420.9662ringnorm0.72680.64930.73190.65900.72850.6497splice0.79560.62200.83510.62030.83700.7115thyroid0.85600.94510.93360.93850.95170.9564titanic0.66920.66870.66870.66470.67000.6689twonorm0.96420.96280.96570.87090.96530.9332waveform0.81440.88280.88310.66270.87980.8417平均0.76110.74530.77710.70180.77760.7727的分类结果,该结果没有参与挑选最佳识别率.结果表明本文算法LDP在13类数据中有5类达到最优,在3类多模数据中有2类达到最优,此外还在13类的平均识别率上获得了最优.5总结为解决多模数据的分类问题,LFDA和MFA都将局部化思想融入到LDA当中.LFDA在构造权值矩阵时完全不考虑异类样本间的局部性,导致远距离的异类样本严重影响最终的投影方向.由于多模数据通常有不止一条边界,而MFA仅考虑最近异类样本间的关系,相当于只考虑了部分边界.本文深入分析了两种算法在工作原理上的弱点和问题,提出了一种新的线性局部判别投影方法LDP.该方法在设计异类样本间权值时也考虑到近邻的因素,其基本思想是希望同类近邻样本在投影后尽量紧凑,而异类近邻样本在投影后尽量分开.该算法有效解决了LFDA和MFA存在的问题,在人工数据集和标准数据集上都取得较好效果.
