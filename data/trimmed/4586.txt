Page1基于增量式分区策略的MapReduce数据均衡方法王卓陈群李战怀潘巍尤立(西北工业大学计算机学院西安710072)摘要MapReduce以其简洁的编程模型,被广泛应用于大规模和高维度数据集的处理,如日志分析、文档聚类和其他数据分析.开源系统Hadoop很好地实现了MapReduce模型,但由于自身采用一次分区机制,即通过Hash/Range分区函数对数据进行一次划分,导致在处理密集数据时,Reduce端常会出现数据倾斜的问题.虽然系统为用户提供了自定义分区函数方法,但不幸的是在不清楚输入数据分布的情况下,数据倾斜问题很难被避免.为解决数据划分的不均衡,该文提出一种将分区向Reducer指派时按照多轮分配的分区策略.该方法首先在Map端产生多于Reducer个数的细粒度分区,同时在Mapper运行过程中实时统计各细粒度分区的数据量;然后由JobTracker根据全局的分区分布信息筛选出部分未分配的细粒度分区,并用代价评估模型将选中的细粒度分区分配到各Reducer上;依照此方法,经过多轮的筛选、分配,最终在执行Reduce()函数前,将所有细粒度分区分配到Reduce端,以此解决分区后各Reducer接收数据总量均衡的问题.最后在Zipf分布数据集和真实数据集上与现有的分区切分方法Closer进行了对比,增量式分区策略更好地解决了数据划分后的均衡问题.关键词增量分配;细粒度分区;数据倾斜;均衡分区;MapReduce;大数据1引言近些年,以MapReduce[1]编程模型为基础的分布式系统在传统行业的日常业务处理中被广泛地应用,如通信行业的用户挖掘业务,保险行业的风险分析业务等.特别是Apache的开源项目Hadoop将MapReduce计算架构和HDFS分布文件系统进行完美的融合,使学术界和工业界对MapReduce的研究和应用都产生了极大热情[3-4].MapReduce模型将计算分为Map和Reduce两个阶段.在Map阶段,由各Mapper对输入元组进行分区处理,并将具有相同分区值的元组传给同一个Reducer进行相关的计算.Hadoop系统默认采用的是Hash分区法,同时支持Range和用户自定义分区法,但采用的都是一次分区机制,即对元组仅进行一次划分,并且采用分区与Reducer一一对应的随机指派策略.对于均匀分布的数据集,该分区机制能很好地实现各Reducer接收数据的均衡性,但对于个别值密集的倾斜数据,默认分区方法很难完成对数据的一次性均匀划分,即使采用自定义分区函数,在没有掌握数据分布的情况下,也很难避免数据倾斜.文献[5-6]指出若采用默认的分区方法,数据划分的不均衡将是一个普遍的现象.而一旦发生数据倾斜,势必会造成Reduce端运行的不均衡,从而影响整个作业的运行时间.文献[7]通过大量实验发现采用默认的Hash分区方法,在92%的任务中出现了Reducer运行的不均衡,而这些Reducer的运行时间一般都高于正常任务22%和38%.由此可见,数据分区的均衡性对MapReduce作业的性能有显著影响,如何进行分区策略的制订已成为近年来研究的热点.为解决原MapReduce自身一次分区生成机制带来的弊端,研究者们提出了两阶段分区机制,即首先按照原分区机制生成数据分区,然后对数据量发生倾斜的分区进行一次调整.分区调整策略如文献[8]中的方法:在Mapper运行中加入采样策略,当Mapper运行到特定时刻,根据采样所得到的分布信息,对发生数据量倾斜的分区进行一次拆分,然后在保证数据一致的基础上将拆分后的分区与数据量较少的分区进行合并,从而产生均衡分区.该方法的关键是何时对倾斜分区进行拆分,早拆分势必会增大采样误差,而过晚调整又会延迟数据从Map端到Reduce端的传输时机,对于不同的数据分布,该方法并没有给出一个通用的最优调整时机.文献[9]提出了一种先预处理输入数据再制定分区策略的方法,即在运行用户的作业前,启动一个采样任务负责调整默认的分区策略,然后使用制定的均衡分区策略来运行用户作业.但是,这种方法需要增加一次对于输入数据集的访问,这也就增大了文件访问和数据传输开销,从而造成繁忙集群中的资源浪费现象.由此可见,一次分区生成和一次分区调整机制,在处理分布类型复杂的多类数据集时往往存在一定的局限性.本文提出一种更高效的增量式分区分配策略,该策略通过在Mapper运行过程中,以渐进的方式将分区分配到Reduce端,来解决数据划分的不均衡问题.该方法首先在Mapper运行阶段产生多于Reducer个数的细粒度分区,并在Mapper运行过程中实时统计各分区的数据量,然后由JobTracker根据统计结果筛选出部分细粒度分区向各Reducer进行分配.伴随着Mapper运行,已分配分区的数据量在Reduce端一定会发生变化,并有可能造成部分Reducer接收的数据量发生倾斜.此时,再由Job-Tracker筛选出部分未分配的细粒度分区进行分配,以此调整发生倾斜的Reducer分区.经过对未分配分区的多轮筛选和分配,最终确保在Mapper结束前将所有的细粒度分区都分配到各Reducer节点,从而解决各Reducer接收数据总量均衡的问题.对各Reducer而言,只需在执行Reduce()函数前,将分配给自身的细粒度分区进行合并,并以合并后的分区作为Reduce()函数的输入分区进行处理即可.本文的主要贡献包括:(1)提出增量式分区调整策略,该方法通过在Mapper运行过程中,多轮次地将细粒度分区重组为均衡的Reducer输入分区,来解决分区后数据划分均衡的问题.(2)提出分区的代价评估模型,该模型以各Mapper运行过程中统计的分区分布为输入,实时地对各分区进行负载评估和记录,经过多轮的筛选和分配,完成将所有分区向Reduce端的分配工作.(3)由于将分区分配到Reducer的计算问题属于NP-Hard,本文提出一种复杂度为O(mlogm)的启发式算法,能高效地解决各Reducer接收数据总量均衡的问题,其中m为用户所定义的Reducer个数.(4)针对模拟数据集和真实数据集做了大量实验,对一次分区方法和本文的多轮分区分配方法进Page3行了对比,实验结果表明,多轮分区分配方法能使Hadoop有稳定的性能提升.本文第2节主要介绍MapReduce数据划分均衡的相关工作;第3节首先介绍Hadoop的默认分区策略,然后阐述增量式分区机制的处理流程;第4节抽象增量式分区的计算模型,给出增量式分区分配的代价模型和实现算法;第5节给出增量式分区在Hadoop系统上的实现技术及实现细节;第6节在标准Zipf分布数据集和真实数据集上,与切割分区的Closer方法进行对比实验,验证该方法的有效性.最后是对本文的总结及概述未来的研究方向和问题.2相关工作自MapReduce模型提出后,学者们就开始关注如何才能解决各Reducer节点接收数据总量均衡的问题.在提出MapReduce模型的文献[1]中,Dean等人采用通用的Hash函数对数据进行简单的一次划分,由于该方法实现简单和具有通用性,所以在Apache实现的Hadoop系统上被定义为默认分区方法,但一次Hash划分很难实现在处理密集数据时各分区的均衡性.随后各研究机构开始对原分区机制进行优化,提出多种分区划分方法.华盛顿大学Kwon等人[10]首先以MapReduce运行的两个阶段为划分标准,将Map和Reduce各阶段可能会产生倾斜的原因进行分析,概括出5种倾斜的分类方法.随后为解决划分分区的不均衡问题,提出SkewReduce策略[11],该方法利用用户定义代价模型来评估分区的大小,并当整个任务运行到特定时机后开始制定分区策略.该策略是以延迟数据的传输为代价来解决各Reducer接收数据均衡性的问题.Ramakrishnan等人[12]提出一种基于采样的分区划分方法,该方法通过在Mapper中新增一个采样进程来获取数据分布,当采样完成一定比例后开始对产生的分区进行拆分和重组.与该方法类似的是Gufler等人[8,13]提出的Closer系统,该系统直接将采样函数增加到Mapper中,避免了不必要的通信开销,当整个Map阶段完成到一定比例后,开始对分区进行拆分和重组,从而再开始传输数据.这两种方法都是用原Hash方法生成分区,然后在采样完成一定比例后开始对分区进行一次调整,在调整后如果该分区再次发生倾斜,则无法进一步调整.因此,对于这两种方法,调整时机将直接影响着划分均衡性的效果,过早进行调整将会造成较大的采样误差,从而降低划分的准确性;而过晚调整又会延迟数据的传输时机,从而增加整个作业的运行时间.以上是基于元组的采样,Kolb等人[14-15]提出一种基于数据块(Block)的采样和切分方法,该方法将原来〈key,value〉形式的键值对转为〈blocking_key,entity〉形式,并以块的大小为单位进行评估和拆分,与基于元组采样类似,该方法采用的仍旧是一次拆分,仍没有解决如何定义拆分时机的问题.Racha[9]提出一种先产生均衡分区函数,然后再运行用户任务的两阶段分区方法.该方法将原来的一轮任务拆分为两轮:第1轮任务是通过对输入数据的采样来完成分区函数的制定,主要工作是对输入数据进行25%的随机采样,通过分析采样数据得出输入数据的分布,从而制定出满足均衡性划分的分区函数;第2轮任务是直接应用已产生的均衡分区函数来执行用户所提交的作业.耿玉娇[16]对前人的工作进行了总结,提出了切分分区和组合分区的两种方法,两种方法都是在作业运行前另起一个采样任务,然后根据采样结果对分区进行一次调整.两轮作业方法是以增加获取数据分布信息的代价来实现数据划分的均衡性,但该方法在处理短作业或输入数据经常发生更新的作业时就会存在一定的局限性.周家帅[17]通过分析Mapper产生的中间结果来制定分区策略,该方法首先对Map函数完成后所产生的临时结果采样,然后根据中间结果的分布情况来决定启动多少Reducer是均衡的分区方案,该方法以延迟Reduce的启动为代价来实现负载均衡.也有研究者,如Kwon等人[18]提出一种对任务运行过程的均衡调度方法SkewTune.该方法通过对正在运行的Reducer建立剩余运行代价的估计模型来实现集群中整体节点运行时的均衡性,当有Reducer完成时,对未完成的Reducer进行代价评估,并将该节点上未被处理的数据迁移到已完成任务的节点上,从而在运行过程中使较长任务的数据能被传输到空闲节点上处理,以此实现各节点运行过程中的均衡性.该方法不考虑Reducer运行前数据划分的均衡问题,而是通过控制各Reducer运行过程中的计算均衡来实现均衡效果,该方法与直接制定分区函数来实现数据均衡性的方法相比,需要在Reducer运行过程中额外增加数据的传输代价.现有关于MapReduce负载均衡的研究都是将同一key值的所有元组作为一个整体进行处理,采Page4用的方法都是通过调整分区大小来解决各Reducer运行过程中的均衡问题.而对于出现某一个key的数据量远多于其他所有key值的极端情况,MapReduce编程机制是无法对该key进行拆分处理的.为解决这种极端情况,在实际的应用中用户一般通过控制算法来解决,即通过选择记录的其他属性作为key值进行分区处理,从而避免产生极端key值的情况.3增量式分区策略本节首先介绍Hadoop平台上默认的分区策略,然后阐述增量式分区策略在该平台上的处理流程.3.1Hadoop默认分区策略Hadoop实现的MapReduce流程,主要可分为2个步骤:Map和Reduce.Map阶段:用户提交作业后,Hadoop系统根据Block块的大小将输入数据进行拆分,同时按照Block与Mapper一一对应的关系启动相应个数的Mapper.各Mapper将HDFS上的输入数据以InputSplit为单位的形式加入到本地节点,然后通过Map()函数将各元组以记录为单位转换为〈key,value〉形式的键值对,并将键值对写入Buffer中,当Buffer到达一定阈值时,溢写整个Buffer中的内容到本地磁盘图1增量式分区的MapReduce处理流程并以一个临时文件的形式进行存储,在溢写的同时,将该Buffer中的所有元组以key为关键字按照分区函数进行划分,默认采用Hash函数,划分方法为has(key)mod(R),其中R为Reducer个数.经Hash函数计算后,将具有相同哈希值的key作为一个分区溢写到本地磁盘;当该Mapper完成后,将产生的所有溢写文件按照分区进行合并(Merger),并最终将该Mapper的所有输出存放到本地的一个临时文件中.同时为确保各Reducer能快速索引到自身分区的位置,系统会将该临时文件分为R块,即每一个分区作为一个块,同时为每个分区都建立一个索引,以此实现各Reducer对自身分区的快速定位.Reduce阶段:在原机制中,各Reducer初始化时就已经决定了自身所要处理的对应分区,并且一个Reducer只能处理一个分区.Reducer初始化完成后就开始等待从Mapper端读取数据,当有Mapper完成后,所有已启动的Reducer便同时开始从该节点读取属于自身的分区数据,由于Mapper输出的临时文件中带有各分区的索引,因此各Reducer能快速读取对应于自身的分区数据,在从Mapper端读取数据后,各Reducer会将读取的数据放入到本地磁盘以等待所有Mapper的完成.在所有Mapper完成后,Reducer便可获取各自分区的全部数据,然后对所有读取的文件进行合并(Merger)、排序(Sort)后,就开始执行Reduce()函数,并最终将结果输出Page5到HDFS上,完成整个作业.通过分析Hadoop系统上默认的MapReduce处理流程可以发现,分区的目的是为了使各Reducer能够接收相等数量的数据,Hadoop系统所实现的分区是简单的一次Hash划分,并且严格保证一个分区被一个Reducer处理.3.2增量式分区策略在Hadoop上的处理流程本文的方法在处理key时与原机制相同,同样将一个key作为一个整体进行处理,但对原机制进行两处修改:一是更改Reducer与分区建立关系的时机;二是将分区与Reducer一对一的关系更改为多对一的关系.本小节提出了增量式分区策略,并给出增量式分区策略的一般处理流程.图1为增量式分区处理MapReduce作业的一般流程,从图中的阴影部分可以看出与原机制相比做了两处改动:一是在Mapper中新增了Counter方法,二是更改了数据从Map端向Reduce端的传输策略.新处理机制同样将一个作业的处理分成两个阶段:Map阶段和Reduce阶段.Map阶段:增量式分区策略,新增一个Counter方法,该方法用于描述已处理数据的分布情况.Mapper在将各元组转换为〈key,value〉形式的键值对后,Counter方法统计出自身所产生各key的元组数,作为描述处理数据分布的统计信息记录下来,然后写入Buffer中.这些统计信息将按照一定的时间间隔被发送到Master节点,Master将所有Mapper的统计信息进行汇总,从而获取已处理完数据的全局分布信息.在Buffer溢写时,增量式分区更改产生分区的粒度,即产生粒度更小的分区.接下来,如同原系统,将Buffer中的数据溢写到本地磁盘,等该Mapper完成后,将所有的临时文件按照分区进图2增量式分区分配过程行合并,同时为每个细粒度分区建立索引.索引是用于记录各分区将要被哪个Reducer进行处理的结构,本文称建索引的过程为分区的分配过程.不同于原机制一次分配机制,增量式分区策略以渐进的方法建立索引,并且由Master节点根据全局数据分布信息决定如何将所有细粒度分区分配到Reducer上.Reduce阶段:增量式分区策略在Map端产生了粒度更小且数量大于Reducer个数的细粒度分区,为保证按照用户定义的Reducer数运行任务,增量式分区策略需要将细粒度分区在Reduce端进行合并,即更改分区与Reducer一对一的关系为多对一关系.更改后,增量式分区策略需要保证各Reducer读取多个分区的数据,因此在Reduce端新增了分区的合并操作.与原机制相同,当所有Mapper完成后,各Reducer即可获取自身的全部数据,从而开始执行Reduce()函数,并最终将输出结果写到HDFS上,完成整个作业.4增量式分区的代价模型及算法实现本节首先对增量式分区的分配过程进行概要描述,然后建立分区分配的代价评估模型,最后给出求解该模型的启发式算法.4.1增量式分区分配模型概述本节主要介绍增量式分区分配方法的一般处理模型,图2是对图1中的阴影部分“增量式分区分配过程”的详细描述,也是各Reducer输入分区的生成过程.在Mapper运行过程中生成多于Reducer数目的分区,即通过重定义分区函数为hash(key)mod(λ×R)Page6来实现.其中R为用户提交任务时所定义的Reducer个数,λ为本方法新增的分区放大系数,用于控制产生分区的粒度,取值是大于等于1的正整数,并定义每个分区为Micro-partition(微分区).在λ>1时,Micro-partition与原分区相比所包含key的种类会有所减少,但与原系统相同,相同key值的元组仍旧属于同一个分区.当Mapper整体任务运行到特定时机时,Master节点开始向各Reducer分配分区,在进行第1次分区分配时,见图2的“第1次分配”部分,首先从所有Micro-partition中选出一定数目的分区向Reducer端分配.在第1次分配完成后,经过特定时间间隔,触发第2次分配,同第1次分配过程相同,首先从未分配的Micro-partition中筛选出部分分区,再进行分配.然而由于所有的分配都发生在Mapper运行过程中,导致第1次分配分区的数据量在第2次分配时在各Reducer端会发生变化,此时可能造成部分Reducer的数据量发生倾斜,因此在进行第2次分配时,需要考虑各Reducer已有的负载量,然后按照各节点分配后数据量总量相同的原则进行分配.伴随着Mapper运行,各节点的统计结果会逐渐精确,即在后几轮分配中,各分区的数据量逐渐趋于稳定,此时分配的目标主要是修正在前几轮分配中,可能由于部分统计而出现的数据划分倾斜.在所有Mapper完成前,必须将所有Micro-partition分配到Reducer端,以此避免延迟传输数据.若定义总的分配轮数为s,应该确保第s次的分配时机在最后一个Mapper结束前,如图2“第s次分配”所描述,至此,完成了所有分区的分配.对各Reducer而言,分配到该节点上多个Micro-partition的集合就是将要被该Reducer处理的全部输入数据,本文定义这个集合为Fine-partition(精细分区).综上所述,增量式分区分配发生在Map阶段,在Mapper运行过程中,根据统计结果的逐渐精确性,对细粒度分区进行多轮筛选和分配,从而解决各Reducer接收数据总量均衡的问题.4.2增量式分区分配的代价评估模型影响Task发生运行不均衡的因素可概括为硬件因素和软件因素,为专注解决因MapReduce自身机制所导致的数据划分倾斜问题,本文假定各节点的硬件资源同构.软件因素主要表现在Hadoop系统本身的参数设置和MapReduce固有的处理机制上.由于Hadoop包含190多种参数,参数的不同选取方案都会对作业的运行产生影响[19],设定本文中所有实验只采用一套参数方案.基于以上假定,可以明确运行时间上的倾斜是由分区机制造成的数据划分不均衡引起的.本文令用户作业中定义了m个Reducer,作业在l个Task节点的集群上运行,增量式分区的两个控制变量为:放大系数λ和分配轮数ι,这两个参数为预留参数,用户可根据作业需求自行设定.评估作业是否发生数据倾斜主要是通过判断各节点接收的数据量是否相等,而数据量的大小又可用元组个数的多少来表示,由于输入数据是以元组为单元进行处理,因此本文用元组个数作为衡量数据量大小的唯一标准,记为ki,其中i∈[1,n],即表示一个Mapper上,第i个Micro-partition中包含的元组总数.增量式分区分配的整个过程就是在Mapper运行中,将产生的n=λ×m个Micro-partition经过ι轮分配到各Reducer节点,最终解决各Reduce处理数据总量均衡的问题.本文将增量式分区分配的整个过程分解为ι个相关的子分配过程,该子过程可描述为:首先,在当前分配轮上,从所有未分配的Micro-partition中选取一定数目的Micro-partition;然后,在前轮分配的基础上,将本轮所筛选的Micro-partition分配到Reducer端,对于第1轮只需直接进行分配,同时,满足在本轮分配后各Reducer接收数据总量的均衡性.求解增量式分区分配的整个过程就是迭代的求解这ι个子过程,而每个子过程又需要依次求解两个子问题:(1)在当前分配轮,筛选出哪些分区进行分配;(2)如何将选中的分区分配到各Reducer节点.接下来,给出求解这两个问题的详细步骤.4.2.1筛选分区增量式分区分配的整个过程发生在Mapper的运行过程中,并且某个分区一旦被分配后将无法更改执行节点,因此分区的筛选原则,将直接影响分配后各Reducer接收数据总量误差的大小.本文将筛选分区过程分解为3个子问题:(1)何时筛选分区?(2)每轮筛选几个分区?(3)筛选哪几个分区?何时筛选分区就是确定每轮的分配时机问题,分区分配过早会导致Map端统计分区信息的不准性较高,分配过晚又会延迟Shuffle时间,因此分配时机对多轮分配策略有重要的影响.Hadoop原系统所实现的MapReduce机制是在第1个Mapper完成后,才开始从Map端向Reduce端传输数据,因此将第1次分配的时间点定义为第1个Mapper的Page7完成时刻,以此达到最早的数据传输时机.传输最后一个Mapper数据必然是与Map函数串行进行的,若在最后一个完成前所有已完成Mapper的数据都已经Shuffle完成,则定义此时为最后一次的分配时机即可,若还有数据没有完成Shuffle,并且最后一个Mapper处理的数据量少,再定义此时为最后一次分配时机就会造成已完成Mapper的分区一直保存在本地磁盘,导致该方法延长了Shuffle时间.为此定义当所有Mapper完成到80%时为最后一次的分配时机,以此能最大限度地使Shuffle与Mapper并行执行,同时又能提高统计的准确性.从而得出总的分配时间段为:最后一次分配时整个Mapper已完成的比例减去第1次分配时整个Mapper完成的比例,为计算方便本文取两次Mapper完成比例的整数部分,用两个整数相减得到总分配时间.同时本文采用时间等分原则,将总分配时间分为ι段,即在分配轮数ι内将所有的Micro-partition分配完成,因此用总分配时间除以ι即可得到相邻两轮分配的间隔时间,由于第1次分配时间定义为第1个Mapper完成时的时间,从而可计算出每轮的分配时间点.在确定出每轮的分配时间点后,接着需要确定每轮分区的分配个数问题,本文采用每轮分配Micro-partition个数相等的原则进行求解.记每轮要分配的分区个数为Na,在给定的分配轮数ι下,用Micro-partition的总个数n=λ×m除以分配轮数ι即可得到每轮要分配的Micro-partition个数.最后,需要从所有未分配分区中筛取出Na个Micro-partition作为本轮的候选分区集合,记为Cand.为此本文定义两个筛选原则:(1)优先筛选数据量大的Micro-partition;(2)优先筛选变化率大的Micro-partition.原则1限定了让数据量大的分区尽早Shuffle,这样可以将数据量相对少的分区稍后传输,从而减少最后一个Mapper完成后传输数据的等待时间.原则2中的变化率用于反映连续两次分配间各分区数据量的变化程度,即用当前分配轮该Micro-partition的统计量减去上轮分配时的统计量再除以上轮分配时该Micro-partition的统计量得到.原则2可以逐步修正因提前分配和部分统计而带来的分配误差.随着分配轮数的递增,各分区的统计结果更加精确,从而可以更精确地修正前面因分配过早造成的分配误差.为实现对各Micro-partition数据量和变化率的记录,本文定义了两个向量,分别为统计向量和分配向量.令α=(k1,k2,…,kn)为一个Mapper上所有Micro-partition的统计向量,即用于描述该Mapper上各分区的数据分布,元素ki表示该Mapper上第i(i∈[1,n])个Micro-partition中包含的元组总数.一个Task上可能运行有多个Mapper,而单个Mapper无法和Master节点进行直接通信,必须借助运行节点上Task的HeartBeat,因此本文定义αs=(k1s,k2s,…,kns)为第s(s∈[1,l])个Task上所有Mapper的α累加和,kis(i∈[1,n])为第i个Micro-partition在第s个Task上数据量,以此统计出该Task上的数据分布.对于Master节点,只需将所有Task的αs累积即可得到全局的数据分布,用n维向量犃表示,记犃=∑分配方式,需要记录下每轮分配时的数据分布情况,因此,记犃t=∑全局Micro-partition的数据分布,αst为在第t轮分配时,第s个Task上所有Micro-partition的数据分布.定义一个m维分配向量,用于标记各Micro-partition是否已被分配到某个Reducer上,记为犃犚=(P1,P2,…,Pm),其中Pr=(x1r,x2r,…,xnr)(r∈[1,m]).分配向量是一个以Micro-partition为行,Reducer为列的二维向量,元素xir∈{0,1}(i∈[1,n]),{1}表示分区值为i的Micro-partition已被分配第r个Reducer上,{0}代表该分区未被分配到第r个Reducer上.由此可见,分配向量中各元素的初始值为0,每轮都会将Na个xir赋值为{1},并且对同一个xi,即同一个Micro-partition,使其满mxir=1,即一个Micro-partition只能被分配到足∑一个Reducer节点上.最终经过ι轮的分配,使得ir=1m满足∑Reducer上.r=1通过定义这两个向量可以实时记录各Micro-partition在Map运行过程中产生数据量的大小和各Micro-partition是否已经被分配到某个Reducer上.接下来,依据筛选原则1和原则2进行分区的筛选.筛选的过程就是计算犃t中所有∑[1,n])到本轮分配t(t>1)为止,在Map端产生的数据量,并从中选取数据量最大的前2Na个Micro-partition,然后比较这2Na个Micro-partition在tPage8轮和t-1轮的变化率,依据这两条规则,最终从数据量最大的前2Na个Micro-partition中选取变化率最大的前Na个Micro-partition作为此轮将要被分配的分区集合Cand.4.2.2分配分区在确定出每轮将要分配的分区后,接着需要将本轮选中的含有Na个Micro-partition元素的集合Cand分配到m个Reducer上.由于第t(t>1)轮的分区分配是建立在第t-1轮基础之上,因此在第t轮分配分区时,不单要考虑本轮Na个分区分配后各节点的负载量,还需要考虑各Reducer节点在第t轮时已承载的负载量,从而解决在t轮分配后,各Reducer接收数据的总量仍是均衡的.本文将每轮分配分区的问题定义为如下形式.设在l个Task节点的集群上,运行着m个Reducer,第t(t∈[1,ι])轮要分配Micro-partition的集合为Cand,令到第t轮为止,在第s(s∈[1,l])个Task上产生的第i(i∈[1,n])个Micro-partitionPi的数据量为ksi,第r(r∈[1,m])个ReducerRr上已承载的负载量为Lr,则分配分区就是实现以下目标:该式满足以下条件:(1)i∈Cand(2)xir=(3)i,满足∑其中Lr为常数,通过计算在t轮的统计向量犃t得到.目标函数是在每轮分配时约束最大的负载量节点,使其增加的负载量最小为原则来实现各Reducer负载量的均衡.因为对于一轮Reduce任务,运行时间最长的那个Reducer决定了整个Reduce阶段的结束时间,因此只需减少最长任务的运行时间即可减少整个任务的运行时间,从而解决分区均衡的问题.同时,约束条件(1)用于控制每轮只分配筛选中的分区,约束条件(2)用于标示变量xir被分配到哪个Reducer上,约束条件(3)使得所有属于Cand的分区在本轮被分配完.通过该目标函数和约束条件,最终确定出每轮Na个Micro-partition与Reducer的一一对应关系,即完成本轮的分区分配任务.最终,经过ι轮的迭代求解,实现分区到Reduce端的增量式分配过程.4.3代价模型的算法设计求解增量式分区分配策略的代价模型,需要两步完成:第1步筛选分区,第2步将选中的分区分配到Reduce端.第1步要解决的问题是排序问题,此步仅需要根据数据量的大小对未分配分区进行排序,然后取出变化率最大的前Na个分区.第2步是最优化问题,本文给出求解该问题的一种启发式算法.筛选分区算法如算法1所示.算法1.筛选分区算法.输入:未分配分区集合犃犚输出:本轮要分配的分区集合Cand1.SortAt//按数据量大小递减进行堆排序2.Pcand←Max2Na{Sort}//取前2Na个分区3.FORPcand//求最大前2Na个分区相邻2次的变化率4.SPcand_i=(kirt-kir(t-1))/kir(t-1)5.ENDFOR6.Cand←MaxNa{SPcand}//取出变化率最大的前Na个算法1的复杂度受两种操作的影响:第1行是按数据总量对所有未分配的分区排序和第3行计算未分配分区的变化率;本文采用堆排序算法,在第1次排序时n取得最大值,每轮分配后以n/ι的数量递减,算法的平均计算杂度为O(nlogn);对于计算未分配分区的变化率,只需要在每轮分配中遍历一遍所有未分配分区,操作步骤为n/ι∈Θ(logn),因此算法1的时间复杂度为O(nlogn).算法1的时间复杂度与未分配分区的个数和排序算法有关,在作业运行的开始阶段,JobTracker只需要花费很小的代价进行Task的调度,而由于未分配分区数目较多,因此筛选算法可能要增加一定的负载量.伴随作业的运行,Task的调度代价会逐渐增大,表现为Map/ReduceTask的启动和停止,而未分配分区的数目会逐渐减少,从而使得筛选算法的代价逐渐减小.根据本文对分配分区问题的定义,可将该每轮的分配分区问题等价于Min-max问题[20],又由于Min-max是NP-Hard[21],因此增量式分区分配问题也是NP-Hard.为能精确和快速地确定出一套近似最优的分配方案,本文提出一种改进的贪心算法,见算法2.算法2.分配分区算法.输入:Cand,LRt输出:犃犚1.CandPs=Sort{Cand}//按数据量递减排序2.WHILECandPs//贪心策略指派CandPs到ReducerPage93.MaxLoad=max{LRt}//取出负载最大的Reducer4.MinLoad=min{LRt}//取出负载最小的Reducer5.MinLoad=MinLoad+CandPs1//取出数据量最大6.UPDATEMaxLoad和MinLoad//记录分配后的7.CandPs=CandPs-CandPs18.ENDWHILE9.WHILE有分区交换操作‖第1次执行10.UPDATEMaxLoad和MinLoad//记录分配或11.MaxLoadValue=Add{MaxLoad}//计算最大12.MinLoadValue=Add{MinLoad}//计算最小13.SortMax=Sort{MaxLoad}//对最大负载节点的14.SortMin=Sort{MinLoad}//对最小负载节点的15.FORiTOMaxLoad//遍历最大负载节点,并取出16.IFSwitchMaxV+SortMax{i}<=17.SwitchMax+=SortMax{i}18.ELSEBREAK19.ENDIF20.ENDFOR21.FORjTOMinLoad//遍历最小负载节点,并取//出负载量之和小于等于该节点总负载量一半的分区22.IFSwitchMinV+SortMin{j}<=MinLoadValue/223.SwitchMin+=SortMin{j}24.ELSEBREAK25.ENDIF26.ENDFOR27.IF((MaxLoadValue-SwitchMaxV+28.MaxLoad=MaxLoad-SwitchMax+SwitchMin29.MinLoad=MinLoad-SwitchMin+SwitchMax30.ELSE没有分区交换31.ENDIF32.ENDWHILE33.更新犃犚算法2由两部分组成,第1~8行是按贪心算法将分区指派到各Reducer节点上,第9~32行用于交换最大负载节点和最小负载节点的分区,以此达到尽量减小最大负载节点负载量的目的.第1部分的执行效率受两种因素的影响,分别是第1行的排序算法和第6行确定最大和最小Reducer的负载节点.本文第1行的排序算法采用的是堆排序,该行的平均时间复杂度为O(mlogm),m为Reducer个数.第6行执行受第2行WHILE循环的影响,且每分配一个分区都需要求出分配后的最大和最小负载节点,这部分总的执行步骤为其中:ι为分配轮数;λ是分区的放大倍数,都是一个常数;m为Reducer个数;n为Micro-partition的总个数.第2部分的执行效率受第9行、第15行和第21行的影响,而这两行又是等价的并且是最基本的循环,以14行为例,该行最坏情况下的值为Na-2,即集群中只有两个Reducer,其中一个Reducer节点上只包含有一个Micro-partition,其余全部Micro-partition在另一个节点,则该节点最坏情况下需要进行Na-1循环才能取出该节点负载总量一半的分区.对于第9行的外循环,在最坏情况下,需要执行的步骤为log(mNa),即将所有分区进行一次对换.因此得出该部分总的执行步骤为T2(m)=(logmNa)×(Na-2+Na-2)将T1(m)和T2(m)相加得到算法2的复杂度为T(m)=T1(m)+T2(m)=综上可知该算法的时间复杂度为O(mlogm),受用户所定义Reducer个数m的影响.引理1.增量式分区分配算法的近似比为2.证明.设集群中有m个Reducer,最大负载Reducer节点上的负载量记为Lmax,最小负载Reducer节点上的负载量记为Lmin.令在最优解中Lmax的值为Lopt,本文算法2得到Lmax的值为L(m).假设2Lopt<L(m),则有L(m)=2Lopt+δ,δ为常数.那么1节点上负载量为Lopt的分区对换到Lmin节点上,得到两个节点的最新负载量为L(m)=Lopt+δ+12Lmin和Lmin=Page10与最小负载量的差为|L(m)-Lmin|=δ.在对换分区前,最大负载量与最小负载量的差为L(m)-Lmin=2Lopt-Lmin+δ>δ,所以算法2继续执行分区对换操作,因此L(m)不是最终值,即L(m)≠Lmax,而这又与已知条件矛盾,所以假设错误.从而得到2LoptL(m),即增量式分区分配算法的近似比为2.本文算法2的第2~8行部分等价于贪心算法,在此基础上,本文算法又增加了分区调整部分,导致时间复杂度为O(mlogm).虽然本文算法的时间复杂度要高于一般贪心算法O(m),但本文的算法具有较高的近似比,而对于实际的集群环境,比如Reducer数目很有限的情况下,分配分区算法的运行时间只占整个任务运行时间的很小比例,而数据划分的均衡性对整个任务的运行时间有更大影响.算法2的高效性将在下文的6.4节,通过真实环境和真实数据集上对比实验进行验证.图3增量式分区在Hadoop系统上的实现在系统实现上,本方法将Counter模块添加到各Mapper的运行线程中,并将统计结果放到Local5增量策略在Hadoop上的实现本节介绍如何在标准Hadoop-1.1.2系统上实现增量式分区策略,并给出系统修改方法和实现的相关技术细节.Hadoop系统所实现的MapReduce架构并不支持增量式分区模型,为实现该功能,本文在原架构上新增了3个功能模块和4个数据构,如图3中的虚线框所示.新增的3个功能模块分别为Counter、DecisionModel和AddNewPartition,分别对应于统计结果的收集、分配Micro-partition和多Micro-partition重组Fine-partition功能,并将3个模块分别在原系统的MapTask、JobTracker和ReduceTask类中实现.同时新增两类结构体:CounterTable和AssignPlan,分别用于实现统计向量犃和分配向量犃犚.CounterTable中.由于LocalCounterTable的大小由Micro-partition个数决定,并且该值在同一个任Page11务中是固定不变的,因此本文以一维数组的形式进行存储,并将该表驻留在内存中.为减少Task与Job-Tracker之间的通信开销,将LocalCounterTable加入到Heartbeat中.当一个Mapper运行完成后,会将产生的临时结果写入本地磁盘,并将各Micro-partition的索引位置记录在MapOutputFile中,以便各Reducer读取,这部分仍采用原结构,不同之处是MapOutputFile将包含更多的索引信息.将Micro-partition的筛选和分配添加到Job-Tracker类中,在该类新增了DecisionModel模块,主要用于实现4.3节中的增量式分区分配算法,GlobalCounterTable用于对所有Task的LocalCounterTable进行汇总,并将分配计划添加到GlobalAssignPlan中,在完成每次决策后需要将该表的更新增加到下次的Heartbeat通信中,从而可以将分配计划实时地传到Reducer节点上.ReduceTask通过对Heartbeat的解析,获取属于自身的分区信息,对于原MapReduce架构,各Reducer只需将自身的分区信息添加到MapOut-putLocation中,并开始等待Mapper的完成.当有Mapper完成时,ReadPartition模块依据分区索引通过Http协议,开始从已完成的Mapper中读取数据.而对于增量式分区,由于Reducer的输入来源于多个分区,因此,增加AddNewPartition模块,渐进地将分区信息增加到LocalPartition中,然后将LocalPartition中的分区信息转换为分区的存储路径并存放到MapOutputLocation中,该模块既可以实现对Reducer初始化时多分区分配,又可以完成在Reducer读取数据过程中的增量式分配,从而实现对Reducer的渐进式分配方法.与原系统相同,等所有输入完成后,开始执行Reduce()函数,并将结果写入HDFS中,完成整个作业.6实验结果与分析本节通过两部分实验来验证增量式分区策略的有效性,第1部分实验重点讨论2个增量参数对该方法的影响,第2部分通过标准Zipf分布数据集和真实数据集,与原系统和Closer[8]方法做对比实验,检验3个系统对数据均衡的处理能力.6.1集群环境本文实验的环境是11个节点的集群,其中1个Master节点负责任务的调度和集群管理,不进行数据计算;10个数据节点,用于数据的存储和计算.每个节点的系统配置为16核主频为2.20GHz的AMD处理器,16GB的RAM和500GB的硬盘,节点之间通过1Gbit的网络连接,各节点用的是64位UbuntuLinux12.01,使用的对比系统是Hadoop1.1.2,所修改的系统也是Hadoop1.1.2.修改Hadoop系统的默认配置,定义每个节点有16个Mapslot与处理器核数相等,使Reduceslot个数等于Reducer个数,分区函数使用默认的Hash分区函数,其他参数也都使用默认值.为使实验结果具有说服力,同一组实验运行10次后取平均值记为最终结果.6.2实验数据实验数据采用真实数据集和合成数据集,两类数据集的详细描述如表1所示.合成数据包含11个子数据集,每个子数据集满足指数为γ的标准Zipf分布,γ取以0.1为增量从0.0~1.0的小数,γ的值越大,数据的分布越倾斜.各子集为单列整数数值数据,数值的取值范围为[1,1000],并包含10亿条记录.为满足实验用例特点,对真实数据集做去杂处理,由于杂质记录很少,此操作并未改变原数据集分布特点.数据集运行实例大小/GB元组数/亿Zipf-γWordCount0.4BTS①WordCount60.0UK02②PageRank6.3增量策略参数评估实验除Hadoop系统本身参数设置外,用于控制增量式分区执行效率的2个参数为分配轮数ι和放大系数λ.本部分实验,重点用来验证这2个增量参数是如何影响系统运行效率的,对运行效率的评估本部分实验使用运行时间和标准方差表示.运行时间用于衡量一个作业运行的快慢,重点体现最长Reducer运行之间的差异,而标准方差用于衡量各Reducer之间数据分布的均衡效果,标准方差的计算首先是获取各Reducer接收到元组的总量(通过输出的日志直接获取),然后以总量为元素计算标准方差,标准方差越大说明分区越不均衡.通过这两个指标既可以反映Reducer的运行时间又可以反映出各Reducer之间接收数据总量的差异性.为分析各①②Page12参数独立影响的大小,在对其中一项进行分析时,令其他所有参数为常量.这部分实验所使用的数据是标准Zipf分布的数据集,运行的是经典WordCount实例,该算法在Reducer端只进行求和操作,因此影响该算法运行时间的因素主要体现在Map阶段.(1)分配轮数ι.分配轮数决定着将分区分几次指派到Reducer上,使用的数据集为Zipf-0.7,统计结果的收集间隔为1E10条记录(一个Heartbeat通信时间内Mapper处理的元组数),评测标准为运行时间和各Reducer节点接收元组总数的标准方差.实验结果如图4所示.图4(a)是分配次数与运行时间关系的实验结果图,横坐标是一次作业运行过程中的分配轮数,纵坐标是整个作业的运行时间.从整个作业的趋势来看,伴随着分配轮数的增加运行时间逐渐递减,ι在图4分配轮数影响实验(2)放大系数λ.放大系数决定着产生Micro-partition的粒度,粒度越小所产生的分区越细,合并后产生Fine-partition的分区越均衡.λ能取到的最大有意义值是使得Micro-partition的个数等于输入key值的个数,即将元组值相同的所有记录作为一个微分区,但这样做又会增大Reducer读取数据时的负载和JobTracker分配任务的负载.因此通过这部分实验来验证,对于不同的数据分布,放大系数是如何对运行时间和均衡性产生影响的.本文使用两个不同分布的数据集Zipf-0.3和Zipf-0.7,统计结果的收集间隔为1E10条记录(一个Heartbeat通信时间内Mapper处理的元组数),分配次数选定为10.实验结果如图5所示.从图5(b)可以发现,对数据集分布相对均衡的取值为10附近时达到稳定值.图4(b)描述的是以标准方差为性能的实验图,该图中曲线的趋势与运行时间的趋势相同,当分配次数到达8时,标准方差趋于稳定值,再增加分配轮数对该值的变化不会产生什么影响.通过实验结果可以发现,分配次数越高产生的均衡效果越好,这是因为伴随着分配次数的增加,可以及时修正因过早分配产生的误差,分配次数越多,修正的效果越好.但由于数据本身的分布特点和产生Micro-partition的粒度,会导致分配次数有一个最优下限,即必须保证在特定放大倍数下且在所有Mapper结束前将所有的分区分配完成.在系统实现上,本文将该值设定为预留值,用户可根据所要处理的数据特征进行自定义,本文定义该值的默认值为10,在分配轮数为10时,该方法基本可以实现大多数分布数据集的数据均衡性.Zipf-0.3,放大系数为4时就能实现各Reducer接收数据总量的均衡性,随着放大系数的增大,即分区粒度的减少,对均衡性产生的影响越小.而对于数据分布相对倾斜的Zipf-0.7,可以明显发现随着放大倍数的增大,数据划分的均衡效果越好,在系数为10X时,均衡性开始趋于稳定,而当放大系数为14X时,产生的均衡效果甚至比Zipf-0.3还好.此实验验证了通过增大放大系数来减小分区粒度能更好地实现数据划分的均衡性,并且对于不同的数据分布都有一个最小的系数使均衡性趋于某个稳定值.对于本组实验,影响其运行时间的主要因素是Shuffle阶段的数据传输,从图5(a)可以发现,随着放大系数增大,运行时间逐渐减少,是因为分区的粒度逐渐变小,数据能更早地以小粒度传输.如同分配轮数λ,该值也是预留值,用户可根据处理数据的特征自定Page13图5放大系数影响实验义取值.为保证每轮分配时,各Reducer都可接收数据,本文同样定义放大系数的默认值为10,即等于分配轮数.6.4对比实验本小节将通过经典WordCount和PageRank算法对比3个系统Hadoop、Closer[8]和本文的方法IPS(IncrementalPartitioningScheduling)对数据均衡的处理能力.Hadoop使用默认的Hash分区和随机分配原则;Closer采用的是一次切割分区的方式,即当某分区超过一定阀值时将该分区分解,并将超过部分的数据传送到其他数据量较小的分区上,以此解决各Reducer接收数据均衡的问题;在所有的对比实验中,定义IPS增量参数ι=10和λ=10,为验证3种方法的效果,在运行同一组实验时让各系统都取相同的参数.(1)WordCount.由于WordCount的操作在Reducer端只进行加法操作,因此运行时间不能真实反映出Reducer端数据均衡性的效果,本部分实验选取的评测指标是各Reducer要处理数据总量的大小,本文选取最大负载节点上的总元组数和标准方差.最大负载节点的总元组数是对目标函数的评测,用于反映处理数据量最多Reducer的运行快慢,而标准方差反映了各Reducer之间数据差异性.首先在生成的11个标准Zipf数据集上对比三系统对数据划分均衡的处理能力.如图6中横坐标代表指数从0.0~1.0的11个标准Zipf分布数据集,图6(a)纵坐标是最大负载节点上的总元组数,图6(b)纵坐标为各Reducer接收数据量的标准方差,图6(c)纵坐标为整个任务的运行时间.从图6(a)可以发现在Zipf-0.0~Zipf-0.3分布之间,Closer和IPS最大负载节点上的数据量是相等的,而且都要小于原系统,从图6(b)标准方差可以发现Closer和IPS的方差趋于0,说明各Reducer所接收的数据量相近,即都趋于最大值,但Hadoop系统的方差要明显大于两者,说明采用默认哈希法在数据分布均匀的情况下,也难能保证各Reducer接收数据的均衡性,图6(c)的运行时间同样验证了这一点.Closer系统从Zipf-0.4~Zipf-1.0,最大负载节点的数据量和标准方差两个指标都表现出递增的趋势,说明随着数据分布倾斜的增大,Closer系统对数据均衡的处理能力逐渐减弱,而从图6(b)可以看出,在Zipf-0.0~Zipf-0.7之间,IPS始终保持两个指标的稳定,说明IPS并不会随着数据分布倾斜的增加而出现性能上的变化,由于运行的实例是WordCount,导致Reduce()函数的执行时间会很短,因此在Zipf-0.0~Zipf-0.7之间,图6(c)的运行时间两者差别不大.对于Zipf-0.9~Zipf-1.0两个数据集,该方法发生了更大的倾斜,这是因为在分布指数为0.9和1.0的标准Zipf分布中,存在一个元组,它的数据量要远大于其他所有元组的数据量之和,造成一个节点的数据量过于庞大,其他节点如何组合相加都要远小于该元组,因此出现划分数据的倾斜问题,对于使用分区切分的Closer系统同样不能对其进行处理,原因是发生倾斜的是不能再次进行拆分的元组,而不是由多个元组组成的分区.但从运行效果来看,即使这种情况,IPS仍能保证集群的均衡效果优于其他两者.图6(a)和图6(c)具有相同的变化趋势,说明最大节点上的处理量和运行时间是正相关的关系,对于数据分布非常倾斜的情况,Page14图6Zipf数据集WordCount的对比实验IPS仍能尽量减少最大节点上的负载量以此来提高整个任务的运行时间.通过Zipf数据集上的实验可以发现IPS方法在取默认值的情况下,就能够处理不同分布的数据集.接下来通过真实数据集来验证3个系统的差异,图7(a)是真实数据集的数据分布图,从该图可以发现,近50%的数据分布在0附近,剩余50%的数据在跨度为超过10E6的值域内.图7(b)、(c)、(d)是3个系统的均衡性能对比实验,横坐标是运行任务的Reducer个数,纵坐标分别为最大节点上的数据量、各节点数据量的标准方差和整个任务的运行时间.图7(b)是3个系统最大负载节点上数据量的对比实验,从实验结果可以看出IPS最大负载节点上的数据量都要好于其他两个系统,随着Reducer数目的增多,各节点接收的数据量会逐渐减少.从图7(c)上可以看出,对于数据分布变化较大的数据集,Closer系统所采用的一次分区调整方法在Reducer为20时,仍出现了较大的数据倾斜误,这是由于分配前期数据的统计量过少,而在运行中出现了最新处理的数据都集中在了某几个分区上,从而导致再次发生倾斜,而IPS仍能用未分配分区对Reducer的输入分区进行调整,所以能更好地实现均衡性,而这种数据的分布差异又导致了图7(d)运行时间的差异,可以发现在Reducer为20时,IPS的运行时间要明显优于两者.在Reducer数目为50时,IPS的均衡性最差,说明此时所采用的增量参数没有使系统产生最好的均衡效果,但均衡性都要明显优于其他两者.并且从图7(c)可以发现,随着Reducer数目的增多,在最大负载节点的数据量保持不变的前提下,标准方差在逐渐增大,出现这种现象是由数据分布特征所引起的.也就是说IPS能平稳控制最大负载节点上的数据量,而造成方差变大的原因是由于各分区的粒度随着Reducer数目的增多而变小,造成彼此间的差异逐渐增大,但IPS能始终控制最大负载节点保持不变,因此并不会对任务的总运行产生影响,这点从图7(b)和(d)可以看出,IPS能Page15图7BTS数据集WordCount对比实验在Reducer数目变动的情况下,通过很好地减少最大节点上的数据量来控制整个任务的运行时间.(2)PageRank.数据的划分是否均衡最终可反映到运行时间上,由于WordCount在Reduce()函数中只进行简单的加法操作,运行时间效果的差异性不能明显地反映出来,因此本节使用Reduce()函数计算相对复杂的PageRank算法,由于PageRank每轮的数据分布都与第1轮的分布相同,因此只需要比较第1轮的运行时间.图8是在真实数据集UK-2002上运行PageRank一次迭代的结果.为分析因数据分布造成对Reducer运行时间的影响,图中将1个作业的运行划分为3个独立阶段,Mapper运行阶段、Shuffle运行阶段和Reducer运行阶段,其中Shuffle运行阶段定义为没有与Mapper运行发生重叠的那部分时间.对比实验分为两组,分别取Reducer个数为20和50,用于验证在不同环境下3个系统的性能表现.首先对比3个系统在Mapper阶段的运行时间,可以发现运行时间相差不到2s,说明加入统计方法后,并没有额外增加Map阶段的运行时间.在Reduce阶段,可以发现IPS始终是3个系统中最优的,IPS与原系统相比有近50%的提升,Closer系统的拆分方法介于两者之间.通过PageRank算法可以说明,以元组个数为单位的分区划分方法不但能实现数量上的均衡性,同时也能优化计算上的均衡.通过3组对比实验可以发现,Closer所采用的Page16一次分区调整策略也难避免在对分区切分后不再发生数据量倾斜的问题,而本文所采用的多轮分区分配方法可以更好地解决数据划分不均衡的问题.7总结与展望本文提出一种增量式分区策略来实现各Reducer接收数据均衡的问题,将原MapReduce的一次分区分配机制,更改为一次生成和多轮分配的递增机制,并通过定义分区代价模型实现每轮分配后各Reducer接收数据总量的均衡性.最终通过标准Zipf分布数据集和真实数据集,验证了该方法的高效性和稳定性.此外,本文重点解决分区后数据划分均衡性问题,对Reducer在运行过程中所出现的计算倾斜并未做研究,作者已开始进行这方面的研究工作,进一步工作是对MapReduce的负载均衡做全面研究.
