Page1面向大样本数据的核化极速神经网络邓万宇1)郑庆华2)陈琳1)1)(西安邮电大学计算机学院西安710121)2)(西安交通大学电子与信息工程学院西安710049)摘要核化极速神经网络KELM(KernelExtremeLearningMachine)将ELM(ExtremeLearningMachine)推广到核方法框架下,取得了更好的稳定性和泛化性.但KELM的训练时间O(n2m+n3+ns)≈O(n3),以样本数n的3次幂急剧膨胀(n为样本数,m为特征维度,s为输出节点个数),不适合处理大样本数据(n20000为基准).为此作者提出一种KELM加速计算框架,并在该框架下结合Nystrm近似低秩分解实现一种快速算法NKELM(NystrmKernelExtremeLearningMachine).NKELM的训练时间O(nmL+mL2+L3+nLs)≈O(n),只是n的一次幂(L为隐含层节点数,通常Ln),远远低于KELM的训练时间,适合处理大样本数据.实验表明,NKELM在大样本数据上具有极快的学习速度,同时产生良好的泛化性能.关键词极速神经网络;随机采样;低秩分解;核方法1引言FeedforwardNeuralNetwork,SLFN)的传统学习方法存在多次迭代、学习效率低下[1]等问题,难以适应大样本数据(20000).为此,Huang等人[2]提出单隐藏层前馈神经网络(SingleHidden-Layer了无需迭代的极速神经网络(ExtremeLearningPage2Machine,ELM).ELM理论认为SLFN模型的逼近能力只与隐含层节点数L有关,而与输入权值无关,因此只要给定足够多的隐含层节点(且L最多为样本数n),即使在输入权随机赋值的情况下,神经网络依然可以零误差逼近任何非线性函数.ELM简单高效,整个学习过程只需3步[2]:(1)根据实际问题设定隐含层节点数L和激励函数h(狓);(2)为输入权随机赋值,并计算隐含层输出矩阵;(3)最后,将输出权求解转化为线性问题解决.ELM只需学习一次,具有极快的学习速度;另外,ELM可获得全局最优解,克服了传统迭代方法局部极值问题;再有,ELM中激励函数无需可微,只需有界分段连续(boundednonlinearpiecewisecontinuous),因此除常用的S型函数(Sigmoid)、径向基函数(RBF)、三角函数(Trigonometric)、模糊函数(Fuzzy)、复数函数(Complex)、高阶(High-order)函数、多项式(Polynomial)函数、小波(Wavelet)等[3]激励函数外,连不可微的硬限幅函数(Hardlim)[4]也可用于ELM.ELM自提出以来获得了广泛关注,在理论与应用方面取得了很大的进展.文献[5-8]等着重提升ELM的泛化能力;文献[6,9-11]主要研究ELM的增量、在线学习问题;文献[10]提出了ELM隐含层节点的自动确定方法;文献[12]提出了ELM集成学习方法;文献[8,13]研究ELM随机选择输入权所带来的噪音干扰与不稳定问题.在应用方面,ELM已被广泛应用于安全评估[14]、数据隐私[15]、脑电图和癫痫检测[16]、图像质量评估[17]、FPGA[18]、人脸识别[19]、人体行为识别[20]等众多领域.上述研究虽然使ELM在激励函数限制条件、学习效率、局部极值等方面取得很多进展,但并未将ELM扩展至核框架(Kernel-basedframework)下,KELM[21]实现了ELM在核框架下的扩展,KELM除拥有支持向量机(SupportVectorMachine,SVM)的很多优点外,它可在立方体(Cubespace)中寻优,比SVM的超平面寻优具有更大的搜索空间,这使KELM从理论上能够获得更好的解,文献[21]中的实验也表明了这一点.与ELM相比,KELM表现出更好的稳定性和泛化性[21],但KELM的训练时间O(n2m+n3+ns)和空间O(n2)代价很高,即使在硬件与计算能力快速提升的今天,有时也难以适应这么快的膨胀速率.针对该问题,本文将为KELM提出一种适合大样本数据的加速计算框架,并在该框架下实现一种快速算法NKELM,解决KELM在处理大样本数据时空间与时间代价过高的问题.本文第1节对论文研究背景进行简介;第2节简要回顾KELM;第3节阐述提出的快速计算框架;第4节在所提框架下实现一种快速算法NKELM;第5节揭示NKELM与KELM的关系;第6节对NKELM的误差边界进行分析;第7节进行实验与性能比较;最后,在第8节对下一步工作进行总结与展望.2核化极速神经网络KELM给定训练样本={犡={狓i|狓i∈犚m}nKELM是基于正则化理论[22-23]与核方法[24]对ELM的扩展,与本文作者Deng等人[5]提出的正则极速神经网络RELM(RegularizedExtremeLearningMachine)有着紧密的联系,二者可看作ELM的两种不同正则化方式.2.1ELM{狋i|狋i∈犚s}nh(狓),根据ELM理论[2],输入权狑=[狑1,…,狑L]、偏置犫=[b1,…,bL]可以随机赋值,而输出权则可通过最小二乘方法获得:其中犡为输入矩阵,犜为输出矩阵,m为特征维度,n为样本数,s为输出节点数,犎为隐含层输出矩阵:犎=基于矩阵计算理论中矩阵逆的性质[25]可知,当犎列满秩时,式(1)可转化为而当犎行满秩时,式(1)可转化为2.2RELM化参数C以使模型更稳定,即RELM根据正则化理论[22]在式(3)中加入正则因犎T犎∈犚L×L,上式矩阵求逆运算的时间O(L3)仅与隐含层节点数L有关,与样本数目n无关,而通常Ln,因此RELM具有很高的计算效率.2.3KELM修正为如果在式(4)中加入控制参数C,则β计算公式Page3文献[21]将h(狓)看作犚m→犚L的特征映射,并定义其对应的ELM核函数:此时犎犎T可看作犡的ELM核矩阵:其中犺(狓)=[hi(狓),…,hL(狓)]表示L个映射构成的(行)矢量.结合式(6)和(8)可知,ELM的预测可写为核形式:其中根据Mercer定理,犺(狓)可推广至隐式映射,即犺(狓)=[φ1(狓),φ2(狓),…,φ(狓)](表示映射可能为无限维).此时犺(狓)具体形式未知,但对应核函数κ(狓1,狓2)已知,常见的有:线性核(Linear):κ(狓i,狓j)=狓i·狓j;多项式核(Polynomial):κ(狓i,狓j)=(a狓i·狓j+b)d;径向基核(RBF):κ(狓i,狓j)=exp(-σ狓i-狓jS型核(Sigmoid):κ(狓i,狓j)=tanh(-a狓i·狓j+b).除上述常用核之外,还有很多核类型[24].此时犡的核矩阵由核函数计算获得可看出KELM中核矩阵犓有两种计算方法:(1)当映射是显式映射时(ELM),由隐含层输出矩阵获得,即犓ELM=犎犎T.(2)当映射是隐式映射时(核方法),由核函数计算获得,即犓=κ(犡,犡).方法(1)中的映射已知,因此犎和犓ELM都是明确的,犎犎T可直接看作犓的低秩分解.方法(2)中映射不明确,只能获得犓,犓的低秩分解需额外计算.综上,KELM算法可以归纳为[21]算法1.KELM算法.输入:训练集={(犡,犜)|犡={狓i|狓i∈犚m}n输出:新样本狓的预测f(狓)1.通过式(8)或式(11)计算核矩阵犓;2.计算矩阵的逆犐3.计算新样本狓的映射犽狓=4.计算预测结果f(狓)=犽狓犐KELM的详细内容可参见文献[21].KELM中无偏置参数b,寻优空间为立方体,放宽了SVM在超平面寻优的限制[21].但KELM的训练时间O(n2m+n3+ns)和空间O(n2)随着样本数n的增加而急剧膨胀,不适合大样本数据,为此,我们下面提出一种加速计算框架.3基于低秩分解的快速计算框架如果核矩阵犓由式(8)犓=犎犎T获得,则犎犎T可直接看作犓的rank-L分解:其中犎∈犚n×L.如果核矩阵犓由式(11)犓=κ(犡,犡)获得,则需要对犓进行rank-L分解.分解方法可参考文献[26-29].设分解结果记为其中犉∈犚n×L.在矩阵分解后,KELM需转换为犉的求解形式.接下来,证明该转化的可行性.定理1.给定训练数据犡和标签犜,测试数据犡e和标签犜e,基于犡训练的KELM与基于犉和犉T训练的线性KELM等价.其中是核矩阵犓的因子分解.证明.犉可看作由原数据经过重新表示后的数据,根据KELM原理[21],采用犉训练线性KELM的数学模型可表示为根据KKT定理可得知对于样本狓,预测函数为f(狓)=犉(狓)犉T犐其中犉(狓)表示原数据狓的重新表示形式.对比式(9)和(17)可知二者完全等价.定理1表明KELM的求解可以分解为两步:首先,对核矩阵进行分解,获得数据的重新表示犉;然后,基于犉计算输出权β.但式(16)并不能实现KELM的加速,因为犉犉T∈犚n×n与核矩阵犓具有同样的规模.但如果式(16)可进一步转换为犉T犉∈犚L×L的形式,计算复杂度将大大降低(通常Ln).该转换的Page4等价性证明如下.阵,C为非0常量,则有定理2.设犘为列满秩矩阵,犙为行满秩矩(SMW)[25]定理可知证明.根据Sherman-Morrison-Woodbury(犃+犘犙)-1=犃-1-犃-1犘(犐+犙犃-1犘)犙犃-1.上式两端乘以犘,可得(犃+犘犙)-1犘=犃-1犘-犃-1犘(犐+犙犃-1犘)-1犙犃-1犘=犃-1犘-犃-1犘(犐+犙犃-1犘)-1(犐+犙犃-1犘-犐)=犃-1犘-犃-1犘+犃-1犘(犐+犙犃-1犘)-1=犃-1犘(犐+犙犃-1犘)-1=犘(犃+犙犃-1犘犃)-1.将犃替换为犐C+犘()犙-1犘=犘犐犐将犉看作犙,犉T看作犘,可得结合式(16)和(19)可知表示):可见式(16)可转换为犉T犉的形式求解.综上,KELM的加速计算框架可归纳为(如图1(1)计算获得核矩阵犓;(2)计算核矩阵分解:犓=犉犉T;(3)通过式(20)计算输出权:β=犐不论犉是通过隐含层输出矩阵获得(犓=犎犎T),还是由矩阵分解获得,上述框架都成立.当犉由隐含层输出矩阵获得时,输出权可表示为可以看出式(21)的形式与RELM计算公式(5)完全一致,表明RELM是犓=犎犎T时的特例.下面讨论犉来自于核矩阵分解的情况,并提出该框架下的快速算法NKELM.4NKELM算法有很多种方法可以获得核矩阵犓的低秩分解[26-29],常用的是最优rank-L分解:首先计算犓的SVD(SingularValueDecomposition)分解[25]:然后选取前L个奇异值和对应的奇异矢量作为犓的低秩逼近,即但这种方法的时间O(n2L)和空间O(n2)代价过高,无法处理大型犓,为此,这里采用Nystrm逼近方法[30]来获得犓的近似分解.犓的Nystrm逼近可表示为其中犣是从犡中选择的nz(L)个样本,犓xz=κ(犡,犣),犓zz=κ(犣,犣).设犓zz的SVD分解记为则犓的近似低秩分解可表示为如果选取式(25)中的前L个奇异值与奇异矢量,则可得到犓的近似rank-L分解:为简单起见,通常直接使nz=L(因此后面将不再区分采样数目与隐含层节点数),并直接采用式(26)作为犓的近似分解.结合式(20)与(26),可得综上,NKELM算法可归纳为算法2.NKELM算法.输入:训练集={(犡,犜)|犡={狓i|狓i∈犚m}n输出:新样本狓的预测f(狓)1.从犡中选取L条数据犣;2.计算核矩阵犓zz=κ(犣,犣),犓xz=κ(犡,犣);3.计算犓zz的SVD分解:犓zz=svd犝z犛z犝Tz;4.计算犉^=犓xz犝z犛-1/2z;5.计算输出权β=犐6.给定样本狓,计算映射后特征矢量:Page57.计算样本狓的输出:通常Ln,犓zz规模很小,计算犓zz的SVD分解的时间与空间代价都很低,其他操作基本都是矩阵乘,这为处理大样本数据提供了很好的支持.5NKELM与KELM的关系NKELM与KELM紧密相关,当L<n时,NKELM可看作KELM的近似;而当L=n时,NKELM则等价于KELM.定理3.当采样数目L=n时,NKELM将转化为KELM.证明.当犣选取整个数据集犡,即当L→n时有进一步有犓xz犝z犛-1/2z→犓犝犛-1/2=犝犛犝T犝犛-1/2=犝犛1/2(32)即根据式(28)可知输出权为根据定理2可知,上式可转换为预测函数可表示为对比KELM的预测函数(9),可知二者完全等价.6NKELM误差边界分析接下来,我们分析Nystrm逼近对预测的影响.为方便起见,首先基于定理2将式(30)写为核形式:f(狓)=犉^(狓)犉^T犐定义δ使得狓∈犡都有κ(狓,狓)δ,定义τ使得狋τ,对于预测误差边界有以下定理.定理4.设f(狓)和f(狓)分别表示KELM和NKELM的预测结果,则狓∈犡,f(狓)-f(狓)δmτ犓^-犓证明.设α=犐则KELM预测可记为NKELM预测可记为易知α^-α=犓^+犐()Cα^-α犓^+犐()C进一步可知其中λmin犓^+犐()C表示犓^+犐λmin犓+犐()C表示犓+犐假设逼近不影响犽狓,令犽^狓=犽狓,则f(狓)为根据式(39)和(43)可知进一步有f(狓)-f(狓)α^-α犽狓δ槡nα^-α(45)因犓^+犐又因犜狋12+狋22+…+狋n根据式(42)、(45)、(46)及式(47)综合可得f(狓)-f(狓)δnτ犓^-犓可以看出,预测误差与矩阵逼近误差犓^-犓直接相关,犓^与犓越近似,预测误差越小,反之越大.而犓^-犓与采样数目nz相关,nz越大,犓^-犓越小,当nz→n时,犓^-犓→0,f(狓)-f(狓)→0,意味着NKELM将与KELM产生相同的预测,误差为零,这与定理3结论一致.7实验结果与分析为了验证算法的有效性,我们将ELM、KELMPage6和SVM进行实验对比.评价指标包括训练时间(TrainingTime)和测试精度(TestingAccuracy).实验平台配置为Intel2.67GHzCPU,4GB内存,64位Windows7操作系统,64位Matlab2011.7.1数据集选取卫星遥感(Satimage)、航空航天(Shuttle)、图像处理(SkinSegment)等5种实际数据进行实验.数据集描述如表1所示.Satimage训练集规模小于20000,选择Satimage数据的原因是让KELM能够正常返回实验结果.如果都是很大的数据,KELM将会内存溢出.其余数据规模都大于20000,以验证所提算法对大样本问题的处理能力.数据集#训练集#测试集#特征#类别Satimage44352000366Shuttle435001450062SkinSeg14505710000032IJCNN4999091701222CovType464810116202547Satimage.根据给定的3×3像素的图像,预测中心像素的类别(如图2所示).一帧地球资源扫描数据(LandsatMSSImagery)包括4种不同频谱下的图像,每幅大小为2340×3380像素(每个像素空间分辨率约为80m×80m).为了实验需要,截取其中的82×100像素构造Satimage数据集.构造方法是:将82×100像素划分成很多3×3像素的正方形,因图像共有4层,因此可切出4个正方形,这4个正方形构造一条数据记录,显然数据维度是9×4=36.图像是矩阵形式,为便于计算,采用从左到右、自上而下的顺序将其矢量化.易知中心像素对应的特征应该是第17,18,19和20维.如果认为中心像素的类别仅与自己有关,那么可以直接采用第17,18,19和20维进行预测.如果认为不但和自己有关,还和周围像素相关,则可全部选取.后一种方法在文献中普遍使用,我们也继续沿用,即采用36维的数据形式.数据共有6个类别:1redsoil,2cottoncrop,3greysoil,4dampgreysoil,5soil图2Satimage:根据36个像素预测中心像素的类别withvegetationstubble,6verydampgreysoil,分类的任务是根据36维特征,预测中心像素的类别.Shuttle.根据飞行器所处的环境和状态信息来判定飞行器应该自动着陆还是人工着陆(如图3所示).数据有6个属性,2个类别,58000条样本.6个属性分别是Stability、Error、Sign、Wind、Magnitude和Visibility.两个类别分别是NoAuto和Auto(即自动着陆与人工着陆).约80%的数据属于第1类(NoAuto),其余数据属于第2类(Auto),因此默认的分类精度是80%,预期的精度是99%~99.9%.训练集和测试集通过随机切割方式产生,大小分别为43500个样本和14500个样本.SkinSegmentation.该数据根据像素的B,G,R(代表色彩空间)3个属性来判断是否为皮肤.数据集基于GRB颜色空间构造,从不同年龄(青年、中年和老年)、性别、种族(白人、黑人和黄种人)的脸部图像的皮肤纹理中提炼产生.这些数据来自FERET①和PAL②两个数据库.数据集有3个属性分别为B,G,R,2个类别分别为1(Skin)和2(NoSkin),245057条样本.训练集和测试集通过随机切割方式产生,大小分别为145057和100000.IJCNN2001(Task1).给定一个长度为T=49990的时间序列,序列中的每个样本k都包含4个输入:x1(k),…,x4(k)和一个输出y(k).另外长度为91701的时间序列用来测试.属性x1(k)表示与系统自然周期相关的二进制同步脉冲,为10位二进制形式,并以9个0,一个1的规模出现.属性x2(k),x3(k),x4(k)是[-1.5,1.5]之间的实数,其中x4(k)和y(k)最为相关,因此后面构造特征时不仅考虑了x4(k),还考虑了它的前后时刻的信息.我们采用文献[31]中的方法对时间序列行进行特征构造:x1(k)扩展为x1(k-5),…,x1(k+4)等10个特征;x2(k)和x3(k)直接使用;x4(k)扩展为x4(k-5),…,x4(k+4)等10个特征,最终形成22个特征.训练数据中①②Page790%的输出为第1类(-1).因此默认的分类精度是90%,分类任务是构造模型以获得更高的分类精度.CovType.通过地图数据(不是遥感数据)的12项参数预测森林覆盖类型.这12项参数主要有海拔(Elevation)、方位朝向(Aspect)、倾斜度(Slope)、所属保护区(wildernessareas,共4个)、土壤类型(soiltype,共40种)等.所属保护区用4位二进制表示,土壤类型用40位二进制表示,其余10个参数用数字表示,数据共有54维特征.类型有7类:Spruce/Fir、LodgepolePine、PonderosaPine、Cottonwood/Willow、Aspen、Douglas-fir和Krummholz.分类模型的任务是根据这54维输入预测森林覆盖的类型.7.2参数设置对ELM①,激励函数选择S型函数h(狓)=1/(1+eλx),参数λ除非特别说明,默认取1.SVM②、KELM③和NKELM选择高斯核κ(狓,狔)=exp(-σ狓-狔2).对于SVM和KELM,需要设置参数C与高斯核本身的参数σ.C和σ取值范围设为C=[2-5,2-9,…,220]和σ=[2-5,2-4,…,25].通常的方法是对C和σ进行排列组合,并采用10-折交叉验证方法寻找最优组合[32],但因数据规模较大,每次运行花费的时间过长,因此我们采样折中的轮换交替方法确定C和σ:首先固定C,通过交叉验证的方法确定σ,然后在σ基础上,进一步确定C.NKELM需要调整的参数是C、σ和采样数目L.参数确定方法是:固定L(取的小一些以加快速度)和C,通过交叉验证的方法确定σ,在σ确定后,对L和C的取值情况进行排列组合(L,C),通过交叉验证的方法从中选择较好的组合.之所以对(L,C)进行组合寻优,是因为二者之间关系非常紧密,单独寻优难以取得较好结果.各算法的最后参数如表2表示.各项参数确定后,按照表1中训练与测试集的大小对数据进行10次随机切分并进行实验,最终汇报10次平均运行结果.表2ELM,SVM,KELM和NKELM的参数设置数据集NKELMSatimage3002202-2300282-22202-2Shuttle1000220221000--2202-2SkinSeg45022022450--2202-2IJCNN13002202-21300--2202-2CovType10002202-21000--2202-27.3NKELM与KELM的比较法的训练时间.表3汇总了所提算法NKELM与其他3种算数据集NKELM/sELM/sKELM/sSVM/sSatimage6.6904.31045.072.122Shuttle16.64032.760-30.590SkinSeg12.90443.924-186.430IJCNN31.35059.570-267.000CovType53.900377.460-2460.000从表3中训练时间可以看出,NKELM的训练时间远远低于KELM.以Satimage为例,NKELM的训练时间是6.69s,而KELM的训练时间是45.07s,训练速度快约7倍.为了解释NKELM比KELM快的原因,我们从时间复杂度入手分析.NKELM中的主要操作包括:(1)计算核矩阵犓xz,时间复杂度为O(nmL);(2)计算核矩阵犓zz,时间复杂度为O(mL2);(3)计算犓zz矩阵的奇异值分解,复杂度为O(L3);(4)计算输出权,时间复杂度为O(nLs).总时间复杂度约为O(nmL+mL2+L3+nLs).KELM中的主要操作有:(1)计算核矩阵犓xx,时间复杂度为O(n2m);(2)计算犓xx核矩阵的逆,时间复杂度为O(n3);(3)计算输出权,时间复杂度为O(ns).总时间复杂度约为O(mn2+n3+ns).可以看出NKELM的时间复杂度与样本数目n是线性关系,而KELM的时间复杂度与n是3次幂关系,远高于NKELM.接下来从空间复杂度对比分析NKELM和KELM.在NKELM中,物理存储主要包括核矩阵犓xz(空间复杂度O(nL))和犓zz(空间复杂度O(L2)),总空间复杂度约为O(nL)+O(L2)≈O(n).KELM算法主要涉及犓xx,物理复杂度约为O(n2),远高于NKELM,这使得NKELM能够在同样配置的机器上处理更大规模的数据.数据集NKELM/%ELM/%KELM/%SVM/%Satimage91.2589.8091.2890.00Shuttle99.7999.74-99.92SkinSeg99.8599.79-99.57IJCNN98.2297.88-98.35CovType78.1177.86-75.67表4显示的是4种算法的测试精度.因为运行中内存溢出,只汇报KELM在Satimage数据集上的输出.通过Satimage上的结果可以看出NKELM的测试精度(91.25%)要低于KELM的预测精度①②③Page8(91.28%),原因是NKELM只是KELM的一种近似,低秩分解误差犓^-犓的存在导致二者预测也存在误差.NKELM可看作一种预测精度与训练时间的折中算法,Nystrm采样越少,速度越快,而与KELM的误差越大,反之,Nystrm采样越多,速度越慢,而与KELM误差越小.7.4NKELM与ELM、SVM的比较从表3可以看出,与ELM相比,NKELM的学习速度非常快.实验中用的NKELM都是属于需要矩阵分解的情形,如果使用无需矩阵分解的NKELM,即RELM,其训练速度将会比ELM更快,可以参考文献[5]中对比情况.与SVM相比,图4采样数目对测试精度的影响(数据集:Satimage)NKELM在大样本时训练速度具有明显优势,且样本越大优势越明显.以含有43500条样本的Shuttle为例,NKELM训练时间为16.64s,SVM为30.59s,NKELM比SVM快约2倍.而在大小为145057的SkinSeg上,NKELM训练时间为12.904s,SVM为186.43s,NKELM比SVM快了约14倍.从预测精度来看,NKELM的预测精度大部分情况下优于ELM,与SVM相当,可见NKELM是一种计算代价小,而预测性能却较好的算法.7.5采样数目犔对NKELM测试精度的影响图4显示了在Satimage数据集上参数C分别为220,215,210,25,20,2-56种情况下采样数目L对Page9NKELM测试精度的影响.L的取值为100,200,300,400,500,1000,2000,3000,4435(完全采样).从图4可以看出,当采样数目达到一定值时(本例中为L=1000左右),测试精度提升已经非常有限(趋于稳定或反而下降),并不会随着采样数目的增加而继续明显提高,这意味着只需选取部分样本构造Nystrm分解即可达到良好的预测性能.上述使用的是随机采样方法,随机采样虽无法获得最优解,但无需优化时间,具有快速优势.除随机采样外,还可根据不同侧重选取其他采样方法[33].图5参数C对测试精度的影响(数据集:Satimage)从图5可以看出随着C的增大(2-5→27),测试精度逐渐上升并达到最高点,但当C再继续增大时测试精度将不再提升,参数C会根据采样规模产生理论上,好的采样应该使犓^-犓尽量小,因为犓^-犓越小,在同样采样规模下NKELM的预测越接近KELM(定理4),从而达到更好的预测精度.但是优化采样将增加时间代价,可能会大大降低NKELM的快速优势.7.6参数犆对NKELM测试精度的影响图5显示了在Satimage数据集上采样数目L分别为100,300,1000,2000,3000,4435(完全采样)6种情况下参数C对NKELM预测能力的影响,C的取值从2-5逐渐增加到220.不同的影响:(1)采样较小时,测试精度不会出现明显的落差,而是在最高点附近的水平趋向基本稳定(图5中Page10(a)~(c)所示);(2)采样较大时,测试精度将从最高点快速下降(图5中(d)~(f)所示),落差非常明显,而后趋向稳定.可见,当NKELM采样数目较小时,如果对精度不太苛求,可以直接把C取得足够大(比如文中的C=220),这样将能省去确定参数C的时间.如果苛求精度且采样较大,C仍需通过优化方法确定.7.7训练时间随采样数目的变化图6显示了NKELM训练时间和测试时间随采样数目的变化.可以看出在NKELM可以取得较好精度的1000样本时训练时间远小于KELM的时间(完全采样).这说明NKELM能够以极快的学习速度取得较好的泛化性能.如果要使NKELM速度更快,可以进一步减少采样,但会使精度下降,具体采样数目可根据应用侧重和具体需求确定.图6时间随采样数目的变化(数据集:Satimage)8结论与展望KELM虽然比ELM具有更好的稳定性和泛化性,但计算复杂度O(mn2+n3+ns)和空间复杂度O(n2)过高,无法有效处理大样本数据.因此我们为KELM提出一种加速框架,并在该框架下基于Nystrm低秩分解提出一种加速算法NKELM.NKELM计算复杂度和空间复杂度都约为O(n),远低于KELM.在Satimage、SkinSegment和Shuttle等数据上的实验表明:(1)NKELM的训练时间远低于KELM,并且数据规模越大,优势越明显;(2)与ELM、SVM等算法相比,NKELM具有更快的学习速度,同时表现出很好的泛化性;(3)当采样数目达到一定值时,测试精度提升已经非常有限,并不会随着采样数目的增加而继续显著提高,这意味着只需选取部分样本而不是整个样本即可达到很好的预测性能.NKELM中参数C、L和σ选取对算法性能影响非常大,目前使用交叉验证的方式确定,如何实现这些参数的自适应确定将是我们下一步的工作;另外当前使用的随机采样是众多采样方法中最简单的一种,如何提高采样质量,如何采用更好的采样方法提升NKELM的性能也是我们将来的研究工作.致谢审稿人提出了宝贵的意见;新加坡南洋理工大学HuangGuang-Bin教授对论文给出了诸多指正.正是这些宝贵的意见和建议才使论文得以进一步的完善和提高,在此表示感谢!
