Page1PiBuffer:面向数据中心的OpenFlow流缓存管理模型毛健彪卞洪飞韩彪李韬孙志刚(国防科学技术大学计算机学院长沙410073)摘要基于OpenFlow的SDN(SoftwareDefinedNetworking)技术在数据中心中得到广泛研究和应用,如何缓解集中的控制平面成为网络性能的瓶颈是其中的研究热点.OpenFlow规范提出,当数据平面有缓存能力时,未命中的报文仅需发送少量摘要信息至控制器触发规则下发,从而减少控制平面与数据平面的通信负载.然而,现有的缓存模型采用报文粒度的缓存方式,使得同一条流的多个未命中报文会被送至控制器造成额外的通信负载,而且交换机处理报文的顺序会导致流内报文乱序,从而降低通信的性能.针对上述问题,该文提出了一种支持流内报文保序的OpenFlow交换机流缓存管理模型.通过基于流粒度的未命中报文缓存方式,进一步减少控制平面与数据平面的通信开销.通过设计流动作预处理机制,实现同一条流内报文传输保序.该文分别基于软件交换机OFSoftSwitch与硬件网络实验平台NetMagic对该流缓存管理模型进行了原型系统验证.关键词数据中心网络;软件定义网络;OpenFlow交换机;流缓存;下一代互联网1引言作为云计算业务基础设施的重要组成部分,数据中心网络承载着云业务内部以及业务之间的通信.随着不同业务对网络资源和灵活性的需求的不断增加,数据中心网络资源管理和配置变得越来越复杂[1].软件定义网络(SoftwareDefinedNetworking,SDN)思想通过控制与转发分离,将网络中交换设备的控制逻辑集中到一个计算设备上,为提升网络管理配置能力带来新的思路.SDN很好地契合了数据中心网络的集中网络管理、灵活组网、网络性能优化等方面的需求.为此,SDN思想在数据中心中得到广泛研究[2-4]和应用[5-6].OpenFlow是SDN体系架构中一个主流的南向接口,实现了基于流的转发.OpenFlow交换机作为“dumb”的弱智能网络转发设备,所有流规则由远程智能的控制平面下发,这使得集中的控制平面成为网络传输性能的瓶颈.由于受交换机CPU性能不足的限制,Devoflow[3]测得某商用OpenFlow交换机和控制器之间的流建立负载的可用带宽上限仅为17Mbit/s.如何提高控制平面的可扩展性成为当前研究热点[3,7-11].在不影响报文转发的前提下,尽量减少控制平面与数据平面的通信负载是提高控制平面可扩展性的一种解决思路.OpenFlow规范1.4①指出,当OpenFlow交换机有缓存能力时,可支持不命中流规则的报文缓存,无需将完整报文封装到Packet_in消息中,仅需要封装完整报文的摘要信息即可.这种方式减少了送至控制平面的Packet_in消息的报文大小,从而降低了控制平面与数据平面的通信开销.据我们分析所知,为实现简化交换机,当前OpenFlow交换机的缓存管理设计[12-13]采用报文粒度的通用缓存模式,即交换机对连续的未命中报文的处理是独立的.这种方式存在一些不足.由于受控制器处理报文时间以及流规则传输时间和装载时间的影响,同一条流的前面多个报文均会因为未命中流规则而产生大量的突发Packet_in消息至控制器.在数据中心流量快速动态变化的特性下[14],面向连接的TCP流也会出现前面多个数据报文未命中流规则而被送至控制器的情况,具体分析见2.1节.但对于控制器下发流规则来说,为保证一致性和减少控制器的处理负载,一条流只需获取一个报文,而无需了解该流的后续报文.此外,交换机为了防止产生更多的未命中报文Packet_in消息,流规则将先被迅速装载至流表中,然后用于释放交换机中的缓存报文.但这样会导致同一条流的后续报文先于缓存的报文发送,造成流内报文乱序.乱序报文到达接收方后,需要重新排序后才能交给上层应用,该重组过程会给端系统带来额外开销.而且对于TCP流,报文乱序会导致通信传输带宽显著下降[15].为进一步减少控制平面与数据平面的通信开销以及解决流内报文乱序问题,本文提出了一种支持流内报文保序的OpenFlow交换机流缓存管理模型Pi(Packet_in)Buffer.该模型是对原有OpenFlow报文处理方式的增强和补充.PiBuffer通过PiBT(Packet_inBufferTable)表,记录同一条流的不命中报文是否已向控制器发送过Packet_in消息,控制该流只有一个Packet_in消息被送往控制器,并将未命中的报文以流粒度的形式缓存.此外,通过设计流动作预处理机制,按报文先后顺序调度已缓存报文和新命中规则报文,实现流内报文保序转发.本文的主要贡献如下:(1)分析现有通用缓存模型存在的不足,指出报文粒度的缓存会产生多余的Packet_in请求,增加交换机与控制器的负担,同时会引起报文乱序.(2)提出流粒度的缓存模型PiBuffer,以流的粒度存储未命中流的报文,减少SDN控制平面与数据平面的通信开销,实现流内保序转发.(3)分别基于软件OpenFlow交换机OFSoft-Switch②和NetMagic网络硬件实验平台③对PiBuffer进行了原型系统实现与验证,为OpenFlow交换机流缓存实现提供参考设计.2问题描述2.1数据中心One-wayFlow-setup方式与流量特性基于OpenFlow的数据中心[2-4]在以被动触发式建立转发路径时通常采用“One-wayFlow-setup”的方式,以减少流建立的开销.如图1所示,假设源主机A向目的主机B发送一条报文流,经过n跳OpenFlow交换机S1,S2,…,Sn.当报文流首报文P1到达第1跳OpenFlow交换机S1时,若未命中转发①②③Page3规则,则S1产生一个Packet_in消息Pkt-in并通过已建立好的OpenFlow通道发送至集中控制器C(涉及到消息的表示,本文用斜体字母表示特定的某个消息,用正常字母表示某一类消息).C处理后为该流建立从A到B的双向转发路径,即对路径上所有相关的交换机S1,S2,…,Sn分别下发包含流规则的Flow_mod消息Flow-mod1,Flow-mod2,…,Flow-modn.为防止丢包,C还需下发一个包含P1相关信息的Packet_out消息Pkt-out至交换机S1,使P1转发至对应的端口.而后,P1将沿着已建立好的转发路径被发送至B.但是,在装载转发规则的过程中,由于交换机S1,S2,…,Sn与控制器的距离不同以及不同交换机当前负载不一致,可能出现规则装载慢于报文到达的情况.例如当P1到达S2,而Flow-mod2中包含的规则还未装载至S2.这就会导致文献[16]中所指出的由于装载规则时延引发了控制逻辑不一致的问题.为分析“One-wayFlow-setup”需满足的条件,本文定义符号如表1所示.为保证P1到达转发路径上时,后续交换机Si(i=2,3,…,n)均不会因未命中转发规则而产生Packet_in消息至控制器,则传输时间需满足如下关系:符号n流从源主机到目的主机经过OpenFlow交换机的数量B某条流的带宽tdtA→S1报文从源主机A至第1跳交换机S1的传输时间tS1→C未命中流表的报文从交换机S1至控制器C的传输时间tCtC→SitS1→Si在该情况下,一条报文流只在OpenFlow数据中心网络边缘(即第1跳交换机)未命中转发规则.因此这种“One-wayFlow-setup”方式的好处是在以被动触发式建立转发路径时,最小化一条流对控制器产生的开销.所以目前基于SDN的网络操作系统(如Floodlight①、POX②等)均默认采用这种方式为未命中报文建立转发路径.特别地,为了尽可能地满足式(1)的延迟条件,这些控制器在建立转发路径时均采用逆序下发规则的方式[16].即先从最后一跳交换机(即Sn)开始依次下发流规则,最后才将规则装载至第1跳交换机(即S1).现有的研究[2-4]也假设在OpenFlow数据中心中满足式(1).本文将基于式(1)分析面向数据中心的OpenFlow流缓存管理模型.即未命中报文缓存只可能发生在第1跳OpenFlow交换机上.需要指出的是,对于TCP流,触发流规则下发的首报文P1并非一定是TCP建立连接的SYN分组或者应用产生的Keep-alive分组.文献[14]指出,在数据中心网络中,一个交换机的新流到达时间间隔为10μs,意味着每秒钟有100000条新流到达一个交换机.由于瞬时存在的活跃流的数量大于交换机硬件表项数量,使得交换机中已有的TCP连接对应的流规则很容易在该流空闲时被替换.例如假设A到B之间已建立转发路径,即在S1,S2,…,Sn均已装载流规则RA-B.当数据报文P1到达S1时,RA-B可能已被替换,需要重新触发控制器下发流规则.因此与UDP流一样,已建立连接的TCP流的数据报文也可能未命中流规则,被送至控制器做进一步处理.此外,在数据中心环境中,报文的环回时间RTT(即2tA→B)通常为10μs到100μs.而文献[17]测得OpenFlow流规则的装载时间约为4ms(即tS1→C+tC+tC→S1Linux操作系统的超时重传时间(RTO)默认为200ms,所以增加的报文传送延迟对报文重传影响较小.2.2交换机缓存问题分析与可行性论证当交换机与控制器建立连接时,控制器通过OFPT_FEATURES_REQUEST消息获取交换机本地是否有缓存和缓存的大小.若交换机无缓存,其报文处理流程如图2(a)所示.P1,P2,P3和P4是A发送至B同一条流上的报文.交换机产生的Pkt-in消息中将携带未命中流表的完整报文至控制器C.C解析P1后下发包含相应①②Page4流规则的Flow-mod消息,消息内包含A,B之间的转发规则RA-B(主要由匹配域MatchA-B和动作域ActionA-B组成).该规则装载至流表后,流后续的报文P4将命中流规则,执行相应的动作.为了不丢包,每对应一个Pkt-in消息,C都需将完整的报文以Pkt-out消息的形式发送至交换机,消息中携带报文的转发动作ActionA-B.优化后的C会识别出P2,P3与已收到的P1属于同一条流,所以不再下发Flow-mod消息.由于流的首报文到达交换机至流规则下发存在延迟,这个过程对OpenFlow通路产生的负载L无缓存大约为此外,对于大报文,交换机对其进行OpenFlow消息的封装容易超过最大传输单元(MTU).所以交换机还需对超过MTU的报文进行分片.这也增加了交换机CPU的负载.当交换机支持缓存未命中报文时,交换机在与控制器建立连接时会通告控制器交换机所能缓存报文的最大数量.而后,控制器下发OFPT_SET_CONFIG消息,设置交换机产生未命中Pkt-in消息的最大字节数,默认为128Bytes.经过对目前已有OpenFlow软件交换机的OFSoftSwitch源码进行分析可知,通用的报文缓存处理流程如图2(b)所示.未命中流表的报文P1,P2和P3被临时放在缓存中,对应的缓存id分别为Buf-id1,Buf-id2和Buf-id3.交换机产生的Pkt-in消息中将携带未命中报文的摘要信息(如图2(b)中所示的{P1},默认为128Bytes)和缓存id.控制器C响应首个Pkt-in消息,下发一条Flow-mod消息.Flow-mod消息中携带Buf-id1和A,B之间的转发规则RA-B.对于该条流后续的Pkt-in消息,C产生Pkt-out消息,其包含未命中报文的缓存id和转发动作ActionA-B.在流表中插入规则后,Flow-mod消息将释放缓存id为Buf-id1所对应的报文P1,使该报文通过整条OpenFlow的报文处理流水线.由于产生Packet_in消息携带的字节数与报文默认MTU的比值约为1/5(300Bytes/1500Bytes),所以整个过程对OpenFlow通路产生的负载L通用缓存大约为因此,相比于图2(a)中无缓存的OpenFlow交换机,两者的控制器-交换机通信负载比值为图2无缓存与通用缓存的OpenFlow交换机工作模式对比从以上分析可知,OpenFlow交换机是否缓存未命中流表的报文决定了控制器CPU与交换机CPU的负载情况.若无缓存,交换机处理简单,但增加了通信负载与控制器的处理开销;若有缓存,使交换机具备了一些智能,分担了报文缓存的工作,减少了控制平面与数据平面通信的开销,但也增加了缓存设计与管理的难度.目前商用硬件OpenFlow交换机①为降低成本、简化报文处理,通常采用无缓存的设计.尽管在硬件交换机中增加大量缓存会提升成本和设计的复杂性,但Lu等人[18]提出采用CPU+DRAM的方式增加交换机的缓存,提升数据中心网络数据平面的功能.此外,目前数据中心中SDN的实现通常基于服务器内部的软件虚拟交换机(例如OpenvSwitch[13]),缓存对于软件交换机来说实现问题不大.因此,总的说来,在交换机中为未命中流表的报文增加缓存是可行的.2.3现有缓存模型存在的不足如图2(b)所示,同一条流的报文以P1-P2-P3-P4①CentecV330Specification.http://www.centecnetworks.Page5的顺序到达交换机.假设P1,P2和P3未命中流表触发控制器下发规则,且在P4到达时,该流的规则已装载至流表中,所以P4查表命中流规则.交换机内一种可能的处理顺序如图3所示.OpenFlow规范中指出,为尽量减少同一条流产生Packet_in报文的数量,Flow_mod消息先装载流规则,然后释放对应Buffer_id的报文使其通过OpenFlow报文处理流水线.P4查表命中流规则转发这一事件先于缓存的P1被释放发生.而后,Pkt-out2,Pkt-out3消息分别释放报文P2和P3,则该流经过OpenFlow交换机后可能的一种报文输出序列为P4-P1-P2-P3.文献[15]分析了TCP端系统为重组乱序报文所需要的缓存容量以及由此产生的报文延时.分析结果表明.报文重排序要求很大的缓存容量,对端系统提出较高要求;其次,重排序引起的报文延时显著,而且报文延时与TCP连接吞吐率和分组大小紧密相关,在较高的连接吞吐率和小尺度分组条件下,重排序操作将对上层网络应用性能产生严重的影响.尽管乱序报文对UDP流的影响不如对TCP流的影响大,但是保序后的报文减少了应用层重组报文的开销,因而提高应用通信的性能.图3OpenFlow交换机内报文的处理先后顺序OpenFlow交换机的缓存可有效减少交换机与控制器之间的信息交互负载.但我们发现,若对交换机的缓存实施有效的管理,则可进一步减少交换机与控制器之间的通信开销.例如对于同一条流的未命中报文只需要产生一个Packet_in消息至控制器.而且,原有的缓存模型易导致上文中分析出现的报文乱序问题,降低应用通信的性能.为此,本文针对OpenFlow交换机流缓存的管理展开研究,以降低OpenFlow数据和控制平面的通信开销,实现报文流内保序转发.3PiBuffer流缓存管理模型由于现有的OpenFlow交换机缓存管理存在不足,本文基于原有的OpenFlow规范提出了面向数据中心的流缓存管理模型PiBuffer.该模型对未命中流表的数据报文,按照流粒度进行缓存与管理,使得每条无转发规则的流只产生一个携带该流标识信息的Packet_in消息,将其发送至控制器,以进一步减少交换机与控制器之间的信息交互,提高控制平面的可扩展能力.同时该模型通过流动作预处理机制,解决现有缓存方式下流表下发导致的流内报文乱序问题.基于第2节不等式(1)的假设,本文仅考虑在第1跳交换机未命中报文的缓存情况.如图4所示,在PiBuffer模型中,OpenFlow流水线的基本过程保持不变,即按照OpenFlow规范实现报文头的解析、查找流表、根据查表结果执行相应动作.引入PiBuffer主要对未命中流表报文的处理过程和流规则的装载过程进行了修改.PiBuffer重新定义了3类消息(Packet_in消息、携带Buffer_id的Flow_mod和Packet_out消息)的处理过程,但不改变其他Open-Flow消息的处理过程.PiBuffer流缓存管理模型主要包含两个功能部件:流动作预处理部件和Pi缓存管理部件.流动作预处理部件主要功能是识别携带Buffer_id的Flow_mod/Packet_out消息,先释放缓存中与该Buffer_id相关的报文,然后按照Flow_mod/Packet_out消息中的流动作进行转发.OpenFlow规范中,流规则包含了流匹配域和流动作,Packet_out消息中携带流动作,Flow_mod消息中携带流规则.所以若为Flow_mod消息,流动作预处理部件在等待缓存的流报文全都释放完后,才将该消息内携带的规则装载至流表中,从而实现流内的报文保序.Pi缓存管理部件主要功能是实现未命中报文的缓存,流缓存区的管理,以及产生流粒度的Packet_in报文,减少交换机发往控制器的Packet_in请求数目.Page6当报文未命中流表时,OpenFlow流水线将未命中的报文输出至Pi缓存管理部件.Pi缓存管理部件根据一定的缓存策略决定是否缓存该报文.根据文献[14]的测量结果可知,在数据中心网络中,报文大小表现为双峰分布,主要集中在200Bytes左右和1400Bytes左右.200Bytes大小的报文通常为应用的keep-alive报文和TCPAck报文.1400Bytes大小的报文为有效数据报文.这些特性将会影响未命中报文的缓存策略.若不缓存,则将完整报文封装为OpenFlow的Packet_in消息,发送至控制器做进一步处理.若缓存,报文以流为粒度组织存放.若是流的首报文,则需提取该报文的前128Bytes,封装成Packet_in消息,发送至控制器.若报文为流的后续报文,则不产生Packet_in消息.所以,在PiBuffer模型下,一条未命中的报文流对OpenFlow通路产生的负载为一个Pkt-in消息和一个Flow-mod消息(或者Pkt-out消息).因此PiBuffer对OpenFlow通路产生的负载LPiBuffer大约为所以,无缓存、通用缓存、基于PiBuffer的缓存,这3种模式在一条带宽为B的流不命中规则的情况下,占用控制器-交换机OpenFlow通路负载比值为L无缓存L通用缓存LPiBuffer=2BT图5PiBuffer流缓存管理模型Pi缓存管理部件主要包括缓存创建/更新模块、缓存释放模块、PiBT(Packet_inBufferTable)和缓存.PiBT是其中的核心,用于记录未命中报文缓存的相关信息.创建/更新模块负责将未命中流表式中单位为字节.PiBuffer通过流动作预处理,实现了先释放未命中报文的缓存后装载规则的方式,达到了报文保序的目的.但这也对交换机的处理能力提出要求.假设交换机释放报文缓存的速率为R,释放所需的时间为ts,其余标记如表1所示.当流持续时间td>tS1→C+tC+tC→S1则交换机处理缓存的能力需满足即若交换机缓存处理能力不满足该条件,会产生PiBuffer缓冲区溢出的情况.4PiBuffer方案设计PiBuffer流缓存管理模型系统方案设计结构如图5所示.该设计基于原有的OpenFlow处理流水线,重定义了对未命中报文的缓存操作过程,以及对包含Buffer_id的OpenFlow消息预处理过程.4.2节和4.3节将对修改后的这两个过程进行详细描述.的报文根据缓存策略决定是否缓存,产生相应的Packet_in消息.缓存释放模块将流动作预处理部件产生的Buffer_id和流动作等信息作为输入,根据PiBT表中的标识释放报文流的缓存信息和位置,取Page7出缓存区报文,与流动作一起输出至OpenFlow流水线中的动作执行模块.释放完该流对应的所有缓存后,缓存释放模块向流动作预处理部件发送一个缓存报文处理完成的状态消息.流动作预处理部件包括动作解析模块和规则装载模块.其中动作解析模块负责解析控制器下发的包含Buffer_id的Flow_mod/Packet_out消息,并将解析得到的流动作和Buffer_id发送至Pi缓存管理部件.在接收到缓存报文处理完成的状态消息后,规则装载模块才将流表规则写入至OpenFlow流水线的流表中.4.1PiBT组织结构PiBT负责记录报文缓存相关的信息,其表项内图6Pi缓存表的组织结构Pi缓存区以单向链表的结构组织缓存的报文.当缓存一个新报文时,将该新报文缓存单元的地址添加到同一条流缓存区内上一个报文缓存单元的下一个报文指针,同时更新当前缓存区地址,指向该新报文的地址.若为软件实现该缓存管理模型,则无需关心每个报文的大小.而基于硬件实现时,为适应不同大小的报文,每个报文缓存区可按照MTU的大小分配.由于之前提到,数据中心中的报文大小分布具有双峰特性.若按照报文大小指定缓存策略,即小报文不缓存(小于300Bytes),仅缓存大报文(大于1400Bytes),那么硬件实现中按照MTU分配每个报文的存储空间是合理的.PiBT的表项数量为交换机可同时支持未命中流缓存的数量.数量越多,表明该交换机越能适应高速动态变化的数据中心流量.该数量的大小受到交换机存储资源的限制.容包括匹配字段、缓冲区首地址、当前缓冲区地址、缓存报文计数和超时时间字段,格式如图6所示.其中,匹配字段为未命中报文流的流标识,用于识别不同流的未命中报文以及区分同一条流的首报文和后续报文.缓冲区首地址为一条未命中流的第1个报文的缓存单元地址,也是该流在缓存区的首地址.当前缓冲区地址表示一条未命中流的最近一次缓存的报文的缓存区地址.缓存报文计数表示当前流中缓存的未命中报文个数.流超时字段用于表项的生存时间.若长时间未收到控制器对未命中报文的响应消息,即超时时间超过一定的阈值,就删除该表项并释放对应的缓存空间.4.2未命中报文缓存过程Pi缓存管理部件的工作流程可分为未命中报文的缓存及释放两个流程.报文缓存流程如下:(1)当缓存创建模块接收到OpenFlow流线部件中的流表查找模块产生的未命中流表项的报文时,缓存创建模块首先根据未命中报文的头字段查询PiBT表.(2)当报文命中PiBT表时,说明该未命中报文为一条未命中后续报文,缓存创建模块将该报文的缓存区地址链接到上一个报文缓存区的指针,并将报文缓存信息更新到命中的PiBT表中.(3)当报文未命中PiBT表时,说明该未命中报文为一条未命中流的首报文,缓存创建模块将为该报文创建一个PiBT表项,并将创建的PiBT表项的地址输出到OpenFlow流水线中的转发模块,并由其将该PiBT地址作为Buffer_id封装到Packet-in消息到控制器.该流程的形式化描述如过程1.Page8过程1.未命中报文缓存处理过程.输入:未命中的报文P输出:Packet_in消息1.IF(!Policy(P))2.Buffer_id(P)=NONE;3.Packet_In=Generate_PI(P,Buffer_id(P));4.ELSEIF(!Storage(P))5.Buffer_id(P)=NONE;6.Packet_In=Generate_PI(P,Buffer_id(P));7.ELSEIF(Lookup(PiBT,P))8.Buffer_address=Find(PiBT);9.Update(PiBT,P);10.ELSE11.New(PiBT,P);12.{P}=abstract(P);13.Packet_In=Generate_PI({P},Buffer_id(P));14.ENDIF4.3流动作预处理过程流动作预处理工作流程如下:(1)当控制器下发包含Buffer_id的Flow_mod/Packet_out消息时,流动作预处理部件中的动作解析模块捕获该消息,提取其中的流表规则和Buffer_id,并输出给Pi缓存管理部件.(2)Pi缓存管理部件根据Buffer_id查找PiBT表,将表中对应的Pi缓存中的报文依次释放至OpenFlow流水线中,并执行相应的报文动作.(3)当缓存释放模块释放完该条流的所有缓存报文后,若为Flow_mod消息,规则装载模块才将流表规则插入到OpenFlow流水线部件中的流表查找模块中的流表中,并更新相应的计数器.该流程的形式化描述如过程2.过程2.流动作预处理过程.输入:包含Buffer_id的Flow_mod/Packet_out消息输出:流动作和缓存报文1.IF(Lookup(PiBT,Buffer_id))THEN2.While(Retrieve(PiBuffer,Buffer_id))DO3.Release(Pkt);4.Pipeline(Pkt,Actions)5.ENDWHILE6.ENDIF7.IF(Flow_mod)THEN8.Rules=Exact(Flow_mod);9.Insert(Flow_table,Rules);10.ENDIF5实验结果与分析在基于OpenFlow的数据中心应用场景中,Open-Flow交换机既有物理硬件交换机的实现形态,也有软件交换机的实现形态.为此,本文分别从软件和硬件两个实现角度对该缓存管理模型进行验证.5.1基于OFSoftSwitch软件交换机的软件实现OFSoftSwitch①软件交换机是由斯坦福大学、爱立信研究中心和CPqD公司共同维护的开源项目,支持OpenFlowv1.3.由于该软件交换机在用户态实现,因此相对OpenvSwitch较易于修改,方便对新功能进行验证.本文基于第4节提出的PiBuffer设计方案,在OFSoftSwitch软件交换机上实现了所提出的流缓存管理模型,替换原有的通用缓存方式.实验所用的机器是一台CPU主频为3.2GHz的IntelCorei5、内存为8GB的台式机.在该机器上构建如图7所示拓扑.控制器采用模块化的Flood-light,其上运行三层转发的应用.在VMware虚拟机软件中运行3台虚拟机,分别命名为VM1、VM2、VM3.其中VM1内运行OFSoftSwitch软件交换机,通过OpenFlow通道与主机内的Floodlight控制器连接.VM2和VM3分别作为源主机和目的主机,并处于不同的子网,只有通过控制器配置流规则,两者才可进行通信.由于本文假设满足不等式(1),仅需考虑第1跳交换机的缓存情况即可,所以该实验场景简化是合理的.图7基于OFSoftSwitch的PiBuffer原型验证环境首先本文对PiBuffer减少了交换机与控制器之间的通信开销进行验证.使用Iperf工具进行测试,将VM2作为源主机,VM3作为目的主机,测量在不同缓存模式下(无缓存、通用缓存、基于PiBuffer的缓存)Floodlight控制器与OFSoftSwitch软件的负载情况.Iperf的流量大小为5Mbps,持续时间为60s,测试结果如图8所示.由于实验中Floodlight控制器并未及时发出Flow_mod消息下载规则,因此在60s内,所有发送至软件交换机的报文都未命中规则.接收带宽与发送带宽均是从交换机的角度而言,分别对应Packet_out消息和Packet_in消息.从结果中可明显看到,基①http://cpqd.github.com/ofsoftswitch13Page9图83种缓存模式占用OpenFlow通道带宽的对比于PiBuffer缓存模式对OpenFlow通道带宽占用的最少,平均值分别约为23KB/s(接收)和74KB/s(发送).基于通用缓存的次之,平均值分别约为43KB/s(接收)和139KB/s(发送).无缓存的情况下,所有的报文都被封装成Packet_in消息送至控制器,并且也会被Packet_out至交换机,平均值分别约为719KB/s(接收)和761KB/s(发送).其接收与发送的占用带宽基本与Iperf所发出的流量的带宽(5Mbps)处于一个量级.无缓存情况下总共占用带宽与通用缓存情况下总共占用带宽比值约为15,与第3节中理论分析一致.在有缓存的情况下,Packet_out消息只需携带32位的Buffer_id,不需要携带报文相关信息,经过计算可知,仅为Packet_in消息的1/3,这也符合实验中所观测到接受速率与发送速率的比值.大量的OpenFlow控制消息还可造成控制器与交换机的过载.对控制通路的带宽占用减少可降低交换机与控制器的CPU利用率.图9反应了引入PiBuffer后对软件交换机占用CPU利用率的影响.通过数据可知,采用PiBuffer模式的软件交换机要比原先更少地占用CPU资源.这是由于PiBuffer缓存管理模型使得更少的报文被发送至控制器,同一条流的非首报文会被缓存在交换机中.而封装产生Packet_in消息相比于缓存报文更消耗CPU资源.PiBuffer更多地将未命中的报文缓存在交换机内部,减少了发往控制器的Packet_in消息,因此相比于通用缓存模式,PiBuffer模式下交换机的CPU资源使用率减少约20%.可以预见,交换机引入PiBuffer会使控制器CPU资源占用较少.所以,从以上实验结果可知,在交换机缓存资源足够的前提下,缓存报文不仅可以减少交换机与控制器之间的通信负载,同时可减少交换机与控制器图93种缓存模式占用软件交换机CPU资源的对比本身的CPU处理负载.其次,本文对PiBuffer解决通用缓存导致的流内乱序问题进行说明和验证.源主机向目的主机连续发送携带顺序序号(1~100)的报文.分析目的主机端接收到的报文序号,结果如图10所示.图10目的主机收到报文序号随时间的关系从2.1节中论述可知,面向连接的TCP报文流在数据中心网络流特性的条件下,在到达Open-Flow交换机时,也可能存在无法命中流规则的情况.从上图实验结果可知,通用缓存会带来较大的流内报文乱序问题.接收到的首报文序号为1,第2个报文为64,依次至97,而后又收到序号为2的报文,这会严重降低TCP流的传输带宽.而对于应用了PiBuffer的交换机,则报文流不会产生报文乱序的情况.5.2基于NetMagic平台的硬件实现本文基于NetMagic可编程硬件网络实验平台实现该缓存管理模型.NetMagic平台的FPGA硬件逻辑可分为报文接收、发送、复制等基本通用逻辑和自定义开发逻辑,其中基本通用处理逻辑为自定义逻辑提供清晰的易用接口[19].缓存模型的逻辑实现如图11所示.Page10图11基于NetMagic的PiBuffer硬件实现逻辑本实验目的是为了测量不同规则装载延迟下(即上文tS1→C+tC+tC→S1中报文的缓存使用情况.搭建了如图12所示的1个原型系统.图中3台PC,1台为控制器,其余两台为报文的发送方A和接收方B;1台基于NetMagic平台实现PiBuffer的OpenFlow交换机.为简化实现但不影响实验结果,该交换机的查表过程为简化后的OpenFlow1.0标准.表2给出了实现该缓存模型所需的FPGA资源使用情况.从资源的占用比例可知,实现流缓存管理功能所需要的FPGA资源并不多,硬件实现可行.在缓存报文时按照2KB大小为每个报文分配缓存空间.资源名称查找表LUT36100寄存器存储器位29399041728640(58.8%)74496(2.5%)实验过程如下:首先由发送端A主机使用VLC软件流化一个视频流向接收端B主机发送,当视频流的首报文流经过NetMagic时,NetMagic将为该流创建一个流缓存表项,并将该流的缓存信息封装成Packet-in发往控制器.然后,当Packet-in到达控制器,通过设定控制器下发流表项的延时,来模拟远端控制器响应延时.延时以50ms为单位,设定9个延时间隔,最大延时设定为500ms.在这个延时期间内,NetMagic将缓存报文信息更新到流缓存表中.另外,由于不同大小的报文的缓冲区大小相同,因此,测试会关心控制器延时期间的缓存报文个数,而缓存报文个数记录在该流的缓存表项中.最后,当控制器下发的Flow-mod到达NetMagic时,由NetMagic读取在流缓存表项中记录的该流的流缓存报文个数字段,并将其封装成特殊报文发往主机B,这样缓存报文的个数就可以通过主机B捕获的特殊报文获得.图13测试结果显示,缓存报文大小随控制器延时的增加成线性增长,其中测试曲线的抖动是由于发送端软件流化视频流产生的速率抖动造成的.Page116相关研究避免集中的控制平面成为网络性能的瓶颈是SDN在数据中心中落地部署所要解决的重要问题之一.目前研究人员主要从两个方面解决该问题:一类是提升控制平面的处理能力;另一类是增加数据平面的功能,尽可能减少控制平面与数据平面的非重要信息的交互.在提升控制平面的处理能力方面,SDN控制器的研发人员运用多线程并行技术、共享队列、批处理等提升单点控制器的报文处理能力[20].但由于单点控制器处理性能有上限,为此研究人员提出利用分布式技术提高控制平面处理能力.HyperFlow[7]通过部署多台分布式控制器来分摊与数据平面的交互开销,各控制器间通过消息订阅/发布方式获取全网的统一视图,从而实现逻辑一致性和集中控制.Onix[8]面向商业化应用,在更高层次上提出了控制平面扩展的分层架构,全局网络状态一致性通过每个控制器上的网络信息库来维护.ONOS[9]是近年来发展较为成熟的开源分布式SDN控制平台.以上研究均可与本文工作互补,提高控制平面的可扩展能力.在OpenFlow数据平面功能增强方面,与本文工作最接近的是Kotani等人[10]提出的Packet_in消息过滤机制.同PiBuffer类似,该机制在发送Packet_in消息之前会记录报文头域的值.但不同的是,过滤机制记录的值用于过滤后续相同值的报文,降低高速报文对控制平面的影响.而PiBuffer并不丢弃这些报文,而是进行缓存,减少丢包带来的影响.此外,DIFANE[11]提出区分权威OpenFlow交换机和普通OpenFlow交换机,将OpenFlow控制平面工作部分卸载到权威OpenFlow交换机上,各普通OpenFlow交换机根据控制平面下发的分区规则与对应的权威OpenFlow交换机通信,再由权威OpenFlow交换机根据控制平面下发的权威规则与控制平面交互,从而减少控制平面与数据平面间的交互开销.另外,分区规则与权威规则只有在网络拓扑结构发生变化时,才被动地更新,可进一步减轻控制平面负载.DevoFlow[3]则在数据平面采用规则复制和局部操作的方式来减小与控制平面的交互,并使用触发、报告、采样等手段进一步减小数据平面的统计信息到控制平面的开销,从而提高SDN控制平面的可扩展性.综上分析,增强控制平面的处理能力方案的优点是通用性强,对SDN网络体系结构影响较小,缺点是SDN网络内部仍充斥大量控制器与交换机间的消息,难以降低OpenFlow消息对网络资源占用的开销.而扩展数据平面功能的方案,则可以从根源上减少控制器和SDN网络的流量负载.本文所研究的OpenFlow交换机流缓存管理技术,属于数据平面增强的方案,实现了OpenFlow规范中指定的优化方法,可以在提高控制平面可扩展性的同时,实现与现有SDN网络兼容互通.7结论与下一步工作为减少基于OpenFlow的数据中心中控制平面与数据平面的交互,提高控制平面的可扩展性,本文利用交换机中的存储资源,提出了一种流粒度的未命中报文缓存管理模型PiBuffer.原有的缓存模式缺乏对未命中报文的有效组织和管理,并没有充分利用缓存对减少流建立开销带来的好处.该模型通过PiBT表记录未命中报文的标识信息,区分同一条流的首报文和后续报文,使得同一条未命中报文流只产生一个Packet-in消息发送到控制器.而且使得流表下发时,先发送缓存的报文,从而实现流内报文保序.实验结果表明,PiBuffer与现有的缓存方式相比,降低了对OpenFlow通道的占用带宽和交换机的CPU开销,实现报文按序转发.实验中发现PiBuffer占用较多的交换机缓存资源且影响了转发效率,下一步工作将对报文缓存大小进行评估并对缓存时的转发性能进行优化.致谢网络与信息安全研究所662教研室的徐东来老师、熊兆中同学在代码调试过程中提供了指导和帮助,在此表示感谢!Page12
