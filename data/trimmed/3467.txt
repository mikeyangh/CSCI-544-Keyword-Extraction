Page1架构大数据:挑战、现状与展望王珊1),2)王会举1),2)覃雄派1),2)周?1),2)1)(数据工程与知识工程教育部重点实验室(中国人民大学)北京100872)2)(中国人民大学信息学院北京100872)摘要大数据分析相比于传统的数据仓库应用,具有数据量大、查询分析复杂等特点.为了设计适合大数据分析的数据仓库架构,文中列举了大数据分析平台需要具备的几个重要特性,对当前的主流实现平台———并行数据库、MapReduce及基于两者的混合架构进行了分析归纳,指出了各自的优势及不足,同时也对各个方向的研究现状及作者在大数据分析方面的努力进行了介绍,对未来研究做了展望.关键词大数据;大规模可扩展;MapReduce;并行数据库;深度分析1引言最近几年,数据仓库又成为数据管理研究的热点领域,主要原因是当前数据仓库系统面临的需求在数据源、需提供的数据服务和所处的硬件环境等方面发生了根本性的变化(详见1.1节),这些变化是我们必须面对的.本文在大数据的时代背景下,对现有数据仓库系统实现方案(主要是并行数据库和MapReduce)进行重新审视,期望能为设计满足时代需求的数据仓库系统提供理论参考.限于篇幅,本文主要关注不同数据仓库实现方案的主体架构及其缺陷在最近几年的改进情况.依据研究立足点的不同,本文将该领域的研究归为三大类:并行数据库、MapReduce、并行数据库和MapReduce技术的混合架构.其中第三类研究又细分为:并行数据库主导型、MapReduce主导型、并行数据库和MapReduce集成型三种.本Page2文第1节分析大数据时代,数据仓库所面临的问题及挑战;第2节列出大数据时代的数据仓库平台需具备的几个重要特性;第3节到第5节就这几个特性对各类平台进行归纳分析;第6节对最新研究做一跟踪归纳;第7节介绍中国人民大学在大数据分析方面的研究工作;第8节对未来研究做出展望;第9节总结全文.1.1三个变化(1)数据量.由TB级升至PB级,并仍在持续爆炸式增长.根据WinterCorp的调查显示,最大的数据仓库中的数据量,每两年增加3倍[1](年均增长率为173%),其增长速度远超摩尔定律增长速度.照此增长速度计算,2015年最大数据仓库中的数据量将逼近100PB.(2)分析需求.由常规分析转向深度分析(DeepAnalytics).数据分析日益成为企业利润必不可少的支撑点.根据TDWI对大数据分析的报告[2](如图1),企业已经不满足于对现有数据的分析和监测,而是更期望能对未来趋势有更多的分析和预测,以增强企业竞争力.这些分析操作包括诸如移动平均线分析、数据关联关系分析、回归分析、市场篮分析等复杂统计分析,我们称之为深度分析.值得补充的是,本文中的大数据分析不仅仅指基于大数据上的深度分析,也包括常规分析.(3)硬件平台.由高端服务器转向由中低端硬件构成的大规模机群平台.由于数据量的迅速增加,并行数据库的规模不得不随之增大,从而导致其成本的急剧上升.出于成本的考虑,越来越多的企业将应用由高端服务器转向了由中低端硬件构成的大规模机群平台.1.2两个问题图2是一个典型的数据仓库架构[3].从图中我们可以看出,传统的数据仓库将整个实现划分为4个层次,数据源中的数据首先通过ETL工具被抽取到数据仓库中进行集中存储和管理,再按照星型模型或雪花模型组织数据,然后OLAP工具从数据仓库中读取数据,生成数据立方体(MOLAP)或者直接访问数据仓库进行数据分析(ROLAP).在大数据时代,此种计算模式存在两个问题:问题1.数据移动代价过高.在数据源层和分析层之间引入一个存储管理层,可以提升数据质量并针对查询进行优化,但也付出了较大的数据迁移代价和执行时的连接代价:数据首先通过复杂且耗时的ETL过程存储到数据仓库中,在OLAP服务器中转化为星型模型或者雪花模型;执行分析时,又通过连接方式将数据从数据库中取出.这些代价在TB级时也许可以接受,但面对大数据,其执行时间至少会增长几个数量级.更为重要的是,对于大量的即席分析,这种数据移动的计算模式是不可取的.问题2.不能快速适应变化.传统的数据仓库假设主题是较少变化的,其应对变化的方式是对数据源到前端展现的整个流程中的每个部分进行修改,然后再重新加载数据,甚至重新计算数据,导致其适应变化的周期较长.这种模式比较适合对数据质量和查询性能要求较高、而不太计较预处理代价的场合.但在大数据时代,分析处在变化的业务环境中,这种模式将难以适应新的需求.1.3一个鸿沟在大数据时代,巨量数据与系统的数据处理能力之间将会产生一个鸿沟:一边是至少PB级的数据量,另一边是面向传统数据分析能力设计的数据仓库和各种BI工具.如果这些系统或工具发展缓慢,该鸿沟将会随着数据量的持续爆炸式增长而逐步拉大.虽然,传统数据仓库可以采用舍弃不重要数据或者建立数据集市的方式来缓解此问题,但毕竟只Page3是权益之策,并非系统级解决方案.而且,舍弃的数据在未来可能会重新使用,以发掘更大的价值.2期望特性本节我们列出对大数据进行分析时,数据仓库系统需具备的几个重要特性(表1所示).特性高度可扩展性横向大规模可扩展,大规模并行处理高性能高度容错性查询失败时,只需重做部分工作支持异构环境对硬件平台一致性要求不高,适应能力强较低的分析延迟业务需求变化时,能快速反应易用且开放接口既能方便查询,又能处理复杂分析较低成本向下兼容性支持传统的商务智能工具高度可扩展性.一个明显的事实是,数据库不能依靠一台或少数几台机器的升级(scale-up纵向扩展)满足数据量的爆炸式增长,而是希望能方便地做到横向可扩展(scale-out)来实现此目标.普遍认为shared-nothing无共享结构(每个节点拥有私有内存和磁盘,并且通过高速网络同其它节点互连)具备较好的扩展性[4].分析型操作往往涉及大规模的并行扫描、多维聚集及星型连接操作,这些操作也比较适合在无共享结构的网络环境运行.Teradata即采用此结构,Oracle在其新产品Exadata中也采用了此结构.高性能.数据量的增长并没有降低对数据库性能的要求,反而有所提高.软件系统性能的提升可以降低企业对硬件的投入成本、节省计算资源,提高系统吞吐量.巨量数据的效率优化,并行是必由之路.1PB数据在50MB/s速度下串行扫描一次,需要230天;而在6000块磁盘上,并行扫描1PB数据只需要1个小时.高度容错.大数据的容错性要求在查询执行过程中,一个参与节点失效时,不需要重做整个查询.而机群节点数的增加会带来节点失效概率的增加.在大规模机群环境下,节点的失效将不再是稀有事件(Google报告,平均每个MapReduce数据处理任务就有1.2个工作节点失效[5]).因此在大规模机群环境下,系统不能依赖于硬件来保证容错性,要更多地考虑软件级容错.支持异构环境.建设同构系统的大规模机群难度较大,原因在于计算机硬件更新较快,一次性购置大量同构的计算机是不可取的,而且也会在未来添置异构计算资源.此外,不少企业已经积累了一些闲置的计算机资源,此种情况下,对异构环境的支持可以有效地利用这些闲置计算资源,降低硬件成本的投入.还需特别关注的是,在异构环境下,不同节点的性能是不一样的,可能出现“木桶效应”,即最慢节点的性能决定整体处理性能.因此,异构的机群需要特别关注负载均衡、任务调度等方面的设计.较低的分析延迟.分析延迟指的是分析前的数据准备时间.在大数据时代,分析所处的业务环境是变化的,因此也要求系统能动态地适应业务分析需求.在分析需求发生变化时,减少数据准备时间,系统能尽可能快地做出反应,快速地进行数据分析.易用且开放的接口.SQL的优点是简单易用,但其主要用于数据的检索查询,对于大数据上的深度分析来讲,是不够的.原因在于:(1)其提供的服务方式依赖于数据移动来实现:将数据从数据库中取出,然后传递给应用程序,该实现方式在大数据时代代价过高;(2)复杂的分析功能,如R或Matlab中的分析功能,SQL是难以胜任的.因此,除对SQL的支持外,系统还应能提供开放的接口,让用户自己开发需要的功能.设计该接口时,除了关注其易用性和开放性,还需要特别注意两点隐藏的要求:(1)基于接口开发的用户自定义函数,能自动在机群上并行执行;(2)分析在数据库内进行,即分析尽可能靠近数据.较低的成本.在满足需求的前提下,某技术成本越低,其生命力就越强.需要指出的是成本是一个综合指标,不仅仅是硬件或软件的代价,还应包括日常运维成本(网络费用、电费、建筑等)和管理人员成本等.据报告,数据中心的主要成本不是硬件的购置成本,而是日常运维成本.因此,在设计系统时需要更多地关注此项内容.向下兼容性.数据仓库发展的30年,产生了大量面向客户业务的数据处理工具(如Informactica、DataStage等)、分析软件(如SPSS、R、Matlab等)和前端展现工具(如水晶报表)等.这些软件是一笔宝贵的财富,已被分析人员所熟悉,是大数据时代中小规模数据分析的必要补充.因此,新的数据仓库需考虑同传统商务智能工具的兼容性.由于这些系统往往提供标准驱动程序,如ODBC、JDBC等,这项需求的实际要求是对SQL的支持.总之,以较低的成本投入、高效地进行数据分析,是大数据分析的基本目标.Page43并行数据库并行数据库起源于20世纪80年代,当前主流的并行数据库都同早期的Gamma[6]和Grace[7]等并行数据库类似.这些数据库都支持标准SQL,并且实现了数据库界过去30年提出的许多先进技术.其主要采用shared-nothing结构,将关系表在节点间横向划分,并且利用优化器来对执行过程进行调度和管理.其目标是高性能和高可用性.并行数据库的最大优势在于性能.这主要得益于数据库界近几十年的研究成果———许多先进的技术手段及算法,如索引、数据压缩、物化视图、结果缓冲、I/O共享、优化的数据连接等.但是在大数据时代,如前言所述,数据移动的实现方式将影响其性能.并行数据库通过SQL向外提供数据访问服务,SQL因其简单易用的特点而被广泛使用.因此,大多BI工具都支持基于标准SQL的数据交互方式,使得关系数据库能较好地兼容当前多数BI工具.某些数据库,如IBMDB2还针对一些BI工具进行了优化.但在大数据分析面前,SQL接口面临巨大挑战.SQL的优势源于其对底层数据访问的封装,但封装在一定程度上影响了其开放性.而且并行数据库提供的用户自定义函数大都是基于单数据库实例设计的,从而不能在机群上并行执行,也即意味着传统的实现方式不适合大数据的处理及分析.而且,在并行数据库中实现用户自定义函数往往需要经过复杂的系统交互,甚至要熟悉数据库的内部结构及系统调用等,从而难以使用.并行数据库在扩展性、容错性、成本、对异构环境的支持等几项上有所欠缺.这几项实际是相互影响的,我们以其最大问题———扩展性为主线展开讨论.并行数据库大多支持有限扩展,一般可扩至数百节点的规模,尚未有数千节点规模的应用案例.并行数据库扩展性有限主要因为如下几点:(1)并行数据库软件级容错能力较差.并行数据库基于高端硬件设计,并且假设查询失败属于稀有事件.因此当查询失败时,一般采取重做查询的方式.而在大规模机群环境下,查询失败将会变为一个普通事件.极端情况下,并行数据有可能出现不停重做查询的局面;(2)并行数据库对异构硬件的支持非常有限,且对于处理较慢的节点反应敏感,容易出现“木桶效应”.如第2节中所论述的,完全基于同构硬件搭建大规模机群在现实中是较难实现的.因而,对异构硬件的支持能力影响了其扩展性;(3)并行数据库若做到大规模可扩展,其代价将会较高(需基于高端硬件来保证可靠性,需购买昂贵的软件系统),从而限制了其扩展性;(4)根据CAP理论①[8],在分布式系统中,数据一致性(Consistency)、可用性(Availability)、子网可分解性(NetworkPartitioning)不可同时兼得,选择其中任两项,便会损害另一项.并行数据库追求的是数据一致性和系统的可用性,从而影响了它的扩展能力.此外,如1.2节所讨论的,基于并行数据库实现的传统数据仓库借助于外围工具(ETL工具、OLAP产品、BI报表工具、统计分析软件等)来完成数据的预处理和分析展现任务,导致其数据处理及分析过程涉及大量的数据迁移和计算,分析延迟往往较高.4MapReduceMapReduce[5]是2004年由Google提出的面向大数据集处理的编程模型,起初主要用作互联网数据的处理,例如文档抓取、倒排索引的建立等.但由于其简单而强大的数据处理接口和对大规模并行执行、容错及负载均衡等实现细节的隐藏,该技术一经推出便迅速在机器学习、数据挖掘、数据分析等领域得到广泛应用[9].MapReduce将数据处理任务抽象为一系列的Map(映射)-Reduce(化简)操作对.Map主要完成数据的过滤操作,Reduce主要完成数据的聚集操作.输入输出数据均以〈key,value〉格式存储.用户在使用该编程模型时,只需按照自己熟悉的语言实现Map函数和Reduce函即可,MapReduce框架会自动对任务进行划分以做到并行执行.下面本文将以基于MapReduce的开源实现Hadoop[10]为主,对其主要特性进行介绍.MapReduce是面向由数千台中低端计算机组成的大规模机群而设计的,其扩展能力得益于其shared-nothing结构、各个节点间的松耦合性和较强的软件级容错能力:节点可以被任意地从机群中移除,而几乎不影响现有任务的执行.该技术被称为RAIN(Redundant/ReliableArrayofIndependent(andInexpensive)Nodes).MapReduce卓越的扩展能力已在工业界(Google、Facebook、Baidu、Taobao①该理论目前尚存争议.Page5等)得到了充分验证.MapReduce对硬件的要求较低,可以基于异构的廉价硬件来搭建机群,且免费开源,因此其构建成本低于并行数据库.但基于MapReduce的应用软件相对较少,许多数据分析功能需要用户自行开发,从而会导致使用成本的增加.作为开源系统,MapReduce具有完全的开放性:其〈key,value〉存储模型具有较强的表现力,可以存储任意格式的数据;Map和Reduce两个基本的函数接口也给用户提供了足够的发挥空间,可以实现各种复杂的数据处理功能.但这种开放性也带来一个问题,就是将本来应由数据库管理系统完成的工作,诸如文件存储格式的设计、模式信息的记录、数据处理算法的实现等,转移给了程序员,从而导致程序员负担过重.程序员水平对系统处理性能起决定性作用.在某些情况下,写MapReduce程序的时间远大于写SQL语句的时间,部分复杂的BI报表分析,可能仅程序的编写和调试就要耗费几天的时间.基于MapReduce平台的分析,无需复杂的数据预处理和写入数据库的过程,而是可以直接基于平面文件进行分析,并且其采用的计算模式是移动计算而非移动数据,因此可以将分析延迟最小化.在同等硬件条件下,MapReduce性能远低于并行数据库[11],这是由其最初的设计定位决定的.MapReduce的设计初衷是面向非结构化数据的处理.这些数据具有数据量大,处理复杂等特点,而且往往是一次性处理.为了获得较好的扩展能力和容错能力,MapReduce采取了基于扫描的处理模式和对中间结果步步物化的执行策略,从而导致较高的I/O代价.为了减少数据预处理时间,MapReduce没有使用模式、索引、物化视图等技术手段.其数据预处理仅是一次数据加载操作,但由此导致了一个问题———较高的元组解析代价[12].在MapReduce表2混合架构型解决方案对比分析并行数据库主导型MapReduce主导型并行数据库和MapReduce集成型集成两者,使两者各自做各自擅长的工作5.1并行数据库主导型该种方式关注于如何利用MapReduce来增强并行数据库的数据处理能力.代表性系统是Greenplum环境下,每个查询都是直接从文件系统中读入原始数据文件,而非传统的从数据库中读入经处理过的文件,因此其元组解析代价远高于关系数据库.对数据分析领域来说,连接是关键操作(如传统的星型查询和雪花查询均是依赖于连接来处理查询),但MapReduce处理连接的性能尤其不尽如人意.原因在于MapReduce最初是针对单数据集设计的处理模型,而连接操作往往涉及多个数据集.在利用MapReduce实现连接时,最直接的方式是每个任务执行一个属性上的连接操作,然后将多个MapReduce任务通过物化的中间结果串接起来.这种实现方式往往涉及中间结果的读写,从而导致大量的I/O操作和网络传输.MapReduce目前基本不兼容现有的BI工具.原因在于其初衷并不是要成为数据库系统,因此它并未提供SQL接口.但已有研究致力于SQL语句与MapReduce任务的转换工作(例如Hive),进而有可能实现MapReduce与现存BI工具的兼容.5并行数据库和MapReduce的混合架构基于以上分析,我们可以清楚地看出,基于并行数据库和MapReduce实现的数据仓库系统都不是大数据分析的理想方案.针对两者哪个更适合时代需求的问题,业界近年展开了激烈争论.当前基本达成如下共识:并行数据库和MapReduce是互补关系,应该相互学习[13-14].基于该观点,大量研究着手将两者结合起来,期望设计出兼具两者优点的数据分析平台.这种架构又可以分为三类:并行数据库主导型、MapReduce主导型、MapReduce和并行数据库集成型(表2对3种架构进行了对比分析).(已被EMC收购)和AsterData(已被Teradata收购).AsterData将SQL和MapReduce进行结合,针对大数据分析提出了SQL/MapReduce框架[15].Page6该框架允许用户使用C++、java、Python等语言编写MapReduce函数,编写的函数可以作为一个子查询在SQL中使用,从而同时获得SQL的易用性和MapReduce的开放性.不仅如此,AsterData基于MapReduce实现了30多个统计软件包,从而将数据分析推向数据库内进行(数据库内分析),大大提升了数据分析的性能.Greenplum也在其数据库中引入了MapReduce处理功能[16].其执行引擎可以同时处理SQL查询和MapReduce任务.这种方式在代码级整合了SQL和MapReduce:SQL可以直接使用MapReduce任务的输出,同时MapReduce任务也可以使用SQL的查询结果作为输入.总的来说,这些系统都集中于利用MapReduce来改进并行数据库的数据处理功能,其根本性问题———可扩展能力和容错能力并未改变.5.2MapReduce主导型该方向的研究主要集中于利用关系数据库的SQL接口和对模式的支持等技术来改善MapReduce的易用性,代表系统是Hive[17]、PigLatin[18]等.Hive是Facebook提出的基于Hadoop的大型数据仓库,其目标是简化Hadoop上的数据聚集、ad-hoc查询及大数据集的分析等操作,以减轻程序员的负担.它借鉴关系数据库的模式管理、SQL接口等技术,把结构化的数据文件映射为数据库表,提供类似于SQL的描述性语言HiveQL供程序员使用,可自动将HiveQL语句解析成一优化的Ma-pReduce任务执行序列.此外,它也支持用户自定义的MapReduce函数.PigLatin是Yahoo!提出的类似于Hive的大数据集分析平台.两者的区别主要在于语言接口.Hive提供了类似SQL的接口,PigLatin提供的是一种基于操作符的数据流式的接口.图3是PigLatin在处理查询时的一个操作实例.该查询的目的是找出“年龄在18~25周岁之间的用户(Users)最频繁访问的5个页面(Pages)”.从图3可以看出,Pig提供的操作接口类似于关系数据库的操作符(对应图中右侧部分中的每一行命令),用户查询的脚本类似于逻辑查询计划(对应图中左侧部分).因此,也可以说Pig利用操作符来对Hadoop进行封装,Hive利用SQL进行封装.5.3MapReduce和并行数据库集成型该方向的代表性研究是耶鲁大学提出的HadoopDB[19](已于2011年商业化为Hadapt[20])、图3PigLatin的一个查询示例(右边为实际脚本)Stonebraker等人设计的Vertica[21]数据库和NCR公司的Teradata[22]数据库.HadoopDB的核心思想是利用Hadoop作为调度层和网络沟通层,关系数据库作为执行引擎,尽可能地将查询压入数据库层处理.目标是想借助Hadoop框架来获得较好的容错性和对异构环境的支持;通过将查询尽可能推入数据库中执行来获得关系数据库的性能优势.HadoopDB的思想是深远的,但目前尚无应用案例,原因在于:(1)其数据预处理代价过高:数据需要进行两次分解和一次数据库加载操作后才能使用;(2)将查询推向数据库层只是少数情况,大多数情况下,查询仍由Hive完成.因为数据仓库查询往往涉及多表连接,由于连接的复杂性,难以做到在保持连接数据局部性的前提下将参与连接的多张表按照某种模式划分;(3)维护代价过高.不仅要维护Hadoop系统,还要维护每个数据库节点;(4)目前尚不支持数据的动态划分,需要手工方式将数据一次性划分好.总的来说,Ha-doopDB在某些情况下,可以同时实现关系数据库的高性能特性和MapReduce的扩展性、容错性,但同时也丧失了关系数据库和MapReduce的某些优点,比如MapReduce较低的预处理代价和维护代价、关系数据库的动态数据重分布等.Vertica采用的是共存策略:根据Hadoop和Vertica各自的处理优势,对数据处理任务进行划分.比如Hadoop负责非结构化数据的处理,Vertica负责结构化数据的处理;Hadoop负责耗时的批量复杂处理,Vertica负责高性能的交互式查询等,从而将两者结合起来.Vertica实际采用的是两套系统,同时支持在MapReduce任务中直接访问Vertica数据库中的数据.由于结构化数据仍在Vertica中处理,在处理结构化大数据上的查询分析时,仍面临扩展性问题;如果将查询推向Hadoop进行,又将面临性能问题.因此,Vertica的扩展性问题和Hadoop的性能问题在该系统中共存.Page7与前两者相比,Teradata的集成相对简单.Teradata采用了存储层的整合:MapReduce任务可以从Teradata数据库中读取数据,Teradata数据库也可以从Hadoop分布式文件系统上读取数据.同样,Teradata和Hadoop各自的根本性问题都未解决.6研究现状对并行数据库来讲,其最大问题在于有限的扩展能力和待改进的软件级容错能力;MapReduce的最大问题在于性能,尤其是连接操作的性能;混合式架构的关键是①,如何能尽可能多地把工作推向合适的执行引擎(并行数据库或MapReduce).本节对近年来在这些问题上的研究做一分析和归纳.6.1并行数据库扩展性和容错性研究华盛顿大学在文献[23]中提出了可以生成具备容错能力的并行执行计划优化器.该优化器可以依靠输入的并行执行计划、各个操作符的容错策略及查询失败的期望值等,输出一个具备容错能力的并行执行计划.在该计划中,每个操作符都可以采取不同的容错策略,在失败时仅重新执行其子操作符(在某节点上运行的操作符)的任务来避免整个查询的重新执行.MIT于2010年设计的Osprey系统[24]基于维表在各个节点全复制、事实表横向切分并冗余备份的数据分布策略,将一星型查询划分为众多独立子查询.每个子查询在执行失败时都可以在其备份节点上重新执行,而不用重做整个查询,使得数据仓库查询获得类似MapReduce的容错能力.数据仓库扩展性方面的研究较少,中国人民大学的LinearDB原型属于这方面的研究,详细参见7.1节.6.2MapReduce性能优化研究MapReduce的性能优化研究集中于对关系数据库的先进技术和特性的移植上.Facebook和俄亥俄州立大学合作,将关系数据库的混合式存储模型应用于Hadoop平台,提出了RCFile存储格式[25].与之不同,文献[26]将列存储技术引入Hadoop平台.Hadoop++[27]系统运用了传统数据库的索引技术,并通过分区数据并置(Co-Partition)的方式来提升性能.文献[28-29]基于MapReduce实现了以流水线方式在各个操作符间传递数据,从而缩短了任务执行时间;在线聚集(onlineaggregation)的操作模式使得用户可以在查询执行过程中看到部分较早返回的结果.两者的不同之处在于前者仍基于sort-merge方式来实现流水线,只是将排序等操作推向了reducer,部分情况下仍会出现流水线停顿的情况;而后者利用hash方式来分布数据,能实现更好的并行流水线操作.文献[30]提出了MRShare架构,对批量查询进行转换,将可共享扫描、共享Map输出结果等的一组任务合并为一个,以提升性能.新加坡国立大学对影响Hadoop性能的因素做了深入分析[12],并提出了5项有效的优化技术,使得Hadoop的性能提升了近3倍,逼近关系数据库的性能.近年的研究热点是基于MapReduce的连接操作的性能优化.文献[31]对MapReduce平台的两表连接算法做了总结,提出了Map端连接、Reduce端连接及广播式连接等算法.文献[32]对MapReduce框架进行了扩展,在Reduce步骤后添加了一Merge步骤来完成连接操作,提出的Map-Reduce-Merge框架可以同时处理两个异构数据源的数据.对于多表连接,当前主流的研究集中于仅通过一个任务来完成连接操作.文献[33-34]提出了一对多复制的方法,在Map阶段结束后,为保证连接操作的局部性,元组会被复制到多个节点.但在节点数和数据量增大的情况下,会带来I/O量及网络传输量的巨大增长.Llama[35]通过预排序和按连接属性划分数据的方式来降低星型连接的代价,但要付出可观的预处理代价和空间代价.不同于以上等值连接优化,文献[36]提出了针对任意连接条件的优化模型.以上连接方式都是先执行连接,然后在连接后的数据上执行聚集操作.而中国人民大学的Dumbo[37]系统却采用了另一种更适应于MapReduce平台的思路:先执行过滤聚集操作,再基于聚集的数据执行连接.详细参考7.2节.6.3HadoopDB的改进连接优化技术和两种聚集优化技术[38].HadoopDB于2011年针对其架构提出了两种两种连接优化的核心思想都是尽可能地将数据的处理推入数据库层执行.第1种优化方式是根据表与表之间的连接关系,通过数据预分解,使参与连接的数据尽可能分布在同一数据库内(参照分解法),从而实现将连接操作下压进数据库内执行.该算法的缺点是应用场景有限,只适用于链式连接.第①其最大问题既包括扩展性也包括性能,这两项分别取决于Page82种连接方式是针对广播式连接而设计的.在执行连接前,先在数据库内为每张参与连接的维表建立一张临时表,使得连接操作尽可能在数据库内执行.该算法的缺点是较多的网络传输和磁盘I/O操作.两种聚集优化技术分别是连接后聚集和连接前聚集.前者是执行完Reduce端连接后,直接对符合条件的记录执行聚集操作;后者是将所有数据先在数据库层执行聚集操作,然后基于聚集数据执行连接操作,并将不符合条件的聚集数据做减法操作.该方式适用的条件有限,主要用于参与连接和聚集的列的基数相乘后小于表记录数的情况.总的来看,HadoopDB的优化技术大都局限性较强,对于复杂的连接操作(如环形连接等)仍不能下推至数据库层执行,并未从根本上解决其性能问题.7MapReduce和关系数据库技术的融合综上所述,当前研究大都集中于功能或特性的移植,即从一个平台学习新的技术,到另一平台重新实现和集成,未涉及执行核心,因此也没有从根本上解决大数据分析问题.鉴于此,中国人民大学高性能数据库实验室的研究小组采取了另一种思路:从数据的组织和查询的执行两个核心层次入手,融合关系数据库和MapReduce两种技术,设计高性能的可扩展的抽象数据仓库查询处理框架.该框架在支持高度可扩展的同时,又具有关系数据库的性能.我们团队尝试过两个研究方向:(1)借鉴MapReduce的思想,使OLAP查询的处理能像MapReduce一样高度可扩展(LinearDB原型);(2)利用关系数据库的技术,使MapReduce在处理OLAP查询时,逼近关系数据库的性能(Dumbo原型).7.1LinearDBLinearDB①[39]原型系统没有直接采用基于连接的星型模型(雪花模型),而是对其进行了改造,设计了扩展性更好的、基于扫描的无连接雪花模型JFSS(Join-FreeSnowflakeSchema).该模型的设计借鉴了泛关系模型的思想,采用层次编码技术[40]将维表层次信息压缩进事实表,使得事实表可以独立执行维表上的谓词判断、聚集等操作,从而使连接的数据在大规模机群上实现局部性,消除了连接操作.图4是一个星型模型和无连接雪花模型的对应示意图.在执行层次上,LinearDB吸取了MapReduce处理模式的设计思想,将数据仓库查询的处理抽象为Transform、Reduce、Merge3个操作(TRM执行模型):(1)Transform.主节点对查询进行预处理,将查询中作用于维表的操作(主要是谓词判断,group-by聚集操作等)转换为事实表上的操作;(2)Reduce.每个数据节点并行地扫描、聚集本地数据,然后将处理结果返回给主节点;(3)Merge.主节点对各个数据节点返回的结果进行合并,并执行后续的过滤、排序等操作.基于TRM执行模型,查询可以划分为众多独立的子任务在大规模机群上并行执行.执行过程中,任何失败子任务都可以在其备份节点重新执行,从而获得较好的容错能力.LinearDB的执行代价主要取决于对事实表的Reduce(主要是扫描)操作,因此,LinearDB可以获得近乎线性的大规模可扩展能力.实验表明,其性能比HadoopDB至少高出一个数量级②.LinearDB的扩展能力、容错能力和高性能在于其巧妙地结合了关系数据库技术(层次编码技术、泛关系模式)和MapReduce处理模式的设计思想,由此,可以看出,结合方式的不同可以导致系统能力的巨大差异.7.2DumboDumbo[37]的核心思想是根据MapReduce的“过滤->聚集”的处理模式,对OLAP查询的处理进行改造,使其适应于MapReduce框架.Dumbo采用了类似于LinearDB的数据组织模式———利用层次编码技术将维表信息压缩进事实表,区别在于Dumbo采用了更加有效的编码方式,并针对Hadoop分布式文件系统的特点对数据的存储进行了优化.在执行层次上,Dumbo对MapReduce框架进行了扩展,设计了新的OLAP查询处理框架———TMRP(Transform->Map->Reduce->Postpro-cess)处理框架(如图5所示).在该框架中,主节点首先对查询进行转换,生成一个MapReduce任务来执行查询.该任务在Map阶段以流水线方式扫描、聚集本地数据,并只将本地的聚集数据传至Re-duce阶段,来进行数据的合并及聚集、排序等操作.在Postprocess阶段,主节点在数据节点上传的聚集数据之上执行连接操作.实验表明,Dumbo性能远超Hadoop和HadoopDB.由此我们可以看出,复杂的OLAP查询在①②Page9图4对比:一个典型星型模型与其对应的无连接雪花模型MapReduce框架下也可以获得接近甚至超越关系数据库的性能,其关键在于如何有效地结合关系数据库和MapReduce两种技术.仅仅停留于表层的移植和集成是难以从根本上解决大数据分析问题的.我们在文献[41]的研究中也展示了如何基于这种新的数据组织方式来实现复杂分析操作———百分位数的高效计算问题.LinearDB和Dumbo虽然基本可以达到预期的设计目标,但两者都需要对数据进行预处理,其预处理代价是普通加载时间的7倍左右.因此其应对变化的能力还较弱,这是我们未来的工作内容之一.8研究展望当前3个方向的研究都不能完美地解决大数据分析问题,也就意味着每个方向都有极具挑战性的工作等待着我们.对并行数据库来说,其扩展性近年虽有较大改善(如Greenplum和AsterData都是面向PB级数据规模设计开发的),但距离大数据的分析需求仍有较大差距.因此,如何改善并行数据库的扩展能力是一项非常有挑战的工作,该项研究将同时涉及数据一致性协议、容错性、性能等数据库领域的Page10图5Dumbo架构(深灰色部分是新增模块,诸多方面.混合式架构方案可以复用已有成果,开发量较小.但只是简单的功能集成似乎并不能有效解决大数据的分析问题,因此该方向还需要更加深入的研究工作,比如从数据模型及查询处理模式上进行研究,使两者能较自然地结合起来,这将是一项非常有意义的工作.中国人民大学的Dumbo[37]系统即是在深层结合方向上努力的一个例子.相比于前两者,MapReduce的性能优化进展迅速,其性能正逐步逼近关系数据库.该方向的研究又分为两个方向:理论界侧重于利用关系数据库技术及理论改善MapReduce的性能;工业界侧重于基于MapReduce平台开发高效的应用软件.针对数据仓库领域,我们认为如下几个研究方向比较重要,且目前研究还较少涉及:(1)多维数据的预计算.MapReduce更多针对的是一次性分析操作.大数据上的分析操作虽然难以预测,但传统的分析,如基于报表和多维数据的分析仍占多数.因此,MapReduce平台也可以利用预计算等手段加快数据分析的速度.基于存储空间的考虑(可以想象,在爆炸数据之上计算数据立方体需要付出昂贵的存储空间代价),MOLAP是不可取的,混合式OLAP(HOLAP)应该是MapReduce平台的优选OLAP实现方案.具体研究如:①基于MapReduce框架的高效Cube计算算法;②物化视图的选择问题,即物化哪些数据;③不同分析操作的物化手段(比如预测分析操作的物化)及如何基于物化的数据进行复杂分析操作(如数据访问路径的选择问题).(2)各种分析操作的并行化实现.大数据分析需要高效的复杂统计分析功能的支持.IBM将开源统计分析软件R集成进Hadoop平台[42],增强了Hadoop的统计分析功能.但更具挑战性的问题是,如何基于MapReduce框架设计可并行化的、高效的分析算法.尤其需要强调的是,鉴于移动数据的巨大代价,这些算法应基于移动计算的方式来实现.(3)查询共享.MapReduce采用步步物化的处理方式,导致其I/O代价及网络传输代价较高.一种有效的降低该代价的方式是在多个查询间共享物化的中间结果,甚至原始数据,以分摊代价并避免重复计算.因此如何在多查询间共享中间结果将是一项非常有实际应用价值的研究.(4)用户接口.如何较好地实现数据分析的展示和操作,尤其是复杂分析操作的直观展示.(5)Hadoop可靠性研究.当前Hadoop采用主从结构,由此决定了主节点一旦失效,将会出现整个系统失效的局面.因此,如何在不影响Hadoop现有实现的前提下,提高主节点的可靠性,将是一项切实的研究.(6)数据压缩.MapReduce的执行模型决定了其性能取决于I/O和网络传输代价.文献[11]在比较并行数据库和MapReduce基于压缩数据的性能时,发现压缩技术并没有改善Hadoop的性能①.但实际情况是,压缩不仅可以节省空间,节省I/O及网络带宽,还可以利用当前CPU的多核并行计算能力,平衡I/O和CPU的处理能力,从而提高性能.比如并行数据库利用数据压缩后,性能往往可以大幅提升.此后,文献[25-26]的研究成功地利用压缩技术提升了Hadoop的性能.但这些研究都基于各自的存储模型,而非Hadoop的默认存储模式(行存模型).因此,MapReduce上的压缩是一个尚待研究的重要问题.(7)多维索引研究.如何基于MapReduce框架实现多维索引,加快多维数据的检索速度.①原因未知.Page11当然,仍有许多其它研究工作,比如基于Hadoop的实时数据分析、弹性研究、数据一致性研究等,都是非常有挑战和意义的研究,限于篇幅我们不再赘述.9总结本文对大数据分析的主流实现平台(并行数据库、MapReduce及两者的混合架构)进行了评价、归纳与对比分析,介绍了中国人民大学在大数据分析方面的研究,并对当前的研究进行了归纳.从文中可以看出,每种分析平台都不是完美的,在大数据面前,都有很长的路要走.大数据分析迫使我们反思传统的数据仓库架构,虚心地研究MapReduce等新生平台,以站在更高的层次来思考问题,从而找到适应时代需求的数据仓库架构.
