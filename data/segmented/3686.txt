Page1/ 计算机系统/ 与/ 计算机网络/ 中/ 的/ 动态/ 优化/ :/ 模型/ 、/ 求解/ 与/ 应用/ 林闯/ 1/ )/ 万剑雄/ 2/ )/ 向/ 旭东/ 2/ )/ 孟坤/ 2/ )/ 王元卓/ 3/ )/ 1/ )/ (/ 清华大学/ 计算机科学/ 与/ 技术/ 系/ 北京/ 100084/ )/ 2/ )/ (/ 北京科技大学/ 计算机/ 与/ 通信/ 工程学院/ 北京/ 100083/ )/ 3/ )/ (/ 中国科学院计算技术研究所/ 北京/ 100190/ )/ 摘要/ 动态/ 优化/ 是/ 计算机系统/ 与/ 计算机网络/ 中/ 进行/ 资源分配/ 与/ 任务调度/ 等/ 方面/ 研究所/ 采用/ 的/ 主要/ 理论/ 工具/ 之一/ ./ 目前/ ,/ 国内外/ 已/ 开展/ 大量/ 研究/ ,/ 致力于/ 深化/ 动态/ 优化/ 的/ 理论/ 研究/ 与/ 工程/ 应用/ ./ 文中/ 从/ 模型/ 、/ 求解/ 与/ 应用/ 3/ 个/ 角度/ ,/ 对/ 马尔可夫/ 决策/ 过程/ 动态/ 优化/ 理论/ 模型/ 进行/ 了/ 综述/ ,/ 并/ 重点/ 介绍/ 了/ 将/ 动态/ 优化/ 理论/ 与/ 随机/ Petri/ 网/ 理论/ 相结合/ 的/ 马尔可夫/ 决策/ Petri/ 网/ 和/ 随机/ 博弈/ 网/ 模型/ ,/ 详细/ 讨论/ 了/ 这些/ 模型/ 的/ 建模/ 方法/ 、/ 求解/ 算法/ 与/ 一些/ 应用/ 实例/ ./ 最后/ ,/ 对/ 全文/ 进行/ 了/ 总结/ ,/ 并/ 对/ 未来/ 可能/ 的/ 研究/ 方向/ 进行/ 了/ 展望/ ./ 关键词/ 动态/ 优化/ ;/ 马尔可夫/ 决策/ 过程/ ;/ 随机/ Petri/ 网/ ;/ 马尔可夫/ 决策/ Petri/ 网/ ;/ 随机/ 博弈/ 网/ 1/ 引言/ 随着/ 计算机网络/ 与/ 计算机系统/ 在/ 国民/ 生活/ 各个领域/ 应用/ 的/ 不断/ 拓展/ ,/ 其/ 承载/ 的/ 业务/ 种类/ 与/ 数量/ 也/ 在/ 不断/ 增加/ ./ 如何/ 在/ 复杂/ 的/ 应用环境/ 中/ 合理/ 地/ 分配/ 系统资源/ 并/ 进行/ 调度/ 任务/ ,/ 以/ 提高/ 计算机系统/ 与/ 计算机网络/ 的/ 运行/ 效率/ ,/ 降低/ 运行/ 成本/ ,/ 是/ 一个/ 亟待解决/ 的/ 问题/ ./ 优化/ 理论/ 是/ 学术界/ 研究/ 计算机系统/ 与/ 计算机网络/ 中/ 资源分配/ 与/ 任务调度/ 问题/ 普遍/ 采用/ 的/ 方法/ 之一/ ./ 从/ 时间/ 这个/ 维度/ 进行/ 分类/ ,/ 优化/ 理论/ 可/ 分为/ 静态/ 优化/ 与/ 动态/ 优化/ 两种/ ./ 其中/ ,/ 静态/ 优化/ 将/ 系统/ 看/ 作为/ 一个/ 时/ 不变/ 系统/ ,/ 即将/ 系统/ 的/ 资源/ 需求量/ 与/ 资源/ 保有量/ 视为/ 一个/ 与/ 时间/ 无关/ 的/ 常量/ ./ 但是/ ,/ 实际/ 的/ 系统/ 往往/ 都/ 是/ 随/ 时间/ 变化/ 的/ ,/ 而且/ 会/ 受到/ 各种/ 外部/ 随机/ 事件/ 的/ 影响/ ./ 静态/ 优化/ 模型/ 忽略/ 了/ 未来/ 可能/ 的/ 系统/ 变化/ ,/ 也/ 不能/ 反映/ 决策者/ 当前/ 行为/ 对/ 未来/ 的/ 影响/ ,/ 无法/ 刻画/ 系统/ 随/ 时间/ 变化/ 的/ 特性/ ./ 因此/ ,/ 本文/ 着重/ 研究/ 动态/ 优化/ 理论/ 在/ 计算机系统/ 与/ 计算机网络/ 中/ 的/ 应用/ ./ 在/ 动态/ 优化/ 理论/ 中/ ,/ 系统/ 的/ 目标/ 函数/ 是/ 系统/ 收益/ 关于/ 时间/ 的/ 累积/ 量/ ./ 相对/ 于/ 静态/ 优化/ 理论/ ,/ 动态/ 优化/ 理论/ 可以/ 较/ 好/ 地/ 对系统/ 的/ 时/ 变性/ 进行/ 刻画/ ,/ 更好/ 地/ 反映/ 系统/ 当前/ 决策/ 对/ 时间/ 累积/ 目标/ 函数/ 的/ 影响/ ./ 动态/ 优化/ 的/ 基本/ 理论/ 模型/ 是/ 马尔可夫/ 决策/ 过程/ (/ MarkovDecisionProcess/ ,/ MDP/ )/ ./ MDP/ 可以/ 用来/ 描述/ 这样/ 一类/ 离散/ 时间/ 决策/ 过程/ :/ 系统/ t/ +/ 1/ 刻/ 状态/ 的/ 转移/ ,/ 只/ 依赖于/ t/ 时刻/ 的/ 系统/ 状态/ 与/ 决策者/ 的/ 行为/ ,/ 而/ 与/ [/ 0/ ,/ t/ -/ 1/ ]/ 时间段/ 内/ 的/ 系统/ 状态/ 与/ 决策者/ 行为/ 无关/ ./ MDP/ 可以/ 从/ 执行/ 时间/ 、/ 决策者/ 观测/ 能力/ 、/ 状态/ 转移/ 的/ 确定/ 关系/ 、/ 时间/ 的/ 连续性/ 、/ 状态/ 转移/ // 收益/ 的/ 确定性/ 、/ 是否/ 具有/ 附加/ 限制/ 条件/ 以及/ 决策/ 目标/ 数量/ 等/ 角度/ 进行/ 分类/ ./ 通常/ 情况/ 下/ ,/ 对于/ 计算机系统/ 与/ 计算机网络/ 中/ 的/ 资源管理/ 问题/ ,/ 由于/ 资源/ 的/ 种类/ 繁多/ ,/ 数量/ 庞大/ ,/ 因而/ 所/ 建立/ 的/ MDP/ 模型/ 通常/ 会/ 遇到/ “/ 状态/ 空间/ 爆炸/ ”/ 问题/ ,/ 即/ MDP/ 模型/ 的/ 状态/ 空间/ 随着/ 问题/ 规模/ 指数/ 级/ 增长/ ,/ 这/ 使得/ 传统/ 精确/ 求解/ 算法/ 如值/ 迭代/ 与/ 策略/ 迭代/ 等/ 无法/ 应用/ ./ 因此/ ,/ 本文/ 详细/ 讨论/ 了/ MDP/ 模型/ 的/ 近似/ 求解/ 算法/ ,/ 将/ 这些/ 算法/ 归为/ 3/ 类/ :/ 贪心/ 算法/ 、/ 基于/ 状态/ 聚合/ 的/ 算法/ 以及/ 基于/ 近似/ 动态/ 规划/ (/ ApproximateDynamicProgramming/ ,/ ADP/ )/ 的/ 算法/ ./ 马尔可夫/ 决策/ 过程/ 在/ 实际/ 应用/ 中/ 体现/ 出/ 一些/ 不足/ ,/ 主要/ 表现/ 在/ :/ (/ 1/ )/ 模型/ 不够/ 直观/ ./ 一方面/ ,/ MDP/ 中/ 的/ 各个/ 模型/ 要素/ 都/ 使用/ 了/ 严格/ 的/ 形式化/ 定义/ ,/ 虽然/ 具有/ 较强/ 的/ 逻辑性/ 与/ 严密性/ ,/ 但是/ 模型/ 的/ 直观性/ 与/ 可/ 理解/ 性/ 却/ 相对/ 较/ 低/ ./ 另一方面/ ,/ 模型/ 建立/ 需要/ 较强/ 的/ 数学/ 背景/ ,/ 例如/ 在/ 推导/ 系统/ 状态/ 转移/ 概率/ 时/ ,/ 往往/ 需要/ 建模/ 者/ 具有/ 一定/ 的/ 随机/ 数学/ 基础/ ,/ 增加/ 了/ 模型/ 建立/ 的/ 难度/ ./ (/ 2/ )/ 在/ 一些/ 复杂/ 应用环境/ 中/ ,/ 单纯/ 的/ MDP/ 模型/ 难以/ 精确/ 刻画/ 系统/ 的/ 特点/ ./ 例如/ ,/ 在/ 网络安全/ 问题/ 中/ ,/ MDP/ 难以描述/ 网络拓扑/ 与/ 各个/ 组件/ 之间/ 的/ 逻辑关系/ ./ 这些/ 不足/ 激励/ 学者/ 们/ 进行/ 了/ 进一步/ 的/ 模型/ 拓展/ 研究/ ,/ 其中/ 较/ 有/ 代表性/ 的/ 是/ 马尔可夫/ 决策/ Petri/ 网/ (/ MarkovDecisionPetriNets/ ,/ MDPN/ )/ 与/ 随机/ 博弈/ 网/ (/ StochasticGameNets/ ,/ SGN/ )/ ./ 这些/ 模型/ 方法/ 将/ 动态/ 优化/ 理论/ 与/ 随机/ Petri/ 网/ 理论/ 相结合/ ,/ 在/ 一定/ 程度/ 上/ 克服/ 了/ 上述/ 缺点/ ./ 随机/ Petri/ 网/ 模型/ 语义/ 明确/ ,/ 使用/ 图形化/ 表示/ 方式/ ,/ 直观/ 易懂/ ./ 系统/ 中/ 各个/ 组件/ 之间/ 的/ 关系/ 可以/ 灵活/ 地/ 使用/ 组件/ 之间/ 的/ 连接/ 弧/ 与/ 变迁/ 可/ 实施/ 函数/ 等/ 方式/ 表现/ ./ 建模/ 者/ 可以/ 将/ 精力/ 更多地/ 放在/ 研究/ 目标/ 系统/ 与/ 精确/ 描述/ 系统/ 与/ 决策者/ 行为/ 方面/ ,/ 而/ 状态/ 转移/ 概率/ 等/ 其它/ 较为/ 复杂/ 的/ 模型/ 元素/ 则/ 可以/ 利用/ Petri/ 网/ 工具/ 中/ 集成/ 的/ 功能/ 实现/ 自动化/ 推导/ ./ MDPN/ 模型/ 将/ MDP/ 理论/ 与/ 随机/ Petri/ 网/ 理论/ 相结合/ ,/ 可以/ 体现/ 出/ 系统/ 与/ 决策者/ 宏观/ 层面/ 上/ 的/ 行为/ 交替/ ./ 利用/ MDPN/ 模型/ ,/ 可以/ 方便/ 地/ 借助/ Petri/ 网/ 图形/ 工具/ 对系统/ 进行/ 建模/ ,/ 并/ 对模型/ 的/ 可达图/ 进行/ 规约/ 得到/ MDP/ 模型/ ./ SGN/ 模型/ 是/ 动态/ 优化/ 模型/ 的/ 进一步/ 扩展/ ,/ 它/ 将/ 动态随机/ 博弈/ 与/ Petri/ 网/ 理论/ 相结合/ ,/ 允许/ 系统/ 中/ 存在/ 多个/ 决策者/ ./ 每个/ 决策者/ 一般/ 都/ 有/ 各自/ 的/ 目标/ 函数/ ,/ 他们/ 之间/ 既/ 可以/ 是/ 合作/ 关系/ ,/ 也/ 可以/ 是/ 竞争/ 关系/ ./ 在/ 建立/ SGN/ 模型/ 时/ ,/ 可以/ 先/ 单独/ 从/ 各个/ 决策者/ 的/ 角度/ 出发/ ,/ 建立/ SGN/ 子/ 模型/ ,/ 再/ 利用/ 模型/ 组合/ 与/ 化简/ 技术/ ,/ 得到/ 完整/ 的/ SGN/ 模型/ ./ 求解/ SGN/ 是/ 一个/ 寻求/ 每个/ 决策者/ 均衡/ 策略/ 的/ 问题/ ,/ 可/ 归结为/ 一个/ 静态/ 非线性/ 规划/ 问题/ ./ 动态/ 优化/ 模型/ 是/ 当前/ 计算机系统/ 与/ 计算机网络/ 的/ 资源分配/ 与/ 任务调度/ 等/ 问题/ 中/ 的/ 研究/ 热点/ ,/ 对/ 降低/ 系统维护/ 成本/ 、/ 提高/ 系统/ 运行/ 效率/ 具有/ 重要/ 的/ 意义/ ./ 本文/ 从/ 建模/ 、/ 求解/ 与/ 应用/ 等/ 角度/ ,/ 论述/ 了/ 马尔可夫/ 决策/ 过程/ 、/ 马尔可夫/ 决策/ Petri/ 网/ 以及/ 随机/ 博弈/ 网等/ 动态/ 优化/ 模型/ 在/ 计算机系统/ 与/ 计算机网络/ 中/ 的/ 应用/ ./ Page32/ 基于/ 马尔可夫/ 决策/ 过程/ 的/ 动态/ 优化/ 模型/ 2.1/ 马尔可夫/ 决策/ 过程/ 一个/ 基本/ 的/ 马尔可夫/ 决策/ 过程/ 包括/ 以下/ 要素/ :/ (/ 1/ )/ 状态/ 集合/ S/ ,/ 描述/ 系统/ 的/ 状态/ ./ (/ 2/ )/ 行为/ 集合/ A/ ,/ 描述/ 决策者/ 在/ 状态/ 空间/ 中/ 可能/ 的/ 行为/ ./ 通常/ 行为/ 集合/ 会/ 依赖于/ 当前/ 状态/ ,/ 即可/ 将/ 其/ 记为/ A/ (/ s/ )/ ./ (/ 3/ )/ 收益/ 函数/ R/ (/ s/ ,/ s/ ,/ a/ )/ ,/ s/ ,/ s/ ∈/ S/ ,/ a/ ∈/ A/ ,/ 描述/ 系统/ 在/ 决策者/ 行为/ 的/ 影响/ 下/ 运行/ 所/ 产生/ 的/ 收益/ ./ (/ 4/ )/ 状态/ 转移/ 关系/ SM/ ,/ 描述/ 系统/ 状态/ 在/ 决策/ 行为/ 影响/ 下/ 的/ 转移/ 过程/ ./ 马尔可夫/ 决策/ 过程/ 的/ 一个/ 显著/ 特征/ 是/ 无后效/ 性/ ,/ 即/ 系统/ 在/ 下/ 一/ 时刻/ 的/ 状态/ 仅/ 依赖于/ 当前/ 所处/ 的/ 状态/ 与/ 决策/ 行为/ ,/ 而/ 与/ 系统/ 的/ 历史/ 无关/ ./ 根据/ SM/ 的/ 性质/ 不同/ ,/ MDP/ 可以/ 分为/ 确定/ MDP/ 与/ 随机/ MDP/ 两大类/ ./ 对于/ 确定/ MDP/ ,/ 在/ 某个/ 状态/ 下/ 的/ 某个/ 行为/ 会/ 导致/ 唯一/ 确定/ 的/ 状态/ 转移/ ,/ 即/ SM/ :/ S/ ×/ A/ →/ S/ ,/ 此时/ 状态/ 转移/ 方程/ 可记/ 为/ s/ =/ SM/ (/ s/ ,/ a/ )/ ;/ 对于/ 随机/ MDP/ ,/ 未来/ 系统/ 状态/ 不仅/ 取决于/ 当前/ 系统/ 状态/ 下/ 决策者/ 的/ 行为/ ,/ 还/ 受到/ 外部/ 随机变量/ W/ 的/ 影响/ ,/ 即/ SM/ :/ S/ ×/ A/ ×/ W/ →/ S/ ,/ 此时/ 状态/ 转移/ 方程/ 可记/ 为/ s/ =/ SM/ (/ s/ ,/ a/ ,/ W/ (/ ω/ )/ )/ ,/ 其中/ W/ (/ ω/ )/ 为/ 外部/ 随机变量/ 的/ 一个/ 实现/ 样本/ ./ 随机/ MDP/ 的/ 未来/ 状态/ 一般/ 服从/ 某种/ 分布/ ,/ 该/ 分布/ 可记/ 为/ P/ (/ s/ |/ s/ ,/ a/ )/ ./ 本文/ 主要/ 研究/ 随机/ MDP/ ,/ 下文/ 中/ 提到/ 的/ 马尔可夫/ 决策/ 过程/ ,/ 一般/ 均/ 指/ 随机/ 马尔可夫/ 决策/ 过程/ ./ 定义/ R/ (/ s/ ,/ a/ )/ =/ ∑/ s/ ∈/ SP/ (/ s/ |/ s/ ,/ a/ )/ R/ (/ s/ ,/ s/ ,/ a/ )/ 为/ 状态/ s/ 下/ 采用/ 行为/ a/ 所/ 产生/ 的/ 收益/ ./ 在/ 马尔可夫/ 决策/ 过程/ 中/ ,/ 策略/ π/ 定义/ 为/ 从/ 状态/ 集合/ S/ 到/ 行为/ 集合/ A/ 的/ 一个/ 映射/ ./ 决策者/ 根据/ 策略/ π/ 来/ 得到/ 当前/ 所/ 需/ 的/ 决策/ 行为/ ./ 一个/ 典型/ 的/ 马尔可夫/ 决策/ 过程/ 的/ 执行/ 流程/ 如下/ :/ 1/ ./ 决策者/ 观察/ 当前/ 所处/ 的/ 状态/ s/ ./ 2/ ./ 根据/ 当前/ 状态/ 确定/ 决策/ 行为/ π/ (/ s/ )/ ./ 3/ ./ 执行/ 行为/ π/ (/ s/ )/ ,/ 系统/ 状态/ 发生/ 转换/ ./ 4/ ./ 重复/ 1/ ./ MDP/ 在/ 系统/ 演进/ 过程/ 中/ ,/ 会/ 产生/ 一个/ 收益/ 序列/ ./ 为/ 比较/ MDP/ 中/ 决策/ 的/ 优劣/ 程度/ ,/ 引入/ 了/ 目标/ 函数/ J/ ./ 它/ 将/ 一个/ 收益/ 序列/ 映射/ 为/ 一个/ 单一/ 的/ 实/ 数值/ ./ 对于/ 无限/ 时间/ MDP/ 来说/ ,/ 其/ 设置/ 一般/ 有/ 3/ 种/ 方法/ :/ (/ 1/ )/ 在/ 无限/ 时间/ MDP/ 中/ 截取/ 一个/ 足够/ 长/ 的/ 有限/ 时间/ MDP/ ,/ 则/ 无限/ 时间/ MDP/ 的/ 目标/ 函数/ 可/ 近似/ 地/ 看作/ 有限/ 时间/ MDP/ 的/ 收益/ 的/ 和/ ./ (/ 2/ )/ 依照/ 时间/ 的/ 推移/ 对/ 未来/ 所得/ 收益/ 进行/ 逐步/ 折扣/ ,/ 保证/ 对/ 时间/ 累加/ 的/ 总/ 收益/ 总是/ 收敛/ 的/ ./ 这种/ 方式/ 更/ 看重/ 当前/ 所得/ 的/ 收益/ ./ (/ 3/ )/ 平均/ 收益/ 在/ 时间/ 趋于/ 无穷/ 处/ 的/ 极限值/ ./ 通过/ 目标/ 函数/ J/ ,/ 可以/ 定义/ 策略/ 之间/ 的/ 偏序/ 关系/ ,/ 这样/ 就/ 可以/ 对/ 策略/ 的/ 优劣/ 进行/ 比较/ 了/ ./ MDP/ 中/ 另/ 一个/ 重要/ 概念/ 是/ 值/ 函数/ V/ π/ (/ s/ )/ ./ V/ π/ (/ s/ )/ 是从/ π/ ×/ S/ 到/ 实数/ 集/ / 的/ 映射/ ,/ 其/ 含义/ 为/ 在/ 采用/ 策略/ π/ 的/ 前提/ 下/ ,/ 在/ 状态/ s/ ∈/ S/ 下/ 所/ 得到/ 的/ 目标/ 函数/ J/ 的/ 期望/ ./ 无限/ 时间/ MDP/ 的/ 值/ 函数/ 满足/ Bellman/ 递推/ 方程/ ,/ 即式/ (/ 1/ )/ :/ V/ π/ (/ st/ )/ =/ R/ (/ st/ ,/ π/ (/ st/ )/ )/ +/ α/ ∑/ st/ +/ 1/ ∈/ S/ 其中/ α/ 为/ 折扣/ 因子/ ./ 式/ (/ 1/ )/ 说明/ ,/ 给定/ 策略/ π/ ,/ 则/ 在/ 状态/ s/ 的/ 值/ 函数/ 等于/ 当前/ 一步/ 决策/ 所得/ 收益/ 与/ 下/ 一/ 时刻/ 折扣/ 后值/ 函数/ 期望/ 的/ 和/ ./ 式/ (/ 1/ )/ 也/ 可以/ 写成/ 如式/ (/ 2/ )/ 所示/ 的/ 向量/ 形式/ ,/ 即/ 2.2/ 马尔可夫/ 决策/ 过程/ 建模/ 与/ 分析/ 时/ ,/ 可/ 使用/ 如下/ 步骤/ :/ 在/ 利用/ 马尔可夫/ 决策/ 过程/ 对系统/ 进行/ 建模/ 分析/ (/ 1/ )/ 明确/ 系统/ 运行/ 目标/ ./ 该/ 步骤/ 中/ 需要/ 确定/ MDP/ 的/ 收益/ 函数/ R/ 与/ 目标/ 函数/ J/ ./ 一方面/ ,/ 不同/ 系统/ 的/ 运行/ 目标/ 可能/ 不同/ ./ 另一方面/ ,/ 即使/ 是/ 对于/ 同一/ 系统/ ,/ 研究/ 角度/ 的/ 差异/ 也/ 会/ 导致/ 不同/ 的/ 收益/ 函数/ 与/ 目标/ 函数/ ./ 以/ 计算机网络/ 为例/ ,/ 较为/ 常用/ 的/ 目标/ 函数/ 有/ ①/ 节点/ 吞吐量/ [/ 1/ -/ 3/ ]/ ;/ ②/ 能量消耗/ [/ 4/ -/ 6/ ]/ ;/ ③/ 信道/ 利用率/ [/ 7/ -/ 8/ ]/ ;/ ④/ 延迟/ [/ 9/ -/ 10/ ]/ ;/ ⑤/ 分组/ 丢失/ 率/ [/ 11/ ]/ ./ 对于/ 随机/ MDP/ ,/ 通常/ 使用/ 带有/ 期望/ 形式/ (/ E/ )/ 的/ 有限/ 马尔可夫/ 决策/ 过程/ :/ 目标/ 函数/ ./ 一般/ 期望/ 目标/ 函数/ 具有/ 如下/ 形式/ :/ 无限/ 马尔可夫/ 决策/ 过程/ :/ Page4/ 其中/ ,/ st/ 与/ at/ 分别/ 为/ t/ 阶段/ 系统/ 所/ 处/ 状态/ 与/ 决策者/ 采取/ 的/ 行为/ ./ 式/ (/ 4/ )/ 与/ 式/ (/ 5/ )/ 分别/ 为/ 无穷/ 时间/ 折扣/ 情形/ 与/ 无穷/ 时间/ 平均/ 情形/ 下/ 的/ 目标/ 函数/ ./ 系统/ 的/ 运行/ 目标/ 通常/ 是/ 最大化/ 或/ 最小化/ 上述/ 目标/ 函数/ J/ ./ (/ 2/ )/ 确定/ 系统/ 运行/ 状态/ 空间/ 与/ 决策者/ 的/ 行为/ 空间/ ./ 系统/ 的/ 状态/ 空间/ 与/ 决策者/ 的/ 行为/ 空间/ 可能/ 是/ 离散/ 可列/ 的/ ./ 例如/ ,/ 在/ 认知/ 无线电/ 系统/ 中/ ,/ 信道/ 可以/ 用/ 两个/ 离散/ 的/ 状态/ 刻画/ ,/ 即/ {/ 空闲/ ,/ 占用/ }/ ,/ 用户/ 的/ 行为/ 也/ 可能/ 是/ 离散/ 的/ ,/ 如/ {/ 发送数据/ ,/ 监听/ 信道/ }/ ./ 状态/ 空间/ 与/ 行为/ 空间/ 也/ 可能/ 是/ 连续/ 的/ ./ 例如/ ,/ 在/ 上例/ 中/ ,/ 若/ 用户/ 的/ 行为/ 变为/ “/ 以/ 概率/ p/ 发送数据/ ”/ ,/ 则/ 用户/ 行为/ 空间/ 是/ 连续/ 的/ ,/ 且/ 其/ 取值/ 范围/ 为/ [/ 0/ ,/ 1/ ]/ ./ (/ 3/ )/ 根据/ 系统/ 状态/ 之间/ 的/ 动态/ 转移/ 关系/ 建立/ Bellman/ 递推/ 方程/ ./ 该/ 步骤/ 中要/ 找到/ 状态/ 之间/ 的/ 转移/ 关系/ ./ 对于/ 随机/ MDP/ 来说/ ,/ 转移/ 关系/ 包括/ 状态/ 转移/ 方程/ s/ =/ SM/ (/ s/ ,/ a/ ,/ W/ (/ ω/ )/ )/ 与/ 转移/ 概率/ P/ (/ s/ |/ s/ ,/ a/ )/ ./ 有时/ 状态/ 转移/ 概率/ 无法/ 精确/ 得知/ ,/ 此时/ 可以/ 使用/ 强化/ 学习/ [/ 12/ ]/ 的/ 方法/ 来/ 求解/ 马尔可夫/ 决策/ 过程/ ./ Bellman/ 方程/ 描述/ 的/ 是/ 值/ 函数/ V/ 的/ 递推/ 关系/ ,/ 该/ 方程/ 在/ 求解/ 最优/ 策略/ 时/ 发挥/ 了/ 重要/ 作用/ ./ (/ 4/ )/ 根据/ 所/ 建立/ 的/ Bellman/ 递推/ 方程/ ,/ 对模型/ 进行/ 求解/ ,/ 得到/ 最优/ 策略/ π/ / ./ 以/ 最大化/ 目标/ 函数/ 为例/ ,/ 求解/ 过程/ 中/ 的/ 关键步骤/ 包括/ π/ / (/ st/ )/ =/ argmaxat/ ∈/ AR/ (/ st/ ,/ at/ )/ +/ α/ ∑/ st/ +/ 1/ ∈/ SV/ / (/ st/ )/ =/ Rst/ ,/ π/ / (/ st/ (/ )/ )/ +/ α/ ∑/ s/ ∈/ S/ 式/ (/ 6/ )/ 按照/ 最大化/ 策略/ ,/ 根据/ 当前/ 所得/ 的/ 值/ 函数/ V/ / 求得/ 在/ 状态/ s/ 下/ 应该/ 采取/ 的/ 策略/ π/ / ,/ 而式/ (/ 7/ )/ 则/ 根据/ π/ / 计算/ 其所/ 对应/ 的/ 值/ 函数/ ./ 式/ (/ 6/ )/ 、/ (/ 7/ )/ 实际上/ 是/ 一个/ 迭代/ 的/ 过程/ ./ 不同/ 的/ 求解/ 算法/ ,/ 如值/ 迭代/ 、/ 策略/ 迭代/ 等/ ,/ 均/ 需要/ 使用/ 以上/ 两个/ 步骤/ ,/ 只是/ 顺序/ 不同/ ./ 下面/ 以/ 一个/ 接纳/ 控制/ 为例/ (/ 图/ 1/ )/ ,/ 举例说明/ MDP/ 的/ 建模/ 方法/ ./ 外部/ 任务/ 到达/ 后/ ,/ 首先/ 缓存/ 在/ 接纳/ 控制器/ 的/ 等待/ 队列/ 中/ ./ 在/ 每个/ 时间槽/ 的/ 开始/ ,/ 接纳/ 控制器/ 将/ 其/ 等待/ 队列/ 中/ 的/ 任务/ 按照/ 某种/ 策略/ 或/ 将/ 任务/ 丢弃/ ,/ 或/ 将/ 任务分配/ 给/ 服务器/ 1/ ~/ n/ 中/ 的/ 一个/ ./ 在/ 这个/ 系统/ 中/ ,/ 决策者/ 为/ 接纳/ 控制器/ ,/ 其/ t/ 时刻/ 的/ 行为/ 是/ 向量/ 狓/ t/ =/ {/ xit/ }/ ,/ 其中/ 每个/ 分量/ xit/ 表示/ 向/ 服务器/ i/ 分配/ 的/ 任务/ 数/ ./ 系统/ 中/ 的/ 外部/ 随机变量/ W/ 包括/ 两/ 部分/ :/ ①/ [/ t/ -/ 1/ ,/ t/ ]/ 内/ 到达/ 接纳/ 控制器/ 的/ 任务/ 数/ λ/ t/ ;/ ②/ [/ t/ ,/ t/ +/ 1/ ]/ 内/ 服务器/ i/ 完成/ 的/ 任务/ 数/ μ/ it/ ./ 系统/ 在/ t/ 时刻/ 的/ 状态/ 可用/ {/ λ/ t/ ,/ q1t/ ,/ …/ ,/ qnt/ }/ 表示/ ./ 其中/ qit/ 表示/ 服务器/ i/ 中/ 的/ 队列/ 长度/ ./ 假设/ 若/ 等待/ 队列/ 中/ 的/ 任务/ 没有/ 得到/ 及时/ 服务/ ,/ 则/ 下/ 一/ 时刻/ 这些/ 任务/ 会/ 丢失/ ./ 系统/ 的/ 状态/ 转移/ 方程/ 可写/ 为/ 此时/ 决策/ 行为/ 受到/ 如下/ 流/ 守恒/ 条件/ 约束/ :/ 若/ 假设/ 系统/ 每个/ 时间槽/ 内/ 的/ 收益/ 与/ 完成/ 的/ 任务/ 数/ 成正比/ ,/ 与/ 服务器/ 中/ 驻留/ 的/ 任务/ 数/ 成反比/ ,/ 则/ 系统/ 在/ [/ t/ ,/ t/ +/ 1/ ]/ 时间段/ 内/ 的/ 收益/ 函数/ 可定义/ 如下/ :/ 其中/ r/ 为/ 每/ 完成/ 一个/ 任务/ 所得/ 的/ 收益/ ,/ ci/ 为/ 在/ 服务器/ i/ 上/ 每个/ 时间槽/ 内/ 服务/ 一个/ 任务/ 所/ 需要/ 的/ 成本/ ./ 该/ 系统/ 的/ 无穷/ 时间/ 折扣/ MDP/ 的/ 目标/ 函数/ 为/ MDP/ 的/ 求解/ 方法/ 将/ 在/ 2.4/ 小节/ 进行/ 详细/ 讨论/ ./ 根据/ 不同/ 的/ 划分/ 依据/ ,/ 可/ 将/ 马尔可夫/ 决策/ 过程/ 2.3/ 马尔可夫/ 决策/ 过程/ 的/ 分类/ 进行/ 分类/ ,/ 如表/ 1/ 所示/ ./ 执行/ 的/ 时间/ 决策者/ 的/ 观测/ 能力/ 完全/ 可/ 观测/ MDP/ ,/ 部分/ 可/ 观测/ MDP/ 转移/ 关系/ 的/ 确定性/ 确定/ MDP/ ,/ 随机/ MDP/ 时间/ 的/ 连续性/ 转移/ 概率/ // 收益/ 的/ 确定性/ 普通/ MDP/ ,/ 带有/ 强化/ 学习/ 的/ MDP/ 是否/ 有/ 附加/ 限制/ 条件/ 不/ 受限/ MDP/ ,/ 受限/ MDP/ (/ CMDP/ )/ 目标/ 的/ 数量/ (/ 1/ )/ 按照/ 系统/ 的/ 执行/ 时间/ 分类/ ./ 现实/ 中/ 系统/ 的/ 运行/ 时间/ 都/ 是/ 有限/ 的/ ./ 对于/ 有限/ 时间/ 马尔可夫/ 决策/ 过程/ ,/ 其/ 目标/ 函数/ 可以/ 简单/ 地/ 写成/ 在/ 系统/ 运行/ 期间/ 内/ 收益/ 的/ 和/ ,/ 如/ 文献/ [/ 3/ ]/ ./ 当/ 系统/ Page5/ (/ 2/ )/ 按照/ 决策者/ 的/ 观测/ 能力/ 分类/ ./ 一般/ 情形/ 下/ ,/ 决策者/ 可以/ 完全/ 观测/ 到/ 系统/ 的/ 状态/ ,/ 并/ 根据/ 所/ 观测/ 到/ 的/ 状态/ 进行/ 决策/ ./ 但是/ ,/ 有些/ 时候/ 决策者/ 不能/ 完全/ 观测/ 到/ 系统/ 状态/ ./ 这时/ ,/ 需要/ 利用/ 部分/ 可/ 观测/ 马尔可夫/ 决策/ 过程/ (/ PartiallyObservableMarkovDecisionProcess/ ,/ POMDP/ )/ 进行/ 建模/ ./ Zhao/ 等/ 人/ [/ 2/ -/ 3/ ]/ 研究/ 了/ 认知/ 无线电/ 系统/ 中次/ 用户/ 的/ 信道/ 监听/ 与/ 接入/ 的/ 问题/ ./ 在/ 该/ 问题/ 中/ ,/ 由于/ 次/ 用户/ 监听/ 可能/ 发生/ 错误/ ,/ 因此/ 系统/ 是/ 一个/ 部分/ 可/ 观测/ 马尔可夫/ 决策/ 过程/ ./ POMDP/ 求解/ 相较/ 于/ MDP/ 来说/ 较为/ 复杂/ ./ 因为/ 决策者/ 没有/ 系统/ 状态/ 的/ 精确/ 信息/ ,/ 所以/ 需要/ 维护/ 一个/ 信任/ 向量/ ,/ 用来/ 描述/ 系统/ 当前/ 位于/ 各个/ 状态/ 的/ 概率/ ./ 信任/ 向量/ 随着/ 系统/ 的/ 演进/ 而/ 不断更新/ ./ (/ 3/ )/ 按照/ 转移/ 关系/ 的/ 确定性/ 分类/ ./ 决策者/ 在/ 某个/ 状态/ 下/ 所/ 做/ 的/ 行为/ ,/ 有时/ 会/ 导致/ 一个/ 确定/ 的/ 结果/ ,/ 即以/ 概率/ 1/ 转移/ 到/ 下/ 一个/ 状态/ ,/ 这/ 称为/ 确定/ 马尔可夫/ 决策/ 过程/ ./ 有时/ ,/ 决策者/ 的/ 行为/ 会/ 导致/ 不/ 确定/ 的/ 结果/ ,/ 这/ 称为/ 随机/ 马尔可夫/ 决策/ 过程/ ./ (/ 4/ )/ 按照/ 时间/ 的/ 连续性/ 分类/ ./ 现实/ 中/ 的/ 一些/ 问题/ 是/ 离散/ 时间/ 的/ ,/ 例如/ 在/ 库存/ 管理/ 问题/ 中/ ,/ 仓库/ 管理员/ 一般/ 每隔/ 一个/ 固定/ 的/ 时间/ 间隔/ 采购/ 商品/ ,/ 更新/ 库存/ ./ 还有/ 一些/ 问题/ 是/ 连续/ 时间/ 的/ ,/ 典型/ 问题/ 如/ 队列/ 管理/ 问题/ [/ 14/ ]/ 与/ 设备/ 维护/ 问题/ [/ 15/ -/ 16/ ]/ ./ 在/ 这些/ 问题/ 中/ ,/ 系统/ 的/ 状态/ 转换/ 间隔时间/ (/ 如/ 顾客/ 到达/ 的/ 间隔时间/ 与/ 设备/ 正常/ 运转/ 的/ 时间/ 等/ )/ 服从/ 指数分布/ ,/ 每次/ 系统/ 状态/ 发生/ 转换/ 时/ 都/ 需要/ 决策者/ 进行/ 决策/ ./ 运行/ 时间/ 很大/ 时/ ,/ 也/ 可以/ 近似/ 地/ 认为/ 系统/ 的/ 运行/ 时间/ 是/ 无限/ 的/ ./ 此时/ 针对/ 该/ 系统/ 建立/ 的/ 马尔可夫/ 决策/ 过程/ 就是/ 无限/ 时间/ 马尔可夫/ 决策/ 过程/ ./ 无限/ 时间/ 马尔可夫/ 决策/ 过程/ 通常/ 采用/ 折扣/ 累积/ 收益/ 或/ 平均/ 收益/ 作为/ 其/ 目标/ 函数/ ,/ 即式/ (/ 4/ )/ 和/ (/ 5/ )/ ./ Haas/ 等/ 人/ [/ 13/ ]/ 分别/ 针对/ 这/ 两种/ 目标/ 函数/ 研究/ 了/ 无线/ 多媒体/ 网络/ 环境/ 中/ 的/ 资源分配/ 问题/ ./ 折扣/ 累积/ 收益/ 目标/ 函数/ 已经/ 被/ 广泛/ 研究/ ,/ 理论/ 成果/ 较为/ 完善/ ./ (/ 5/ )/ 按照/ 转移/ 概率/ // 收益/ 的/ 确定性/ 分类/ ./ 在/ 一些/ 复杂/ 系统/ 中/ ,/ 系统/ 的/ 状态/ 转移/ 概率/ P/ (/ s/ |/ s/ ,/ a/ )/ 难以/ 精确测量/ ,/ 收益/ 函数/ R/ (/ s/ ,/ a/ )/ 也/ 无法/ 推导/ 出显式/ 的/ 解析/ 式/ ./ 这时/ ,/ 就/ 需要/ 建立/ 基于/ 强化/ 学习/ 的/ MDP/ 模型/ ,/ 采用/ 跟踪/ 实际/ 系统/ 运行/ 过程/ 或/ MonteCarlo/ 模拟/ 等/ 方法/ ,/ 不断/ 学习/ 系统/ 的/ 未知/ 特性/ ./ 值得注意/ 的/ 是/ ,/ 基于/ 强化/ 学习/ 的/ MDP/ 也/ 是/ 一种/ MDP/ 的/ 近似/ 求解/ 方法/ ,/ 简化/ 了/ Bellman/ 方程/ 中/ 值/ 函数/ 期望/ 的/ 计算/ ./ (/ 6/ )/ 根据/ 是否/ 有/ 附加/ 限制/ 条件/ 分类/ ./ 有时/ 决策者/ 行为/ 会/ 受到/ 一些/ 客观条件/ 的/ 影响/ ,/ 这时/ 可/ 将/ 该/ 问题/ 归结为/ 一个/ 受限/ 马尔可夫/ 决策/ 过程/ (/ ConstrainedMarkovDecisionProcess/ ,/ CMDP/ )/ 问题/ ./ 以/ 折扣/ 情形/ 为例/ ,/ 一个/ CMDP/ 模型/ 可/ 表达/ 为/ 约束/ (/ 9/ )/ 中/ c/ (/ st/ ,/ at/ )/ 可/ 视为/ 阶段/ t/ 所/ 产生/ 的/ 资源/ 消耗/ ,/ C/ 为/ 客观/ 资源/ 总量/ 限制/ ./ 这类/ 问题/ 在/ 计算机系统/ 内有/ 大量/ 的/ 应用/ ,/ 如/ Djonin/ 等/ 人/ [/ 17/ ]/ 研究/ 了/ 在/ MIMO/ 系统/ 中/ ,/ 在/ 数据/ 延迟/ 受限/ 的/ 情况/ 下/ ,/ 最小化/ 平均/ 发射功率/ 的/ 问题/ ./ 求解/ CMDP/ 可以/ 使用/ 线性规划/ 法/ [/ 18/ ]/ 与/ 拉格朗/ 日法/ [/ 19/ ]/ 等/ ./ (/ 7/ )/ 按照/ 目标/ 数量/ 分类/ ./ 很多/ 动态/ 优化/ 问题/ 只/ 考虑/ 一个/ 目标/ 函数/ ,/ 也/ 就是/ 常见/ 的/ 单/ 目标/ 优化/ 问题/ ./ 如果/ 目标/ 函数/ 有/ 多个/ ,/ 就/ 需要/ 用/ 多/ 目标/ 优化/ 建模/ ./ 一般/ 处理/ 多/ 目标/ 问题/ 的/ 方式/ 有/ 3/ 种/ :/ ①/ 将/ 一部分/ 目标/ 函数/ 转化/ 为/ 约束/ ,/ 进而/ 转化/ 为/ CMDP/ 模型/ [/ 20/ ]/ ;/ ②/ 将/ 各个/ 目标/ 加权/ 平均/ ,/ 组合成/ 一个/ 整体/ 目标/ [/ 21/ ]/ ;/ ③/ 求解/ 帕雷托/ 前沿/ (/ ParetoFrontier/ )/ [/ 22/ -/ 23/ ]/ ./ 2.4/ 马尔可夫/ 决策/ 过程/ 的/ 求解/ 夫/ 决策/ 过程/ 的/ 求解/ ./ 求解/ 算法/ 分类/ 如图/ 2/ 所示/ ./ 由于/ 篇幅/ 所/ 限/ ,/ 本文/ 主要/ 讨论/ 无穷/ 折扣/ 马尔可/ 2.4/ ./ 1/ 精确/ 求解/ 算法/ 折扣/ 情形/ 下/ 的/ 最优/ 解/ 满足/ 运算符/ T/ 定义/ 如下/ :/ [/ T/ (/ 犞/ )/ ]/ s/ =/ maxa/ ∈/ A/ (/ s/ )/ R/ (/ s/ ,/ a/ )/ +/ α/ ∑/ s/ ∈/ S/ 其中/ ,/ [/ ·/ ]/ s/ 为/ 向量/ 的/ 第/ s/ 个/ 分量/ ./ 满足/ 式/ (/ 10/ )/ 的/ 值/ 函数/ 即/ 是/ 最优/ 值/ 函数/ 犞/ / ./ 可以/ 看到/ ,/ 式/ (/ 11/ )/ 实际上/ 是/ 采取/ 最大化/ 策略/ 的/ 式/ (/ 1/ )/ 的/ 变形/ ./ Page6/ (/ 1/ )/ 值/ 迭代/ 算法/ ./ 值/ 迭代/ 算法/ 实际上/ 是/ 近似算法/ ,/ 随着/ 迭代/ 过程/ 的/ 进行/ ,/ 该/ 算法/ 会/ 不断/ 逼近/ 最优/ 解/ ./ 值/ 迭代/ 算法/ 如/ 算法/ 1/ 所示/ ./ 算法/ 1/ ./ 值/ 迭代/ 算法/ ./ 1/ ./ n/ =/ 0/ ,/ 给定/ 初值/ 犞/ 0/ =/ 狏/ ./ 2/ ./ 根据/ 迭代/ 式/ 犞/ n/ =/ T/ (/ 犞/ n/ -/ 1/ )/ ,/ 计算/ 第/ n/ 次/ 迭代/ 的/ 值/ 函/ 3/ ./ 重复/ 步/ 2/ ./ 可以/ 证明/ ,/ 算法/ 1/ 在/ n/ →/ 时/ 收敛/ 于/ 最优/ 值/ 函数/ 犞/ / ./ 此外/ ,/ 还/ 可以/ 在/ 每/ 一次/ 迭代/ 时/ 估计/ 出/ 最优/ 解/ 的/ 区间/ ,/ 即/ 犞/ n/ +/ α/ 其中/ ,/ 犲/ 为/ 全/ 1/ 向量/ ,/ α/ n/ 与/ β/ n/ 定义/ 如下/ :/ 式/ (/ 12/ )/ 也/ 可以/ 作为/ 值/ 迭代/ 算法/ 运行/ 结束/ 的/ 判定/ 方法/ ./ 例如/ ,/ 可/ 事先/ 指定/ 一/ 精度/ ε/ ,/ 使得/ 成立/ 时/ 算法/ 终止/ ./ (/ 2/ )/ 策略/ 迭代/ 算法/ ./ 可以/ 证明/ ,/ 当/ 状态/ 集合/ 与/ 行为/ 集合/ 有限/ 时/ ,/ 策略/ 迭代/ 算法/ 可以/ 在/ 有限/ 迭代/ 次数/ 内/ 获得/ 最优/ 解/ ,/ 且/ 迭代/ 次数/ 上界/ 为/ 策略/ 数/ ,/ 即/ ∏/ s/ ∈/ SA/ (/ s/ )/ ,/ 其中/ |/ ·/ |/ 为/ 集合/ 内/ 的/ 元素/ 个数/ ./ 策略/ 迭代/ 算法/ 如/ 算法/ 2/ 所示/ ./ 算法/ 2/ 首先/ 确定/ 一个/ 初始/ 策略/ π/ 0/ ,/ 并/ 直接/ 根据/ 式/ (/ 2/ )/ 求解/ 得到/ 该/ 策略/ 所/ 对应/ 的/ 值/ 函数/ ./ 最后/ ,/ 再/ 根据/ 所得/ 的/ 值/ 函数/ 对/ 策略/ 进行/ 更新/ ./ 若/ 更新/ 前后/ 的/ 策略/ 相同/ ,/ 则/ 说明/ 已经/ 找到/ 了/ 最优/ 策略/ ,/ 算法/ 结束/ ./ 算法/ 2/ ./ 策略/ 迭代/ 算法/ ./ 1/ ./ n/ =/ 0/ ,/ 给定/ 初始/ 策略/ π/ 0.2/ ./ 通过/ 求解/ (/ 犐/ -/ α/ 犘/ π/ n/ )/ 犞/ n/ =/ 犚/ π/ n/ 确定/ 犞/ n/ ./ 3/ ./ 确定/ π/ n/ +/ 1/ 使/ 其/ 满足/ 4/ ./ if/ π/ n/ +/ 1/ =/ π/ n/ ,/ 算法/ 终止/ ,/ 设定/ 最优/ 策略/ π/ / =/ π/ n/ ./ elsen/ =/ n/ +/ 1/ ,/ 转到/ 步/ 2/ ./ 此外/ ,/ 学者/ 们/ 还/ 基于/ 以上/ 两种/ 基本/ 算法/ 设计/ 了/ 一些/ 变形/ 算法/ ,/ 如/ 修正/ 的/ 策略/ 迭代/ (/ ModifiedPolicyIteration/ )/ 等/ ,/ 此处/ 不再/ 赘述/ ./ 2.4/ ./ 2/ 近似/ 求解/ 算法/ 在/ 一个/ 实际/ 系统/ 中/ ,/ 资源/ 种类/ 与/ 资源/ 数量/ 都/ 极其/ 庞大/ ,/ 导致/ 所/ 建立/ 的/ MDP/ 模型/ 无法/ 利用/ 精确/ 算法/ 进行/ 求解/ ,/ 原因/ 在于/ :/ ①/ 需要/ 为/ 每个/ 状态/ 存储/ 其值/ 函数/ ./ 在/ 状态/ 数较/ 多时/ ,/ 现有/ 的/ 技术/ 无法/ 提供/ 足够/ 的/ 存储空间/ ;/ ②/ 在/ 迭代/ 过程/ 中/ ,/ 计算/ 值/ 函数/ 要/ 遍历/ 所有/ 状态/ ,/ 会/ 导致/ 迭代/ 一次/ 所/ 需/ 时间/ 较长/ ,/ 算法/ 收敛/ 速度/ 太慢/ ./ 基于/ 这些/ 考虑/ ,/ 人们/ 开始/ 寻找/ MDP/ 的/ 近似/ 求解/ 算法/ ,/ 使得/ 在/ 有限/ 的/ 时空/ 复杂度/ 范围/ 内/ ,/ 得到/ 可/ 接受/ 的/ 次优/ 解/ ./ (/ 1/ )/ 贪心/ 算法/ 贪心/ 算法/ 又/ 称为/ 近视/ 策略/ (/ myopicpolicy/ )/ ,/ 它/ 可/ 表示/ 为/ :/ 在/ 时刻/ t/ ,/ 求解/ 如下/ 优化/ 问题/ 例如/ ,/ 在/ 图/ 1/ 的/ 接纳/ 控制/ 问题/ 中/ ,/ 贪心/ 算法/ 为/ 贪心/ 算法/ 是/ 最/ 简单/ 的/ 一类/ 近似算法/ ./ 它/ 只/ 关注/ 系统/ 当前/ 的/ 收益/ ,/ 而/ 忽略/ 当前/ 决策/ 对/ 未来/ 收益/ 的/ 影响/ ./ 这种/ 方法/ 虽然/ 未必/ 是/ 最优/ 的/ ,/ 但是/ 至少/ 提供/ 了/ 一种/ 动态/ 优化/ 问题/ 的/ 简单/ 求解/ 方案/ ./ 贪心/ 算法/ 的/ 最大/ 优点/ 在于/ ,/ 其/ 求解/ 过程/ 没有/ 算法/ 1/ 与/ 算法/ 2/ 中/ 的/ 迭代/ 过程/ ,/ 因而/ 时间/ 复杂度/ 较/ 低/ ./ 此外/ ,/ 也/ 不/ 需要/ 提供/ 存储/ 值/ 函数/ 的/ 空间/ ./ 在/ 一些/ 特殊/ 的/ 动态/ 优化/ 模型/ 中/ ,/ 贪心/ 策略/ 就是/ 最优/ 策略/ ./ Karush/ 与/ Dear/ [/ 24/ ]/ 将/ 一个/ 学习/ 过程/ 利用/ POMDP/ 建模/ ,/ 并/ 证明/ 了/ 贪心/ 策略/ 在/ 该类/ 问题/ 中/ 的/ 最优性/ ./ Krishnamurthy/ 等/ 人/ [/ 25/ ]/ 研究/ 了/ 目标/ 跟踪/ 中/ 的/ 动态/ 传感器/ 调度/ 问题/ ,/ 并/ 给出/ 了/ 贪心/ 策略/ 是/ 最优/ 策略/ 的/ 一些/ 充分条件/ ./ 文献/ [/ 26/ -/ 28/ ]/ 分别/ 从/ 不同/ 角度/ 研究/ 了/ 机会/ 频谱/ 接入/ 问题/ ,/ 并/ 证明/ 了/ 贪心/ 策略/ 的/ 最优性/ ./ 然而/ ,/ 通常/ 情况/ 下/ 贪心/ 策略/ 并非/ 最优/ 策略/ ./ 如/ Ahmad/ 等/ 人/ [/ 29/ ]/ 指出/ ,/ 在/ 负相关/ 系统/ 转移/ 的/ 机会/ 频谱/ 接入/ 问题/ 中/ ,/ 若/ 信道/ 都/ 是/ 独立/ 同/ 分布/ 的/ Gilbert/ -/ Elliot/ 信道/ ,/ 当/ 信道/ 数量/ 大于/ 3/ 时/ ,/ 贪心/ 策略/ 并/ 不是/ 最优/ 策略/ ./ 虽然/ 如此/ ,/ 在/ 很多/ 应用/ 中/ ,/ 贪心/ 算法/ 表现/ 出较/ 好/ 的/ 适应性/ [/ 30/ -/ 31/ ]/ ./ (/ 2/ )/ 基于/ 状态/ 聚合/ 的/ 算法/ 精确/ 求解/ 算法/ 应用/ 的/ 最大/ 障碍/ 是/ 状态/ 空间/ 爆炸/ 问题/ ./ 因此/ ,/ 一种/ 很/ 直观/ 的/ 近似/ 求解/ 策略/ 是/ 将/ 问题/ 空间/ 进行/ 聚合/ 化简/ ,/ 使得/ 问题/ 规模/ 减少/ ,/ 便于/ 精确/ 算法/ 求解/ ./ 以图/ 1/ 中/ 问题/ 为例/ ,/ 假设/ 接纳/ 控制器/ 的/ 等待/ 队列/ 与/ 所有/ 服务器/ 的/ 服务/ 队列/ 最大/ 容量/ 均/ 为/ 100/ 个任/ Page7/ 务/ ,/ 则/ 该/ 问题/ MDP/ 模型/ 的/ 状态/ 空间/ 共有/ 100n/ +/ 1/ 个/ 状态/ ./ 此时/ ,/ 可/ 设定/ 如下/ 状态/ 聚合/ 策略/ :/ 为/ 每个/ 队列/ 设定/ 一个/ 阈值/ ,/ 当/ 队列/ 长度/ 低于/ 该/ 阈值/ 时/ ,/ 则/ 认为/ 处于/ 宏/ 状态/ “/ 低/ 负载/ ”/ ,/ 反之/ ,/ 则/ 处于/ 宏/ 状态/ “/ 高/ 负载/ ”/ ./ 这样/ ,/ 每个/ 队列/ 的/ 状态/ 可以/ 化简/ 为/ 2/ ,/ 整个/ 系统/ 的/ 状态/ 也/ 缩小/ 为/ 2n/ +/ 1/ ./ 一种/ 常用/ 的/ MDP/ 状态/ 聚合/ 方法/ 来源于/ 马尔可夫/ 过程/ 的/ 近似/ 求解/ 理论/ ,/ 见/ 算法/ 3/ ./ 假设/ 有/ 状态/ 转换/ 如图/ 2/ 所示/ 的/ 马尔可夫/ 决策/ 过程/ ./ 如果/ 存在/ 一种/ 对/ 状态/ 空间/ 的/ 划分/ ,/ 在/ 每个/ 划分/ 内/ ,/ 任选/ 一个/ 状态/ ,/ 使得/ :/ ①/ 以该/ 状态/ 作为/ 起始/ 状态/ ,/ 则/ 转移/ 到/ 该/ 状态/ 所属/ 划分/ 内/ 状态/ 的/ 概率/ 很大/ ;/ ②/ 以该/ 状态/ 作为/ 起始/ 状态/ ,/ 则/ 转移/ 到/ 不/ 属于/ 该/ 状态/ 所属/ 划分/ 内/ 状态/ 的/ 概率/ 很小/ ,/ 则/ 这个/ MDP/ 可以/ 进行/ 状态/ 聚合/ 化简/ ./ 例如/ 在/ 图/ 3/ 中/ ,/ 实线/ 转移/ 概率/ 比/ 虚线/ 转移/ 概率/ 大/ 很多/ ,/ 则/ 该/ 模型/ 可以/ 利用/ 图中/ 所示/ 方式/ 进行/ 聚合/ ./ Liu/ 等/ 人/ [/ 32/ ]/ 利用/ 该/ 方法/ 近似/ 求解/ 了/ 分布式/ Web/ 服务/ 系统/ 中/ 的/ 服务器/ 选择/ 问题/ ./ 算法/ 3/ ./ MDP/ 状态/ 空间/ 化简/ 算法/ ./ 1/ ./ 将/ 状态/ 空间/ S/ 进行/ 划分/ :/ {/ S1/ ,/ S2/ ,/ …/ ,/ Sn/ }/ ./ 2/ ./ fori/ =/ 1ton3/ ./ 将/ 所有/ 转到/ Si/ 以外/ 状态/ 的/ 概率/ 都/ 设置/ 为/ 0.4/ ./ 将/ Si/ 内/ 的/ 状态/ 转移/ 概率/ 进行/ 归一化/ 处理/ ./ 5/ ./ 计算/ Si/ 内/ 状态/ 的/ 稳态/ 概率分布/ ,/ 利用/ π/ =/ π/ 犘/ ,/ 其中/ 6/ ./ 计算/ Si/ 到/ Sj/ 的/ 转移/ 概率/ Pij/ =/ ∑/ k/ ∈/ Si7/ ./ endfor/ 值得注意/ 的/ 是/ ,/ 只有/ 在/ 上面/ 两个/ 假设/ 条件/ 都/ 满足/ 的/ 时候/ ,/ 算法/ 3/ 才能/ 得到/ 精度/ 较/ 高/ 的/ 近似/ 解/ ./ 否则/ ,/ 误差/ 会/ 比较/ 大/ ./ 此外/ ,/ 这种/ 近似/ 方法/ 还有/ 一个/ 缺点/ ,/ 即/ 不能/ 得到/ 近似/ 解与/ 精确/ 解/ 之间/ 的/ 关系/ ./ 为了/ 克服/ 这些/ 问题/ ,/ 学者/ 们/ 又/ 提出/ 了/ 其它/ 解决方案/ ,/ 有/ 界/ 参数/ MDP/ (/ Bounded/ -/ ParametersMDP/ ,/ BMDP/ )/ 就是/ 这些/ 方案/ 中较/ 有/ 影响力/ 的/ 方法/ 之一/ ./ BMDP/ 由/ Givan/ 等/ 人/ [/ 33/ -/ 34/ ]/ 提出/ ,/ 它/ 是非/ 精确/ 状态/ 转移/ 概率/ MDP/ (/ MarkovDecisionProcesseswithImpreciselyKnownTransitionProbabilities/ ,/ MDPIPs/ )/ 模型/ 的/ 一种/ 特殊/ 情况/ ./ BMDP/ 是/ 一个/ 4/ 元组/ S/ ,/ A/ ,/ R/ / ,/ P/ {/ 每个/ 状态/ 的/ 收益/ 函数/ R/ / 与/ 状态/ 转移/ 概率/ P/ / 是/ 一个/ 区间/ ,/ 而/ 不是/ 一个点/ 值/ ./ 若/ 一个/ MDPM/ ,/ 其/ 状态/ 、/ 行为/ 集/ 与/ BMDPM/ / 的/ 状态/ 、/ 行为/ 集/ 完全相同/ ,/ 且/ M/ 的/ 收益/ 函数/ R/ 与/ 转移/ 概率/ P/ 都/ 在/ M/ / 所/ 规定/ 的/ 区间/ 内/ ,/ 则/ 称/ M/ ∈/ M/ / ./ 在/ 一个/ BMDPM/ / 中/ ,/ 给定/ 一个/ 决策/ 策略/ π/ ,/ 则/ 该/ 策略/ 所/ 产生/ 的/ 值/ 函数/ 也/ 是/ 一个/ 区间/ ,/ 称为/ 区间/ 值/ 函数/ Va/ 其中/ V/ π/ M/ / 中/ 的/ 一个/ MDPM/ 的/ 值/ 函数/ ./ 可/ 根据/ 实际/ 工程/ 应用/ 背景/ ,/ 定义/ 区间/ 值/ 函数/ 的/ 比较/ 方法/ ./ 例如/ 对于/ 策略/ a/ 与/ b/ ,/ 可定义/ :/ ①/ 乐观/ 最优/ / / Vb/ ②/ 悲观/ 最优/ / / VbVa/ 可以/ 证明/ ,/ 存在/ M/ ∈/ M/ / ,/ 使得/ 所有/ 状态/ 的/ 值/ 函数/ 能/ 同时/ 达到/ 最大/ 或/ 最小/ ,/ 并称/ 这/ 两个/ MDP/ 分别/ 为/ 关于/ 策略/ π/ 的/ 最大/ MDP/ 与/ 最小/ MDP/ ./ 寻找/ 最大/ 或/ 最小/ MDP/ 的/ 过程/ ,/ 相当于/ 寻找/ 关于/ 值/ 函数/ 上/ 界/ 降序/ 排列/ 状态/ 空间/ 序列/ 与值/ 函数/ 下界/ 升序/ 排列/ 状态/ 空间/ 序列/ 的/ 序列/ 最大/ MDP/ (/ OrderMaximizingMDP/ )/ ./ 具体来讲/ ,/ 一个/ 状态/ 空间/ 序列/ O/ =/ {/ s1/ ,/ s2/ ,/ …/ ,/ sn/ }/ 为/ 状态/ 空间/ 中/ 所有/ 状态/ 的/ 一个/ 排列/ 顺序/ ,/ 则/ 状态/ 空间/ 序列/ O/ 的/ 序列/ 最大/ 下标/ r/ 与/ 序列/ 最大/ MDPMO/ 可定义/ 如下/ ./ 定义/ 1/ (/ 序列/ 最大/ 下标/ 与/ 序列/ 最大/ MDP/ )/ [/ 34/ ]/ ./ 对于/ 某个/ 状态/ s/ 与/ 决策/ 行为/ a/ ,/ 其/ 关于/ 序列/ O/ 的/ 序列/ 最大/ 下标/ r/ 为/ argmax1/ / r/ / n/ ∑/ r/ -/ 1/ 相应/ 的/ 序列/ 最大/ MDP/ 是/ 一个/ 满足/ 式/ (/ 18/ )/ 的/ MDPMO/ ∈/ M/ 利用/ BMDP/ 可以/ 对/ 问题/ 空间/ 进行/ 状态/ 聚合/ ./ 一个/ 精确/ MDP/ 经过/ 聚合/ 后/ ,/ 一般/ 都/ 可以/ 归结为/ 一个/ BMDP/ 问题/ ,/ 可以/ 利用/ 区间/ 迭代/ 求解/ 算法/ 进行/ 求解/ ,/ 即/ Page8IVI/ / opt/ (/ V/ / )/ (/ s/ )/ =/ maxa/ ∈/ A/ (/ s/ )/ [/ minM/ ∈/ M/ / 计算/ 式/ (/ 20/ )/ 实际上/ 可以/ 看/ 作为/ 具有/ 2/ 个/ 决策者/ 的/ 2/ 步/ 博弈/ 过程/ ./ 以/ 乐观/ 最优/ 为例/ ,/ 在/ 第/ 1/ 步中/ ,/ 决策者/ 1/ 与/ 决策者/ 2/ 为/ 合作/ 配合/ 关系/ ,/ 决策者/ 1/ 利用/ 所/ 定义/ 的/ 乐观/ 最优/ 比较/ 运算符/ / 求得/ 最大化/ 区间/ 值/ 函数/ 上界/ 的/ 策略/ π/ ↑/ opt/ ./ 在/ 第/ 2/ 步中/ ,/ 决策者/ 1/ 与/ 决策者/ 2/ 为/ 对立/ 竞争/ 关系/ ,/ 决策者/ 2/ 求得/ 策略/ π/ ↑/ opt/ 的/ 最小/ MDP/ ,/ 并/ 计算/ 区间/ 值/ 函数/ 的/ 下界/ ./ 该/ 过程/ 可用/ 算法/ 4/ 描述/ ./ 其中/ ,/ Sort/ _/ Dec/ _/ Order/ 与/ Sort/ _/ Inc/ _/ Order/ 为/ 排序/ 函数/ ,/ Order/ _/ Max/ _/ Ind/ 利用/ 式/ (/ 17/ )/ 求得/ 对应/ 的/ 序列/ 最大/ 下标/ ./ 这样/ ,/ 就/ 可以/ 在/ 缩小/ 的/ 问题/ 空间/ 中/ ,/ 求得/ 原/ 问题/ 具有/ 边界/ 的/ 解/ ./ 算法/ 4/ ./ 区间/ 迭代/ 算法/ ./ 1/ ./ Oup/ =/ Sort/ _/ Dec/ _/ Order/ (/ V/ ↑/ )/ ,/ 2/ ./ foralls/ ∈/ Sdo3/ ./ foralls/ ∈/ Sdo4/ ./ rup/ =/ Order/ _/ Max/ _/ Ind/ (/ M/ / ,/ Oup/ ,/ s/ ,/ a/ )/ ./ 5/ ./ rdown/ =/ Order/ _/ Max/ _/ Ind/ (/ M/ / ,/ Odown/ ,/ s/ ,/ a/ )/ ./ 6/ ./ fori/ =/ 1tondo7/ ./ 根据/ 式/ (/ 18/ )/ 、/ (/ 19/ )/ 计算/ Pup/ (/ sOdown/ (/ i/ )/ |/ s/ ,/ a/ )/ 与/ 8/ ./ endfor9/ ./ endfor10/ ./ V/ ↑/ =/ max/ 犪/ / A/ (/ s/ )/ R/ ↑/ (/ s/ ,/ a/ )/ +/ α/ ∑/ s/ ∈/ S11/ ./ if/ |/ a/ |/ =/ 1anda/ =/ {/ a/ }/ then12/ ./ V/ ↓/ =/ R/ ↓/ (/ s/ ,/ a/ )/ +/ α/ ∑/ s/ ∈/ S13/ ./ π/ (/ s/ )/ =/ a/ ./ 14/ ./ else15/ ./ π/ (/ s/ )/ =/ a/ ./ 16/ ./ endif17/ ./ endfor/ (/ 3/ )/ 基于/ 近似/ 动态/ 规划/ 的/ 算法/ 近似/ 动态/ 规划/ (/ ApproximateDynamicPro/ -/ gramming/ ,/ ADP/ )/ 是/ 一种/ 解决/ 大规模/ 动态/ 优化/ 问题/ 的/ 现代/ 近似/ 求解/ 方法/ ./ 目前/ ,/ 关于/ 近似/ 动态/ 规划/ 的/ 代表性/ 专著/ ,/ 主要/ 有/ 3/ 本/ [/ 12/ ,/ 35/ -/ 36/ ]/ ,/ 分别/ 从/ 人工智能/ 、/ 控制论/ 以及/ 运筹学/ 的/ 角度/ 对/ 近似/ 动态/ 规划/ 进行/ 了/ 详细/ 的/ 论述/ ./ 近似/ 动态/ 规划/ 能/ 有效/ 解决/ 马尔可夫/ 决策/ 过程/ 中/ 的/ 状态/ 空间/ 爆炸/ 问题/ ./ 在/ ADP/ 中/ ,/ 式/ (/ 11/ )/ 通常/ 改写/ 为/ V/ (/ st/ )/ =/ maxat/ ∈/ A/ (/ st/ )/ R/ (/ st/ ,/ at/ )/ +/ α/ ·/ E/ {/ V/ (/ st/ +/ 1/ )/ }/ (/ 21/ )/ 在/ 式/ (/ 21/ )/ 中/ ,/ 状态/ 空间/ 爆炸/ 问题/ 表现/ 为/ :/ (/ 1/ )/ 问题/ 状态/ 空间/ S/ 太大/ ,/ 现有/ 的/ 技术/ 无法/ 提供/ 足够/ 的/ 存储空间/ ;/ (/ 2/ )/ 外部/ 随机变量/ 有时/ 无法/ 精确测量/ 其/ 分布/ ,/ 或/ 即使/ 分布/ 已知/ ,/ 也/ 会/ 由于/ 随机变量/ 状态/ 太多/ 而/ 导致/ 其/ 期望/ 难于/ 计算/ ./ 在/ 近似/ 动态/ 规划/ 中/ ,/ 主要/ 使用/ 基于/ 值/ 函数/ 近似/ (/ ValueFunctionApproximation/ )/ 与/ 后/ 决策/ 状态/ (/ Post/ -/ DecisionStateVariable/ )/ 的/ 前/ 向/ 动态/ 规划/ 方法/ 来/ 克服/ 以上/ 问题/ ./ 令/ 系统/ 的/ 状态/ 转换/ 方程/ 为/ 其中/ ,/ W/ (/ ω/ t/ )/ 是/ t/ 时刻/ 外部/ 随机变量/ 的/ 一个/ 样本/ ,/ 则/ 基本/ 的/ 近似/ 动态/ 规划/ 算法/ 可/ 表述/ 为/ 算法/ 5/ ./ 算法/ 5/ ./ 基本/ 近似/ 动态/ 规划/ 算法/ ./ 1/ ./ 初始化/ :/ 2/ ./ fort/ =/ 0toTdo3/ ./ 求解/ 4/ ./ 利用/ 下式/ 对/ 珚/ V/ (/ st/ )/ 进行/ 更新/ 5/ ./ 选定/ 一个/ 采样/ 路径/ ω/ t/ ./ 6/ ./ 计算/ 下/ 一个/ 状态/ 7/ ./ endfor/ 算法/ 5/ 首先/ 初始化/ 所有/ 状态/ 的/ 值/ 函数/ ,/ 并/ 指定/ 一个/ 初始状态/ ./ 然后/ ,/ 利用/ MonteCarlo/ 方法/ 对/ 随机/ 外部/ 信息/ 进行/ 一次/ 采样/ ./ 算法/ 的/ 核心/ 是步/ 2/ ~/ 7/ ,/ 首先/ 求解/ 一步/ 优化/ 问题/ (/ 步/ 3/ )/ ,/ 并/ 利用/ 所/ 得出/ 的/ v/ ^/ t/ 对值/ 函数/ 进行/ 更新/ ./ 其中/ η/ t/ 是/ 步长/ ./ 该/ 算法/ 与/ 用于/ 求解/ 一般/ 马尔可夫/ 决策/ 过程/ 迭代/ 算法/ 的/ 最/ 根本/ 区别/ ,/ 在于/ 时间/ 是/ 顺序/ 演进/ 的/ ,/ 而/ 不是/ 倒序/ 演进/ 的/ ./ 算法/ 运行/ 的/ 过程/ ,/ 实际上/ 是/ 一个/ 系统/ 仿真/ 的/ 过程/ ./ 以图/ 1/ 中/ 接纳/ 控制/ 问题/ 为例/ ,/ 其/ ADP/ 求解/ 算法/ 可/ 描述/ 如下/ :/ 1/ ./ 设定/ 每个/ 状态值/ 函数/ 的/ 初始值/ ,/ 选取/ 起始/ 状态/ ,/ 并令/ t/ =/ 0.2/ ./ 采集/ t/ 时刻/ 系统/ 状态/ ,/ 根据/ 当前/ 的/ 值/ 函数/ ,/ 计算/ 当前/ 决策/ 行为/ xt/ ,/ 并/ 得到/ 当前/ 状态值/ 函数/ 的/ 一个/ 样本/ v/ ^/ t/ (/ 算法/ 5/ 步/ 3/ )/ ./ 其中/ ,/ E/ 珚/ V/ (/ st/ +/ 1/ )/ |/ s/ {/ }/ t/ 可用/ MonteCarlo/ 模拟/ 的/ 方法/ 求得/ ./ 3/ ./ 根据/ 值/ 函数/ 样本/ ,/ 更新/ 当前/ 状态/ 的/ 值/ 函数/ (/ 算法/ 5/ 步/ 4/ )/ ./ Page94/ ./ 使用/ MonteCarlo/ 方法/ 得到/ 外部/ 随机变量/ 的/ 样本/ ,/ 即/ 任务/ 到达/ 数/ λ/ t/ 与/ 任务/ 完成/ 数/ μ/ it/ ./ 5/ ./ t/ ←/ t/ +/ 1/ ,/ 并/ 根据/ 式/ (/ 8/ )/ 得到/ t/ +/ 1/ 时刻/ 的/ 系统/ 状态/ ,/ 重复/ 步/ 2/ ./ 该/ 算法/ 其/ 优点/ 显而易见/ ,/ 在/ 迭代/ 过程/ 中/ 不/ 需要/ 枚举/ 系统/ 的/ 所有/ 状态/ 来/ 计算/ 值/ 函数/ ,/ 一定/ 程度/ 上/ 规避/ 了/ 状态/ 空间/ 爆炸/ 问题/ ./ 但是/ ,/ 算法/ 5/ 中/ 仍然/ 存在/ 不足/ ./ 例如/ ,/ 该/ 算法/ 为/ 每个/ 状态/ 均/ 设立/ 一个/ 变量/ 珚/ V/ (/ s/ )/ 用以/ 存储/ 其值/ 函数/ ./ 当/ 问题/ 状态/ 空间/ 较大/ 时/ ,/ 难以/ 提供/ 足够/ 的/ 存储空间/ ./ 同时/ ,/ 该/ 算法/ 只/ 更新/ 所/ 遍历/ 到/ 的/ 状态/ 的/ 值/ 函数/ ,/ 而/ 未/ 遍历/ 到/ 的/ 状态/ 的/ 值/ 函数/ 却/ 得不到/ 更新/ ./ 下面/ 我们/ 就/ 算法/ 5/ 中/ 的/ 各个/ 步骤/ 展开/ 论述/ ,/ 详细/ 介绍/ 近似/ 动态/ 规划/ 算法/ 克服/ 状态/ 空间/ 爆炸/ 问题/ 的/ 主要/ 手段/ ./ ①/ 后/ 决策/ 状态/ 后/ 决策/ 状态/ 是/ 决策者/ 做/ 完/ 决策/ 后/ 、/ 且/ 外部/ 随机/ 信息/ 到达/ 前/ 系统/ 的/ 状态/ ./ 这样/ ,/ 式/ (/ 22/ )/ 就/ 分为/ 了/ 两步/ :/ 其中/ ,/ sxt/ 称为/ t/ 时刻/ 的/ 后/ 决策/ 状态/ ,/ st/ +/ 1/ 称为/ t/ +/ 1/ 时刻/ 的/ 前/ 决策/ 状态/ ./ 后/ 决策/ 状态/ 可以/ 看/ 作为/ 前/ 决策/ 状态/ 与/ 决策/ 行为/ 的/ 确定/ 函数/ ./ 以图/ 1/ 中/ 接纳/ 控制/ 问题/ 为例/ ,/ {/ λ/ t/ ,/ q1t/ ,/ …/ ,/ qnt/ }/ 为/ 系统/ 的/ 前/ 决策/ 状态/ ,/ 而/ 其后/ 决策/ 状态/ 为/ {/ qx/ ,/ 1t/ ,/ …/ ,/ qx/ ,/ nt/ }/ ,/ 它们/ 之间/ 的/ 状态/ 转移/ 如下/ 后/ 决策/ 状态/ 的/ 值/ 函数/ 定义/ 如下/ :/ 注意/ 到式/ (/ 26/ )/ 中/ ,/ 等式/ 右边/ 已经/ 没有/ 期望/ 运算/ ./ 即/ 它/ 是/ 下/ 一/ 时刻/ 前/ 决策/ 状态值/ 函数/ 的/ 期望/ ./ 此时/ ,/ 步/ 3/ 可以/ 改写/ 为/ v/ ^/ t/ =/ maxat/ ∈/ A/ (/ st/ )/ R/ (/ st/ ,/ at/ )/ +/ α/ ·/ 珚/ V/ (/ SM/ ,/ x/ (/ st/ ,/ at/ )/ )/ (/ 26/ )/ ②/ 值/ 函数/ 近似/ 在/ 算法/ 1/ 与/ 算法/ 2/ 中/ ,/ 值/ 函数/ 表现/ 为/ 一种/ “/ 查表/ ”/ 形式/ (/ TableLookupForm/ )/ ,/ 即/ 算法/ 需要/ 维护/ 一个/ 值/ 函数/ 表/ ,/ 表项/ 为/ 每个/ 状态/ s/ 所/ 对应/ 的/ 值/ 函数/ V/ (/ s/ )/ ./ 这种/ 方式/ 使得/ 值/ 函数/ 的/ 存储/ 与/ 计算/ 都/ 较为/ 困难/ ./ 在/ ADP/ 中/ ,/ 可以/ 利用/ 函数/ 近似/ 的/ 方法/ ,/ 利用/ 一些/ 简单/ 的/ 函数/ 形式/ 拟合/ 后/ 决策/ 状态/ 的/ 值/ 函数/ ./ 线性/ 值/ 函数/ 近似/ 是/ 普遍/ 使用/ 的/ 一种/ 值/ 函数/ 近似/ 方法/ ./ 令/ / 为/ 动态/ 优化/ 问题/ 中/ 的/ 特征/ 集/ ,/ 该/ 特征/ 集/ 与/ 问题/ 结构/ 本身/ 有/ 较大/ 相关性/ ./ 如/ 在/ 分布式/ 库存/ 管理/ 问题/ 中/ ,/ 特征/ 集/ 可以/ 包括/ 各地/ 库存量/ 、/ 各地/ 仓库/ 在/ 单位/ 时间/ 内/ 到达/ 的/ 货物/ 量/ 、/ 库存/ 变化/ 的/ 方差/ 以及/ 这些/ 特征/ 的/ 平方/ 等/ [/ 37/ ]/ ./ 定义/ 基/ 函数/ (/ BasisFunction/ )/ / f/ (/ sxt/ )/ ,/ f/ ∈/ / 为/ 关于/ 后/ 决策/ 状态/ sxt/ 中/ 某/ 一/ 特征/ f/ 数量/ 关系/ 的/ 函数/ ,/ 即/ / f/ (/ sxt/ )/ 为/ 从/ 后/ 决策/ 状态/ 集合/ 到/ 实数/ 集合/ 的/ 映射/ ,/ 则/ 后/ 决策/ 状态/ 的/ 值/ 函数/ 可以/ 利用/ 如下/ 方式/ 进行/ 近似/ :/ 此时/ 算法/ 5/ 中步/ 3/ 可以/ 进一步/ 改写/ 为/ v/ ^/ t/ =/ maxat/ ∈/ A/ (/ st/ )/ {/ R/ (/ st/ ,/ at/ )/ +/ α/ ·/ ∑/ f/ ∈/ / 这样/ ,/ 估计值/ 函数/ 的/ 过程/ ,/ 就/ 转化/ 为/ 估计/ θ/ f/ 的/ 过程/ ,/ 即/ θ/ f/ 随/ 时间/ 演进/ 而/ 不断更新/ ,/ 因此/ 也/ 可/ 记作/ θ/ f/ ,/ t/ ./ 一般/ 情况/ 下/ ,/ 特征/ 集/ 的/ 空间/ 远/ 小于/ 问题/ 的/ 状态/ 空间/ ./ 因此/ ,/ 值/ 函数/ 近似/ 可以/ 较/ 好地解决/ 状态/ 空间/ 爆炸/ 的/ 问题/ ./ ③/ 值/ 函数/ 样本/ 的/ 取得/ 2.1/ 小节/ 提到/ ,/ 状态/ st/ 的/ 值/ 函数/ V/ (/ st/ )/ 为/ 从/ 状态/ st/ 开始/ 到/ 时间/ 趋于/ 无穷/ 时/ 收益/ 函数/ 的/ 累加/ ./ 在/ 策略/ π/ 作用/ 下/ ,/ V/ (/ st/ )/ 的/ 一个/ 无偏/ 估计/ 样本/ 可以/ 直观/ 地写/ 为式/ (/ 29/ )/ 可以/ 用/ 一个/ 有限/ 时间/ 累计/ 收益/ 进行/ 近似/ ,/ 即取/ 一个/ 足够/ 大/ 的/ T/ ,/ 使得/ α/ T/ -/ t/ →/ 0/ ,/ 则/ 式/ (/ 29/ )/ 还/ 可/ 改写/ 为/ v/ ^/ (/ st/ )/ =/ ∑/ V/ (/ st/ )/ -/ α/ V/ (/ s/ )/ 由于/ α/ ∈/ (/ 0/ ,/ 1/ )/ 且/ V/ (/ s/ )/ 有/ 界/ ,/ 因而/ α/ V/ (/ s/ )/ →/ 0/ ,/ 式/ (/ 31/ )/ 可/ 近似/ 地/ 变换/ 为/ v/ ^/ (/ st/ )/ =/ V/ (/ st/ )/ +/ ∑/ 其中/ R/ (/ s/ τ/ ,/ a/ π/ τ/ )/ -/ V/ (/ s/ τ/ )/ +/ α/ V/ (/ s/ τ/ +/ 1/ )/ 称为/ 即时/ 差分/ (/ TemporalDifference/ ,/ TD/ )/ 或/ Bellman/ 误差/ (/ Bell/ -/ manError/ )/ ,/ 表示/ 当前/ 值/ 函数/ 估计值/ 与/ 上次/ 值/ 函数/ 估计值/ 之间/ 的/ 差/ ./ 在/ 一些/ 文献/ 中/ ,/ 折扣/ 因子/ α/ 有时/ 用/ λ/ 表示/ ,/ 因此/ 这种/ 取得/ 值/ 函数/ 样本/ 的/ 方法/ 又/ 叫做/ TD/ (/ λ/ )/ ./ 当/ 折扣/ 因子/ α/ =/ 0/ 时/ ,/ 又/ 可以/ 得到/ 一种/ 特殊/ 的/ 表示/ 方式/ :/ 式/ (/ 33/ )/ 称为/ TD/ (/ 0/ )/ ./ 注意/ 式/ (/ 33/ )/ 与/ 带有/ 后/ 决策/ 状/ Page10/ 态变量/ 的/ Bellman/ 方程/ (/ 26/ )/ 极为/ 相似/ ./ 当/ π/ 为式/ (/ 26/ )/ 中/ 的/ 最大化/ 策略/ 时/ ,/ V/ (/ st/ +/ 1/ )/ 为式/ (/ 26/ )/ 中/ 珚/ V/ (/ SM/ ,/ x/ (/ st/ ,/ at/ )/ )/ 的/ 无偏/ 样本/ ./ 当/ 利用/ 形如式/ (/ 27/ )/ 所示/ 的/ 值/ 函数/ 近似/ 方法/ 时/ ,/ ADP/ 算法/ 并/ 不/ 关注/ 值/ 函数/ 本身/ ,/ 而/ 着重/ 考察/ 值/ 函数/ 的/ 导数/ θ/ f/ ./ 例如/ ,/ 在/ 资源管理/ 问题/ 中/ ,/ / f/ (/ sxt/ )/ 可以/ 代表/ 具有/ 某一/ 特性/ f/ 的/ 资源/ 的/ 数量/ ,/ 这时/ ,/ θ/ f/ 的/ 物理/ 含义/ 是/ 该类/ 资源/ 的/ 边际/ 收益/ [/ 38/ ]/ ./ θ/ f/ 的/ 样本/ 可以/ 通过/ 以下/ 两种/ 方法/ 得到/ :/ (/ i/ )/ 优化/ 问题/ 的/ 对偶/ 变量/ ./ 一般/ 资源管理/ 问题/ 都/ 存在/ 资源/ 数量/ 的/ 约束/ ,/ 该/ 约束/ 所/ 对应/ 的/ 对偶/ 变量/ θ/ ^/ f/ 就是/ 资源/ 的/ 影子/ 价格/ ,/ 即/ θ/ f/ 的/ 样本/ ./ (/ ii/ )/ 数值/ 微分/ ./ 在/ 状态/ st/ ,/ 根据/ 式/ (/ 33/ )/ 可得/ v/ ^/ (/ st/ )/ ./ 此时/ ,/ 可/ 将/ 状态/ st/ 的/ f/ 类/ 资源/ 的/ 数量/ 减/ 1/ 得到/ 状态/ 珓/ st/ ,/ 重新/ 进行/ 优化/ ,/ 得到/ v/ ^/ (/ 珓/ st/ )/ ,/ 则/ 数值/ 微分/ 可/ 表示/ 为/ ④/ 值/ 函数/ 更新/ 方法/ 随机/ 梯度/ 法是/ 一种/ 常用/ 的/ 值/ 函数/ 更新/ 方法/ ,/ 可以/ 通过/ 逐步/ 学习/ 值/ 函数/ 样本/ v/ ^/ ,/ 使/ 珚/ V/ 不断/ 逼近/ 真实/ 值/ 函数/ ./ 随机/ 梯度/ 法/ 的/ 目标/ 是/ 即/ 寻找/ 最/ 符合/ 样本/ v/ ^/ 的/ 值/ 函数/ 珚/ V/ ./ 由于/ v/ ^/ (/ s/ )/ 是/ 一个/ 随机变量/ ,/ 因此/ 该/ 问题/ 为/ 一个/ 随机/ 优化/ 问题/ ,/ 其/ 求解/ 算法/ 与/ 静态/ 优化/ 问题/ 中/ 的/ 梯度/ 法/ 类似/ ,/ 称为/ 随机/ 梯度/ 法/ ./ 若/ 在/ t/ 时刻/ ,/ 系统/ 位于/ 状态/ s/ ,/ 对应/ 的/ 步长/ 为/ η/ t/ ,/ 则/ 珚/ V/ (/ s/ )/ ←/ 珚/ V/ (/ s/ )/ -/ η/ t/ (/ 珚/ V/ (/ s/ )/ -/ v/ ^/ (/ s/ )/ )/ =/ (/ 1/ -/ η/ t/ )/ 珚/ V/ (/ s/ )/ +/ η/ tv/ ^/ (/ s/ )/ ./ 注意/ 到/ 该式/ 就是/ 算法/ 5/ 中步/ 4/ ./ 若/ 使用/ 后/ 决策/ 状态/ 的/ 值/ 函数/ ,/ 则/ 优化/ 目标/ (/ 34/ )/ 变为/ 此时/ 更新/ 方法/ 为/ 珚/ V/ (/ sxt/ -/ 1/ )/ ←/ 珚/ V/ (/ sxt/ -/ 1/ )/ -/ η/ t/ (/ 珚/ V/ (/ sxt/ -/ 1/ )/ -/ v/ ^/ (/ st/ )/ )/ 若/ 使用/ 形如式/ (/ 27/ )/ 的/ 后/ 决策/ 状态值/ 函数/ 近似/ 策略/ ,/ 则/ 随机/ 梯度/ 法/ 的/ 目标/ 变为/ 即/ 寻找/ 最/ 接近/ 实际/ 值/ 函数/ 的/ 后/ 决策/ 状态/ 近似值/ 函数/ 珚/ V/ (/ sxt/ -/ 1/ |/ θ/ )/ ./ 此时/ 只/ 需/ 更新/ θ/ :/ θ/ ←/ θ/ -/ η/ t/ 珚/ V/ (/ sxt/ -/ 1/ |/ θ/ )/ -/ v/ ^/ (/ st/ 其中/ ,/ 由式/ (/ 27/ )/ 得/ / θ/ 珚/ V/ (/ sxt/ -/ 1/ |/ θ/ )/ =/ 此外/ ,/ 还有/ 一些/ 基于/ 线性/ 回归/ 的/ 值/ 函数/ 更新/ 算法/ ,/ 如/ 最小/ 二乘/ 即时/ 差分/ (/ LeastSquaresTemporalDifferences/ ,/ LSTD/ )/ 与/ 最小/ 二乘/ 策略/ 估计/ (/ LeastSquaresPolicyEvaluation/ ,/ LSPE/ )/ [/ 35/ ]/ ./ 这/ 两种/ 算法/ 的/ 主要/ 区别/ 在于/ ,/ LSTD/ 采集/ 所有/ 值/ 函数/ 样本/ 后/ 一次/ 进行/ 拟合/ ,/ 而/ LSPE/ 为/ 一种/ 边/ 采集/ 值/ 函数/ 样本/ 边/ 拟合/ 的/ 递归/ 算法/ ./ ⑤/ 状态/ 聚合/ 2.4/ ./ 2/ 节/ 介绍/ 了/ 一些/ 基于/ 状态/ 聚合/ 的/ 近似/ 求解/ 算法/ ./ 事实上/ ,/ ADP/ 中/ 也/ 可以/ 使用/ 状态/ 聚合/ ./ 不失/ 一般性/ ,/ 一个/ 聚合/ 状态/ sg/ 的/ 值/ 函数/ V/ (/ sg/ )/ 可定义/ 为/ 该/ 聚合/ 状态/ 所/ 包含/ 状态/ 的/ 值/ 函数/ 的/ 平均值/ ,/ 即/ 状态/ 聚合/ 解决/ 了/ 状态/ 空间/ 爆炸/ 问题/ ,/ 但是/ 随之而来/ 的/ 问题/ 是/ 如何/ 确定/ 合理/ 的/ 状态/ 聚合/ 策略/ 以/ 获得/ 较/ 好/ 的/ 近似/ 解/ ./ George/ 等/ 人/ [/ 39/ ]/ 提出/ 了/ 一种/ 多层/ 状态/ 聚合/ 的/ 思想/ ,/ 将/ 状态/ 的/ 近似值/ 函数/ 定义/ 为/ 不同/ 层次/ 聚合/ 值/ 函数/ 的/ 加权/ 平均/ ./ 令/ G/ 为/ 聚合/ 层次/ 的/ 集合/ ,/ 则/ 其中/ sg/ 为/ 非/ 聚合/ 状态/ s/ 在/ 第/ g/ 层/ 聚合/ 中/ 所/ 对应/ 的/ 聚合/ 状态/ ./ wg/ 可以/ 通过/ 跟踪/ 各层/ 聚合/ 状态值/ 函数/ 的/ 误差/ 与/ 方差/ 等/ 参数/ 确定/ ./ 这种/ 方法/ 在/ 实际/ 样本/ 较少/ 的/ 问题/ 中/ ,/ 显示/ 出较/ 强/ 的/ 适应性/ ,/ 可以/ 加速/ 算法/ 的/ 收敛/ 速度/ ./ ⑥/ 步长/ 步长/ 一般/ 可/ 分为/ 两类/ ,/ 一类/ 是/ 确定/ 步长/ ,/ 如/ η/ t/ =/ 1/ // (/ t/ +/ 1/ )/ 或/ η/ t/ =/ a/ // (/ t/ +/ a/ )/ 等/ ;/ 另一类/ 是/ 随机/ 步长/ ,/ 这/ 类/ 步长/ 与/ 每次/ 取得/ 的/ 样本/ v/ ^/ 或/ θ/ ^/ 等/ 相关/ ,/ 一般/ 收敛/ 速度/ 较快/ ./ 本文/ 中以/ 确定/ 步长/ 为例/ ,/ 简要/ 介绍/ ADP/ 值/ 函数/ 更新/ 算法/ 中/ 的/ 步长/ ./ 为/ 保证/ 随机/ 梯度/ 法/ 收敛/ ,/ 一般/ 要求/ 确定/ 步长/ η/ 满足/ 如下/ 条件/ :/ (/ i/ )/ η/ t/ / 0/ ;/ (/ ii/ )/ ∑/ Page11/ ./ 在/ 算法/ 5/ 的/ 步/ 4/ 中/ ,/ 由于/ 值/ 函数/ 样本/ v/ ^/ t/ 与/ 所/ 估计/ 的/ 值/ 函数/ 珚/ V/ (/ st/ )/ 单位/ 相同/ ,/ 因而/ 可以/ 简单/ 地取/ 0/ / η/ t/ / 1/ ,/ 如令/ η/ t/ =/ 1/ // (/ t/ +/ 1/ )/ ,/ t/ =/ 0/ ,/ 1/ ,/ …/ ./ 然而/ ,/ 在/ 随机/ 梯度/ 法式/ (/ 37/ )/ 中/ ,/ 由于/ 等式/ 右边/ 珚/ V/ (/ s/ |/ θ/ t/ -/ 1/ )/ -/ v/ (/ θ/ t/ -/ 1/ )/ 与/ θ/ 的/ 单位/ 不/ 一定/ 相同/ ,/ η/ 的/ 取值/ 还/ 需/ 仔细/ 调整/ ./ Powell/ 对/ 步长/ 进行/ 了/ 较为/ 详细/ 的/ 介绍/ [/ 36/ ]/ ,/ 有/ 兴趣/ 的/ 读者/ 可以/ 参考/ ./ ⑦/ 探索/ (/ Exploration/ )/ 与/ 利用/ (/ Exploitation/ )/ 问题/ 算法/ 5/ 中/ 采用/ 前向/ 动态/ 规划法/ ,/ 且/ 下/ 一个/ 状态/ st/ +/ 1/ 的/ 选取/ 都/ 与/ 当前/ 状态/ st/ 所/ 做/ 的/ 决策/ at/ 有关/ ,/ 这/ 称为/ 依照/ 策略/ 的/ 学习/ 方式/ (/ On/ -/ PolicyLearning/ )/ ./ 这种/ 方法/ 充分利用/ 了/ 前期/ 估计/ 得到/ 的/ 统计/ 信息/ ,/ 会/ 不断/ 提高/ 所/ 遍历/ 到/ 的/ 状态/ 的/ 值/ 函数/ ,/ 而/ 没有/ 遍历/ 过/ 的/ 状态/ 的/ 值/ 函数/ 的/ 数值/ 则/ 相对/ 较/ 低/ ./ 这/ 很/ 容易/ 导致/ 算法/ 收敛/ 于/ 局部/ 最优/ 解而非/ 全局/ 最优/ 解/ ./ 针对/ 这个/ 问题/ ,/ 学者/ 们/ 又/ 提出/ 了/ 不/ 依照/ 策略/ 的/ 学习/ 方式/ (/ Off/ -/ PolicyLearning/ )/ ./ 但是/ ,/ 这种/ 方式/ 不能/ 保证/ ADP/ 算法/ 收敛/ ./ 因此/ ,/ 又/ 提出/ 了/ 一些/ 折中/ 的/ 方案/ ,/ 如/ Boltzmann/ 探索/ [/ 40/ ]/ 等/ ,/ 在/ 算法/ 前期/ ,/ 先/ 利用/ Off/ -/ PolicyLearning/ 遍历/ 尽量/ 多/ 的/ 状态/ ,/ 采集/ 足够/ 多/ 的/ 统计/ 信息/ ,/ 而/ 在/ 算法/ 后期/ ,/ 则/ 使用/ On/ -/ PolicyLearning/ 方法/ ,/ 加快/ 收敛/ 速度/ ./ 3/ 基于/ 马尔可夫/ 决策/ Petri/ 网/ 的/ 动态/ 优化/ 模型/ Beccuti/ 等/ 人/ [/ 41/ -/ 42/ ]/ 于/ 2007/ 年/ 提出/ 了/ 马尔可夫/ 决策/ Petri/ 网/ (/ MarkovDecisionPetriNets/ ,/ MDPN/ )/ ,/ 将/ MDP/ 的/ 思想/ 融入/ 了/ Petri/ 网中/ ,/ 其/ 目的/ 是/ 为了/ 提供/ 一种/ 比/ MDP/ 更/ 高层/ 的/ 建模/ 工具/ ,/ 从/ 宏观/ 的/ 角度/ 反映/ 决策者/ 行为/ 与/ 系统/ 行为/ 的/ 交替/ ,/ 并/ 从/ 语义/ 的/ 角度/ 严格/ 定义/ 两种/ 行为/ 的/ 转换/ 过程/ ./ 3.1/ 马尔可夫/ 决策/ Petri/ 网/ 马尔可夫/ 决策/ Petri/ 网可/ 分为/ 两种/ 子网/ :/ 代表/ 系统/ 行为/ 的/ 随机/ 子网/ (/ ProbabilisticSubnet/ )/ 以及/ 代表/ 决策者/ 行为/ 的/ 非/ 确定/ 子网/ (/ NondeterministicSubnet/ )/ ./ 这/ 两种/ 子网/ 通过/ 立即/ 变迁/ NdtoPr/ 与/ PrtoNd/ 同步/ ./ 随机/ 子网/ 的/ 行为/ 通过/ 两类/ 变迁/ Trunpr/ 与/ Tstoppr/ 来/ 描述/ ./ Trunpr/ 代表/ 系统/ 运行/ 的/ 中间/ 过程/ ,/ 而/ Tstoppr/ 代表/ 系统/ 当前/ 阶段/ 运行/ 过程/ 的/ 终止/ ./ 随机/ 子网/ 中/ 的/ 每个/ 变迁/ 都/ 对应/ 一个/ 权值/ (/ weight/ )/ ,/ 用来/ 计算/ 某个/ 状态/ 下/ 系统/ 可/ 实施变迁/ 的/ 概率/ ./ 此外/ ,/ 每个/ 变迁/ 还/ 对应/ 系统/ 中/ 一个/ 触发/ 该/ 变迁/ 的/ 行为/ (/ act/ )/ ,/ 包括/ 组件/ 集合/ 的/ 一个/ 子集/ ./ 在/ MDPN/ 中/ ,/ 系统/ 由/ 多个/ 组件/ 构成/ ./ 这些/ 组件/ 有些/ 是/ 可控/ 的/ ,/ 有些/ 是/ 不/ 可控/ 的/ ./ 非/ 确定/ 子网/ 用/ 两类/ 变迁/ Tnd/ 制/ 行为/ ,/ 而/ Tndl/ 代表/ 组件/ 级/ 的/ 控制/ 行为/ ./ 与/ 随机/ 子网/ 中/ 的/ 变迁/ 类似/ ,/ 非/ 确定/ 子网/ 中/ 的/ 这/ 两类/ 变迁/ 又/ 可以/ 细分/ 为/ Trunnd/ 定子/ 网中/ 的/ 变迁/ 还/ 对应/ 一个/ 对象/ ,/ 用以/ 说明/ 该/ 变迁/ 对应/ 行为/ 的/ 施加/ 组件/ 对象/ ./ 定义/ 2/ (/ 马尔可夫/ 决策/ Petri/ 网/ )/ [/ 42/ ]/ ./ 一个/ 马尔可夫/ 决策/ Petri/ 网是/ 一个/ 四元组/ MN/ =/ {/ Comppr/ ,/ Compnd/ ,/ Npr/ ,/ Nnd/ }/ ,/ 其中/ Comppr/ 是/ 一个/ 有限/ 非空/ 系统/ 组件/ 集合/ ;/ Comppr/ / Comppr/ ∪/ {/ ids/ }/ 是非/ 空/ 可控/ 组件/ 集合/ ,/ 其中/ ids/ 代表/ 整个/ 系统/ ;/ Npr/ 由/ 3/ 部分/ 构成/ :/ ①/ 一个/ 带有/ 优先级/ 的/ Petri/ 网/ P/ ,/ Tpr/ ,/ Ipr/ ,/ Opr/ ,/ Hpr/ ,/ priopr/ ,/ m/ {/ 的/ 权值/ weight/ :/ Tpr/ →/ / ;/ ③/ 一个/ 对应/ 的/ 行为/ act/ :/ Tpr/ →/ 2Comppr/ ,/ 其中/ Tpr/ =/ Trunpr/ ∪/ Tstoppr/ ;/ Nnd/ 由/ 两/ 部分/ 构成/ :/ ①/ 一个/ 带有/ 优先级/ 的/ Petri/ 网/ {/ P/ ,/ Tnd/ ,/ Ind/ ,/ Ond/ ,/ Hnd/ ,/ priond/ ,/ m0/ }/ ;/ ②/ 一个/ 对应/ 的/ 对象/ obj/ :/ Tnd/ →/ Compnd/ ,/ 其中/ Tnd/ =/ Trunnd/ ∪/ Tstopnd/ ./ 此外/ ,/ MDPN/ 还/ 需/ 满足/ 以下/ 条件/ :/ ①/ 一个/ 变迁/ 不能/ 既/ 是非/ 确定/ 变迁/ 又/ 是/ 随机/ 变迁/ ;/ ②/ 每个/ 系统/ 组件/ 至少/ 可以/ 触发/ 一个/ Tstoppr/ 类型/ 的/ 变迁/ ;/ ③/ 每个/ 可控/ 系统/ 组件/ 至少/ 是/ 一个/ Tstopnd/ 类型/ 变迁/ 的/ 对象/ ./ 在/ MDPN/ 中/ ,/ 收益/ 分为/ 两/ 部分/ ./ 第一/ 部分/ 是/ 状态/ 收益/ ,/ 即/ 系统/ 到达/ 某个/ 状态/ 后/ 得到/ 的/ 收益/ ./ 第二/ 部分/ 是/ 行为/ 收益/ ,/ 定义/ 为/ 一连串/ 决策/ 行为/ 所/ 得到/ 的/ 收益/ ./ 行为/ 收益/ 与/ 行为/ 序列/ 的/ 顺序/ 无关/ ./ 3.2/ 马尔可夫/ 决策/ Petri/ 网/ 的/ 建模/ 与/ 分析/ 当/ 构建/ 好/ 决策者/ 行为/ 子/ 模型/ 与/ 系统/ 行为/ 子/ 模型/ 后/ ,/ 需要/ 加入/ 一些/ 附加/ 的/ 位置/ 与/ 变迁/ ,/ 将/ 两个/ 子/ 模型/ 连接起来/ ./ 一个/ 基本/ 的/ MDPN/ 模型/ 如图/ 4/ 所示/ ./ 位置/ i/ 、/ RunprStoppr/ 系统/ 组件/ 、/ 整个/ 系统/ 以及/ 决策者/ 之间/ 进行/ 同步/ ./ 对于/ 每个/ 组件/ i/ ,/ 都/ 有/ 一个/ Stoppr/ 者/ 采取/ 了/ 针对/ 整个/ 系统/ 的/ 全局性/ 行为/ ,/ 则/ 需/ 插入/ 位置/ Stopnd0/ 与/ Runnd0/ ./ 若/ 采取/ 的/ 是/ 针对/ 某个/ 系统/ 组件/ 的/ 局部/ 行为/ ,/ 则/ 需/ 插入/ 位置/ Stopndi/ 与/ Runndi/ ./ 变迁/ NdtoPr/ 与/ PrtoNd/ 描述/ 系统/ 行为/ 与/ 决策者/ 行为/ 的/ 交替/ 进行/ ./ NdtoPr/ 只有/ 在/ Stopnd0/ 与/ 所有/ Page12Stopndi/ 位置/ 都/ 有/ 标记/ 时/ 才能/ 实施/ ,/ 代表/ 模型/ 由/ 决策/ 态/ 转移/ 到/ 系统/ 运行/ 态/ ./ 而/ PrtoNd/ 相反/ 只有/ 在/ i/ 位置/ 都/ 有/ 标记/ 才能/ 实施/ ,/ 代表/ 由/ 系统/ 运行/ 态/ Stoppr/ 转移/ 到/ 决策/ 态/ ./ 3.3/ 马尔可夫/ 决策/ Petri/ 网/ 的/ 求解/ RGMDPMDPN/ 的/ 求解/ 过程/ 可以/ 分为/ 如下/ 4/ 个/ 步骤/ [/ 41/ ]/ :/ (/ 1/ )/ 由/ MDWN/ 模型/ 求得/ 该/ 模型/ 的/ 可达图/ RG/ 可达/ 状态/ 集合/ (/ ReachabilitySet/ ,/ RS/ )/ 可/ 分为/ 两/ 部分/ :/ 非/ 确定/ 状态/ (/ RSnd/ )/ 与/ 随机/ 状态/ (/ RSpr/ )/ ./ 在/ 非/ 确定/ 状态/ 中/ ,/ 只有/ Tnd/ 类型/ 的/ 变迁/ 是/ 可/ 实施/ 的/ ,/ 而/ 在/ 随机/ 状态/ 中/ ,/ 只有/ Tpr/ 类型/ 的/ 变迁/ 是/ 可/ 实施/ 的/ ./ (/ 2/ )/ 将/ 可达图/ RG/ 规约/ 为/ 非/ 确定/ 可达图/ RGnd/ 在/ RG/ 中/ ,/ 定义/ 非/ 确定/ 子/ 路径/ 与/ 随机/ 子/ 路径/ 分别/ 为/ RG/ 中/ 经过/ 同样/ 类型/ 状态/ 的/ 最大/ 路径/ ./ 搜索/ 所有/ 非/ 确定/ 子/ 路径/ ,/ 并/ 将/ 每个/ 非/ 确定/ 子/ 路径/ 压缩/ 为/ 一个/ 决策/ 状态/ ,/ 代表/ 所有/ 可能/ 的/ 决策/ 行为/ ,/ 得到/ 非/ 确定/ 可达图/ RGnd/ ./ (/ 3/ )/ 将/ 非/ 确定/ 可达图/ RGnd/ 规约/ 为/ MDP/ 可达图/ 搜索/ 所有/ 随机/ 子/ 路径/ ,/ 通过/ 路径/ 途中/ 经过/ 变迁/ 的/ 权值/ ,/ 计算/ 各个/ 路径/ 的/ 概率/ ,/ 并/ 将/ 每个/ 随机/ 子/ 路径/ 压缩/ 为/ RG/ 中/ 的/ 一条/ 有/ 向/ 弧/ ,/ 代表/ 宏观/ 的/ 系统/ 状态/ 转移/ ,/ 得到/ MDP/ 可达图/ RGMDP/ ./ (/ 4/ )/ 计算/ 对应/ MDP/ 的/ 转移/ 概率/ 转移/ 概率/ 矩阵/ 为/ 其中/ ,/ 犘/ (/ pr/ ,/ pr/ )/ 为/ RG/ 中/ 从/ 一个/ 随机/ 状态/ 转移/ 到/ 另/ 一个/ 随机/ 状态/ 、/ 且/ 途中/ 没有/ 非/ 确定/ 状态/ 的/ 概率/ ,/ 犘/ (/ pr/ ,/ nd/ )/ 为/ 从/ 一个/ 随机/ 状态/ 转移/ 到/ 非/ 确定/ 状态/ 的/ 概率/ ./ 转移/ 矩阵/ 犘/ 可用/ 式/ (/ 42/ )/ 进行/ 计算/ :/ 犘/ =/ (/ 5/ )/ 根据/ Bellman/ 方程/ 计算/ MDP/ 中/ 的/ 最优/ 策略/ 可/ 根据/ 算法/ 1/ 或/ 算法/ 2/ 求得/ MDP/ 中/ 的/ 最优/ 策略/ ,/ 也/ 就是/ MDWN/ 模型/ 中/ 的/ 最优控制/ 策略/ ./ 3.4/ 应用/ 与/ 扩展/ 本/ 小节/ 中将/ 以/ 一个/ 可/ 修复/ 系统/ 为例/ [/ 41/ ]/ ,/ 对/ MDWN/ 模型/ 的/ 各个/ 要素/ 进行/ 说明/ ,/ 其/ 模型/ 如图/ 5/ 所示/ ./ 左/ 半部/ 分为/ 随机/ 子网/ ,/ 描述/ 一个/ 既/ 可能/ 正常/ 工作/ (/ 变迁/ Workfine/ )/ 、/ 又/ 可能/ 失效/ (/ 变迁/ FailProc/ )/ 的/ 系统/ 组件/ ./ 右半部/ 分为/ 非/ 确定/ 子网/ ,/ 描述/ 决策者/ 的/ 行为/ ,/ 包括/ 分配资源/ 以/ 维修/ 失效/ 组件/ (/ 变迁/ AssignRes/ )/ 与/ Page13/ 不/ 分配资源/ (/ 变迁/ NoAssignRes/ )/ ./ 在/ 随机/ 子网/ 中/ ,/ Tstoppr/ 类型/ 的/ 变迁/ 有/ WorkFine/ 、/ Fail/ 、/ Wait/ 、/ EndReq/ ,/ Trunpr/ 类型/ 的/ 变迁/ 仅/ 有/ 一个/ ,/ 即/ Resume/ ./ 非/ 确定/ 子网/ 中/ 所有/ 的/ 变迁/ 均/ 为/ Tstopnd/ 类型/ ./ 在/ MDPN/ 模型/ 中/ ,/ 在/ 位置/ 、/ 标记/ 以及/ 变迁/ 中/ 增加/ 颜色/ 的/ 概念/ 后/ ,/ 可/ 进一步/ 得到/ 马尔可夫/ 良构/ Petri/ 网/ (/ MarkovDecisionWell/ -/ formedNets/ ,/ MDWN/ )/ 模型/ ./ 这种/ 模型/ 可以/ 较/ 好/ 地/ 处理/ 具有/ 对称/ 属性/ 的/ 系统/ ,/ 有效/ 地/ 缩小/ 问题/ 空间/ ./ MDPN/ 与/ MDWN/ 的/ 模型/ 与/ 算法/ 都/ 已经/ 集成/ 在/ GreatSPN/ 工具/ 中/ [/ 43/ -/ 45/ ]/ ./ 进行/ 仿真/ 时/ ,/ 在/ 每个/ 系统/ 终结状态/ 可/ 利用/ 式/ (/ 26/ )/ 进行/ 决策/ ,/ 并/ 得到/ 一个/ 值/ 函数/ 样本/ ./ 注意/ 行为/ at/ 可能/ 是/ 一个/ 行为/ 序列/ ./ 此时/ ,/ 可/ 利用/ 式/ (/ 35/ )/ ,/ 在/ 值/ 函数/ 样本/ 的/ 基础/ 上/ ,/ 更新/ 其上/ 一/ 时刻/ 决策/ 终结状态/ 的/ 值/ 函数/ ./ 这样/ ,/ 就/ 将/ ADP/ 中/ 的/ 前/ 向/ 动态/ 规划/ 算法/ 集成/ 到/ 了/ MDWN/ 中/ ./ 目前/ ,/ 针对/ MDPN/ 与/ MDWN/ 模型/ 的/ 应用/ 研究/ 已经/ 逐步/ 开展/ ./ 文献/ [/ 46/ ]/ 分别/ 利用/ MDPN/ // MDWN/ 研究/ 了/ 高质量/ 视频/ 处理/ 中/ 的/ 资源管理/ 问题/ ./ 文献/ [/ 47/ ]/ 研究/ 了/ 无线/ 传感器/ 网络/ 中/ ,/ 对象/ 移动/ 跟踪/ 的/ 最优/ 能源管理/ 问题/ ./ 文献/ [/ 48/ -/ 49/ ]/ 研究/ 了/ 一类/ 非/ 确定/ 可/ 维修/ 故障/ 树/ (/ NondeterministicRepairableFaultTrees/ ,/ NdRFT/ )/ 模型/ 与/ MDWN/ 模型/ 转换/ 的/ 方法/ ,/ 并/ 将/ MDWN/ 模型/ 作为/ 求解/ NdRFT/ 模型/ 最优/ 策略/ 的/ 方法/ ./ 3.5/ MDPN/ 与/ ADP/ 的/ 结合/ 3.3/ 小节/ 中/ 的/ MDPN/ 求解/ 方法/ ,/ 将/ MDPN/ 规约/ 为/ MDP/ ,/ 然后/ 再/ 利用/ 精确/ 求解/ 算法/ 进行/ 求解/ ./ 这种/ 方式/ 使得/ MDPN/ 的/ 求解/ 仍然/ 存在/ “/ 状态/ 空间/ 爆炸/ ”/ 问题/ ./ 为此/ ,/ 我们/ 将/ MDPN/ 与/ ADP/ 结合/ ,/ 利用/ ADP/ 中/ MonteCarlo/ 仿真/ 的/ 方法/ ,/ 解决/ MDPN/ 的/ 近似/ 求解/ 问题/ ./ 在/ 结合/ ADP/ 方法/ 的/ MDPN/ 中/ ,/ 不/ 需要/ 通过/ Petri/ 模型/ 得到/ 完全/ 的/ 可达图/ RG/ ,/ 也/ 不/ 需要/ 通过/ 式/ (/ 42/ )/ 计算/ 状态/ 之间/ 的/ 转移/ 概率/ ./ 相反/ 的/ ,/ 在/ 模拟系统/ 与/ 决策者/ 行为/ 的/ 同时/ ,/ 不断/ 地/ 更新/ 可/ 达/ 状态/ 集/ RS/ ./ 由于/ MDP/ 中/ 只/ 关注/ 决策/ 与/ 系统/ 运行/ 的/ 最终/ 状态/ ,/ 因此/ 只/ 需/ 记录/ 位置/ Stopndi/ 中/ 全部/ 都/ 有/ 标记/ 的/ 状态/ (/ 称为/ 决策/ 终结状态/ )/ ,/ 或者/ 位置/ Stoppr/ 有/ 标记/ 时/ 的/ 状态/ (/ 称为/ 系统/ 终结状态/ )/ ./ 若该/ 状态/ 在/ RS/ 中/ 不/ 存在/ ,/ 才/ 将/ 该/ 状态/ 加入/ RS/ 中/ ./ 若/ 新/ 加入/ RS/ 中/ 的/ 状态/ 为/ 决策/ 终结状态/ ,/ 则/ 为/ 其/ 关联/ 一个/ 后/ 决策/ 值/ 函数/ 并/ 设定/ 其/ 初始值/ ,/ 其/ 功能/ 与/ 式/ (/ 25/ )/ 类似/ ./ 4/ 基于/ 随机/ 博弈/ 网/ 的/ 动态/ 优化/ 模型/ 上述/ MDP/ 、/ MDPN/ 以及/ MDWN/ 模型/ ,/ 都/ 只能/ 描述/ 具有/ 集中式/ 控制/ 设施/ 的/ 系统/ ,/ 即/ 系统/ 内/ 只有/ 一个/ 决策者/ ./ 在/ 现实生活/ 中/ ,/ 还/ 存在/ 着/ 大量/ 具有/ 多个/ 决策者/ 的/ 系统/ ./ 上述/ 模型/ 在/ 处理/ 这/ 类/ 问题/ 时/ ,/ 只能/ 从/ 各个/ 决策者/ 的/ 角度/ 分别/ 建模/ ,/ 而/ 将/ 其他/ 决策者/ 视为/ 不/ 可控/ 外部/ 随机/ 事件/ ,/ 无法/ 体现/ 出/ 决策者/ 之间/ 的/ 联系/ ./ 文献/ [/ 50/ ]/ 于/ 2008/ 年/ 首次/ 提出/ 了/ 随机/ 博弈/ 网/ (/ StochasticGameNets/ ,/ SGN/ )/ ,/ 将/ 动态随机/ 博弈/ 与/ 随机/ Petri/ 网/ 结合/ ,/ 能够/ 对/ 具有/ 多个/ 决策者/ 的/ 系统/ 进行/ 建模/ 分析/ ./ 动态随机/ 博弈/ 可以/ 看作/ 是/ 马尔可夫/ 决策/ 过程/ 的/ 扩展/ ,/ 可/ 包含/ 多个/ 决策者/ 并/ 能/ 体现/ 出/ 他们/ 之间/ 的/ 复杂/ 关系/ ,/ 包括/ :/ (/ 1/ )/ 竞争/ 关系/ ./ 即/ 每个/ 决策者/ 只/ 关心/ 最大化/ 自己/ 的/ 收益/ ;/ (/ 2/ )/ 合作/ 关系/ ./ 即/ 所有/ 决策者/ 作为/ 一个/ 群体/ 关心/ 的/ 是/ 总/ 收益/ ./ 将/ 动态随机/ 博弈/ 与/ 随机/ Petri/ 网/ 相结合/ ,/ 有助于/ 系统/ 的/ 细粒度/ 建模/ 与/ 简化/ 求解/ ./ 4.1/ 随机/ 博弈/ 网/ 定义/ 3/ (/ 随机/ 博弈/ 网/ )/ [/ 51/ ]/ ./ 一个/ 随机/ 博弈/ 网是/ 一个/ 9/ 元组/ :/ SGN/ =/ {/ N/ ,/ P/ ,/ T/ ,/ F/ ,/ π/ ,/ λ/ ,/ R/ ,/ U/ ,/ M0/ }/ ,/ 其中/ :/ N/ =/ {/ 1/ ,/ 2/ ,/ …/ ,/ n/ }/ 是/ 决策者/ (/ 博弈/ 局中人/ )/ 的/ P/ 是/ 有限/ 的/ 位置/ 集合/ ;/ T/ =/ T1/ ∪/ T2/ ∪/ …/ ∪/ Tn/ 是/ 有限/ 变迁/ 的/ 集合/ ,/ 其中/ π/ :/ T/ →/ [/ 0/ ,/ 1/ ]/ 是/ 决策者/ 选择/ 某个/ 变迁/ 的/ 概率/ ;/ F/ / I/ ∪/ O/ 是/ 弧/ 的/ 集合/ ,/ 其中/ I/ / (/ P/ ×/ T/ )/ ,/ O/ / (/ T/ ×/ P/ )/ ,/ 且/ 有/ P/ ∩/ T/ =/ / ,/ P/ ∪/ T/ ≠/ / ./ 记/ x/ 的/ 前/ 集合/ 为/ ·/ x/ =/ {/ y/ |/ (/ y/ ,/ x/ )/ ∈/ F/ }/ ,/ x/ 的/ 后/ 集合/ 为/ x/ ·/ =/ {/ y/ |/ (/ x/ ,/ y/ )/ ∈/ F/ }/ ;/ R/ :/ T/ →/ (/ R1/ ,/ R2/ ,/ …/ ,/ RN/ )/ 为/ 决策者/ 采用/ 某个/ 变迁/ 所/ 对应/ 行为/ 所得/ 的/ 收益/ 函数/ ,/ 其中/ Ri/ ∈/ (/ -/ ,/ +/ )/ ,/ i/ ∈/ N/ ;/ λ/ =/ {/ λ/ 1/ ,/ λ/ 2/ ,/ …/ ,/ λ/ W/ }/ 为/ 变迁/ 的/ 实施/ 速率/ ,/ 其中/ W/ 是/ 变迁/ 的/ 个数/ ;/ U/ 是/ 决策者/ 的/ 总/ 收益/ 函数/ ;/ M0/ 是/ 起始/ 状态/ ,/ 代表/ 所有/ 决策者/ 的/ 最初/ 状态/ ./ 在/ 该/ 定义/ 中/ ,/ P/ 是/ 博弈/ 的/ 状态/ ,/ 在/ 某个/ 位置/ p/ ∈/ P/ 中有/ 标记/ 意味着/ 所有/ 决策者/ 都/ 在/ 该/ 状态/ 中/ ./ 位置/ p/ 中/ 的/ 标记/ s/ 对应/ 一个/ 收益/ 向量/ Tk/ 是/ 第/ k/ ∈/ N/ 个/ 决策者/ 的/ 行为/ ;/ 集合/ ;/ Page14p/ (/ s/ )/ 为/ 决策者/ k/ 在/ 状态/ p/ 中/ 所得/ 的/ 收益/ ./ 当/ 其中/ hk/ 变迁/ t/ 实施/ ,/ 所有/ 决策者/ 都/ 会/ 得到/ 收益/ 其中/ Ri/ (/ t/ )/ 为/ 决策者/ i/ 所得/ 的/ 收益/ ./ 若/ 标记/ 经过/ 变迁/ t/ 到达/ 位置/ p/ ,/ 则/ 收益/ 都/ 会/ 累加/ 在/ 标记/ 的/ 收益/ 向量/ 犺/ p/ (/ s/ )/ 中/ ./ 在/ SGN/ 中/ ,/ 当/ 系统/ 运行/ 至/ 状态/ p/ 时/ ,/ 决策者/ k/ 的/ 策略/ 可定义/ 为/ 其中/ ,/ π/ (/ tk/ 显然/ ,/ 对于/ 所有/ 状态/ p/ ,/ 都/ 有/ 进一步/ ,/ 参照/ 博弈论/ 中/ 纳什/ 均衡/ 的/ 概念/ ,/ 可以/ 定义/ SGN/ 中/ 的/ 均衡/ 策略/ π/ / =/ (/ π/ / 1/ ,/ π/ / 2/ ,/ …/ ,/ π/ / n/ )/ 满足/ Uk/ (/ π/ / 1/ ,/ …/ ,/ π/ / k/ -/ 1/ ,/ π/ / k/ ,/ π/ / k/ +/ 1/ ,/ …/ ,/ π/ / n/ )/ / Uk/ (/ π/ / 1/ ,/ …/ ,/ π/ / k/ -/ 1/ ,/ π/ k/ ,/ π/ / k/ +/ 1/ ,/ …/ ,/ π/ / n/ )/ ,/ / k/ ∈/ [/ 1/ ,/ 2/ ,/ …/ ,/ n/ ]/ 其中/ π/ k/ 是/ 除/ π/ / k/ 外/ 所有/ 其它/ 可能/ 的/ 策略/ ./ 均衡/ 策略/ 的/ 含义/ 在于/ ,/ 某个/ 决策者/ 在/ 其他/ 决策者/ 都/ 不/ 偏离/ 均衡/ 策略/ 的/ 情况/ 下/ ,/ 采用/ 非均衡/ 策略/ 不会/ 取得/ 比/ 采用/ 均衡/ 策略/ 更/ 高/ 的/ 收益/ ./ 换句话说/ ,/ 该/ 决策者/ 没有/ 偏离/ 均衡/ 决策/ 的/ 动机/ ./ 值得注意/ 的/ 是/ ,/ 在/ MDP/ 中/ 一般/ 都/ 使用/ 确定/ 行为/ (/ 称为/ 纯/ 策略/ )/ 作为/ 最优/ 解/ (/ 一些/ 例外/ 的/ 情况/ 如/ 探索/ // 利用/ 问题/ 中/ 会/ 采取/ 一些/ 不/ 确定/ 行为/ 来/ 主动/ 学习/ 值/ 函数/ )/ ./ 而/ 在/ SGN/ 中/ ,/ 一般/ 采用/ 在/ 行为/ 空间/ 的/ 概率分布/ (/ 混合策略/ )/ 作为/ 均衡/ 解/ ,/ 因为/ 在/ 多/ 人/ 决策问题/ 中/ ,/ 在/ 纯/ 策略/ 意义/ 下/ 一般/ 不/ 存在/ 均衡/ 解/ ,/ 而/ 在/ 混合策略/ 意义/ 下/ 一定/ 存在/ 均衡/ 解/ ./ 4.2/ 随机/ 博弈/ 网/ 的/ 建模/ 与/ 分析/ 构建/ 一个/ SGN/ 模型/ 一般/ 分为/ 4/ 个/ 步骤/ [/ 52/ ]/ :/ (/ 1/ )/ 建立/ 每个/ 决策者/ 的/ 子/ SGN/ 模型/ ./ 在/ 实际/ 系统/ 中/ ,/ 识别/ 出/ SGN/ 对应/ 的/ 要素/ ,/ 包括/ ①/ 变迁/ ./ 变迁/ 代表/ 决策者/ 的/ 行为/ ./ 注意/ 行为/ 集合/ 中/ 也/ 可能/ 包括/ 空/ 行为/ / ,/ 即/ 决策者/ 不/ 采取任何/ 行为/ ./ ②/ 收益/ ./ 对于/ 每个/ 变迁/ t/ ,/ 赋予/ 其/ 一个/ 收益/ 函数/ R/ ,/ 其/ 每个/ 分量/ Ri/ 代表/ 决策者/ i/ 在/ 该/ 行为/ 结束/ 后/ 所得/ 的/ 收益/ ./ ③/ 位置/ 集合/ P/ ./ 每个/ 位置/ p/ 代表/ 系统/ 的/ 一个/ 状态/ ./ (/ 2/ )/ 描述/ 纳什/ 均衡/ 条件/ ./ 对于/ 竞争/ 博弈/ ,/ 每个/ 决策者/ 的/ 目标/ 是/ 最大化/ 自己/ 的/ 收益/ ;/ 对于/ 合作/ 博弈/ ,/ 每个/ 决策者/ 的/ 目标/ 是/ 最大化/ 所有/ 决策者/ 的/ 收益/ 的/ 总和/ ./ 对于/ 有限/ 时间/ SGN/ ,/ 可/ 仿照/ MDP/ 中式/ (/ 3/ )/ 定义/ 决策者/ i/ 的/ 总/ 收益/ :/ 其中/ N/ 是/ 时间/ 的/ 长度/ ,/ R/ π/ n/ 是/ 阶段/ n/ 使用/ 策略/ π/ 时/ 所得/ 的/ 收益/ ./ 注意/ U/ π/ i/ 与/ 所有/ 决策者/ 的/ 策略/ 都/ 相关/ ,/ 而/ 并非/ 只/ 与/ i/ 自己/ 的/ 策略/ 相关/ ./ 对于/ 只有/ 两个/ 决策者/ 的/ 系统/ ,/ 均衡/ 策略/ π/ / =/ {/ π/ 1/ / ,/ π/ 2/ / }/ 满足/ U/ π/ 1/ / ,/ π/ 2/ / 1/ 且/ U/ π/ 1/ / ,/ π/ 2/ / U/ π/ 1/ ,/ π/ 2/ / (/ 3/ )/ 求解/ 纳什/ 均衡/ 策略/ ./ 一般/ 情形/ 下/ 求解/ 均衡/ 策略/ 难度/ 较大/ ./ 本文/ 仅/ 对/ 只有/ 两个/ 决策者/ 的/ 特殊/ 情况/ 进行/ 讨论/ ,/ 此时/ ,/ 系统/ 求解/ 问题/ 可以/ 化归为/ 一个/ 静态/ 非线性/ 规划/ 问题/ ,/ 详细/ 请/ 参见/ 4.3/ 小节/ ./ (/ 4/ )/ 合并/ 子/ 模型/ ,/ 建立/ 全局/ SGN/ 模型/ ./ 将子/ 模型/ 中/ 含义/ 相同/ 的/ 位置/ 合并/ ,/ 可/ 将/ 所有/ 子/ 模型/ 进行/ 组合/ ,/ 得到/ 全局/ SGN/ 模型/ ./ 4.3/ 随机/ 博弈/ 网/ 的/ 求解/ 文献/ [/ 52/ -/ 54/ ]/ 给出/ 了/ 二人/ 动态/ 博弈/ 的/ 纳什/ 均衡/ 求解/ 方法/ ,/ 该/ 方法/ 基于/ 文献/ [/ 55/ ]/ ,/ 将/ 二人/ 动态/ 博弈/ 问题/ 化归为/ 一个/ 静态/ 非线性/ 规划/ (/ NonLinearPro/ -/ gramming/ ,/ NLP/ )/ 问题/ :/ minU1/ ,/ U2/ ,/ π/ 1/ ,/ π/ 2s/ ./ t/ ./ :/ R1/ (/ pi/ )/ π/ 2/ (/ pi/ )/ +/ α/ 犜/ (/ pi/ ,/ U1/ )/ π/ 2/ (/ pi/ )/ / 1TU1/ (/ pi/ )/ ,/ (/ π/ 1/ (/ pi/ )/ )/ TR2/ (/ pi/ )/ +/ α/ (/ π/ 1/ (/ pi/ )/ )/ T/ 犜/ (/ pi/ ,/ U2/ )/ / 1TU2/ (/ pi/ )/ ,/ 其中/ ,/ 值/ 向量/ 为/ 犜/ (/ p/ ,/ U/ )/ =/ {/ [/ 犘/ (/ p1/ |/ p/ ,/ t1/ ,/ t2/ )/ ,/ …/ ,/ 犘/ (/ p/ |/ P/ |/ |/ p/ ,/ t1/ ,/ t2/ )/ ]/ TUk/ }/ 且/ 有/ k/ ∈/ {/ 1/ ,/ 2/ }/ ,/ i/ ∈/ {/ 1/ ,/ …/ ,/ |/ P/ |/ }/ ,/ pi/ ∈/ 犘/ ,/ t1/ ∈/ 犜/ 1/ ,/ t2/ ∈/ 犜/ 2/ ./ 该/ 非线性/ 规划/ 的/ 最小/ 全局/ 解/ ,/ 就是/ SGN/ 中/ 的/ 纳什/ 均衡/ 解/ ./ 4.4/ 随机/ 博弈/ 网/ 的/ 模型/ 化简/ 与/ 合并/ 当/ 利用/ SGN/ 对/ 实际/ 问题/ 建模/ 时/ ,/ 通常/ 会/ 遇到/ 的/ 一个/ 问题/ 是/ 决策者/ 的/ 行为/ 复杂/ ,/ 导致/ 所/ 建立/ 的/ SGN/ 模型/ 难于/ 求解/ 分析/ ./ 文献/ [/ 56/ ]/ 针对/ 这一/ 问题/ ,/ 给出/ 了/ 一些/ SGN/ 模型/ 化简/ 的/ 方法/ ,/ 例如/ 在/ 图/ 6/ 左半/ 部分/ 所示/ 的/ 模型/ ,/ 可以/ 等价/ 地/ 化简/ 为/ 右半/ 部分/ 的/ 简单/ 模型/ ./ 4.2/ 小节/ 中/ 提到/ ,/ 在/ 构建/ SGN/ 全局/ 模型/ 时/ ,/ 需要/ 进行/ 子/ 模型/ 合并/ ./ 文献/ [/ 57/ ]/ 讨论/ 了/ 在/ 利用/ SGN/ 对/ 网络/ 攻防/ 进行/ 建模/ 时/ ,/ 子/ 模型/ 合并/ 的/ 方法/ ,/ 将/ 决策者/ 之间/ 的/ 关系/ 分为/ 两类/ :/ 禁止/ 类型/ 与/ 结束/ 类型/ ,/ 相应/ 的/ 组合/ 方法/ 如图/ 7/ 所示/ ./ Page15/ 一方面/ ,/ 当/ 防御/ 与/ 攻击行为/ 都/ 可以/ 实施/ 时/ ,/ 若/ 防御/ 先/ 实施/ ,/ 则/ 可/ 禁止/ 攻击行为/ 的/ 实施/ (/ 图/ 7/ (/ a/ )/ )/ ./ 另一方面/ ,/ 防御/ 行为/ 的/ 实施/ ,/ 也/ 可以/ 使得/ 整个/ 攻击/ 过程/ 结束/ (/ 图/ 7/ (/ b/ )/ )/ ./ 4.5/ 应用/ 与/ 扩展/ 文献/ [/ 57/ ]/ 在/ SGN/ 的/ 基础/ 上/ 进行/ 延伸/ ,/ 进一步/ 提出/ 了/ 针对/ 网络安全/ 攻防/ 的/ 攻击/ -/ 防御/ 随机/ 博弈/ 网/ (/ Attack/ -/ DefenseStochasticGameNets/ ,/ ADSGN/ )/ ,/ 准确/ 地/ 刻画/ 了/ 网络/ 攻击者/ 与/ 防御者/ 之间/ 的/ 零/ 和/ 竞争/ 博弈/ 关系/ ./ 本/ 小节/ 中以/ 企业/ 网中/ 的/ 安全/ 攻防/ 问题/ 为例/ ,/ 说明/ SGN/ 的/ 建模/ 方法/ ./ 在/ 一个/ 典型/ 企业/ 网中/ ,/ 从/ 攻击者/ 与/ 网络管理员/ 的/ 观点/ 来看/ ,/ 网/ 拓扑/ 结构/ 可/ 抽象/ 为/ 图/ 8/ ./ 攻击者/ 可/ 进行/ 一些/ 攻击行为/ ,/ 如/ 扫描/ 网络/ 脆弱性/ 、/ 攻击/ 数据库/ 、/ 破译/ 服务器/ 密码/ 等/ ./ 网络管理员/ 可以/ 进行/ 一些/ 相应/ 的/ 防御/ 措施/ ,/ 例如/ 利用/ 入侵/ 检测/ 系统/ 进行/ 扫描/ 、/ 阻止/ 攻击者/ IP/ 进入/ 系统/ 、/ 移除/ 嗅探器/ 等/ ./ 攻击者/ SGN/ 子/ 模型/ 与/ 防御者/ SGN/ 子/ 模型/ 分别/ 如图/ 9/ 、/ 图/ 10/ 所示/ ./ 这/ 两个/ 子/ 模型/ 从/ 不同/ 决策者/ 的/ 角度/ 刻画/ 了/ 决策者/ 在/ 每个/ 决策/ 时间/ 可能/ 采取/ 的/ 攻防/ 行为/ ./ 应用/ 4.4/ 小节/ 中/ 的/ 模型/ 化简/ 与/ 合并/ 技术/ ,/ 可/ 将/ 图/ 9/ 、/ 图/ 10/ 中/ 的/ SGN/ 子/ 模型/ 合并/ 为/ 图/ 11/ 所示/ 的/ SGN/ 完整/ 模型/ ./ Page16/ 图/ 11SGN/ 完整/ 模型/ 目前/ ,/ 关于/ SGN/ 的/ 应用/ 研究/ 绝大多数/ 都/ 集中/ 在/ 网络安全/ 方面/ ,/ 如/ 文献/ [/ 58/ ]/ 研究/ 了/ 利用/ 网络连接/ 关系/ 与/ 脆弱性/ 信息/ 等/ 输入/ 数据/ 生成/ SGN/ 模型/ 的/ 方法/ ,/ 文献/ [/ 52/ -/ 53/ ,/ 57/ ]/ 研究/ 了/ 企业/ 网中/ 的/ 安全/ 问题/ ,/ 文献/ [/ 56/ ,/ 59/ ]/ 研究/ 了/ 电子商务/ 中/ 的/ 若干/ 安全/ 问题/ ,/ 文献/ [/ 54/ ]/ 研究/ 了/ 电子邮件/ 蠕虫/ 病毒/ 的/ 传播/ 问题/ ./ 另外/ ,/ 在/ 无线网络/ 领域/ ,/ 也/ 有/ 一些/ 初步/ 的/ 研究成果/ ,/ 如/ 文献/ [/ 60/ ]/ 研究/ 了/ 无线网络/ 中/ 共享/ 信道/ 竞争/ 的/ 性能/ 评价/ 问题/ ./ 总之/ ,/ SGN/ 是/ 一个/ 正在/ 发展/ 与/ 完善/ 中/ 的/ 研究/ 领域/ ,/ 在/ 理论/ 与/ 应用/ 方面/ 均/ 具有/ 较为/ 广阔/ 的/ 前景/ ./ 5/ 结论/ 与/ 展望/ 本文/ 对/ 动态/ 优化/ 在/ 计算机系统/ 与/ 计算机网络/ 中/ 的/ 建模/ 、/ 求解/ 与/ 应用/ 进行/ 了/ 综述/ ./ 相较/ 于/ 静态/ 优化/ ,/ 动态/ 优化/ 可以/ 精确/ 地/ 刻画/ 系统/ 的/ 时/ 变性/ ./ 本文/ 主要/ 讨论/ 了/ 3/ 种/ 理论/ 模型/ ,/ 即/ 马尔可夫/ 决策/ 过程/ 模型/ 、/ 马尔可夫/ 决策/ Petri/ 网/ 模型/ 以及/ 随机/ 博弈/ 网/ 模型/ ,/ 对/ 这些/ 模型/ 的/ 建模/ 方法/ 、/ 求解/ 算法/ 、/ 与/ 应用/ 实例/ 进行/ 了/ 较为/ 深入/ 的/ 研究/ ./ 计算机系统/ 与/ 计算机网络/ 中/ 的/ 资源/ 种类/ 复杂/ ,/ 数量/ 众多/ ./ 面对/ 这种/ 复杂/ 的/ 应用环境/ ,/ 如何/ 合理/ 地/ 运用/ 动态/ 优化/ 理论/ 对系统/ 进行/ 建模/ ,/ 并/ 采取/ 适当/ 的/ 求解/ 算法/ 进行/ (/ 近似/ )/ 求解/ 具有/ 极大/ 挑战性/ ./ 在/ 本文/ 最后/ 以/ 以下几点/ 为例/ ,/ 列举/ 一些/ 未来/ 可能/ 的/ 研究/ 方向/ :/ (/ 1/ )/ 马尔可夫/ 决策/ 过程/ 的/ 近似/ 求解/ 算法/ ./ 众所周知/ ,/ 目前/ 还/ 不/ 存在/ 适用/ 于/ 所有/ MDP/ 近似/ 求解/ 的/ 统一/ “/ 万能/ 药/ ”/ 算法/ ./ 很多/ 看似/ 合适/ 的/ 算法/ 得出/ 的/ 近似/ 解/ 往往/ 质量/ 较差/ ,/ 在/ 某些/ 环境/ 下/ 甚至/ 会/ 出现/ 算法/ 不/ 收敛/ 的/ 情况/ ./ 近似/ 解/ 的/ 质量/ 在/ 很大/ 程度/ 上/ 还/ 取决于/ 算法/ 设计者/ 对/ 领域/ 专业知识/ 的/ 理解/ 程度/ 与/ 算法/ 设计/ 经验/ ,/ Powell/ 甚至/ 将/ ADP/ 近似值/ 函数/ 中/ 的/ 特征函数/ 选取/ 称为/ 一种/ “/ 艺术/ (/ art/ )/ ”/ [/ 36/ ]/ ./ 对于/ 近似/ 求解/ 算法/ 的/ 应用/ 范围/ 、/ 解/ 的/ 质量/ 以及/ 收敛性/ 等/ 一系列/ 问题/ ,/ 还/ 需要/ 进一步/ 深入研究/ ./ (/ 2/ )/ 马尔可夫/ 决策/ Petri/ 网/ 与/ 随机/ 博弈/ 网等/ 模型/ 的/ 近似/ 求解/ 问题/ ./ 一方面/ ,/ 在/ MDPN/ // MDWN/ 与/ SGN/ 模型/ 中/ ,/ 虽然/ 存在/ 一些/ 对模型/ 进行/ 化简/ 的/ 方法/ (/ 如/ 4.4/ 节/ )/ ,/ 但是/ 这些/ 方法/ 往往/ 局限于/ 对/ 某些/ 特定/ 模型/ 结构/ 的/ 化简/ ,/ 还/ 无法/ 处理/ 更为/ 复杂/ 的/ 模型/ ./ 另一方面/ ,/ 这些/ 模型/ 均/ 采用/ 精确/ 求解/ 算法/ ,/ 这/ 使得/ 利用/ 这/ 两种/ 模型/ 对/ 大规模/ 系统/ 进行/ 建模/ 分析/ 时/ 求解/ 较为/ 困/ Page17/ 难/ ,/ 大大/ 限制/ 了/ 其/ 应用/ 范围/ ./ 在/ 3.5/ 节中/ 我们/ 对/ MDWN/ 中/ 结合/ ADP/ 算法/ 的/ 方式/ 进行/ 了/ 一些/ 初步/ 的/ 探索/ ,/ 但/ 还/ 不够/ 深入/ ./ 后续/ 工作/ 还/ 应对/ 这些/ 模型/ 的/ 近似/ 求解/ 算法/ 进行/ 研究/ ./ (/ 3/ )/ 随机/ 博弈/ 网/ 的/ 应用/ 研究/ 拓展/ ./ 目前/ 随机/ 博弈/ 网/ 模型/ 方法/ 主要/ 应用/ 于/ 网络安全/ 分析/ 中/ ,/ 而/ 就/ 随机/ 博弈/ 网/ 的/ 模型/ 特点/ 来说/ ,/ 它/ 可以/ 适用/ 于/ 模型/ 分析/ 具有/ 多个/ 独立/ 决策者/ 参与/ 的/ 计算机系统/ 应用/ ,/ 如/ 无线网络/ 、/ 对/ 等/ 网络/ (/ P2P/ )/ 以及/ 社交/ 网络/ 等/ ./ 进一步/ 的/ 研究/ 工作/ 将/ 针对/ 这些/ 应用/ 的/ 特点/ ,/ 研究/ 有/ 针对性/ 随机/ 博弈/ 网/ 的/ 建模/ 与/ 分析方法/ ,/ 拓展/ 随机/ 博弈/ 网/ 的/ 应用领域/ ./ 

