Page1/ 一种/ 面向/ 社区/ 型/ 问句/ 检索/ 的/ 主题/ 翻译/ 模型/ 张伟/ 男/ 张宇/ 刘挺/ (/ 哈尔滨工业大学/ 计算机科学/ 与/ 技术/ 学院/ 社会/ 计算/ 与/ 信息检索/ 研究/ 中心/ 哈尔滨/ 150001/ )/ 摘要/ 基于/ 统计/ 机器翻译/ 模型/ 的/ 问句/ 检索/ 模型/ ,/ 其/ 相关性/ 排序/ 机制/ 主要/ 依赖于/ 词项/ 间/ 的/ 翻译/ 概率/ ,/ 然而/ 已有/ 的/ 模型/ 没有/ 很/ 好/ 地/ 控制/ 翻译/ 模型/ 的/ 噪声/ ,/ 使得/ 当前/ 的/ 问句/ 检索/ 模型/ 存在/ 不/ 完善/ 之处/ ./ 文中/ 提出/ 一种/ 基于/ 主题/ 翻译/ 模型/ 的/ 问句/ 检索/ 模型/ ,/ 从/ 理论/ 上/ 说明/ ,/ 该/ 模型/ 利用/ 主题/ 信息/ 对/ 翻译/ 进行/ 合理/ 的/ 约束/ ,/ 达到/ 控制/ 翻译/ 模型/ 噪声/ 的/ 效果/ ,/ 从而/ 提高/ 问句/ 检索/ 的/ 结果/ ./ 实验/ 结果表明/ ,/ 文中/ 提出/ 的/ 模型/ 在/ MAP/ (/ MeanAveragePrecision/ )/ 、/ MRR/ (/ MeanReciprocalRank/ )/ 以及/ p/ @/ 1/ (/ precisionatpositionone/ )/ 等/ 指标/ 上/ 显著/ 优于/ 当前/ 最/ 先进/ 的/ 问句/ 检索/ 模型/ ./ 关键词/ 社区/ 型/ 问答/ ;/ 问句/ 检索/ ;/ 主题/ 模型/ ;/ 翻译/ 模型/ ;/ LDA/ (/ LatentDirichletAllocation/ )/ ;/ 社会/ 计算/ ;/ 社交/ 网络/ 1/ 引言/ 社区/ 型/ 问答/ (/ CommunityQuestionAnswering/ ,/ CQA/ )/ 服务/ 逐渐/ 成为/ 人们/ 在/ 互联网/ 上/ 获取信息/ 以及/ 知识/ 的/ 重要途径/ ./ 典型/ 的/ 社区/ 型/ 问答/ 服务/ 包括/ 百度/ 知道/ (/ http/ :/ // // zhidao/ ./ baidu/ ./ com/ // )/ 、/ 知乎/ (/ http/ :/ // // www/ ./ zhihu/ ./ com/ // )/ 、/ Yahoo/ !/ Answers/ (/ http/ :/ // // answers/ ./ yahoo/ ./ com/ // )/ 以及/ Quora/ (/ https/ :/ // // www/ ./ quora/ ./ com/ // )/ 等/ ./ 随着/ 用户/ 对于/ CQA/ 服务/ 的/ 广泛/ 使用/ ,/ 大量/ 的/ 用户/ 生成/ Page2/ 内容/ (/ UserGeneratedContent/ ,/ UGC/ )/ 以/ 问句/ 和/ 答案/ 的/ 形式/ 被/ 积累/ 下来/ ,/ 形成/ 了/ 优质/ 的/ 数据/ 资源/ ./ 与/ 传统/ 的/ 搜索引擎/ 检索/ 不同/ ,/ CQA/ 检索/ 不是/ 返回/ 与/ 用户/ 查询/ 相关/ 的/ 文档/ 列表/ ,/ 而是/ 返回/ 用户/ 查询/ 的/ 答案/ ,/ 从而/ 能够/ 直接/ 满足用户/ 的/ 查询/ 需求/ ./ 但是/ ,/ CQA/ 查询/ 中/ ,/ 检索/ 的/ 文档/ 是/ 用户/ 生成/ 的/ 问句/ 和/ 答案/ ,/ 其/ 长度/ 远/ 小于/ 传统意义/ 的/ 文档/ [/ 1/ ]/ ./ 其中/ 大多数/ 的/ 查询/ 词项/ 在/ 用户/ 的/ 问答/ 对/ 中仅/ 出现/ 一次/ ,/ 因此/ 使得/ 基于/ 词频/ 和/ 文档/ 频率/ 统计/ 的/ 检索/ 模型/ 不再/ 适用/ 于/ 问句/ 检索/ 任务/ ./ 基于/ Unigram/ 语言/ 模型/ 的/ 问句/ 检索/ 模型/ [/ 2/ -/ 3/ ]/ 假设/ 查询/ 中/ 的/ 词项/ 之间/ 是/ 相互/ 独立/ 的/ ,/ 在/ 检索/ 的/ 过程/ 中其/ 相似性/ 排序/ 主要/ 依赖于/ 字符串/ 的/ 严格/ 匹配/ ./ 尽管/ 语言/ 模型/ 能够/ 利用/ 参数/ 和/ 大/ 数据/ 集/ (/ Collection/ )/ 进行/ 相应/ 的/ 平滑/ 处理/ ,/ 但是/ 其/ 无法/ 解决/ 问句/ 检索/ 中/ 的/ 词/ 不/ 匹配/ 问题/ ./ 即/ 语言/ 模型/ 无法/ 建立/ 用户/ 对于/ 同/ 一种/ 语义/ 的/ 不同/ 表述/ 形式/ 之间/ 的/ 联系/ ./ 基于/ 统计/ 机器翻译/ 模型/ 的/ 问句/ 检索/ 模型/ [/ 4/ -/ 7/ ]/ 是/ 当前/ 用于/ 问句/ 检索/ 中/ 的/ 最/ 先进/ 的/ 模型/ ,/ 能够/ 在/ 一定/ 程度/ 上/ 克服/ 上述/ 不足/ ./ 其/ 利用/ 单/ 语言/ 相似/ 的/ 句子/ 或者/ 双/ 语言/ 互为/ 翻译/ 的/ 句子/ 作为/ 对齐/ 句/ 对/ ,/ 输入/ 到/ 统计/ 机器翻译/ 模型/ 中/ ,/ 获取/ 单/ 语言/ 词项/ 间/ 的/ 翻译/ 概率/ ,/ 并/ 将/ 其/ 作为/ 词项/ 间/ 的/ 相似性/ 度量/ ,/ 或者/ 根据/ 词项/ 的/ 翻译/ 之间/ 的/ 相似性/ 来/ 度量/ 当前/ 词项/ 间/ 的/ 相似性/ ./ 然而/ ,/ 由于/ 目前/ 用于/ 问句/ 检索/ 的/ 统计/ 翻译/ 模型/ 仅仅/ 利用/ 统计/ 共现/ 信息/ 作为/ 依据/ 来/ 度量/ 词项/ 间/ 的/ 相似性/ ,/ 因此/ 导致/ 语义/ 相关/ 和/ 无关/ 的/ 词项/ 之间/ 的/ 翻译/ 概率/ 无法/ 区分/ ./ 图/ 1/ 所示/ 为/ IBMModel1/ 在/ 两个/ 语义/ 相似/ 问句/ 之间/ 的/ 词项/ 翻译/ 关系/ ./ 图/ 1IBMModel1/ 在/ 语义/ 相似/ 问句/ 之间/ 的/ 词项/ 翻译/ 关系/ 由图/ 1/ 我们/ 可以/ 看出/ ,/ 除了/ “/ weight/ ”/ 和/ “/ fat/ ”/ 之外/ ,/ 其他/ 的/ 词项/ 之间/ 在/ 语义上/ 是/ 不/ 相似/ 的/ ,/ 但是/ 由于/ IBMModel1/ 无法/ 识别/ 词项/ 间/ 的/ 这种/ 潜在/ 语义/ 相似/ 关系/ ,/ 使得/ 已有/ 的/ 基于/ 统计/ 翻译/ 模型/ 的/ 问句/ 检索/ 模型/ 在/ 其/ 所/ 依赖/ 的/ 词项/ 互译/ 信息/ 上/ 存在/ 较大/ 的/ 噪声/ ,/ 从而/ 影响/ 语义上/ 相似/ 问句/ 的/ 检索/ 结果/ ./ 因此/ 传统/ 的/ 基于/ 统计/ 机器翻译/ 模型/ 的/ 问句/ 检索/ 模型/ 很难/ 召回/ 图/ 1/ 中/ 所示/ 的/ 相似/ 问句/ ./ 针对/ 上述/ 问题/ ,/ 我们/ 提出/ 了/ 一种/ 面向/ 社区/ 型/ 问句/ 检索/ 的/ 主题/ 翻译/ 模型/ ,/ 通过/ 在/ 传统/ 的/ 基于/ 统计/ 机器翻译/ 模型/ 的/ 检索/ 模型/ 中/ ,/ 引入/ 词项/ 间/ 在/ 潜在/ 主题/ 下/ 的/ 语义/ 相似性/ ,/ 从而/ 能够/ 解决/ 传统/ 翻译/ 模型/ 中词/ 项/ 翻译/ 准确性/ 较差/ 的/ 问题/ ./ 具体/ 地/ ,/ 我们/ 通过/ 利用/ 潜在/ 主题/ 对/ 互为/ 翻译/ 的/ 词项/ 进行/ 约束/ ,/ 从而/ 更加/ 合理/ 地/ 度量/ 检索/ 模型/ 中词/ 项/ 之间/ 的/ 相似/ 度/ ,/ 并/ 取得/ 更好/ 的/ 问句/ 检索/ 结果/ ./ 2/ 相关/ 工作/ 基于/ 语言/ 模型/ 的/ 信息检索/ 模型/ 首次/ 由/ Ponte/ 和/ Croft/ [/ 8/ ]/ 提出/ ,/ 并/ 被/ 广泛应用/ 到/ 信息检索/ 的/ 各个/ 相关/ 领域/ [/ 3/ -/ 4/ ,/ 9/ -/ 13/ ,/ 23/ ]/ ./ Jeon/ 等/ 人/ [/ 2/ ]/ 率先/ 将/ 语言/ 模型/ 应用/ 到/ 问句/ 检索/ 中/ ,/ 他们/ 采用/ Unigram/ 模型/ 对/ 社区/ 型/ 问答/ 服务/ 中/ 的/ 问答/ 对/ 进行/ 建模/ ,/ 从而/ 将/ 其/ 应用/ 于/ 相似/ 问句/ 的/ 发现/ 工作/ 中/ ./ 最近/ ,/ Zhang/ 等/ 人/ [/ 14/ ]/ 利用/ 依存/ 句法分析/ 技术/ 对/ 用户/ 提出/ 的/ 自然语言/ 问句/ 中词/ 项间/ 的/ 关联性/ 进行/ 度量/ ,/ 从而/ 将/ 重新分配/ 后/ 的/ 权重/ 融入/ 到/ 已有/ 的/ 问句/ 检索/ 模型/ 中/ ,/ 得到/ 更好/ 的/ 问句/ 检索/ 结果/ ./ 但是/ 由于/ 上述/ 方法/ 没有/ 考虑/ 用户/ 在/ 相同/ 语义上/ 的/ 不同/ 表述/ 形式/ 的/ 信息/ ,/ 从而/ 使得/ 语义上/ 相似/ 但/ 字符串/ 表面/ 不/ 相似/ 的/ 问句/ 无法/ 被/ 召回/ ./ 2.1/ 基于/ 统计/ 机器翻译/ 模型/ 的/ 问句/ 检索/ 模型/ 针对/ 已有/ 的/ 基于/ 字符/ 匹配/ 的/ 检索/ 模型/ 在/ 匹配/ 用户/ 表述/ 多样性/ 上面/ 的/ 不足/ ,/ 研究/ 人员/ 将/ 基于/ 统计/ 机器翻译/ 模型/ 引入/ 到/ 信息检索/ 模型/ 中/ ,/ 用以/ 获取/ 用户/ 查询/ 中/ 的/ 词项/ 和/ 候选/ 文档/ 中/ 的/ 词项/ 之间/ 的/ 语义/ 相似性/ ./ 基于/ 统计/ 机器翻译/ 模型/ 的/ 信息检索/ 模型/ 最初/ 由/ Berger/ 和/ Lafferty/ [/ 15/ ]/ 提出/ ,/ 他们/ 将/ IBMModel1/ 以及/ 其/ 简化/ 版本/ (/ 他们/ 称其为/ Model0/ )/ 应用/ 于/ 信息检索/ 系统/ 中/ 并/ 在/ TREC/ 数据/ 上/ 验证/ 其/ 有效性/ ./ 随后/ ,/ Murdock/ 和/ Croft/ [/ 1/ ]/ 验证/ 了/ IBMModel1/ 在/ 句子/ 级/ 信息检索/ 上/ 的/ 表现/ 优于/ 传统/ 的/ QL/ (/ QueryLikeli/ -/ hood/ )/ 检索/ 模型/ ./ Xue/ 等/ 人/ [/ 4/ ]/ 将/ 语言/ 模型/ 中/ 的/ 平滑/ 机制/ 融入/ 到/ 统计/ 机器翻译/ 模型/ 中/ ,/ 从而/ 提出/ 了/ 一种/ 基于/ 翻译/ 模型/ 的/ 语言/ 模型/ ,/ 并/ 将/ 其/ 应用/ 到/ 问句/ 检索/ 中/ ./ 然而/ 其/ 采用/ 问句/ 和/ 自身/ 的/ 答案/ 作为/ 平行/ 语料/ 训练/ 翻译/ 模型/ ,/ 包含/ 了/ 很大/ 的/ 噪声/ ./ 鉴于/ 此/ ,/ Bernhard/ 和/ Gurevych/ [/ 5/ ]/ 通过/ 融合/ 多个/ 优质/ 的/ 单/ 语言/ 平行/ 语料/ ,/ 从而/ 使得/ 基于/ 翻译/ 模型/ 的/ 检索/ 模型/ 效果/ 得到/ 了/ 显著/ 的/ 提升/ ./ 其/ 采用/ 的/ 资源/ 包括/ WikiAnswer/ ①/ 中/ 的/ 问答/ 对/ ,/ 用户/ 标注/ 的/ 相似/ 问句/ 对/ 以及/ 同一/ 单词/ 在/ 不同/ 词/ ①/ http/ :/ // // wiki/ ./ answers/ ./ com/ // Page3/ 典中/ 的/ 解释/ 等/ ./ Zhou/ 等/ 人/ [/ 6/ ]/ 利用/ 短语/ 级/ 的/ 统计/ 机器翻译/ 模型/ 计算/ 查询/ 与/ 待/ 检索/ 文档/ 之间/ 的/ 相似性/ ,/ 并/ 将/ 其/ 应用/ 于/ 问句/ 检索/ 中/ ,/ 取得/ 优于/ 词/ 级别/ 的/ 翻译/ 模型/ 的/ 问句/ 检索/ 结果/ ./ 尽管/ 基于/ 短语/ 级/ 翻译/ 模型/ 的/ 检索/ 模型/ 能够/ 为/ 词项/ 引入/ 上下文/ 的/ 信息/ ,/ 从而/ 在/ 一定/ 程度/ 上/ 解决/ 翻译/ 歧义/ 的/ 问题/ ,/ 但是/ 其/ 仍然/ 没有/ 合理/ 的/ 机制/ 来/ 控制/ 统计/ 机器翻译/ 模型/ 在/ 翻译/ 过程/ 中/ 产生/ 的/ 噪声/ ./ 本文/ 利用/ 主题/ 信息/ 作为/ 一种/ 隐含/ 语义/ 约束/ ,/ 借此/ 来/ 调整/ 翻译/ 模型/ 用于/ 问句/ 检索/ 时词/ 项间/ 的/ 相似/ 度/ ,/ 从而/ 合理/ 地/ 实现/ 对/ 翻译/ 噪声/ 的/ 控制/ ./ 进而/ 更/ 好地解决/ 问句/ 检索/ 中/ 的/ 词/ 不/ 匹配/ 问题/ ./ 2.2/ 基于/ 主题/ 模型/ 的/ 问句/ 检索/ 模型/ 主题/ 模型/ 作为/ 一种/ 文档/ 表征/ 模型/ ,/ 近年来/ 被/ 广泛应用/ 于/ 信息检索/ 和/ 文本/ 挖掘/ 的/ 相关/ 任务/ 中/ [/ 17/ ]/ ./ 其中/ 代表性/ 的/ 主题/ 模型/ 主要/ 有/ PLSA/ [/ 16/ ]/ (/ ProbabilisticLatentSemanticAnalysis/ )/ 和/ LDA/ [/ 17/ ]/ (/ LatentDirichletAllocation/ )/ ./ Steyvers/ 等/ 人/ [/ 18/ ]/ 通过/ 将/ 作者/ 和/ 主题/ 之间/ 建立/ 起/ 潜在/ 的/ 语义/ 对应/ 关系/ ,/ 从而/ 提出/ 一种/ 新/ 的/ 作者/ 主题/ 模型/ ./ 并/ 将/ 此/ 模型/ 用于/ 在/ 论文/ 数据库/ 中/ 发现/ 研究/ 主题/ 的/ 趋势/ 演变/ 以及/ 为/ 特定/ 的/ 作者/ 生成/ 摘要/ 及/ 相关/ 论文/ 推荐/ 等/ ./ Wei/ 和/ Croft/ [/ 19/ ]/ 利用/ LDA/ 模型/ 对/ 文档/ 和/ 查询/ 之间/ 的/ 关系/ 进行/ 建模/ ,/ 通过/ 线性/ 结合/ 的/ 方式/ 将/ LDA/ 模型/ 融合/ 到/ 最终/ 的/ 检索/ 模型/ 中/ ,/ 在/ TREC/ 数据/ 集上/ 取得/ 较/ 好/ 的/ 检索/ 结果/ ./ 类似/ 地/ ,/ Cai/ 等/ 人/ [/ 7/ ]/ 利用/ LDA/ 检索/ 模型/ 融入/ 到/ 翻译/ 模型/ 中/ ,/ 并/ 将/ 其/ 应用/ 于/ 问句/ 检索/ 任务/ ./ Ji/ 等/ 人/ [/ 20/ ]/ 通过/ 对/ 问句/ 和/ 答案/ 分别/ 进行/ 主题/ 建模/ ,/ 并/ 假设/ 问句/ 和/ 答案/ 之间/ 不仅/ 属于/ 同一个/ 主题/ 而且/ 还/ 应该/ 共享/ 一些/ 词汇/ 信息/ ./ 即/ 利用/ 答案/ 作为/ 一种/ 查询/ 扩展/ 应用/ 于/ 原始/ 查询/ 中/ ,/ 并用/ 其/ 提高/ 问句/ 检索/ 结果/ ./ 尽管/ 上述/ 工作/ 验证/ 了/ 主题/ 信息/ 对于/ 问句/ 检索/ 的/ 作用/ ,/ 但是/ 据/ 我们/ 所知/ ,/ 通过/ 原理/ 性/ 的/ 分析/ 并/ 从/ 已有/ 的/ 检索/ 模型/ 中/ 合理/ 地/ 推导/ 并/ 融入/ 主题/ 模型/ 信息/ 的/ 工作/ 仍然/ 罕有/ 涉及/ ./ 本文/ 从/ 理论/ 上/ 验证/ 了/ 我们/ 所/ 提出/ 的/ 方法/ 能够/ 将/ 主题/ 模型/ 合理/ 地/ 融入/ 到/ 当前/ 最/ 先进/ 的/ 问句/ 检索/ 模型/ 中/ ,/ 从而/ 提出/ 一种/ 新/ 的/ 基于/ 主题/ 翻译/ 模型/ 的/ 问句/ 检索/ 模型/ ./ 2.3/ 基于/ 主题/ 模型/ 的/ 机器翻译/ 模型/ 近年来/ ,/ 基于/ 主题/ 模型/ 的/ 机器翻译/ 模型/ 受到/ 了/ 广泛/ 的/ 关注/ [/ 25/ -/ 29/ ]/ ,/ 其/ 主要/ 思想/ 是/ 通过/ 主题/ 模型/ 解决/ 翻译/ 模型/ 的/ 适应性/ 问题/ ./ 其中/ Zhao/ 和/ Xing/ [/ 25/ ]/ 提出/ 一种/ 双语/ 主题/ 混合/ 模型/ ,/ 并/ 将/ 其/ 用于/ 统计/ 机器翻译/ 的/ 词/ 对齐/ 任务/ 中/ ./ 在/ 词/ 和/ 句子/ 级别/ 上/ ,/ 作者/ 提出/ 了/ 三种/ 模型/ 来/ 获取/ 双语/ 文档/ 中/ 的/ 主题/ 信息/ ,/ 并/ 最终/ 将/ 三种/ 模型/ 进行/ 融合/ 用以/ 提高/ 词/ 对齐/ 的/ 效果/ ,/ 进而/ 提高/ 机器翻译/ 的/ 效果/ ./ Tam/ 等/ 人/ [/ 26/ ]/ 提出/ 了/ 一种/ 基于/ 双语/ LSA/ 的/ 跨/ 语言/ 的/ 语言/ 模型/ 以及/ 翻译/ 词典/ 自/ 适应/ 的/ 方法/ ./ 他们/ 首先/ 通过/ 主题/ 模型/ 对源/ 文本/ 进行/ 主题/ 分布/ 推断/ ,/ 然后/ 将/ 得到/ 的/ 主题/ 分布/ 用于/ 目标语言/ 的/ n/ -/ gram/ 语言/ 模型/ ,/ 以此/ 来/ 提高/ 翻译/ 模型/ 的/ 自适应性/ ./ Gong/ 等/ 人/ [/ 27/ ]/ 指出/ 现有/ 统计/ 机器翻译/ 只/ 考虑/ 句子/ 级别/ 的/ 信息/ 是/ 不合理/ 的/ ,/ 并/ 由此/ 引入/ 文档/ 主题/ 信息/ 用以/ 提高/ 机器翻译/ 的/ 效果/ ./ 具体/ 地/ ,/ 他们/ 利用/ LDA/ 主题/ 模型/ 对/ 文档/ 片段/ 进行/ 建模/ ,/ 并/ 在/ 翻译/ 过程/ 中/ 加入/ 主题/ 分布/ 概率/ 信息/ ./ Eidelman/ 等/ 人/ [/ 28/ ]/ 将/ 主题/ 模型/ 信息/ 作为/ 特征/ 融入/ 翻译/ 模型/ ,/ 用以/ 提高/ 翻译/ 模型/ 的/ 自/ 适应能力/ ./ Xiao/ 等/ 人/ [/ 29/ ]/ 提出/ 一种/ 基于/ 规则/ 的/ 主题/ 相似/ 度/ 计算/ 模型/ ,/ 并/ 将/ 其/ 应用/ 在/ 层次化/ 短语/ 结构/ 的/ 统计/ 机器翻译/ 模型/ 中/ ,/ 提升/ 翻译/ 模型/ 的/ 性能/ ./ 考虑/ 到/ 翻译/ 模型/ 在/ 问句/ 检索/ 上/ 的/ 良好/ 表现/ ,/ 本文/ 将/ 主题/ 模型/ 引入/ 到/ 翻译/ 模型/ 中/ ,/ 通过/ 主题/ 的/ 信息/ 提高/ 翻译/ 模型/ 的/ 性能/ ,/ 进而/ 提高/ 问句/ 检索/ 的/ 效果/ ./ 3/ 基于/ 主题/ 翻译/ 模型/ 的/ 问句/ 检索/ 模型/ 3.1/ LDA/ 背景/ 简介/ LDA/ 模型/ 是/ 由/ Blei/ 等/ 人/ [/ 17/ ]/ 提出/ 的/ 一种/ 新式/ 的/ 语义/ 一致/ 的/ 主题/ 模型/ ./ 它/ 的/ 提出/ 迅速/ 得到/ 了/ 统计/ 机器翻译/ ,/ 自然语言/ 处理/ 以及/ 信息检索/ 相关/ 研究者/ 的/ 关注/ ./ 同时/ ,/ LDA/ 是/ 一种/ 概率/ 图/ 模型/ ,/ 其/ 表示/ 形式/ 如图/ 2/ 所示/ ./ 其中/ Nm/ 为/ 第/ m/ 篇/ 文档/ 的/ 长度/ ,/ n/ 为/ 单篇/ 文档/ 中词/ 项/ 的/ 索引/ 号/ ,/ K/ 为/ 主题/ 数/ ,/ M/ 为/ 文档/ 集/ 的/ 规模/ ,/ 即/ 文档/ 数/ ./ 布/ )/ 选择/ 一个/ 多项式/ 分布/ / z/ ;/ 布/ )/ 选择/ 一个/ 多项式/ 分布/ θ/ m/ ;/ 选择/ 一个/ 主题/ z/ (/ z/ ∈/ {/ 1/ ,/ …/ ,/ K/ }/ )/ ;/ LDA/ 模型/ 生成/ 文本/ 内容/ 的/ 过程/ 如下/ 所示/ :/ (/ 1/ )/ 为/ 每个/ 主题/ z/ (/ 以/ 参数/ β/ 服从/ Dirichlet/ 分/ (/ 2/ )/ 为/ 每个/ 文档/ wm/ (/ 以/ 参数/ α/ 服从/ Dirichlet/ 分/ (/ 3/ )/ 为/ 每个/ 文档/ wm/ 中/ 的/ 词项/ wm/ ,/ n/ (/ n/ ∈/ [/ 1/ ,/ Nm/ ]/ )/ (/ 4/ )/ 从/ 多项式/ 分布/ / z/ 中/ 选择/ 词项/ wm/ ,/ n/ ./ 相应/ 地/ ,/ 对于/ 单篇/ 文档/ w/ →/ Page4/ 容/ 的/ 可能性/ (/ likelihood/ )/ 如下/ 所示/ :/ p/ (/ w/ →/ m/ |/ α/ →/ ,/ β/ →/ )/ =/ / p/ (/ / →/ 这里/ ,/ 在/ 社区/ 型/ 问答/ 服务/ 中/ ,/ 一个/ 问答/ 对/ 被/ 看作/ 是/ 一篇/ 文档/ ,/ 因此/ 在/ 本文/ 中/ 所用/ 到/ 的/ 主题/ 模型/ 建模/ 对象/ 图/ 3/ 基于/ 主题/ 翻译/ 模型/ 的/ 问句/ 检索/ 模型/ 接下来/ ,/ 我们/ 从/ 最/ 先进/ 的/ 问句/ 检索/ 模型/ 入手/ ,/ 推导/ 出/ 融合/ 主题/ 信息/ 的/ 新/ 的/ 问句/ 检索/ 排序/ 模型/ ./ 目前/ 最/ 先进/ 的/ 基于/ 统计/ 机器翻译/ 模型/ 的/ 问句/ 检索/ 模型/ [/ 4/ ]/ ,/ 其/ 排序/ 机制/ 如下/ 所示/ :/ PTLM/ (/ w/ |/ (/ q/ ,/ a/ )/ )/ =/ λ/ 1pml/ (/ w/ |/ q/ )/ +/ 其中/ ,/ w/ 表示/ 查询/ 中/ 的/ 特定/ 词项/ ,/ q/ 表示/ 待/ 检索/ 的/ 问句/ ,/ t/ 为/ q/ 中/ 的/ 词项/ ,/ a/ 为/ q/ 相对/ 应/ 的/ 答案/ ./ TLM/ 代表/ Xue/ 等/ 人/ [/ 4/ ]/ 提出/ 的/ 基于/ 翻译/ 模型/ 的/ 语言/ 模型/ ,/ ml/ 表示/ 极大/ 似然/ 估计/ 方法/ ./ 且/ 存在/ 关系/ λ/ 1/ +/ λ/ 2/ +/ λ/ 3/ =/ 1/ ./ 这里/ ,/ p/ (/ w/ |/ t/ )/ 为/ 从/ 查询/ 中/ 的/ 词项/ w/ 到/ 待/ 检索/ 问句/ 中/ 的/ 词项/ t/ 之间/ 的/ 翻译/ 概率/ ,/ 我们/ 在/ 此基础/ 之上/ ,/ 考虑/ 引入/ 主题/ 信息/ ,/ 从而/ 通过/ 主题/ 空间/ 上/ 的/ 相似性/ 来/ 提高/ 翻译/ 模型/ 的/ 准确性/ ,/ 具体/ 推导/ 如下/ 所示/ :/ p/ ^/ (/ w/ |/ t/ )/ =/ ∑/ K/ ∝/ γ/ p/ (/ w/ |/ t/ )/ +/ (/ 1/ -/ γ/ )/ ∑/ K/ =/ γ/ p/ (/ w/ |/ t/ )/ +/ (/ 1/ -/ γ/ )/ ∑/ K/ 为/ 问答/ 对/ 数据/ 集合/ ./ 3.2/ 基于/ 主题/ 翻译/ 的/ 问句/ 检索/ 模型/ 本节/ 将/ 介绍/ 我们/ 提出/ 的/ 基于/ 主题/ 翻译/ 的/ 问句/ 检索/ 模型/ ./ 图/ 3/ 所示/ 为/ 该/ 模型/ 的/ 系统/ 框图/ ./ 该/ 模型/ 以/ 用户/ 的/ 自然语言/ 问句/ 查询/ 为/ 输入/ ,/ 通过/ 利用/ 主题/ 模型/ 对/ 翻译/ 模型/ 的/ 质量/ 进行/ 提高/ ,/ 从而/ 实现/ 问句/ 检索/ 结果/ 的/ 优化/ ./ ∝/ γ/ p/ (/ w/ |/ t/ )/ +/ (/ 1/ -/ γ/ )/ ∑/ K/ 这里/ ,/ γ/ =/ γ/ 衡/ 翻译/ 模型/ 和/ 主题/ 模型/ 之间/ 的/ 权重/ ./ 式/ (/ 3/ )/ 第/ 1/ 步到/ 第/ 2/ 步/ 的/ 推导/ ,/ 是/ 为了/ 对/ p/ (/ w/ |/ t/ ,/ zi/ )/ 进行/ 估计/ 时/ 便于/ 计算所/ 采用/ 的/ 一种/ 近似/ 处理/ ,/ 这里/ ,/ 我们/ 采用/ 了/ 一种/ 经常/ 用于/ 联合/ 条件/ 概率/ 估计/ 的/ 线性插值/ 法/ [/ 19/ ]/ ,/ 具体/ 地/ ,/ 我们/ 使用/ p/ (/ w/ |/ t/ )/ 和/ p/ (/ w/ |/ zi/ )/ 对/ p/ (/ w/ |/ t/ ,/ zi/ )/ 进行/ 估计/ ./ p/ (/ w/ |/ t/ )/ 利用/ GIZA/ ++/ [/ 21/ ]/ 获得/ ,/ p/ (/ w/ |/ zi/ )/ 和/ p/ (/ t/ |/ zi/ )/ 由/ LDA/ 模型/ 通过/ Gibbs/ 采样/ 方法/ 估计/ 得出/ ./ 这样/ 我们/ 可以/ 得到/ 最终/ 的/ 问句/ 检索/ 排序/ 模型/ 如下/ 所示/ :/ PT2LM/ (/ w/ |/ (/ q/ ,/ a/ )/ )/ =/ μ/ 1pml/ (/ w/ |/ q/ )/ +/ μ/ 2/ ∑/ t/ ∈/ q/ (/ pml/ (/ t/ |/ q/ )/ ∑/ K/ μ/ 3/ ∑/ t/ ∈/ qPage5/ 其中/ ,/ T2LM/ 表示/ 我们/ 提出/ 的/ 基于/ 主题/ 翻译/ 模型/ 的/ 问句/ 检索/ 模型/ ./ 且/ 存在/ μ/ 1/ +/ μ/ 2/ +/ μ/ 3/ +/ μ/ 4/ =/ 1/ ./ 从/ 直观/ 上/ 看/ ,/ 我们/ 可以/ 得出结论/ ,/ 当/ p/ (/ w/ |/ zi/ )/ 和/ p/ (/ t/ |/ zi/ )/ 的/ 值/ 越/ 接近/ 时/ ,/ 则/ 式/ (/ 4/ )/ 中/ 第/ 3/ 项/ 的/ 值/ 越/ 高/ ,/ 即/ 查询/ 中/ 的/ 词项/ w/ 以及/ 待/ 检索/ 问句/ 中/ 的/ 词项/ t/ 属于/ 同一/ 主题/ 的/ 可能性/ 越大/ ,/ 则/ 其/ 相似性/ 值越/ 高/ ,/ 从而/ 实现/ 利用/ 主题/ 信息/ 对词/ 项间/ 相似性/ 的/ 度量/ 机制/ 的/ 更新/ ./ 4/ 实验/ 结果/ 及/ 分析/ 4.1/ 实验/ 数据/ 集/ 我们/ 利用/ API/ ①/ 从/ Yahoo/ !/ Answers/ 中/ 获取/ 了/ 共/ 1123134/ 个/ 完整/ 问答/ 对/ ,/ 其中/ 包括/ 问句/ 的/ title/ 、/ content/ 以及/ answers/ ./ 该/ 数据/ 集/ 覆盖/ 了/ 较为/ 广泛/ 的/ 主题/ ,/ 例如/ Health/ 、/ Internet/ 等/ ./ 我们/ 从中/ 随机/ 的/ 选择/ 了/ 200/ 个/ 问句/ 作为/ 我们/ 的/ 查询/ 集合/ ,/ 在/ 去除/ 停/ 用词/ 之后/ ,/ 我们/ 手工/ 地/ 过滤/ 掉/ 长度/ 小于/ 2/ 个/ 词/ 的/ 查询/ ,/ 最终/ 得到/ 了/ 168/ 个/ 问句/ 查询/ ,/ 我们/ 在/ 其中/ 随机/ 选择/ 了/ 140/ 个/ 问句/ 作为/ 测试/ 查询/ ,/ 剩余/ 的/ 28/ 个/ 作为/ 开发/ 集/ 用于/ 参数/ 调整/ ./ 另外/ ,/ 由于/ 我们/ 需要/ 的/ 是/ 在/ 大规模/ 数据/ 上/ 对/ 主题/ 进行/ 建模/ ,/ 因此/ 本文/ 主题/ 模型/ 建模/ 的/ 对象/ 是/ 整个/ 问答/ 对/ 数据/ 集/ ./ 为了/ 获取/ 查询/ 相关性/ 问句/ 集合/ ,/ 对于/ 查询/ 测试/ 集中/ 的/ 每个/ 查询/ ,/ 我们/ 汇集/ 多个/ 搜索/ 模型/ (/ 如/ 向量/ 空间/ 模型/ 、/ BM25/ 模型/ 、/ 语言/ 模型/ 以及/ 翻译/ 模型/ 等/ )/ 的/ 前/ 20/ 个/ 检索/ 结果/ ,/ 并/ 聘请/ 两位/ 以/ 英语/ 为/ 母语/ ,/ 且/ 不/ 熟悉/ 当前/ 实验/ 方法/ 设计/ 的/ 学生/ 进行/ 手工/ 标注/ 检索/ 结果/ 为/ 相关/ (/ 数字/ 1/ )/ 或/ 不/ 相关/ (/ 数字/ 0/ )/ ,/ 当/ 标注/ 出现/ 冲突/ 时/ ,/ 由/ 第/ 3/ 位/ 标注/ 人员/ 对/ 标注/ 结果/ 进行/ 判定/ ./ 我们/ 采用/ p/ @/ 1/ (/ precisionatpositionone/ )/ 、/ MAP/ (/ MeanAveragePrecision/ )/ 和/ MRR/ (/ MeanReciprocalRank/ )/ [/ 24/ ]/ 作为/ 评价/ 指标/ ./ 4.2/ 实验/ 对比/ 系统/ 最/ 先进/ 模型/ 作为/ 对比/ 系统/ ,/ 具体/ 设置/ 如下/ :/ 我们/ 选取/ 了/ 在/ 问句/ 检索/ 方面/ 的/ 经典/ 模型/ 及/ 当前/ (/ 1/ )/ 语言/ 模型/ (/ LM/ )/ ./ Jeon/ 等/ 人/ [/ 2/ ]/ 提出/ 的/ 基于/ 语言/ 模型/ 的/ 问句/ 检索/ 模型/ ./ (/ 2/ )/ 翻译/ 模型/ (/ TRM/ )/ ./ Murdock/ 和/ Croft/ [/ 1/ ]/ 提出/ 的/ 基于/ 统计/ 机器翻译/ 模型/ 的/ 句子/ 检索/ 模型/ ./ (/ 3/ )/ 基于/ 翻译/ 模型/ 的/ 语言/ 模型/ (/ TLM/ )/ ./ Xue/ 等/ 人/ [/ 4/ ]/ 提出/ 的/ 基于/ 翻译/ 模型/ 的/ 语言/ 模型/ ./ (/ 4/ )/ 基于/ 词项/ 赋权/ 的/ TLM/ (/ drTLM/ )/ ./ Zhang/ 等/ 人/ [/ 14/ ]/ 提出/ 的/ 利用/ 依存/ 句法分析/ 图/ 进行/ 问/ 句词/ 项/ 重新/ 赋权/ 的/ TLM/ ./ (/ 5/ )/ 主题/ 模型/ (/ TM/ )/ ./ Wei/ 和/ Croft/ [/ 19/ ]/ 提出/ 的/ 基于/ LDA/ 的/ 信息检索/ 模型/ ./ 此外/ ,/ 还有/ 很多/ 在/ 问句/ 检索/ 方面/ 的/ 杰出/ 工作/ ,/ 如/ Cao/ 等/ 人/ [/ 12/ ]/ 、/ Cai/ 等/ 人/ [/ 7/ ]/ 和/ Zhou/ 等/ 人/ [/ 6/ ]/ 等/ ,/ 前/ 两项/ 工作/ 依赖于/ 问句/ 所在/ 的/ 类别/ 信息/ ,/ 后/ 一项/ 工作/ 基于/ 短语/ 级/ 翻译/ 模型/ ./ 然而/ ,/ 一方面/ ,/ 我们/ 所/ 提出/ 方法/ 的/ 目标/ 是/ 构建/ 一个/ 通用/ 的/ 问句/ 检索/ 模型/ ,/ 使/ 其/ 可以/ 不/ 受/ 应用/ 场景/ 的/ 影响/ ./ 因此/ ,/ 我们/ 并/ 没有/ 利用/ Yahoo/ !/ Answers/ 的/ 类别/ 信息/ 作为/ 辅助/ 信息/ 来/ 指导/ 问句/ 检索/ 模型/ ./ 另一方面/ ,/ 考虑/ 到/ 短语/ 识别/ 本身/ 存在/ 一定/ 的/ 错误/ ,/ 以及/ 短语/ 级别/ 的/ 问句/ 检索/ 存在/ 数据/ 稀疏/ 问题/ ,/ 都/ 会/ 影响/ 问句/ 检索/ 的/ 效果/ ,/ 因此/ 我们/ 采用/ 词/ 作为/ 基础/ 单元/ 进行/ 问句/ 检索/ 的/ 相关/ 研究/ ./ 文献/ [/ 20/ ]/ 利用/ 主题/ 模型/ 度量/ 问题/ 与/ 答案/ 词项/ 之间/ 主题/ 分布/ 的/ 相似性/ ,/ 指导/ 问句/ 检索/ 模型/ ,/ 而/ 本文/ 的/ 方法/ 更加/ 注重/ 于/ 度量/ 问句/ 与/ 问/ 句词/ 项/ 之间/ 的/ 主题/ 分布/ 的/ 相似性/ ,/ 以此/ 对/ 问句/ 检索/ 模型/ 进行/ 改进/ ,/ 由于/ 本文/ 与/ 文献/ [/ 20/ ]/ 在/ 任务/ 的/ 基本/ 假设/ 上/ 存在/ 差异/ ,/ 以及/ 文献/ [/ 20/ ]/ 与/ 文献/ [/ 19/ ]/ 在/ 主题/ 建模/ 形式/ 上/ 相似/ ,/ 又/ 同时/ 应用/ 于/ 检索/ 任务/ 中/ ,/ 因此/ ,/ 我们/ 对比/ 了/ 本文/ 方法/ 和/ 文献/ [/ 19/ ]/ 的/ 实验/ 结果/ ,/ 而/ 没有/ 与/ 文献/ [/ 20/ ]/ 进行/ 直接/ 的/ 比较/ ./ 我们/ 在/ 开发/ 集中/ ,/ 利用/ WEKA/ [/ 22/ ]/ 提供/ 的/ GridSearch/ 工具/ 将/ 上述/ 对比/ 系统/ 以及/ 我们/ 系统/ 的/ 参数/ 分别/ 调/ 至/ 最优/ ,/ 其中/ μ/ 1/ =/ 0.3/ ,/ μ/ 2/ =/ 0.1/ ,/ μ/ 3/ =/ 0.4/ ,/ μ/ 4/ =/ 0.2/ ,/ 主题/ 数/ K/ =/ 80/ ./ 表/ 1/ 所示/ 为/ 实验/ 结果/ 对比/ ./ 本文/ 所有/ 的/ 结果/ 都/ 是/ 在/ p/ </ 0.05/ 的/ 条件/ 下/ 进行/ t/ -/ test/ 统计/ 显著性/ 检验/ 的/ 结果/ ./ 在/ 统计/ 显著性/ 检验/ 中/ ,/ p/ 值/ 表示/ 当/ 假设/ 成立/ 时/ ,/ 获得/ 一个/ 测试/ 统计/ 至少/ 被/ 观测/ 到/ 一次/ 的/ 概率/ ./ p/ 值/ 通常/ 与/ 接受/ 假设检验/ 结果/ 成立/ 的/ 置信度/ 相对/ 应/ ,/ 即/ p/ </ 0.05/ 意味着/ 我们/ 可以/ 以/ 高于/ 0.95/ 的/ 置信度/ 接受/ 假设检验/ 的/ 结果/ ./ 本/ 实验/ 中/ ,/ 我们/ 以/ 步长/ 为/ 10/ ,/ 变化/ 范围/ 为/ 10/ ~/ 50/ ,/ 对/ GibbsMAP0/ ./ 26350.26780/ ./ 28890.30430/ ./ 41700.4375/ %/ ofMAPimprovementsoverLMTRMTLMTMdrTLMp/ @/ 1MRR/ 注/ :/ 粗体/ 为/ 我们/ 所/ 提出/ 的/ 方法/ 及/ 相应/ 的/ 结果/ ,/ 其中/ T2LM/ 在/ p/ </ 0.05/ 的/ 情况/ 下/ ,/ 在/ 统计/ 上/ 显著/ 优于/ LM/ 、/ TRM/ 、/ TLM/ 和/ TM/ ./ ①/ http/ :/ // // developer/ ./ yahoo/ ./ com/ // answers/ // Page6/ 采样/ 的/ 样本量/ 进行/ 了/ 测试/ ,/ 测试/ 结果表明/ ,/ 采样/ 样本量/ 的/ 变化/ 在/ 实验/ 结果/ 上/ 的/ 差异性/ 可以/ 忽略不计/ ,/ 本文/ 采用/ 的/ 采样/ 样本量/ 为/ 40/ ./ 由表/ 1/ 我们/ 可以/ 得出/ 以下/ 的/ 分析/ :/ (/ 1/ )/ TM/ 和/ T2LM/ 在/ 问句/ 检索/ 上/ 的/ 效果/ 要/ 优于/ LM/ 、/ TRM/ 和/ TLM/ ./ 这/ 是因为/ 前/ 两个/ 模型/ 引入/ 了/ 主题/ 信息/ 辅助/ 的/ 问句/ 检索/ 模型/ ./ 主题/ 信息/ 对于/ TM/ 而言/ 其/ 作用/ 相当于/ 一种/ 查询/ 扩展/ ,/ 而/ 对于/ T2LM/ 来说/ ,/ 由于/ 其/ 引入/ 了/ 词/ 项间/ 的/ 翻译/ 信息/ ,/ 因此/ 主题/ 信息/ 既/ 可以/ 看作/ 是/ 基于/ 翻译/ 模型/ 的/ 扩展/ ,/ 同时/ 通过/ 主题/ 的/ 限定/ 也/ 能够/ 帮助/ 控制/ 翻译/ 模型/ 在/ 词项/ 互译/ 过程/ 中/ 产生/ 的/ 噪声/ ,/ 因此/ 主题/ 模型/ 在/ 应用/ 于/ 基于/ 翻译/ 模型/ 的/ 问句/ 检索/ 模型/ 上/ 更/ 有/ 优势/ ./ (/ 2/ )/ 对比/ TLM/ 和/ TM/ 的/ 结果/ 我们/ 可以/ 看出/ ,/ 虽然/ 这/ 两个/ 模型/ 都/ 是/ 部分/ 地/ 基于/ 语言/ 模型/ 的/ 问句/ 检索/ 模型/ ,/ 但是/ 基于/ 主题/ 模型/ 的/ 语言/ 模型/ (/ TM/ )/ 的/ 实验/ 结果/ 要/ 优于/ 基于/ 翻译/ 模型/ 的/ 语言/ 模型/ (/ TLM/ )/ ./ 从而/ 说明/ 对于/ 问句/ 查询/ 扩展/ 而言/ ,/ 主题/ 模型/ 的/ 效果/ 要/ 优于/ 翻译/ 模型/ ./ 这/ 是因为/ 主题/ 模型/ 的/ 工作/ 原理/ 更/ 类似/ 于/ 词项/ 聚类/ ,/ 即将/ 语义上/ 相似/ 的/ 词项/ 聚类/ 成/ 若干个/ 主题/ 类别/ ./ 但/ 翻译/ 模型/ 则/ 是/ 基于/ 词项/ 间/ 的/ 统计/ 共/ 现性/ 的/ ,/ 而共现/ 频度/ 高/ 的/ 词项/ 未必/ 是/ 同一/ 类别/ 的/ ,/ 因此/ 在/ 问句/ 检索/ 的/ 查询/ 扩展/ 方面/ 主题/ 模型/ 更/ 有/ 优势/ ./ (/ 3/ )/ 对比/ T2LM/ 和/ TLM/ 的/ 结果/ 我们/ 可以/ 看出/ ,/ 融入/ 主题/ 信息/ 的/ T2LM/ 模型/ 在/ MAP/ 上面/ 高出/ TLM/ 模型/ 51.44/ %/ ./ 说明/ 其/ 对/ 基于/ 翻译/ 模型/ 的/ 问句/ 检索/ 模型/ 有/ 较大/ 的/ 帮助/ ./ 同时/ 我们/ 注意/ 到/ ,/ 在/ T2LM/ 中/ μ/ 3/ 的/ 值/ 最大/ ,/ 这/ 说明/ 了/ 其/ 在/ 问句/ 检索/ 上面/ 性能/ 的/ 提升/ 主要/ 来自/ 于/ 主题/ 模型/ 部分/ ./ 此外/ ,/ 由于/ 直接/ 对/ 词项/ 翻译/ 结果/ 进行/ 评价/ 需要/ 大量/ 的/ 人力/ ,/ 因此/ 本/ 实验/ 中/ 没有/ 讨论/ 在/ 加入/ 主题/ 信息/ 之后/ 的/ 词项/ 翻译/ 结果/ 和/ 原始/ 的/ 词项/ 翻译/ 结果/ 的/ 比较/ ./ 由于/ TLM/ 方法/ 的/ 原始/ 文献/ 中/ ,/ 没有/ 发布/ 实验/ 数据/ 集/ ,/ 因此/ 我们/ 无法/ 在/ 其/ 原始数据/ 集上/ 重现/ 实验/ 结果/ ./ 另外/ ,/ TLM/ 在/ 我们/ 的/ 数据/ 集上/ 的/ 表现/ 低于/ 其/ 原始/ 文献/ 中/ 的/ 结果/ ,/ 这/ 是因为/ 我们/ 实验/ 中/ 所/ 采用/ 的/ 数据/ 集是/ 来自/ Yahoo/ !/ Answers/ ,/ 而/ TLM/ 原始/ 文献/ 中/ 的/ 数据/ 集/ 来自/ 于/ Wondir/ ①/ ,/ 这是/ 数据/ 集上/ 的/ 差异/ ./ 此外/ ,/ TLM/ 原始/ 文献/ 中/ 的/ 测试/ 集是/ 来自/ 于/ TRECQA/ 的/ 50/ 个/ 问题/ ,/ TRECQA/ 与表/ 2/ 问句/ 检索/ 结果/ 比较/ 示例/ Rank1IstheAppleStoreonlineagoodstoretobuyfrom/ ?/ 2IsthereanyonlineshopforApplefruittreetobesendto3Isbigapplepetsupplyreliableforliveanimalshipments/ ?/ IsitcheapertobuyaniPodontheonlineapplestoreorinaYahoo/ !/ Answers/ 的/ 数据/ 差异/ 很大/ ,/ TRECQA/ 的/ 数据/ 是/ 人工/ 构造/ 的/ 问题/ ,/ 而/ 我们/ 采用/ 的/ 测试数据/ 是/ 完全/ 由/ 用户/ 生成/ 的/ 数据/ ./ 因此/ 导致/ 了/ TLM/ 在/ 本文/ 上/ 的/ 表现/ 以及/ 与/ T2LM/ 之间/ 结果/ 的/ 差异/ ./ (/ 4/ )/ 值得注意/ 的/ 是/ ,/ 我们/ 对比/ 了/ 目前/ 最/ 先进/ 的/ 基于/ 词项/ 重要性/ 赋权/ 的/ 问句/ 检索/ 模型/ drTLM/ ./ 通过/ 比较/ 发现/ ,/ T2LM/ 的/ 结果/ 要/ 优于/ drTLM/ ,/ 这/ 主要/ 因为/ T2LM/ 一方面/ 是/ 通过/ 改变/ 查询/ 中词/ 项/ 的/ 权重/ 从而/ 提升/ 问句/ 检索/ 的/ 效果/ ,/ 更/ 重要/ 的/ 是/ 另一方面/ 其/ 能够/ 从/ 翻译/ 模型/ 中/ 获得/ 词项/ 扩展/ 增益/ ./ 由于/ 在/ 使用/ LDA/ 的/ 过程/ 中/ ,/ 主题/ 数/ 需要/ 在/ 实验/ 前/ 给定/ ,/ 因此/ 我们/ 考虑/ 了/ 主题/ 数/ 对于/ 实验/ 结果/ 的/ 影响/ ,/ 图/ 4/ 所示/ 为/ 在/ 开发/ 集上/ MAP/ 随着/ 主题/ 数/ 的/ 变化/ 曲线/ ./ 图/ 4T2LM/ 模型/ 的/ 主题/ 数/ 与/ MAP/ 的/ 变化/ 曲线/ 由图/ 4/ 可以/ 看出/ ,/ 在/ 主题/ 数/ 小于/ 80/ 的/ 时候/ MAP/ 的/ 值/ 随着/ 主题/ 数/ 增长/ 而/ 增长/ ,/ 在/ 80/ 之后/ 则/ 趋于平稳/ ./ 通过观察/ 主题/ 分析/ 后/ 的/ 输出/ 数据/ 我们/ 可以/ 看出/ ,/ 当/ 设定/ 的/ 主题/ 数/ 超过/ 80/ 之后/ ,/ 主题/ 区分度/ 的/ 模糊性/ 便/ 显现出来/ ,/ 以/ 100/ 个/ 主题/ 为例/ ,/ 我们/ 通过/ 输出/ 每个/ 主题/ 的/ 高频词/ 列表/ 中/ 观察/ 到/ ,/ 有/ 20/ 个/ 主题/ 的/ 高频词/ 都/ 能够/ 在/ 其他/ 80/ 个/ 主题/ 中/ 找到/ ,/ 而/ 在/ 这/ 20/ 个/ 主题/ 中/ ,/ 除去/ 其/ 高频词/ 之外/ 的/ 其他/ 主题词/ 则/ 多数/ 为/ 噪声/ 词/ ,/ 因此/ ,/ 我们/ 可以/ 推断/ 这/ 20/ 个/ 主题/ 能够/ 被/ 包含/ 在/ 其余/ 的/ 80/ 个/ 主题/ 中/ ./ 对/ 其他/ 主题/ 数/ 的/ 观察/ 结果/ 与/ 上述/ 结果/ 相似/ ./ 因此/ 在/ 我们/ 的/ 实验/ 中/ ,/ 主题/ 数/ 选取/ 为/ 80/ ./ 表/ 2/ 所示/ 为/ TLM/ 和/ T2LM/ 在/ “/ IstheAppleStoreonlineagoodstore/ ?/ ”/ 查询/ 上前/ 3/ 位/ 检索/ 结果/ 的/ 对比/ ,/ 其中/ 粗体/ 为/ 相关/ 检索/ 结果/ ./ 由表/ 2/ 我们/ 可以/ IstheAppleStoreonlineagoodstoretobuyfrom/ ?/ ShouldIgetmyipodnanoattheAppleStoreoronline/ ?/ regularstore/ // shop/ ?/ ①/ http/ :/ // // www/ ./ wondir/ ./ com/ // Page7/ 直观/ 地/ 看出/ T2LM/ 检索/ 结果/ 明显/ 优于/ TLM/ ,/ 这/ 是因为/ 在/ T2LM/ 中/ ,/ 其/ 问句/ 检索/ 结果/ 被/ 限定/ 在/ 3/ 个/ 主题/ 中/ ,/ 在/ 本/ 实验/ 中为/ 第/ 27/ ,/ 32/ 和/ 58/ ,/ 如图/ 3/ 中/ 所示/ ,/ 可见/ 这/ 3/ 个/ 主题/ 都/ 与/ AppleStore/ 有关/ ,/ 且/ 都/ 是/ 电子产品/ 类别/ ,/ 因此/ T2LM/ 能够/ 召回/ 更/ 多/ 相关/ 的/ 检索/ 结果/ 且/ 排序/ 靠前/ ./ 5/ 实现/ 和/ 应用/ 时/ 的/ 关键技术/ 点/ 本文/ 所/ 提出/ 的/ 基于/ 主题/ 翻译/ 模型/ 的/ 问句/ 检索/ 模型/ 在/ 实现/ 和/ 应用/ 时/ ,/ 主要/ 依赖/ 的/ 是/ 主题/ 分布/ 信息/ 和/ 词/ 的/ 互译/ 概率/ 信息/ ./ 前者/ 是/ 通过/ LDA/ 主题/ 模型/ 训练/ 得到/ 的/ 词/ 的/ 主题/ 分布/ 信息/ ,/ 后者/ 是/ 通过/ Giza/ ++/ 词/ 对齐/ 后/ 得到/ 的/ 词/ 与/ 词/ 的/ 互译/ 信息/ ./ 此外/ ,/ 为了/ 保证/ 实际效果/ 的/ 准确性/ ,/ 计算/ 主题/ 分布/ 信息/ 时/ ,/ 需要/ 根据/ 特定/ 的/ 数据/ 集/ 调整/ 主题/ 的/ 数量/ ,/ 同时/ ,/ 需要/ 在/ 相应/ 的/ 数据/ 集上/ 获取/ 单语/ 平行/ 语料/ 作为/ 翻译/ 模型/ 输入/ ./ 最后/ ,/ 在/ 应用/ 时/ ,/ 需要/ 根据/ 特定/ 的/ 应用/ 来/ 调整/ 整个/ 问句/ 检索/ 模型/ 的/ 各个/ 参数/ ,/ 以/ 达到/ 最优/ 的/ 问句/ 检索/ 效果/ ./ 6/ 结论/ 及/ 未来/ 工作/ 本文/ 提出/ 了/ 一种/ 基于/ 主题/ 翻译/ 模型/ 的/ 问句/ 检索/ 模型/ ,/ 通过/ 在/ 基于/ 统计/ 机器翻译/ 模型/ 的/ 问句/ 检索/ 模型/ 中/ 引入/ 主题/ 信息/ ,/ 从而/ 解决/ 了/ 由于/ 翻译/ 模型/ 产生/ 的/ 噪声/ 而/ 影响/ 问句/ 检索/ 结果/ 的/ 问题/ ./ 同时/ 我们/ 在/ 理论/ 上/ 说明/ 我们/ 所/ 提出/ 的/ 主题/ 模型/ 可以/ 合理/ 地/ 融合/ 到/ 已有/ 的/ 最/ 先进/ 的/ 检索/ 模型/ 中/ ,/ 实验/ 结果/ 证实/ 了/ 其/ 有效性/ ./ 尽管/ 主题/ 模型/ 能够/ 作为/ 一种/ 潜在/ 语义/ 扩展/ 增强/ 问句/ 检索/ 的/ 效果/ ,/ 但是/ 我们/ 也/ 应当/ 发现/ ,/ 目前/ 的/ 技术/ 没有/ 很/ 好地解决/ 主题/ 间/ 的/ 歧义/ 关系/ 问题/ ,/ 因此/ 在/ 后续/ 工作/ 中/ ,/ 我们/ 会/ 进一步/ 深入探讨/ 如何/ 解决/ 主题/ 的/ 歧义/ 性/ 问题/ ,/ 以期/ 获得/ 更好/ 的/ 问句/ 检索/ 效果/ ./ 致谢/ 编辑/ 及/ 审稿/ 老师/ 给/ 了/ 宝贵意见/ ,/ 在/ 此/ 表示感谢/ !/ 

