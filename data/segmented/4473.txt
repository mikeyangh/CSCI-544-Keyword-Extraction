Page1Redbud/ 并行/ 文件系统/ 的/ 可/ 扩展/ 存储管理/ 机制/ 易/ 乐天/ 舒/ 继武/ 郑纬民/ (/ 清华大学/ 计算机科学/ 与/ 技术/ 系/ 北京/ 100084/ )/ 摘要/ 数据/ 规模/ 和/ 并发/ 访问/ 的/ 需求/ 日益增长/ ,/ 可/ 扩展/ 能力/ 成为/ 并行/ 文件系统/ 的/ 重要/ 需求/ 之一/ ./ 文中/ 提出/ 了/ 一种/ 基于/ 非对称/ 并行/ 文件系统/ Redbud/ 的/ 高/ 可/ 扩展/ 资源管理/ 机制/ ./ 该/ 管理机制/ 根据/ 数据/ 的/ 访问/ 特征/ ,/ 使用/ 不同/ 的/ 树形/ 结构/ 管理/ 不同/ 类型/ 的/ 数据/ ,/ 满足/ 了/ 文件/ 数据/ 和/ 元/ 数据/ 的/ 并发/ 检索/ 需求/ ;/ 该/ 管理机制/ 还/ 使用/ 文件/ 级/ 的/ 数据分布/ 机制/ ,/ 允许/ 用户/ 利用/ 各种/ 策略/ 进行/ 目录/ 和/ 文件/ 的/ 管理/ ,/ 能/ 满足/ 文件/ 级/ 的/ 数据/ 访问/ 性能/ 、/ 目录/ 级/ 数据/ 可靠性/ 等/ 实际/ 应用/ 需求/ ./ 多个/ 基准/ 测试程序/ 和/ 实际/ 应用程序/ 的/ 测试/ 结果表明/ ,/ 文件/ 的/ 独占/ 访问/ 能/ 达到/ 磁盘/ 95/ %/ 的/ 性能/ ;/ 同时/ ,/ 随着/ 设备/ 和/ 应用/ 节点/ 的/ 增加/ ,/ 数据/ 和/ 元/ 数据/ 的/ 并发/ 访问/ 性能/ 线性/ 增长/ ./ 关键词/ Redbud/ 并行/ 文件系统/ ;/ 高性能/ 计算/ ;/ 存储/ 资源管理/ ;/ 共享/ 磁盘/ 1/ 引言/ 在/ 具备/ 存储/ 区域/ 网络/ (/ StorageAreaNetwork/ ,/ SAN/ )/ 的/ 数据中心/ 中/ ,/ 基于/ 共享/ 磁盘/ 的/ 并行/ 文件系统/ 被/ 广泛/ 地/ 部署/ ,/ 为/ 各种/ 应用/ 提供/ 存储/ 需求/ ./ 这些/ 数据服务/ 通常/ 呈现/ 两种/ 趋势/ :/ 一方面/ ,/ 数据服务/ 规模/ 越来越/ 大/ ,/ 比如/ ,/ 科学计算/ 已经/ 成为/ 数据/ 密集型/ 的/ 应用/ ,/ 许多/ 实验/ 数据/ 每周/ 以/ TB/ 级/ 增长/ ,/ 其/ 存储/ 数据量/ 逐渐/ 迈向/ EB/ 级/ ;/ 2008/ 年/ 的/ Jaguar/ 系统/ [/ 1/ ]/ 则/ 有/ 10PB/ 的/ Page2/ 存储容量/ 和/ 192/ 个/ IO/ 节点/ ./ 另一方面/ ,/ 并发/ 访问/ 的/ 需求/ 越来越/ 高/ ,/ 比如/ ,/ 高性能/ 科学计算/ 产生/ 大量/ 的/ 中间/ 文件/ ,/ 这些/ 文件/ 通常/ 被/ 多个/ 应用/ 节点/ 并发/ 访问/ [/ 2/ ]/ ;/ CPU/ 多/ 核技术/ 的/ 发展/ 也/ 加剧/ 了/ 并发/ 访问/ 的/ 趋势/ ./ 这些/ 趋势/ 对/ 存储资源/ 的/ 管理/ 提出/ 了/ 更/ 高/ 的/ 可扩展性/ 要求/ ./ 在/ 传统/ 的/ 基于/ SAN/ 环境/ 的/ 并行/ 文件系统/ 中/ ,/ 所有/ 共享/ 磁盘/ 以/ 逻辑/ 卷/ 的/ 方式/ 管理/ ,/ 比如/ ,/ GFS2/ (/ GlobalFileSystemVersion2/ )/ 采用/ LVM/ (/ LogicalVolumeManager/ )/ 建立/ 存储/ 池/ ①/ ,/ GPFS/ (/ GeneralParallelFileSystem/ )/ 采用/ 分段/ 放置/ 的/ 方式/ ,/ 将/ 所有/ 数据/ 以/ 条带/ 化/ 的/ 方式/ 存储/ [/ 2/ ]/ ./ 当/ 系统/ 规模/ 扩展/ 时/ ,/ 这种/ 方法/ 难以/ 为/ 每个/ 逻辑/ 卷/ 增加/ 存储空间/ ;/ 而且/ ,/ 这些/ 方法/ 采用/ 静态/ 的/ 方法/ 分布/ 数据/ ,/ 比如/ GFS2/ 条带/ 卷/ 的/ 宽度/ 被/ 设定/ 后/ ,/ 无法/ 针对/ 指定/ 文件/ 或者/ 目录/ 进行/ 布局调整/ ,/ 降低/ 了/ 系统/ 的/ 扩展性/ ./ Redbud/ 是/ 一种/ 基于/ SAN/ 环境/ 的/ 并行/ 文件系统/ ,/ 与/ 许多/ 分布式文件系统/ 类似/ ②/ ,/ 它/ 采用/ 非对称/ 存储/ 结构/ :/ 将/ 数据/ 和/ 元/ 数据分布/ 在/ 不同/ 的/ 存储设备/ 中/ ./ 为了/ 高效/ 支持/ 海量/ 数据/ 的/ 并行/ 访问/ ,/ 基于/ 这种/ 非对称/ 存储/ 结构/ ,/ 本文/ 提出/ 一种/ 高可/ 扩展/ 的/ 存储管理/ 机制/ —/ —/ —/ Rstore/ ./ 通过/ 把/ 存储设备/ 划分/ 为/ 并行/ 分配/ 组/ ,/ Rstore/ 允许/ 文件/ 存储空间/ 的/ 分配/ 和/ 回收/ 分布/ 到/ 不同/ 的/ 区域/ 中/ ,/ 避免/ 访问/ 冲突/ 产生/ 的/ IO/ 等待/ ,/ 满足/ 了/ 并发/ 访问/ 的/ 需求/ ./ Rstore/ 以/ 分层/ 的/ 方法/ 管理/ 海量/ 存储空间/ ,/ 分为/ 语义/ 感知/ 层/ 和/ 空间/ 映射/ 层/ ./ 其中/ ,/ 语义/ 感知/ 层/ 跟踪/ 文件/ 的/ 分配/ 语义/ ,/ 开发/ 文件/ 分布/ 的/ 局部性/ ,/ 比如/ 将/ 同一/ 客户端/ 的/ 文件/ 分配/ 在/ 临近/ 的/ 位置/ ;/ 同时/ ,/ 它/ 允许/ 用户/ 根据/ 应用/ 需求/ 为/ 目录/ 配置/ 分布/ 策略/ ,/ 比如/ ,/ 用户/ 可以/ 为/ 重要/ 的/ 目录/ 定制/ 更/ 高/ 的/ 可靠性/ 级别/ ,/ 保证/ 子/ 文件/ 以多/ 副本/ 的/ 方式/ 分布/ 在/ 多个/ 硬件/ 失效/ 域/ 中/ ./ 空间/ 映射/ 层/ 利用/ 基于/ 区间/ 范围/ (/ extent/ )/ 记录/ 文件/ 的/ 分配情况/ ,/ 使用/ 多个/ 树形/ 结构/ 建立/ 空间/ 的/ 快速/ 索引/ ,/ 比如/ ,/ 使用/ B/ 树/ 提供/ 元/ 数据/ 的/ 快速/ 索引/ ,/ 将/ 键值/ 与/ 文件/ 访问/ 的/ 上下文/ 语义/ 相/ 联系/ ,/ 保持/ 了/ 目录/ 元/ 数据/ 和/ 进程/ 的/ 访问/ 局部性/ ./ 我们/ 采用/ 基准/ 测试程序/ 和/ 真实/ 的/ 并行/ 应用程序/ 进行/ 了/ 测试/ ,/ 结果表明/ ,/ Rstore/ 在/ 两个/ 方面/ 获得/ 了/ 较/ 好/ 的/ 可扩展性/ :/ 首先/ ,/ 在/ 文件/ 的/ 数据/ 访问/ 中/ ,/ 对/ 文件/ 的/ 独占/ 访问/ 性能/ 达到/ 磁盘/ 95/ %/ 的/ 性能/ ,/ 而/ 在/ 并发/ 访问/ 负载/ 下/ ,/ 性能/ 基本/ 与/ 独占/ 访问/ 性能/ 持平/ ;/ 并且/ ,/ 在/ 设备/ 和/ 客户端/ 增加/ 时/ ,/ Redbud/ 系统/ 的/ 数据/ 和/ 元/ 数据/ 访问/ 性能/ 接近/ 线性/ 增长/ ./ 其次/ ,/ 在/ 典型/ 的/ 并行/ 文件系统/ 元/ 数据/ 操作/ 中/ ,/ Redbud/ 的/ 磁盘/ 访问/ 性能/ 相比/ 传统/ 的/ 文件系统/ 提高/ 20/ %/ 以上/ ;/ 在/ 元/ 数据/ 服务器/ 为/ IntelXeon1/ ./ 60GHzCPU/ ,/ 2048MB/ 内存/ 的/ 配置/ 下/ ,/ 元/ 数据服务/ 引入/ 的/ CPU/ 开销/ 维持/ 在/ 1/ %/ ~/ 6/ %/ 范围/ 中/ ,/ 体现/ 出较/ 好/ 的/ 元/ 数据/ 服务器/ 可/ 扩展/ 能力/ ./ 2/ 背景/ 与/ 相关/ 工作/ 与/ 传统/ 的/ 并行/ 文件系统/ 类似/ [/ 3/ -/ 5/ ]/ ,/ Redbud/ 主要/ 由/ 客户端/ (/ Client/ )/ 、/ 元/ 数据/ 服务器/ (/ MetaDataServer/ ,/ MDS/ )/ 和/ 存储/ 节点/ 三/ 部分/ 组成/ ./ 其中/ ,/ 客户端/ 为/ 上层/ 应用/ 提供/ 标准/ 的/ POSIX/ 语义/ 接口/ ,/ 是/ Redbud/ 文件系统/ 的/ 文件/ 访问/ 入口/ ./ 2.1/ Redbud/ 非对称/ 存储/ 结构/ Redbud/ 的/ 数据/ 存储/ 采用/ 非对称/ 结构/ ,/ 将/ 元/ 数据/ 和/ 文件/ 数据/ 分别/ 存储/ 在/ 不同/ 的/ 存储设备/ 中/ ./ 通过/ SAN/ 环境/ 的/ Zone/ 技术/ ,/ 元/ 数据/ 的/ 访问/ 被/ 隔离/ 在/ MDS/ 访问/ 域/ 内/ ,/ 避免/ 客户端/ 访问/ 保存/ 了/ 元/ 数据/ 的/ 磁盘/ 设备/ ./ 由于/ 依据/ 存储/ 内容/ 划分/ 存储设备/ ,/ 这种/ 非对称/ 结构/ 允许/ 对元/ 数据/ 存储/ 和/ 数据/ 存储/ 进行/ 不同/ 的/ 优化/ ,/ 比如/ 管理员/ 可以/ 把/ 海量/ 的/ 数据/ 存储/ 节点/ 在/ 物理/ 上/ 配置/ 为/ 廉价/ 的/ 存储介质/ ,/ 以/ 保证/ 文件系统/ 的/ 预算/ 和/ IO/ 带宽/ ;/ 元/ 数据/ 存储/ 节点/ 可/ 配置/ 为/ 延迟/ 低/ 的/ 存储介质/ 甚至/ 新型/ 的/ 固态/ 存储设备/ ,/ 以/ 满足/ 元/ 数据/ 请求/ 的/ 服务质量/ ./ Redbud/ 利用/ 元/ 数据/ 文件系统/ 保存/ 所有/ 元/ 数据/ ,/ 构建/ 全局/ 目录/ 结构/ ./ 进行/ 元/ 数据/ 操作/ 时/ ,/ MDS/ 解析/ 来自/ 客户端/ 的/ 元/ 数据/ 请求/ ,/ 并/ 转发/ 到/ 元/ 数据/ 文件系统/ ;/ 在/ 访问/ 文件/ 数据/ 时/ ,/ 客户端/ 采用/ 带外/ 的/ 方式/ 与/ MDS/ 交互/ ,/ 在/ 获取/ 文件/ 布局/ 信息/ (/ layout/ )/ 后/ ,/ 直接/ 访问/ 存储/ 文件/ 数据/ 的/ 存储设备/ ,/ 避免/ 了/ 单/ 服务器/ 成为/ 数据传输/ 瓶颈/ ./ 2.2/ 并行/ 文件系统/ 的/ 存储管理/ 机制/ 并行/ 文件系统/ 的/ 存储管理/ 机制/ 主要/ 分为/ 文件/ 布局/ 方法/ 、/ 数据分布/ 机制/ 和/ 元/ 数据检索/ 机制/ 三个/ 方面/ ./ 其中/ ,/ 文件/ 布局/ 方法/ 通常/ 与/ 存储/ 模式/ 相关/ ,/ 比如/ ,/ 基于/ 对象/ 存储设备/ 的/ 并行/ 文件系统/ 一般/ 利用/ 对象/ 存储/ 目标/ 器/ (/ objectstoragetarget/ )/ 提供/ 对象/ 的/ 分布/ 机制/ [/ 3/ -/ 4/ ]/ ,/ 我们/ 专注/ 于/ 基于/ SAN/ 环境/ 的/ 并行/ 文件系统/ 的/ 相关/ 工作/ :/ 文件/ 布局/ (/ layout/ )/ 方法/ ./ 用来/ 维护/ 文件/ 的/ 逻辑/ 位置/ 到/ 物理/ 位置/ 的/ 映射/ 关系/ ./ 目前/ ,/ 在/ 基于/ SAN/ 环境/ 的/ 并行/ 文件系统/ 中/ [/ 6/ -/ 7/ ]/ ,/ 文件/ 布局/ 都/ 采用/ 基于/ 块/ ①/ ②/ Page3/ (/ block/ )/ 的/ 方法/ ,/ 利用/ 多个/ 块/ 位图/ 检索/ 磁盘空间/ ./ GPFS/ 首先/ 设定/ 普通/ 块/ 的/ 大小/ 为/ 256KB/ ,/ 多个/ 小/ 文件/ 可以/ 被/ 填充/ 在/ 同一个/ 块/ 内/ ,/ 但是/ 由于/ 许多/ 科学计算/ 中/ 产生/ 的/ 文件/ 相当/ 大/ (/ TB/ 级/ )/ ,/ 导致/ 索引/ 块/ 的/ 元/ 数据/ 过大/ [/ 2/ ]/ ./ Frangipani/ 采用/ 两种/ 不同/ 的/ 块/ 大小/ ,/ 大/ 文件/ 使用/ 较大/ 的/ 块/ 存储/ ,/ 这种/ 方法/ 在/ 一定/ 的/ 程度/ 上/ 减少/ 了/ 元/ 数据/ 开销/ 较大/ 的/ 问题/ ,/ 但是/ 由于/ 难以预测/ 将来/ 产生/ 的/ 文件大小/ ,/ 这种/ 方法/ 可能/ 造成/ 大量/ 空间/ 的/ 损失/ [/ 6/ ]/ ./ Rstore/ 使用/ B/ +/ 树/ 检索/ 空闲/ 空间/ ,/ 满足/ 多种/ 空间/ 分配/ 的/ 需求/ (/ 见/ 3.2/ 节/ )/ ./ 数据分布/ 机制/ ./ 用来/ 将/ 数据分布/ 在/ 多个/ 存储设备/ 中/ ,/ 影响/ 着/ 系统/ IO/ 性能/ 和/ 可靠性/ ./ 比如/ ,/ 为了/ 利用/ 磁盘/ 的/ 并行/ 访问/ 性能/ ,/ GFS2/ 和/ CXFS/ ①/ 利用/ 逻辑/ 卷/ ,/ 在/ 块/ 级别/ 将/ 数据分布/ 在/ 多个/ 磁盘/ 设备/ 中/ ,/ 以/ 获得/ 较/ 高/ 的/ IO/ 聚合/ 带宽/ ,/ GPFS/ 、/ Zebra/ [/ 7/ ]/ 在/ 文件/ 级/ 实现/ 数据/ 条带/ 化/ 策略/ ,/ 允许/ 进行/ 数据/ 访问/ 的/ 负载/ 均衡/ ;/ 在/ 可靠性/ 方面/ ,/ 它们/ 通常/ 依靠/ 硬件/ RAID/ (/ RedundantArraysofIndependentDisks/ )/ 的/ 方式/ 实现/ 数据/ 的/ 冗余/ ./ 一些/ 基于/ 智能/ 设备/ 的/ 文件系统/ 提出/ 了/ 多/ 副本/ 的/ 策略/ ,/ 比如/ ,/ 通过/ 轮询/ 的/ 方式/ ,/ Google/ 文件系统/ [/ 5/ ]/ 将/ 文件/ chunk/ 分布/ 在/ 多个/ 数据/ 服务器/ 中/ ,/ Ceph/ [/ 4/ ]/ 则/ 允许/ 客户端/ 使用/ 文件名/ 生成/ hash/ 值/ ,/ 并/ 根据/ 该值/ 选择对象/ 存储设备/ ,/ 在/ 系统/ 默认/ 的/ 情况/ 下/ ,/ 它们/ 均/ 为/ 所有/ 文件/ 保存/ 3/ 个/ 副本/ ,/ 避免/ 副本/ 失效/ 带来/ 的/ 数据/ 丢失/ ./ 这图/ 1Rstore/ 体系结构/ Rstore/ 采用/ 分层/ 的/ 方法/ 提高/ 存储管理/ 的/ 可/ 扩展/ 能力/ ./ 存储设备/ 被/ 划分/ 为/ 多个/ 并行/ 分配/ 组/ (/ ParallelAllocateGroup/ ,/ PAG/ )/ ,/ 系统/ 运行/ 时/ ,/ 每个/ 在线/ 的/ PAG/ 些/ 分布/ 机制/ 在/ 系统/ 构建/ 之初/ 就/ 被/ 设定/ ,/ 无法/ 随着/ 需求/ 的/ 变化/ 灵活/ 更改/ ./ 然而/ ,/ 大型/ 数据中心/ 往往/ 存在/ 多种/ 复杂/ 应用/ [/ 8/ ]/ ,/ 比如/ ,/ 某些/ 计算/ 应用/ 要求/ 存储系统/ 提供/ 更/ 高/ 的/ IO/ 带宽/ ,/ 而/ 某些/ 关键/ 数据服务/ 则/ 希望/ 其/ 具有/ 更/ 高/ 的/ 可靠性/ ./ Rstore/ 提供/ 了/ 基于/ 策略/ 的/ 目录/ 级/ 文件/ 分布/ 机制/ ,/ 允许/ 为/ 应用/ 的/ 工作/ 目录/ 配置/ 灵活/ 的/ 数据分布/ ,/ 提高/ 了/ 系统/ 的/ 可扩展性/ 和/ 可管理性/ (/ 见/ 3.3/ 节/ )/ ./ 元/ 数据检索/ 机制/ ./ 用于/ 目录/ 结构/ 以及/ 文件/ 信息/ 的/ 索引/ ./ 目前/ ,/ 大多数/ 并行/ 文件系统/ 采用/ 直接/ 索引/ 的/ 方法/ ②/ ,/ 建立/ inode/ 号/ 和/ 目录/ entry/ 的/ 关系/ ,/ 在/ 进行/ 目录/ 遍/ 历时/ 查找/ 相应/ 的/ 元/ 数据/ 区域/ ,/ 然后/ 根据/ 目录/ 内容/ 继续/ 访问/ 其/ 子目录/ ./ 这种/ 方法/ 的/ 缺点/ 是/ 难以/ 在/ 遍历/ 目录/ 的/ 过程/ 中/ 充分/ 挖掘/ 访问/ 的/ 局部性/ [/ 9/ ]/ ./ Ceph/ 采用/ 类似/ 日志/ 文件系统/ 的/ 方法/ ,/ 任何/ 元/ 数据/ 被/ 顺序/ 地/ 写入/ 对象/ 存储设备/ 中/ ,/ 当有/ 元/ 数据/ 被/ 释放/ 时/ (/ 如/ 文件/ 删除/ )/ ,/ 后台/ 进行/ 空间/ 的/ 回收/ ,/ 这种/ 方法/ 提高/ 了/ 元/ 数据/ 写/ 性能/ ,/ 但是/ 由于/ 无法/ 保持/ 进程/ 的/ 局部性/ ,/ 读/ 操作/ 的/ 性能/ 将/ 受到/ 较大/ 影响/ ./ 在/ 元/ 数据/ 存储/ 中/ ,/ Rstore/ 使用/ B/ 树/ 目录/ 建立/ 元/ 数据/ 索引/ ,/ 利用/ 键值/ 维护/ 进程/ 和/ 目录/ 路径/ 访问/ 局部性/ (/ 见/ 3.4/ 节/ )/ ./ 3Rstore/ 设计/ 如图/ 1/ 所示/ ,/ 基于/ Redbud/ 非对称/ 的/ 存储/ 结构/ ,/ ①/ ②/ Page4/ 隶属于/ 一个/ MDS/ ,/ 由该/ MDS/ 负责/ 其/ 空间/ 管理/ ;/ 所有/ 文件系统/ 元/ 数据/ 被/ 存储/ 在/ 专用/ 的/ 元/ 数据/ PAG/ (/ Meta/ -/ PAG/ )/ 中/ ,/ 而/ 文件/ 数据/ 及其/ 副本/ 则/ 被/ 存储/ 在/ 数据/ PAG/ (/ Data/ -/ PAG/ )/ 中/ ./ 下层/ 是/ 空间/ 映射/ 层/ (/ Mappinglayer/ )/ ,/ 它/ 负责/ 将/ 文件/ 布局/ 的/ 逻辑/ 地址映射/ 到/ 物理地址/ ,/ 并/ 为/ PAG/ 构建/ 空间/ 的/ 索引/ 结构/ ,/ 实现/ 空闲/ 空间/ 的/ 检索/ 方法/ ,/ 满足/ 空间/ 并行/ 分配/ 和/ 回收/ 的/ 需求/ ./ 上层/ 是/ 语义/ 感知/ 层/ (/ Context/ -/ Awarelayer/ )/ ,/ 它/ 感知/ 文件/ 访问/ 的/ 上下文/ 语义/ 信息/ ,/ 利用/ 访问/ 局部性/ 为/ Meta/ -/ PAG/ 聚集/ inode/ 等/ 文件/ 元/ 数据/ 信息/ ,/ 实现/ 元/ 数据/ 索引/ 机制/ ,/ 并/ 利用/ 基于/ 策略/ 的/ 分布/ 机制/ 为/ 文件/ 数据/ 分配/ 存储空间/ ./ 3.1/ 文件/ 逻辑/ 地址映射/ 如图/ 1/ 所示/ ,/ Rstore/ 采用/ extent/ 的/ 方法/ 建立/ 文件/ 布局/ 索引/ ,/ 实现/ 文件/ 逻辑/ 地址/ 到/ 物理/ 空间/ 的/ 映射/ ,/ 每个/ extent/ 表示/ 为/ 一个/ 四元组/ 〈/ 文件/ 偏移/ (/ f/ _/ offset/ )/ ,/ PAG/ 起始/ 地址/ (/ p/ _/ offset/ )/ ,/ 长度/ (/ length/ )/ ,/ 〈/ PAG/ 号/ +/ 状态/ (/ status/ )/ 〉/ 〉/ ,/ 所有/ 的/ extent/ 存储/ 在/ Meta/ -/ PAG/ 中/ ./ Extent/ 的/ 状态/ 标识/ 表示/ 了/ 每个/ extent/ 在/ 磁盘/ 上/ 的/ 空间/ 占用/ 状态/ ,/ 主要/ 分为/ 3/ 种/ :/ normal/ 表示/ 普通/ 的/ 可/ 读写/ 文件/ 空间/ ,/ hole/ 用于/ 支持/ 文件/ 的/ 空洞/ ,/ 表示/ 未/ 分配/ 的/ 空间/ ,/ pre/ -/ allocated/ 表示/ 为/ 文件/ 持久/ 预/ 分配/ 的/ 空间/ ,/ 提高/ 文件/ 分配/ 的/ 连续性/ ./ 为了/ 提高/ 存储空间/ 效率/ ,/ 在/ extent/ 的/ 表示/ 中/ ,/ 块/ (/ block/ )/ 是/ 最小/ 的/ 数据/ 空间/ 单元/ ,/ 每个/ 块/ 大小/ 为/ 4KB/ ,/ 使用/ 32/ 位/ 的/ 块/ 号/ 记录/ Extent/ 在/ 存储设备/ 上/ 的/ 偏移/ 和/ 长度/ ,/ 每个/ extent/ 结构/ 大小/ 为/ 20/ 字节/ ./ 在/ 较坏/ 的/ 情况/ 下/ ,/ 如/ 文件系统/ 碎片/ 较/ 多/ 或者/ 文件/ 平均/ 大小/ 较/ 小/ 的/ 办公/ 环境/ 等/ ,/ 假设/ 平均/ 4/ 个/ 4KB/ 的/ 块/ 表示/ 一个/ 连续/ 的/ 物理/ 段/ ,/ 则/ extent/ 仅会/ 使用/ 约/ 文件/ 数据/ 大小/ 0.2/ %/ 的/ 内存空间/ ,/ 而/ DrewRoselli/ 等/ 人/ 在/ 2008/ 年/ 针对/ 网络/ 文件系统/ 的/ 非/ HPC/ 应用/ 统计/ 中/ 发现/ 文件/ 平均/ 大小/ 正在/ 逐年/ 增长/ [/ 10/ -/ 11/ ]/ ,/ 因此/ extent/ 的/ 表示/ 方法/ 具有/ 较/ 高/ 的/ 空间/ 效率/ ./ 如图/ 1/ 所示/ ,/ 客户端/ 为/ 访问/ 的/ 文件/ 建立/ layout/ 缓存/ ,/ 以/ 开发/ 文件/ 访问/ 的/ 局部性/ ./ 每次/ 在/ 获取/ layout/ 之前/ ,/ 客户端/ 首先/ 检查/ 缓存/ 中/ 是否/ 存在/ ./ 客户端/ 的/ 每次/ 文件/ 写/ 操作/ 可能/ 更新/ extent/ 的/ 状态/ ,/ 比如/ 对/ 一个/ 文件/ 的/ 预/ 分配/ 段/ 进行/ 写入/ 操作/ 时/ ,/ 需要/ 将/ extent/ 的/ 状态/ 从/ pre/ -/ allocated/ 更新/ 到/ normal/ ,/ 因此/ 需要/ 跟踪/ 写/ 操作/ 对/ extent/ 的/ 状态/ 更新/ ,/ 并/ 将/ 这些/ 更新/ 发送到/ MDS/ ,/ 以/ 确保/ 文件/ extent/ 的/ 一致性/ ./ 然而/ 如果/ 在/ 每次/ 写/ 页面/ 操作/ 就/ 更新/ extent/ 的/ 状态/ 会/ 引入/ 巨大/ 的/ 通信/ 开销/ ,/ 因此/ 客户端/ 以/ 页面/ 为/ 单位/ 建立/ extent/ 的/ 状态/ 索引/ ,/ 并/ 以/ 聚集/ (/ batch/ )/ 的/ 方法/ 将/ 多次/ 更新/ 一次性/ 发送到/ MDS/ ./ 为了/ 提高/ 缓存/ 中/ 的/ extent/ 状态/ 检索/ 效率/ ,/ 客户端/ 以基树/ (/ radixtree/ )/ 的/ 方法/ 记录/ 每个/ 页面/ 的/ 写/ 状态/ 变化/ ,/ 一旦/ 页面/ 因为/ 写入/ 而/ 改变/ ,/ 则/ 被/ 加入/ 该树中/ ./ 为了/ 保证/ extent/ 的/ 一致性/ ,/ 只有/ MDS/ 才/ 有/ 修改/ extent/ 状态/ 的/ 权限/ ,/ 因此/ ,/ 在/ 客户端/ 每次/ 写/ 操作/ 之前/ ,/ 如果/ 页面/ 对应/ 的/ extent/ 缓存/ 是/ 不可/ 写/ (/ hole/ )/ 或者/ 未写/ (/ pre/ -/ allocated/ )/ 状态/ ,/ 则/ 它/ 首先/ 检索/ 基树/ ,/ 查看/ 该/ 页面/ 是否/ 已经/ 被/ 修改/ 过/ ,/ 如果/ 已经/ 被/ 修改/ ,/ 则/ 表示/ 对应/ 的/ 内存/ 存储/ 了/ 最新/ 的/ 数据/ ./ 3.2/ 空间/ 检索/ 方法/ 空间/ 映射/ 层为/ 每个/ PAG/ 提供/ 两棵/ 独立/ 的/ B/ +/ 树/ 进行/ 空闲/ 空间/ 的/ 快速/ 索引/ ,/ 其中/ 一棵/ 是/ 偏移/ 树/ ,/ 它/ 采用/ 空闲/ 空间/ 的/ 起始/ 块/ 号/ (/ offset/ )/ 作为/ 键值/ ,/ 另/ 一棵/ 是/ 长度/ 树/ ,/ 它/ 使用/ 空闲/ 空间/ 的/ 长度/ (/ length/ )/ 作为/ 键值/ ./ Rstore/ 根据/ 不同/ 的/ 文件/ 空间/ 分配/ 请求/ ,/ 使用/ 不同/ 的/ B/ +/ 树/ 进行/ 空间/ 分配/ ,/ 比如/ ,/ 如果/ 进行/ 文件/ 的/ 扩展/ 写/ ,/ 为了/ 保持/ 文件/ 数据/ 在/ 磁盘/ 上/ 的/ 连续性/ ,/ 选择/ 第/ 一棵/ B/ +/ 树/ ,/ 搜索/ 距离/ 当前/ 文件/ 末尾/ 最近/ 的/ 空闲/ 空间/ 进行/ 分配/ ;/ 如果/ 需要/ 分配/ 大段/ 的/ 连续/ 空间/ ,/ 那么/ 使用/ 第二/ 棵/ B/ +/ 树/ 可以/ 迅速/ 找到/ 和/ 需要/ 的/ 长度/ 接近/ 的/ 一段/ 连续/ 空间/ ./ 两棵/ B/ +/ 树/ 都/ 索引/ 某个/ PAG/ 内/ 所有/ 的/ 空闲/ 空间/ ,/ 因此/ 每次/ 空间/ 分配/ 以后/ 需要/ 同步/ 两棵/ B/ +/ 树/ ,/ 以/ 保证/ 两棵树/ 的/ 一致性/ ./ 在/ 这/ 两棵/ B/ +/ 树中/ ,/ 每/ 段/ 连续/ 空闲/ 空间/ 通过/ 一个/ segment/ 来/ 表示/ ,/ 表示/ 为/ 〈/ 块/ 号/ (/ blockNumber/ )/ ,/ 长度/ (/ length/ )/ 〉/ ./ 图/ 2/ 所示/ 的/ B/ +/ 树/ 使用/ 块/ 号/ 作为/ 键值/ ,/ 每个/ B/ +/ 树/ 的/ 外部/ 节点/ 按/ 顺序/ 保存/ 所有/ 被/ 索引/ 的/ extent/ ;/ B/ +/ 树/ 的/ 每个/ 内部/ 节点/ 或者/ 外部/ 节点/ 是/ 磁盘/ Page5/ 的/ 一个/ 块/ ,/ 每个/ 块/ 都/ 用/ 一个/ 头部/ 结构/ 来/ 标明/ 此块/ 在/ 树/ 中/ 的/ 层数/ ,/ 并用/ 左块/ 号/ 和/ 右块/ 号/ 标明/ 左右/ 兄弟/ 节点/ ;/ 每个/ B/ +/ 树/ 的/ 内部/ 节点/ 由/ 键值/ (/ key/ )/ 和/ 相应/ 的/ 子/ 节点/ 的/ 块/ 号/ 组成/ ./ 如图/ 2/ 所示/ ,/ 根/ 节点/ 在/ 属于/ 内部/ 节点/ 的/ 时候/ ,/ 只/ 包含/ 两个/ 子/ 节点/ 〈/ key23/ ,/ block24/ 〉/ 和/ 〈/ key600/ ,/ block25/ 〉/ ;/ 为/ 查找/ 到/ start/ 为/ 733/ 的/ segment/ ,/ 依次/ 查找/ 了/ root/ 、/ block25/ 、/ block355/ ,/ 最终/ 到/ 〈/ start/ =/ 733/ ,/ length/ =/ 10/ 〉/ ./ Rstore/ 将/ B/ +/ 树/ 本身/ 存储/ 在/ PAG/ 中/ ,/ 其/ 占用/ 的/ 存储空间/ 也/ 从/ B/ +/ 树中/ 进行/ 分配/ ,/ 因此/ ,/ 在/ 某些/ 智能/ 存储设备/ 上/ 构建/ PAG/ 时/ ,/ 允许/ 其/ 自主/ 管理/ 空闲/ 空间/ ,/ 减少/ MDS/ 的/ 负载/ ,/ 提高/ 资源分配/ 的/ 灵活性/ ./ 然而/ ,/ 因为/ 分配/ 空间/ 会/ 改变/ B/ +/ 树/ 本身/ 的/ 状态/ ,/ 如果/ 通过/ B/ +/ 树/ 管理/ 自身/ 的/ 空间/ ,/ 可能/ 陷入/ 多层/ 循环/ ./ 比如/ ,/ 为/ 文件/ 分配/ 空间/ 可能/ 造成/ 节点/ 的/ 分裂/ ,/ 因此/ 需要/ 向/ B/ +/ 插入/ 一个/ 新/ 的/ segment/ ,/ 而/ 此时/ 正好/ 需要/ 额外/ 的/ 空间/ 供/ B/ +/ 树/ 使用/ ,/ 此时/ 一种/ 选择/ 是从/ 插入/ 的/ segment/ 分配/ 空间/ ,/ 但是/ 插入/ 的/ 索引/ 的/ 空间/ 可能/ 并/ 不够/ 用/ ,/ 产生矛盾/ ;/ 而/ 如果/ 再次/ 从/ B/ +/ 树/ 分配/ 空间/ 仍然/ 可能/ 出现/ 分裂/ ,/ 产生/ 新/ 的/ 节点/ 插入/ ,/ 陷入/ 循环/ 状态/ ./ 在/ Rstore/ 中/ ,/ PAG/ 使用/ 一个/ 专用/ 的/ 空闲/ 列表/ (/ free/ -/ list/ )/ 来/ 管理/ 两棵/ B/ +/ 树/ 的/ 存储空间/ ,/ free/ -/ list/ 所/ 占用/ 的/ 块/ 与/ 普通/ 空间/ 一样/ ,/ 由/ B/ +/ 树/ 进行/ 分配/ 和/ 释放/ ./ 假设/ 树/ 的/ 深度/ 表示/ 为/ level/ ,/ sizeof/ (/ free/ -/ list/ )/ 表示/ 列表/ 的/ 大小/ ,/ 那么/ 对/ 任一/ B/ +/ 树/ 进行/ 更新/ 操作/ 时/ :/ (/ 1/ )/ 如果/ 插入/ 一个/ 索引/ ,/ 则/ 只会/ 发生/ 节点/ 增加/ ./ 最/ 极端/ 的/ 情况/ 下/ ,/ B/ +/ 树/ 的/ 块/ 会/ 发生/ 分裂/ 而/ 整体/ 深度/ (/ level/ )/ 会/ 增加/ 1/ ,/ 所以/ 需要/ free/ -/ List/ 提供/ 额外/ 的/ Level/ +/ 1/ 个块/ ;/ 而/ 需要/ 同步/ 两棵/ B/ +/ 树/ ,/ 假设/ 另外/ 一棵/ B/ +/ 树/ 的/ 层数/ 为/ Level2/ ,/ 因此/ 在/ 插入/ 一个/ 节点/ 的/ 情况/ 下/ 必须/ 满足/ :/ Level/ +/ Level2/ +/ 2/ </ sizeof/ (/ free/ -/ list/ )/ ./ (/ 2/ )/ 如果/ 删除/ 一个/ 索引/ ,/ 只会/ 发生/ 节点/ 的/ 减少/ ./ 最/ 极端/ 的/ 情况/ 下/ B/ +/ 树/ 的/ 块/ 会/ 发生/ 合并/ ,/ 而视/ 整体/ 的/ Level/ 减少/ 1/ 层/ ,/ 所以/ 需要/ Free/ -/ list/ 提供/ 额外/ 的/ Level/ 个/ 空闲/ 块/ 满足/ 释放/ 需要/ ,/ 因此/ 必须/ 满足/ sizeof/ (/ free/ -/ list/ )/ </ 128/ -/ Level/ -/ Level2/ ./ 因此/ ,/ 在/ 每次/ 对/ B/ +/ 树/ 进行/ 更新/ 操作前/ ,/ PAG/ 首先/ 确保/ free/ -/ list/ 有/ 足够/ 的/ 空闲/ 块/ ,/ 供/ B/ +/ 树/ 增加/ 深度/ 时/ 使用/ ;/ 同时/ ,/ 确保/ free/ -/ list/ 有/ 足够/ 的/ 剩余/ 空间/ ,/ 当/ B/ +/ 树/ 降低/ 深度/ 时/ ,/ 释放/ 相应/ 的/ 存储空间/ ./ 如果/ free/ -/ list/ 不/ 满足要求/ 时/ ,/ 使用/ B/ +/ 树/ 进行/ 分配/ 或者/ 释放/ ,/ 由于/ 此时/ 分配/ 造成/ 的/ B/ +/ 树/ 状态/ 变化/ 虽然/ 会/ 影响/ free/ -/ list/ 的/ 上/ 下限/ ,/ 却/ 不会/ 引起/ 空间/ 分配/ 的/ 循环/ ./ 3.3/ 基于/ 策略/ 的/ 数据分布/ 机制/ Rstore/ 利用/ 多线程/ PAG/ 的/ 方法/ ,/ 从/ 外部/ 避免/ 对同/ 一棵树/ 操作/ 带来/ 的/ 瓶颈/ ,/ 比如/ ,/ 当有/ 多个/ 文件/ 分配/ 和/ 回收/ 操作/ 并发/ 进行/ 时/ ,/ 空间/ 的/ 分配/ 被/ 分布/ 在/ 不同/ 的/ PAG/ 中/ ,/ 满足/ 了/ 海量/ 存储系统/ 中/ 大量/ 客户端/ 并发/ 访问/ 的/ 需求/ ./ 另一方面/ ,/ Rstore/ 采用/ 基于/ 策略/ 的/ 文件/ 分布/ 机制/ ,/ 允许/ 用户/ 使用/ 配置文件/ 的/ 方法/ ,/ 指定/ 特定/ 目录/ 及其/ 子/ 文件/ 的/ 分布/ ,/ 比如/ ,/ 为了/ 满足/ 应用/ 的/ 性能需求/ ,/ 其/ 工作/ 目录/ 中/ 的/ 文件/ 可以/ 被/ 配置/ 为/ 条带/ 化/ ;/ 而/ 为了/ 提高/ 某个/ 目录/ 的/ 数据安全/ 保护/ 级别/ ,/ 可以/ 为/ 该/ 目录/ 选择/ 更/ 复杂/ 的/ 加密算法/ 等/ ./ 为了/ 减少/ 文件系统/ 操作/ 的/ 复杂度/ ,/ Rstore/ 提供/ 了/ 一系列/ 的/ 策略/ ,/ 以/ 隐藏/ 物理/ 硬件/ 的/ 复杂性/ ,/ 并/ 满足用户/ 的/ 各种/ 需求/ ,/ 目前/ Rstore/ 中/ 主要/ 实现/ 了/ 4/ 种/ 常见/ 的/ 数据管理/ 策略/ ,/ 如表/ 1/ 所示/ ./ 策略/ 名称/ 可靠性/ 策略/ (/ R/ )/ 文件/ 数据/ 以/ RAID/ 或者/ 副本/ 的/ 方式/ 冗余/ 性能/ 策略/ (/ P/ )/ 安全策略/ (/ S/ )/ 文件/ 数据/ 通过/ 可/ 选择/ 的/ 加密算法/ 进行/ 加密/ 分级管理/ (/ H/ )/ 根据/ 文件/ 的/ 各种/ 访问/ 属性/ ,/ 如/ size/ 等/ ,/ 文件/ 数据/ 用户/ 可以/ 通过/ 用户/ 工具/ 随意/ 地为/ 每个/ 目录/ 配置/ 不同/ 的/ 分布/ 策略/ ./ 在/ 每种/ 策略/ 中/ ,/ 语义/ 感知/ 层为/ 每个/ 文件/ 保留/ 一个/ 标志/ 分布/ 策略/ 的/ 扩展/ 属性/ 组/ ,/ 以/ 标识/ 当前/ 的/ 分布/ 信息/ ;/ 如果/ 是/ 目录/ 文件/ ,/ 则/ 其/ 所有/ 的/ 子/ 文件/ 继承/ 该/ 目录/ 的/ 分布/ 策略/ ./ 比如/ ,/ 在/ 典型/ 的/ 条带/ 化/ 性能/ 策略/ (/ RF/ -/ striping/ )/ 中/ ,/ 语义/ 感知/ 层为/ 文件/ 分配/ 三元组/ 〈/ AGG/ ,/ strip/ _/ width/ ,/ dup/ _/ alg/ 〉/ ,/ 其中/ ,/ AGG/ 表示/ 文件/ 允许/ 被/ 分配/ 到/ 哪些/ PAG/ 中/ ,/ strip/ _/ width/ 表示/ 文件/ 的/ 条带/ 化/ 宽度/ ,/ dup/ _/ alg/ 表示/ 副本/ 的/ 放置/ 策略/ ,/ 如/ 副本/ 的/ 位置/ 、/ 数量/ 等/ (/ 用于/ 负载/ 均衡/ )/ ./ 在/ 解析/ 性能/ 策略/ 时/ ,/ 语义/ 感知/ 层/ 对/ 大/ 文件/ 采用/ 文件/ 级/ 条带/ 化/ 的/ 方法/ ,/ 当/ 文件大小/ 超过/ 系统/ 预设/ 阀值/ 时/ ,/ 尽可能/ 地/ 将/ 大/ 文件/ 分割/ ,/ 并/ 存储/ 到/ AGG/ 指定/ 的/ PAG/ 中/ ;/ 同一个/ 进程/ 分配/ 的/ 小/ 文件/ ,/ 则/ 尽可能/ 地/ 存储/ 在/ 同一个/ PAG/ 中/ 的/ 磁盘/ 的/ 连续/ 位置/ ./ 如图/ 3/ 所示/ ,/ “/ // dir/ _/ 1/ ”/ 目录/ 中/ 的/ 文件/ 被/ 配置/ 分布/ 到/ PAG1/ 和/ PAG2/ 两个/ 分配/ 组中/ ,/ 请求/ 创建/ 文件/ foo/ _/ 1/ 时/ ,/ 语义/ 感知/ 层/ 首先/ 使该/ 文件/ 继承/ 其父/ 目录/ 的/ 资源/ 分布/ 策略/ ,/ 获取/ AGG/ 指定/ 的/ PAG/ 号/ ,/ 并/ 进行/ 排列/ ,/ 然后/ 根据/ 文件/ 的/ 请求/ 的/ offset/ 和/ inode/ 号/ 进行/ hash/ 产生/ 空间/ 的/ 分配/ 逻辑/ 号/ ,/ 最后/ 对/ 该/ 逻辑/ 号/ 进行/ 掩码/ 操作/ 决/ Page6/ 图/ 3/ 分层/ 资源管理/ 机/ 制定/ 其/ extent/ 分配/ 的/ PAG/ 号/ ./ 而/ 客户端/ 从/ MDS/ 获取/ 了/ layout/ 之后/ ,/ 为了/ 定位/ 文件/ 数据/ 的/ 物理/ 位置/ ,/ 将/ 首先/ 检查/ extent/ 所处/ 的/ PAG/ 号/ ,/ 然后/ 将/ 其/ 内部/ 偏移/ 转化/ 为/ 具体/ 设备/ 上/ 的/ 物理/ 位置/ ./ “/ // dir/ _/ 2/ ”/ 目录/ 中/ 的/ 文件/ 副本/ 则/ 被/ 分配/ 在/ 与/ PAG2/ 处于/ 不同/ 失效/ 域/ 的/ PAG3/ 中/ ,/ 如/ 两者/ 使用/ 不同/ 的/ 电源/ 、/ 不同/ 的/ 阵列/ 控制器/ 等/ ./ 在/ 我们/ 的/ 测试/ 中/ 使用/ 了/ 两种/ 典型/ 的/ 性能/ 策略/ (/ RF/ -/ striping/ 和/ RF/ -/ Roundrobin/ )/ ,/ 结果表明/ ,/ 这种/ 目录/ 级/ 的/ 数据分布/ 方法/ 为/ 用户/ 提供/ 了/ 更/ 多/ 的/ 选择/ 和/ 更/ 高/ 的/ 灵活性/ ./ 为了/ 减少/ MDS/ 的/ 负载/ ,/ 在/ 大部分/ 的/ 策略/ 中/ ,/ 额外/ 的/ 计算/ 工作/ 由/ 客户端/ 进行/ 处理/ ,/ 比如/ 在/ 可靠性/ 策略/ 中/ ,/ 文件/ 如果/ 以/ RAID/ -/ 5/ 的/ 方式/ 分布/ ,/ 则/ 客户端/ 总是/ 在/ 更新/ 文件/ 时/ 同时/ 更新/ 文件/ 校验/ 数据/ 块/ 的/ 信息/ ;/ 在/ 安全策略/ 中/ ,/ 客户端/ 则/ 根据/ 不同/ 的/ 加密/ 模式/ 进行/ 安全/ 元/ 数据文件/ 的/ 加解密/ ./ 而/ 分级管理/ 策略/ 要求/ 监视/ 文件/ 的/ 变化/ 过程/ ,/ 并/ 根据/ 迁移/ 的/ 策略/ 进行/ 具有/ 不同/ 性能/ 属性/ 的/ 存储介质/ 间/ 的/ 迁移/ ,/ 因此/ 与/ 其他/ 策略/ 不同/ 的/ 是/ ,/ 它/ 需要/ MDS/ 指定/ 一个/ 专用/ 的/ 客户端/ 进行/ 分级管理/ ,/ 以/ 避免/ 客户端/ 离线/ 带来/ 的/ 异常情况/ ./ 3.4/ 元/ 数据检索/ 机制/ Rstore/ 以/ B/ 树/ 构建/ 元/ 数据/ 索引/ ,/ 其中/ 树/ 节点/ 以块/ 号/ 为/ 键值/ ,/ 对应/ 于/ 文件系统/ 中/ 每个/ 目录/ 的/ ID/ 号/ ,/ 文件/ 的/ inode/ 和/ layout/ 被/ 存放/ 在/ 叶/ 节点/ ,/ 每/ 一个/ 文件/ 对应/ 一个/ ID/ 号/ ,/ 进行/ 路径/ 查找/ 时/ 从/ 根目录/ 开始/ 以/ 目录/ ID/ 逐级/ 解析/ ./ 采用/ B/ 树/ 构建/ 文件系统/ 的/ 传统/ 方法/ 中/ ,/ 通常/ 尽可能/ 地/ 将/ 目录/ 中/ 所有/ 子/ 文件/ 保存/ 在/ 磁盘/ 的/ 邻近/ 位置/ ,/ 如/ XFS/ 在/ 目录/ 项中/ 记录/ 文件名/ 和/ 用于/ 索引/ 文件/ 的/ inode/ 号/ [/ 11/ ]/ 、/ Reiserfs/ 将/ 目录/ 项/ 存储/ 在/ directory/ -/ item/ 中/ ,/ 采用/ item/ 号/ 索引/ 对应/ 文件/ 的/ inode/ 块/ 地址/ ①/ ,/ 这些/ 方法/ 开发/ 了/ 目录/ 中子/ 文件/ 的/ 存储/ 局部性/ ./ 在/ 元/ 数据/ 存储/ 的/ 构建/ 方法/ 中/ ,/ 仅仅/ 开发/ 目录/ 的/ 子/ 文件/ 存储/ 局部性/ 是/ 不够/ 的/ ,/ 这/ 是因为/ 子/ 文件/ 的/ inode/ 索引/ 与/ 其父/ 目录/ 的/ 索引/ 号/ 相差/ 较/ 远/ ,/ 因此/ 在/ 进行/ 目录/ lookup/ 操作/ 之后/ 需要/ 经过/ 较长/ 的/ 磁盘/ 定位/ 时间/ 才/ 可以/ 访问/ 其子/ 文件/ inode/ 与/ 内容/ ,/ 这种/ 问题/ 在/ 本地/ 文件系统/ 中/ 的/ 影响/ 较/ 小/ ,/ 但是/ 在/ Redbud/ 中/ ,/ 客户端/ 为了/ 访问/ 某个/ 文件/ ,/ 首先/ 需要/ 访问/ 元/ 数据/ 存储/ 的/ 元/ 数据文件/ 以/ 获取/ layout/ 信息/ ,/ 因此/ 长时间/ 磁盘/ 定位/ 延迟/ 将/ 对/ 元/ 数据/ 访问/ 效率/ 有着/ 重要/ 影响/ ./ 根据/ 元/ 数据文件/ 较/ 小/ 的/ 特点/ ,/ 元/ 数据/ 存储/ 的/ 构建/ 中/ 使用/ 了/ 目录/ 嵌入/ 机制/ 以/ 提高/ Meta/ -/ PAG/ 的/ 磁盘/ 访问/ 效率/ ./ 在/ 目录/ 嵌入/ 机制/ 中/ ,/ 进行/ 元/ 数据文件/ 空间/ 分配/ 时/ 尽可能/ 地/ 将/ 各种/ 相关/ 的/ 元/ 数据/ 信息/ 存放/ 在/ 一起/ ,/ 如同/ 一个/ 目录/ 或/ 同一个/ 进程/ 相关/ 的/ 文件/ inode/ 和/ layout/ 信息/ ./ 创建/ 文件/ 时/ ,/ 文件/ inode/ 被/ 嵌入/ 目录/ 的/ 内容/ 中/ ,/ 每个/ inode/ 占据/ 一个/ 块/ 大小/ ,/ 如果/ 是子/ 文件/ ,/ 则/ 当/ 文件/ 进行/ 空间/ 分配/ 时/ layout/ 被/ 存储/ 在/ inode/ 块/ 中/ ,/ 由于/ 元/ 数据文件/ 较/ 小/ ,/ 绝大部分/ 的/ layout/ 将/ 存储/ 在/ inode/ 的/ 末尾/ ,/ 对于/ 较为/ 极端/ 的/ 碎片/ 情况/ ,/ 文件大小/ 超过/ 一个/ 块/ 大/ 小时/ ,/ 新/ 的/ 块/ 将/ 被/ 分配/ 在/ 目录/ 块/ 的/ 预/ 分配/ 位置/ ./ 因此/ ,/ 客户端/ 进行/ 元/ 数据/ 访问/ 时/ ,/ 即使/ 采用/ 了/ 操作/ 聚合/ 的/ 方法/ [/ 12/ ]/ ,/ 绝大部分/ 情况/ 下/ 只/ 需要/ 一次/ IO/ 和/ 网络/ 交互/ 即可/ 预取/ 所有/ 需要/ 的/ 相关/ 元/ 数据/ 信息/ ,/ 而/ 存在/ 多个/ 并发/ IO/ 操作/ 时/ (/ 如/ 多个/ 客户端/ 共享/ 访问/ 同一个/ 目录/ )/ ,/ 期间/ 仅/ 存在/ 极少/ 的/ 磁头/ 定位/ 时间/ ./ 元/ 数据/ 存储/ 以/ 〈/ 父/ 目录/ ID/ ,/ 进程/ ID/ 的/ hash/ 值/ ,/ ①/ ReiserfsforLinux/ ./ http/ :/ // // rfsd/ ./ sourceforge/ ./ net/ // resources/ ./ Page7/ 类型/ ,/ 文件名/ ID/ 的/ hash/ 值/ 〉/ 四元组/ 表示/ 目录/ inode/ 的/ 索引/ 键值/ ,/ 在/ 查找/ 某个/ 目录/ 时/ 可以/ 通过/ 该/ 键值/ 找到/ 对应/ 的/ inode/ 块/ 号/ ,/ 其中/ 父/ 目录/ ID/ 是/ 其父/ 目录索引/ 键值/ 中/ 的/ 一部分/ ,/ 用以/ 聚集/ 某个/ 目录/ 的/ 子/ 文件/ ,/ 进程/ ID/ 是/ 一个/ 与/ 客户端/ ID/ 相关联/ 的/ ID/ 号/ ,/ 在/ 键值/ 中/ 加入/ 进程/ ID/ 的/ hash/ 值则/ 可以/ 聚集/ 同一/ 进程/ 的/ 所有/ 相关/ 文件/ ,/ 文件名/ ID/ 的/ hash/ 值/ 用于/ 大/ 目录/ 的/ 索引/ 支持/ ,/ 类型/ 用于/ 区分/ 文件/ mode/ ./ 图/ 4/ 表示/ Redbud/ 文件系统/ 的/ 树形/ 结构/ ,/ 其中/ p/ 表示/ 树中/ 的/ 块/ 指针/ ,/ inode/ 集合/ 表示/ 聚集/ 在/ 同一个/ 目录/ 的/ 子/ 文件/ inode/ 及其/ layout/ 内容/ ,/ 元/ 数据/ 存储/ 在/ 为/ “/ // dir/ // foo1/ ”/ 分配/ inode/ 节点/ 时/ ,/ 首先/ 在/ 根目录/ (/ “/ // ”/ 节点/ )/ 的/ 目录/ 内容/ 中/ 查找/ 并/ 获取/ 其/ 直接/ 父/ 目录/ “/ // dir/ ”/ 的/ inode/ 结构/ ID/ 号/ (/ 图/ 4/ 中/ 假设/ 为/ 3/ )/ ,/ 以该/ ID/ 为/ 索引/ 键值/ 重新/ 遍历/ 文件系统/ 树/ 并/ 找到/ 父/ 目录/ 图/ 4/ 元/ 数据/ 存储/ 树形/ 结构/ Rstore/ 通过/ 扩展/ 哈希/ 的/ 方法/ 支持/ 大/ 目录/ ,/ 当/ 目录/ 项数/ 超过/ 10k/ 时/ ,/ 目录/ 的/ 内容/ 中/ 包含/ 子/ 文件/ 的/ 键值/ 和/ 文件名/ ,/ 以/ 文件名/ 为/ 关键字/ 产生/ 哈希/ 值/ ,/ 并/ 通过/ 二级/ 间接/ 块/ 的/ 方法/ 存储/ ,/ 使用/ 链表/ 的/ 方法/ 处理/ 哈希/ 冲突/ ./ 指向/ 二级/ 间接/ 块/ 的/ 指针/ 以/ 类似/ inode/ -/ tail/ 的/ 机制/ 存储/ 在/ 目录/ inode/ 块/ 的/ 末尾/ ,/ 以/ 指针/ 的/ 存储空间/ 大小/ 为/ 2KB/ ,/ 每个/ 指针/ 为/ 4/ 字节/ 计算/ ,/ 每个/ 目录/ 可以/ 支持/ 几亿/ 个子/ 文件/ 或/ 子目录/ 的/ 快速/ 搜索/ ./ 4/ 测试/ 与/ 评估/ 4.1/ 测试/ 介绍/ 我们/ 测试/ 使用/ 的/ MDS/ 和/ 客户端/ 均/ 采用/ 双核/ IntelXeon1/ ./ 60GHzCPU/ ,/ 2048MB/ 内存/ ,/ 每台/ 机器配置/ 千兆/ 以太网卡/ 及/ Qlogic2432/ 光纤卡/ ;/ 各个/ 节点/ 之间/ 通过/ catalyst3750/ 千兆/ 以太网交换机/ 相连/ ,/ MDS/ 及/ 客户端/ 通过/ silkworm3800FC/ 交换机/ 单个/ 的/ inode/ 块/ ,/ 然后/ 使用/ 进程/ ID/ 进行/ hash/ 得到/ 其/ inode/ 结构/ ID/ 的/ 第二/ 部分/ (/ 为/ 18/ )/ ,/ 并/ 为/ 该/ inode/ 分配/ 一个/ 新块/ ,/ 以/ 目录/ 嵌入/ 的/ 方法/ 存储/ 在/ 目录/ 的/ 内容/ 中/ ,/ 当/ 客户端/ 对/ foo1/ 文件/ 进行/ 写/ 操作/ 时/ ,/ 新/ 分配/ 的/ layout/ 记入/ 该/ inode/ 的/ 末尾/ ,/ 至此/ ,/ foo1/ 的/ inode/ 以及/ layout/ 与/ 其父/ 目录/ 存放/ 在/ 连续/ 的/ 磁盘/ 块/ 中/ ;/ 当该/ 进程/ 再次/ 在/ 该/ 目录/ 中/ 分配/ inode/ 结构/ 时/ (/ 如图/ 中/ foo2/ 文件/ )/ ,/ 由于/ 其父/ 目录/ ID/ 与/ foo1/ 一样/ ,/ 进程/ ID/ 通过/ hash/ 计算/ 又/ 产生/ 相同/ 的/ key/ 值/ ,/ 该/ inode/ 将/ 被/ 存储/ 在/ foo1/ 相邻/ 的/ 磁盘/ 位置/ ./ 对于/ 来自/ 不同/ 进程/ 但/ 处于/ 同一/ 目录/ 下/ 的/ 文件/ inode/ ,/ 由于/ 它们/ 的/ key/ 值中父/ 目录/ ID/ 项/ 相同/ ,/ 因此/ 在/ 磁盘/ 上/ 处于/ 邻近/ 位置/ ,/ 通过/ 较少/ 的/ 磁头/ 定位/ 时间/ 即可/ 遍历/ ./ 为/ 新/ 的/ 子目录/ 分配/ 空间/ 时/ ,/ 元/ 数据/ 存储/ 为/ 其/ 进行/ 持久/ 预/ 分配/ 8/ 个块/ 用于/ 将来/ 子/ 文件/ 的/ inode/ 存储/ ./ 大小/ 为/ 146GB/ 的/ 磁盘/ 组成/ 的/ FC/ 磁盘阵列/ 相连/ ./ 在/ 测试/ 之前/ ,/ 我们/ 先对/ Redbud/ 文件系统/ 使用/ 的/ 磁盘/ 进行/ 了/ 读写/ 测试/ 作为/ 性能/ 比较/ 依据/ ,/ 在/ Linux/ -/ 2.6/ ./ 27/ 内核/ 基础/ 上/ ,/ 采用/ 标准/ 的/ 磁盘/ 设备/ 读写访问/ 接口/ 测/ 得/ 性能/ 峰值/ 为/ 读/ 为/ 70.2/ MB/ // s/ ,/ 写为/ 74.3/ MB/ // s/ ./ 4.2/ 数据/ 块/ 索引/ 效率/ 我们/ 首先/ 比较/ Redbud/ 、/ EXT3/ 和/ NFS/ [/ 12/ ]/ 三种/ 文件系统/ 的/ 性能/ ,/ 以/ 验证/ Redbud/ 采用/ extent/ 的/ 方法/ 进行/ 数据/ 块/ 索引/ 的/ 效率/ ./ 在/ 本/ 实验/ 中/ 部署/ 于/ NFS/ 服务器端/ 的/ 文件系统/ 为/ EXT3/ ①/ ,/ Redbud/ 配置/ 一个/ 客户端/ 访问/ 文件/ ./ 我们/ 在/ 前端/ 节点/ 上/ 运行/ 文件系统/ 性能/ 的/ 专用/ 测试工具/ IOzone/ ②/ ,/ 文件/ 读写访问/ 的/ 吞吐/ 率/ 结果/ 如图/ 5/ 所示/ ./ ①/ ②/ Page8/ 图/ 5Redbud/ 、/ EXT3/ 、/ NFS/ 读写/ 文件/ 的/ 吞吐/ 率/ 我们/ 首先/ 测试/ 了/ 在/ 配置/ 一个/ 硬盘/ 的/ 情况/ 下/ ,/ 顺序/ 读写/ 和/ 随机/ 读写/ 的/ 性能/ ,/ 其中/ 随机/ 读写/ 为/ 512KB/ 的/ 大块/ 读写/ ./ 从图/ 5/ 可见/ ,/ Redbud/ 文件系统/ 顺序/ 写/ (/ seqwrite/ )/ 和/ 随机/ 写/ (/ randomwrite/ )/ 访问/ 的/ 吞吐/ 率则/ 分别/ 比/ EXT3/ 高出/ 20/ %/ 和/ 14/ %/ ,/ 这/ 是因为/ 写/ 过程/ 中/ Redbud/ 客户端/ 采用/ layout/ 缓存/ 机制/ ,/ 根据/ 文件/ 的/ 访问/ 特征/ 缓存/ 了/ MDS/ 预/ 分配/ 的/ layout/ ,/ 经过/ 缓存/ 查找/ 获取/ PAG/ 的/ 磁盘/ 写/ 地址/ ;/ EXT3/ 则/ 首先/ 查找文件/ 相应/ 组中/ bitmap/ 并/ 从中/ 分配/ 磁盘/ 块/ ,/ 即使/ 其/ 采用/ 了/ 固定/ 8/ 个块/ 大小/ 的/ 预/ 分配机制/ ,/ 但/ 在/ 写访问/ 过程/ 中/ 仍然/ 进行/ 频繁/ 的/ 文件/ 元/ 数据/ 更新/ ./ Redbud/ 顺序/ 读/ (/ seqread/ )/ 和/ 随机/ 读/ (/ randomread/ )/ 文件/ 的/ 吞吐/ 率/ 略高于/ 本地/ 文件系统/ EXT3/ ,/ 这/ 是因为/ 在/ 该/ 文件/ 的/ 创建/ 过程/ 中/ 空间/ 映射/ 层为/ 整个/ 文件/ 分配/ 了/ 连续/ 空间/ ,/ 而读/ 过程/ 中/ Redbud/ 客户端/ 尽管/ 需要/ 与/ MDS/ 交互/ 获取/ 文件/ 映射/ 信息/ ,/ 但是/ 由于/ 操作/ 聚合/ 的/ 作用/ ,/ 一次/ 交互/ 即可/ 获取/ 并/ 缓存/ 文件/ 所有/ layout/ ;/ 相比之下/ ,/ EXT3/ 在/ 空闲/ 空间/ 分配/ 过程中将/ 大/ 文件/ 的/ 不同/ 部分/ 分布/ 到/ 不同/ 的/ 组/ (/ group/ )/ 中/ ,/ 因此/ 增加/ 了/ 读/ 访问/ 的/ 磁盘/ 定位/ 时间/ ./ 这些/ 测试/ 结果显示/ ,/ Redbud/ 的/ 访问/ 性能/ 达到/ 了/ 磁盘/ 峰值/ 的/ 95/ %/ ,/ 证明/ Redbud/ 文件系统/ 采用/ extent/ 的/ 数据/ 块/ 索引/ 方法/ 能/ 将/ 物理/ 设备/ 的/ 性能/ 几乎/ 完全/ 提供/ 给/ 上层/ 应用/ ./ 从图/ 6/ 还/ 可以/ 看出/ Redbud/ 读写/ 文件/ 的/ 吞吐/ 率/ 比/ NFS/ 高出/ 65/ %/ 和/ 75/ %/ ,/ 这/ 是因为/ NFS/ 客户端/ 频繁/ 地/ 与/ NFS/ 服务器进行/ 文件/ 页面/ 的/ 获取/ 和/ 提交/ 交互/ ,/ 如/ readpage/ 、/ writepage/ 、/ commit/ 等/ 操作/ ,/ 所有/ 文件/ 数据/ 经过/ NFS/ 服务器/ ,/ 最后/ 由/ 其/ 将/ IO/ 请求/ 转发/ 至/ 本地/ 文件系统/ ,/ 这些/ 耗时/ 的/ 操作/ 流程/ 均/ 限制/ 了/ NFS/ 的/ 吞吐量/ 和/ 可/ 扩展/ 能力/ ./ 4.3/ 数据/ 访问/ 可扩展性/ 我们/ 通过/ 测试/ 系统/ 规模/ 增大/ 的/ 情况/ 下/ Redbud/ 文件系统/ 聚合/ 带宽/ 的/ 变化/ ,/ 以/ 验证/ 其/ 数据/ 访问/ 的/ 可/ 扩展/ 能力/ ./ 首先/ 在/ IO/ 节点/ 规模/ 不变/ 的/ 情况/ 下为/ Redbud/ 增加/ 客户端/ 的/ 个数/ ,/ 并/ 进行/ 文件/ 的/ 创建/ 写/ (/ write/ -/ after/ -/ create/ )/ 、/ 覆盖/ 写/ (/ overwrite/ )/ 和/ 读/ (/ read/ )/ 操作/ ,/ 系统/ 聚合/ 带宽/ 统计/ 结果/ 如图/ 6/ 所示/ ./ 结果显示/ ,/ 在/ Redbud/ 文件系统/ 中/ ,/ 应用程序/ 进行/ 读写操作/ 时/ ,/ 聚合/ 吞吐量/ 随着/ 客户端/ 节点/ 个数/ 的/ 增加/ 而/ 线性/ 地/ 增长/ ;/ 其中/ 创建/ 写/ 操作/ 的/ 性能/ 略低于/ 其他/ 操作/ ,/ 这/ 是因为/ 尽管/ 客户端/ 与/ MDS/ 的/ 交互/ 过程/ 中/ 采用/ 了/ 预/ 分配/ 方法/ 减少/ 客户端/ 从/ MDS/ 获取/ layout/ 的/ 操作/ 次数/ ,/ 但/ 空间/ 映射/ 层/ 分配/ 空闲/ 空间/ 时/ 进行/ B/ +/ 树/ 的/ 更新/ 操作/ ,/ 这些/ 元/ 数据/ 的/ 更新/ 先/ 被/ 写入/ 共享/ 日志/ 区/ ,/ 然后/ 响应/ 客户端/ ,/ 而/ 在/ 其他/ 操作/ 中/ ,/ 客户端/ 可以/ 直接/ 获取/ 文件/ layout/ ./ 图/ 7/ 是/ 在/ 客户端/ 个数/ 不变/ 的/ 情况/ 下/ 增加/ 存储设备/ 的/ 系统/ 聚合/ 带宽/ 测试/ 结果/ ,/ 在/ 该/ 测试/ 中/ 我们/ 将/ Redbud/ 的/ 测试/ 目录/ 配置/ 为/ RF/ -/ striping/ 和/ RF/ -/ Roundrobin/ 两种/ 性能/ 策略/ ,/ 在/ RF/ -/ striping/ 策略/ 中/ ,/ MDS/ 在/ 为/ 该/ 目录/ 的/ 子/ 文件/ 分配/ 空间/ 时/ ,/ 根据/ 文件/ id/ 和/ PAG/ 号/ 以/ fchunk/ (/ 100MB/ )/ 为/ 单位/ 分割/ 文件/ ,/ 将/ 每个/ 文件/ 的/ 不同/ 部分/ 分布/ 在/ 不同/ 的/ 存储设备/ 中/ ;/ 而/ RF/ -/ Roundrobin/ 则/ 表示/ MDS/ 以/ 文件/ 为/ 粒度/ ,/ 将/ 整个/ 文件/ 分布/ 在/ 同一个/ PAG/ 中/ ,/ 查找/ 空闲/ PAG/ 时/ 采用/ Roundrobin/ 的/ 方法/ ./ 从图/ 7/ 中/ 可以/ 看出/ ,/ 在/ 采用/ 两种/ 不同/ 的/ 文件/ 存储空间/ 分配/ 方法/ 时/ ,/ 系统/ 的/ 吞吐/ Page9/ 量/ 均/ 随/ IO/ 节点/ 的/ 增加/ 呈/ 线性/ 增长/ ./ 相比/ 图/ 5/ 的/ 性能/ 结果/ ,/ 在/ 系统/ 规模/ 扩展/ 能力/ 测试/ 中/ ,/ 性能/ 的/ 增长/ 并/ 不/ 呈现/ 成倍增长/ ,/ 这/ 是因为/ 多个/ 客户端/ 并行/ 访问/ 时/ 增加/ 了/ 磁头/ 的/ 定位/ 时间/ ,/ 带来/ 了/ 一定/ 的/ 性能/ 损失/ ./ 由于/ MDS/ 集中管理/ 各种/ 元/ 数据/ 的/ 状态/ ,/ 在/ 客户端/ 的/ 访问/ 过程/ 中/ 对/ layout/ 进行/ 检索/ 、/ 分发/ 等/ 处理/ ,/ 因此/ MDS/ 的/ CPU/ 负载/ 程度/ 直接/ 影响/ 着/ 系统/ 的/ 可扩展性/ ./ 接下来/ ,/ 我们/ 检验/ 在/ 客户端/ 节点/ 增加/ 的/ 情况/ 下/ 各种/ 不同/ 的/ 应用/ 对/ MDS/ 的/ 访问/ 负载/ 的/ 影响/ ./ 图/ 8/ (/ a/ )/ 显示/ 了/ 运行/ IOzone/ 时/ ,/ MDS/ 的/ CPU/ 负载/ 结果/ ./ 在/ 本/ 实验/ 中/ ,/ 每个/ 客户端/ 上/ 运行/ 5/ 个/ IOzone/ 测试程序/ 对/ 结果/ 文件/ 分别/ 进行/ 读写访问/ ,/ 以/ 增加/ MDS/ 的/ 访问/ 压力/ ./ 我们/ 可以/ 观察/ 到/ ,/ MDS/ 负载/ 情况/ 随着/ 客户端/ 的/ 增加/ 而/ 增加/ ,/ 这/ 是因为/ 逐渐/ 增多/ 的/ 文件/ layout/ 访问/ 请求/ 导致/ MDS/ 的/ 任务/ 增加/ ./ 写/ 操作/ 的/ 负载/ 略/ 高/ 是因为/ MDS/ 要/ 进行/ extent/ 的/ 状态/ 转换/ ./ 为了/ 提供/ 更/ 多/ 的/ 文件/ 访问/ 模式/ ,/ 接下来/ 我们/ 运行/ 两个/ 实际/ 的/ IO/ 密集型/ 应用/ ,/ 高性能/ 计算机/ 的/ 性能/ 评估/ 程序/ (/ NASParallelBenchmarks/ ①/ 和/ 一个/ 典型/ 的/ 建筑/ 网格/ 计算/ 程序/ Abaqus/ ②/ ,/ 运行/ 过程/ 中/ ,/ 两种/ 测试程序/ 均/ 对/ 结果/ 文件/ 进行/ 随机/ 和/ 顺序/ 访问/ ./ 图/ 8/ (/ b/ )/ 显示/ 了/ 随着/ 应用/ 客户端/ 节点/ 扩展/ ,/ MDS/ 的/ CPU/ 负载/ 结果/ ,/ 在/ 实验/ 中/ ,/ 我们/ 在/ 每个/ 客户端/ 上/ 独立/ 地/ 运行/ 一个/ 计算/ 程序/ ,/ 其中/ 1.00/ 和/ 0.05/ 表示/ Abaqus/ 两种/ 具有/ 不同/ 精确度/ 的/ 计算/ 用例/ ,/ 精度/ 越小/ 表示/ 磁盘/ 数据/ 访问量/ 越多/ ./ 我们/ 可以/ 观察/ 到/ ,/ 虽然/ 节点/ 的/ 数目/ 增加/ ,/ 但是/ MDS/ 的/ CPU/ 利用率/ 控制/ 在/ 1.6/ %/ 内/ ,/ 具有/ 较强/ 的/ 可/ 扩展/ 能力/ ./ 我们/ 接下来/ 为/ Redbud/ 文件系统/ 配置/ 了/ 多个/ 客户端/ 对/ 不同/ 文件/ 和/ 同一个/ 文件/ 的/ 不同/ 部分/ 访问/ ,/ 每个/ 被/ 测试/ 的/ 文件大小/ 为/ 50GB/ ,/ 所有/ 被/ 访问/ 的/ 文件/ 以/ RF/ -/ striping/ (/ fchunk/ =/ 1MB/ )/ 方式/ 存放/ 在/ 10/ 个/ IO/ 节点/ 中/ ./ 图/ 9/ 显示/ 了/ 不同/ 的/ 并发/ 访问/ 模式/ 下/ 的/ 测试/ 结果/ ./ 其中/ concurrent/ 模式/ 表示/ 多个/ 节点/ 访问/ 同一个/ 文件/ ,/ 与/ 当前/ 许多/ 并行程序/ 一样/ [/ 13/ ]/ ,/ 客户端/ 将/ 已经/ 创建/ 好/ 的/ 文件/ 分为/ 多个/ 连续/ 的/ 区间/ ,/ 每个/ 节点/ 首先/ 分别/ 给/ 其中/ 一个/ 区间/ 加上/ Linux/ 字节/ 锁/ ,/ 然后/ 再/ 进行/ 读写访问/ ./ 为了/ 增加/ 并发/ 访问/ 压力/ ,/ 每隔/ 1min/ 各个/ 客户端/ 节点/ 交换/ 访问/ 区间/ ./ exclusive/ 模式/ 则/ 表示/ 表示/ 多个/ 客户端/ 节点/ 访问/ 不同/ 的/ 文件/ ,/ 节点/ 之间/ 不/ 存在/ 同一/ 文件/ 的/ 并发/ 访问/ ./ 从图/ 9/ 中/ 可以/ 看出/ 当/ 并发/ 访问/ 的/ 结点/ 个数/ 增长/ 时/ ,/ 并发/ 访问/ 与/ 独占/ 访问/ 的/ 性能/ 基本/ 持平/ ./ 少量/ 的/ 性能/ 损失/ 主要/ 来自/ 于/ 不同/ 客户端/ 节点/ 对/ 同一/ 磁盘/ 访问/ 时/ ,/ 磁头/ 访问/ 定位/ 时间/ 增长/ ./ 同时/ 我们/ 能/ 观察/ 到/ 在/ 客户端/ 节点/ 增加/ 的/ 过程/ 中/ ,/ CPU/ 的/ 平均/ 最高/ 使用率/ 略有/ 增加/ ,/ 其/ 原因/ 是/ 随着/ 并发/ 度/ 的/ 增加/ ,/ MDS/ 需要/ 更新/ 和/ 跟踪/ layout/ 的/ 状态/ 和/ 使用/ 情况/ ,/ 但是/ CPU/ 的/ 利用率/ 仍然/ 维持/ 在/ 非常/ 小/ 的/ 范围/ 内/ (/ 2/ %/ )/ ./ 4.4/ 元/ 数据/ 访问/ 性能/ 在/ Redbud/ 中/ ,/ 与/ 本地/ 文件系统/ 类似/ ,/ 文件/ 布局/ 组成/ 每个/ 用户/ 可视/ 文件/ 的/ 元/ 数据文件/ ,/ 并/ 以/ B/ 树/ 的/ 方式/ 检索/ ./ 本/ 实验/ 中/ ,/ 我们/ 对比/ 采用/ 本地/ 文件系统/ 存储/ 元/ 数据文件/ 和/ 我们/ 专用/ 的/ 元/ 数据/ 存储/ 在/ 典型/ 的/ layout/ 工作/ 负载/ 下/ 的/ 访问/ 性能/ ./ 为了/ 避免/ 网络/ 延迟/ ①/ ②/ Page10/ 带来/ 的/ 干扰/ ,/ 我们/ 首先/ 将/ 元/ 数据/ 存储/ 的/ 代码/ 单独/ 抽取/ 出来/ 进行/ 测试/ ./ 图/ 10/ 显示/ 了/ 元/ 数据/ 存储/ 的/ 测试/ 结果/ ,/ 其中/ MFS/ (/ metafilesystem/ )/ 表示/ RedbudMeta/ -/ PAG/ 的/ 访问/ 结果/ ./ 在/ 本/ 实验/ 中/ ,/ makefile/ 工作/ 负载/ 递归/ 地/ 创建/ 一个/ 5000/ 层/ 的/ 目录/ ,/ 每个/ 目录/ 里面/ 有/ 500/ 个/ 文件/ ,/ 并且/ 所有/ 的/ 文件/ 在/ 创建/ 之后/ 进行/ 缀/ 后写/ 操作/ (/ append/ -/ write/ )/ ;/ readlayout/ 则/ 首先/ 对/ 目录/ 进行/ readdir/ 操作/ ,/ 然后/ 读/ 刚才/ 创建/ 的/ 文件/ 内容/ ./ 为了/ 避免/ 缓存/ 的/ 效果/ ,/ 我们/ 在/ 创建/ 文件/ 之后/ 清空/ 了/ 缓存/ ./ 在/ PostMark/ 的/ 测试/ 中/ ,/ 文件/ 个数/ 设置/ 为/ 100k/ 个/ ,/ 事务/ 数目/ 为/ 500k/ 次/ ,/ 文件/ 的/ 大小/ 与/ 每次/ 事务/ 大小/ 一样/ ./ 在/ makefile/ 和/ readlayout/ 的/ 工作/ 负载/ 中/ ,/ 由于/ 采用/ 了/ 嵌入/ 目录/ 机制/ ,/ write/ -/ after/ -/ create/ 和/ read/ -/ after/ -/ readdir/ 操作/ 一个/ 小/ 文件/ 只会/ 引入/ 一次/ 磁盘/ 访问/ ,/ 因此/ 在/ 文件/ 的/ 大小/ 不/ 大于/ 4KB/ 时/ ,/ MFS/ 相比/ 其他/ 文件系统/ 的/ 访问/ 性能/ 提高/ 出/ 20/ %/ 以上/ ,/ 而/ 在/ 文件/ 大于/ 4KB/ 时/ ,/ 由于/ 新/ 的/ 块/ 也/ 被/ 分配/ 在/ inode/ 附近/ ,/ 因此/ 与/ 其他/ 文件系统/ 的/ 性能/ 相当/ ./ 在/ PostMark/ 测试/ 中/ ,/ 由于/ Redbud/ 采用/ 扩展/ hash/ 的/ 方法/ 支持/ 大/ 目录/ ,/ 因此/ 在/ PostMark/ 随机/ 选择/ 文件/ 进行/ 访问/ 时/ ,/ 相比/ 其他/ 两个/ 采用/ 线性/ 检索/ 的/ 文件系统/ ,/ 提高/ 了/ 总体/ 的/ 访问/ 性能/ ./ 随后/ 我们/ 通过/ 计算/ 测试用例/ 的/ 完成/ 时间/ 检验/ 元/ 数据/ 操作/ 的/ 性能/ ,/ 我们/ 在/ 客户端/ 上/ 嵌套/ 地/ 创建/ 1000/ 个/ 子目录/ ,/ 在/ 每个/ 子目录/ 中/ 创建/ 100/ 个/ 文件/ ,/ 并/ 分别/ 进行/ readdir/ 、/ stat/ 操作/ ,/ 最后/ 将/ 不同/ 操作/ 的/ 完成/ 时间/ 与/ 导出/ ext3/ 的/ NFS/ 第四/ 版本/ 对比/ ,/ 其中/ 在/ 进行/ 了/ 创建/ 操作/ 之后/ 首先/ 清空/ 文件系统/ 缓存/ 再/ 进行/ 后续/ 的/ 访问/ ,/ 以/ 避免/ 更新/ 操作/ 产生/ 的/ 缓存/ 对/ 读/ 访问/ 的/ 影响/ ./ 测试/ 结果/ 如图/ 11/ 所示/ ,/ Redbud/ 的/ 子/ 文件/ 和/ 目录/ 创建/ 完成/ 时间/ 是/ NFS/ 的/ 81/ %/ 左右/ ,/ 这/ 是因为/ 文件/ 的/ 创建/ 过程/ 中/ 文件系统/ 首先/ 需要/ 查找/ (/ lookup/ )/ 对应/ 文件/ 是否/ 存在/ ,/ 即使/ NFS/ 允许/ 其/ 客户端/ 缓存/ 所有/ 目录/ 项元/ 数据/ ,/ 但/ 由于/ 其/ 缓存/ 的/ 弱/ 一致性/ ,/ 这类/ lookup/ 操作/ 必须/ 被/ 发送到/ NFS/ 服务器/ ,/ 而/ 在/ Redbud/ 的/ 文件创建/ 过程/ 中/ ,/ 客户端/ 通过/ 回调/ 更新/ 机制/ ,/ 客户端/ 缓存/ 了/ 目录/ 的/ 所有/ 子目录/ 项/ ,/ 并/ 具有/ 修改/ 权限/ ,/ 因此/ 能/ 确保/ 目录/ 内容/ 与/ MDS/ 一致/ ,/ 对/ 新创建/ 文件/ 的/ lookup/ 操作/ 均/ 在/ 本地/ 缓存/ 中/ 进行/ ./ 尽管/ Redbud/ 在/ 其/ 通信协议/ 中/ 聚合/ 了/ readdir/ -/ stat/ 操作/ 对/ ,/ 但是/ 从图/ 11/ 中/ 我们/ 发现/ 其/ 性能/ 仍然/ 较/ NFS/ 高/ ,/ 这/ 是因为/ NFS/ 读/ 目录/ 的/ 过程/ 中其/ 客户端/ 与/ 服务器/ 的/ 每次/ 交互/ 仅仅/ 传输/ 一个/ 页面/ 的/ 数据/ ,/ 当/ 目录/ 较大/ 时/ ,/ 为了/ 取得/ 所有/ 目录/ 项/ ,/ NFS/ 需要/ 多次/ 交互/ ;/ 而/ 在/ Redbud/ 协议/ 中/ 避免/ 了/ 这种/ 问题/ ,/ 而且/ 元/ 数据/ 存储/ 将/ 文件属性/ 信息/ 聚集/ 在/ 目录/ 内容/ 中/ ,/ 通过/ 较少/ 的/ 磁盘/ 定位/ 时间/ 即可/ 将/ 文件属性/ 信息/ 读入/ MDS/ 缓存/ 中/ ./ Redbud/ 的/ stat/ 操作/ 完成/ 时间/ 为/ NFS/ 的/ 24/ %/ 左右/ ,/ 这/ 是因为/ Redbud/ 的/ 客户端/ 在/ 前述/ readdir/ 操作/ 获取/ 了/ 目录/ 中子/ 文件/ 的/ 属性/ 信息/ 之后/ ,/ 除了/ 一些/ 验证/ 消息/ 外/ ,/ 所有/ 的/ getattr/ 查找/ 操作/ 均/ 在/ 本地/ 缓存/ 中/ 进行/ ./ 如/ 2.1/ 节/ 所述/ ,/ Redbud/ 的/ 非对称/ 结构/ 分离/ 了/ 元/ 数据/ 和/ 数据/ 的/ 存储/ ,/ 并/ 因此/ 避免/ 大块/ 的/ 数据/ 访问/ 对元/ 数据/ 访问/ 的/ 干扰/ ./ 我们/ 接下来/ 验证/ 在/ 有/ 数据/ 访问/ 干扰/ 时/ ,/ Redbud/ 的/ 元/ 数据/ 访问/ 性能/ ./ 图/ 12/ 显示/ 了/ Redbud/ 和/ NFS/ 的/ 比较/ 结果/ ./ 在/ 本/ 实验/ 中/ ,/ 客户端/ 创建/ 5000/ 个/ 递归/ 的/ 目录/ ,/ 每个/ 目录/ 里面/ 有/ 50/ 个子/ 文件/ ,/ 然后/ 其他/ 客户端/ 进行/ utime/ 和/ stat/ 操作/ ./ 在/ 每次/ 运行/ 中/ ,/ 一半/ 客户端/ 上/ 运行/ 5/ 个/ IOzone/ 程序/ 用于/ 实现/ 大块/ 数据/ 访问/ ,/ 其他/ 的/ 客户端/ 进行/ 元/ 数据/ 访问/ ./ 从图/ 中/ 我们/ 可以/ 观察/ 到/ Redbud/ 的/ 聚合/ 元/ 数据/ 访问/ 性能/ 随着/ 客户端/ 的/ 增加/ 而/ 增长/ ./ 同时/ ,/ 虽然/ 客户端/ 在/ 进行/ 数据/ 操作/ 时/ 与/ MDS/ 交互进行/ 大量/ 的/ layout/ 操作/ ,/ 但是/ Redbud/ 通过/ 减少/ 元/ 数据/ 存储/ 的/ 磁盘/ 定位/ 和/ 检索/ 时间/ ,/ 比/ NFS/ 的/ 元/ 数据/ 访问/ 性能/ 提高/ 了/ 8/ %/ 以上/ (/ 10/ 个/ 节点/ 的/ 情况/ 下/ 获得/ 的/ 提高/ 比例/ 最少/ )/ ./ 图/ 12/ 中/ stat/ 操作/ 的/ 性能/ 结果/ 也/ 证明/ 了/ 由于/ 元/ 数据/ 服务器/ 的/ 缓存/ 避免/ 了/ 磁盘操作/ ,/ 因此/ 极大地提高/ 元/ 数据/ 访问/ 性能/ ./ Page115/ 小结/ 本文/ 提出/ 了/ 在/ 基于/ 共享/ 磁盘/ 的/ 并行/ 文件系统/ Redbud/ 中/ 分层/ 的/ 资源管理/ 机制/ ,/ 正是/ 由于/ 采用/ 非对称/ 的/ 结构/ ,/ 分离/ 元/ 数据/ 和/ 数据/ 的/ 存储/ ,/ 因此/ 存储/ 资源管理/ 机制/ 可以/ 针对/ 不同/ 的/ 存储/ 需求/ 进行/ 优化/ ,/ 提供/ 了/ 高/ 可/ 扩展/ 能力/ ./ 资源管理/ 机制/ 将/ 文件/ 分布/ 的/ 语义/ 和/ 空间/ 的/ 管理/ 分离/ 在/ 不同/ 的/ 级别/ ,/ 在/ 语义/ 感知/ 层/ ,/ 它/ 专注/ 于/ 文件/ 的/ 分布/ 策略/ ,/ 开发/ 元/ 数据/ 访问/ 的/ 局部性/ ,/ 并/ 提供/ 基于/ 策略/ 的/ 文件/ 级/ 分布/ 方法/ ,/ 满足/ 当前/ 大规模/ 数据中心/ 多种/ 应用/ 的/ 需求/ ./ 在/ 空间/ 映射/ 层/ ,/ Redbud/ 专注/ 于/ 空间/ 分配/ 策略/ ,/ 为了/ 提高/ 资源/ 的/ 检索/ 效率/ ,/ 采用/ 不同/ 的/ 树型/ 结构/ 检索/ 文件/ 的/ 空间/ 分配/ 、/ 文件/ 布局/ 等/ ./ 评价/ 结果显示/ ,/ 这种/ 资源管理/ 机制/ 能/ 有效/ 地/ 适应/ 各种/ 访问/ 模式/ ,/ 在/ 客户端/ 节点/ 和/ 存储/ 节点/ 增长/ 时/ ,/ 性能/ 同时/ 获得/ 扩展/ ,/ 并/ 具有/ 较/ 高/ 的/ 灵活性/ ./ 

