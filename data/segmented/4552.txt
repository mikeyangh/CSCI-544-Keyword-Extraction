Page1/ 基于/ 差异/ 合并/ 的/ 分布式/ 随机/ 梯度/ 下降/ 算法/ 1/ )/ (/ 中国科学院计算技术研究所/ 网络/ 数据/ 科学/ 与/ 技术/ 重点/ 实验室/ 北京/ 100190/ )/ 2/ )/ (/ 中国科学院/ 大学/ 北京/ 100190/ )/ 摘要/ 大规模/ 随机/ 梯度/ 下降/ 算法/ 是/ 近年来/ 的/ 热点/ 研究/ 问题/ ,/ 提高/ 其/ 收敛/ 速度/ 和/ 性能/ 具有/ 重要/ 的/ 应用/ 价值/ ./ 大规模/ 随机/ 梯度/ 下降/ 算法/ 可以/ 分为/ 数据/ 并行/ 和/ 模型/ 并行/ 两大类/ ./ 在/ 数据/ 并行算法/ 中/ ,/ 模型/ 合并/ 是/ 一种/ 比较/ 常用/ 的/ 策略/ ./ 目前/ ,/ 基于/ 模型/ 合并/ 的/ 随机/ 梯度/ 下降/ 算法/ 普遍/ 采用/ 平均/ 加权/ 方式/ 进行/ 合并/ ,/ 虽然/ 取得/ 了/ 不错/ 的/ 效果/ ,/ 但是/ ,/ 这种/ 方式/ 忽略/ 了/ 参与/ 合并/ 的/ 模型/ 的/ 内在/ 差异性/ ,/ 最终/ 导致/ 算法/ 收敛/ 速度慢/ ,/ 模型/ 的/ 性能/ 及/ 稳定性/ 较差/ ./ 针对/ 上述/ 问题/ ,/ 该文/ 在/ 分布式/ 场景/ 下/ ,/ 提出/ 了/ 基于/ 模型/ 差异/ 进行/ 合并/ 的/ 策略/ ,/ 差异性/ 主要/ 体现/ 在/ 两/ 方面/ ,/ 各/ 模型/ 在/ 其/ 训练/ 数据/ 上/ 错误率/ 的/ 差异/ 和/ 训练/ 不同/ 阶段/ 模型/ 合并/ 策略/ 的/ 差异/ ./ 此外/ ,/ 该文/ 对/ 合并/ 后/ 的/ 模型/ 采用/ 规范化/ 技术/ ,/ 将/ 其/ 投射/ 到/ 与/ 合并/ 前/ 模型/ Frobenius/ 范数/ 相同/ 的/ 球体/ 上/ ,/ 提高/ 了/ 模型/ 的/ 收敛/ 性能/ ./ 作者/ 在/ Epsilon/ 、/ RCV1/ -/ v2/ 和/ URL3/ 个/ 数据/ 集上/ ,/ 验证/ 了/ 提出/ 的/ 基于/ 差异/ 合并/ 的/ 分布式/ 随机/ 梯度/ 下降/ 算法/ 相对/ 于/ 平均/ 加权/ 方式/ 具有/ 收敛/ 速度/ 更/ 快/ 、/ 模型/ 性能/ 更好/ 的/ 性质/ ./ 关键词/ 分布式/ ;/ 随机/ 梯度/ 下降/ ;/ 规范化/ ;/ 模型/ 合并/ ;/ 社交/ 网络/ ;/ 社会/ 计算/ 1/ 引言/ 机器/ 学习/ 算法/ 中/ 的/ 随机/ 梯度/ 下降/ 算法/ 由于/ 使用/ 简单/ 、/ 收敛/ 速度/ 快/ 、/ 效果/ 可靠/ 等/ 优点/ 得到/ 了/ 普遍/ 应用/ ,/ 但是/ 该/ 算法/ 需要/ 在/ 训练/ 数据/ 上/ 不断/ 迭代/ ,/ 遍历/ 多遍/ 数据/ ,/ 这/ 在/ 数据/ 规模/ 较大/ 以及/ 单台/ 机器/ 计算能力/ 有限/ 的/ 情况/ 下/ ,/ 算法/ 执行/ 效率/ 往往/ 比较/ 低/ ./ 在/ 大/ 数据/ 背景/ 下/ ,/ 分布式/ 随机/ 梯度/ 下降/ 算法/ 得到/ 了/ 广泛/ 的/ 研究/ ,/ 提高/ 其/ 收敛/ 速度/ 和/ 性能/ 具有/ 重要/ 的/ 应用/ 价值/ ./ 根据/ 文献/ [/ 1/ ]/ ,/ 我们/ 可以/ 把/ 当前/ 的/ 大规模/ 随机/ 梯度/ 下降/ 算法/ 分为/ 两类/ :/ 数据/ 并行/ 和/ 模型/ 并行/ ./ 在/ 数据/ 并行算法/ 研究/ 中/ ,/ 之前/ 的/ 工作/ [/ 2/ -/ 6/ ]/ 将/ 数据/ 随机/ 划分/ 分布/ 到/ 不同/ 机器/ 上/ ,/ 通过/ 在/ 多个/ 机器/ 上/ 同时/ 独立/ 地/ 执行/ 随机/ 梯度/ 下降/ 算法/ ,/ 将/ 得到/ 的/ 多个/ 模型/ 进行/ 合并/ 作为/ 最终/ 的/ 模型/ 或者/ 下/ 一次/ 迭代/ 的/ 初始模型/ ,/ 这种/ 策略/ 取得/ 了/ 很/ 好/ 的/ 效果/ ./ 目前/ ,/ 基于/ 模型/ 合并/ 的/ 分布式/ 随机/ 梯度/ 下降/ 算法/ 普遍/ 采用/ 平均/ 加权/ 方式/ ,/ 平等/ 对待/ 每台/ 机器/ 上/ 得到/ 的/ 模型/ ./ 这样/ 的/ 合并/ 方式/ 存在/ 以下/ 两个/ 缺点/ :/ 第一/ ,/ 采用/ 平均/ 加权/ 的/ 方式/ ,/ 忽略/ 了/ 各/ 模型/ 内在/ 的/ 差异性/ ,/ 合并/ 得到/ 的/ 模型/ 不能/ 很/ 好/ 地/ 反映/ 全局/ 数据/ 间/ 的/ 差异/ 特点/ ,/ 导致/ 学习/ 收敛/ 速度/ 变慢/ ;/ 第二/ ,/ 将/ 合并/ 得到/ 的/ 模型/ 作为/ 下/ 一轮/ 迭代/ 计算/ 的/ 初始模型/ 时/ ,/ 从/ 每台/ 机器/ 的/ 角度/ 来看/ ,/ 相当于/ 在/ 模型/ 空间/ 中/ 根据/ 自己/ 当前/ 的/ 模型/ 和/ 远程/ 机器/ 上/ 的/ 模型/ 搜索/ 下/ 一个/ 更好/ 的/ 点/ ,/ 直接/ 平均/ 加权/ 合并/ 得到/ 的/ 模型/ 相对/ 本地/ 模型/ 变化/ 比较/ 大/ ,/ 一定/ 程度/ 上/ 影响/ 了/ 模型/ 性能/ ./ 虽然/ 文献/ [/ 2/ ]/ 尝试/ 基于/ 模型/ 错误率/ 进行/ 合并/ ,/ 但/ 其/ 实验/ 结果表明/ 基于/ 模型/ 错误率/ 的/ 合并/ 策略/ 和/ 平均/ 加权/ 合并/ 的/ 策略/ 效果/ 基本/ 没有/ 差别/ ./ 针对/ 上述/ 问题/ ,/ 本文/ 提出/ 了/ 一种/ 基于/ 差异/ 合并/ 的/ 分布式/ 随机/ 梯度/ 下降/ 算法/ DA/ -/ DSGD/ (/ DistributedStochasticGradientDescentwithDiscriminativeAggregating/ )/ ,/ 通过/ 两种/ 策略/ 来/ 提升/ 分布式/ 随机/ 梯度/ 下降/ 算法/ 的/ 收敛/ 速度/ 和/ 模型/ 性能/ :/ (/ 1/ )/ 基于/ 性能/ 的/ 加权/ 合并/ ./ 与/ 平均/ 加权/ 方式/ 和/ 简单/ 的/ 基于/ 模型/ 错误率/ 进行/ 合并/ 的/ 策略/ 不同/ ,/ 参与/ 合并/ 的/ 各个/ 模型/ 的/ 权重/ 综合/ 考虑/ 了/ 其/ 在/ 所在/ 机器/ 上/ 已/ 使用/ 训练/ 数据/ 的/ 误差/ 以及/ 整个/ 学习/ 过程/ 的/ 进度/ ./ 特别/ 是/ 随着/ 学习/ 过程/ 的/ 推进/ ,/ 给予/ 性能/ 较/ 好/ 的/ 远程/ 模型/ 更/ 高/ 的/ 权重/ ,/ 从而/ 能/ 更好/ 地/ 捕捉/ 全局/ 数据/ 的/ 特点/ ,/ 保证/ 模型/ 的/ 稳定性/ ;/ (/ 2/ )/ 合并/ 模型/ 的/ 规范化/ ./ 对于/ 加权/ 合并/ 后/ 的/ 模型/ ,/ 我们/ 使用/ 规范化/ 技术/ 使得/ 合并/ 后/ 的/ 模型/ 与/ 本地/ 模型/ 的/ Frobenius/ 范数/ 相同/ ,/ 即/ 两个/ 模型/ 处于/ 模型/ 空间/ 的/ 同一个/ 球体/ 表面/ 上/ ,/ 模型/ 合并/ 相当于/ 利用/ 全局/ 信息/ 修正/ 了/ 本地/ 模型/ 的/ 方向/ ,/ 从而/ 提高/ 本地/ 模型/ 的/ 性能/ ./ 我们/ 在/ 分布式/ 环境/ 下/ ,/ 通过/ 实验/ 验证/ 了/ 上述/ 合并/ 策略/ 的/ 有效性/ ,/ 使用/ 上述/ 合并/ 策略/ 的/ 算法/ 相对/ 平均/ 加权/ 收敛/ 速度/ 更快/ 更/ 稳定/ ,/ 而且/ 算法/ 能够/ 达到/ 更优/ 的/ 性能/ ./ 本文/ 第/ 2/ 节/ 介绍/ 相关/ 工作/ ;/ 第/ 3/ 节/ 介绍/ 本文/ 使用/ 的/ Pegasos/ 学习/ 算法/ 以及/ 分布式/ 通信/ 框架/ ;/ 第/ 4/ 节/ 详细/ 阐述/ 本文/ 提出/ 的/ 基于/ 差异/ 合并/ 的/ 分布式/ 随机/ 梯度/ 下降/ 算法/ DA/ -/ DSGD/ ;/ 第/ 5/ 节/ 通过/ 实验/ 验证/ 本文/ 提出/ 的/ 算法/ 的/ 有效性/ ;/ 第/ 6/ 节对/ 本文/ 进行/ 总结/ 并/ 讨论/ 下/ 一步/ 研究/ 的/ 方向/ ./ 2/ 相关/ 工作/ 分布式/ 随机/ 梯度/ 下降/ 算法/ 是/ 近几年来/ 的/ 一个/ 热点/ 研究/ 问题/ ./ 本文/ 关注/ 基于/ 模型/ 合并/ 的/ 分布式/ 随机/ 梯度/ 下降/ 算法/ ,/ 由于/ 篇幅/ 限制/ ,/ 我们/ 并/ 不/ 详细/ 介绍/ 共享内存/ 多核/ 并行/ 下/ 的/ 随机/ 梯度/ 下降/ 算法/ [/ 7/ -/ 9/ ]/ 和/ 基于/ 参数/ 服务器/ 的/ 算法/ [/ 1/ ,/ 9/ -/ 12/ ]/ 等/ 相关/ 工作/ ./ 基于/ 模型/ 合并/ 是/ 处理/ 大规模/ 数据/ 的/ 一种/ 常用/ 策略/ ,/ 特别/ 是/ 在/ 完全/ 分布式/ 的/ 环境/ 下/ ,/ 例如/ p2p/ 网络/ 中/ ,/ 大部分/ 算法/ 普遍/ 采用/ 模型/ 合并/ 的/ 方式/ 进行/ 训练/ [/ 13/ -/ 14/ ]/ ./ 文献/ [/ 3/ ]/ 在/ MapReduce/ 和/ Bigtable/ 框架/ 下/ 验证/ 了/ DGD/ (/ DistributedGradientDescent/ )/ 、/ IPM/ (/ IterativeParameterMixtures/ )/ 和/ DAT/ (/ DistributedAsyn/ -/ chronousTraining/ )/ 等/ 算法/ 的/ 性能/ ./ DGD/ 算法/ 在/ Map/ 阶段/ ,/ 每个/ 机器/ 获取/ 最新/ 模型/ ,/ 然后/ 计算/ 各自/ 机器/ 上/ 数据/ 的/ 梯度/ ,/ Reduce/ 阶段/ 将/ Map/ 阶段/ 得到/ 的/ 梯度/ 加/ 到/ 一起/ 更新/ 模型/ ,/ 重复/ 多次/ 直到/ 模型/ 收敛/ ./ 与/ DGD/ 不同/ ,/ IPM/ 算法/ [/ 2/ -/ 3/ ]/ 在/ Map/ 阶段/ 各/ 机器/ 独立/ 地/ 执行/ 一遍/ 随机/ 梯度/ 下降/ ,/ Reduce/ 阶段/ 将/ Map/ 阶/ Page3/ 段/ 得到/ 的/ 模型/ 进行/ 平均/ 加权/ 合并/ ,/ 得到/ 下/ 一轮/ 迭代/ 的/ 初始模型/ ./ DGD/ 和/ IPM/ 算法/ 在/ 每/ 一轮/ 迭代/ 后/ 需要/ 同步/ 更新/ 全局/ 模型/ ,/ 为/ 避免/ 同步/ 带来/ 的/ 额外/ 时间/ 开销/ ,/ DAT/ 算法/ 在/ 计算/ 的/ 时候/ 每个/ 机器/ 在/ 每/ 一轮/ 迭代/ 开始/ 时/ 异步/ 地/ 获取/ 全局/ 模型/ ,/ 然后/ 在/ 本地/ 机器/ 上/ 独立/ 地/ 运行/ 随机/ 梯度/ 下降/ 算法/ ,/ 并用/ 每/ 一轮/ 结束/ 时/ 得到/ 的/ 模型/ 与/ 这/ 一轮/ 初始模型/ 的/ 差/ 对/ 全局/ 模型/ 进行/ 异步/ 地/ 更新/ ./ DAT/ 是/ 一种/ 简单/ 的/ 基于/ 参数/ 服务器/ 实现/ 的/ 分布式/ 随机/ 梯度/ 下降/ 算法/ ./ DAT/ 相对/ DGD/ 和/ IPM/ 虽然/ 节省/ 了/ 同步/ 的/ 时间/ 开销/ ,/ 但是/ DAT/ 算法/ 中/ 每个/ 机器/ 上/ 的/ 模型/ 不是/ 最新/ 的/ 模型/ ,/ 导致/ 异步/ 更新/ 时/ 使用/ 的/ 梯度/ 不能/ 很/ 好/ 的/ 反应/ 当前/ 全局/ 模型/ 的/ 梯度/ ,/ 每/ 一轮/ 迭代/ 的/ 收敛/ 速度/ 相对/ 较慢/ [/ 11/ ]/ ./ 由于/ 网络/ 延迟/ 以及/ 同步/ 所/ 需/ 的/ 额外/ 开销/ ,/ MapReduce/ 计算/ 框架/ 并/ 不/ 太/ 适合/ 随机/ 梯度/ 下降/ 这种/ 需要/ 在/ 数据/ 上/ 迭代/ 多次/ 、/ 顺序/ 更新/ 模型/ 的/ 算法/ ./ 文献/ [/ 5/ ]/ 提出/ 的/ PSGD/ (/ ParallelStochasticGradientDescent/ )/ 算法/ 与/ IPM/ 算法/ 类似/ ,/ 但/ PSGD/ 算法/ 只有/ 一轮/ 的/ MapReduce/ ,/ 在/ Map/ 阶段/ ,/ 每个/ 机器/ 独立/ 地/ 执行/ 完整/ 的/ 随机/ 梯度/ 下降/ 算法/ ,/ 直到/ 模型/ 收敛/ ;/ 在/ Reduce/ 阶段/ ,/ 对/ 每个/ 机器/ 上/ 的/ 模型/ 进行/ 平均/ 加权/ 合并/ ,/ 得到/ 最终/ 的/ 模型/ ./ PSGD/ 相对/ IPM/ 算法/ 大大/ 地/ 降低/ 了/ 机器间/ 的/ 通信/ 开销/ ,/ 但是/ 由于/ 每台/ 机器/ 都/ 只/ 使用/ 本地/ 的/ 数据/ ,/ 训练/ 过程/ 没/ 能够/ 利用/ 全局/ 数据/ 信息/ 提高/ 本地/ 模型/ 的/ 性能/ ./ 介于/ IPM/ 和/ PSGD/ 之间/ ,/ 文献/ [/ 15/ ]/ 提出/ 的/ BM/ -/ DSGD/ (/ DistributedStochasticGradientDescentwithButterflyMixing/ )/ 算法/ 采用/ 蝴蝶/ 状/ 通信/ 方式/ ,/ 去除/ 参数/ 服务器/ 中心/ 节点/ ,/ 每轮/ 迭代/ 中/ 各个/ 节点/ 独立/ 地/ 执行/ 随机/ 梯度/ 下降/ 算法/ ,/ 然后/ 将/ 模型/ 仅/ 发送给/ 下/ 一个/ 通信/ 节点/ ,/ 同时/ 每个/ 节点/ 平均/ 加权/ 合并/ 本地/ 模型/ 与/ 接收/ 到/ 的/ 模型/ ,/ 这种/ 方式/ 较/ PSGD/ 加强/ 了/ 全局/ 数据/ 在/ 训练/ 过程/ 中/ 对于/ 本地/ 模型/ 的/ 作用/ ,/ 蝴蝶型/ 的/ 通信/ 方式/ 与/ IPM/ 相比/ 又/ 降低/ 了/ 通信/ 代价/ ,/ 因此/ 具有/ 良好/ 的/ 性能/ ,/ 在/ 分布式/ 随机/ 梯度/ 方法/ 的/ 研究/ 中/ 成为/ 一个/ 热点/ ./ 其/ 具体/ 通信/ 机制/ 及/ 优点/ 将/ 在/ 3.2/ 小节/ 中/ 进行/ 详细/ 介绍/ ./ 综上所述/ ,/ 据/ 我们/ 目前/ 调研/ 的/ 结果/ ,/ 大部分/ 分布式/ 梯度/ 下降/ 算法/ 在/ 做/ 模型/ 合并/ 时/ 都/ 只是/ 简单/ 地/ 进行/ 平均/ 加权/ ,/ 忽略/ 了/ 模型/ 之间/ 的/ 差异性/ ,/ 而且/ 平均/ 加权/ 合并/ 得到/ 的/ 模型/ 在/ 模型/ 空间/ 中/ 的/ 分布/ 范围/ 较大/ ,/ 可能/ 会/ 影响/ 算法/ 收敛/ 的/ 速度/ 和/ 性能/ ./ 本文/ 以/ BM/ -/ DSGD/ 算法/ 的/ 通信/ 框架/ 为/ 基础/ ,/ 在/ 进行/ 模型/ 合并/ 时/ 利用/ 各个/ 模型/ 在/ 其/ 机器/ 上/ 数据/ 的/ 性能/ ,/ 同时/ 考虑/ 整个/ 学习/ 算法/ 的/ 进度/ 对模型/ 进行/ 加权/ 合并/ ./ 另外/ ,/ 通过/ 限制/ 结果/ 模型/ 在/ 模型/ 空间/ 的/ 分布/ 范围/ ,/ 很/ 好/ 地/ 提高/ 了/ 模型/ 的/ 收敛/ 速度/ 和/ 性能/ ./ 3/ 基本/ 学习/ 算法/ 与/ 通信/ 机制/ 本文/ 采用/ Pegasos/ [/ 16/ ]/ 作为/ 每个/ 节点/ 上/ 的/ 基本/ 学习/ 算法/ ,/ 因为/ Pegasos/ 是/ 一种/ 使用/ 随机/ 梯度/ 下降/ 算法/ 求解/ 支持/ 向量/ 机/ (/ SupportVectorMachine/ )/ 原始/ 问题/ 的/ 知名/ 算法/ ,/ 而/ 支持/ 向量/ 机是/ 被/ 广泛/ 使用/ 和/ 研究/ 的/ 机器/ 学习/ 算法/ ./ 本文/ 提出/ 的/ DA/ -/ DSGD/ 算法/ 执行/ 时/ 各/ 机器/ 节点/ 间/ 的/ 通信/ 计算/ 采用/ 与/ BM/ -/ DSGD/ [/ 15/ ]/ 算法/ 相同/ 的/ 方式/ ./ 本节/ 中/ ,/ 我们/ 分别/ 介绍/ Pegasos/ 和/ BM/ -/ DSGD/ 算法/ ./ 3.1/ Pegasos/ 算法/ 支持/ 向量/ 机是/ 目前/ 广泛/ 使用/ 的/ 机器/ 学习/ 算法/ 之一/ ,/ 对于/ 给定/ 的/ 训练/ 数据/ 集/ S/ =/ {/ (/ xi/ ,/ yi/ )/ }/ mxi/ ∈/ Rn/ ,/ yi/ ∈/ {/ +/ 1/ ,/ -/ 1/ }/ ,/ 支持/ 向量/ 机/ 可以/ 形式化/ 为式/ (/ 1/ )/ 定义/ 的/ 最小化/ 问题/ ./ Pegasos/ 使用/ 随机/ 梯度/ 下降/ 直接/ 求解/ 式/ (/ 1/ )/ ,/ 具体/ 求解/ 步骤/ 如/ 算法/ 1/ 所示/ ./ minw/ 算法/ 1/ ./ PegasosAlgorithm/ ./ 输入/ :/ S/ ,/ λ/ ,/ T/ ,/ k/ 输出/ :/ wT/ +/ 11/ ./ INITIALIZE/ :/ Choosew1s/ ./ t/ ./ w1/ / 1/ // 槡/ λ/ 2/ ./ FORt/ =/ 1/ ,/ 2/ ,/ …/ ,/ T3/ ./ ChooseAt/ / S/ ,/ whereAt/ =/ k4/ ./ SetA/ +/ 5/ ./ Set/ η/ t/ =/ 6/ ./ Setwt/ +/ 7/ ./ Setwt/ +/ 1/ =/ min1/ ,/ 1/ // 槡/ λ/ Pegasos/ 算法/ 输入/ 参数/ 为/ 迭代/ 次数/ T/ 和/ 用来/ 计算/ 梯度/ 的/ 样本数/ k/ ,/ 每/ 一轮/ 迭代/ 分/ 两步/ :/ 第/ 1/ 步/ 选择/ 训练/ 集/ S/ 中/ 的/ k/ 个/ 样本/ 组成/ 集合/ At/ ,/ 对/ At/ 中/ 导致/ 目标/ 函数/ 产生/ 非零/ 损失/ 的/ 数据/ 点/ 计算/ 梯度/ 更新/ 模型/ w/ ;/ 第/ 2/ 步/ 将/ 得到/ 的/ w/ 映射/ 到/ 集合/ B/ =/ {/ w/ :/ w/ / 1/ // 槡/ λ/ }/ 中/ ./ 3.2/ BM/ -/ DSGD/ 通信/ 机制/ BM/ -/ DSGD/ (/ ButterflyMixingDistributedSto/ -/ chasticGradientDescent/ )/ 采用/ 蝴蝶/ 状/ 通信/ 方式/ ,/ 如/ Page4/ 图/ 1/ 所示/ ./ 假设/ 集群/ 中有/ 2n/ 个/ 机器/ ,/ 这里/ 以/ n/ =/ 2/ 为例/ 进行/ 说明/ ./ BM/ -/ DSGD/ 算法/ 每/ 一轮/ 迭代/ 分/ 两步/ ,/ 每个/ 节点/ 先/ 独立/ 地/ 执行/ 一遍/ 随机/ 梯度/ 下降/ 算法/ ,/ 然后/ 将/ 得到/ 的/ 模型/ 按图/ 1/ 的/ 通信/ 方式/ 发送给/ 对应/ 节点/ ,/ 同时/ 将/ 收到/ 的/ 来自/ 其他/ 节点/ 的/ 模型/ 与/ 本地/ 模型/ 进行/ 平均/ 加权/ ,/ 合并/ 得到/ 的/ 模型/ 作为/ 本地/ 节点/ 下/ 一次/ 迭代/ 的/ 初始模型/ ./ 例如/ 第/ 1/ 轮/ 通信/ 时/ ,/ 节点/ 1/ 与/ 节点/ 2/ 互发/ 模型/ ,/ 节点/ 3/ 与/ 节点/ 4/ 互发/ 模型/ ;/ 第/ 2/ 轮/ 通信/ 节点/ 1/ 与/ 节点/ 3/ 互发/ 模型/ ,/ 节点/ 2/ 与/ 节点/ 4/ 互发/ 模型/ ./ BM/ -/ DSGD/ 算法/ 每/ 经过/ n/ 轮/ 迭代/ ,/ 使/ 每台/ 机器/ 上/ 的/ 数据/ 信息/ 会/ 传播/ 到/ 整个/ 集群/ 上/ ./ 相对/ DGD/ [/ 3/ ]/ 和/ IPM/ [/ 2/ -/ 3/ ]/ 算法/ 来说/ ,/ BM/ -/ DSGD/ 避免/ 了/ 同步/ 更新/ 全局/ 模型/ 带来/ 的/ 网络/ 开销/ 及/ 中心/ 节点/ 瓶颈/ 限制/ ,/ 每个/ 节点/ 同时/ 进行/ 模型/ 合并/ ,/ 大大降低/ 了/ 每/ 一轮/ 迭代/ 的/ 通信/ 开销/ ;/ 相对/ PSGD/ 算法/ ,/ BM/ -/ DSGD/ 在/ 迭代/ 的/ 过程/ 中/ 能/ 利用/ 不同/ 机器/ 上/ 的/ 数据/ 信息/ ,/ 提高/ 算法/ 的/ 收敛/ 速度/ 和/ 性能/ ;/ 相对/ 基于/ 参数/ 服务器/ 实现/ 的/ 大规模/ 随机/ 梯度/ 下降/ 算法/ ,/ BM/ -/ DSGD/ 是/ 一种/ 更加/ 分布式/ 化/ 的/ 实现/ ,/ 每个/ 节点/ 独立/ 地/ 保存/ 最新/ 的/ 模型/ ,/ 能/ 并行/ 地/ 进行/ 模型/ 应用/ ./ 特别/ 地/ ,/ 在/ p2p/ 网络/ 中/ ,/ 例如/ 传感器/ 网络/ 和/ 移动/ 手机/ 网络/ 中/ ,/ 基于/ 模型/ 合并/ 的/ BM/ -/ DSGD/ 是/ 一种/ 典型/ 且/ 普遍/ 应用/ 的/ 训练/ 框架/ [/ 13/ -/ 14/ ]/ ./ 4DA/ -/ DSGD/ 算法/ 通过/ 上述/ 相关/ 工作/ 的/ 回顾/ ,/ 我们/ 发现/ ,/ 目前/ 基于/ 模型/ 合并/ 的/ 分布式/ 随机/ 梯度/ 下降/ 算法/ 在/ 进行/ 模型/ 合并/ 时/ 基本/ 采用/ 平均/ 加权/ 方式/ ,/ 忽略/ 了/ 模型/ 之间/ 的/ 差异性/ ,/ 合并/ 结果/ 相对/ 本地/ 模型/ 变化/ 较大/ ,/ 作为/ 下/ 一轮/ 迭代/ 的/ 初始模型/ 可能/ 导致/ 模型/ 收敛/ 速度/ 较慢/ ,/ 而/ 提高/ 大规模/ 随机/ 梯度/ 下降/ 算法/ 的/ 收敛/ 速度/ 和/ 性能/ 具有/ 重要/ 的/ 应用/ 价值/ ,/ 能/ 极大/ 地/ 节省/ 计算资源/ 和/ 训练/ 时间/ ./ 针对/ 这个/ 问题/ ,/ 本文/ 提出/ 基于/ 差异/ 合并/ 的/ 分布式/ 随机/ 梯度/ 下降/ 算法/ ,/ DA/ -/ DSGD/ (/ DistributedStochasticGradientDescentwithDiscriminativeAggregating/ )/ ,/ 算法/ 具体步骤/ 如/ 算法/ 2/ 所示/ ./ 核心思想/ 是/ 在/ 合并/ 时/ 充分考虑/ 每个/ 模型/ 在/ 其/ 所在/ 机器/ 上/ 已/ 使用/ 训练/ 数据/ 的/ 误差/ 以及/ 整个/ 学习/ 过程/ 的/ 进度/ 这两项/ 差异/ ,/ 从而/ 加快/ 算法/ 收敛/ 速度/ ;/ 同时/ 使用/ 规范化/ 技术/ 使得/ 合并/ 后/ 的/ 模型/ 与/ 本地/ 模型/ 的/ Frobenius/ 范数/ 相同/ ,/ 降低/ 合并/ 给/ 模型/ 带来/ 的/ 巨大变化/ ,/ 提高/ 学习/ 精度/ ./ 算法/ 2/ ./ DA/ -/ DSGD/ 算法/ ./ 输入/ :/ S/ ,/ λ/ ,/ PT/ ,/ k/ ,/ T/ ,/ N/ 输出/ :/ wT1/ ./ ShuffleSandrandomlysplitintoNdisjointsets2/ ./ INITIALIZE/ :/ forallnodesi/ =/ 1/ ,/ 2/ ,/ …/ ,/ N/ ,/ choose3/ ./ FORt/ =/ 1/ ,/ 2/ ,/ …/ ,/ TPARALLELDO4/ ./ wi5/ ./ ε/ i/ =/ Average0/ -/ 1lossoflocalseentrainingdata6/ ./ j/ =/ NextCommunicateNode/ (/ i/ ,/ t/ ,/ N/ )/ 7/ ./ send/ (/ wi8/ ./ μ/ i/ =/ ln9/ ./ wi10/ ./ wi11/ ./ ENDFOR12/ ./ wT/ =/ ∑/ 过程/ 1/ ./ PegasosIterateOnce/ (/ Si/ ,/ λ/ ,/ PT/ ,/ k/ ,/ wi1/ ./ Setwi2/ ./ FORpt/ =/ 1/ ,/ 2/ ,/ …/ ,/ PT3/ ./ ChooseApt/ / S/ ,/ whereApt/ =/ k4/ ./ SetA/ +/ 5/ ./ Set/ η/ t/ -/ 1/ ,/ pt/ =/ 6/ ./ Setwi7/ ./ Setwi8/ ./ ENDFOR9/ ./ RETURNwiPage5/ 过程/ 2/ ./ NextCommunicateNode/ (/ i/ ,/ t/ ,/ N/ )/ ./ 1/ ./ t/ =/ (/ t/ -/ 1/ )/ %/ log2N/ +/ 12/ ./ j/ =/ i/ +/ 2t/ -/ 13/ ./ lower/ _/ bound/ =/ 04/ ./ range/ =/ 2t5/ ./ bias/ =/ 06/ ./ WHILEbias/ </ N7/ ./ IFi/ </ =/ bias/ +/ range8/ ./ lower/ _/ bound/ =/ bias9/ ./ BREAK10/ ./ ENDIF11/ ./ bias/ +/ =/ range12/ ./ ENDWHILE13/ ./ IFj/ >/ lower/ _/ bound/ +/ step14/ ./ j/ =/ lower/ _/ bound/ +/ j/ %/ range15/ ./ ENDIF16/ ./ RETURNjDA/ -/ DSGD/ 算法/ 输入/ 参数/ 中/ S/ 为/ 训练/ 数据/ 集/ ,/ λ/ 为/ Pegasos/ 算法/ 目标/ 函数/ 中/ 模型/ 参数/ w/ 的/ 正则/ 因子/ ,/ PT/ 为/ 本地/ 节点/ 每/ 一轮/ 迭代/ 中/ Pegasos/ 算法/ 的/ 迭代/ 次数/ ,/ k/ 是/ Pegasos/ 算法/ 每/ 一轮/ 迭代/ 中/ 用来/ 计算/ 梯度/ 的/ 训练样本/ 数/ ,/ T/ 是/ DA/ -/ DSGD/ 算法/ 每个/ 节点/ 的/ 迭代/ 次数/ ,/ N/ 是/ 集群/ 中/ 参与/ 计算/ 的/ 节点/ 数/ ./ 其中/ ,/ PT/ 的/ 值/ 用来/ 调节/ 本地/ 计算/ 时间/ 和/ 网络通信/ 时间/ 的/ 平衡/ ,/ 显然/ 太小/ 的/ PT/ 值会/ 使/ 算法/ 的/ 大部分/ 时间/ 开销/ 都/ 在/ 网络通信/ 上/ ./ 具体/ 地/ ,/ 训练/ 数据/ 被/ 随机/ 均匀/ 划分/ 到/ N/ 个/ 节点/ 上/ ,/ 每个/ 节点/ 随机/ 初始化/ 模型/ 参数/ wi/ 限制/ 初始模型/ wi/ 代中/ ,/ 各/ 计算/ 节点/ 调用函数/ PegasosIterateOnce/ 执行/ PT/ 次/ Pegasos/ 算法/ 的/ 内部/ 迭代/ 运算/ ,/ 然后/ 在/ 本地/ 已经/ 被/ 使用/ 过/ 的/ 训练/ 数据/ 上/ 计算/ 模型/ 的/ 平均/ 错误率/ ./ 算法/ 2/ 的/ 步骤/ 7/ 中/ NextCommunicateNode/ 函数/ 根据/ 当前/ 迭代/ 次数/ 以及/ 蝴蝶/ 状/ 通信/ 网络结构/ 选择/ 通信/ 节点/ ,/ 并/ 互相/ 发送/ 节点/ 上/ 的/ 模型/ 和/ 错误率/ ,/ 各/ 节点/ 将/ 接收/ 到/ 的/ 模型/ 及其/ 错误率/ 与/ 本地/ 模型/ 进行/ 差异化/ 合并/ ./ 最后/ 收集/ 合并/ 各/ 机器/ 上/ 的/ 模型/ 得到/ DA/ -/ DSGD/ 算法/ 的/ 最终/ 输出/ 模型/ ./ DA/ -/ DSGD/ 与/ BM/ -/ DSGD/ 算法/ 的/ 主要/ 差别/ 在于/ BM/ -/ DSGD/ 采用/ 平均/ 加权/ 方式/ 进行/ 模型/ 合并/ ,/ 而/ DA/ -/ DSGD/ 对模型/ 进行/ 差异化/ 合并/ ./ DA/ -/ DSGD/ 借鉴/ 集成/ 学习/ 算法/ AdaBoost/ [/ 17/ ]/ 的/ 权重/ 计算公式/ ,/ 根据/ 模型/ 的/ 错误率/ 使用/ 算法/ 2/ 中/ 步骤/ 8/ 公式/ 得到/ 模型/ 的/ 基本/ 权重/ ;/ 同时/ ,/ 考虑/ 到/ 算法/ 不同/ 阶段/ 模型/ 表现/ 的/ 差异/ 以及/ 对/ 全局/ 数据/ 信息/ 的/ 利用/ 情况/ ,/ 我们/ 选择/ 算法/ 2/ 中/ 步骤/ 9/ 的/ 公式/ 在/ 步骤/ 8/ 的/ 基础/ 上/ 重新/ 对/ 权重/ 进行/ 计算/ ,/ 得到/ 最终/ 合并/ 使用/ 的/ 权重/ (/ 具体做法/ 在/ 下/ 一段落/ 解释/ )/ ./ 此外/ ,/ 在/ 每轮/ 迭代/ 的/ 最后/ 一步/ (/ 算法/ 2/ 步骤/ 10/ )/ ,/ 我们/ 将/ 合并/ 得到/ 的/ 模型/ 投射/ 到/ 与/ 本地/ 节点/ 合并/ 前/ 模型/ 的/ Frobenius/ 范数/ 相同/ 的/ 球面/ 上/ ,/ 实验/ 结果表明/ 这么/ 做/ 能够/ 使/ 最终/ 模型/ 的/ 误差/ 更/ 小/ ,/ 模型/ 也/ 更/ 稳定/ ./ 本文/ 使用/ 的/ 模型/ 差异化/ 计算公式/ 能够/ 很/ 好/ 地/ 刻画/ 学习/ 算法/ 不同/ 阶段/ 应该/ 采取/ 的/ 合并/ 策略/ ./ 算法/ 2/ 步骤/ 9/ 中/ 本地/ 模型/ wi/ 化/ 情况/ 如图/ 2/ 所示/ ./ 其中/ ,/ “/ errorratei/ ”/ 和/ “/ errorratej/ ”/ 坐标/ 分别/ 表示/ 模型/ wi/ 也/ 就是/ 算法/ 2/ 中/ 的/ ε/ i/ 和/ ε/ j/ ./ 从图/ 2/ 可以/ 看到/ ,/ 在/ 训练/ 刚/ 开始/ 的/ 时候/ ,/ 各个/ 节点/ 上/ 模型/ 的/ 误差率/ 相对/ 较大/ ,/ 例如/ 在/ 0.4/ ~/ 0.5/ 之间/ ,/ 模型/ 还/ 不能/ 很/ 好/ 地/ 建模/ 训练/ 数据/ ,/ 来自/ 其他/ 节点/ 的/ 模型/ 对/ 本地/ 数据/ 的/ 建模/ 效果/ 也/ 很/ 差/ ,/ 此时/ ,/ 如果/ 本地/ 模型/ 误差率/ 与/ 接收/ 到/ 的/ 模型/ 误差率/ 接近/ ,/ DA/ -/ DSGD/ 就/ 给予/ 本地/ 模型/ 更大/ 的/ 权重/ ,/ 即/ μ/ i/ μ/ i/ +/ μ/ j/ 差率/ 显著/ 低于/ 本地/ 模型/ 误差率/ 时/ ,/ DA/ -/ DSGD/ 给予/ 接收/ 到/ 的/ 模型/ 较大/ 的/ 权重/ ./ 而/ 在/ 算法/ 训练/ 后期/ ,/ 各/ 模型/ 表现/ 趋于稳定/ ,/ 在/ 各/ 节点/ 数据/ 属于/ 独立/ 同/ 分布/ 的/ 假设/ 下/ ,/ 各/ 模型/ 的/ 训练/ 误差率/ 比较/ 接近/ ,/ 此时/ 误差率/ 也/ 相对/ 较/ 小/ ,/ DA/ -/ DSGD/ 给予/ 接收/ 到/ 的/ 模型/ 更大/ 的/ 权重/ ,/ 原因/ 在于/ 此时/ 来自/ 其他/ 节点/ 的/ 模型/ 相对/ 本地/ 模型/ 包含/ 的/ 数据/ 信息/ 更大/ ,/ 能/ 更好/ 地/ 建模/ 全局/ 训练/ 数据/ ./ DA/ -/ DSGD/ 相对/ BM/ -/ DSGD/ 在/ 模型/ 合并/ 时/ 采用/ 了/ 差异化/ 的/ 合并/ 策略/ ,/ 虽然/ 需要/ 额外/ 计算/ 各/ 节点/ 的/ 错误率/ ,/ 但是/ 在/ 分布式/ 环境/ 下/ ,/ 这些/ 开销/ 相对/ 每次/ 迭代/ 中/ 的/ 通信/ 开销/ 小得多/ ./ 所以/ 本文/ 并/ 不/ 讨论/ DA/ -/ DSGD/ 与/ BM/ -/ DSGD/ 在/ 运行/ 时间/ 上/ 的/ 差异/ ./ 另外/ ,/ DA/ -/ DSGD/ 差异化/ 合并/ 的/ 思想/ 能/ 方便/ 地/ 扩展/ 到/ MapReduce/ 等/ Page6/ 其他/ 通信/ 框架/ 上/ ./ 5/ 实验/ 分析/ 本文/ 在/ Epsilon/ 、/ RCV1/ -/ v2/ [/ 18/ ]/ 和/ URL/ [/ 19/ ]/ 3/ 个/ 分类/ 数据/ 集上/ 对比/ DA/ -/ DSGD/ 与/ BM/ -/ DSGD/ 算法/ 的/ 收敛/ 速度/ 和/ 最终/ 模型/ 的/ 性能/ ,/ 采用/ 二/ 分类/ 而/ 不是/ 多/ 分类/ 任务/ 进行/ 实验/ 主要/ 是因为/ 多/ 分类/ 问题/ 可以/ 转换/ 为/ 二/ 分类/ 问题/ ,/ 同时/ ,/ 这/ 也/ 是/ 大部分/ 相关/ 工作/ 采用/ 的/ 做法/ [/ 13/ -/ 14/ ]/ ./ 另外/ ,/ 为了/ 探究/ 差异化/ 合并/ 和/ 规范化/ 技术/ 对模型/ 收敛/ 速度/ 及/ 性能/ 的/ 影响/ ,/ 我们/ 对/ BM/ -/ DSGD/ 合并/ 后/ 的/ 模型/ 使用/ DA/ -/ DSGD/ 采用/ 的/ 规范化/ 技术/ ,/ 得到/ SBM/ -/ DSGD/ ,/ 对/ DA/ -/ DSGD/ 去除/ 规范化/ 步骤/ ,/ 得到/ UDA/ -/ DSGD/ ,/ 并/ 对/ 这些/ 算法/ 的/ 性能/ 及/ 收敛性/ 进行/ 对比/ 实验/ ./ 下面/ 分别/ 介绍/ 实验/ 使用/ 的/ 数据/ 集/ 和/ 实验/ 结果/ ./ 5.1/ 数据/ 集/ Epsilon/ 是/ 人工/ 构造/ 的/ 数据/ 集/ ,/ 来自/ 2008/ 年/ Pascal/ 大规模/ 学习/ 竞赛/ ,/ 是/ 二/ 分类/ 问题/ ./ Epsilon/ 数据/ 集/ 包含/ 400000/ 条/ 训练/ 数据/ 和/ 100000/ 条/ 测试数据/ ,/ 共/ 2000/ 个/ 特征/ ,/ 特征/ 预处理/ 时先/ 按/ 特征/ 进行/ z/ -/ score/ 归一化/ ,/ 再/ 对/ 每条/ 数据/ 按/ 长度/ 为/ 1/ 进行/ 归一化/ ./ RCV1/ -/ v2/ 数据/ 集共/ 804414/ 篇/ 文档/ ,/ 我们/ 使用/ CCAT/ 类别/ 对/ 其/ 进行/ 二/ 分类/ ,/ 使用/ 其中/ 的/ 621392/ 篇/ 文档/ 作为/ 训练/ 数据/ ,/ 183022/ 篇/ 文档/ 作为/ 测试数据/ ,/ 文档/ 用词/ 的/ 向量/ 空间/ 模型表示/ ,/ 去除/ 停/ 用词/ 并/ 进行/ 词干/ 还原/ ,/ 特征/ 使用/ tf/ -/ idf/ 计算/ 方式/ ,/ 每个/ 文档/ 向量/ 按/ 长度/ 为/ 1/ 进行/ 归一化/ ./ 为/ 降低/ 特征/ 数量/ ,/ 我们/ 去/ 图/ 3N/ =/ 16/ ,/ 32/ ,/ 64/ 时/ 算法/ 的/ 测试/ 集/ 错误率/ 下面/ 我们/ 以/ N/ =/ 16/ 为例/ ,/ 进行/ 详细/ 的/ 分析/ ./ 首先/ 我们/ 对比/ DA/ -/ DSGD/ 与/ BM/ -/ DSGD/ 两种/ 算法/ 的/ 性除/ 在/ 训练/ 集中/ 文档/ 频率/ 小于/ 5/ 的/ 词项/ ,/ 最终/ 的/ 特征/ 数为/ 118840/ ./ URL/ 数据/ 集共/ 2396130/ 条/ 数据/ ,/ 3231961/ 个/ 特征/ ,/ 用于/ 判断/ URL/ 是否/ 为/ 恶意/ 链接/ ,/ 是/ 二/ 分类/ 问题/ ./ 特征/ 主要/ 有/ 词法/ 特征/ 和/ 基于/ 主机/ 的/ 特征/ 两大类/ ,/ 都/ 是从/ 网页/ url/ 抽取/ 的/ ,/ 不/ 涉及/ 网页/ 具体内容/ ./ 词法/ 特征/ 是/ 对/ 网页/ url/ 按/ 分隔符/ 切分/ 得到/ 词项/ ,/ 再/ 用词/ 袋/ 模型表示/ ,/ 基于/ 主机/ 的/ 特征/ 包括/ 域名/ 注册/ 日期/ 、/ 注册/ 者/ 、/ 登/ 记者/ 、/ 主机/ 地理位置/ 信息/ 、/ IP地址/ 前缀/ 等/ ,/ 关于/ 该/ 数据/ 集/ 的/ 更加/ 详细/ 的/ 说明/ 请/ 参考文献/ [/ 19/ ]/ ./ 我们/ 对/ 该/ 数据/ 集/ 进行/ 随机/ 划分/ ,/ 划分/ 后/ 训练/ 数据/ 集有/ 2255171/ 条/ 数据/ ,/ 测试/ 集有/ 140959/ 条/ 数据/ ./ 5.2/ 实验/ 结果/ 我们/ 对/ N/ 取/ 不同/ 的/ 值/ ,/ N/ =/ 16/ ,/ 32/ ,/ 64/ ,/ 分别/ 进行/ 实验/ ,/ 验证/ 提出/ 的/ 算法/ 的/ 有效性/ ./ 我们/ 发现/ N/ 取/ 不同/ 值时/ 得到/ 的/ 实验/ 结论/ 是/ 一致/ 的/ ,/ 于是/ ,/ 我们/ 对/ N/ =/ 16/ 的/ 实验/ 结果/ 进行/ 详细分析/ ,/ 验证/ DA/ -/ DSGD/ 相对/ BM/ -/ DSGD/ 在/ 性能/ 和/ 收敛/ 速度/ 上/ 的/ 优势/ ./ 实验/ 中/ ,/ 迭代/ 次数/ T/ 取/ 足够/ 大/ 的/ 值/ ,/ 保证/ 观察/ 到/ 算法/ 的/ 收敛/ 情况/ ./ 参考文献/ [/ 15/ ]/ BM/ -/ DSGD/ 和/ 文献/ [/ 16/ ]/ Pegasos/ 算法/ 的/ 实验/ ,/ 我们/ 取/ PT/ =/ 100/ ,/ k/ =/ 10/ ,/ λ/ =/ 10/ -/ 4.5/ ./ 2.1/ DA/ -/ DSGD/ 与/ BM/ -/ DSGD/ 两种/ 算法/ 的/ 性能/ 针对/ N/ 的/ 不同/ 取值/ ,/ 我们/ 得到/ 图/ 3/ 的/ 实验/ 结果/ ,/ 可以/ 看到/ ,/ 本文/ 提出/ 的/ 算法/ 在/ Epsilon/ 、/ RCV1/ -/ v2/ 和/ URL3/ 个/ 数据/ 集上/ 相对/ BM/ -/ DSGD/ 算法/ 收敛/ 速度/ 更/ 快/ 、/ 性能/ 更好/ ./ 能/ ./ 为此/ ,/ 我们/ 计算/ 测试/ 集/ 错误率/ 随/ 迭代/ 次数/ T/ 的/ 变化/ ,/ 在/ 3/ 个/ 数据/ 集上/ 的/ 测试/ 集/ 错误率/ 如图/ 4/ 所示/ ./ Page7/ 可以/ 看出/ ,/ DA/ -/ DSGD/ 在/ 测试/ 集上/ 的/ 性能/ 明显/ 优于/ BM/ -/ DSGD/ 的/ 性能/ ./ 例如/ T/ =/ 300/ 时/ ,/ 在/ 3/ 个数/ 图/ 4/ 测试/ 集/ 错误率/ 图/ 5Pegasos/ 目标/ 函数/ 值图/ 6/ 训练/ 集/ 错误率/ 据/ 集上/ 我们/ 的/ 方法/ 比/ BM/ -/ DSGD/ 的/ 性能/ 分别/ 提高/ 了/ 1.86/ %/ 、/ 0.49/ %/ 和/ 0.08/ %/ ./ Page8/ 下面/ 我们/ 对比/ DA/ -/ DSGD/ 与/ BM/ -/ DSGD/ 两种/ 算法/ 的/ 收敛/ 速度/ ./ 为此/ ,/ 我们/ 分别/ 计算/ Pegasos/ 目标/ 函数/ 值/ 和/ 训练/ 集/ 错误率/ 随/ 迭代/ 次数/ T/ 的/ 变化/ ,/ 注意/ 这些/ 结果/ 都/ 是/ 在/ 全部/ 训练/ 数据/ 上/ 进行/ 计算/ 的/ ./ Pegasos/ 目标/ 函数/ 值/ 和/ 训练/ 集/ 错误率/ 在/ 3/ 个/ 数据/ 集上/ 的/ 结果/ 分别/ 如图/ 5/ 和/ 图/ 6/ 所示/ ./ 可以/ 看到/ ,/ DA/ -/ DSGD/ 算法/ 相对/ BM/ -/ DSGD/ 算法/ 收敛/ 得/ 更/ 快/ ./ 具体/ 地/ ,/ DA/ -/ DSGD/ 仅用/ 300/ 轮到/ 400/ 轮/ 的/ 迭代/ 其/ 目标/ 函数/ 值/ 和/ 训练/ 误差/ 就/ 基本/ 收敛/ 了/ ,/ 而/ BM/ -/ DSGD/ 目标/ 函数/ 值/ 虽然/ 基本/ 收敛/ 了/ ,/ 但是/ 其/ 训练/ 错误率/ 还有/ 很大/ 的/ 波动/ ,/ 当/ 我们/ 将/ 迭代/ 次数/ T/ 增加/ 到/ 1000/ 次时/ ,/ 发现/ BM/ -/ DSGD/ 算法/ 在/ Epsilon/ 和/ URL/ 两个/ 数据/ 集上/ 的/ 训练/ 错误率/ 依然/ 没有/ 收敛/ ./ 图/ 7/ 测试/ 集/ 错误率/ 首先/ 我们/ 研究/ 差异化/ 模型/ 合并/ 因素/ 的/ 影响/ ,/ 为此/ 我们/ 对比/ DA/ -/ DSGD/ 和/ SBM/ -/ DSGD/ 算法/ ,/ 同时对比/ UDA/ -/ DSGD/ 和/ BM/ -/ DSGD/ 算法/ ./ 通过/ 实验/ 结果/ ,/ 我们/ 发现/ DA/ -/ DSGD/ 相对/ SBM/ -/ DSGD/ 收敛/ 速度/ 更/ 快/ ,/ 随着/ 迭代/ 次数/ 的/ 增加/ ,/ DA/ -/ DSGD/ 很快/ 就/ 收敛/ 了/ ,/ 但是/ SBM/ -/ DSGD/ 还有/ 一定/ 程度/ 的/ 波动/ ,/ 但是/ 两者/ 最终/ 收敛/ 值/ 一致/ ./ 对比/ UDA/ -/ DSGD/ 和/ BM/ -/ DSGD/ 我们/ 得到/ 了/ 相同/ 的/ 结果/ ./ 3/ 个/ 数据/ 集上/ 得到/ 的/ 结果/ 也/ 基本一致/ ,/ 相对/ RCV1/ -/ v2/ 数据/ 集/ ,/ Epsilon/ 和/ URL/ 数据/ 集上/ 的/ 差异/ 更/ 明显/ ,/ 部分/ 原因/ 可能/ 在于/ RCV1/ -/ v2/ 上/ CCAT/ 的/ 分类/ 任务/ 相对/ 容易/ ,/ 算法/ 基本/ 都/ 能/ 很快/ 收敛/ ./ 接下来/ 研究/ 规范化/ 技术/ 对模型/ 收敛/ 速度/ 和/ 收敛/ 值/ 的/ 影响/ ,/ 为此/ 我们/ 对比/ DA/ -/ DSGD/ 和/ UDA/ -/ DSGD/ 算法/ ,/ 同时对比/ SBM/ -/ DSGD/ 和/ BM/ -/ DSGD/ 算法/ ./ 在/ 3/ 个/ 数据/ 集上/ 的/ 实验/ 结果/ 说明/ ,/ 通过/ 使用/ 规范化/ 技/ 5.2/ ./ 2/ 差异化/ 合并/ 和/ 规范化/ 两个/ 因素/ 的/ 深度/ 分析/ 下面/ 我们/ 进一步/ 分析/ DA/ -/ DSGD/ 中/ 差异化/ 合并/ 和/ 规范化/ 这/ 两个/ 因素/ 对/ 收敛/ 速度/ 和/ 收敛/ 值/ 的/ 影响/ ./ 我们/ 对/ DA/ -/ DSGD/ 去除/ 规范化/ ,/ 采用/ 与/ DA/ -/ DSGD/ 相同/ 的/ 合并/ 权重/ 计算公式/ ,/ 但是/ 用/ 权重/ 归一化/ 取代/ 合并/ 结果/ 规范化/ ,/ 得到/ UDA/ -/ DSGD/ ./ 同时/ ,/ 对/ BM/ -/ DSGD/ 的/ 平均/ 加权/ 合并/ 结果/ 进行/ 规范化/ ,/ 得到/ SBM/ -/ DSGD/ ./ 本文/ 实验/ 结果/ 中/ 测试/ 集/ 错误率/ 和/ 训练/ 集/ 错误率/ 曲线/ 趋势/ 基本一致/ ,/ 所以/ 在/ 接下来/ 的/ 分析/ 中/ 我们/ 使用/ 测试/ 集/ 错误率/ 来/ 比较/ 各种/ 算法/ 的/ 收敛/ 速度/ 和/ 收敛/ 值/ ,/ 探究/ 差异化/ 合并/ 和/ 规范化/ 两个/ 因素/ 的/ 不同/ 作用/ ./ 在/ Epsilon/ 、/ RCV1/ -/ v2/ 和/ URL/ 的/ 3/ 个/ 数据/ 集上/ 不同/ 算法/ 的/ 实验/ 结果/ 如图/ 7/ 所示/ ./ 术/ ,/ DA/ -/ DSGD/ 相对/ UDA/ -/ DSGD/ 、/ SBM/ -/ DSGD/ 相对/ BM/ -/ DSGD/ 均/ 收敛/ 到/ 更好/ 的/ 值/ ./ 综合/ 以上/ 实验/ 结果表明/ ,/ 差异化/ 合并/ 能够/ 提高/ 模型/ 收敛/ 速度/ ,/ 而/ 规范化/ 技术/ 能/ 使/ 模型/ 收敛/ 到/ 更好/ 的/ 值/ ./ 5.2/ ./ 3/ 实验/ 小结/ 上述/ 对/ DA/ -/ DSGD/ 算法/ 实验/ 结果/ 的/ 分析/ 和/ 对比/ ,/ 用/ 实验/ 验证/ 了/ 差异化/ 模型/ 合并/ 策略/ 和/ 规范化/ 技术/ 能/ 很/ 好/ 地/ 提高/ 模型/ 的/ 收敛/ 速度/ 和/ 性能/ ,/ 使/ 模型/ 随着/ 迭代/ 次数/ 的/ 增加/ ,/ 其/ 性能/ 有/ 更/ 稳定/ 和/ 更好/ 的/ 表现/ ./ 另外/ ,/ 我们/ 也/ 尝试/ 使用/ 不同/ 的/ 权重/ 计算/ 方式/ ,/ 直接/ 对模型/ 在/ 其/ 所在/ 机器/ 上/ 使用/ 过/ 的/ 训练/ 数据/ 的/ 准确率/ 进行/ 线性/ 加权/ ,/ 实验/ 结果/ 与/ 平均/ 加权/ 合并/ 方式/ 接近/ ,/ 因为/ 每个/ 节点/ 上/ 的/ 模型/ 从/ 训练/ 数据/ 属于/ 独立/ 同/ 分布/ 的/ 假设/ 上/ 来看/ ,/ 其/ 准确率/ 应该/ 是/ 接近/ 的/ ,/ 直接/ 线性/ 加权/ 的/ 方式/ 不能/ 很/ 好/ 地/ 捕捉/ 模型/ 之间/ 的/ 差异性/ ./ Page9/ 另外/ ,/ 最终/ 模型/ 合并/ 时/ 对/ DA/ -/ DSGD/ 算法/ 根据/ 各/ 节点/ 模型/ 按/ 性能/ 仅取/ 前/ K/ 个/ 进行/ 合并/ ,/ 能/ 进一步/ 稍微/ 提高/ 最终/ 模型/ 的/ 性能/ ./ 6/ 总结/ 本文/ 针对/ 分布式/ 随机/ 梯度/ 下降/ 算法/ 普遍/ 采用/ 平均/ 加权/ 方式/ 进行/ 模型/ 合并/ 存在/ 的/ 收敛/ 速度慢/ 和/ 最终/ 模型/ 性能/ 较差/ 的/ 问题/ ,/ 提出/ 了/ 基于/ 模型/ 性能/ 进行/ 差异化/ 合并/ 的/ 策略/ ,/ 同时/ 对/ 合并/ 得到/ 的/ 模型/ 进行/ 规范化/ ,/ 使/ 得到/ 的/ 模型/ 能/ 更好/ 地/ 利用/ 全局/ 数据/ 信息/ ,/ 提高/ 收敛/ 速度/ 和/ 性能/ ./ 实验/ 结果/ 证明/ ,/ 差异化/ 合并/ 策略/ 相对/ 平均/ 加权/ 方式/ ,/ 能/ 提高/ 模型/ 收敛/ 速度/ ,/ 同时/ ,/ 规范化/ 技术/ 的/ 使用/ ,/ 使/ 模型/ 收敛/ 到/ 了/ 更好/ 的/ 点/ ./ 后续/ 研究/ 工作/ 中/ ,/ 我们/ 打算/ 使用/ 逻辑/ 回归/ 等/ 其他/ 学习/ 算法/ ,/ 同时/ 考虑/ MapReduce/ 等/ 分布式计算/ 框架/ ,/ 验证/ 本文/ 提出/ 的/ 差异化/ 合并/ 策略/ 和/ 规范化/ 技术/ 的/ 普遍/ 有效性/ ./ 另外/ ,/ 对于/ 本文/ 使用/ 的/ 差异化/ 加权/ 方式/ ,/ 我们/ 将/ 更/ 细致/ 的/ 研究/ 算法/ 迭代/ 的/ 不同/ 阶段/ 其/ 加权/ 机制/ 对模型/ 收敛/ 速度/ 的/ 影响/ ,/ 同时/ ,/ 我们/ 也/ 会/ 研究/ 其他/ 形式/ 的/ 权重/ 计算/ 方式/ ./ 

