Page1BW/ -/ netRAID/ :/ 一种/ 后/ 端/ 集中/ 冗余/ 管理/ 的/ 网络/ RAID/ 系统/ 那/ 文武/ 1/ )/ ,/ 2/ )/ 柯剑/ 1/ )/ ,/ 2/ )/ 朱旭东/ 1/ )/ ,/ 2/ )/ 孟晓/ ?/ 1/ )/ ,/ 2/ )/ 卜庆忠/ 1/ )/ 许鲁/ 1/ )/ 1/ )/ (/ 中国科学院计算技术研究所/ 北京/ 100190/ )/ 2/ )/ (/ 中国科学院/ 研究生院/ 北京/ 100039/ )/ 摘要/ 在/ 大型/ 网络/ 存储系统/ 中/ ,/ 在/ 设备/ 间/ 采用/ 冗余/ 策略/ 是/ 提高/ 数据/ 可靠性/ 的/ 重要/ 方法/ ./ 针对/ 目前/ 网络/ 存储系统/ 前端/ 集中/ 冗余/ 管理/ 中/ 存在/ 的/ 性能/ 瓶颈/ 问题/ ,/ 结合/ 带外/ 虚拟化/ 存储管理/ 架构/ ,/ 文中/ 提出/ 了/ 一种/ 前端/ 并行/ 数据传输/ 和/ 后/ 端/ 集中/ 冗余/ 管理/ 的/ 网络/ RAID/ 存储系统/ ./ 应用服务器/ 从元/ 数据/ 服务器/ 获得/ 地址映射/ 信息/ 后/ 可/ 直接/ 并行/ 访问/ 存储设备/ ,/ 充分利用/ 了/ 所有/ 存储/ 节点/ 的/ 聚合/ I/ // O/ 性能/ ;/ 冗余/ 管理/ 服务器/ 在/ 磁盘/ 上以/ 日志/ 方式/ 缓存/ 镜像/ 块/ 数据/ ,/ 然后/ 在/ 后台/ 异步/ 计算/ 校验/ 块/ ,/ 并/ 将/ 新/ 校验/ 块/ 数据/ 更新/ 到/ 对应/ 的/ 存储设备/ 节点/ ,/ 从而/ 避免/ 了/ 前端/ 集中/ 冗余/ 管理/ 的/ 单点/ 性能/ 瓶颈/ 和/ 可靠性/ 问题/ ./ 对/ 不同/ 访问/ 活跃度/ 的/ 数据/ 采用/ RAID1/ // RAID5/ 异构/ 分布/ 的/ 管理/ 方法/ ,/ 取得/ 了/ 系统/ 性能/ 、/ 可靠性/ 和/ 价格/ 的/ 平衡/ ./ 关键词/ 网络/ RAID/ 系统/ ;/ 后/ 端/ 集中/ 冗余/ 管理/ ;/ 冗余/ 管理/ 协议/ ;/ 活跃/ 数据/ 缓存/ ;/ 数据/ 可靠性/ 分析/ 1/ 引言/ 网络/ 存储/ 是/ 传统/ 的/ 存储技术/ 借助于/ 网络/ 技术/ 发展/ 形成/ 的/ 一个/ 新兴/ 的/ 交叉/ 技术/ ,/ 研制/ 大容量/ 、/ 高性能/ 、/ 高/ 可用/ 、/ 可/ 扩展/ 、/ 高性价比/ 、/ 高效能/ 和/ 易/ 管理/ 的/ 存储系统/ 是/ 目前/ 网络/ 存储/ 领域/ 的/ 重要/ 发展/ 方向/ ./ 数据/ 冗余技术/ 是/ 以/ 存储空间/ 代价/ 换取/ 数据/ 可靠性/ 的/ 有效/ 策略/ ,/ 能够/ 在/ 一定/ 的/ 可靠性/ 指标/ 范围/ 内/ ,/ 容忍/ 部分/ 存储设备/ 故障/ ,/ 保证/ 任何/ 数据/ 不/ 丢失/ ,/ 同时/ 还/ 能/ 保证数据/ 可以/ 连续/ 访问/ ./ 在/ 网络/ 存储系统/ 中/ 保障/ 用户/ 数据/ 的/ 可靠性/ 非常/ 重要/ ./ 因此/ 如何/ 实现/ 存储/ 节点/ 间/ 的/ 数据/ 冗余/ 功能/ 成为/ 网络/ 存储/ 研究/ 中/ 相当/ 重要/ 的/ 问题/ ./ 当前/ 的/ 网络/ 存储系统/ 主要/ 有/ 以下/ 两种/ 冗余/ 管理/ 架构/ :/ (/ 1/ )/ 前端/ 集中式/ 主从/ 管理/ 在/ 应用服务器/ 和/ 存储/ 节点/ 之间/ 有/ 一个/ 性能/ 强劲/ 的/ 控制/ 服务器/ ,/ 能够/ 快速/ 存储转发/ 数据/ 到/ 存储设备/ 节点/ ,/ 同时/ 也/ 负责管理/ 节点/ 间/ 的/ 数据/ 冗余/ ./ 其/ 优点/ 是/ 系统/ 实现/ 和/ 存储管理/ 简单/ ,/ 数据/ 冗余/ 一致性/ 语义/ 易于/ 保证/ ,/ 而且/ 可以/ 采用/ 高性能/ 硬件/ 或/ Cache/ 等/ 技术/ 对/ 读写/ 进行/ 优化/ ./ 缺点/ 是/ 控制/ 服务器/ 存储转发/ 所有/ 的/ 数据/ 导致/ I/ // O/ 吞吐/ 率/ 受限于/ 其/ 网络接口/ 带宽/ 和/ 处理/ 能力/ ,/ 而且/ I/ // O/ 路径/ 增长/ 带来/ 的/ 转发/ 延迟/ 导致/ I/ // O/ 响应/ 时间/ 增加/ ,/ 难以/ 充分发挥/ 分布式/ 网络/ 存储系统/ 并发/ 通信/ 和/ 并行/ 存储/ 的/ 能力/ ,/ 系统/ 扩展性/ 比较/ 差/ ;/ 另外/ RAID5/ 冗余/ 即时/ 计算/ 模式/ 以及/ 数据/ 小写/ 更新/ 问题/ 将/ 导致用户/ 数据/ 写/ 性能/ 严重/ 下降/ ./ (/ 2/ )/ 分布式/ 对/ 等/ 管理/ 分布式/ 存储/ 没有/ 集中/ 的/ 冗余/ 管理/ 服务器/ ,/ 多个/ 存储设备/ 节点/ 协调/ 管理/ ,/ 优点/ 是/ 能/ 充分发挥/ 分布式系统/ 并行/ 读写能力/ ,/ 缺点/ 是/ 每个/ 节点/ 既/ 是/ Client/ 又/ 是/ Server/ ,/ 节点/ 间/ 冗余/ 管理/ 较/ 复杂/ ./ 目前/ 系统/ 多/ 采用/ 镜像/ 冗余/ 的/ 方式/ 来/ 降低/ 管理/ 复杂度/ ,/ 但/ 镜像/ 方式/ 的/ 存储系统/ 的/ 空间/ 利用率/ 只有/ 50/ %/ ./ 如/ 采用/ RAID5/ 冗余/ 方式/ ,/ 冗余/ 计算/ 将会/ 抢占/ 存储/ 服务器/ 的/ 数据传输/ 带宽/ 和/ CPU/ 资源/ ,/ 冗余/ 请求/ 竞争/ 磁盘/ 资源/ 而且/ 干扰/ 正常/ 用户/ 读写/ 请求/ ,/ 降低/ 了/ 存储/ 服务质量/ ./ 在/ 带外/ 虚拟化/ 存储/ 架构/ 中/ ,/ 数据传输/ 的/ 显著特点/ 是/ 应用服务器/ 直接/ 访问/ 存储设备/ 节点/ ./ 针对/ 上述/ 前端/ 集中/ 冗余/ 管理/ 的/ 问题/ ,/ 本文/ 提出/ 了/ 一种/ 新/ 的/ 网络/ RAID/ 系统/ :/ BW/ -/ netRAID/ ./ 主要/ 创新/ 点/ 是/ 虚拟存储/ 元/ 数据管理/ 和/ 冗余/ 管理/ 功能/ 分离/ ,/ 在/ 数据传输/ 通道/ 后/ 端的/ 冗余/ 管理/ 服务器/ 上/ 后台/ 异步/ 执行/ 计算/ ,/ 从而/ 减少/ 数据/ 冗余/ 对/ 用户/ 数据/ 读写/ 性能/ 的/ 影响/ ./ BW/ -/ netRAID/ 采用/ 后/ 端/ 集中/ 冗余/ 管理/ 方式/ ,/ 读/ 请求/ 直接/ 从/ 存储设备/ 读取数据/ ,/ 写/ 请求/ 数据/ 既/ 要/ 存入/ 存储设备/ ,/ 同时/ 还/ 镜像/ 到/ 冗余/ 管理/ 服务器/ ;/ 然后/ 当/ 系统/ 空闲/ 或者/ 资源/ 不足/ 时才/ 在/ 后台/ 异步/ 计算/ RAID5/ 校验/ 块/ ,/ 并/ 更新/ 到/ 另外/ 的/ 存储设备/ 上/ ./ 优点/ 是/ :/ 带外/ 虚拟化/ 并行/ 访问/ 存储设备/ 能够/ 有效/ 聚合/ 所有/ 存储设备/ 的/ I/ // O/ 性能/ ;/ 而后/ 端/ 冗余/ 管理/ 能够/ 避免/ 前端/ 集中/ 冗余/ 管理/ 的/ 单点/ 性能/ 瓶颈/ 问题/ ./ 对/ 不同/ 访问/ 活跃度/ 的/ 数据/ 采用/ RAID1/ // RAID5/ 异构/ 分布/ 的/ 管理/ 方法/ ,/ 取得/ 了/ 系统/ 的/ 性能/ 、/ 可靠性/ 和/ 价格/ 的/ 平衡/ ./ 本文/ 第/ 2/ 节/ 概述/ 系统结构/ 和/ 主要/ 的/ 软件/ 模块/ 功能/ 以及/ 关键技术/ 特点/ ;/ 第/ 3/ 节/ 讨论/ 系统/ 正常/ 、/ 冗余/ 计算/ 和/ 重构/ 状态/ 的/ 数据/ 读写/ 协议/ ;/ 第/ 4/ 节/ 描述/ 系统/ 实现/ 方法/ ,/ 包括/ 元/ 数据结构/ 、/ 设备/ 创建/ 和/ 初始化/ 、/ 节点/ 故障/ 监测/ 和/ 恢复/ 以及/ 活跃/ 数据/ 缓存/ 机制/ ;/ 第/ 5/ 节/ 评价/ 系统/ 性能/ 的/ 扩展性/ 和/ 数据恢复/ 速度/ ;/ 第/ 6/ 节/ 分析/ 系统/ 如何/ 保证数据/ 冗余/ 的/ 完整性/ 以及/ 具体/ 的/ 数据/ 布局/ 方式/ ,/ 并/ 给出/ 系统/ 的/ 数据/ 可靠性/ 分析/ 结论/ ;/ 第/ 7/ 节/ 介绍/ 相关/ 研究/ 工作/ ;/ 第/ 8/ 节/ 总结/ 并/ 指出/ 下/ 一步/ 的/ 研究/ 方向/ ./ 2/ 系统/ 概述/ 2.1/ BW/ -/ netRAID/ 系统结构/ 主要/ 有/ 4/ 类/ 组件/ :/ 图/ 1/ 描述/ 了/ BW/ -/ netRAID/ 的/ 系统/ 组件/ 图/ ,/ 系统/ (/ 1/ )/ 存储设备/ 节点/ (/ StoragedeviceNode/ ,/ SN/ )/ 它/ 有/ 独立/ 的/ CPU/ 、/ 内存/ 、/ 网络接口/ 和/ 大容量/ 的/ 磁盘阵列/ ,/ 用来/ 存储/ 应用服务器/ 的/ 数据/ ./ 当/ 存储设备/ 节点/ 出现/ 软硬件/ 故障/ 或者/ 内部/ RAID/ 损坏/ 时/ ,/ 可能/ 导致/ 其上/ 的/ 数据/ 不能/ 被/ 访问/ 甚至/ 丢失/ ./ (/ 2/ )/ 应用服务器/ (/ ApplicationServer/ ,/ AS/ )/ 它/ 上面/ 运行/ 应用服务/ 程序/ ,/ 如/ 文件/ 服务器/ 、/ Web/ 服务器/ 、/ 数据库/ 服务器/ 和/ 流媒体/ 服务器/ 等/ ./ 应用服务器/ 通过/ 网络/ 虚拟/ 磁盘/ 读写/ 存储设备/ 节点/ 上/ 的/ 数据/ ./ 存储设备/ 间/ 数据/ 冗余/ 关系/ 对/ 应用服务器/ 透明/ ./ (/ 3/ )/ 虚拟化/ 元/ 数据/ 服务器/ (/ VirtualizationMeta/ -/ dataServer/ ,/ VMS/ )/ 它/ 以/ 带外/ 虚拟化/ 方式/ 管理/ 应用服务器/ 的/ 数据/ 请求/ 到/ 存储设备/ 节点/ 上/ 数据/ 之间/ 的/ 映射/ ./ 应用服务器/ 先/ 从/ 元/ 数据管理/ 服务器/ 获取/ 并/ 缓存/ 网络/ 虚拟/ 磁盘/ 的/ 地址映射/ 信息/ ,/ 在/ 数据/ 读写/ 时/ 直接/ 访问/ 存储设备/ 节点/ ,/ 执行/ 读写/ 请求/ 操作/ ./ Server/ ,/ RMS/ )/ (/ 4/ )/ 冗余/ 管理/ 服务器/ (/ RedundancyManagementPage3/ 它/ 维护/ 和/ 管理/ 存储设备/ 节点/ 之间/ 数据/ 的/ 冗余/ 关系/ ,/ 保证/ 不会/ 因为/ 单个/ 存储设备/ 节点/ 的/ 故障/ 而/ 导致/ 图/ 1BW/ -/ netRAID/ 系统/ 组件/ 图/ 应用服务器/ 、/ 元/ 数据管理/ 服务器/ 和/ 存储设备/ 节点/ 是/ 网络/ 存储系统/ 的/ 带外/ 虚拟化/ 存储管理/ 的/ 基本/ 结构/ ①/ ./ 传统/ 的/ 前端/ 集中/ 管理系统/ 中/ 控制/ 服务器/ 既/ 负责/ 资源/ 地址映射/ 和/ 数据/ 转发/ ,/ 又/ 管理/ 多个/ 存储设备/ 间/ 的/ 冗余/ 计算/ ./ BW/ -/ netRAID/ 系统/ 中/ 新/ 增加/ 了/ 冗余/ 管理/ 服务器/ ,/ 它/ 专职/ 负责/ 存储设备/ 节点/ 间/ 的/ 数据/ 冗余/ 管理/ ,/ 而/ 资源/ 地址映射/ 由元/ 数据/ 服务器/ 管理/ ./ 元/ 数据管理/ 和/ 冗余/ 管理/ 功能/ 分离/ 是/ 本/ 系统/ 的/ 主要/ 特点/ ./ 元/ 数据管理/ 服务器/ 在/ 数据通道/ 的/ 前端/ 以带/ 外/ 方式/ 管理/ 资源/ 地址映射/ ,/ 冗余/ 管理/ 服务器/ 在/ 数据通道/ 的/ 后/ 端/ 以/ RAID1/ // RAID5/ 分级/ 存储/ 方式/ 管理/ 存储设备/ 节点/ 上/ 数据/ 块/ 的/ 冗余/ 关系/ ./ 2.2/ 软件/ 模块/ 图/ 2/ 给出/ BW/ -/ netRAID/ 系统/ 中/ 主要/ 的/ 软件/ 模块/ 和/ 数据/ 请求/ 传输/ 示意图/ ./ 应用服务器/ 上/ 的/ 网络/ 磁盘/ 从元/ 数据/ 服务器/ 获取/ 地址映射/ 关系/ 后/ ,/ 缓存/ 到/ 地址映射/ 表/ (/ mappingtable/ )/ ,/ 再/ 将/ 请求/ 通过/ 虚拟/ 磁盘/ (/ VirtualDisk/ ,/ VD/ )/ 直接/ 发送到/ 对应/ 的/ 存储设备/ 节点/ 上/ 的/ 数据/ 收发/ 模块/ (/ data/ -/ server/ )/ ,/ 根据/ 相应/ 状态/ 的/ 数据/ 读写/ 协议/ 完成/ 读写操作/ ./ 存储设备/ 节点/ 的/ 数据/ 卷/ (/ data/ -/ lv/ )/ 存储/ 应用服务器/ 读写/ 的/ 原始数据/ ,/ 而/ 冗余/ 卷/ (/ parity/ -/ lv/ )/ 存储/ 不同/ 设备/ 间/ 的/ RAID5/ 校验/ 块/ 数据/ ./ 冗余/ 管理/ 服务器/ 上/ 的/ 数据/ 收发/ 模块/ 接收/ 处理/ 存储设备/ 节点/ 发起/ 的/ 数据/ 读写/ 请求/ ,/ 当/ 后台/ 异步/ 用户/ 数据/ 不能/ 访问/ 或/ 丢失/ ,/ 并/ 提供数据/ 重构/ 机制/ 和/ 恢复/ 策略/ ./ 计算/ RAID5/ 时/ 进程/ (/ RAID/ -/ update/ )/ 也/ 向/ 冗余/ 数据/ 收发/ 模块/ (/ parity/ -/ server/ )/ 发起/ 校验/ 块/ 的/ 读写/ 请求/ ./ 对于/ 镜像/ 写/ 请求/ ,/ 冗余/ 管理/ 服务器/ 先/ 以/ 日志/ 写/ 方式/ 缓存/ 到/ 本地/ 的/ 磁盘/ (/ LogDisk/ )/ 上/ ,/ 当/ RAID5/ 计算/ 时/ 再/ 读/ 回/ ./ 如果/ 冗余/ 管理/ 服务器/ 要/ 读取/ 或者/ 恢复/ 存储设备/ 节点/ 上/ 的/ 数据/ ,/ 则/ 直接/ 访问/ 存储设备/ 节点/ 的/ 数据/ 收发/ 模块/ 获取数据/ ./ 2.3/ 关键技术/ 本节/ 从/ 系统/ 冗余/ 级别/ 、/ 读写/ 性能/ 和/ 冗余/ 计算技术/ 等/ 3/ 个/ 方面/ 分析/ BW/ -/ netRAID/ 的/ 关键技术/ 特点/ ./ (/ 1/ )/ 系统/ 冗余/ 级别/ 从/ 应用服务器/ 上/ 的/ 用户/ 数据/ 读写/ 角度看/ ,/ BW/ -/ netRAID/ 是/ 一个/ Stripping/ +/ Mirroring/ 的/ 系统/ ,/ 并发/ 读写/ 多个/ 存储设备/ 节点/ 能/ 更/ 充分发挥/ 分布式/ 网络/ 存储系统/ 的/ 性能/ 优势/ ./ 而/ 从/ 存储设备/ 节点/ 上/ 的/ 数据/ 存储/ 关系/ 看/ ,/ 是/ 一个/ RAID5/ 系统/ ,/ 系统/ 有/ 较优/ 的/ 空间/ 利用率/ 和/ 可靠性/ // 空间/ 利用率/ 比/ 的/ 优势/ ./ (/ 2/ )/ 读写/ 性能/ 整个/ 系统/ 的/ 数据/ 读/ 性能/ 不再/ 受限于/ 前端/ 的/ 集中管理/ 服务器/ ,/ 充分发挥/ 多个/ 存储设备/ 节点/ 的/ 并发/ 读/ 性能/ ,/ 系统/ 扩展性/ 高/ ,/ 解决/ 了/ 前端/ 集中式/ 管理/ 的/ 读性/ ①/ RobPeglar/ ,/ VirtualizationI/ -/ What/ ,/ Why/ ,/ WhereandHow/ ?/ Page4/ 图/ 2BW/ -/ netRAID/ 的/ 软件/ 结构图/ 能/ 瓶颈/ 问题/ ./ 数据/ 写/ 操作/ 将/ 新/ 写/ 的/ 数据/ 块/ 镜像/ 到/ 后/ 端的/ 冗余/ 管理/ 服务器/ 上/ ,/ 写/ 性能/ 和/ 镜像/ 冗余/ 方式/ 相当/ ,/ 比/ RAID5/ 方式/ 得到/ 大幅度提高/ ./ 而且/ 针对/ 写/ 性能/ 可能/ 受限/ 单个/ 集中/ 点/ 的/ 问题/ ,/ 冗余/ 管理/ 服务器/ 采用/ 延迟/ 写/ 和/ 写/ 聚合/ ,/ 以及/ 在/ 磁盘/ 上用/ 日志/ 方式/ 缓存/ 活跃/ 数据/ 等/ 优化/ 技术/ 来/ 进一步提高/ 写/ 性能/ ./ (/ 3/ )/ 冗余/ 计算/ 由/ 单个/ 服务器/ 集中管理/ 冗余/ 存储/ 的/ 机制/ 和/ 策略/ ,/ 能够/ 简化/ 数据/ 冗余/ 一致性/ 的/ 管理/ ./ 降级/ 和/ 重构/ 状态/ 时先/ 从/ 冗余/ 管理/ 服务器/ 的/ 缓存/ 磁盘/ 中/ 读取/ ,/ 没有/ 时才/ 由/ 冗余/ 管理/ 服务器/ 重构/ 丢失/ 的/ 数据/ ,/ 而且/ 还/ 支持/ 优先/ 恢复/ 活跃/ 的/ 数据/ ,/ 缓存/ 的/ 数据/ 不必/ 执行/ RAID5/ 重构/ ,/ 减少/ 了/ RAID5/ 冗余/ 计算/ 时/ 数据/ 块/ 的/ 同步/ 传输/ 和/ 计算/ 负担/ ./ 存储设备/ 节点/ 不/ 参与/ 冗余/ 存储管理/ ,/ 即使/ 某个/ 节点/ 有/ 故障/ 也/ 不会/ 直接/ 影响/ 其它/ 节点/ ,/ 非/ 故障/ 节点/ 照常/ 读写/ ,/ 最大/ 程度/ 上/ 减少/ 冗余/ 存储/ 对/ 存储/ 服务器/ 性能/ 的/ 影响/ ,/ 保证/ 了/ 整个/ 系统/ 的/ 存储/ 服务质量/ ./ 3/ 数据/ 读写/ 协议/ BW/ -/ netRAID/ 系统/ 主要/ 有/ 4/ 种/ 运行/ 状态/ :/ 正常/ 状态/ 、/ 降级/ 状态/ 、/ 重构/ 状态/ 和/ 失效/ 状态/ ./ 当/ 每个/ 存储设备/ 节点/ 都/ 正常/ 工作/ 时/ ,/ 整个/ 系统/ 处于/ 正常/ 状态/ ;/ 而/ 当/ 其中/ 某个/ 存储设备/ 节点/ 故障/ 时/ ,/ 导致/ 其上/ 的/ 数据/ 不能/ 被/ 访问/ 甚至/ 丢失/ ,/ 系统/ 进入/ 降级/ 状态/ ;/ 当/ 故障/ 设备/ 被/ 修复/ 或者/ 用/ 新/ 设备/ 替代/ 后/ ,/ 系统/ 转入/ 重构/ 状态/ ,/ 冗余/ 管理/ 服务器/ 负责/ 重构/ 丢失/ 的/ 数据/ 并/ 恢复/ 到/ 存储设备/ 节点/ 上/ ;/ 当/ 所有/ 丢失/ 的/ 数据/ 都/ 被/ 恢复/ 后/ ,/ 存储设备/ 节点/ 间/ 的/ 数据/ 冗余/ 关系/ 重新/ 回到/ 一致/ 的/ 状态/ ,/ 系统/ 也/ 恢复/ 到/ 正常/ 状态/ ./ 如果/ 多个/ 设备/ 同时/ 故障/ 超过/ 了/ 系统/ 冗余/ 能力/ ,/ 则/ 会/ 导致/ 数据/ 不能/ 被/ 访问/ 或者/ 部分/ 数据/ 丢失/ ,/ 系统/ 变为/ 失效/ 状态/ ./ 下面/ 通过/ 分析/ 每种/ 状态/ 下/ 的/ 读写操作/ 过程/ 中/ 的/ 内存/ 、/ 磁盘/ 和/ 网络/ 上/ 的/ I/ // O/ 操作/ ,/ 描述/ 数据/ 读写/ 协议/ 流程/ ./ 限于/ 篇幅/ ,/ 这里/ 只/ 给出/ 正常/ 状态/ 下/ 数据/ 写/ 协议/ 、/ 后台/ RAID5/ 计算/ 协议/ 和/ 重构/ 状态/ 下/ 丢失/ 数据恢复/ 协议/ ./ 3.1/ 正常/ 状态/ 下/ 数据/ 写/ 协议/ 正常/ 状态/ 下/ 数据/ 写/ 协议/ 包括/ 存储设备/ 节点/ 的/ 写/ 请求/ 处理/ 和/ 冗余/ 管理/ 服务器/ 的/ 镜像/ 数据/ 缓存/ 处理/ ./ 具体步骤/ 如图/ 3/ 所示/ :/ (/ 1/ )/ 应用服务器/ 上/ 的/ 用户/ 应用程序/ 向/ 网络/ 虚拟/ 磁盘/ 发起/ 写/ 请求/ ;/ (/ 2/ )/ 网络/ 虚拟/ 磁盘/ 根据/ 地址映射/ 信息/ 将/ 写/ 请求/ Page5/ 通过/ 网络/ 数据传输/ 协议/ 传给/ 对应/ 的/ 存储设备/ 节点/ ;/ (/ 3/ )/ 存储设备/ 节点/ 将/ 写/ 请求/ 数据/ 分别/ 存储/ 到/ 冗余/ 管理/ 服务器/ 和/ 本地/ 卷/ 设备/ 上/ ,/ 组成/ RAID1/ 镜像/ 冗余/ ;/ 如果/ 冗余/ 管理/ 服务器/ 上/ 没有/ 这个/ 数据/ 块/ 的/ 旧/ 数据/ 块/ ,/ 存储设备/ 节点/ 在/ 写/ 新/ 数据/ 块/ 之前/ 还/ 需要/ 读取/ 存储设备/ 节点/ 上/ 的/ 旧/ 数据/ 块/ 并/ 传输/ 到/ 冗余/ 管理/ 服务器/ 上/ ;/ (/ 4/ )/ 冗余/ 管理/ 服务器/ 先/ 在/ 内存/ 中/ 缓存/ 接收/ 的/ 镜像/ 数据/ 块/ ,/ 返回/ 镜像/ 写/ 完成/ 信息/ 给/ 存储设备/ 节点/ ;/ 当/ 缓存/ 到/ 一定/ 数目/ 的/ 数据/ 块/ 后/ ,/ 再/ 将/ 多个/ 数据/ 块/ 以/ 一次/ 连续/ 写/ 方式/ 写/ 到/ 本地/ 日志/ 磁盘/ 上/ ;/ (/ 5/ )/ 存储设备/ 节点/ 的/ 底层/ 卷/ 设备/ 和/ 冗余/ 管理/ 服务器/ 的/ 写/ 请求/ 都/ 完成/ 后/ ,/ 存储设备/ 节点/ 返回/ 写/ 请求/ 完成/ 信息/ 给/ 应用服务器/ 的/ 网络/ 虚拟/ 磁盘/ ;/ (/ 6/ )/ 网络/ 虚拟/ 磁盘/ 返回/ 写/ 请求/ 完成/ 信息/ 给/ 上层/ 存储/ 应用/ ./ 3.2/ 正常/ 状态/ 下/ 后台/ 计算/ RAID5/ 协议/ 在/ 冗余/ 管理/ 服务器/ 上/ ,/ 当/ 系统/ 空闲/ 或者/ 磁盘/ 剩余/ 空间/ 超过/ 指定/ 阈值/ 时/ ,/ 后台/ 进程/ 异步/ 计算/ RAID5/ 校验/ 块/ ./ 具体步骤/ 如图/ 4/ 所示/ :/ 1/ ./ 从/ 冗余/ 管理/ 服务器/ 的/ 磁盘/ 日志/ 缓存/ 中/ 读取数据/ 块/ ,/ 同时/ 也/ 读取/ 其/ 对应/ 的/ 旧/ 数据/ 块/ 和校验/ 块/ ;/ 如果/ 缓存/ 了/ 校验/ 块/ ,/ 转到/ 步/ 3/ ,/ 如果/ 没有/ 转到/ 步/ 2/ ;/ 2/ ./ 根据/ RAID5/ 布局/ 关系/ 从/ 对应/ 存储设备/ 节点/ 上读/ 回/ 2.4/ ./ 将/ 读/ 请求/ 数据/ 返回/ 给/ 冗余/ 管理/ 服务器/ ;/ 3/ ./ 根据/ RAID5/ 公式/ 计算/ 得到/ 新/ 的/ 校验/ 块/ 并/ 缓存/ 到/ 磁/ 4/ ./ 冗余/ 管理/ 服务器/ 再/ 发起/ 新/ 的/ 校验/ 块/ 写/ 请求/ ,/ 将/ 新/ 校盘/ 日志/ 中/ ;/ 校验/ 块/ :/ 请求/ ;/ 2.1/ ./ 冗余/ 管理/ 服务器/ 向/ 存储设备/ 节点/ 发起/ 校验/ 块/ 的/ 读/ 2.2/ ./ 存储设备/ 节点/ 将/ 读/ 请求/ 转发给/ 底层/ 卷/ 设备/ ;/ 2.3/ ./ 从/ 磁盘阵列/ 设备/ 上/ 获取/ 所/ 需/ 的/ 读/ 请求/ 数据/ 后/ ,/ 返回/ 读/ 请求/ 完成/ 信息/ ;/ 验块/ 写/ 到/ 这个/ RAID5/ 条带/ 对应/ 的/ 存储设备/ 节点/ 上/ ;/ 5/ ./ 存储设备/ 节点/ 将/ 接到/ 的/ 写/ 请求/ 转发给/ 底层/ 卷/ 设备/ ;/ 6/ ./ 存储设备/ 节点/ 的/ 磁盘阵列/ 设备/ 返回/ 写/ 请求/ 完成/ 信息/ ;/ 7/ ./ 冗余/ 管理/ 服务器/ 收到/ 存储设备/ 节点/ 返回/ 的/ 新/ 校验/ 块/ 的/ 写/ 请求/ 完成/ 信息/ ./ 冗余/ 管理/ 服务器/ 根据/ 数据/ 活跃/ 程度/ 和/ 存储空间/ 占用/ 情况/ 决定/ 是否/ 释放/ 磁盘/ 上/ 缓存/ 的/ RAID1/ 镜像/ 块/ 数据/ 和/ RAID5/ 校验/ 块/ 数据/ ./ 对于/ 不再/ 有用/ 的/ 旧/ 数据/ 块/ 和校验/ 块/ ,/ 由/ 资源/ 回收/ 进程/ 删除/ 并/ 释放/ 其/ 占用/ 的/ 空间/ ./ 3.3/ 重构/ 状态/ 下/ 丢失/ 数据恢复/ 协议/ 当/ 存储设备/ 节点/ 的/ 故障/ 被/ 修复/ 或者/ 新/ 设备/ 加入/ 到/ BW/ -/ netRAID/ 系统/ 中/ ,/ 系统/ 进入/ 重构/ 状态/ ./ 冗余/ 管理/ 服务器/ 作为/ 存储设备/ 节点/ 间/ 数据/ 冗余/ 关系/ 的/ 集中管理/ 点/ ,/ 启动/ 数据/ 重构/ 进程/ ,/ 重构/ 丢失/ 的/ 数据/ 并/ 同步/ 到/ 对应/ 的/ 存储设备/ 节点/ 上/ ./ 重构/ 进程/ 首先/ 恢复/ 缓存/ 的/ 活跃/ 数据/ ,/ 然后/ 再/ 重构/ 其它/ 的/ 数据/ ./ 丢失/ 的/ 活跃/ 数据/ 只/ 需/ 从/ 冗余/ 管理/ 服务器/ 的/ 磁盘/ 缓存/ 日志/ 中/ 就/ 可以/ 找回/ ,/ 而/ 其它/ 非/ 活跃/ 数据/ 需要/ 利用/ RAID5/ 算法/ 进行/ 重构/ 恢复/ ./ 恢复/ 非/ 活跃/ 数据/ 块/ 的/ 具体步骤/ 如图/ 5/ 所示/ :/ 1/ ./ 根据/ 要/ 重构/ 数据/ 块/ 的/ RAID5/ 冗余/ 关系/ ,/ 查询/ 磁盘/ 缓存/ 日志/ 上/ 是否/ 缓存/ 了/ 相同/ 条带/ 的/ 数据/ 块/ 和校验/ 块/ ,/ 如果/ 已经/ 缓存/ 则/ 读/ 回到/ 内存/ 中/ ,/ 如果/ 没有/ 则/ 从/ 存储设备/ 节点/ 读回/ ;/ 2/ ./ 向/ 组成/ RAID5/ 的/ 多个/ 存储设备/ 节点/ 发起/ 读/ 请求/ ;/ 3/ ./ 每个/ 存储设备/ 节点/ 将/ 读/ 请求/ 转发给/ 底层/ 卷/ 设备/ ;/ 4/ ./ 从卷/ 设备/ 上/ 获取/ 读/ 请求/ 数据/ 后/ ,/ 返回/ 读/ 请求/ 完成/ 信息/ ;/ 5/ ./ 冗余/ 管理/ 服务器/ 接从/ 存储设备/ 节点/ 收到/ 读/ 请求/ 的/ 数据/ ;/ 备上/ ;/ 6/ ./ 当/ 相同/ 条带/ 的/ 所有/ 数据/ 块/ 和校验/ 块/ 都/ 被/ 读/ 回到/ 冗余/ 管理/ 服务器/ 后/ ,/ 根据/ RAID5/ 恢复/ 算法/ 计算/ 得到/ 故障/ 的/ 存储设备/ 节点/ 上/ 丢失/ 的/ 数据/ ;/ 7/ ./ 冗余/ 管理/ 服务器/ 将/ 计算/ 得到/ 的/ 数据/ 以写/ 请求/ 方式/ 传送/ 给/ 刚/ 修复/ 的/ 存储设备/ 节点/ ;/ 8/ ./ 刚/ 修复/ 的/ 存储设备/ 节点/ 将/ 接收/ 的/ 数据/ 写/ 到/ 底层/ 卷设/ Page69/ ./ 存储设备/ 节点/ 的/ 底层/ 卷/ 设备/ 完成/ 写/ 操作/ ;/ 10/ ./ 存储设备/ 节点/ 将/ 写/ 完成/ 信息/ 返回/ 给/ 冗余/ 管理/ 服务器/ ./ 当/ 丢失/ 的/ 非/ 活跃/ 数据/ 被/ 正确/ 恢复/ 后/ ,/ 后续/ 的/ 用户/ 读写/ 请求/ 直接/ 访问/ 相应/ 的/ 存储设备/ 节点/ ./ 4/ 系统/ 实现/ 本/ 节/ 介绍/ BW/ -/ netRAID/ 系统/ 实现/ 中/ 关键/ 的/ 数据结构/ 和/ 性能/ 优化/ 方法/ ./ 首先/ 概述/ 系统/ 的/ 元/ 数据结构/ ,/ 然后/ 描述/ 系统/ 的/ 启动/ 运行/ 、/ 故障/ 监测/ 和/ 恢复/ 机制/ ./ 最后/ 分析/ 使用/ 磁盘/ 缓存/ 镜像/ 数据/ 的/ 优点/ 和/ 缓存数据/ 结构/ ,/ 以及/ 怎样/ 计算/ RAID5/ 数据/ 和/ 回收/ 失效/ 数据/ 占用/ 的/ 存储空间/ ./ 4.1/ 元/ 数据/ BW/ -/ netRAID/ 的/ 元/ 数据/ 包括/ 记录/ 网络/ RAID/ 设备/ 状态/ 的/ 系统/ 元/ 数据/ ,/ 以及/ 系统/ 运行/ 过程/ 中/ 标识/ 数据/ 块/ 状态/ 的/ 数据/ 块/ 元/ 数据/ ./ 系统/ 元/ 数据/ 记录/ 了/ 组成/ 网络/ 存储系统/ 的/ 存储设备/ 节点/ 的/ 名称/ 、/ 逻辑/ 卷/ 和校验/ 卷/ 名字/ 和/ 大小/ 、/ 网络接口/ 标识/ 等/ ./ 这些/ 元/ 数据/ 信息/ 存储/ 在/ 存储设备/ 节点/ 的/ 逻辑/ 卷/ 和校验/ 卷/ 的/ 保留/ 区域/ 内/ 以及/ 冗余/ 管理/ 服务器/ 上/ ./ 当/ 系统启动/ 时/ ,/ 各个/ 存储设备/ 节点/ 分别/ 从/ 各自/ 的/ 逻辑/ 卷/ 和校验/ 卷上/ 读取/ 与/ 系统/ 状态/ 信息/ 相关/ 的/ 元/ 数据/ ./ 在/ 运行/ 过程/ 中/ ,/ 设备/ 状态/ 发生变化/ 时/ (/ 如/ 节点/ 故障/ )/ ,/ 要/ 同步/ 更新/ 每个/ 节点/ 的/ 系统/ 元/ 数据/ 到/ 磁盘/ 上/ ,/ 保持/ 系统/ 元/ 数据/ 的/ 一致性/ ./ 数据/ 块/ 元/ 数据/ 包括/ 存储设备/ 节点/ 上/ 的/ RAID5/ 初始化/ 位/ 图表/ 、/ RAID5/ 重构/ 位/ 图表/ 和/ 数据/ 块/ 更新/ 位/ 图表/ 以及/ 冗余/ 管理/ 服务器/ 上/ 的/ 缓存数据/ 块/ 索引/ 表/ ./ 存储设备/ 节点/ 用/ 数据/ 块/ 更新/ 位/ 图表/ ./ 冗余/ 管理/ 服务器/ 用/ 缓存数据/ 块/ 索引/ 表/ 同时/ 记录/ 数据/ 块/ 的/ 状态/ ,/ 保证数据/ 块/ 状态/ 的/ 元/ 数据/ 不会/ 因为/ 单个/ 节点/ 异常/ 宕机/ 而/ 丢失/ ;/ 然后/ 定时/ 将/ 修改/ 的/ 元/ 数据/ 刷新/ 到/ 磁盘/ 上/ ,/ 避免/ 每次/ 改变/ 都/ 更新/ 到/ 磁盘/ 的/ 同步/ 写/ 操作/ 产生/ 过大/ 的/ I/ // O/ 负担/ 的/ 问题/ ./ 4.2/ 创建/ 和/ 数据/ 同步/ 存储设备/ 节点/ 启动/ 后/ 根据/ 配置/ 访问/ 冗余/ 管理/ 服务器/ ,/ 协商/ 网络/ RAID/ 的/ 组成/ ./ 目前/ 系统/ 支持/ N/ 个/ 存储设备/ 上/ 的/ 相同/ 容量/ (/ C/ )/ 的/ 逻辑/ 卷/ 组成/ RAID5/ ,/ 另外/ 每个/ 存储设备/ 节点/ 还要/ 保留/ 容量/ 为/ C/ // (/ N/ -/ 1/ )/ 的/ 冗余/ 卷/ ,/ 用来/ 存储/ 节点/ 间/ 的/ RAID5/ 校验/ 块/ ./ 然后/ 利用/ 初始化/ 位/ 图表/ 同步/ RAID5/ 条带/ 数据/ ,/ 分别/ 在/ 多个/ 存储设备/ 节点/ 对/ 数据/ 块/ 和校验/ 块/ 置/ 零/ ,/ 与/ 读取数据/ 块/ 到/ 一个/ 集中/ 点/ 再/ 计算/ RAID5/ 校验/ 块/ 的/ 同步/ 计算方法/ 相比/ ,/ 能够/ 更/ 大地/ 发挥/ 多个/ 存储设备/ 节点/ 并发/ 读写能力/ ,/ 数据/ 初始化/ 同步/ 时间/ 短/ ./ 4.3/ 节点/ 失效/ 监测/ 和/ 恢复/ 冗余/ 管理/ 服务器/ 通过/ “/ 心跳/ ”/ 机制/ 来/ 监控/ 和/ 管理/ 所有/ 的/ 存储设备/ 节点/ ,/ 存储设备/ 节点/ 自己/ 监控/ 内部/ 的/ 磁盘/ 状态/ ./ 当/ 存储设备/ 节点/ 主动/ 上报/ 内部/ 磁盘/ 故障/ 或者/ 冗余/ 管理/ 服务器/ 监测/ 到/ 某个/ 节点/ 失效/ 时/ ,/ 系统/ 进入/ 降级/ 状态/ ,/ 执行/ 降级/ 状态/ 读写/ 协议/ ./ 应用服务器/ 的/ 存储/ 代理/ 发现/ 不能/ 访问/ 故障/ 节点/ ,/ 则/ 会/ 从/ 元/ 数据管理/ 服务器/ 得到/ 指向/ 冗余/ 管理/ 服务器/ 的/ 新/ 映射/ 地址/ ./ 存储设备/ 节点/ 的/ 内部/ 故障/ 修复/ 或者/ 新/ 的/ 存储设备/ 节点/ 加入/ 到/ 系统/ 中后/ ,/ 系统/ 进入/ 重构/ 状态/ ,/ 执行/ 重构/ 状态/ 读写/ 协议/ ./ 直到/ 所有/ 的/ 数据/ 重新/ 恢复/ 一致/ 的/ 冗余/ 关系/ 后/ ,/ 网络/ RAID/ 系统/ 再/ 恢复/ 到/ 正常/ 状态/ ./ 4.4/ 在/ 磁盘/ 上/ 缓存/ 活跃/ 数据/ 多个/ 存储设备/ 节点/ 的/ 写/ 请求/ 都/ 要/ 镜像/ 到/ 冗余/ 管理/ 服务器/ ,/ 随着/ RAID/ 规模/ 增大/ 和/ I/ // O/ 写/ 负载/ 增加/ ,/ 冗余/ 管理/ 服务器/ 可能/ 成为/ 性能/ 的/ 瓶颈/ ./ 缓存/ 写/ 请求/ 并/ 立即/ 向/ 存储设备/ 节点/ 返回/ 写/ 确认/ 能够/ 提高/ I/ // O/ 响应/ 时间/ 和/ 写/ 性能/ ,/ 缓存数据/ 越/ 多则/ 提高/ 写/ 性能/ 越多/ ./ 用/ 内存/ 缓存/ 速度/ 快/ 但/ 容量/ 有限/ ,/ 异常/ 宕机/ 还会/ 丢失/ 内存/ 中/ 的/ 数据/ ,/ 而/ SSD/ 价格/ 还/ 比较/ 昂贵/ ./ 磁盘/ 容量/ 大/ 价格便宜/ ,/ 但/ 磁盘/ 连续/ 写/ 速度/ 快/ 而/ 随机/ 写/ 速度慢/ ./ 因此/ 采用/ 日志/ 方式/ 缓存/ 写/ 请求/ 数据/ ,/ 能/ 改变/ 随机/ 写为/ 连续/ 写/ 操作/ ,/ 从而/ 提高/ 磁盘/ 上/ 缓存数据/ 的/ 性能/ ./ 4.4/ ./ 1/ 缓存数据/ 块/ 根据/ 数据/ 冗余/ 完整性/ 的/ 需要/ (/ 参见/ 第/ 6/ 节/ )/ ,/ 缓存/ 的/ 数据/ 既/ 要/ 包括/ 新/ 数据/ 块/ 也/ 要/ 包括/ 旧/ 数据/ 块/ ./ 日志/ 技术/ 能够/ 记录/ 写/ 请求/ 的/ 多个/ 数据/ 版本/ ,/ 当/ 后台/ RAID5/ 计算/ 时/ ,/ 只/ 需/ 利用/ 最新/ 的/ 版本/ 和/ 最/ 老/ 的/ 版本/ 来/ 计算/ ./ 资源/ 回收/ 程序/ 需要/ 保留/ 最新/ 和/ 最/ 老/ 的/ 数据/ 块/ ,/ 优先/ 回收/ 中间/ 的/ 失效/ 的/ 数据/ 版本/ ,/ 并/ 释放/ 其/ 占用/ 的/ 存储空间/ ./ Page74/ ./ 4.2/ 缓存/ 校验/ 块/ 冗余/ 管理/ 服务器/ 既/ 缓存/ 活跃/ 数据/ 块/ ,/ 当/ 计算/ RAID5/ 校验/ 块/ 后/ ,/ 也/ 尽可能/ 地/ 缓存/ 校验/ 块/ ./ 如果/ 不/ 缓存/ 这些/ 校验/ 块/ ,/ 那么/ 在/ 小写/ 更新/ 情况/ 下/ 每次/ 计算/ RAID5/ 校验/ 块/ 时/ ,/ 都/ 需要/ 从/ 相应/ 的/ 存储设备/ 节点/ 上读/ 回旧/ 校验/ 块/ ,/ 会/ 增加/ 网络/ 数据传输/ 和/ 存储设备/ 节点/ 上/ 读数据/ 的/ 负担/ ./ 即使/ 将/ 前后/ 数据/ 块/ 的/ 变化/ 差异/ 传给/ 存储设备/ 节点/ ,/ 由/ 存储设备/ 节点/ 接着/ 完成/ RAID5/ 计算/ ,/ 也/ 仍然/ 会/ 增加/ 存储设备/ 节点/ 的/ 计算/ 和/ 读/ 校验/ 块/ 的/ 负担/ ./ 4.4/ ./ 3RAID5/ 更新/ 尽管/ 磁盘/ 容量/ 较大/ ,/ 但是/ 相对/ 于/ 网络/ 存储系统/ 的/ 总/ 存储容量/ 还是/ 较/ 小/ ./ 因此/ 为/ 避免/ 冗余/ 管理/ 服务器/ 上/ 的/ 磁盘/ 缓存/ 区/ 最终/ 被/ 写/ 满/ ,/ 需要/ 设置/ 剩余/ 容量/ 下限/ ,/ 启动/ 后台/ 进程/ 及时/ 做/ 异步/ RAID5/ 计算/ ,/ 并/ 释放/ 部分/ 存储空间/ ./ 在/ 正常/ 状态/ 下/ 只有/ 数据/ 写/ 请求/ 到达/ 冗余/ 管理/ 服务器/ ,/ 读/ 请求/ 时/ 冗余/ 管理/ 服务器/ 空闲/ ,/ 另外/ 利用/ 内存/ 暂时/ 缓存/ 部分/ 新/ 数据/ 块/ 使得/ 不必/ 每次/ 写/ 操作/ 都/ 访问/ 磁盘/ ./ 上述/ 方法/ 使得/ 冗余/ 管理/ 服务器/ 能够/ 得到/ 一定/ 的/ 空闲/ 时间/ 用于/ RAID5/ 计算/ 和校验/ 块/ 更新/ ./ 4.4/ ./ 4/ 资源/ 回收/ 日志/ 方式/ 缓存数据/ 需要/ 有/ 资源/ 回收/ 进程/ 不断/ 删除/ 部分/ 数据/ 以便/ 重新/ 利用/ 它们/ 占用/ 的/ 空间/ ./ 系统/ 优先/ 回收/ 写/ 请求/ 中/ 的/ 中间/ 版本/ ,/ 再/ 根据/ 数据/ 的/ 活跃度/ 回收/ 最近/ 很少/ 访问/ 的/ 数据/ 块/ 占用/ 的/ 空间/ ./ 如果/ 剩余/ 空间/ 仍然/ 低于/ 阈值/ ,/ 则/ 先/ 执行/ RAID5/ 计算/ 后/ ,/ 再/ 回收/ 已经/ 完成/ RAID5/ 计算/ 的/ 数据/ 块/ ,/ 直到/ 剩余/ 空间/ 大于/ 资源/ 回收/ 下限/ 的/ 阈值/ ./ 5/ 系统/ 性能/ 评估/ 本节/ 对/ BW/ -/ netRAID/ 系统/ 的/ 性能/ 进行/ 测试/ 和/ 评估/ ./ 测试/ 系统/ 包括/ 1/ 个元/ 数据管理/ 服务器/ 、/ 4/ 个/ 应用服务器/ 、/ 6/ 个/ 存储设备/ 节点/ 和/ 1/ 个/ 冗余/ 管理/ 服务器/ ;/ 节点/ 间/ 通过/ 千兆/ 以太网/ 连接/ ./ 每个/ 存储设备/ 上/ 的/ 数据/ 卷/ 和校验/ 卷/ 组成/ 一个/ 网络/ RAID/ 设备/ ,/ 然后/ 在/ 网络/ RAID/ 设备/ 上/ 划分/ 逻辑/ 卷/ ,/ 每个/ 逻辑/ 卷/ 大小/ 为/ 20G/ ,/ 逻辑/ 卷以/ stripping/ 方式/ 映射/ 到/ 多个/ 存储设备/ 节点/ 上/ ./ 每次/ 测试/ 都/ 重新/ 创建/ 网络/ RAID/ 设备/ 和/ 逻辑/ 卷/ ,/ 然后/ 从/ 应用/ 服务器端/ 读写/ 网络/ 虚拟/ 磁盘/ ,/ 统计/ 多个/ 应用服务器/ 聚合/ 的/ 读写/ 性能/ ./ 使用/ IOMeter/ ①/ 作为/ I/ // O/ 性能/ 评测/ 工具/ ,/ 负载/ 类型/ 为/ 4KB/ 块/ 的/ 连续/ 读/ 和/ 连续/ 写/ ./ 测试/ 机/ 的/ 软硬件/ 配置/ 如表/ 1/ 所示/ ./ ASXeon2/ ./ 4G1/ 网络/ 虚拟/ SNXeon2/ ./ 4G11/ 个/ 120G/ 的/ VMSXeon2/ ./ 4G11/ 个/ 120G/ 的/ 3.0/ G24/ 个/ 200G/ 的/ RMSPentiumD5/ ./ 1/ 扩展性/ 测试/ 扩展性/ 测试/ 用来/ 比较/ 后/ 端/ 集中/ 冗余/ 管理/ 方式/ (/ BW/ -/ netRAID/ )/ 和/ 前端/ 集中/ 冗余/ 管理/ 方式/ (/ RAID0/ // RAID5controller/ )/ 随/ 应用服务器/ 个数/ 和/ 存储设备/ 节点/ 个数/ 变化/ 时/ 的/ 读写/ 性能/ ./ 在/ 前端/ 集中管理/ 方式/ 中/ ,/ 聚合/ 的/ 读写/ 性能/ 受限于/ 集中管理/ 服务器/ 转发/ 性能/ 和/ 存储设备/ 节点/ 聚合/ 读写/ 性能/ ./ 在/ BW/ -/ netRAID/ 中/ ,/ 读/ 性能/ 只/ 受限于/ 存储设备/ 节点/ 聚合/ 读/ 性能/ ,/ 而/ 写/ 性能/ 受限于/ 存储设备/ 节点/ 聚合/ 写/ 性能/ 和/ 后/ 端/ 冗余/ 管理/ 服务器/ 缓存/ 写/ 性能/ ./ 因此/ 测试/ 结果显示/ ,/ 在/ 同样/ 配置/ 下/ BW/ -/ netRAID/ 系统/ 比/ 前端/ 集中/ 冗余/ 管理/ 方式/ 性能/ 高/ ,/ 而且/ 随/ 应用服务器/ 和/ 存储设备/ 节点/ 个数/ 增加/ ,/ 系统/ 性能/ 扩展性/ 更好/ ./ 5.1/ ./ 1/ 应用服务器/ 个数/ 变化/ 由/ 4/ 个/ 存储设备/ 节点/ 组成/ 网络/ RAID/ ,/ 然后/ 分别/ 测试/ 1/ ~/ 5/ 个/ 应用服务器/ 时/ 的/ 顺序/ 读/ 性能/ ,/ 测试/ 中/ 所有/ 应用服务器/ 同时/ 并发/ 读/ 同一个/ 卷/ ./ 由于/ 磁盘/ 读/ 请求/ 的/ 搜索/ 距离/ 小/ 以及/ 数据/ 预取/ 和/ 缓存/ 效果/ ,/ 使得/ 读/ 操作/ 多数/ 在/ 内存/ 中/ 完成/ ./ 图/ 6/ 中/ 前端/ 集中/ 方式/ 中/ 应用服务器/ 个数/ 为/ 4/ 个/ 时/ 就/ 到达/ 最高/ (/ 千兆/ 网络/ 最大/ 传输/ 带宽/ )/ ,/ 聚合/ 读/ 性能/ 受限于/ 集中/ 的/ 控制点/ 的/ 最大/ 网络/ 传输/ 带宽/ ./ BW/ -/ netRAID/ 读/ 性能/ 随/ 应用服务器/ 个数/ 线性/ 增加/ ,/ 预期/ 最高/ 可/ 到达/ 4/ 个/ 存储设备/ 节/ ①/ IOMeterDiskandFileSystembenchmark/ ,/ http/ :/ // // www/ ./ iometer/ ./ orgPage8/ 点/ 的/ 聚合/ 网络带宽/ ,/ 扩展性/ 更好/ ./ 另外/ 由于/ 前端/ 集中/ 方式/ 比后/ 端/ 集中/ 方式/ 要/ 多/ 一次/ 存储转发/ ,/ 所以/ 即使/ 没/ 到达/ 集中/ 控制点/ 最大/ 性能/ 时/ ,/ 前端/ 集中/ 方式/ 系统/ 的/ 聚合/ 读/ 性能/ 也/ 一直/ 低于/ BW/ -/ netRAID/ 系统/ ./ 由/ 4/ 个/ 存储设备/ 节点/ 组成/ 网络/ RAID/ ,/ 然后/ 分别/ 测试/ 1/ ~/ 5/ 个/ 应用服务器/ 时/ 顺序/ 写/ 性能/ ,/ 测试/ 的/ 应用服务器/ 同时/ 分别/ 写/ 不同/ 的/ 卷/ ./ 多个/ 应用服务器/ 共享/ 相同/ 的/ 网络/ 存储设备/ ,/ 同时/ 写会/ 导致/ 不同/ 卷/ 的/ 磁盘/ 写/ 请求/ 的/ 搜索/ 距离/ 大/ ,/ 系统/ 性能/ 受限于/ 存储设备/ 节点/ 的/ 磁盘/ 写/ 性能/ ./ 图/ 7/ 的/ 测试/ 结果表明/ 后/ 端/ 集中/ 冗余/ 管理/ 的/ 方式/ 比/ 前端/ 集中/ 冗余/ 管理/ 方式/ 写/ 性能/ 高/ ./ RAID5/ 前端/ 集中/ 方式/ 由于/ 还要/ 同时/ 进行/ 冗余/ 计算/ ,/ 其/ 性能比/ RAID0/ 前端/ 集中/ 方式/ 更/ 低/ ./ 5.1/ ./ 2/ 存储设备/ 节点/ 个数/ 变化/ 测试/ 由/ 2/ ~/ 6/ 个/ 存储设备/ 节点/ 组成/ 网络/ RAID/ ,/ 然后/ 4/ 个/ 应用服务器/ 分别/ 顺序/ 读/ 不同/ 的/ 卷/ ./ 图/ 8/ 表明/ 存储设备/ 节点/ 个数/ 相同/ 时后/ 端/ 冗余/ 管理/ 比/ 前端/ 冗余/ 管理/ 读/ 性能/ 高/ 一些/ ./ 两种/ 结构/ 的/ 读/ 性能/ 都/ 随/ 存储设备/ 节点/ 个数/ 线性/ 增加/ ./ 但/ 前端/ 冗余/ 管理/ 预期/ 最大/ 只能/ 达到/ 前端/ 集中控制/ 服务器/ 的/ 最大/ 网络带宽/ ,/ 而后/ 端/ 冗余/ 管理/ 预期/ 最大/ 可/ 达到/ 所有/ 存储设备/ 节点/ 的/ 最大/ 网络带宽/ ./ 图/ 8/ 存储设备/ 节点/ 个数/ 变化/ 的/ 读/ 性能/ (/ 4/ 个/ 测试/ 由/ 2/ ~/ 6/ 个/ 存储设备/ 节点/ 组成/ 网络/ RAID/ ,/ 然后/ 4/ 个/ 应用服务器/ 分别/ 顺序/ 写/ 不同/ 的/ 卷/ ./ 图/ 9/ 表明/ 后/ 端/ 冗余/ 管理/ 比/ 前端/ 冗余/ 管理/ 写/ 性能/ 高/ 一些/ ./ 后端/ 冗余/ 管理/ 的/ 写/ 性能/ 当/ 存储设备/ 节点/ 个数/ 较/ 少时/ 随/ 节点/ 个数/ 增加/ ,/ 当/ 节点/ 个数/ 超过/ 4/ 个/ 时/ 不再/ 增加/ ,/ 说明/ 此时/ 写/ 性能/ 受限于/ 后/ 端/ 冗余/ 管理/ 服务器/ 的/ 缓存/ 写/ 性能/ ./ RAID0/ 方式/ 前端/ 冗余/ 管理/ 写/ 性能/ 当/ 存储设备/ 节点/ 个数/ 较/ 少时/ 随/ 节点/ 个数/ 增加/ ,/ 但/ 节点/ 个数/ 超过/ 5/ 个/ 时/ 不再/ 增加/ ,/ 说明/ 此时/ 写/ 性能/ 受限于/ 前端/ 冗余/ 管理/ 服务器/ 的/ 转发/ 写/ 性能/ ./ RAID5/ 方式/ 前端/ 集中管理/ 由于/ 冗余/ 计算/ 的/ 负担/ 导致/ 其/ 性能/ 最低/ ./ 5.2/ 磁盘/ 缓存/ 读/ 和/ 重构/ 读/ 速度/ 比较/ 当/ 系统/ 降级/ 和/ 重构/ 状态/ 时要/ 读取/ 丢失/ 的/ 数据/ 块/ 或者/ 重构/ 进程/ 主动/ 恢复/ 丢失/ 数据/ 时/ ,/ 根据/ 数据/ 块/ 状态/ 有/ 两种/ 执行/ 方式/ :/ (/ 1/ )/ 从/ 冗余/ 管理/ 服务器/ 上/ 的/ 磁盘/ 缓存/ 读数据/ ,/ 记作/ logging/ -/ read/ ;/ (/ 2/ )/ 先/ 从/ 多个/ 存储设备/ 节点/ 上/ 读数据/ 再/ 做/ RAID5/ 计算/ 后/ 得到/ 读/ 请求/ 数据/ ,/ 记作/ RAID5/ -/ degrade/ -/ read/ ./ 测试/ 由/ 3/ ~/ 6/ 个/ 存储设备/ 节点/ 组成/ 网络/ RAID/ ,/ 当/ 一个/ 存储设备/ 节点/ 故障/ 时/ ,/ 系统/ 进入/ 降级/ 状态/ ,/ 再/ 测试/ 从/ 一个/ 应用服务器/ 上/ 顺序/ 读/ 一个/ 卷/ 的/ 速度/ ./ 图/ 10/ 表明/ 从/ 磁盘/ 缓存/ 读/ 性能/ 是/ RAID5/ 计算/ 恢复/ 读/ 性能/ 的/ 1.25/ ~/ 1.5/ 倍/ ,/ RAID5/ 计算/ 恢复/ 读/ 性能/ 随/ 存储设备/ 节点/ 个数/ 增加/ 而/ 减少/ ,/ 因此/ 说明/ 缓存数据/ 能/ 缩短/ 重构/ 时间/ ./ 上面/ 测图/ 10/ 从/ 磁盘/ 缓存/ 读/ 和/ RAID5/ 计算/ 恢复/ 读/ 的/ 对比/ Page9/ 试是/ 无/ 负载/ 的/ 理想/ 情况/ 下/ ,/ 如果/ 存储设备/ 节点/ 上/ 有/ 应用服务器/ 的/ 读写/ 请求/ ,/ 彼此/ 竞争/ 资源/ 会/ 导致/ 重构/ 读/ 性能/ 降低/ ./ 6/ 数据/ 布局/ 和/ 可靠性/ 分析/ 本/ 节/ 首先/ 讨论/ 在/ 数据/ 更新/ 时/ 保证/ 冗余/ 关系/ 完整性/ 的/ 必要性/ ,/ 给出/ 冗余/ 组中/ 数据/ 块/ 所属/ 的/ RAID1/ 和/ RAID5/ 冗余/ 级别/ 的/ 变换/ 过程/ ,/ 最后/ 针对/ 上述/ 数据/ 块/ 异构/ 分布/ ,/ 分析/ 了/ 不同/ 类别/ 的/ 数据/ 块/ 可靠性/ ./ 6.1/ 数据/ 布局/ 系统/ 必须/ 保证/ 正常/ 状态/ 下/ 每个/ 数据/ 块/ 都/ 有/ 冗余/ 保护/ ,/ 才能/ 在/ 某个/ 存储设备/ 节点/ 故障/ 或者/ 其上/ 的/ 磁盘/ 损坏/ 导致/ 数据/ 丢失/ 后/ ,/ 通过/ 剩余/ 节点/ 上/ 的/ 数据恢复/ 丢失/ 的/ 数据/ ./ 存储设备/ 节点/ 上/ 数据/ 在/ 网络/ RAID/ 初始化/ 操作/ 后/ ,/ 建立/ RAID5/ 冗余/ 关系/ ;/ 当/ 存储设备/ 节点/ 上/ 某个/ 数据/ 块/ 被/ 更新/ 后/ ,/ 它/ 和/ 冗余/ 管理/ 服务器/ 上/ 的/ 镜像/ 块/ 建立/ RAID1/ 冗余/ 关系/ ./ 但是/ 其它/ 节点/ 上/ 相同/ 的/ RAID5/ 条带/ 中/ 的/ 数据/ 块/ 缺少/ 了/ 冗余/ 保护/ ,/ 也就是说/ 此时/ 新写/ 块/ 所在/ 的/ 节点/ 故障/ ,/ 丢失/ 的/ 数据/ 可以/ 用/ 冗余/ 管理/ 服务器/ 上/ 的/ 镜像/ 块/ 数据恢复/ ,/ 但是/ 如果/ 其它/ 节点/ 故障/ 丢失/ 数据/ ,/ 就/ 不能/ 利用/ 剩余/ 节点/ 上/ 的/ 数据恢复/ ./ 因此/ BW/ -/ netRAID/ 在/ 冗余/ 管理/ 服务器/ 上/ 保留/ 旧/ 数据/ 块/ ,/ 在/ 冗余/ 管理/ 服务器/ 和/ 其它/ 存储设备/ 节点/ 间/ 维持/ 原来/ 的/ RAID5/ 冗余/ 关系/ ./ 只有/ 当/ 后台/ RAID5/ 计算/ 新/ 校验/ 块/ 并/ 将/ 其/ 写/ 到/ 对应/ 存储设备/ 节点/ 上后/ ,/ 才/ 重新/ 在/ 多个/ 存储设备/ 节点/ 间/ 形成/ 新/ 的/ RAID5/ 冗余/ 关系/ ./ 图/ 11/ 描述/ 了/ 一个/ 数据/ 块/ 的/ RAID1/ // RAID5/ 冗余/ 级别/ 变换/ 的/ 过程/ ./ 如果/ 一个/ RAID5/ 条带/ 的/ 多个/ 块/ 都/ 被/ 更新/ ,/ 那么/ 对应/ 的/ 旧/ 数据/ 块/ 也/ 都/ 被/ 缓存/ 到/ 冗余/ 管理/ 服务器/ 上/ ./ 当/ 其它/ 存储设备/ 节点/ 故障/ 丢失/ 数据/ 时/ ,/ 这些/ 旧/ 数据/ 块/ 将/ 作为/ 一个/ 整体/ 参与/ RAID5/ 恢复/ 计算/ ./ 如果/ 冗余/ 管理/ 服务器/ 故障/ 丢失/ 了/ 这些/ 旧/ 数据/ 块/ ,/ 则/ 只/ 需/ 重新/ 计算/ 同步/ 的/ 这个/ RAID5/ 条带/ ,/ 并/ 不会/ 导致/ 数据/ 丢失/ ./ 6.2/ 数据/ 可靠性/ 分析/ 将/ 数据/ 块/ 按照/ RAID1/ // RAID5/ 的/ 不同/ 布局/ 分类/ 讨论/ 数据/ 块/ 的/ 平均/ 丢失/ 时间/ ./ MTTFnode/ 是/ 节点/ 平均/ 失效/ 时间/ ,/ MTTRmirror/ 是从/ 冗余/ 管理/ 服务器/ 上/ 磁盘/ 日志/ 缓存/ 中/ 恢复/ 镜像/ 数据/ 块/ 的/ 时间/ ,/ MTTRRAID5/ 是/ 冗余/ 管理/ 服务器/ 从/ 其它/ 存储/ 节点/ 读取数据/ 然后/ RAID5/ 重构/ 恢复/ 数据/ 的/ 时间/ ./ (/ 1/ )/ 如图/ 11/ (/ a/ )/ 所示/ ,/ N/ 个/ 存储设备/ 节点/ 上/ 的/ 数据/ 块/ 建立/ RAID5/ 冗余/ 关系/ ./ 此/ 状态/ 下/ 数据/ 块/ D1/ 属于/ 由/ N/ 个/ 存储/ 节点/ 上/ 的/ 数据/ 组成/ 的/ RAID5/ ,/ 数据/ 可靠性/ 为/ MTTDL/ =/ MTTF2node/ (/ 2/ )/ 如图/ 11/ (/ b/ )/ 所示/ ,/ 刚/ 做/ 完写/ 更新/ 的/ 数据/ 块/ D1/ 在/ 存储设备/ 节点/ 和/ 冗余/ 管理/ 服务器/ 上/ 各有/ 一份/ 数据/ 拷贝/ ,/ 并且/ 在/ 冗余/ 管理/ 服务器/ 上/ 也/ 保留/ 了/ 其/ 对应/ 的/ 旧块/ D1old/ ./ 此/ 状态/ 下块/ D1/ 属于/ 由/ 所在/ 存储设备/ 节点/ 和/ 冗余/ 管理/ 服务器/ 组成/ 的/ RAID1/ ,/ 数据/ 可靠性/ 为/ (/ 3/ )/ 如图/ 11/ (/ e/ )/ 所示/ ,/ RAID5/ 计算/ 后/ ,/ 新/ 校验/ 块拷贝/ 到/ 对应/ 的/ 存储设备/ 节点/ ,/ 并且/ 在/ 冗余/ 管理/ 服务器/ 上/ 保留/ 数据/ 块/ D1/ ./ 此/ 状态/ 下/ D1/ 块/ 既/ 属于/ 由/ N/ 个/ 存储设备/ 节点/ 上/ 的/ 数据/ 组成/ 的/ RAID5/ ,/ 而且/ 也/ 属于/ 由/ D1/ 所在/ 的/ 存储设备/ 节点/ 和/ 冗余/ 管理/ 服务器/ 组成/ 的/ RAID1/ ./ 图/ 12/ 给出/ 了/ 活跃/ 数据/ 的/ Markov/ 状态/ 变化/ 图/ ,/ 状态/ 0/ 表示/ 无故障/ 状态/ ,/ 状态/ 1/ 表示/ 冗余/ 管理/ 服务器/ 或/ D1/ 所在/ 存储设备/ 节点/ 之一/ 故障/ ,/ 状态/ 2/ 表/ Page10/ 示/ 冗余/ 管理/ 服务器/ 和/ D1/ 所在/ 存储设备/ 节点/ 都/ 故障/ ,/ 状态/ 3/ 表示/ 另外/ 一个/ 存储设备/ 节点/ 故障/ ,/ 状态/ 4/ 表示/ 另外/ 一个/ 存储设备/ 节点/ 和/ 冗余/ 管理/ 服务器/ 或/ D1/ 所在/ 存储设备/ 节点/ 之一/ 故障/ ,/ 状态/ F/ 表示/ 冗余/ 管理/ 服务器/ 和/ D1/ 所在/ 存储设备/ 节点/ 都/ 故障/ 而且/ 又/ 有/ 另外/ 一个/ 存储设备/ 节点/ 故障/ 或者/ 两个/ 存储设备/ 节点/ 同时/ 故障/ ./ 状态/ F/ 时/ 数据/ 发生/ 丢失/ ./ 因此/ 根据/ 活跃/ 数据/ 的/ Markov/ 状态/ 转换/ 图/ ,/ 计算/ 得到/ 其/ 数据/ 可靠性/ 为/ MTTDL/ =/ MTTF3node/ // (/ 2/ (/ N/ -/ 1/ )/ ·/ MTTRmirror/ ·/ 假设/ 存储设备/ 节点/ 个数/ 为/ N/ =/ 10/ ;/ 每个/ 存储设备/ 节点/ 的/ 数据/ 块/ 个数/ 为/ C/ (/ 总/ 数据量/ 为/ 4TB/ )/ ;/ 一个/ 存储设备/ 节点/ 在/ 冗余/ 管理/ 服务器/ 上/ 的/ 活跃/ 数据/ 块/ 占/ 其/ 总/ 数据/ 块/ 的/ 比例/ 为/ r/ =/ 0.1/ ;/ 从/ 磁盘/ 日志/ 恢复/ 和/ RAID5/ 计算/ 恢复/ 的/ 速度/ 分别/ 为/ v1/ =/ 50MB/ ,/ v2/ =/ v1/ // 1.5/ ;/ 平均/ 恢复/ 时间/ 为/ 数据/ 块/ 数量/ 除于/ 恢复/ 速度/ ./ 根据/ 上面/ 的/ 数据/ 块/ 分类/ 和/ 可靠性/ 公式/ ,/ 图/ 13/ 出示/ 一组/ 随/ 存储设备/ 节点/ 平均/ 无故障/ 时间/ 变化/ 的/ 数据/ 可靠性/ 数值/ ,/ 缓存/ 到/ 冗余/ 管理/ 服务器/ 上/ 活跃/ 数据/ 块/ 比/ 普通/ 数据/ 块/ 的/ 数据/ 可靠性/ 高/ 3/ ~/ 5/ 个/ 数量级/ ./ 7/ 相关/ 研究/ 本/ 节/ 首先/ 讨论/ 存储设备/ 节点/ 间/ 采用/ 数据/ 冗余/ 的/ 必要性/ ,/ 然后/ 对比/ 分析/ 典型/ 的/ 网络/ 存储系统/ 中/ 的/ 冗余/ 管理/ 方法/ ./ 最后/ 比较/ BW/ -/ netRAID/ 的/ 采用/ RAID1/ // RAID5/ 异构/ 分布/ 和/ 日志/ 方式/ 缓存数据/ 的/ 性能/ 优化/ 方法/ ./ 7.1/ 存储/ 节点/ 间/ 数据/ 冗余/ 必要性/ 文献/ [/ 1/ ]/ 详细分析/ 了/ 单个/ 冗余/ 难以/ 保证/ 大型/ 存储系统/ 足够/ 的/ 数据/ 可靠性/ ,/ 进而/ 讨论/ 了/ 采用/ 多级/ 冗余/ (/ 如/ mirror2/ ,/ mirror3/ ,/ RAID5/ +/ 1/ )/ 后/ 的/ 存储系统/ 的/ 数据/ 可靠性/ ./ 文献/ [/ 2/ ]/ 则/ 分析/ 了/ RAID0/ 、/ RAID1/ 和/ RAID5/ 算法/ 组合/ 得到/ 的/ 双层/ RAID/ 的/ 可靠性/ 模型/ 和/ 性能/ ./ 文献/ [/ 3/ ]/ 也/ 是/ 通过/ 组合/ 不同/ RAID/ 算法/ 的/ 方式/ ,/ 分析/ 比较/ 了/ 由/ 通用/ 部件/ “/ IBMbrick/ ”/ 组成/ 存储系统/ 的/ 可靠性/ ,/ 并且/ 通过/ 仿真/ 方式/ 分析/ 了/ 不同/ 参数/ 对/ 可靠性/ 的/ 影响/ ./ 综上所述/ ,/ 在/ 大规模/ 存储系统/ 中/ 存储/ 节点/ 间/ 采用/ 数据/ 冗余技术/ 能/ 显著/ 地/ 提高/ 整个/ 系统/ 的/ 数据/ 可靠性/ ./ 7.2/ 冗余/ 管理/ 在/ 存储系统/ 使用/ 镜像/ 冗余/ 的/ 方式/ ,/ 可以/ 避免/ 校验/ 块/ 计算/ 的/ 开销/ ,/ 但/ 用户/ 数据/ 占用/ 存储系统/ 资源/ 的/ 总量/ 变为/ 原来/ 的/ 2/ 倍/ ,/ 系统/ 的/ 性价比/ 较/ 低/ ./ 因此/ 多/ 采用/ RAID5/ 方式/ 提高/ 系统/ 整体/ 性价比/ ,/ 但/ 冗余/ 管理/ 的/ 问题/ 也/ 变得/ 更为/ 复杂/ ./ 冗余/ 管理/ 重点/ 解决/ 在/ 哪个/ 节点/ 如何/ 计算/ RAID/ 冗余/ 信息/ 和/ 为/ 计算/ 冗余/ 信息/ 如何/ 传输数据/ 的/ 问题/ ./ 在/ 传统/ 的/ 前端/ 集中/ 主从/ 冗余/ 管理/ 方式/ 中/ ,/ 系统/ 需要/ 有/ 一个/ 集中/ 的/ 控制点/ (/ 如/ AutoRAID/ [/ 4/ ]/ )/ ,/ 负责/ 冗余/ 计算/ 和/ 数据传输/ ,/ 这种/ 方式/ 不/ 需要/ 复杂/ 的/ 管理/ 协议/ ,/ 但/ 问题/ 是/ 为/ 避免/ 成为/ 性能/ 瓶颈/ 点/ 而/ 必须/ 配置/ 一个/ 性能/ 远高于/ 存储设备/ 节点/ 的/ 控制/ 服务器/ ,/ 系统/ 成本/ 变高/ ./ 如果/ 采用/ 分布式/ 对/ 等/ 存储管理/ 方式/ (/ 如/ RAID/ -/ x/ [/ 5/ ]/ )/ ,/ 不/ 需要/ 集中管理/ 服务器/ ,/ 任意/ 两个/ 存储设备/ 节点/ 之间/ 都/ 能/ 相互/ 并发/ 通信/ ,/ 但是/ 需要/ 采用/ 严格/ 的/ 锁/ 机制/ 提供/ 单一/ 的/ 地址/ 空间/ (/ SingleI/ // OSpaceImage/ )/ ,/ 来/ 保证/ 用户/ 数据/ 读写/ 的/ 一致性/ ./ 如果/ 在/ 应用服务器/ 上/ 计算/ RAID5/ 校验/ 块/ (/ 如/ NetRAID/ [/ 6/ ]/ )/ ,/ 再/ 将/ 数据/ 块/ 和校验/ 块/ 传输/ 到/ 存储/ 节点/ ,/ 不但/ 对/ 应用服务器/ 性能/ 影响/ 大/ ,/ 而且/ 也/ 需要/ 采用/ 锁/ 机制/ 保证/ 多个/ 服务器/ 并发/ 写/ 的/ 数据/ 一致性/ ./ 如果/ 在/ 存储设备/ 节点/ 上/ 计算/ RAID5/ 校验/ 块/ (/ 如/ ClusterRAID/ [/ 7/ ]/ )/ ,/ 数据/ 经/ 存储/ 节点/ 转发/ ,/ 用户/ 请求/ 的/ 延迟/ 较大/ ,/ 而且/ 节点/ 间/ 数据传输/ 占用/ 存储系统/ 的/ 网络带宽/ 多/ ./ 冗余/ 计算/ 和/ 数据传输/ 过多/ 占用/ 系统资源/ 而/ 影响/ 应用/ 性能/ ,/ 现有/ 的/ 冗余/ 存储管理/ 架构/ 难以/ 满足/ 系统/ 的/ 需求/ ./ 表/ 2/ 给出/ 了/ 几种/ 典型/ 存储系统/ 的/ 冗余/ 管理/ 方式/ 对比/ ./ Page11/ 系统/ 名称/ 管理/ 方式/ AutoRAID/ 前端/ 集中管理/ 控制器/ 集中/ 计算/ RAID5RAID/ -/ xBW/ -/ netRAID/ 带外/ 存储管理/ ,/ 后/ NetRAID/ 没有/ 管理/ 服务器/ 应用服务器/ 集中/ 计算/ ClusterRAID/ 没有/ 管理/ 服务器/ 存储设备/ 节点/ 集中/ 计算/ NSSM/ [/ 8/ ]/ 元/ 数据/ 集中管理/ 存储设备/ 节点/ 上/ 的/ RAIDVNS/ -/ II/ [/ 9/ ]/ 2/ 个/ 控制/ 服务器/ 在/ BW/ -/ netRAID/ 是/ 在/ 带外/ 虚拟化/ 存储管理/ 系统/ 基础/ 上/ 提出/ 的/ 新/ 的/ 后/ 端/ 集中/ 冗余/ 管理/ 的/ 存储/ 结构/ :/ (/ 1/ )/ 对于/ 冗余/ 计算/ 问题/ ,/ BW/ -/ netRAID/ 采用/ 独立/ 的/ 后/ 端/ 管理/ 服务器/ 在/ 后台/ 集中/ 计算/ ,/ 避免/ 了/ 在/ 数据通道/ 上/ 集中/ 计算/ 引起/ 的/ 性能/ 瓶颈/ 问题/ ,/ 也/ 避免/ 了/ 每次/ 读写/ 过程/ 中/ 存储设备/ 节点/ 间/ 的/ 锁/ 协议/ 开销/ 问题/ ./ (/ 2/ )/ 对于/ 数据传输/ 负荷/ 问题/ ,/ BW/ -/ netRAID/ 采用/ 在/ 后/ 端/ 冗余/ 管理/ 服务器/ 上/ 缓存/ 活跃/ 数据/ ,/ 只/ 在/ 冗余/ 计算/ 缺少/ 所/ 需/ 数据/ 时才/ 从/ 存储设备/ 节点/ 读数据/ ,/ 减少/ 了/ 存储设备/ 节点/ 的/ 过/ 多/ 输入/ 和/ 输出/ 数据/ 问题/ ,/ 以/ 少量/ 空间/ 代价/ 换取/ 到/ 更/ 高/ 的/ 性能/ 和/ 可靠性/ ./ (/ 3/ )/ 优化/ 读写/ 请求/ 处理/ ,/ 使得/ 读/ 请求/ 只/ 需/ 直接/ 访问/ 存储设备/ 节点/ ,/ 读/ 性能/ 不再/ 受限于/ 前端/ 集中/ 控制器/ 性能/ ,/ 具有/ 分布式/ 存储/ 的/ 高/ 扩展性/ ./ 而且/ 通过/ 日志/ 方式/ 在/ 磁盘/ 上/ 缓存/ 写/ 请求/ 数据/ ,/ 再/ 异步/ 计算/ 校验/ 块/ 的/ 方法/ ,/ 有效/ 地/ 提高/ 了/ 写/ 请求/ 的/ 性能/ ./ 7.3/ RAID1/ 和/ RAID5/ 异构/ 分布/ 表/ 3/ 对比/ 了/ 不同/ 存储系统/ 的/ RAID1/ 和/ RAID5/ 方式/ ./ 上述/ 系统/ 的/ 数据/ 变化/ 的/ 共同/ 特征/ 是/ 新/ 修改/ 的/ 活跃/ 数据/ 先写/ 到/ RAID1/ ,/ 不/ 活跃/ 数据/ 再/ 后台/ 迁移/ 或者/ 重新/ 计算/ 得到/ RAID5/ ,/ 数据/ 块/ 关系/ 都/ 先是/ RAID1/ 再/ 变为/ RAID5/ ./ 它们/ 之间/ 的/ 区别/ 在于/ 镜像/ 块/ 和校验/ 块/ 的/ 存储/ 方式/ ./ AutoRAID/ 采用/ 分级/ 存储/ ,/ Hotmir/ -/ roring/ [/ 10/ ]/ 采用/ 交叉/ 分布/ 存储/ 在/ 磁盘/ 不同/ 的/ 区域/ ,/ DPGADR/ [/ 11/ ]/ 和/ BW/ -/ netRAID/ 的/ 镜像/ 块/ 是/ 单独/ 存储系统/ 名称/ 存储/ 方式/ RAID1/ // RAID5/ 异构/ 分布/ AutoRAID/ 分级/ 存储/ 上层/ 逻辑/ 空间/ 是/ RAID1/ ,/ 下/ Hotmirroring/ 交叉/ 分布/ 磁盘/ 设备/ 的/ 上半部/ 用于/ DPGADR/ 镜像/ 块/ 存储/ 在/ 复/ BW/ -/ netRAID/ 镜像/ 块/ 缓存/ 在/ 冗到/ 另外/ 的/ 存储设备/ 上/ ,/ 但/ DPGADR/ 中非/ 活跃/ 数据/ 是/ 降级/ RAID5/ ,/ 而/ BW/ -/ netRAID/ 中是/ 标准/ RAID5/ ./ 7.4/ 数据/ 块/ 和校验/ 块/ 日志/ 表/ 4/ 对比/ 了/ 几种/ 数据/ 块/ 和校验/ 块/ 日志/ 技术/ ./ 通过/ 日志/ 方法/ 缓存数据/ ,/ 然后/ 后台/ 再/ 计算/ RAID5/ 是/ 上述/ 方法/ 的/ 共同/ 特征/ ./ 区别/ 在于/ 缓存/ 哪类/ 数据/ 和/ 存储/ 的/ 方式/ ./ LoggingRAID/ [/ 12/ ]/ 仅仅/ 缓存/ 了/ 新/ 写/ 的/ 数据/ 到/ 日志/ 区/ ,/ 当/ 重新/ 计算/ RAID5/ 检验/ 块/ 时/ 仍然/ 需要/ 从/ 数据/ 区读/ 回旧/ 数据/ 块/ ./ Paritylogging/ [/ 13/ ]/ 仅仅/ 缓存/ 了/ 校验/ 块/ 的/ 更新/ 数据/ ,/ Datalogging/ [/ 14/ ]/ 缓存/ 校验/ 块/ 更新/ 数据/ 或者/ 新/ 数据/ ,/ 当/ 重新/ 计算/ RAID5/ 检验/ 块/ 时/ 仍/ 需读/ 回旧/ 校验/ 块/ ./ BW/ -/ netRAID/ 用/ 磁盘/ 缓存/ 日志/ 来/ 增大/ 日志/ 区/ ,/ 不但/ 缓存/ 新/ 数据/ ,/ 而且/ 还/ 尽可能/ 地/ 缓存/ 旧/ 数据/ 和/ 旧/ 检验/ 块/ ,/ 因此/ 以/ 少量/ 空间/ 代价/ 减少/ 存储设备/ 节点/ 间/ 数据/ 重读/ 的/ 性能/ 开销/ ,/ 而且/ 也/ 提高/ 了/ 数据/ 可靠性/ ./ LoggingRAID/ 旧/ 数据/ 块/ 新/ 数据/ 块/ Paritylogging/ 新/ 数据/ 块/ 新/ 数据/ 块/ 与/ 旧/ 数据/ 块/ 的/ 异或/ 数据/ 块/ Datalogging/ 新/ 数据/ 块/ 旧/ 数据/ 块/ ,/ 新/ 数据/ 块/ BW/ -/ netRAID/ 新/ 数据/ 块/ 旧/ 数据/ 块/ ,/ 新/ 数据/ 块/ ,/ 旧/ 校验/ 块/ 8/ 结论/ 本文/ 提出/ 了/ 一种/ 新/ 的/ 后/ 端/ 集中/ 冗余/ 管理/ 的/ 网络/ RAID/ 存储系统/ ./ 通过/ 带外/ 冗余/ 管理/ 的/ 方式/ 解决/ 了/ 前端/ 冗余/ 管理/ 的/ 性能/ 瓶颈/ 问题/ ,/ 并且/ 系统/ 根据/ 数据/ 更新/ 的/ 活跃度/ 采用/ RAID1/ 和/ RAID5/ 异构/ 分布/ 以及/ 活跃/ 数据/ 日志/ 缓存/ 的/ 方法/ ,/ 以/ 数据/ 缓存/ 的/ 空间/ 代价/ 换取/ 系统/ 的/ 整体/ 性能/ 和/ 可靠性/ 提升/ ./ 在/ 目前/ 工作/ 的/ 基础/ 上/ 我们/ 还/ 将/ 在/ 以下/ 3/ 个/ 方面/ 继续/ 深入研究/ :/ (/ 1/ )/ 数据/ 读写/ 和/ 冗余/ 管理/ 分离/ 的/ 分布式/ 网络/ RAID/ 系统/ ;/ (/ 2/ )/ 借助/ 工作/ 负载/ 特征分析/ ,/ 改进/ 活跃/ 数据/ 缓存/ 和/ 资源/ 回收/ 算法/ ,/ 根据/ 访问/ 模式/ 调整/ RAID1/ // RAID5/ 的/ 数据/ 块/ 分布/ ;/ (/ 3/ )/ 支持/ 多重/ 数据/ 冗余/ 功能/ ,/ 能够/ 容忍/ 多个/ 存储设备/ 节点/ 故障/ ./ 

