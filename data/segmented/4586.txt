Page1/ 基于/ 增量/ 式/ 分区/ 策略/ 的/ MapReduce/ 数据/ 均衡/ 方法/ 王卓/ 陈群/ 李战怀/ 潘巍/ 尤立/ (/ 西北工业大学/ 计算机/ 学院/ 西安/ 710072/ )/ 摘要/ MapReduce/ 以其/ 简洁/ 的/ 编程/ 模型/ ,/ 被/ 广泛应用/ 于/ 大规模/ 和/ 高/ 维度/ 数据/ 集/ 的/ 处理/ ,/ 如/ 日志/ 分析/ 、/ 文档/ 聚类/ 和/ 其他/ 数据分析/ ./ 开源/ 系统/ Hadoop/ 很/ 好/ 地/ 实现/ 了/ MapReduce/ 模型/ ,/ 但/ 由于/ 自身/ 采用/ 一次/ 分区/ 机制/ ,/ 即/ 通过/ Hash/ // Range/ 分区/ 函数/ 对/ 数据/ 进行/ 一次/ 划分/ ,/ 导致/ 在/ 处理/ 密集/ 数据/ 时/ ,/ Reduce/ 端/ 常会/ 出现/ 数据/ 倾斜/ 的/ 问题/ ./ 虽然/ 系统/ 为/ 用户/ 提供/ 了/ 自定义/ 分区/ 函数/ 方法/ ,/ 但/ 不幸/ 的/ 是/ 在/ 不/ 清楚/ 输入/ 数据分布/ 的/ 情况/ 下/ ,/ 数据/ 倾斜/ 问题/ 很难/ 被/ 避免/ ./ 为/ 解决/ 数据/ 划分/ 的/ 不/ 均衡/ ,/ 该文/ 提出/ 一种/ 将/ 分区/ 向/ Reducer/ 指派/ 时/ 按照/ 多轮/ 分配/ 的/ 分区/ 策略/ ./ 该/ 方法/ 首先/ 在/ Map/ 端/ 产生/ 多于/ Reducer/ 个数/ 的/ 细粒度/ 分区/ ,/ 同时/ 在/ Mapper/ 运行/ 过程/ 中/ 实时/ 统计/ 各/ 细粒度/ 分区/ 的/ 数据量/ ;/ 然后/ 由/ JobTracker/ 根据/ 全局/ 的/ 分区/ 分布/ 信息/ 筛选/ 出/ 部分/ 未/ 分配/ 的/ 细粒度/ 分区/ ,/ 并用/ 代价/ 评估/ 模型/ 将/ 选中/ 的/ 细粒度/ 分区/ 分配/ 到/ 各/ Reducer/ 上/ ;/ 依照/ 此/ 方法/ ,/ 经过/ 多轮/ 的/ 筛选/ 、/ 分配/ ,/ 最终/ 在/ 执行/ Reduce/ (/ )/ 函数/ 前/ ,/ 将/ 所有/ 细粒度/ 分区/ 分配/ 到/ Reduce/ 端/ ,/ 以此/ 解决/ 分区/ 后/ 各/ Reducer/ 接收数据/ 总量/ 均衡/ 的/ 问题/ ./ 最后/ 在/ Zipf/ 分布/ 数据/ 集/ 和/ 真实/ 数据/ 集上/ 与/ 现有/ 的/ 分区/ 切分/ 方法/ Closer/ 进行/ 了/ 对比/ ,/ 增量/ 式/ 分区/ 策略/ 更/ 好地解决/ 了/ 数据/ 划分/ 后/ 的/ 均衡/ 问题/ ./ 关键词/ 增量/ 分配/ ;/ 细粒度/ 分区/ ;/ 数据/ 倾斜/ ;/ 均衡/ 分区/ ;/ MapReduce/ ;/ 大/ 数据/ 1/ 引言/ 近些年/ ,/ 以/ MapReduce/ [/ 1/ ]/ 编程/ 模型/ 为/ 基础/ 的/ 分布式系统/ 在/ 传统/ 行业/ 的/ 日常/ 业务/ 处理/ 中/ 被/ 广泛/ 地/ 应用/ ,/ 如/ 通信/ 行业/ 的/ 用户/ 挖掘/ 业务/ ,/ 保险行业/ 的/ 风险/ 分析/ 业务/ 等/ ./ 特别/ 是/ Apache/ 的/ 开源/ 项目/ Hadoop/ 将/ MapReduce/ 计算/ 架构/ 和/ HDFS/ 分布/ 文件系统/ 进行/ 完美/ 的/ 融合/ ,/ 使/ 学术界/ 和/ 工业界/ 对/ MapReduce/ 的/ 研究/ 和/ 应用/ 都/ 产生/ 了/ 极大/ 热情/ [/ 3/ -/ 4/ ]/ ./ MapReduce/ 模型/ 将/ 计算/ 分为/ Map/ 和/ Reduce/ 两个/ 阶段/ ./ 在/ Map/ 阶段/ ,/ 由/ 各/ Mapper/ 对/ 输入/ 元组/ 进行/ 分区/ 处理/ ,/ 并/ 将/ 具有/ 相同/ 分区/ 值/ 的/ 元组/ 传给/ 同一个/ Reducer/ 进行/ 相关/ 的/ 计算/ ./ Hadoop/ 系统/ 默认/ 采用/ 的/ 是/ Hash/ 分区/ 法/ ,/ 同时/ 支持/ Range/ 和/ 用户/ 自定义/ 分区/ 法/ ,/ 但/ 采用/ 的/ 都/ 是/ 一次/ 分区/ 机制/ ,/ 即/ 对/ 元组/ 仅/ 进行/ 一次/ 划分/ ,/ 并且/ 采用/ 分区/ 与/ Reducer/ 一一对应/ 的/ 随机/ 指派/ 策略/ ./ 对于/ 均匀分布/ 的/ 数据/ 集/ ,/ 该/ 分区/ 机制/ 能/ 很/ 好/ 地/ 实现/ 各/ Reducer/ 接收数据/ 的/ 均衡性/ ,/ 但/ 对于/ 个别/ 值/ 密集/ 的/ 倾斜/ 数据/ ,/ 默认/ 分区/ 方法/ 很难/ 完成/ 对/ 数据/ 的/ 一次性/ 均匀/ 划分/ ,/ 即使/ 采用/ 自定义/ 分区/ 函数/ ,/ 在/ 没有/ 掌握/ 数据分布/ 的/ 情况/ 下/ ,/ 也/ 很/ 难/ 避免/ 数据/ 倾斜/ ./ 文献/ [/ 5/ -/ 6/ ]/ 指出/ 若/ 采用/ 默认/ 的/ 分区/ 方法/ ,/ 数据/ 划分/ 的/ 不/ 均衡/ 将/ 是/ 一个/ 普遍/ 的/ 现象/ ./ 而/ 一旦/ 发生/ 数据/ 倾斜/ ,/ 势必会/ 造成/ Reduce/ 端/ 运行/ 的/ 不/ 均衡/ ,/ 从而/ 影响/ 整个/ 作业/ 的/ 运行/ 时间/ ./ 文献/ [/ 7/ ]/ 通过/ 大量/ 实验/ 发现/ 采用/ 默认/ 的/ Hash/ 分区/ 方法/ ,/ 在/ 92/ %/ 的/ 任务/ 中/ 出现/ 了/ Reducer/ 运行/ 的/ 不/ 均衡/ ,/ 而/ 这些/ Reducer/ 的/ 运行/ 时间/ 一般/ 都/ 高于/ 正常/ 任务/ 22/ %/ 和/ 38/ %/ ./ 由此可见/ ,/ 数据/ 分区/ 的/ 均衡性/ 对/ MapReduce/ 作业/ 的/ 性能/ 有/ 显著/ 影响/ ,/ 如何/ 进行/ 分区/ 策略/ 的/ 制订/ 已/ 成为/ 近年来/ 研究/ 的/ 热点/ ./ 为/ 解决/ 原/ MapReduce/ 自身/ 一次/ 分区/ 生成/ 机制/ 带来/ 的/ 弊端/ ,/ 研究者/ 们/ 提出/ 了/ 两/ 阶段/ 分区/ 机制/ ,/ 即/ 首先/ 按照/ 原/ 分区/ 机制/ 生成/ 数据/ 分区/ ,/ 然后/ 对/ 数据量/ 发生/ 倾斜/ 的/ 分区/ 进行/ 一次/ 调整/ ./ 分区/ 调整/ 策略/ 如/ 文献/ [/ 8/ ]/ 中/ 的/ 方法/ :/ 在/ Mapper/ 运行/ 中/ 加入/ 采样/ 策略/ ,/ 当/ Mapper/ 运行/ 到/ 特定/ 时刻/ ,/ 根据/ 采样/ 所/ 得到/ 的/ 分布/ 信息/ ,/ 对/ 发生/ 数据量/ 倾斜/ 的/ 分区/ 进行/ 一次/ 拆分/ ,/ 然后/ 在/ 保证数据/ 一致/ 的/ 基础/ 上将/ 拆分/ 后/ 的/ 分区/ 与/ 数据量/ 较少/ 的/ 分区/ 进行/ 合并/ ,/ 从而/ 产生/ 均衡/ 分区/ ./ 该/ 方法/ 的/ 关键/ 是/ 何时/ 对/ 倾斜/ 分区/ 进行/ 拆分/ ,/ 早/ 拆分/ 势必会/ 增大/ 采样/ 误差/ ,/ 而/ 过/ 晚/ 调整/ 又/ 会/ 延迟/ 数据/ 从/ Map/ 端到/ Reduce/ 端的/ 传输/ 时机/ ,/ 对于/ 不同/ 的/ 数据分布/ ,/ 该/ 方法/ 并/ 没有/ 给出/ 一个/ 通用/ 的/ 最优/ 调整/ 时机/ ./ 文献/ [/ 9/ ]/ 提出/ 了/ 一种/ 先/ 预处理/ 输入/ 数据/ 再/ 制定/ 分区/ 策略/ 的/ 方法/ ,/ 即/ 在/ 运行/ 用户/ 的/ 作业/ 前/ ,/ 启动/ 一个/ 采样/ 任务/ 负责/ 调整/ 默认/ 的/ 分区/ 策略/ ,/ 然后/ 使用/ 制定/ 的/ 均衡/ 分区/ 策略/ 来/ 运行/ 用户/ 作业/ ./ 但是/ ,/ 这种/ 方法/ 需要/ 增加/ 一次/ 对于/ 输入/ 数据/ 集/ 的/ 访问/ ,/ 这/ 也/ 就/ 增大/ 了/ 文件/ 访问/ 和/ 数据传输/ 开销/ ,/ 从而/ 造成/ 繁忙/ 集群/ 中/ 的/ 资源/ 浪费/ 现象/ ./ 由此可见/ ,/ 一次/ 分区/ 生成/ 和/ 一次/ 分区/ 调整机制/ ,/ 在/ 处理/ 分布/ 类型/ 复杂/ 的/ 多类/ 数据/ 集时/ 往往/ 存在/ 一定/ 的/ 局限性/ ./ 本文/ 提出/ 一种/ 更/ 高效/ 的/ 增量/ 式/ 分区/ 分配/ 策略/ ,/ 该/ 策略/ 通过/ 在/ Mapper/ 运行/ 过程/ 中/ ,/ 以/ 渐进/ 的/ 方式/ 将/ 分区/ 分配/ 到/ Reduce/ 端/ ,/ 来/ 解决/ 数据/ 划分/ 的/ 不/ 均衡/ 问题/ ./ 该/ 方法/ 首先/ 在/ Mapper/ 运行/ 阶段/ 产生/ 多于/ Reducer/ 个数/ 的/ 细粒度/ 分区/ ,/ 并/ 在/ Mapper/ 运行/ 过程/ 中/ 实时/ 统计/ 各/ 分区/ 的/ 数据量/ ,/ 然后/ 由/ JobTracker/ 根据/ 统计/ 结果/ 筛选/ 出/ 部分/ 细粒度/ 分区/ 向/ 各/ Reducer/ 进行/ 分配/ ./ 伴随/ 着/ Mapper/ 运行/ ,/ 已/ 分配/ 分区/ 的/ 数据量/ 在/ Reduce/ 端/ 一定/ 会/ 发生变化/ ,/ 并/ 有/ 可能/ 造成/ 部分/ Reducer/ 接收/ 的/ 数据量/ 发生/ 倾斜/ ./ 此时/ ,/ 再/ 由/ Job/ -/ Tracker/ 筛选/ 出/ 部分/ 未/ 分配/ 的/ 细粒度/ 分区/ 进行/ 分配/ ,/ 以此/ 调整/ 发生/ 倾斜/ 的/ Reducer/ 分区/ ./ 经过/ 对/ 未/ 分配/ 分区/ 的/ 多轮/ 筛选/ 和/ 分配/ ,/ 最终/ 确保/ 在/ Mapper/ 结束/ 前/ 将/ 所有/ 的/ 细粒度/ 分区/ 都/ 分配/ 到/ 各/ Reducer/ 节点/ ,/ 从而/ 解决/ 各/ Reducer/ 接收数据/ 总量/ 均衡/ 的/ 问题/ ./ 对/ 各/ Reducer/ 而言/ ,/ 只/ 需/ 在/ 执行/ Reduce/ (/ )/ 函数/ 前/ ,/ 将/ 分配/ 给/ 自身/ 的/ 细粒度/ 分区/ 进行/ 合并/ ,/ 并/ 以/ 合并/ 后/ 的/ 分区/ 作为/ Reduce/ (/ )/ 函数/ 的/ 输入/ 分区/ 进行/ 处理/ 即可/ ./ 本文/ 的/ 主要/ 贡献/ 包括/ :/ (/ 1/ )/ 提出/ 增量/ 式/ 分区/ 调整/ 策略/ ,/ 该/ 方法/ 通过/ 在/ Mapper/ 运行/ 过程/ 中/ ,/ 多/ 轮次/ 地/ 将/ 细粒度/ 分区/ 重组/ 为/ 均衡/ 的/ Reducer/ 输入/ 分区/ ,/ 来/ 解决/ 分区/ 后/ 数据/ 划分/ 均衡/ 的/ 问题/ ./ (/ 2/ )/ 提出/ 分区/ 的/ 代价/ 评估/ 模型/ ,/ 该/ 模型/ 以/ 各/ Mapper/ 运行/ 过程/ 中/ 统计/ 的/ 分区/ 分布/ 为/ 输入/ ,/ 实时/ 地/ 对/ 各/ 分区/ 进行/ 负载/ 评估/ 和/ 记录/ ,/ 经过/ 多轮/ 的/ 筛选/ 和/ 分配/ ,/ 完成/ 将/ 所有/ 分区/ 向/ Reduce/ 端的/ 分配/ 工作/ ./ (/ 3/ )/ 由于/ 将/ 分区/ 分配/ 到/ Reducer/ 的/ 计算/ 问题/ 属于/ NP/ -/ Hard/ ,/ 本文/ 提出/ 一种/ 复杂度/ 为/ O/ (/ mlogm/ )/ 的/ 启发式/ 算法/ ,/ 能/ 高效/ 地/ 解决/ 各/ Reducer/ 接收数据/ 总量/ 均衡/ 的/ 问题/ ,/ 其中/ m/ 为/ 用户/ 所/ 定义/ 的/ Reducer/ 个数/ ./ (/ 4/ )/ 针对/ 模拟/ 数据/ 集/ 和/ 真实/ 数据/ 集做/ 了/ 大量/ 实验/ ,/ 对/ 一次/ 分区/ 方法/ 和/ 本文/ 的/ 多轮/ 分区/ 分配/ 方法/ 进/ Page3/ 行/ 了/ 对比/ ,/ 实验/ 结果表明/ ,/ 多轮/ 分区/ 分配/ 方法/ 能/ 使/ Hadoop/ 有/ 稳定/ 的/ 性能/ 提升/ ./ 本文/ 第/ 2/ 节/ 主要/ 介绍/ MapReduce/ 数据/ 划分/ 均衡/ 的/ 相关/ 工作/ ;/ 第/ 3/ 节/ 首先/ 介绍/ Hadoop/ 的/ 默认/ 分区/ 策略/ ,/ 然后/ 阐述/ 增量/ 式/ 分区/ 机制/ 的/ 处理/ 流程/ ;/ 第/ 4/ 节/ 抽象/ 增量/ 式/ 分区/ 的/ 计算/ 模型/ ,/ 给出/ 增量/ 式/ 分区/ 分配/ 的/ 代价/ 模型/ 和/ 实现/ 算法/ ;/ 第/ 5/ 节/ 给出/ 增量/ 式/ 分区/ 在/ Hadoop/ 系统/ 上/ 的/ 实现/ 技术/ 及/ 实现/ 细节/ ;/ 第/ 6/ 节在/ 标准/ Zipf/ 分布/ 数据/ 集/ 和/ 真实/ 数据/ 集上/ ,/ 与/ 切割/ 分区/ 的/ Closer/ 方法/ 进行/ 对比/ 实验/ ,/ 验证/ 该/ 方法/ 的/ 有效性/ ./ 最后/ 是/ 对/ 本文/ 的/ 总结/ 及/ 概述/ 未来/ 的/ 研究/ 方向/ 和/ 问题/ ./ 2/ 相关/ 工作/ 自/ MapReduce/ 模型/ 提出/ 后/ ,/ 学者/ 们/ 就/ 开始/ 关注/ 如何/ 才能/ 解决/ 各/ Reducer/ 节点/ 接收数据/ 总量/ 均衡/ 的/ 问题/ ./ 在/ 提出/ MapReduce/ 模型/ 的/ 文献/ [/ 1/ ]/ 中/ ,/ Dean/ 等/ 人/ 采用/ 通用/ 的/ Hash/ 函数/ 对/ 数据/ 进行/ 简单/ 的/ 一次/ 划分/ ,/ 由于/ 该/ 方法/ 实现/ 简单/ 和/ 具有/ 通用性/ ,/ 所以/ 在/ Apache/ 实现/ 的/ Hadoop/ 系统/ 上/ 被/ 定义/ 为/ 默认/ 分区/ 方法/ ,/ 但/ 一次/ Hash/ 划分/ 很难/ 实现/ 在/ 处理/ 密集/ 数据/ 时/ 各/ 分区/ 的/ 均衡性/ ./ 随后/ 各/ 研究/ 机构/ 开始/ 对原/ 分区/ 机制/ 进行/ 优化/ ,/ 提出/ 多种/ 分区/ 划分/ 方法/ ./ 华盛顿大学/ Kwon/ 等/ 人/ [/ 10/ ]/ 首先/ 以/ MapReduce/ 运行/ 的/ 两个/ 阶段/ 为/ 划分/ 标准/ ,/ 将/ Map/ 和/ Reduce/ 各/ 阶段/ 可能/ 会/ 产生/ 倾斜/ 的/ 原因/ 进行/ 分析/ ,/ 概括/ 出/ 5/ 种/ 倾斜/ 的/ 分类/ 方法/ ./ 随后/ 为/ 解决/ 划分/ 分区/ 的/ 不/ 均衡/ 问题/ ,/ 提出/ SkewReduce/ 策略/ [/ 11/ ]/ ,/ 该/ 方法/ 利用/ 用户/ 定义/ 代价/ 模型/ 来/ 评估/ 分区/ 的/ 大小/ ,/ 并/ 当/ 整个/ 任务/ 运行/ 到/ 特定/ 时机/ 后/ 开始/ 制定/ 分区/ 策略/ ./ 该/ 策略/ 是/ 以/ 延迟/ 数据/ 的/ 传输/ 为/ 代价/ 来/ 解决/ 各/ Reducer/ 接收数据/ 均衡性/ 的/ 问题/ ./ Ramakrishnan/ 等/ 人/ [/ 12/ ]/ 提出/ 一种/ 基于/ 采样/ 的/ 分区/ 划分/ 方法/ ,/ 该/ 方法/ 通过/ 在/ Mapper/ 中/ 新增/ 一个/ 采样/ 进程/ 来/ 获取数据/ 分布/ ,/ 当/ 采样/ 完成/ 一定/ 比例/ 后/ 开始/ 对/ 产生/ 的/ 分区/ 进行/ 拆分/ 和/ 重组/ ./ 与/ 该/ 方法/ 类似/ 的/ 是/ Gufler/ 等/ 人/ [/ 8/ ,/ 13/ ]/ 提出/ 的/ Closer/ 系统/ ,/ 该/ 系统/ 直接/ 将/ 采样/ 函数/ 增加/ 到/ Mapper/ 中/ ,/ 避免/ 了/ 不必要/ 的/ 通信/ 开销/ ,/ 当/ 整个/ Map/ 阶段/ 完成/ 到/ 一定/ 比例/ 后/ ,/ 开始/ 对/ 分区/ 进行/ 拆分/ 和/ 重组/ ,/ 从而/ 再/ 开始/ 传输数据/ ./ 这/ 两种/ 方法/ 都/ 是/ 用/ 原/ Hash/ 方法/ 生成/ 分区/ ,/ 然后/ 在/ 采样/ 完成/ 一定/ 比例/ 后/ 开始/ 对/ 分区/ 进行/ 一次/ 调整/ ,/ 在/ 调整/ 后/ 如果/ 该/ 分区/ 再次发生/ 倾斜/ ,/ 则/ 无法/ 进一步/ 调整/ ./ 因此/ ,/ 对于/ 这/ 两种/ 方法/ ,/ 调整/ 时机/ 将/ 直接/ 影响/ 着/ 划分/ 均衡性/ 的/ 效果/ ,/ 过早/ 进行/ 调整/ 将会/ 造成/ 较大/ 的/ 采样/ 误差/ ,/ 从而/ 降低/ 划分/ 的/ 准确性/ ;/ 而/ 过/ 晚/ 调整/ 又/ 会/ 延迟/ 数据/ 的/ 传输/ 时机/ ,/ 从而/ 增加/ 整个/ 作业/ 的/ 运行/ 时间/ ./ 以上/ 是/ 基于/ 元组/ 的/ 采样/ ,/ Kolb/ 等/ 人/ [/ 14/ -/ 15/ ]/ 提出/ 一种/ 基于/ 数据/ 块/ (/ Block/ )/ 的/ 采样/ 和/ 切分/ 方法/ ,/ 该/ 方法/ 将/ 原来/ 〈/ key/ ,/ value/ 〉/ 形式/ 的/ 键值/ 对/ 转为/ 〈/ blocking/ _/ key/ ,/ entity/ 〉/ 形式/ ,/ 并/ 以块/ 的/ 大小/ 为/ 单位/ 进行/ 评估/ 和/ 拆分/ ,/ 与/ 基于/ 元组/ 采样/ 类似/ ,/ 该/ 方法/ 采用/ 的/ 仍旧/ 是/ 一次/ 拆分/ ,/ 仍/ 没有/ 解决/ 如何/ 定义/ 拆分/ 时机/ 的/ 问题/ ./ Racha/ [/ 9/ ]/ 提出/ 一种/ 先/ 产生/ 均衡/ 分区/ 函数/ ,/ 然后/ 再/ 运行/ 用户/ 任务/ 的/ 两/ 阶段/ 分区/ 方法/ ./ 该/ 方法/ 将/ 原来/ 的/ 一轮/ 任务/ 拆/ 分为/ 两轮/ :/ 第/ 1/ 轮/ 任务/ 是/ 通过/ 对/ 输入/ 数据/ 的/ 采样/ 来/ 完成/ 分区/ 函数/ 的/ 制定/ ,/ 主要/ 工作/ 是/ 对/ 输入/ 数据/ 进行/ 25/ %/ 的/ 随机/ 采样/ ,/ 通过/ 分析/ 采样/ 数据/ 得出/ 输入/ 数据/ 的/ 分布/ ,/ 从而/ 制定/ 出/ 满足/ 均衡性/ 划分/ 的/ 分区/ 函数/ ;/ 第/ 2/ 轮/ 任务/ 是/ 直接/ 应用/ 已/ 产生/ 的/ 均衡/ 分区/ 函数/ 来/ 执行/ 用户/ 所/ 提交/ 的/ 作业/ ./ 耿玉娇/ [/ 16/ ]/ 对/ 前人/ 的/ 工作/ 进行/ 了/ 总结/ ,/ 提出/ 了/ 切分/ 分区/ 和/ 组合/ 分区/ 的/ 两种/ 方法/ ,/ 两种/ 方法/ 都/ 是/ 在/ 作业/ 运行/ 前/ 另/ 起/ 一个/ 采样/ 任务/ ,/ 然后/ 根据/ 采样/ 结果/ 对/ 分区/ 进行/ 一次/ 调整/ ./ 两轮/ 作业/ 方法/ 是/ 以/ 增加/ 获取数据/ 分布/ 信息/ 的/ 代价/ 来/ 实现/ 数据/ 划分/ 的/ 均衡性/ ,/ 但/ 该/ 方法/ 在/ 处理/ 短/ 作业/ 或/ 输入/ 数据/ 经常/ 发生/ 更新/ 的/ 作业/ 时/ 就/ 会/ 存在/ 一定/ 的/ 局限性/ ./ 周家/ 帅/ [/ 17/ ]/ 通过/ 分析/ Mapper/ 产生/ 的/ 中间/ 结果/ 来/ 制定/ 分区/ 策略/ ,/ 该/ 方法/ 首先/ 对/ Map/ 函数/ 完成/ 后/ 所/ 产生/ 的/ 临时/ 结果/ 采样/ ,/ 然后/ 根据/ 中间/ 结果/ 的/ 分布/ 情况/ 来/ 决定/ 启动/ 多少/ Reducer/ 是/ 均衡/ 的/ 分区/ 方案/ ,/ 该/ 方法/ 以/ 延迟/ Reduce/ 的/ 启动/ 为/ 代价/ 来/ 实现/ 负载/ 均衡/ ./ 也/ 有/ 研究者/ ,/ 如/ Kwon/ 等/ 人/ [/ 18/ ]/ 提出/ 一种/ 对/ 任务/ 运行/ 过程/ 的/ 均衡/ 调度/ 方法/ SkewTune/ ./ 该/ 方法/ 通过/ 对/ 正在/ 运行/ 的/ Reducer/ 建立/ 剩余/ 运行/ 代价/ 的/ 估计/ 模型/ 来/ 实现/ 集群/ 中/ 整体/ 节点/ 运行/ 时/ 的/ 均衡性/ ,/ 当有/ Reducer/ 完成/ 时/ ,/ 对/ 未/ 完成/ 的/ Reducer/ 进行/ 代价/ 评估/ ,/ 并/ 将/ 该/ 节点/ 上/ 未/ 被/ 处理/ 的/ 数据/ 迁移/ 到/ 已/ 完成/ 任务/ 的/ 节点/ 上/ ,/ 从而/ 在/ 运行/ 过程/ 中使/ 较长/ 任务/ 的/ 数据/ 能/ 被/ 传输/ 到/ 空闲/ 节点/ 上/ 处理/ ,/ 以此/ 实现/ 各/ 节点/ 运行/ 过程/ 中/ 的/ 均衡性/ ./ 该/ 方法/ 不/ 考虑/ Reducer/ 运行/ 前/ 数据/ 划分/ 的/ 均衡/ 问题/ ,/ 而是/ 通过/ 控制/ 各/ Reducer/ 运行/ 过程/ 中/ 的/ 计算/ 均衡/ 来/ 实现/ 均衡/ 效果/ ,/ 该/ 方法/ 与/ 直接/ 制定/ 分区/ 函数/ 来/ 实现/ 数据/ 均衡性/ 的/ 方法/ 相比/ ,/ 需要/ 在/ Reducer/ 运行/ 过程/ 中/ 额外/ 增加/ 数据/ 的/ 传输/ 代价/ ./ 现有/ 关于/ MapReduce/ 负载/ 均衡/ 的/ 研究/ 都/ 是/ 将/ 同一/ key/ 值/ 的/ 所有/ 元组/ 作为/ 一个/ 整体/ 进行/ 处理/ ,/ 采/ Page4/ 用/ 的/ 方法/ 都/ 是/ 通过/ 调整/ 分区/ 大小/ 来/ 解决/ 各/ Reducer/ 运行/ 过程/ 中/ 的/ 均衡/ 问题/ ./ 而/ 对于/ 出现/ 某/ 一个/ key/ 的/ 数据量/ 远/ 多于/ 其他/ 所有/ key/ 值/ 的/ 极端/ 情况/ ,/ MapReduce/ 编程/ 机制/ 是/ 无法/ 对/ 该/ key/ 进行/ 拆分/ 处理/ 的/ ./ 为/ 解决/ 这种/ 极端/ 情况/ ,/ 在/ 实际/ 的/ 应用/ 中/ 用户/ 一般/ 通过/ 控制算法/ 来/ 解决/ ,/ 即/ 通过/ 选择/ 记录/ 的/ 其他/ 属性/ 作为/ key/ 值/ 进行/ 分区/ 处理/ ,/ 从而/ 避免/ 产生/ 极端/ key/ 值/ 的/ 情况/ ./ 3/ 增量/ 式/ 分区/ 策略/ 本/ 节/ 首先/ 介绍/ Hadoop/ 平台/ 上/ 默认/ 的/ 分区/ 策略/ ,/ 然后/ 阐述/ 增量/ 式/ 分区/ 策略/ 在/ 该/ 平台/ 上/ 的/ 处理/ 流程/ ./ 3.1/ Hadoop/ 默认/ 分区/ 策略/ Hadoop/ 实现/ 的/ MapReduce/ 流程/ ,/ 主要/ 可/ 分为/ 2/ 个/ 步骤/ :/ Map/ 和/ Reduce/ ./ Map/ 阶段/ :/ 用户/ 提交/ 作业/ 后/ ,/ Hadoop/ 系统/ 根据/ Block/ 块/ 的/ 大小/ 将/ 输入/ 数据/ 进行/ 拆分/ ,/ 同时/ 按照/ Block/ 与/ Mapper/ 一一对应/ 的/ 关系/ 启动/ 相应/ 个数/ 的/ Mapper/ ./ 各/ Mapper/ 将/ HDFS/ 上/ 的/ 输入/ 数据/ 以/ InputSplit/ 为/ 单位/ 的/ 形式/ 加入/ 到/ 本地/ 节点/ ,/ 然后/ 通过/ Map/ (/ )/ 函数/ 将/ 各/ 元组/ 以/ 记录/ 为/ 单位/ 转换/ 为/ 〈/ key/ ,/ value/ 〉/ 形式/ 的/ 键值/ 对/ ,/ 并/ 将/ 键值/ 对/ 写入/ Buffer/ 中/ ,/ 当/ Buffer/ 到达/ 一定/ 阈值/ 时/ ,/ 溢写/ 整个/ Buffer/ 中/ 的/ 内容/ 到/ 本地/ 磁盘/ 图/ 1/ 增量/ 式/ 分区/ 的/ MapReduce/ 处理/ 流程/ 并/ 以/ 一个/ 临时文件/ 的/ 形式/ 进行/ 存储/ ,/ 在/ 溢/ 写/ 的/ 同时/ ,/ 将/ 该/ Buffer/ 中/ 的/ 所有/ 元组/ 以/ key/ 为/ 关键字/ 按照/ 分区/ 函数/ 进行/ 划分/ ,/ 默认/ 采用/ Hash/ 函数/ ,/ 划分/ 方法/ 为/ has/ (/ key/ )/ mod/ (/ R/ )/ ,/ 其中/ R/ 为/ Reducer/ 个数/ ./ 经/ Hash/ 函数/ 计算/ 后/ ,/ 将/ 具有/ 相同/ 哈希/ 值/ 的/ key/ 作为/ 一个/ 分区/ 溢写/ 到/ 本地/ 磁盘/ ;/ 当该/ Mapper/ 完成/ 后/ ,/ 将/ 产生/ 的/ 所有/ 溢写/ 文件/ 按照/ 分区/ 进行/ 合并/ (/ Merger/ )/ ,/ 并/ 最终/ 将/ 该/ Mapper/ 的/ 所有/ 输出/ 存放/ 到/ 本地/ 的/ 一个/ 临时文件/ 中/ ./ 同时/ 为/ 确保/ 各/ Reducer/ 能/ 快速/ 索引/ 到/ 自身/ 分区/ 的/ 位置/ ,/ 系统/ 会/ 将/ 该/ 临时文件/ 分为/ R/ 块/ ,/ 即/ 每/ 一个/ 分区/ 作为/ 一个/ 块/ ,/ 同时/ 为/ 每个/ 分区/ 都/ 建立/ 一个/ 索引/ ,/ 以此/ 实现/ 各/ Reducer/ 对/ 自身/ 分区/ 的/ 快速/ 定位/ ./ Reduce/ 阶段/ :/ 在/ 原/ 机制/ 中/ ,/ 各/ Reducer/ 初始化/ 时/ 就/ 已经/ 决定/ 了/ 自身/ 所/ 要/ 处理/ 的/ 对应/ 分区/ ,/ 并且/ 一个/ Reducer/ 只能/ 处理/ 一个/ 分区/ ./ Reducer/ 初始化/ 完成/ 后/ 就/ 开始/ 等待/ 从/ Mapper/ 端/ 读取数据/ ,/ 当有/ Mapper/ 完成/ 后/ ,/ 所有/ 已/ 启动/ 的/ Reducer/ 便/ 同时/ 开始/ 从/ 该/ 节点/ 读取/ 属于/ 自身/ 的/ 分区/ 数据/ ,/ 由于/ Mapper/ 输出/ 的/ 临时文件/ 中/ 带有/ 各/ 分区/ 的/ 索引/ ,/ 因此/ 各/ Reducer/ 能/ 快速/ 读取/ 对应/ 于/ 自身/ 的/ 分区/ 数据/ ,/ 在/ 从/ Mapper/ 端/ 读取数据/ 后/ ,/ 各/ Reducer/ 会/ 将/ 读取/ 的/ 数据/ 放入/ 到/ 本地/ 磁盘/ 以/ 等待/ 所有/ Mapper/ 的/ 完成/ ./ 在/ 所有/ Mapper/ 完成/ 后/ ,/ Reducer/ 便/ 可/ 获取/ 各自/ 分区/ 的/ 全部/ 数据/ ,/ 然后/ 对/ 所有/ 读取/ 的/ 文件/ 进行/ 合并/ (/ Merger/ )/ 、/ 排序/ (/ Sort/ )/ 后/ ,/ 就/ 开始/ 执行/ Reduce/ (/ )/ 函数/ ,/ 并/ 最终/ 将/ 结果/ 输出/ Page5/ 到/ HDFS/ 上/ ,/ 完成/ 整个/ 作业/ ./ 通过/ 分析/ Hadoop/ 系统/ 上/ 默认/ 的/ MapReduce/ 处理/ 流程/ 可以/ 发现/ ,/ 分区/ 的/ 目的/ 是/ 为了/ 使/ 各/ Reducer/ 能够/ 接收/ 相等/ 数量/ 的/ 数据/ ,/ Hadoop/ 系统/ 所/ 实现/ 的/ 分区/ 是/ 简单/ 的/ 一次/ Hash/ 划分/ ,/ 并且/ 严格/ 保证/ 一个/ 分区/ 被/ 一个/ Reducer/ 处理/ ./ 3.2/ 增量/ 式/ 分区/ 策略/ 在/ Hadoop/ 上/ 的/ 处理/ 流程/ 本文/ 的/ 方法/ 在/ 处理/ key/ 时/ 与/ 原/ 机制/ 相同/ ,/ 同样/ 将/ 一个/ key/ 作为/ 一个/ 整体/ 进行/ 处理/ ,/ 但/ 对/ 原/ 机制/ 进行/ 两处/ 修改/ :/ 一是/ 更改/ Reducer/ 与/ 分区/ 建立/ 关系/ 的/ 时机/ ;/ 二是/ 将/ 分区/ 与/ Reducer/ 一对一/ 的/ 关系/ 更/ 改为/ 多/ 对/ 一/ 的/ 关系/ ./ 本/ 小节/ 提出/ 了/ 增量/ 式/ 分区/ 策略/ ,/ 并/ 给出/ 增量/ 式/ 分区/ 策略/ 的/ 一般/ 处理/ 流程/ ./ 图/ 1/ 为/ 增量/ 式/ 分区/ 处理/ MapReduce/ 作业/ 的/ 一般/ 流程/ ,/ 从图/ 中/ 的/ 阴影/ 部分/ 可以/ 看出/ 与/ 原/ 机制/ 相比/ 做/ 了/ 两处/ 改动/ :/ 一是/ 在/ Mapper/ 中/ 新增/ 了/ Counter/ 方法/ ,/ 二是/ 更改/ 了/ 数据/ 从/ Map/ 端/ 向/ Reduce/ 端的/ 传输/ 策略/ ./ 新/ 处理/ 机制/ 同样/ 将/ 一个/ 作业/ 的/ 处理/ 分成/ 两个/ 阶段/ :/ Map/ 阶段/ 和/ Reduce/ 阶段/ ./ Map/ 阶段/ :/ 增量/ 式/ 分区/ 策略/ ,/ 新增/ 一个/ Counter/ 方法/ ,/ 该/ 方法/ 用于/ 描述/ 已/ 处理/ 数据/ 的/ 分布/ 情况/ ./ Mapper/ 在/ 将/ 各/ 元组/ 转换/ 为/ 〈/ key/ ,/ value/ 〉/ 形式/ 的/ 键值/ 对后/ ,/ Counter/ 方法/ 统计/ 出/ 自身/ 所/ 产生/ 各/ key/ 的/ 元组/ 数/ ,/ 作为/ 描述/ 处理/ 数据分布/ 的/ 统计/ 信息/ 记录下来/ ,/ 然后/ 写入/ Buffer/ 中/ ./ 这些/ 统计/ 信息/ 将/ 按照/ 一定/ 的/ 时间/ 间隔/ 被/ 发送到/ Master/ 节点/ ,/ Master/ 将/ 所有/ Mapper/ 的/ 统计/ 信息/ 进行/ 汇总/ ,/ 从而/ 获取/ 已/ 处理/ 完/ 数据/ 的/ 全局/ 分布/ 信息/ ./ 在/ Buffer/ 溢写/ 时/ ,/ 增量/ 式/ 分区/ 更改/ 产生/ 分区/ 的/ 粒度/ ,/ 即/ 产生/ 粒度/ 更/ 小/ 的/ 分区/ ./ 接下来/ ,/ 如同/ 原/ 系统/ ,/ 将/ Buffer/ 中/ 的/ 数据/ 溢写/ 到/ 本地/ 磁盘/ ,/ 等/ 该/ Mapper/ 完成/ 后/ ,/ 将/ 所有/ 的/ 临时文件/ 按照/ 分区/ 进图/ 2/ 增量/ 式/ 分区/ 分配/ 过程/ 行/ 合并/ ,/ 同时/ 为/ 每个/ 细粒度/ 分区/ 建立/ 索引/ ./ 索引/ 是/ 用于/ 记录/ 各/ 分区/ 将要/ 被/ 哪个/ Reducer/ 进行/ 处理/ 的/ 结构/ ,/ 本文/ 称建/ 索引/ 的/ 过程/ 为/ 分区/ 的/ 分配/ 过程/ ./ 不同于/ 原/ 机制/ 一次/ 分配机制/ ,/ 增量/ 式/ 分区/ 策略/ 以/ 渐进/ 的/ 方法/ 建立/ 索引/ ,/ 并且/ 由/ Master/ 节点/ 根据/ 全局/ 数据分布/ 信息/ 决定/ 如何/ 将/ 所有/ 细粒度/ 分区/ 分配/ 到/ Reducer/ 上/ ./ Reduce/ 阶段/ :/ 增量/ 式/ 分区/ 策略/ 在/ Map/ 端/ 产生/ 了/ 粒度/ 更小且/ 数量/ 大于/ Reducer/ 个数/ 的/ 细粒度/ 分区/ ,/ 为/ 保证/ 按照/ 用户/ 定义/ 的/ Reducer/ 数/ 运行/ 任务/ ,/ 增量/ 式/ 分区/ 策略/ 需要/ 将/ 细粒度/ 分区/ 在/ Reduce/ 端/ 进行/ 合并/ ,/ 即/ 更改/ 分区/ 与/ Reducer/ 一对一/ 的/ 关系/ 为/ 多/ 对/ 一/ 关系/ ./ 更改/ 后/ ,/ 增量/ 式/ 分区/ 策略/ 需要/ 保证/ 各/ Reducer/ 读取/ 多个/ 分区/ 的/ 数据/ ,/ 因此/ 在/ Reduce/ 端/ 新增/ 了/ 分区/ 的/ 合并/ 操作/ ./ 与/ 原/ 机制/ 相同/ ,/ 当/ 所有/ Mapper/ 完成/ 后/ ,/ 各/ Reducer/ 即可/ 获取/ 自身/ 的/ 全部/ 数据/ ,/ 从而/ 开始/ 执行/ Reduce/ (/ )/ 函数/ ,/ 并/ 最终/ 将/ 输出/ 结果/ 写到/ HDFS/ 上/ ,/ 完成/ 整个/ 作业/ ./ 4/ 增量/ 式/ 分区/ 的/ 代价/ 模型/ 及/ 算法/ 实现/ 本/ 节/ 首先/ 对/ 增量/ 式/ 分区/ 的/ 分配/ 过程/ 进行/ 概要/ 描述/ ,/ 然后/ 建立/ 分区/ 分配/ 的/ 代价/ 评估/ 模型/ ,/ 最后/ 给出/ 求解/ 该/ 模型/ 的/ 启发式/ 算法/ ./ 4.1/ 增量/ 式/ 分区/ 分配/ 模型/ 概述/ 本/ 节/ 主要/ 介绍/ 增量/ 式/ 分区/ 分配/ 方法/ 的/ 一般/ 处理/ 模型/ ,/ 图/ 2/ 是/ 对/ 图/ 1/ 中/ 的/ 阴影/ 部分/ “/ 增量/ 式/ 分区/ 分配/ 过程/ ”/ 的/ 详细描述/ ,/ 也/ 是/ 各/ Reducer/ 输入/ 分区/ 的/ 生成/ 过程/ ./ 在/ Mapper/ 运行/ 过程/ 中/ 生成/ 多于/ Reducer/ 数目/ 的/ 分区/ ,/ 即/ 通过/ 重/ 定义/ 分区/ 函数/ 为/ hash/ (/ key/ )/ mod/ (/ λ/ ×/ R/ )/ Page6/ 来/ 实现/ ./ 其中/ R/ 为/ 用户/ 提交/ 任务/ 时所/ 定义/ 的/ Reducer/ 个数/ ,/ λ/ 为本/ 方法/ 新增/ 的/ 分区/ 放大系数/ ,/ 用于/ 控制/ 产生/ 分区/ 的/ 粒度/ ,/ 取值/ 是/ 大于/ 等于/ 1/ 的/ 正整数/ ,/ 并/ 定义/ 每个/ 分区/ 为/ Micro/ -/ partition/ (/ 微分/ 区/ )/ ./ 在/ λ/ >/ 1/ 时/ ,/ Micro/ -/ partition/ 与/ 原/ 分区/ 相比/ 所/ 包含/ key/ 的/ 种类/ 会/ 有所/ 减少/ ,/ 但/ 与/ 原/ 系统/ 相同/ ,/ 相同/ key/ 值/ 的/ 元组/ 仍旧/ 属于/ 同一个/ 分区/ ./ 当/ Mapper/ 整体/ 任务/ 运行/ 到/ 特定/ 时机/ 时/ ,/ Master/ 节点/ 开始/ 向/ 各/ Reducer/ 分配/ 分区/ ,/ 在/ 进行/ 第/ 1/ 次/ 分区/ 分配/ 时/ ,/ 见图/ 2/ 的/ “/ 第/ 1/ 次/ 分配/ ”/ 部分/ ,/ 首先/ 从/ 所有/ Micro/ -/ partition/ 中/ 选出/ 一定/ 数目/ 的/ 分区/ 向/ Reducer/ 端/ 分配/ ./ 在/ 第/ 1/ 次/ 分配/ 完成/ 后/ ,/ 经过/ 特定/ 时间/ 间隔/ ,/ 触发/ 第/ 2/ 次/ 分配/ ,/ 同/ 第/ 1/ 次/ 分配/ 过程/ 相同/ ,/ 首先/ 从未/ 分配/ 的/ Micro/ -/ partition/ 中/ 筛选/ 出/ 部分/ 分区/ ,/ 再/ 进行/ 分配/ ./ 然而/ 由于/ 所有/ 的/ 分配/ 都/ 发生/ 在/ Mapper/ 运行/ 过程/ 中/ ,/ 导致/ 第/ 1/ 次/ 分配/ 分区/ 的/ 数据量/ 在/ 第/ 2/ 次/ 分配/ 时/ 在/ 各/ Reducer/ 端会/ 发生变化/ ,/ 此时/ 可能/ 造成/ 部分/ Reducer/ 的/ 数据量/ 发生/ 倾斜/ ,/ 因此/ 在/ 进行/ 第/ 2/ 次/ 分配/ 时/ ,/ 需要/ 考虑/ 各/ Reducer/ 已有/ 的/ 负载量/ ,/ 然后/ 按照/ 各/ 节点/ 分配/ 后/ 数据量/ 总量/ 相同/ 的/ 原则/ 进行/ 分配/ ./ 伴随/ 着/ Mapper/ 运行/ ,/ 各/ 节点/ 的/ 统计/ 结果/ 会/ 逐渐/ 精确/ ,/ 即/ 在/ 后/ 几轮/ 分配/ 中/ ,/ 各/ 分区/ 的/ 数据量/ 逐渐/ 趋于稳定/ ,/ 此时/ 分配/ 的/ 目标/ 主要/ 是/ 修正/ 在/ 前/ 几轮/ 分配/ 中/ ,/ 可能/ 由于/ 部分/ 统计/ 而/ 出现/ 的/ 数据/ 划分/ 倾斜/ ./ 在/ 所有/ Mapper/ 完成/ 前/ ,/ 必须/ 将/ 所有/ Micro/ -/ partition/ 分配/ 到/ Reducer/ 端/ ,/ 以此/ 避免/ 延迟/ 传输数据/ ./ 若/ 定义/ 总/ 的/ 分配/ 轮数/ 为/ s/ ,/ 应该/ 确保/ 第/ s/ 次/ 的/ 分配/ 时机/ 在/ 最后/ 一个/ Mapper/ 结束/ 前/ ,/ 如图/ 2/ “/ 第/ s/ 次/ 分配/ ”/ 所/ 描述/ ,/ 至此/ ,/ 完成/ 了/ 所有/ 分区/ 的/ 分配/ ./ 对/ 各/ Reducer/ 而言/ ,/ 分配/ 到/ 该/ 节点/ 上/ 多个/ Micro/ -/ partition/ 的/ 集合/ 就是/ 将要/ 被/ 该/ Reducer/ 处理/ 的/ 全部/ 输入/ 数据/ ,/ 本文/ 定义/ 这个/ 集合/ 为/ Fine/ -/ partition/ (/ 精细/ 分区/ )/ ./ 综上所述/ ,/ 增量/ 式/ 分区/ 分配/ 发生/ 在/ Map/ 阶段/ ,/ 在/ Mapper/ 运行/ 过程/ 中/ ,/ 根据/ 统计/ 结果/ 的/ 逐渐/ 精确性/ ,/ 对/ 细粒度/ 分区/ 进行/ 多轮/ 筛选/ 和/ 分配/ ,/ 从而/ 解决/ 各/ Reducer/ 接收数据/ 总量/ 均衡/ 的/ 问题/ ./ 4.2/ 增量/ 式/ 分区/ 分配/ 的/ 代价/ 评估/ 模型/ 影响/ Task/ 发生/ 运行/ 不/ 均衡/ 的/ 因素/ 可/ 概括/ 为/ 硬件/ 因素/ 和/ 软件/ 因素/ ,/ 为/ 专注/ 解决/ 因/ MapReduce/ 自身/ 机制/ 所/ 导致/ 的/ 数据/ 划分/ 倾斜/ 问题/ ,/ 本文/ 假定/ 各/ 节点/ 的/ 硬件资源/ 同构/ ./ 软件/ 因素/ 主要/ 表现/ 在/ Hadoop/ 系统/ 本身/ 的/ 参数设置/ 和/ MapReduce/ 固有/ 的/ 处理/ 机制/ 上/ ./ 由于/ Hadoop/ 包含/ 190/ 多种/ 参数/ ,/ 参数/ 的/ 不同/ 选取/ 方案/ 都/ 会/ 对/ 作业/ 的/ 运行/ 产生/ 影响/ [/ 19/ ]/ ,/ 设定/ 本文/ 中/ 所有/ 实验/ 只/ 采用/ 一套/ 参数/ 方案/ ./ 基于/ 以上/ 假定/ ,/ 可以/ 明确/ 运行/ 时间/ 上/ 的/ 倾斜/ 是/ 由/ 分区/ 机制/ 造成/ 的/ 数据/ 划分/ 不/ 均衡/ 引起/ 的/ ./ 本文/ 令/ 用户/ 作业/ 中/ 定义/ 了/ m/ 个/ Reducer/ ,/ 作业/ 在/ l/ 个/ Task/ 节点/ 的/ 集群/ 上/ 运行/ ,/ 增量/ 式/ 分区/ 的/ 两个/ 控制变量/ 为/ :/ 放大系数/ λ/ 和/ 分配/ 轮数/ ι/ ,/ 这/ 两个/ 参数/ 为/ 预留/ 参数/ ,/ 用户/ 可/ 根据/ 作业/ 需求/ 自行/ 设定/ ./ 评估/ 作业/ 是否/ 发生/ 数据/ 倾斜/ 主要/ 是/ 通过/ 判断/ 各/ 节点/ 接收/ 的/ 数据量/ 是否/ 相等/ ,/ 而/ 数据量/ 的/ 大小/ 又/ 可用/ 元组/ 个数/ 的/ 多少/ 来/ 表示/ ,/ 由于/ 输入/ 数据/ 是/ 以/ 元组/ 为/ 单元/ 进行/ 处理/ ,/ 因此/ 本文/ 用/ 元组/ 个数/ 作为/ 衡量/ 数据量/ 大小/ 的/ 唯一标准/ ,/ 记为/ ki/ ,/ 其中/ i/ ∈/ [/ 1/ ,/ n/ ]/ ,/ 即/ 表示/ 一个/ Mapper/ 上/ ,/ 第/ i/ 个/ Micro/ -/ partition/ 中/ 包含/ 的/ 元组/ 总数/ ./ 增量/ 式/ 分区/ 分配/ 的/ 整个/ 过程/ 就是/ 在/ Mapper/ 运行/ 中/ ,/ 将/ 产生/ 的/ n/ =/ λ/ ×/ m/ 个/ Micro/ -/ partition/ 经过/ ι/ 轮/ 分配/ 到/ 各/ Reducer/ 节点/ ,/ 最终/ 解决/ 各/ Reduce/ 处理/ 数据/ 总量/ 均衡/ 的/ 问题/ ./ 本文/ 将/ 增量/ 式/ 分区/ 分配/ 的/ 整个/ 过程/ 分解/ 为/ ι/ 个/ 相关/ 的/ 子/ 分配/ 过程/ ,/ 该子/ 过程/ 可/ 描述/ 为/ :/ 首先/ ,/ 在/ 当前/ 分配/ 轮上/ ,/ 从/ 所有/ 未/ 分配/ 的/ Micro/ -/ partition/ 中/ 选取/ 一定/ 数目/ 的/ Micro/ -/ partition/ ;/ 然后/ ,/ 在/ 前轮/ 分配/ 的/ 基础/ 上/ ,/ 将/ 本轮/ 所/ 筛选/ 的/ Micro/ -/ partition/ 分配/ 到/ Reducer/ 端/ ,/ 对于/ 第/ 1/ 轮/ 只/ 需/ 直接/ 进行/ 分配/ ,/ 同时/ ,/ 满足/ 在/ 本轮/ 分配/ 后/ 各/ Reducer/ 接收数据/ 总量/ 的/ 均衡性/ ./ 求解/ 增量/ 式/ 分区/ 分配/ 的/ 整个/ 过程/ 就是/ 迭代/ 的/ 求解/ 这/ ι/ 个子/ 过程/ ,/ 而/ 每个/ 子/ 过程/ 又/ 需要/ 依次/ 求解/ 两个/ 子/ 问题/ :/ (/ 1/ )/ 在/ 当前/ 分配/ 轮/ ,/ 筛选/ 出/ 哪些/ 分区/ 进行/ 分配/ ;/ (/ 2/ )/ 如何/ 将/ 选中/ 的/ 分区/ 分配/ 到/ 各/ Reducer/ 节点/ ./ 接下来/ ,/ 给出/ 求解/ 这/ 两个/ 问题/ 的/ 详细/ 步骤/ ./ 4.2/ ./ 1/ 筛选/ 分区/ 增量/ 式/ 分区/ 分配/ 的/ 整个/ 过程/ 发生/ 在/ Mapper/ 的/ 运行/ 过程/ 中/ ,/ 并且/ 某个/ 分区/ 一旦/ 被/ 分配/ 后/ 将/ 无法/ 更改/ 执行/ 节点/ ,/ 因此/ 分区/ 的/ 筛选/ 原则/ ,/ 将/ 直接/ 影响/ 分配/ 后/ 各/ Reducer/ 接收数据/ 总量/ 误差/ 的/ 大小/ ./ 本文/ 将/ 筛选/ 分区/ 过程/ 分解/ 为/ 3/ 个子/ 问题/ :/ (/ 1/ )/ 何时/ 筛选/ 分区/ ?/ (/ 2/ )/ 每轮/ 筛选/ 几个/ 分区/ ?/ (/ 3/ )/ 筛选/ 哪几个/ 分区/ ?/ 何时/ 筛选/ 分区/ 就是/ 确定/ 每轮/ 的/ 分配/ 时机/ 问题/ ,/ 分区/ 分配/ 过早/ 会/ 导致/ Map/ 端/ 统计/ 分区/ 信息/ 的/ 不准/ 性较/ 高/ ,/ 分配/ 过晚/ 又/ 会/ 延迟/ Shuffle/ 时间/ ,/ 因此/ 分配/ 时机/ 对/ 多轮/ 分配/ 策略/ 有/ 重要/ 的/ 影响/ ./ Hadoop/ 原/ 系统/ 所/ 实现/ 的/ MapReduce/ 机制/ 是/ 在/ 第/ 1/ 个/ Mapper/ 完成/ 后/ ,/ 才/ 开始/ 从/ Map/ 端/ 向/ Reduce/ 端/ 传输数据/ ,/ 因此/ 将/ 第/ 1/ 次/ 分配/ 的/ 时间/ 点/ 定义/ 为/ 第/ 1/ 个/ Mapper/ 的/ Page7/ 完成/ 时刻/ ,/ 以此/ 达到/ 最早/ 的/ 数据传输/ 时机/ ./ 传输/ 最后/ 一个/ Mapper/ 数据/ 必然/ 是/ 与/ Map/ 函数/ 串行/ 进行/ 的/ ,/ 若/ 在/ 最后/ 一个/ 完成/ 前/ 所有/ 已/ 完成/ Mapper/ 的/ 数据/ 都/ 已经/ Shuffle/ 完成/ ,/ 则/ 定义/ 此时/ 为/ 最后/ 一次/ 的/ 分配/ 时机/ 即可/ ,/ 若/ 还有/ 数据/ 没有/ 完成/ Shuffle/ ,/ 并且/ 最后/ 一个/ Mapper/ 处理/ 的/ 数据量/ 少/ ,/ 再/ 定义/ 此时/ 为/ 最后/ 一次/ 分配/ 时机/ 就/ 会/ 造成/ 已/ 完成/ Mapper/ 的/ 分区/ 一直/ 保存/ 在/ 本地/ 磁盘/ ,/ 导致/ 该/ 方法/ 延长/ 了/ Shuffle/ 时间/ ./ 为此/ 定义/ 当/ 所有/ Mapper/ 完成/ 到/ 80/ %/ 时为/ 最后/ 一次/ 的/ 分配/ 时机/ ,/ 以此/ 能/ 最大/ 限度/ 地使/ Shuffle/ 与/ Mapper/ 并行执行/ ,/ 同时/ 又/ 能/ 提高/ 统计/ 的/ 准确性/ ./ 从而/ 得出/ 总/ 的/ 分配/ 时间段/ 为/ :/ 最后/ 一次/ 分配/ 时/ 整个/ Mapper/ 已/ 完成/ 的/ 比例/ 减去/ 第/ 1/ 次/ 分配/ 时/ 整个/ Mapper/ 完成/ 的/ 比例/ ,/ 为/ 计算/ 方便/ 本文/ 取/ 两次/ Mapper/ 完成/ 比例/ 的/ 整数/ 部分/ ,/ 用/ 两个/ 整数/ 相减/ 得到/ 总/ 分配/ 时间/ ./ 同时/ 本文/ 采用/ 时间/ 等/ 分/ 原则/ ,/ 将/ 总/ 分配/ 时间/ 分为/ ι/ 段/ ,/ 即/ 在/ 分配/ 轮数/ ι/ 内/ 将/ 所有/ 的/ Micro/ -/ partition/ 分配/ 完成/ ,/ 因此/ 用/ 总/ 分配/ 时间/ 除以/ ι/ 即可/ 得到/ 相邻/ 两轮/ 分配/ 的/ 间隔时间/ ,/ 由于/ 第/ 1/ 次/ 分配/ 时间/ 定义/ 为/ 第/ 1/ 个/ Mapper/ 完成/ 时/ 的/ 时间/ ,/ 从而/ 可/ 计算/ 出/ 每轮/ 的/ 分配/ 时间/ 点/ ./ 在/ 确定/ 出/ 每轮/ 的/ 分配/ 时间/ 点后/ ,/ 接着/ 需要/ 确定/ 每轮/ 分区/ 的/ 分配/ 个数/ 问题/ ,/ 本文/ 采用/ 每轮/ 分配/ Micro/ -/ partition/ 个数/ 相等/ 的/ 原则/ 进行/ 求解/ ./ 记/ 每轮/ 要/ 分配/ 的/ 分区/ 个数/ 为/ Na/ ,/ 在/ 给定/ 的/ 分配/ 轮数/ ι/ 下/ ,/ 用/ Micro/ -/ partition/ 的/ 总/ 个数/ n/ =/ λ/ ×/ m/ 除以/ 分配/ 轮数/ ι/ 即可/ 得到/ 每轮/ 要/ 分配/ 的/ Micro/ -/ partition/ 个数/ ./ 最后/ ,/ 需要/ 从/ 所有/ 未/ 分配/ 分区/ 中/ 筛/ 取出/ Na/ 个/ Micro/ -/ partition/ 作为/ 本轮/ 的/ 候选/ 分区/ 集合/ ,/ 记为/ Cand/ ./ 为此/ 本文/ 定义/ 两个/ 筛选/ 原则/ :/ (/ 1/ )/ 优先/ 筛选/ 数据量/ 大/ 的/ Micro/ -/ partition/ ;/ (/ 2/ )/ 优先/ 筛选/ 变化率/ 大/ 的/ Micro/ -/ partition/ ./ 原则/ 1/ 限定/ 了/ 让/ 数据量/ 大/ 的/ 分区/ 尽早/ Shuffle/ ,/ 这样/ 可以/ 将/ 数据量/ 相对/ 少/ 的/ 分区/ 稍后/ 传输/ ,/ 从而/ 减少/ 最后/ 一个/ Mapper/ 完成/ 后/ 传输数据/ 的/ 等待时间/ ./ 原则/ 2/ 中/ 的/ 变化率/ 用于/ 反映/ 连续/ 两次/ 分配/ 间/ 各/ 分区/ 数据量/ 的/ 变化/ 程度/ ,/ 即用/ 当前/ 分配/ 轮该/ Micro/ -/ partition/ 的/ 统计/ 量/ 减去/ 上轮/ 分配/ 时/ 的/ 统计/ 量/ 再/ 除/ 以上/ 轮/ 分配/ 时该/ Micro/ -/ partition/ 的/ 统计/ 量/ 得到/ ./ 原则/ 2/ 可以/ 逐步/ 修正/ 因/ 提前/ 分配/ 和/ 部分/ 统计/ 而/ 带来/ 的/ 分配/ 误差/ ./ 随着/ 分配/ 轮数/ 的/ 递增/ ,/ 各/ 分区/ 的/ 统计/ 结果/ 更加/ 精确/ ,/ 从而/ 可以/ 更/ 精确/ 地/ 修正/ 前面/ 因/ 分配/ 过早/ 造成/ 的/ 分配/ 误差/ ./ 为/ 实现/ 对/ 各/ Micro/ -/ partition/ 数据量/ 和/ 变化率/ 的/ 记录/ ,/ 本文/ 定义/ 了/ 两个/ 向量/ ,/ 分别/ 为/ 统计/ 向量/ 和/ 分配/ 向量/ ./ 令/ α/ =/ (/ k1/ ,/ k2/ ,/ …/ ,/ kn/ )/ 为/ 一个/ Mapper/ 上/ 所有/ Micro/ -/ partition/ 的/ 统计/ 向量/ ,/ 即/ 用于/ 描述/ 该/ Mapper/ 上/ 各/ 分区/ 的/ 数据分布/ ,/ 元素/ ki/ 表示/ 该/ Mapper/ 上/ 第/ i/ (/ i/ ∈/ [/ 1/ ,/ n/ ]/ )/ 个/ Micro/ -/ partition/ 中/ 包含/ 的/ 元组/ 总数/ ./ 一个/ Task/ 上/ 可能/ 运行/ 有/ 多个/ Mapper/ ,/ 而/ 单个/ Mapper/ 无法/ 和/ Master/ 节点/ 进行/ 直接/ 通信/ ,/ 必须/ 借助/ 运行/ 节点/ 上/ Task/ 的/ HeartBeat/ ,/ 因此/ 本文/ 定义/ α/ s/ =/ (/ k1s/ ,/ k2s/ ,/ …/ ,/ kns/ )/ 为/ 第/ s/ (/ s/ ∈/ [/ 1/ ,/ l/ ]/ )/ 个/ Task/ 上/ 所有/ Mapper/ 的/ α/ 累加/ 和/ ,/ kis/ (/ i/ ∈/ [/ 1/ ,/ n/ ]/ )/ 为/ 第/ i/ 个/ Micro/ -/ partition/ 在/ 第/ s/ 个/ Task/ 上/ 数据量/ ,/ 以此/ 统计/ 出该/ Task/ 上/ 的/ 数据分布/ ./ 对于/ Master/ 节点/ ,/ 只/ 需/ 将/ 所有/ Task/ 的/ α/ s/ 累积/ 即可/ 得到/ 全局/ 的/ 数据分布/ ,/ 用/ n/ 维/ 向量/ 犃/ 表示/ ,/ 记/ 犃/ =/ ∑/ 分配/ 方式/ ,/ 需要/ 记录/ 下/ 每轮/ 分配/ 时/ 的/ 数据分布/ 情况/ ,/ 因此/ ,/ 记/ 犃/ t/ =/ ∑/ 全局/ Micro/ -/ partition/ 的/ 数据分布/ ,/ α/ st/ 为/ 在/ 第/ t/ 轮/ 分配/ 时/ ,/ 第/ s/ 个/ Task/ 上/ 所有/ Micro/ -/ partition/ 的/ 数据分布/ ./ 定义/ 一个/ m/ 维/ 分配/ 向量/ ,/ 用于/ 标记/ 各/ Micro/ -/ partition/ 是否/ 已/ 被/ 分配/ 到/ 某个/ Reducer/ 上/ ,/ 记/ 为/ 犃/ 犚/ =/ (/ P1/ ,/ P2/ ,/ …/ ,/ Pm/ )/ ,/ 其中/ Pr/ =/ (/ x1r/ ,/ x2r/ ,/ …/ ,/ xnr/ )/ (/ r/ ∈/ [/ 1/ ,/ m/ ]/ )/ ./ 分配/ 向量/ 是/ 一个/ 以/ Micro/ -/ partition/ 为行/ ,/ Reducer/ 为列/ 的/ 二维/ 向量/ ,/ 元素/ xir/ ∈/ {/ 0/ ,/ 1/ }/ (/ i/ ∈/ [/ 1/ ,/ n/ ]/ )/ ,/ {/ 1/ }/ 表示/ 分区/ 值为/ i/ 的/ Micro/ -/ partition/ 已/ 被/ 分配/ 第/ r/ 个/ Reducer/ 上/ ,/ {/ 0/ }/ 代表/ 该/ 分区/ 未/ 被/ 分配/ 到/ 第/ r/ 个/ Reducer/ 上/ ./ 由此可见/ ,/ 分配/ 向量/ 中/ 各/ 元素/ 的/ 初始值/ 为/ 0/ ,/ 每轮/ 都/ 会/ 将/ Na/ 个/ xir/ 赋值/ 为/ {/ 1/ }/ ,/ 并且/ 对/ 同一个/ xi/ ,/ 即/ 同一个/ Micro/ -/ partition/ ,/ 使/ 其满/ mxir/ =/ 1/ ,/ 即/ 一个/ Micro/ -/ partition/ 只能/ 被/ 分配/ 到/ 足/ ∑/ 一个/ Reducer/ 节点/ 上/ ./ 最终/ 经过/ ι/ 轮/ 的/ 分配/ ,/ 使得/ / ir/ =/ 1m/ 满足/ ∑/ Reducer/ 上/ ./ r/ =/ 1/ 通过/ 定义/ 这/ 两个/ 向量/ 可以/ 实时/ 记录/ 各/ Micro/ -/ partition/ 在/ Map/ 运行/ 过程/ 中/ 产生/ 数据量/ 的/ 大小/ 和/ 各/ Micro/ -/ partition/ 是否/ 已经/ 被/ 分配/ 到/ 某个/ Reducer/ 上/ ./ 接下来/ ,/ 依据/ 筛选/ 原则/ 1/ 和/ 原则/ 2/ 进行/ 分区/ 的/ 筛选/ ./ 筛选/ 的/ 过程/ 就是/ 计算/ 犃/ t/ 中/ 所有/ ∑/ [/ 1/ ,/ n/ ]/ )/ 到/ 本轮/ 分配/ t/ (/ t/ >/ 1/ )/ 为止/ ,/ 在/ Map/ 端/ 产生/ 的/ 数据量/ ,/ 并/ 从中/ 选取/ 数据量/ 最大/ 的/ 前/ 2Na/ 个/ Micro/ -/ partition/ ,/ 然后/ 比较/ 这/ 2Na/ 个/ Micro/ -/ partition/ 在/ tPage8/ 轮/ 和/ t/ -/ 1/ 轮/ 的/ 变化率/ ,/ 依据/ 这/ 两条/ 规则/ ,/ 最终/ 从/ 数据量/ 最大/ 的/ 前/ 2Na/ 个/ Micro/ -/ partition/ 中/ 选取/ 变化率/ 最大/ 的/ 前/ Na/ 个/ Micro/ -/ partition/ 作为/ 此轮/ 将要/ 被/ 分配/ 的/ 分区/ 集合/ Cand/ ./ 4.2/ ./ 2/ 分配/ 分区/ 在/ 确定/ 出/ 每轮/ 将要/ 分配/ 的/ 分区/ 后/ ,/ 接着/ 需要/ 将/ 本轮/ 选中/ 的/ 含有/ Na/ 个/ Micro/ -/ partition/ 元素/ 的/ 集合/ Cand/ 分配/ 到/ m/ 个/ Reducer/ 上/ ./ 由于/ 第/ t/ (/ t/ >/ 1/ )/ 轮/ 的/ 分区/ 分配/ 是/ 建立/ 在/ 第/ t/ -/ 1/ 轮/ 基础/ 之上/ ,/ 因此/ 在/ 第/ t/ 轮/ 分配/ 分区/ 时/ ,/ 不单/ 要/ 考虑/ 本轮/ Na/ 个/ 分区/ 分配/ 后/ 各/ 节点/ 的/ 负载量/ ,/ 还/ 需要/ 考虑/ 各/ Reducer/ 节点/ 在/ 第/ t/ 轮时/ 已/ 承载/ 的/ 负载量/ ,/ 从而/ 解决/ 在/ t/ 轮/ 分配/ 后/ ,/ 各/ Reducer/ 接收数据/ 的/ 总量/ 仍/ 是/ 均衡/ 的/ ./ 本文/ 将/ 每轮/ 分配/ 分区/ 的/ 问题/ 定义/ 为/ 如下/ 形式/ ./ 设在/ l/ 个/ Task/ 节点/ 的/ 集群/ 上/ ,/ 运行/ 着/ m/ 个/ Reducer/ ,/ 第/ t/ (/ t/ ∈/ [/ 1/ ,/ ι/ ]/ )/ 轮要/ 分配/ Micro/ -/ partition/ 的/ 集合/ 为/ Cand/ ,/ 令到/ 第/ t/ 轮/ 为止/ ,/ 在/ 第/ s/ (/ s/ ∈/ [/ 1/ ,/ l/ ]/ )/ 个/ Task/ 上/ 产生/ 的/ 第/ i/ (/ i/ ∈/ [/ 1/ ,/ n/ ]/ )/ 个/ Micro/ -/ partitionPi/ 的/ 数据量/ 为/ ksi/ ,/ 第/ r/ (/ r/ ∈/ [/ 1/ ,/ m/ ]/ )/ 个/ ReducerRr/ 上/ 已/ 承载/ 的/ 负载量/ 为/ Lr/ ,/ 则/ 分配/ 分区/ 就是/ 实现/ 以下/ 目标/ :/ 该式/ 满足/ 以下/ 条件/ :/ (/ 1/ )/ i/ ∈/ Cand/ (/ 2/ )/ xir/ =/ (/ 3/ )/ / i/ ,/ 满足/ ∑/ 其中/ Lr/ 为/ 常数/ ,/ 通过/ 计算/ 在/ t/ 轮/ 的/ 统计/ 向量/ 犃/ t/ 得到/ ./ 目标/ 函数/ 是/ 在/ 每轮/ 分配/ 时/ 约束/ 最大/ 的/ 负载量/ 节点/ ,/ 使/ 其/ 增加/ 的/ 负载量/ 最小/ 为/ 原则/ 来/ 实现/ 各/ Reducer/ 负载量/ 的/ 均衡/ ./ 因为/ 对于/ 一轮/ Reduce/ 任务/ ,/ 运行/ 时间/ 最长/ 的/ 那个/ Reducer/ 决定/ 了/ 整个/ Reduce/ 阶段/ 的/ 结束/ 时间/ ,/ 因此/ 只/ 需/ 减少/ 最长/ 任务/ 的/ 运行/ 时间/ 即可/ 减少/ 整个/ 任务/ 的/ 运行/ 时间/ ,/ 从而/ 解决/ 分区/ 均衡/ 的/ 问题/ ./ 同时/ ,/ 约束条件/ (/ 1/ )/ 用于/ 控制/ 每轮/ 只/ 分配/ 筛选/ 中/ 的/ 分区/ ,/ 约束条件/ (/ 2/ )/ 用于/ 标示/ 变量/ xir/ 被/ 分配/ 到/ 哪个/ Reducer/ 上/ ,/ 约束条件/ (/ 3/ )/ 使得/ 所有/ 属于/ Cand/ 的/ 分区/ 在/ 本轮/ 被/ 分配/ 完/ ./ 通过/ 该/ 目标/ 函数/ 和/ 约束条件/ ,/ 最终/ 确定/ 出/ 每轮/ Na/ 个/ Micro/ -/ partition/ 与/ Reducer/ 的/ 一一对应/ 关系/ ,/ 即/ 完成/ 本轮/ 的/ 分区/ 分配任务/ ./ 最终/ ,/ 经过/ ι/ 轮/ 的/ 迭代/ 求解/ ,/ 实现/ 分区/ 到/ Reduce/ 端的/ 增量/ 式/ 分配/ 过程/ ./ 4.3/ 代价/ 模型/ 的/ 算法/ 设计/ 求解/ 增量/ 式/ 分区/ 分配/ 策略/ 的/ 代价/ 模型/ ,/ 需要/ 两步/ 完成/ :/ 第/ 1/ 步/ 筛选/ 分区/ ,/ 第/ 2/ 步/ 将/ 选中/ 的/ 分区/ 分配/ 到/ Reduce/ 端/ ./ 第/ 1/ 步要/ 解决/ 的/ 问题/ 是/ 排序/ 问题/ ,/ 此步/ 仅/ 需要/ 根据/ 数据量/ 的/ 大小/ 对/ 未/ 分配/ 分区/ 进行/ 排序/ ,/ 然后/ 取出/ 变化率/ 最大/ 的/ 前/ Na/ 个/ 分区/ ./ 第/ 2/ 步是/ 最优化/ 问题/ ,/ 本文/ 给出/ 求解/ 该/ 问题/ 的/ 一种/ 启发式/ 算法/ ./ 筛选/ 分区/ 算法/ 如/ 算法/ 1/ 所示/ ./ 算法/ 1/ ./ 筛选/ 分区/ 算法/ ./ 输入/ :/ 未/ 分配/ 分区/ 集合/ 犃/ 犚/ 输出/ :/ 本轮/ 要/ 分配/ 的/ 分区/ 集合/ Cand1/ ./ SortAt/ // // 按/ 数据量/ 大小/ 递减/ 进行/ 堆排序/ 2/ ./ Pcand/ ←/ Max2Na/ {/ Sort/ }/ // // 取前/ 2Na/ 个/ 分区/ 3/ ./ FORPcand/ // // 求/ 最大/ 前/ 2Na/ 个/ 分区/ 相邻/ 2/ 次/ 的/ 变化率/ 4/ ./ SPcand/ _/ i/ =/ (/ kirt/ -/ kir/ (/ t/ -/ 1/ )/ )/ // kir/ (/ t/ -/ 1/ )/ 5/ ./ ENDFOR6/ ./ Cand/ ←/ MaxNa/ {/ SPcand/ }/ // // 取出/ 变化率/ 最大/ 的/ 前/ Na/ 个/ 算法/ 1/ 的/ 复杂度/ 受/ 两种/ 操作/ 的/ 影响/ :/ 第/ 1/ 行是/ 按/ 数据/ 总量/ 对/ 所有/ 未/ 分配/ 的/ 分区/ 排序/ 和/ 第/ 3/ 行/ 计算/ 未/ 分配/ 分区/ 的/ 变化率/ ;/ 本文/ 采用/ 堆排序/ 算法/ ,/ 在/ 第/ 1/ 次/ 排序/ 时/ n/ 取得/ 最大值/ ,/ 每轮/ 分配/ 后以/ n/ // ι/ 的/ 数量/ 递减/ ,/ 算法/ 的/ 平均/ 计算/ 杂度/ 为/ O/ (/ nlogn/ )/ ;/ 对于/ 计算/ 未/ 分配/ 分区/ 的/ 变化率/ ,/ 只/ 需要/ 在/ 每轮/ 分配/ 中/ 遍历/ 一遍/ 所有/ 未/ 分配/ 分区/ ,/ 操作步骤/ 为/ n/ // ι/ ∈/ Θ/ (/ logn/ )/ ,/ 因此/ 算法/ 1/ 的/ 时间/ 复杂度/ 为/ O/ (/ nlogn/ )/ ./ 算法/ 1/ 的/ 时间/ 复杂度/ 与/ 未/ 分配/ 分区/ 的/ 个数/ 和/ 排序/ 算法/ 有关/ ,/ 在/ 作业/ 运行/ 的/ 开始/ 阶段/ ,/ JobTracker/ 只/ 需要/ 花费/ 很小/ 的/ 代价/ 进行/ Task/ 的/ 调度/ ,/ 而/ 由于/ 未/ 分配/ 分区/ 数目/ 较/ 多/ ,/ 因此/ 筛选/ 算法/ 可能/ 要/ 增加/ 一定/ 的/ 负载量/ ./ 伴随/ 作业/ 的/ 运行/ ,/ Task/ 的/ 调度/ 代价/ 会/ 逐渐/ 增大/ ,/ 表现/ 为/ Map/ // ReduceTask/ 的/ 启动/ 和/ 停止/ ,/ 而/ 未/ 分配/ 分区/ 的/ 数目/ 会/ 逐渐/ 减少/ ,/ 从而/ 使得/ 筛选/ 算法/ 的/ 代价/ 逐渐/ 减小/ ./ 根据/ 本文/ 对/ 分配/ 分区/ 问题/ 的/ 定义/ ,/ 可/ 将/ 该/ 每轮/ 的/ 分配/ 分区/ 问题/ 等价/ 于/ Min/ -/ max/ 问题/ [/ 20/ ]/ ,/ 又/ 由于/ Min/ -/ max/ 是/ NP/ -/ Hard/ [/ 21/ ]/ ,/ 因此/ 增量/ 式/ 分区/ 分配/ 问题/ 也/ 是/ NP/ -/ Hard/ ./ 为/ 能/ 精确/ 和/ 快速/ 地/ 确定/ 出/ 一套/ 近似/ 最优/ 的/ 分配/ 方案/ ,/ 本文/ 提出/ 一种/ 改进/ 的/ 贪心/ 算法/ ,/ 见/ 算法/ 2/ ./ 算法/ 2/ ./ 分配/ 分区/ 算法/ ./ 输入/ :/ Cand/ ,/ LRt/ 输出/ :/ 犃/ 犚/ 1/ ./ CandPs/ =/ Sort/ {/ Cand/ }/ // // 按/ 数据量/ 递减/ 排序/ 2/ ./ WHILECandPs/ // // 贪心/ 策略/ 指派/ CandPs/ 到/ ReducerPage93/ ./ MaxLoad/ =/ max/ {/ LRt/ }/ // // 取出/ 负载/ 最大/ 的/ Reducer4/ ./ MinLoad/ =/ min/ {/ LRt/ }/ // // 取出/ 负载/ 最小/ 的/ Reducer5/ ./ MinLoad/ =/ MinLoad/ +/ CandPs1/ // // 取出/ 数据量/ 最大/ 6/ ./ UPDATEMaxLoad/ 和/ MinLoad/ // // 记录/ 分配/ 后/ 的/ 7/ ./ CandPs/ =/ CandPs/ -/ CandPs18/ ./ ENDWHILE9/ ./ WHILE/ 有/ 分区/ 交换/ 操作/ ‖/ 第/ 1/ 次/ 执行/ 10/ ./ UPDATEMaxLoad/ 和/ MinLoad/ // // 记录/ 分配/ 或/ 11/ ./ MaxLoadValue/ =/ Add/ {/ MaxLoad/ }/ // // 计算/ 最大/ 12/ ./ MinLoadValue/ =/ Add/ {/ MinLoad/ }/ // // 计算/ 最小/ 13/ ./ SortMax/ =/ Sort/ {/ MaxLoad/ }/ // // 对/ 最大/ 负载/ 节点/ 的/ 14/ ./ SortMin/ =/ Sort/ {/ MinLoad/ }/ // // 对/ 最小/ 负载/ 节点/ 的/ 15/ ./ FORiTOMaxLoad/ // // 遍历/ 最大/ 负载/ 节点/ ,/ 并/ 取出/ 16/ ./ IFSwitchMaxV/ +/ SortMax/ {/ i/ }/ </ =/ 17/ ./ SwitchMax/ +/ =/ SortMax/ {/ i/ }/ 18/ ./ ELSEBREAK19/ ./ ENDIF20/ ./ ENDFOR21/ ./ FORjTOMinLoad/ // // 遍历/ 最小/ 负载/ 节点/ ,/ 并取/ // // 出/ 负载量/ 之/ 和/ 小于/ 等于/ 该/ 节点/ 总/ 负载量/ 一半/ 的/ 分区/ 22/ ./ IFSwitchMinV/ +/ SortMin/ {/ j/ }/ </ =/ MinLoadValue/ // 223/ ./ SwitchMin/ +/ =/ SortMin/ {/ j/ }/ 24/ ./ ELSEBREAK25/ ./ ENDIF26/ ./ ENDFOR27/ ./ IF/ (/ (/ MaxLoadValue/ -/ SwitchMaxV/ +/ 28/ ./ MaxLoad/ =/ MaxLoad/ -/ SwitchMax/ +/ SwitchMin29/ ./ MinLoad/ =/ MinLoad/ -/ SwitchMin/ +/ SwitchMax30/ ./ ELSE/ 没有/ 分区/ 交换/ 31/ ./ ENDIF32/ ./ ENDWHILE33/ ./ 更新/ 犃/ 犚/ 算法/ 2/ 由/ 两/ 部分/ 组成/ ,/ 第/ 1/ ~/ 8/ 行是/ 按/ 贪心/ 算法/ 将/ 分区/ 指派/ 到/ 各/ Reducer/ 节点/ 上/ ,/ 第/ 9/ ~/ 32/ 行/ 用于/ 交换/ 最大/ 负载/ 节点/ 和/ 最小/ 负载/ 节点/ 的/ 分区/ ,/ 以此/ 达到/ 尽量/ 减小/ 最大/ 负载/ 节点/ 负载量/ 的/ 目的/ ./ 第/ 1/ 部分/ 的/ 执行/ 效率/ 受/ 两种/ 因素/ 的/ 影响/ ,/ 分别/ 是/ 第/ 1/ 行/ 的/ 排序/ 算法/ 和/ 第/ 6/ 行/ 确定/ 最大/ 和/ 最小/ Reducer/ 的/ 负载/ 节点/ ./ 本文/ 第/ 1/ 行/ 的/ 排序/ 算法/ 采用/ 的/ 是/ 堆排序/ ,/ 该行/ 的/ 平均/ 时间/ 复杂度/ 为/ O/ (/ mlogm/ )/ ,/ m/ 为/ Reducer/ 个数/ ./ 第/ 6/ 行/ 执行/ 受/ 第/ 2/ 行/ WHILE/ 循环/ 的/ 影响/ ,/ 且/ 每/ 分配/ 一个/ 分区/ 都/ 需/ 要求/ 出/ 分配/ 后/ 的/ 最大/ 和/ 最小/ 负载/ 节点/ ,/ 这部分/ 总/ 的/ 执行/ 步骤/ 为/ 其中/ :/ ι/ 为/ 分配/ 轮数/ ;/ λ/ 是/ 分区/ 的/ 放大/ 倍数/ ,/ 都/ 是/ 一个/ 常数/ ;/ m/ 为/ Reducer/ 个数/ ;/ n/ 为/ Micro/ -/ partition/ 的/ 总/ 个数/ ./ 第/ 2/ 部分/ 的/ 执行/ 效率/ 受/ 第/ 9/ 行/ 、/ 第/ 15/ 行/ 和/ 第/ 21/ 行/ 的/ 影响/ ,/ 而/ 这/ 两行/ 又/ 是/ 等价/ 的/ 并且/ 是/ 最/ 基本/ 的/ 循环/ ,/ 以/ 14/ 行为/ 例/ ,/ 该行/ 最坏/ 情况/ 下/ 的/ 值/ 为/ Na/ -/ 2/ ,/ 即/ 集群/ 中/ 只有/ 两个/ Reducer/ ,/ 其中/ 一个/ Reducer/ 节点/ 上/ 只/ 包含/ 有/ 一个/ Micro/ -/ partition/ ,/ 其余/ 全部/ Micro/ -/ partition/ 在/ 另/ 一个/ 节点/ ,/ 则/ 该/ 节点/ 最坏/ 情况/ 下/ 需要/ 进行/ Na/ -/ 1/ 循环/ 才能/ 取出/ 该/ 节点/ 负载/ 总量/ 一半/ 的/ 分区/ ./ 对于/ 第/ 9/ 行/ 的/ 外/ 循环/ ,/ 在/ 最坏/ 情况/ 下/ ,/ 需要/ 执行/ 的/ 步骤/ 为/ log/ (/ mNa/ )/ ,/ 即将/ 所有/ 分区/ 进行/ 一次/ 对换/ ./ 因此/ 得出/ 该/ 部分/ 总/ 的/ 执行/ 步骤/ 为/ T2/ (/ m/ )/ =/ (/ logmNa/ )/ ×/ (/ Na/ -/ 2/ +/ Na/ -/ 2/ )/ 将/ T1/ (/ m/ )/ 和/ T2/ (/ m/ )/ 相加/ 得到/ 算法/ 2/ 的/ 复杂度/ 为/ T/ (/ m/ )/ =/ T1/ (/ m/ )/ +/ T2/ (/ m/ )/ =/ 综上/ 可知/ 该/ 算法/ 的/ 时间/ 复杂度/ 为/ O/ (/ mlogm/ )/ ,/ 受/ 用户/ 所/ 定义/ Reducer/ 个数/ m/ 的/ 影响/ ./ 引理/ 1/ ./ 增量/ 式/ 分区/ 分配/ 算法/ 的/ 近似/ 比为/ 2/ ./ 证明/ ./ 设/ 集群/ 中有/ m/ 个/ Reducer/ ,/ 最大/ 负载/ Reducer/ 节点/ 上/ 的/ 负载量/ 记为/ Lmax/ ,/ 最小/ 负载/ Reducer/ 节点/ 上/ 的/ 负载量/ 记为/ Lmin/ ./ 令/ 在/ 最优/ 解中/ Lmax/ 的/ 值/ 为/ Lopt/ ,/ 本文/ 算法/ 2/ 得到/ Lmax/ 的/ 值/ 为/ L/ (/ m/ )/ ./ 假设/ 2Lopt/ </ L/ (/ m/ )/ ,/ 则/ 有/ L/ (/ m/ )/ =/ 2Lopt/ +/ δ/ ,/ δ/ 为/ 常数/ ./ 那么/ 1/ 节点/ 上/ 负载量/ 为/ Lopt/ 的/ 分区/ 对换/ 到/ Lmin/ 节点/ 上/ ,/ 得到/ 两个/ 节点/ 的/ 最新/ 负载量/ 为/ L/ (/ m/ )/ =/ Lopt/ +/ δ/ +/ 12Lmin/ 和/ Lmin/ =/ Page10/ 与/ 最小/ 负载量/ 的/ 差为/ |/ L/ (/ m/ )/ -/ Lmin/ |/ =/ δ/ ./ 在/ 对换/ 分区/ 前/ ,/ 最大/ 负载量/ 与/ 最小/ 负载量/ 的/ 差为/ L/ (/ m/ )/ -/ Lmin/ =/ 2Lopt/ -/ Lmin/ +/ δ/ >/ δ/ ,/ 所以/ 算法/ 2/ 继续执行/ 分区/ 对换/ 操作/ ,/ 因此/ L/ (/ m/ )/ 不是/ 最终/ 值/ ,/ 即/ L/ (/ m/ )/ ≠/ Lmax/ ,/ 而/ 这/ 又/ 与/ 已知/ 条件/ 矛盾/ ,/ 所以/ 假设/ 错误/ ./ 从而/ 得到/ 2Lopt/ / L/ (/ m/ )/ ,/ 即/ 增量/ 式/ 分区/ 分配/ 算法/ 的/ 近似/ 比为/ 2/ ./ 本文/ 算法/ 2/ 的/ 第/ 2/ ~/ 8/ 行/ 部分/ 等价/ 于/ 贪心/ 算法/ ,/ 在/ 此基础/ 上/ ,/ 本文/ 算法/ 又/ 增加/ 了/ 分区/ 调整/ 部分/ ,/ 导致/ 时间/ 复杂度/ 为/ O/ (/ mlogm/ )/ ./ 虽然/ 本文/ 算法/ 的/ 时间/ 复杂度/ 要/ 高于/ 一般/ 贪心/ 算法/ O/ (/ m/ )/ ,/ 但/ 本文/ 的/ 算法/ 具有/ 较/ 高/ 的/ 近似/ 比/ ,/ 而/ 对于/ 实际/ 的/ 集群/ 环境/ ,/ 比如/ Reducer/ 数目/ 很/ 有限/ 的/ 情况/ 下/ ,/ 分配/ 分区/ 算法/ 的/ 运行/ 时间/ 只/ 占/ 整个/ 任务/ 运行/ 时间/ 的/ 很小/ 比例/ ,/ 而/ 数据/ 划分/ 的/ 均衡性/ 对/ 整个/ 任务/ 的/ 运行/ 时间/ 有/ 更/ 大/ 影响/ ./ 算法/ 2/ 的/ 高效性/ 将/ 在下文/ 的/ 6.4/ 节/ ,/ 通过/ 真实/ 环境/ 和/ 真实/ 数据/ 集上/ 对比/ 实验/ 进行/ 验证/ ./ 图/ 3/ 增量/ 式/ 分区/ 在/ Hadoop/ 系统/ 上/ 的/ 实现/ 在/ 系统/ 实现/ 上/ ,/ 本/ 方法/ 将/ Counter/ 模块/ 添加/ 到/ 各/ Mapper/ 的/ 运行/ 线程/ 中/ ,/ 并/ 将/ 统计/ 结果/ 放到/ Local5/ 增量/ 策略/ 在/ Hadoop/ 上/ 的/ 实现/ 本/ 节/ 介绍/ 如何/ 在/ 标准/ Hadoop/ -/ 1.1/ ./ 2/ 系统/ 上/ 实现/ 增量/ 式/ 分区/ 策略/ ,/ 并/ 给出/ 系统/ 修改/ 方法/ 和/ 实现/ 的/ 相关/ 技术细节/ ./ Hadoop/ 系统/ 所/ 实现/ 的/ MapReduce/ 架构/ 并/ 不/ 支持/ 增量/ 式/ 分区/ 模型/ ,/ 为/ 实现/ 该/ 功能/ ,/ 本文/ 在/ 原/ 架构/ 上/ 新增/ 了/ 3/ 个/ 功能模块/ 和/ 4/ 个/ 数据/ 构/ ,/ 如图/ 3/ 中/ 的/ 虚/ 线框/ 所示/ ./ 新增/ 的/ 3/ 个/ 功能模块/ 分别/ 为/ Counter/ 、/ DecisionModel/ 和/ AddNewPartition/ ,/ 分别/ 对应/ 于/ 统计/ 结果/ 的/ 收集/ 、/ 分配/ Micro/ -/ partition/ 和/ 多/ Micro/ -/ partition/ 重组/ Fine/ -/ partition/ 功能/ ,/ 并/ 将/ 3/ 个/ 模块/ 分别/ 在/ 原/ 系统/ 的/ MapTask/ 、/ JobTracker/ 和/ ReduceTask/ 类中/ 实现/ ./ 同时/ 新增/ 两类/ 结构/ 体/ :/ CounterTable/ 和/ AssignPlan/ ,/ 分别/ 用于/ 实现/ 统计/ 向量/ 犃/ 和/ 分配/ 向量/ 犃/ 犚/ ./ CounterTable/ 中/ ./ 由于/ LocalCounterTable/ 的/ 大小/ 由/ Micro/ -/ partition/ 个数/ 决定/ ,/ 并且/ 该值/ 在/ 同一个/ 任/ Page11/ 务中/ 是/ 固定/ 不变/ 的/ ,/ 因此/ 本文/ 以/ 一维/ 数组/ 的/ 形式/ 进行/ 存储/ ,/ 并/ 将/ 该表/ 驻留/ 在/ 内存/ 中/ ./ 为/ 减少/ Task/ 与/ Job/ -/ Tracker/ 之间/ 的/ 通信/ 开销/ ,/ 将/ LocalCounterTable/ 加入/ 到/ Heartbeat/ 中/ ./ 当/ 一个/ Mapper/ 运行/ 完成/ 后/ ,/ 会/ 将/ 产生/ 的/ 临时/ 结果/ 写入/ 本地/ 磁盘/ ,/ 并/ 将/ 各/ Micro/ -/ partition/ 的/ 索引/ 位置/ 记录/ 在/ MapOutputFile/ 中/ ,/ 以便/ 各/ Reducer/ 读取/ ,/ 这部分/ 仍/ 采用/ 原/ 结构/ ,/ 不同之处/ 是/ MapOutputFile/ 将/ 包含/ 更/ 多/ 的/ 索引/ 信息/ ./ 将/ Micro/ -/ partition/ 的/ 筛选/ 和/ 分配/ 添加/ 到/ Job/ -/ Tracker/ 类中/ ,/ 在/ 该类/ 新增/ 了/ DecisionModel/ 模块/ ,/ 主要/ 用于/ 实现/ 4.3/ 节中/ 的/ 增量/ 式/ 分区/ 分配/ 算法/ ,/ GlobalCounterTable/ 用于/ 对/ 所有/ Task/ 的/ LocalCounterTable/ 进行/ 汇总/ ,/ 并/ 将/ 分配/ 计划/ 添加/ 到/ GlobalAssignPlan/ 中/ ,/ 在/ 完成/ 每次/ 决策/ 后/ 需要/ 将/ 该表/ 的/ 更新/ 增加/ 到/ 下次/ 的/ Heartbeat/ 通信/ 中/ ,/ 从而/ 可以/ 将/ 分配/ 计划/ 实时/ 地/ 传到/ Reducer/ 节点/ 上/ ./ ReduceTask/ 通过/ 对/ Heartbeat/ 的/ 解析/ ,/ 获取/ 属于/ 自身/ 的/ 分区/ 信息/ ,/ 对于/ 原/ MapReduce/ 架构/ ,/ 各/ Reducer/ 只/ 需/ 将/ 自身/ 的/ 分区/ 信息/ 添加/ 到/ MapOut/ -/ putLocation/ 中/ ,/ 并/ 开始/ 等待/ Mapper/ 的/ 完成/ ./ 当有/ Mapper/ 完成/ 时/ ,/ ReadPartition/ 模块/ 依据/ 分区/ 索引/ 通过/ Http/ 协议/ ,/ 开始/ 从/ 已/ 完成/ 的/ Mapper/ 中/ 读取数据/ ./ 而/ 对于/ 增量/ 式/ 分区/ ,/ 由于/ Reducer/ 的/ 输入/ 来源于/ 多个/ 分区/ ,/ 因此/ ,/ 增加/ AddNewPartition/ 模块/ ,/ 渐进/ 地/ 将/ 分区/ 信息/ 增加/ 到/ LocalPartition/ 中/ ,/ 然后/ 将/ LocalPartition/ 中/ 的/ 分区/ 信息/ 转换/ 为/ 分区/ 的/ 存储/ 路径/ 并存/ 放到/ MapOutputLocation/ 中/ ,/ 该/ 模块/ 既/ 可以/ 实现/ 对/ Reducer/ 初始化/ 时多/ 分区/ 分配/ ,/ 又/ 可以/ 完成/ 在/ Reducer/ 读取数据/ 过程/ 中/ 的/ 增量/ 式/ 分配/ ,/ 从而/ 实现/ 对/ Reducer/ 的/ 渐进式/ 分配/ 方法/ ./ 与/ 原/ 系统/ 相同/ ,/ 等/ 所有/ 输入/ 完成/ 后/ ,/ 开始/ 执行/ Reduce/ (/ )/ 函数/ ,/ 并/ 将/ 结果/ 写入/ HDFS/ 中/ ,/ 完成/ 整个/ 作业/ ./ 6/ 实验/ 结果/ 与/ 分析/ 本/ 节/ 通过/ 两/ 部分/ 实验/ 来/ 验证/ 增量/ 式/ 分区/ 策略/ 的/ 有效性/ ,/ 第/ 1/ 部分/ 实验/ 重点/ 讨论/ 2/ 个/ 增量/ 参数/ 对/ 该/ 方法/ 的/ 影响/ ,/ 第/ 2/ 部分/ 通过/ 标准/ Zipf/ 分布/ 数据/ 集/ 和/ 真实/ 数据/ 集/ ,/ 与/ 原/ 系统/ 和/ Closer/ [/ 8/ ]/ 方法/ 做/ 对比/ 实验/ ,/ 检验/ 3/ 个/ 系统对/ 数据/ 均衡/ 的/ 处理/ 能力/ ./ 6.1/ 集群/ 环境/ 本文/ 实验/ 的/ 环境/ 是/ 11/ 个/ 节点/ 的/ 集群/ ,/ 其中/ 1/ 个/ Master/ 节点/ 负责/ 任务/ 的/ 调度/ 和/ 集群/ 管理/ ,/ 不/ 进行/ 数据/ 计算/ ;/ 10/ 个/ 数据/ 节点/ ,/ 用于/ 数据/ 的/ 存储/ 和/ 计算/ ./ 每个/ 节点/ 的/ 系统配置/ 为/ 16/ 核/ 主频/ 为/ 2.20/ GHz/ 的/ AMD/ 处理器/ ,/ 16GB/ 的/ RAM/ 和/ 500GB/ 的/ 硬盘/ ,/ 节点/ 之间/ 通过/ 1Gbit/ 的/ 网络连接/ ,/ 各/ 节点/ 用/ 的/ 是/ 64/ 位/ UbuntuLinux12/ ./ 01/ ,/ 使用/ 的/ 对比/ 系统/ 是/ Hadoop1/ ./ 1.2/ ,/ 所/ 修改/ 的/ 系统/ 也/ 是/ Hadoop1/ ./ 1.2/ ./ 修改/ Hadoop/ 系统/ 的/ 默认/ 配置/ ,/ 定义/ 每个/ 节点/ 有/ 16/ 个/ Mapslot/ 与/ 处理器/ 核数/ 相等/ ,/ 使/ Reduceslot/ 个数/ 等于/ Reducer/ 个数/ ,/ 分区/ 函数/ 使用/ 默认/ 的/ Hash/ 分区/ 函数/ ,/ 其他/ 参数/ 也/ 都/ 使用/ 默认值/ ./ 为/ 使/ 实验/ 结果/ 具有/ 说服力/ ,/ 同/ 一组/ 实验/ 运行/ 10/ 次后/ 取/ 平均值/ 记/ 为/ 最终/ 结果/ ./ 6.2/ 实验/ 数据/ 实验/ 数据/ 采用/ 真实/ 数据/ 集/ 和/ 合成/ 数据/ 集/ ,/ 两类/ 数据/ 集/ 的/ 详细描述/ 如表/ 1/ 所示/ ./ 合成/ 数据/ 包含/ 11/ 个子/ 数据/ 集/ ,/ 每个/ 子/ 数据/ 集/ 满足/ 指数/ 为/ γ/ 的/ 标准/ Zipf/ 分布/ ,/ γ/ 取以/ 0.1/ 为/ 增量/ 从/ 0.0/ ~/ 1.0/ 的/ 小数/ ,/ γ/ 的/ 值/ 越/ 大/ ,/ 数据/ 的/ 分布/ 越/ 倾斜/ ./ 各/ 子集/ 为/ 单列/ 整数/ 数值数据/ ,/ 数值/ 的/ 取值/ 范围/ 为/ [/ 1/ ,/ 1000/ ]/ ,/ 并/ 包含/ 10/ 亿条/ 记录/ ./ 为/ 满足/ 实验/ 用例/ 特点/ ,/ 对/ 真实/ 数据/ 集做/ 去/ 杂/ 处理/ ,/ 由于/ 杂质/ 记录/ 很少/ ,/ 此/ 操作/ 并未/ 改变/ 原/ 数据/ 集/ 分布/ 特点/ ./ 数据/ 集/ 运行/ 实例/ 大小/ // GB/ 元组/ 数/ // 亿/ Zipf/ -/ γ/ WordCount0/ ./ 4BTS/ ①/ WordCount60/ ./ 0UK02/ ②/ PageRank6/ ./ 3/ 增量/ 策略/ 参数/ 评估/ 实验/ 除/ Hadoop/ 系统/ 本身/ 参数设置/ 外/ ,/ 用于/ 控制/ 增量/ 式/ 分区/ 执行/ 效率/ 的/ 2/ 个/ 参数/ 为/ 分配/ 轮数/ ι/ 和/ 放大系数/ λ/ ./ 本/ 部分/ 实验/ ,/ 重点/ 用来/ 验证/ 这/ 2/ 个/ 增量/ 参数/ 是/ 如何/ 影响/ 系统/ 运行/ 效率/ 的/ ,/ 对/ 运行/ 效率/ 的/ 评估/ 本/ 部分/ 实验/ 使用/ 运行/ 时间/ 和/ 标准/ 方差/ 表示/ ./ 运行/ 时间/ 用于/ 衡量/ 一个/ 作业/ 运行/ 的/ 快慢/ ,/ 重点/ 体现/ 最长/ Reducer/ 运行/ 之间/ 的/ 差异/ ,/ 而/ 标准/ 方差/ 用于/ 衡量/ 各/ Reducer/ 之间/ 数据分布/ 的/ 均衡/ 效果/ ,/ 标准/ 方差/ 的/ 计算/ 首先/ 是/ 获取/ 各/ Reducer/ 接收/ 到/ 元组/ 的/ 总量/ (/ 通过/ 输出/ 的/ 日志/ 直接/ 获取/ )/ ,/ 然后/ 以/ 总量/ 为/ 元素/ 计算/ 标准/ 方差/ ,/ 标准/ 方差/ 越大/ 说明/ 分区/ 越/ 不/ 均衡/ ./ 通过/ 这/ 两个/ 指标/ 既/ 可以/ 反映/ Reducer/ 的/ 运行/ 时间/ 又/ 可以/ 反映/ 出/ 各/ Reducer/ 之间/ 接收数据/ 总量/ 的/ 差异性/ ./ 为/ 分析/ 各/ ①/ ②/ Page12/ 参数/ 独立/ 影响/ 的/ 大小/ ,/ 在/ 对/ 其中/ 一项/ 进行/ 分析/ 时/ ,/ 令/ 其他/ 所有/ 参数/ 为/ 常量/ ./ 这部分/ 实验所/ 使用/ 的/ 数据/ 是/ 标准/ Zipf/ 分布/ 的/ 数据/ 集/ ,/ 运行/ 的/ 是/ 经典/ WordCount/ 实例/ ,/ 该/ 算法/ 在/ Reducer/ 端/ 只/ 进行/ 求和/ 操作/ ,/ 因此/ 影响/ 该/ 算法/ 运行/ 时间/ 的/ 因素/ 主要/ 体现/ 在/ Map/ 阶段/ ./ (/ 1/ )/ 分配/ 轮数/ ι/ ./ 分配/ 轮数/ 决定/ 着/ 将/ 分/ 区分/ 几次/ 指派/ 到/ Reducer/ 上/ ,/ 使用/ 的/ 数据/ 集为/ Zipf/ -/ 0.7/ ,/ 统计/ 结果/ 的/ 收集/ 间隔/ 为/ 1E10/ 条/ 记录/ (/ 一个/ Heartbeat/ 通信/ 时间/ 内/ Mapper/ 处理/ 的/ 元组/ 数/ )/ ,/ 评测/ 标准/ 为/ 运行/ 时间/ 和/ 各/ Reducer/ 节点/ 接收/ 元组/ 总数/ 的/ 标准/ 方差/ ./ 实验/ 结果/ 如图/ 4/ 所示/ ./ 图/ 4/ (/ a/ )/ 是/ 分配/ 次数/ 与/ 运行/ 时间/ 关系/ 的/ 实验/ 结果/ 图/ ,/ 横坐标/ 是/ 一次/ 作业/ 运行/ 过程/ 中/ 的/ 分配/ 轮数/ ,/ 纵坐标/ 是/ 整个/ 作业/ 的/ 运行/ 时间/ ./ 从/ 整个/ 作业/ 的/ 趋势/ 来看/ ,/ 伴随/ 着/ 分配/ 轮数/ 的/ 增加/ 运行/ 时间/ 逐渐/ 递减/ ,/ ι/ 在/ 图/ 4/ 分配/ 轮数/ 影响/ 实验/ (/ 2/ )/ 放大系数/ λ/ ./ 放大系数/ 决定/ 着/ 产生/ Micro/ -/ partition/ 的/ 粒度/ ,/ 粒度/ 越小所/ 产生/ 的/ 分区/ 越细/ ,/ 合并/ 后/ 产生/ Fine/ -/ partition/ 的/ 分区/ 越/ 均衡/ ./ λ/ 能取/ 到/ 的/ 最大/ 有/ 意义/ 值/ 是/ 使得/ Micro/ -/ partition/ 的/ 个数/ 等于/ 输入/ key/ 值/ 的/ 个数/ ,/ 即将/ 元组/ 值/ 相同/ 的/ 所有/ 记录/ 作为/ 一个/ 微分/ 区/ ,/ 但/ 这样/ 做/ 又/ 会/ 增大/ Reducer/ 读取数据/ 时/ 的/ 负载/ 和/ JobTracker/ 分配任务/ 的/ 负载/ ./ 因此/ 通过/ 这部分/ 实验/ 来/ 验证/ ,/ 对于/ 不同/ 的/ 数据分布/ ,/ 放大系数/ 是/ 如何/ 对/ 运行/ 时间/ 和/ 均衡性/ 产生/ 影响/ 的/ ./ 本文/ 使用/ 两个/ 不同/ 分布/ 的/ 数据/ 集/ Zipf/ -/ 0.3/ 和/ Zipf/ -/ 0.7/ ,/ 统计/ 结果/ 的/ 收集/ 间隔/ 为/ 1E10/ 条/ 记录/ (/ 一个/ Heartbeat/ 通信/ 时间/ 内/ Mapper/ 处理/ 的/ 元组/ 数/ )/ ,/ 分配/ 次数/ 选定/ 为/ 10/ ./ 实验/ 结果/ 如图/ 5/ 所示/ ./ 从图/ 5/ (/ b/ )/ 可以/ 发现/ ,/ 对/ 数据/ 集/ 分布/ 相对/ 均衡/ 的/ 取值/ 为/ 10/ 附近/ 时/ 达到/ 稳定/ 值/ ./ 图/ 4/ (/ b/ )/ 描述/ 的/ 是/ 以/ 标准/ 方差/ 为/ 性能/ 的/ 实验/ 图/ ,/ 该图/ 中/ 曲线/ 的/ 趋势/ 与/ 运行/ 时间/ 的/ 趋势/ 相同/ ,/ 当/ 分配/ 次数/ 到达/ 8/ 时/ ,/ 标准/ 方差/ 趋于稳定/ 值/ ,/ 再/ 增加/ 分配/ 轮数/ 对/ 该值/ 的/ 变化/ 不会/ 产生/ 什么/ 影响/ ./ 通过/ 实验/ 结果/ 可以/ 发现/ ,/ 分配/ 次数/ 越高/ 产生/ 的/ 均衡/ 效果/ 越/ 好/ ,/ 这/ 是因为/ 伴随/ 着/ 分配/ 次数/ 的/ 增加/ ,/ 可以/ 及时/ 修正/ 因/ 过早/ 分配/ 产生/ 的/ 误差/ ,/ 分配/ 次数/ 越/ 多/ ,/ 修正/ 的/ 效果/ 越好/ ./ 但/ 由于/ 数据/ 本身/ 的/ 分布/ 特点/ 和/ 产生/ Micro/ -/ partition/ 的/ 粒度/ ,/ 会/ 导致/ 分配/ 次数/ 有/ 一个/ 最优/ 下限/ ,/ 即/ 必须/ 保证/ 在/ 特定/ 放大/ 倍数/ 下且/ 在/ 所有/ Mapper/ 结束/ 前/ 将/ 所有/ 的/ 分区/ 分配/ 完成/ ./ 在/ 系统/ 实现/ 上/ ,/ 本文/ 将/ 该值/ 设定/ 为/ 预留/ 值/ ,/ 用户/ 可/ 根据/ 所/ 要/ 处理/ 的/ 数据/ 特征/ 进行/ 自定义/ ,/ 本文/ 定义/ 该值/ 的/ 默认值/ 为/ 10/ ,/ 在/ 分配/ 轮数/ 为/ 10/ 时/ ,/ 该/ 方法/ 基本/ 可以/ 实现/ 大多数/ 分布/ 数据/ 集/ 的/ 数据/ 均衡性/ ./ Zipf/ -/ 0.3/ ,/ 放大系数/ 为/ 4/ 时/ 就/ 能/ 实现/ 各/ Reducer/ 接收数据/ 总量/ 的/ 均衡性/ ,/ 随着/ 放大系数/ 的/ 增大/ ,/ 即/ 分区/ 粒度/ 的/ 减少/ ,/ 对/ 均衡性/ 产生/ 的/ 影响/ 越小/ ./ 而/ 对于/ 数据分布/ 相对/ 倾斜/ 的/ Zipf/ -/ 0.7/ ,/ 可以/ 明显/ 发现/ 随着/ 放大/ 倍数/ 的/ 增大/ ,/ 数据/ 划分/ 的/ 均衡/ 效果/ 越/ 好/ ,/ 在/ 系数/ 为/ 10X/ 时/ ,/ 均衡性/ 开始/ 趋于稳定/ ,/ 而/ 当/ 放大系数/ 为/ 14X/ 时/ ,/ 产生/ 的/ 均衡/ 效果/ 甚至/ 比/ Zipf/ -/ 0.3/ 还好/ ./ 此/ 实验/ 验证/ 了/ 通过/ 增大/ 放大系数/ 来/ 减小/ 分区/ 粒度/ 能/ 更好/ 地/ 实现/ 数据/ 划分/ 的/ 均衡性/ ,/ 并且/ 对于/ 不同/ 的/ 数据分布/ 都/ 有/ 一个/ 最小/ 的/ 系数/ 使/ 均衡性/ 趋于/ 某个/ 稳定/ 值/ ./ 对于/ 本组/ 实验/ ,/ 影响/ 其/ 运行/ 时间/ 的/ 主要/ 因素/ 是/ Shuffle/ 阶段/ 的/ 数据传输/ ,/ 从图/ 5/ (/ a/ )/ 可以/ 发现/ ,/ 随着/ 放大系数/ 增大/ ,/ 运行/ 时间/ 逐渐/ 减少/ ,/ 是因为/ 分区/ 的/ 粒度/ 逐渐/ 变小/ ,/ 数据/ 能/ 更/ 早/ 地/ 以/ 小/ 粒度/ 传输/ ./ 如同/ 分配/ 轮数/ λ/ ,/ 该值/ 也/ 是/ 预留/ 值/ ,/ 用户/ 可/ 根据/ 处理/ 数据/ 的/ 特征/ 自定/ Page13/ 图/ 5/ 放大系数/ 影响/ 实验/ 义/ 取值/ ./ 为/ 保证/ 每轮/ 分配/ 时/ ,/ 各/ Reducer/ 都/ 可/ 接收数据/ ,/ 本文/ 同样/ 定义/ 放大系数/ 的/ 默认值/ 为/ 10/ ,/ 即/ 等于/ 分配/ 轮数/ ./ 6.4/ 对比/ 实验/ 本/ 小节/ 将/ 通过/ 经典/ WordCount/ 和/ PageRank/ 算法/ 对比/ 3/ 个/ 系统/ Hadoop/ 、/ Closer/ [/ 8/ ]/ 和/ 本文/ 的/ 方法/ IPS/ (/ IncrementalPartitioningScheduling/ )/ 对/ 数据/ 均衡/ 的/ 处理/ 能力/ ./ Hadoop/ 使用/ 默认/ 的/ Hash/ 分区/ 和/ 随机/ 分配原则/ ;/ Closer/ 采用/ 的/ 是/ 一次/ 切割/ 分区/ 的/ 方式/ ,/ 即/ 当/ 某/ 分区/ 超过/ 一定/ 阀值/ 时/ 将/ 该/ 分区/ 分解/ ,/ 并/ 将/ 超过/ 部分/ 的/ 数据/ 传送/ 到/ 其他/ 数据量/ 较/ 小/ 的/ 分区/ 上/ ,/ 以此/ 解决/ 各/ Reducer/ 接收数据/ 均衡/ 的/ 问题/ ;/ 在/ 所有/ 的/ 对比/ 实验/ 中/ ,/ 定义/ IPS/ 增量/ 参数/ ι/ =/ 10/ 和/ λ/ =/ 10/ ,/ 为/ 验证/ 3/ 种/ 方法/ 的/ 效果/ ,/ 在/ 运行/ 同/ 一组/ 实验/ 时/ 让/ 各/ 系统/ 都/ 取/ 相同/ 的/ 参数/ ./ (/ 1/ )/ WordCount/ ./ 由于/ WordCount/ 的/ 操作/ 在/ Reducer/ 端/ 只/ 进行/ 加法/ 操作/ ,/ 因此/ 运行/ 时间/ 不能/ 真实/ 反映/ 出/ Reducer/ 端/ 数据/ 均衡性/ 的/ 效果/ ,/ 本/ 部分/ 实验/ 选取/ 的/ 评测/ 指标/ 是/ 各/ Reducer/ 要/ 处理/ 数据/ 总量/ 的/ 大小/ ,/ 本文/ 选取/ 最大/ 负载/ 节点/ 上/ 的/ 总/ 元组/ 数/ 和/ 标准/ 方差/ ./ 最大/ 负载/ 节点/ 的/ 总/ 元组/ 数是/ 对/ 目标/ 函数/ 的/ 评测/ ,/ 用于/ 反映/ 处理/ 数据量/ 最多/ Reducer/ 的/ 运行/ 快慢/ ,/ 而/ 标准/ 方差/ 反映/ 了/ 各/ Reducer/ 之间/ 数据/ 差异性/ ./ 首先/ 在/ 生成/ 的/ 11/ 个/ 标准/ Zipf/ 数据/ 集上/ 对比/ 三/ 系统对/ 数据/ 划分/ 均衡/ 的/ 处理/ 能力/ ./ 如图/ 6/ 中/ 横坐标/ 代表/ 指数/ 从/ 0.0/ ~/ 1.0/ 的/ 11/ 个/ 标准/ Zipf/ 分布/ 数据/ 集/ ,/ 图/ 6/ (/ a/ )/ 纵坐标/ 是/ 最大/ 负载/ 节点/ 上/ 的/ 总/ 元组/ 数/ ,/ 图/ 6/ (/ b/ )/ 纵坐标/ 为/ 各/ Reducer/ 接收数据/ 量/ 的/ 标准/ 方差/ ,/ 图/ 6/ (/ c/ )/ 纵坐标/ 为/ 整个/ 任务/ 的/ 运行/ 时间/ ./ 从图/ 6/ (/ a/ )/ 可以/ 发现/ 在/ Zipf/ -/ 0.0/ ~/ Zipf/ -/ 0.3/ 分布/ 之间/ ,/ Closer/ 和/ IPS/ 最大/ 负载/ 节点/ 上/ 的/ 数据量/ 是/ 相等/ 的/ ,/ 而且/ 都/ 要/ 小于/ 原/ 系统/ ,/ 从图/ 6/ (/ b/ )/ 标准/ 方差/ 可以/ 发现/ Closer/ 和/ IPS/ 的/ 方差/ 趋于/ 0/ ,/ 说明/ 各/ Reducer/ 所/ 接收/ 的/ 数据量/ 相近/ ,/ 即/ 都/ 趋于/ 最大值/ ,/ 但/ Hadoop/ 系统/ 的/ 方差/ 要/ 明显/ 大于/ 两者/ ,/ 说明/ 采用/ 默认/ 哈希/ 法在/ 数据分布/ 均匀/ 的/ 情况/ 下/ ,/ 也/ 难能/ 保证/ 各/ Reducer/ 接收数据/ 的/ 均衡性/ ,/ 图/ 6/ (/ c/ )/ 的/ 运行/ 时间/ 同样/ 验证/ 了/ 这/ 一点/ ./ Closer/ 系统/ 从/ Zipf/ -/ 0.4/ ~/ Zipf/ -/ 1.0/ ,/ 最大/ 负载/ 节点/ 的/ 数据量/ 和/ 标准/ 方差/ 两个/ 指标/ 都/ 表现/ 出/ 递增/ 的/ 趋势/ ,/ 说明/ 随着/ 数据分布/ 倾斜/ 的/ 增大/ ,/ Closer/ 系统对/ 数据/ 均衡/ 的/ 处理/ 能力/ 逐渐/ 减弱/ ,/ 而/ 从/ 图/ 6/ (/ b/ )/ 可以/ 看出/ ,/ 在/ Zipf/ -/ 0.0/ ~/ Zipf/ -/ 0.7/ 之间/ ,/ IPS/ 始终保持/ 两个/ 指标/ 的/ 稳定/ ,/ 说明/ IPS/ 并/ 不会/ 随着/ 数据分布/ 倾斜/ 的/ 增加/ 而/ 出现/ 性能/ 上/ 的/ 变化/ ,/ 由于/ 运行/ 的/ 实例/ 是/ WordCount/ ,/ 导致/ Reduce/ (/ )/ 函数/ 的/ 执行/ 时间/ 会/ 很/ 短/ ,/ 因此/ 在/ Zipf/ -/ 0.0/ ~/ Zipf/ -/ 0.7/ 之间/ ,/ 图/ 6/ (/ c/ )/ 的/ 运行/ 时间/ 两者/ 差别/ 不大/ ./ 对于/ Zipf/ -/ 0.9/ ~/ Zipf/ -/ 1.0/ 两个/ 数据/ 集/ ,/ 该/ 方法/ 发生/ 了/ 更/ 大/ 的/ 倾斜/ ,/ 这/ 是因为/ 在/ 分布/ 指数/ 为/ 0.9/ 和/ 1.0/ 的/ 标准/ Zipf/ 分布/ 中/ ,/ 存在/ 一个/ 元组/ ,/ 它/ 的/ 数据量/ 要/ 远大于/ 其他/ 所有/ 元组/ 的/ 数据量/ 之/ 和/ ,/ 造成/ 一个/ 节点/ 的/ 数据量/ 过于/ 庞大/ ,/ 其他/ 节点/ 如何/ 组合/ 相加/ 都/ 要/ 远/ 小于/ 该/ 元组/ ,/ 因此/ 出现/ 划分/ 数据/ 的/ 倾斜/ 问题/ ,/ 对于/ 使用/ 分区/ 切分/ 的/ Closer/ 系统/ 同样/ 不能/ 对/ 其/ 进行/ 处理/ ,/ 原因/ 是/ 发生/ 倾斜/ 的/ 是/ 不能/ 再次/ 进行/ 拆分/ 的/ 元组/ ,/ 而/ 不是/ 由/ 多个/ 元组/ 组成/ 的/ 分区/ ./ 但/ 从/ 运行/ 效果/ 来看/ ,/ 即使/ 这种/ 情况/ ,/ IPS/ 仍能/ 保证/ 集群/ 的/ 均衡/ 效果/ 优于/ 其他/ 两者/ ./ 图/ 6/ (/ a/ )/ 和/ 图/ 6/ (/ c/ )/ 具有/ 相同/ 的/ 变化趋势/ ,/ 说明/ 最大/ 节点/ 上/ 的/ 处理量/ 和/ 运行/ 时间/ 是/ 正/ 相关/ 的/ 关系/ ,/ 对于/ 数据分布/ 非常/ 倾斜/ 的/ 情况/ ,/ Page14/ 图/ 6Zipf/ 数据/ 集/ WordCount/ 的/ 对比/ 实验/ IPS/ 仍能/ 尽量减少/ 最大/ 节点/ 上/ 的/ 负载量/ 以此/ 来/ 提高/ 整个/ 任务/ 的/ 运行/ 时间/ ./ 通过/ Zipf/ 数据/ 集上/ 的/ 实验/ 可以/ 发现/ IPS/ 方法/ 在/ 取/ 默认值/ 的/ 情况/ 下/ ,/ 就/ 能够/ 处理/ 不同/ 分布/ 的/ 数据/ 集/ ./ 接下来/ 通过/ 真实/ 数据/ 集来/ 验证/ 3/ 个/ 系统/ 的/ 差异/ ,/ 图/ 7/ (/ a/ )/ 是/ 真实/ 数据/ 集/ 的/ 数据/ 分布图/ ,/ 从/ 该/ 图/ 可以/ 发现/ ,/ 近/ 50/ %/ 的/ 数据分布/ 在/ 0/ 附近/ ,/ 剩余/ 50/ %/ 的/ 数据/ 在/ 跨度/ 为/ 超过/ 10E6/ 的/ 值域/ 内/ ./ 图/ 7/ (/ b/ )/ 、/ (/ c/ )/ 、/ (/ d/ )/ 是/ 3/ 个/ 系统/ 的/ 均衡/ 性能/ 对比/ 实验/ ,/ 横坐标/ 是/ 运行/ 任务/ 的/ Reducer/ 个数/ ,/ 纵坐标/ 分别/ 为/ 最大/ 节点/ 上/ 的/ 数据量/ 、/ 各/ 节点/ 数据量/ 的/ 标准/ 方差/ 和/ 整个/ 任务/ 的/ 运行/ 时间/ ./ 图/ 7/ (/ b/ )/ 是/ 3/ 个/ 系统/ 最大/ 负载/ 节点/ 上/ 数据量/ 的/ 对比/ 实验/ ,/ 从/ 实验/ 结果/ 可以/ 看出/ IPS/ 最大/ 负载/ 节点/ 上/ 的/ 数据量/ 都/ 要/ 好/ 于/ 其他/ 两个/ 系统/ ,/ 随着/ Reducer/ 数目/ 的/ 增多/ ,/ 各/ 节点/ 接收/ 的/ 数据量/ 会/ 逐渐/ 减少/ ./ 从图/ 7/ (/ c/ )/ 上/ 可以/ 看出/ ,/ 对于/ 数据分布/ 变化/ 较大/ 的/ 数据/ 集/ ,/ Closer/ 系统/ 所/ 采用/ 的/ 一次/ 分区/ 调整/ 方法/ 在/ Reducer/ 为/ 20/ 时/ ,/ 仍/ 出现/ 了/ 较大/ 的/ 数据/ 倾斜/ 误/ ,/ 这是/ 由于/ 分配/ 前期/ 数据/ 的/ 统计/ 量/ 过少/ ,/ 而/ 在/ 运行/ 中/ 出现/ 了/ 最新/ 处理/ 的/ 数据/ 都/ 集中/ 在/ 了/ 某/ 几个/ 分区/ 上/ ,/ 从而/ 导致/ 再次发生/ 倾斜/ ,/ 而/ IPS/ 仍能/ 用/ 未/ 分配/ 分区/ 对/ Reducer/ 的/ 输入/ 分区/ 进行/ 调整/ ,/ 所以/ 能/ 更好/ 地/ 实现/ 均衡性/ ,/ 而/ 这种/ 数据/ 的/ 分布/ 差异/ 又/ 导致/ 了/ 图/ 7/ (/ d/ )/ 运行/ 时间/ 的/ 差异/ ,/ 可以/ 发现/ 在/ Reducer/ 为/ 20/ 时/ ,/ IPS/ 的/ 运行/ 时间/ 要/ 明显/ 优于/ 两者/ ./ 在/ Reducer/ 数目/ 为/ 50/ 时/ ,/ IPS/ 的/ 均衡性/ 最差/ ,/ 说明/ 此时/ 所/ 采用/ 的/ 增量/ 参数/ 没有/ 使/ 系统/ 产生/ 最好/ 的/ 均衡/ 效果/ ,/ 但/ 均衡性/ 都/ 要/ 明显/ 优于/ 其他/ 两者/ ./ 并且/ 从图/ 7/ (/ c/ )/ 可以/ 发现/ ,/ 随着/ Reducer/ 数目/ 的/ 增多/ ,/ 在/ 最大/ 负载/ 节点/ 的/ 数据量/ 保持/ 不变/ 的/ 前提/ 下/ ,/ 标准/ 方差/ 在/ 逐渐/ 增大/ ,/ 出现/ 这种/ 现象/ 是/ 由/ 数据分布/ 特征/ 所/ 引起/ 的/ ./ 也就是说/ IPS/ 能/ 平稳/ 控制/ 最大/ 负载/ 节点/ 上/ 的/ 数据量/ ,/ 而/ 造成/ 方差/ 变大/ 的/ 原因/ 是/ 由于/ 各/ 分区/ 的/ 粒度/ 随着/ Reducer/ 数目/ 的/ 增多/ 而/ 变小/ ,/ 造成/ 彼此间/ 的/ 差异/ 逐渐/ 增大/ ,/ 但/ IPS/ 能/ 始终/ 控制/ 最大/ 负载/ 节点/ 保持/ 不变/ ,/ 因此/ 并/ 不会/ 对/ 任务/ 的/ 总/ 运行/ 产生/ 影响/ ,/ 这点/ 从图/ 7/ (/ b/ )/ 和/ (/ d/ )/ 可以/ 看出/ ,/ IPS/ 能/ Page15/ 图/ 7BTS/ 数据/ 集/ WordCount/ 对比/ 实验/ 在/ Reducer/ 数目/ 变动/ 的/ 情况/ 下/ ,/ 通过/ 很/ 好/ 地/ 减少/ 最大/ 节点/ 上/ 的/ 数据量/ 来/ 控制/ 整个/ 任务/ 的/ 运行/ 时间/ ./ (/ 2/ )/ PageRank/ ./ 数据/ 的/ 划分/ 是否/ 均衡/ 最终/ 可/ 反映/ 到/ 运行/ 时间/ 上/ ,/ 由于/ WordCount/ 在/ Reduce/ (/ )/ 函数/ 中/ 只/ 进行/ 简单/ 的/ 加法/ 操作/ ,/ 运行/ 时间/ 效果/ 的/ 差异性/ 不能/ 明显/ 地/ 反映/ 出来/ ,/ 因此/ 本/ 节/ 使用/ Reduce/ (/ )/ 函数/ 计算/ 相对/ 复杂/ 的/ PageRank/ 算法/ ,/ 由于/ PageRank/ 每轮/ 的/ 数据分布/ 都/ 与/ 第/ 1/ 轮/ 的/ 分布/ 相同/ ,/ 因此/ 只/ 需要/ 比较/ 第/ 1/ 轮/ 的/ 运行/ 时间/ ./ 图/ 8/ 是/ 在/ 真实/ 数据/ 集/ UK/ -/ 2002/ 上/ 运行/ PageRank/ 一次/ 迭代/ 的/ 结果/ ./ 为/ 分析/ 因/ 数据分布/ 造成/ 对/ Reducer/ 运行/ 时间/ 的/ 影响/ ,/ 图/ 中将/ 1/ 个/ 作业/ 的/ 运行/ 划分/ 为/ 3/ 个/ 独立/ 阶段/ ,/ Mapper/ 运行/ 阶段/ 、/ Shuffle/ 运行/ 阶段/ 和/ Reducer/ 运行/ 阶段/ ,/ 其中/ Shuffle/ 运行/ 阶段/ 定义/ 为/ 没有/ 与/ Mapper/ 运行/ 发生/ 重叠/ 的/ 那/ 部分/ 时间/ ./ 对比/ 实验/ 分为/ 两组/ ,/ 分别/ 取/ Reducer/ 个数/ 为/ 20/ 和/ 50/ ,/ 用于/ 验证/ 在/ 不同/ 环境/ 下/ 3/ 个/ 系统/ 的/ 性能/ 表现/ ./ 首先/ 对比/ 3/ 个/ 系统/ 在/ Mapper/ 阶段/ 的/ 运行/ 时间/ ,/ 可以/ 发现/ 运行/ 时间/ 相差/ 不到/ 2s/ ,/ 说明/ 加入/ 统计/ 方法/ 后/ ,/ 并/ 没有/ 额外/ 增加/ Map/ 阶段/ 的/ 运行/ 时间/ ./ 在/ Reduce/ 阶段/ ,/ 可以/ 发现/ IPS/ 始终/ 是/ 3/ 个/ 系统/ 中/ 最优/ 的/ ,/ IPS/ 与/ 原/ 系统/ 相比/ 有近/ 50/ %/ 的/ 提升/ ,/ Closer/ 系统/ 的/ 拆分/ 方法/ 介于/ 两者之间/ ./ 通过/ PageRank/ 算法/ 可以/ 说明/ ,/ 以/ 元组/ 个数/ 为/ 单位/ 的/ 分区/ 划分/ 方法/ 不但/ 能/ 实现/ 数量/ 上/ 的/ 均衡性/ ,/ 同时/ 也/ 能/ 优化/ 计算/ 上/ 的/ 均衡/ ./ 通过/ 3/ 组/ 对比/ 实验/ 可以/ 发现/ ,/ Closer/ 所/ 采用/ 的/ Page16/ 一次/ 分区/ 调整/ 策略/ 也/ 难/ 避免/ 在/ 对/ 分区/ 切分/ 后/ 不再/ 发生/ 数据量/ 倾斜/ 的/ 问题/ ,/ 而/ 本文/ 所/ 采用/ 的/ 多轮/ 分区/ 分配/ 方法/ 可以/ 更/ 好地解决/ 数据/ 划分/ 不/ 均衡/ 的/ 问题/ ./ 7/ 总结/ 与/ 展望/ 本文/ 提出/ 一种/ 增量/ 式/ 分区/ 策略/ 来/ 实现/ 各/ Reducer/ 接收数据/ 均衡/ 的/ 问题/ ,/ 将/ 原/ MapReduce/ 的/ 一次/ 分区/ 分配机制/ ,/ 更/ 改为/ 一次/ 生成/ 和/ 多轮/ 分配/ 的/ 递增/ 机制/ ,/ 并/ 通过/ 定义/ 分区/ 代价/ 模型/ 实现/ 每轮/ 分配/ 后/ 各/ Reducer/ 接收数据/ 总量/ 的/ 均衡性/ ./ 最终/ 通过/ 标准/ Zipf/ 分布/ 数据/ 集/ 和/ 真实/ 数据/ 集/ ,/ 验证/ 了/ 该/ 方法/ 的/ 高效性/ 和/ 稳定性/ ./ 此外/ ,/ 本文/ 重点/ 解决/ 分区/ 后/ 数据/ 划分/ 均衡性/ 问题/ ,/ 对/ Reducer/ 在/ 运行/ 过程/ 中/ 所/ 出现/ 的/ 计算/ 倾斜/ 并未/ 做/ 研究/ ,/ 作者/ 已/ 开始/ 进行/ 这方面/ 的/ 研究/ 工作/ ,/ 进一步/ 工作/ 是/ 对/ MapReduce/ 的/ 负载/ 均衡/ 做/ 全面/ 研究/ ./ 

