Page1/ 一种/ 低/ 开销/ 的/ 面向/ 节点/ 内/ 互连/ 的/ 网络接口/ 控制器/ 苏勇/ 1/ )/ ,/ 2/ )/ 曹政/ 2/ )/ 刘/ 飞龙/ 1/ )/ ,/ 2/ )/ 王展/ 1/ )/ ,/ 2/ )/ 刘小丽/ 2/ )/ 安学军/ 2/ )/ 孙凝晖/ 2/ )/ 1/ )/ (/ 中国科学院/ 大学/ 计算机/ 与/ 控制/ 工程学院/ 北京/ 100190/ )/ 2/ )/ (/ 中国科学院计算技术研究所/ 计算机/ 体系结构/ 国家/ 重点/ 实验室/ 北京/ 100190/ )/ 摘要/ 高性能/ 计算/ 和/ 云/ 计算/ 的/ 飞速发展/ 对/ 高性能/ 互连/ 网络/ 的/ 设计/ 提出/ 了/ 越来越/ 高/ 的/ 要求/ :/ 除了/ 要/ 保证/ 高带宽/ 、/ 低/ 延迟/ 和/ 高可靠性/ 等/ 特性/ ,/ 还要/ 面临/ 成本/ 和/ 系统/ 规模/ 的/ 挑战/ ./ 该文/ 针对/ 这些/ 特性/ 和/ 挑战/ 提出/ 了/ 一种/ 低/ 开销/ 的/ 基于/ cHPP/ 体系结构/ 的/ 超/ 节点/ 网络接口/ 控制器/ :/ (/ 1/ )/ 设计/ 了/ 兼容/ PCIe/ 的/ 网络通信/ 协议/ ,/ 降低/ 协议/ 转换/ 开销/ 、/ 减少/ 通信/ 延迟/ 并/ 增强/ 系统/ 可扩展性/ 能/ ;/ (/ 2/ )/ 采用/ PCIe/ 高速/ 通信接口/ 并/ 支持/ 用户/ 级/ 通信/ 提高/ 软硬件/ 交互/ 效率/ ,/ 面向/ MPI/ 编程/ 模型/ 抽象/ 出/ 高效/ 通信/ 原语/ (/ 如/ NAP/ 、/ PUT/ 和/ GET/ )/ 加速/ 大/ 数据传输/ ;/ (/ 3/ )/ 硬件/ 支持/ I/ // O/ 虚拟化/ 实现/ 超/ 节点/ 内/ 对/ 网络接口/ 控制器/ 的/ 高效/ 共享/ ./ 为了/ 对/ 该文/ 的/ 设计/ 进行/ 功能/ 和/ 性能/ 验证/ ,/ 文章/ 基于/ FPGA/ 实现/ 了/ 系统/ 原型/ ,/ 实验/ 结果显示/ 最低/ 延迟/ 为/ 1.242/ μ/ s/ ,/ 有效/ 数据/ 带宽/ 可达/ 3.19/ GB/ // s/ ./ 关键词/ 互连/ ;/ 网络接口/ 控制器/ ;/ 直接/ 存储器/ 访问/ ;/ PCIExpress/ ;/ I/ // O/ 虚拟化/ 1/ 引言/ 大规模/ 并行/ 系统/ 广泛应用/ 于/ 高性能/ 计算/ 和/ 云/ 计算/ 领域/ ,/ 两个/ 领域/ 都/ 对/ 通信/ 系统/ 提出/ 新/ 的/ 需求/ ./ 高性能/ 计算/ 在/ 科学研究/ 、/ 工程技术/ 以及/ 国防/ 军工/ 等/ 方面/ 的/ 应用/ 取得/ 了/ 巨大成功/ ,/ 计算能力/ 突飞猛进/ ./ 基于/ 异构/ 架构/ 的/ “/ 天河/ II/ ”/ ①/ 超级计算机/ 以/ 峰值/ 计算速度/ 每秒/ 5.49/ 亿/ 亿次/ 、/ 持续/ 计算速度/ 每秒/ 3.39/ 亿/ 亿次/ 双/ 精度/ 浮点运算/ 的/ 优异/ 性能/ 成为/ 全球/ 最快/ 的/ 超级计算机/ ./ 异构计算/ 是/ 一种/ 高效/ 利用/ 各种/ 计算资源/ 的/ 并行/ 和/ 分布式计算/ 技术/ ,/ 在/ 提升/ 计算/ 性能/ 的/ 同时/ 降低成本/ 和/ 能耗/ ,/ 已经/ 成为/ 高性能/ 计算/ 发展/ 的/ 新/ 趋势/ ./ 因此/ ,/ 对/ 异构计算/ 模式/ 的/ 支持/ 是/ 高性能/ 计算/ 的/ 现实/ 需求/ ,/ 但是/ 在/ 传统/ 结构/ 中/ ,/ 协处理器/ 仅仅/ 挂载/ 在/ I/ // O/ 总线/ 上/ 作为/ 加速/ 部件/ 使用/ ,/ 大量/ 通信/ 需要/ 用主/ 处理器/ 内存/ 来/ 中转/ ,/ 难以获得/ 等同/ 的/ 网络/ 性能/ ./ 计算能力/ 的/ 不断/ 提高/ 也/ 要求/ 互连/ 网络/ 必须/ 具有/ 超/ 高带宽/ 、/ 超低/ 延迟/ 的/ 高性能/ ,/ 因此/ ,/ 异构计算/ 服务器/ 中/ 的/ 通信/ 性能/ 亟待/ 提高/ ./ 此外/ ,/ 随着/ 系统/ 规模/ 的/ 不断/ 增长/ ,/ 系统/ 的/ 成本/ 和/ 功耗/ 越来越/ 高/ ,/ 有效/ 降低成本/ 和/ 功耗/ 也/ 是/ 高性能/ 互连/ 网络/ 面临/ 的/ 难点/ 问题/ ./ 云/ 计算/ 在/ 助力/ 企业/ 发展/ ,/ 推动/ 技术/ 进步/ 等/ 方面/ 发挥/ 了/ 重要/ 作用/ ./ 虚拟化/ 技术/ 在/ 云/ 计算/ 领域/ 得到/ 了/ 迅猛发展/ ,/ 特别/ 是/ I/ // O/ 虚拟化/ ,/ 就/ 像/ 光纤/ 入户/ 技术/ 一样/ ,/ 成为/ 虚拟化/ 技术/ 的/ “/ 最后/ 一/ 公里/ ”/ ./ 在/ 虚拟化/ 环境/ 下/ ,/ 大量/ 并发/ 的/ 高/ 吞吐/ 率/ 负载/ 对/ 网络接口/ 控制器/ 提出/ 了/ 严峻/ 挑战/ ,/ 因此/ ,/ 迫切需要/ 加强/ 对/ I/ // O/ 资源/ 的/ 合理/ 高效/ 的/ 共享/ ./ 面对/ 上述/ 在/ 性能/ 和/ 共享/ 能力/ 上/ 的/ 挑战/ ,/ 本文/ 基于/ HPP/ (/ HyperParallelProcess/ )/ 体系结构/ 提出/ 了/ cHPP/ (/ configurableHPP/ )/ 体系结构/ ,/ 针对/ 面向/ 异构计算/ 的/ 通信/ 加速/ 和/ 基于/ 硬件/ 的/ 节点/ 内/ I/ // O/ 资源/ 高效/ 共享/ 设计/ ,/ 实现/ 了/ 一种/ 低/ 开销/ 的/ 面向/ 节点/ 内/ 互连/ 的/ 网络接口/ 控制器/ ./ 本文/ 第/ 2/ 节/ 介绍/ cHPP/ 体系结构/ ;/ 第/ 3/ 节/ 阐述/ 网络接口/ 控制器/ 设计/ 的/ 关键技术/ ;/ 第/ 4/ 节/ 阐述/ 网络接口/ 控制器/ 的/ 具体/ 实现/ 方法/ ;/ 第/ 5/ 节/ 进行/ 性能/ 评测/ 并/ 得出/ 有关/ 结论/ ;/ 第/ 6/ 节/ 阐述/ 目前/ 主流/ 高性能/ 计算机/ 网络接口/ 控制器/ 的/ 相关/ 研究/ ,/ 对比/ 各自/ 的/ 特点/ 和/ 不足/ ;/ 最后/ 是/ 全文/ 的/ 总结/ 和/ 对/ 未来/ 工作/ 的/ 展望/ ./ 2/ 超/ 节点/ 控制器/ 结构/ 2.1/ HPP/ 体系结构/ 超/ 并行/ (/ HPP/ )/ 体系结构/ [/ 1/ ]/ 是/ 中国科学院计算技术研究所/ 提出/ 的/ 一种/ 基于/ 超/ 节点/ 通信/ 性能/ 优化/ 的/ 高性能/ 计算机/ 体系结构/ ,/ 在/ 保证/ 分布式系统/ 的/ 高/ 扩展性/ 的/ 同时/ 基于/ 硬件/ 实现/ 了/ 全局/ 物理/ 内存/ 共享/ ,/ 支持/ 基于/ 共享/ 存储/ 的/ 编程/ 模型/ ./ 超/ 节点/ 是/ 指/ 多个/ 处理单元/ 通过/ 一个/ 超/ 节点/ 控制器/ 连接起来/ 构成/ 的/ 超级/ 节点/ ,/ 该/ 结构/ 能够/ 有效/ 降低/ 超/ 节点/ 内部/ 计算/ 单元/ 间/ 的/ 通信/ 延迟/ ,/ 同时/ 能够/ 减少/ 节点/ 数量/ 、/ 降低/ 互连/ 网络/ 规模/ ,/ 系统/ 的/ 平均/ 通信/ 延迟/ 也/ 随之/ 降低/ ./ 如图/ 1/ 所示/ ,/ HPP/ 体系结构/ 实现/ 了/ 芯片/ 、/ 节点/ 、/ 系统/ 多级/ 并行/ 结构/ :/ 通过/ 多核/ 处理器/ 实现/ 核间/ 并行/ 、/ 超/ 节点/ 内部/ 采用/ 异构/ 加速器/ 和/ 通用/ 处理器/ 实现/ 处理器/ 间/ 并行/ 、/ 超/ 节点/ 间/ 通过/ 互连/ 网络/ 构成/ 机群系统/ 实现/ 超/ 节点/ 间/ 并行/ ;/ 支持/ 全局/ 地址/ 空间/ ,/ 对超/ 节点/ 内/ 的/ 内存/ 和/ I/ // O/ 资源/ 统一/ 编址/ ,/ 使超/ 节点/ 的/ 处理器/ 可/ 对/ 全局/ 资源/ 高效/ 共享/ ;/ 支持/ 多通道/ 并发/ 的/ 核到/ 核/ 之间/ 的/ 通信/ ;/ 超/ 节点/ 操作系统/ 具有/ 单一/ 系统/ 映像/ 并/ 有效/ 支持/ MPI/ (/ MessagePassingInterface/ )/ 和/ PGAS/ (/ PartitionedGlobalAddressSpace/ )/ 编程/ 模型/ ./ 2.2/ cHPP/ 体系结构/ 为/ 满足/ 高性能/ 计算/ 和/ 云/ 计算/ 对/ 性能/ 的/ 共同/ 需求/ ,/ 同时/ 兼顾/ 异构计算/ 和/ 虚拟化/ 对/ 通信/ 系统/ 的/ 差异性/ 需求/ ,/ 在/ 曙光/ 6000HPP/ 体系结构/ 的/ 基础/ 上/ 提出/ 了/ cHPP/ 体系结构/ ./ cHPP/ 体系结构/ 的/ 首要/ 目标/ 是/ 加速/ 异构计算/ ,/ 提高/ 节点/ 计算/ 密度/ ,/ 通过/ cHPP/ 控制器/ ,/ 结合/ 高效/ 通信接口/ ,/ 实现/ 了/ 异构/ 处理器/ 间/ 的/ 直接/ 通信/ ./ 相比/ 于/ 传统/ 结构/ ,/ 这种/ 体系结构/ 可以/ 消除/ 通信/ 瓶颈/ ,/ 提高/ 通信/ 效率/ ,/ 并/ 有利于/ 系统/ 规模/ 扩展/ ./ 基于/ 硬件/ 支持/ 资源/ 的/ 聚集/ 和/ 高效/ 共享/ 是/ cHPP/ 体系结构/ 的/ 另/ 一/ 重要/ 目标/ ,/ 因此/ cHPP/ 控制器/ 除了/ 实现/ 处理器/ 对/ 内存/ 和/ I/ // O/ 资源/ 的/ 高速/ 访问/ ,/ 还/ 支持/ 灵活/ 有效/ 的/ 资源/ 聚集/ 和/ 高效/ 的/ 共享/ ./ cHPP/ 控制器/ 用于/ 构建/ 面向/ 高性能/ 计算/ 应用/ ,/ ①/ Top500/ ./ org/ (/ 2013/ )/ TOP500List/ ./ http/ :/ // // top500/ ./ org/ // lists/ // Page3/ 同时/ 兼顾/ 云/ 计算/ 需求/ 的/ 新型/ 高性能/ 服务器/ ./ 因此/ cHPP/ 控制器/ 既/ 要/ 实现/ 高性能/ 的/ 互连/ ,/ 还要/ 提供/ 虚拟化/ 的/ 相关/ 支持/ ./ 图/ 2/ 描述/ 的/ 是/ cHPP/ 控制器/ 用于/ 节点/ 内/ 互连/ 的/ 场景/ ,/ cHPP/ 控制器/ 既/ 可以/ 连接/ 通用/ 处理器/ 和/ 协处理器/ ,/ 也/ 可以/ 直接/ 连接/ I/ // O/ 设备/ 或/ I/ // O/ 桥/ ./ cHPP/ 控制器/ 提供/ 全面/ 的/ I/ // O/ 虚拟化/ 支持/ ,/ 节点/ 内/ 的/ I/ // O/ 设备/ 可以/ 被/ 虚拟/ 成/ 若干/ 虚拟/ 设备/ ./ 此外/ ,/ cHPP/ 控制器/ 本身/ 也/ 可以/ 被/ 虚拟/ 成/ 若干/ 虚拟/ 控制器/ ./ 所有/ 的/ 虚拟/ 设备/ 和/ 虚拟/ 控制器/ 均/ 可/ 被/ 直接/ 分配/ 给/ 节点/ 内/ 的/ 虚拟机/ ,/ 实现/ I/ // O/ 设备/ 被/ 处理器/ 的/ 直接/ 共享/ ./ 2.3/ cHPP/ 超/ 节点/ 控制器/ 为/ 满足/ 上述/ 场景/ 的/ 需要/ ,/ cHPP/ 控制器/ 主要/ 负责/ 超/ 节点/ 数据通信/ 和/ I/ // O/ 高效/ 共享/ 的/ 功能/ ,/ 支持/ 全局/ 地址/ 空间/ 和/ 用户/ 级/ 通信/ ,/ 采用/ PCIExpress/ 标准/ (/ PCIe/ )/ 的/ 高速/ I/ // O/ 接口/ 提升/ 链路/ 性能/ ,/ 并/ 增加/ 了/ 对/ 单根/ 虚拟化/ (/ SR/ -/ IOV/ )/ 规范/ 的/ 支持/ ,/ 实现/ 超/ 节点/ 内/ I/ // O/ 资源/ 的/ 高效/ 共享/ ./ 如图/ 3/ 所示/ ,/ 每个/ cHPP/ 控制器/ 含/ 多个/ 网络接口/ 控制器/ 和/ 交换/ 模块/ ,/ cHPP/ 控制器/ 间/ 通过/ PCIe/ 链路/ 直接/ 互连/ ,/ 支持/ 任意/ 网络拓扑/ 结构/ ./ 系统/ 规模/ 可/ 根据/ 需求/ 灵活/ 配置/ ,/ 具有/ 良好/ 的/ 伸缩性/ ./ PCIe/ 交叉开关/ 模块/ 用于/ 实现/ 处理器/ 与/ I/ // O/ 设备/ 间/ PCIe/ 消息/ 的/ 交换/ ;/ IntraDMA/ 交叉开关/ 模块/ 用于/ 实现/ 节点/ 内/ DMA/ 数据/ 的/ 交换/ ./ 因此/ ,/ cHPP/ 控制器/ 可/ 支持/ 多种/ 拓扑/ 的/ 直接/ 网络/ 互连/ ,/ 可/ 提供/ 节点/ 内/ 多处理器/ 间/ 高速通信/ 和/ I/ // O/ 高效/ 共享/ ./ 网络接口/ 控制器/ 是/ 保证/ 节点/ 内/ 高性能/ 通信/ 和/ 资源共享/ 的/ 关键部件/ ,/ 网络接口/ 控制器/ 的/ 微/ 体系结构/ 和/ 网络通信/ 协议/ 是/ 本文/ 的/ 主要/ 研究/ 内容/ ./ 3/ 网络接口/ 控制器/ 设计/ cHPP/ 控制器/ 面向/ 高性能/ 计算/ ,/ 同时/ 兼顾/ 云/ 计算/ 的/ 需求/ ,/ 其/ 网络接口/ 控制器/ 需要/ 支持/ 高性能/ 的/ 通信/ 和/ 高效/ 的/ 共享/ ./ 通信/ 延迟/ 决定/ 了/ 高性能/ 互连/ 网络/ 的/ 性能/ 而/ 可扩展性/ 则/ 决定/ 了/ 网络/ 的/ 规模/ ,/ 二者/ 是/ 衡量/ 网络/ 性能/ 的/ 关键因素/ ./ 网络通信/ 协议/ 决定/ 了/ 网络接口/ 控制器/ 数据传输/ 的/ 效率/ ,/ 通信接口/ 决定/ 了/ 软硬件/ 的/ 交互/ 效率/ ,/ 通信/ 原语/ 的/ 定义/ 和/ 实现/ 机制/ 则/ 决定/ 了/ 控制器/ 的/ 功能/ 和/ 硬件/ 效率/ ,/ 共享/ 机制/ 决定/ 了/ 控制器/ 被/ 共享/ 的/ 能力/ ,/ 这些/ 方面/ 共同/ 构成/ 了/ 网络接口/ 控制器/ 的/ 有机/ 整体/ ./ 因此/ ,/ 本/ 节/ 围绕/ 低/ 延迟/ 与/ 可扩展性/ 对/ 节点/ 内/ 通信/ 进行/ 优化/ ,/ 从/ 低/ 开销/ 通信协议/ 、/ 高性能/ 通信接口/ 、/ 高效/ 通信/ 原语/ 和/ 高效/ 的/ I/ // O/ 共享/ 4/ 个/ 方面/ 对/ 网络接口/ 控制器/ 进行/ 了/ 设计/ 分析/ ./ 3.1/ 低/ 延迟/ 与/ 可扩展性/ 能/ 分析/ LogP/ [/ 2/ ]/ 模型/ 以/ 很少/ 的/ 参数/ 来/ 反映/ 并行计算/ 的/ 关键技术/ ,/ 但/ 主要/ 是/ 针对/ 短消息/ 进行/ 分析/ ./ LogGP/ [/ 3/ ]/ 模型/ 对长/ 消息/ 传输/ 进行/ 了/ 分析/ ./ 本文/ 在/ LogGP/ 模型/ 的/ 基础/ 上/ 对/ 基于/ Chain/ -/ DMA/ 引擎/ 的/ 网络接口/ 控制器/ 执行长/ 消息/ 传输/ 进行/ 了/ 系统/ 延迟/ 分析/ ./ 端到/ 端/ 传输/ 时间/ 定义/ 为/ 发送/ 节点/ 网络接口/ 控制器/ 开始/ DMA/ 操作/ 到/ 接收/ 节点/ 接收/ 最后/ 一个/ 字节/ 为止/ ./ 如图/ 4/ 所示/ ,/ 采用/ 基于/ Chain/ -/ DMA/ 引擎/ 的/ 网络接口/ 控制器/ 传输/ M/ 字节/ 消息/ 所/ 需/ 的/ 时间/ 由式/ (/ 1/ )/ 定义/ ./ T/ (/ P/ )/ =/ Oc/ +/ Ods/ +/ (/ M/ -/ 1/ )/ ×/ G/ +/ L/ (/ P/ )/ +/ Odr/ (/ 1/ )/ 其中/ ,/ Oc/ 为/ 处理器/ 启动/ 消息/ 发送/ 的/ 时间/ ;/ Ods/ 是/ DMA/ 启动/ 开销/ ,/ 包括/ 门铃/ 启动/ 时间/ 和/ 描述符/ 读取/ 时间/ ;/ M/ 是/ 消息/ 长度/ (/ 字节/ )/ ;/ G/ 是/ 网络/ “/ 间隔/ ”/ (/ 周期/ 每/ 字节/ )/ ,/ 其/ 倒数/ 为/ 网络带宽/ ;/ L/ (/ P/ )/ =/ H/ ×/ r/ 是/ 消息/ 包头/ 通过/ 网络/ 的/ 平均/ 时间/ ,/ H/ 是/ 消息/ 通过/ 的/ 平均/ 跳数/ ,/ r/ 是/ 每/ 跳/ 处理/ 时间/ 和/ 线路/ 延迟/ 之/ 和/ ,/ P/ 是/ 处理器/ 数目/ ;/ Odr/ 是/ DMA/ 接收/ 处理/ 延迟/ ./ 从式/ (/ 1/ )/ 可以/ 看出/ :/ 设计/ 高效/ 的/ 软硬件/ 接口/ 可以/ 加速/ 消息/ 发送/ ,/ 有利于/ 压缩/ Oc/ 开销/ ;/ 建立/ 快速/ 的/ DMA/ 启动/ 机制/ ,/ 有利于/ 降低/ Ods/ 开销/ ;/ 构建/ 高效/ 的/ DMA/ 引擎/ ,/ 设计/ 低/ 开销/ 的/ 通信协议/ 都/ 有利于/ 减少/ Odr/ 实现/ 数据/ 的/ 快速/ 传递/ ;/ 提供/ 更/ 高/ 的/ 网络带宽/ 可/ 降低/ G/ ,/ 因此/ 采用/ 高带宽/ 的/ 通信/ 链路/ 有利于/ 减少/ 通信/ 延迟/ ;/ 压缩/ 网络/ 直径/ ,/ 降低/ 平均/ 跳数/ H/ 也/ 可/ 减少/ 网络/ 延迟/ ./ 因此/ ,/ 可以/ 围绕/ 上述/ 各项/ 指标/ 设计/ 网络接口/ 控制器/ 以/ 提供/ 高性能/ 的/ 通信/ 网络/ ./ 对于/ 大规模/ 互连/ 网络/ 来说/ ,/ 网络拓扑/ 结构/ 应/ 具有/ Page4/ 良好/ 的/ 扩展性/ ./ 理想/ 拓扑/ 结构/ 应/ 具有/ 对称性/ 、/ 扩展/ 粒度/ 低/ 、/ 等/ 分/ 带宽/ 大/ 、/ 网络/ 直径/ 短/ 、/ 节点/ 度/ 适度/ 等/ 优良/ 特性/ ./ 为此/ ,/ 比/ 对/ 了/ 不同/ 网络拓扑/ 结构/ 对/ 延迟/ 和/ 可扩展性/ 能/ 的/ 影响/ ./ 为/ 简化/ 分析/ ,/ 根据/ 以往/ 的/ 经验/ 数据/ ,/ 参数/ 取值/ 分别/ 为/ Oc/ =/ 200ns/ ,/ Ods/ =/ 528ns/ ,/ r/ =/ 240ns/ ,/ Odr/ =/ 160ns/ ,/ M/ =/ 2KB/ ,/ G/ =/ 1/ // 4/ (/ ns/ // Byte/ )/ ./ 根据/ 文献/ [/ 2/ ]/ 的/ 统计/ ,/ 直接/ 网络/ 的/ 平均/ 距离/ H/ 与/ 处理器/ 数目/ P/ 的/ 关系/ 如表/ 1/ 所示/ ./ 图/ 5/ 显示/ 了/ 在/ 各种/ 不同/ 拓扑/ 网络/ 情况/ 下/ 的/ 通信/ 延迟/ 性能/ ./ 数据/ 表明/ ,/ 网络接口/ 控制器/ 在/ Hypercube/ 网络/ 具有/ 最好/ 的/ 延迟/ 性能/ ,/ 而且/ 随着/ 规模/ 的/ 扩展/ 缓慢/ 的/ 增长/ ,/ 说明/ 具有/ 良好/ 的/ 可扩展性/ ./ 图/ 4/ 基于/ Chain/ -/ DMA/ 引擎/ 的/ 数据传输/ 时延/ 图/ 6/ 网络/ 包/ 硬件/ 包头/ 格式/ 3.2/ 低/ 开销/ 通信协议/ 由于/ 具有/ 高带宽/ 、/ 低/ 延迟/ 、/ 高/ 吞吐/ 率/ 等/ 特性/ ,/ PCIe/ 已经/ 成为/ 实际上/ 的/ I/ // O/ 总线/ 标准/ ./ PCIe/ 提供/ 高速/ 点对点/ 的/ 单/ // 双工/ 通信/ ,/ 物理/ 链路/ 采用/ 差动/ 信令/ 以/ 提高/ 传输/ 距离/ ./ 最新/ 的/ PCIe3/ ./ 0/ 架构/ 单/ 信道/ (/ ×/ 1/ )/ 单向/ 带宽/ 接近/ 8GB/ // s/ ./ 主流/ 处理器/ 都/ 直接/ 支持/ PCIe/ 协议/ ,/ 通过/ PCIe/ 可/ 直接/ 与/ I/ // O/ 设备/ 通信/ ,/ 减少/ 协议/ 转换/ 开销/ ,/ 降低/ 通信/ 延迟/ ;/ 支持/ SR/ -/ IOV/ 技术/ 可/ 实现/ 多/ 虚拟机/ 间/ I/ // O/ 资源/ 的/ 高效/ 共享/ ;/ 基于/ 信用/ 的/ 流控/ 机制/ 确保/ 链路层/ 的/ 可靠/ 传输/ ;/ CRC/ 校验/ 支持/ 链路层/ 错误/ 检测/ ,/ 支持/ 出错/ 自动/ 重传/ ,/ 链路/ 可靠性/ 高/ ./ 因此/ ,/ 从/ 带宽/ 、/ 传输/ 距离/ 、/ 可靠性/ 等/ 方面/ 看/ ,/ PCIe/ 可/ 用于/ 高性能/ 计算/ 的/ 系统/ 级/ 网络/ 互连/ ,/ 实现/ 大规模/ 高性能/ 互连/ 网络/ 的/ 高速/ 互连/ ,/ 但是/ 作为/ I/ // O/ 总线/ ,/ PCIe/ 的/ 典型/ 应用/ 是/ 树形/ 拓扑/ ,/ 处理机/ 是/ 树/ 的/ 根/ ,/ 而/ I/ // O/ 设备/ 则/ 是/ 树/ 的/ 叶子/ ./ 标准/ PCIe/ 交换机/ 构成/ 的/ 网络/ ,/ 通常/ 是/ 若干个/ 功能/ 独立/ 的/ 子树/ 的/ 集合/ ,/ 子树/ 之间/ 并/ 无/ 数据交换/ ./ 虽然/ 这/ 已经/ 满足/ 了/ 在/ I/ // O/ 扩展/ 方面/ 的/ 需要/ ,/ 但是/ 根与根/ 之间/ 不能/ 通信/ ,/ 终端/ 叶子/ 节点/ 之间/ 也/ 无法/ 直接/ 通信/ ,/ 难以/ 构造/ 复杂/ 的/ 拓扑/ 网络/ ,/ 无法/ 用于/ 处理器/ 间通信/ ./ 因此/ ,/ 网络通信/ 协议/ 在/ PCIe/ 协议/ 的/ 基础/ 上/ 进行/ 了/ 拓展/ ,/ 可/ 充分利用/ PCIe/ 标准/ 的/ 优良/ 性能/ 并/ 拓展/ 了/ 处理器/ 间/ 直接/ 通信/ 能力/ ./ 完整/ 的/ 网络/ 包是/ 由/ 硬件/ 包头/ 、/ 有效/ 数据/ 载荷/ 和/ 软件包/ 尾/ 构成/ ./ 基于/ PCIe/ 事务/ 层包/ 格式/ ,/ 本文/ 的/ 网络/ 包头/ 格式/ 如图/ 6/ 所示/ ,/ 其中/ 黑色/ 加粗/ 字/ 段/ 为重/ 定义/ 字/ 段/ ,/ 包括/ PKTType/ 定义/ 了/ 网络/ 包/ 的/ 类型/ (/ 包括/ NAP/ // PUT/ // GET/ )/ ;/ SRC/ _/ ID/ 包含/ 了/ 源/ 数据/ 的/ 处理器/ 号/ ,/ 虚/ 功能/ 号/ 和/ 队列/ 对/ 号/ ;/ DST/ _/ ID/ 包含/ 了/ 目标/ 处理器/ 号/ 、/ 目标/ 虚/ 功能/ 号/ 和/ 目标/ 队列/ 对/ 号/ ;/ 增加/ 了/ QPMagic/ 域/ 用于/ 保护/ 信息/ ,/ 并/ 通过/ S/ _/ Flag/ 域/ 进行/ 控制/ (/ 如/ 完成/ 事件/ 发送/ 控制/ 等/ )/ ./ 沿用/ 了/ PCIe/ 类型/ 域/ (/ Type/ )/ 、/ 流量/ 类型/ 域/ 、/ 长度/ (/ Length/ )/ 、/ 地址/ (/ Address/ )/ 、/ 请求/ ID/ (/ RequesterID/ )/ 等/ PCIe/ 的/ 链路层/ 关键/ 域/ ,/ 其/ Page5/ 含义/ 与/ PCIe/ 协议/ 一致/ ./ 网络接口/ 控制器/ 根据/ 数据/ 长度/ 域/ (/ Length/ )/ 、/ 地址/ 域/ (/ Address/ )/ 、/ 源/ 节点/ 和/ 目标/ 节点/ 的/ 数据/ 及/ 控制/ 信息/ 进行/ 数据传输/ ./ 基于/ PCIe/ 协议/ 定义/ 的/ 网络/ 消息/ 格式/ 更/ 适合/ 于/ 局部/ 互连/ :/ (/ 1/ )/ 充分发挥/ PCIe/ 高速/ 可靠/ 的/ 链路层/ 性能/ ,/ 使/ 网络/ 消息/ 直接/ 面向/ 内存/ 操作/ ,/ 不仅/ 减少/ 了/ I/ // O/ 总线/ 与/ 网络/ 间/ 的/ 协议/ 转换/ 开销/ ,/ 还/ 无缝/ 兼容/ PCIe/ 设备/ ,/ 实现/ I/ // O/ 设备/ 访问/ 与/ 网络通信/ 功能/ 的/ 融合/ ;/ (/ 2/ )/ 提高/ 了/ 有效/ 负载/ 率/ ,/ PCIe/ 包头/ 为/ 16Bytes/ ,/ 数据/ 负载/ 为/ 0/ ~/ 4096Bytes/ ,/ 相比/ 面向/ 大规模/ 系统/ 的/ 互连/ 网络/ (/ 如/ InfiniBand/ [/ 4/ ]/ 的/ 包头/ 最大/ 为/ 94Bytes/ ,/ IBMBluegene/ // Q/ [/ 5/ -/ 6/ ]/ 和/ KComputerTofu/ [/ 7/ ]/ 网络/ 包头/ 均/ 为/ 32Bytes/ )/ ,/ 在/ 相同/ 物理/ 链路/ 带宽/ 条件/ 下/ ,/ 可/ 有效/ 提高/ 有效/ 负载/ 带宽/ ./ 此外/ ,/ 为/ 使/ 主机/ 可以/ 识别/ 全局/ 资源/ ,/ 实现/ 处理器/ 对/ 节点/ 内/ 资源/ 的/ 高效/ 访问/ ,/ 通信协议/ 支持/ 全局/ 统一/ 地址/ 空间/ ,/ 对超/ 节点/ 内/ 全局/ 物理/ 内存/ 和/ I/ // O/ 地址/ 空间/ 统一/ 编址/ ,/ 使得/ 内存/ 及/ I/ // O/ 资源/ 均/ 拥有/ 全局/ 唯一/ 的/ 地址/ ./ 全局/ 地址/ 空间/ 避免/ 了/ 系统/ 中/ 复杂/ 的/ 地址映射/ 和/ 变换/ 关系/ ,/ 用户/ 可以/ 直接/ 访问/ 系统资源/ ,/ 能够/ 简化/ 系统/ 设计/ ,/ 提高/ 系统/ 性能/ ./ 由于/ 地址/ 空间/ 全局/ 可见/ ,/ 为/ 防止/ 误操作/ 或/ 恶意/ 进程/ 造成/ 的/ 非法/ 访问/ ,/ DMA/ 引擎/ 还/ 采用/ 了/ 简单/ 高效/ 的/ 地址/ 保护/ 机制/ :/ 通过/ 绑定/ 密钥/ 来/ 限制/ 访问/ 权限/ ,/ 接收/ 方会/ 检验/ 网络/ 包/ 包头/ 的/ QPMagic/ 字段/ (/ 如图/ 6/ 所示/ )/ ,/ 只有/ 与/ 通信/ 建立/ 阶段/ 协商一致/ 的/ 密钥/ 匹配/ 才/ 允许/ 数据传输/ ,/ 否则/ 视为/ 非法/ 访问/ ,/ 接收/ 方/ 拒绝/ 数据/ 请求/ ,/ 进而/ 实现/ 安全/ 、/ 隔离/ 的/ 用户/ 访问/ ./ 3.3/ 高性能/ 通信接口/ 3.3/ ./ 1/ 高速/ PCIe/ 接口/ 网络接口/ 控制器/ 不仅/ 要/ 实现/ 多处理器/ 间/ 的/ 互连/ 通信/ ,/ 也/ 要/ 实现/ I/ // O/ 设备/ 的/ 扩展/ ./ 本文/ 的/ 网络接口/ 控制器/ 端口/ 选择/ PCIe/ 总线/ ,/ 可/ 充分利用/ PCIe/ 高带宽/ 、/ 低/ 延迟/ 的/ 传输/ 性能/ ,/ 高/ 可靠/ 的/ 链路层/ 通信/ 和/ 丰富/ 的/ 服务质量/ 支持/ 等/ 方面/ 的/ 优势/ ./ 通过/ 实现/ 兼容/ PCIe/ 的/ 网络通信/ 协议/ (/ 见/ 第/ 3.2/ 节/ )/ ,/ 在/ 无缝/ 兼容/ 现有/ PCIe/ 设备/ 的/ 前提/ 下/ ,/ 将/ PCIe/ 总线/ 扩展/ 至/ 处理器/ 间/ 互连/ 领域/ ./ 3.3/ ./ 2/ 用户/ 级/ 通信/ 本文/ 设计/ 了/ 用户/ 级/ 通信接口/ 降低/ 通信/ 过程/ 中/ 的/ 软件/ 开销/ ./ 用户/ 级/ 通信/ [/ 8/ ]/ 也/ 被/ 称为/ 操作系统/ 旁路/ ,/ 通过/ 消除/ 操作系统/ 转发/ 引入/ 的/ 内存/ 拷贝/ ,/ 实现/ 应用程序/ 对/ 底层/ 硬件/ 设备/ 的/ 直接/ 访问/ ./ 本文/ 实现/ 了/ 基于/ QP/ (/ QueuePair/ :/ 队列/ 对/ )/ 的/ 用户/ 级/ 通信接口/ ,/ QP/ 包括/ 发送/ 队列/ (/ SendQueue/ )/ 、/ 接收/ 队列/ (/ ReceiveQueue/ )/ 和/ 控制/ 队列/ (/ ControlQueue/ )/ ,/ 其中/ 发送/ 队列/ 缓存/ 需要/ 执行/ 的/ 发送/ 请求/ ,/ 接收/ 队列/ 缓存/ 接收/ 到/ 的/ 数据/ ,/ 控制/ 队列/ 用于/ 缓存/ 发送/ 队列/ 和/ 接收/ 队列/ 所/ 需/ 的/ 完成/ 事件/ 通知/ ./ 需要/ 说明/ 的/ 是/ ,/ 控制/ 队列/ 仅/ 实现/ 了/ 硬件/ 向/ 应用程序/ 的/ 通知/ 机制/ (/ 发送/ 或/ 接收/ 完成/ )/ ,/ 应用程序/ 向/ 硬件/ 的/ 通知/ (/ 启动/ 一次/ 发送/ 操作/ )/ 采用/ 了/ 门铃/ (/ Doorbell/ )/ 机制/ (/ 详见/ 第/ 3.4/ 节/ )/ ./ QP/ 的/ 队列/ 存在/ 于/ 主机/ 内存/ ,/ 与/ 应用程序/ 或/ 通信/ 进程/ 绑定/ ,/ 因此/ 应用程序/ 通过/ 操作/ 其/ 独占/ 的/ QP/ 实现/ 对/ 硬件/ 的/ 访问/ ./ 3.4/ 高效/ 通信/ 原语/ 3.4/ ./ 1/ 通信/ 原语/ 定义/ 通过/ 定义/ 高效/ 的/ 硬件/ 原语/ ,/ 网络接口/ 控制器/ 可/ 实现/ 通信协议/ 的/ 卸载/ (/ Offloading/ )/ ,/ 提高/ 通信/ 效率/ ./ 消息传递/ 编程/ 模型/ (/ MPI/ )/ 是/ 目前/ 高性能/ 计算/ 应用/ 中/ 最/ 常见/ 的/ 编程/ 模型/ ,/ 是/ 高性能/ 计算/ 领域/ 事实上/ 的/ 标准/ ,/ 因此/ cHPP/ 控制器/ 主要/ 针对/ MPI/ 的/ 两种/ 通信/ 方式/ 进行/ 加速/ :/ 适用/ 于/ 小/ 消息/ 的/ eager/ 通信/ 模式/ 和/ 适用/ 于/ 大/ 消息传递/ 的/ rendezvous/ 模式/ ,/ 抽象/ 出/ NAP/ (/ Non/ -/ AddressPacket/ )/ 、/ PUT/ 、/ GET/ 等/ 通信/ 原语/ ./ 无地址/ 包/ (/ NAP/ )/ 是/ 为了/ 加速/ 小/ 消息/ 通信/ 抽象/ 出来/ 的/ 通信/ 原语/ ./ NAP/ 命令/ 只/ 需/ 知道/ 目标/ 节点/ 信息/ ,/ 不需/ 具体地址/ 信息/ ,/ 因此/ 叫做/ 无地址/ 包/ ./ DMA/ 引擎/ 将/ 接收/ 到/ 的/ NAP/ 消息/ 存放/ 在/ 内存/ 中/ 事先/ 分配/ 好/ 的/ 接收缓冲区/ 内/ ,/ 再/ 将/ 其/ 复制到/ 用户程序/ 空间/ ,/ 主要/ 用于/ 少量/ 数据/ 和/ 控制/ 信息/ 的/ 传递/ ,/ 可以/ 用来/ 实现/ MPIeager/ 通信/ 和/ 同步操作/ ./ NAP/ 操作/ 有/ 两种/ :/ 立即/ 数/ 和/ 间接/ 数/ ,/ NAP/ 立即/ 数是/ 指/ 描述符/ 中/ 直接/ 包含/ 待/ 传输/ 的/ 数据/ ,/ 而/ NAP/ 间接/ 数是/ 指/ 描述符/ 中/ 只/ 包含/ 有/ 需要/ 传输数据/ 的/ 地址/ 和/ 长度/ 信息/ ,/ 两种/ 类型/ 是/ 通过/ 描述符/ 中/ 的/ 类型/ 域/ 区分/ 的/ ./ PUT/ 通信/ 原语/ 用于/ 大/ 数据量/ 消息/ 的/ 传输/ ,/ 可/ 看作/ 是/ 大块/ 内存/ 写/ 操作/ ,/ 需要/ 事先/ 通过/ 握手/ 协商/ 获得/ 内存地址/ 信息/ ,/ 可以/ 直接/ 将/ 数据/ 写入/ 到/ 用户程序/ 的/ 内存空间/ ./ PUT/ 通信/ 原语/ 支持/ MPI/ 的/ rendezvous/ 通信/ 和/ MPI/ -/ 2/ 的/ put/ 操作/ ,/ 采用/ Chain/ -/ DMA/ 描述符/ ,/ 可/ 支持/ 复杂/ 的/ 通信/ 模式/ ./ GET/ 通信/ 原语/ 和/ PUT/ 类似/ ,/ 只是/ 数据流/ 是/ 反向/ 的/ ,/ 用于/ 大/ 数据量/ 消息/ 的/ 读取/ 和/ 写/ 回/ ./ 图/ 7/ 为/ PUT/ 和/ GET/ 操作/ 的/ 示意图/ ,/ 在/ 启动/ PUT/ 和/ GET/ 操作/ 之前/ ,/ 发送/ 方均/ 需要/ 2/ 次/ NAP/ 操作/ 协商/ 传输/ 的/ 源/ 数据/ 和/ 目标/ 地址/ 等/ 信息/ ./ 对于/ PUT/ 操作/ ,/ 从/ 发送/ 方/ 读取/ 的/ 数据/ 将/ 封装/ 为/ 若干/ PUT/ 数/ Page6/ 据/ 包发往/ 目标/ 节点/ ,/ 待/ 数据传输/ 完成/ 接收/ 方/ 发送/ NAP/ 消息/ 告知/ 通信/ 完成/ ;/ 而/ GET/ 操作/ 中/ ,/ 发送/ 方则/ 直接/ 将/ 协商/ 获取/ 的/ 地址/ 等/ 信息/ 打包/ 为/ GET/ 数据包/ 发送/ ,/ 接收/ 方对/ GET/ 数据包/ 进行/ 解析/ ,/ 根据/ 所得/ 信息/ ,/ 将/ 发送/ 方/ GET/ 操作/ 重构/ 为/ 接收/ 方/ 的/ PUT/ 操作/ 后/ ,/ 执行/ 该/ PUT/ 操作/ 流程/ ./ 3.4/ ./ 2/ 原语/ 启动/ 机制/ 通信/ 原语/ 的/ 启动/ ,/ 依赖于/ 主机/ 向/ DMA/ 引擎/ 提供/ 包含/ 源/ 数据/ 和/ 目的/ 地址/ 等/ 相关/ 信息/ ./ DMA/ 启动/ 方式/ 可以/ 分为/ 描述符/ 启动/ 和/ 门铃/ 启动/ 两种/ ./ 在/ 描述符/ 启动/ 方式/ 中/ ,/ 处理器/ 直接/ 将/ 描述符/ 写往/ DMA/ 引擎/ ,/ DMA/ 引擎/ 根据/ 描述符/ 信息/ 去/ 读取/ 内存/ 数据/ ./ 描述符/ 启动/ 的/ 优点/ 在于/ 启动/ 速度/ 快/ ,/ DMA/ 引擎/ 接收/ 到/ 描述符/ 后/ 可以/ 直接/ 发起/ 内存/ 读取/ 操作/ ,/ 缺点/ 是/ 写/ 描述符/ 过程/ 占用/ 处理器/ 时间/ ,/ 且/ 原子/ 性差/ ,/ 为/ 保证/ 写/ 描述符/ 过程/ 中/ 不/ 被/ 其他/ DMA/ 描述符/ 打断/ ,/ 增加/ 了/ 软件/ 的/ 加锁/ 操作/ 开销/ ./ 在/ 门铃/ 启动/ 方式/ 中/ ,/ 用户/ 进程/ 先/ 将/ 描述符/ 存储/ 在/ 指定/ 内存/ 中/ ,/ 通过/ 向/ 对应/ 的/ 门铃/ FIFO/ (/ FirstInputFirstOutput/ )/ 写入/ 门铃/ 信息/ 启动/ DMA/ 操作/ ,/ 门铃/ 承载/ 描述符/ 在/ 内存空间/ 的/ 具体/ 信息/ ,/ 由/ 地址/ 域/ 和/ 长度/ 域/ 构成/ :/ 地址/ 域/ 是/ 描述符/ 所在/ 的/ 物理/ 内存空间/ 首/ 地址/ ,/ 长度/ 域/ 表示/ 描述符/ 的/ 长度/ 信息/ ./ 门铃/ 操作/ 触发/ 描述符/ 命令/ 单元/ 读取/ 描述符/ 内容/ ,/ DMA/ 引擎/ 得到/ 描述符/ 后/ 再/ 根据/ 描述符/ 信息/ 去/ 读取数据/ 进行/ 传输/ ./ 门铃/ 启动/ 的/ 优点/ 在于/ 操作/ 的/ 原子/ 性/ ,/ 由于/ 描述符/ 内容/ 较/ 少/ ,/ 因此/ 主机/ 只/ 需写/ 一次/ 门铃/ 寄存器/ (/ 在/ 网络接口/ 控制器/ 中/ )/ 即可/ ,/ 不会/ 被/ 其他/ 操作/ 所/ 打断/ ./ 同时/ 可以/ 使/ 描述符/ 的/ 格式/ 更加/ 灵活/ 和/ 复杂/ ./ 门铃/ 启动/ 的/ 缺点/ 在于/ 启动/ 速度慢/ ,/ 相对/ 于/ 描述符/ 启动/ 方式/ ,/ 需要/ 先/ 读取/ 描述符/ ,/ 额外/ 引入/ 了/ 一次/ 内存/ 读取/ 操作/ ./ 通常/ DMA/ 引擎/ 是/ 被/ 所有/ 应用程序/ 所/ 共享/ 的/ ,/ 当/ 多个/ 进程/ 或/ 线程/ 希望/ 同时/ 使用/ DMA/ 引擎/ 时会/ 引起/ 竞争/ ./ 因此/ ,/ DMA/ 启动/ 操作/ 应该/ 是/ “/ 原子/ ”/ 的/ ,/ 是/ 指/ 一次/ DMA/ 操作/ 不会/ 被/ 其它/ 的/ DMA/ 操作/ 打断/ ./ 通过/ 门铃/ 启动/ 实现/ DMA/ 引擎/ 的/ 虚拟化/ 可以/ 避免/ 竞争/ ./ 门铃/ 启动/ 机制/ 可/ 支持/ 更/ 多/ 的/ 进程/ 高效/ 地/ 进行/ DMA/ 操作/ ,/ 既保证/ 了/ DMA/ 启动/ 的/ 原子/ 性/ ,/ 又/ 实现/ 了/ 良好/ 的/ 扩展性/ ./ 而且/ ,/ 在/ 高性能/ 计算/ 中/ ,/ 进行/ 大/ 数据量/ 的/ 传输/ 时/ ,/ 门铃/ 启动/ 增加/ 的/ 额外/ 开销/ 相比/ 软件/ 加锁/ 开销/ 并/ 不/ 显著/ ./ 特别/ 是/ 在/ 多/ 核多/ 进程/ 的/ 情况/ 下/ ,/ 门铃/ 启动/ 的/ 原子/ 性/ 尤为重要/ ,/ 因此/ 本文/ 的/ 网络接口/ 控制器/ DMA/ 引擎/ 选用/ 门铃/ 启动/ 的/ 方式/ ./ 描述符/ 定义/ 了/ DMA/ 操作/ 的/ 必要/ 信息/ ,/ 由/ 用户/ 进程/ 事先/ 写入/ 指定/ 内存空间/ ./ 描述符/ 命令/ 单元/ 根据/ 门铃/ 信息/ ,/ 将/ 描述符/ 从/ 主存/ 读取/ 到/ DMA/ 引擎/ ,/ DMA/ 引擎/ 根据/ 描述符/ 判断/ DMA/ 操作/ 的/ 类型/ ,/ 获得/ 数据传输/ 的/ 控制/ 和/ 数据/ 信息/ ./ 如图/ 8/ 所示/ ,/ Chain/ -/ DM/ A型/ 描述符/ 在/ 硬件/ 包头/ 信息/ 域/ 定义/ 了/ DMA/ 操作/ 的/ 类型/ 、/ 控制/ 信息/ 和/ 地址/ 保护/ 等/ 信息/ ;/ 源/ 数据/ 信息/ 域/ 和/ 目标/ 数据/ 信息/ 域/ 指定/ 了/ 数据/ 的/ 源地址/ 和/ 长度/ 信息/ 以及/ 接收/ 方/ 数据/ 存储/ 信息/ ,/ 采用/ 队列/ 形式/ 的/ 数据结构/ 可以/ 实现/ 灵活/ 的/ 数据传输/ ;/ 软件包/ 尾/ 由/ 上层/ 软件/ 使用/ 并/ 对/ 硬件/ 透明/ ./ Chain/ -/ DM/ A型/ 描述符/ 优化/ 了/ 虚/ 地址/ 连续/ 、/ 物理地址/ 离散/ 的/ 数据/ 块/ 的/ 传输/ ,/ 只/ 需/ 读取/ 一次/ 描述符/ 即可/ 实现/ 任意/ 源地址/ 、/ 任意/ 长度/ 、/ 任意/ 目标/ 地址/ 的/ 数据传输/ ./ 同时/ Chain/ -/ DMA/ 只/ 需源/ 数据/ 信息/ 域/ 队列/ 中/ 的/ 总长度/ 与/ 目标/ 数据/ 信息/ 域/ 队列/ 中/ 的/ 总长度/ 相等/ 即可/ ,/ 并/ 不/ 要求/ 队列/ 中/ 每一项/ 均/ 相等/ ,/ 因此/ 可/ 灵活/ 的/ 支持/ 多个/ 页面/ 的/ 大量/ 数据传输/ ,/ 对于/ 大/ 数据通信/ 有/ 加速/ 作用/ ./ 3.4/ ./ 3/ 执行/ 流程/ 及/ 死锁/ 避免/ 通信/ 原语/ 的/ 执行/ 流程/ 开始/ 于/ DMA/ 根据/ 门铃/ 信息/ 读取/ 描述符/ 信息/ 之后/ ,/ DMA/ 引擎/ 根据/ 描述符/ 类型/ 域/ 信息/ 执行/ 相应/ 的/ 操作/ :/ (/ 1/ )/ NAP/ ./ 将/ 描述符/ 中/ 的/ 数据/ (/ 立即/ 数/ )/ 或/ 根据/ 描述符/ 中/ 源地址/ 读回/ 的/ 数据/ (/ 间接/ 数/ )/ 封装/ 成/ 一个/ NAP/ 网络/ 包发往/ 网络/ ,/ 由于/ NAP/ 网络/ 包不含/ 目标/ 地址/ 信息/ ,/ 接收/ 方/ 的/ 接收/ 引擎/ 接收/ 到/ NAP/ 网络/ 包后/ ,/ 先/ 向/ 本地/ 申请/ 缓存/ 地址/ ,/ 再/ 将/ 数据/ 写入/ 数据/ 接收缓冲区/ ;/ (/ 2/ )/ GET/ ./ 图/ 9/ 中/ 标号/ ①/ 至/ ⑤/ 是/ 节点/ a/ 发送/ Page7GET/ 描述符/ 向/ 节点/ b/ 读取数据/ 的/ 流程/ —/ —/ —/ 发送/ 引擎/ 将/ GET/ 描述符/ 直接/ 作为/ 数据/ 封装/ 为/ GET/ 网络/ 包发往/ 节点/ b/ (/ 图/ 9/ ①/ 和/ ②/ )/ ,/ 节点/ b/ 接收/ 引擎/ 从/ 网络/ 包/ 提取/ 到/ GET/ 描述符/ 后/ 转换/ 为/ 本地/ 发起/ 的/ PUT/ 操作/ (/ 图/ 9/ ③/ )/ ,/ 节点/ b/ 的/ 发送/ 引擎/ 将/ 所/ 请求/ 的/ 数据/ 以/ PUT/ 网络/ 包/ 的/ 形式/ 发/ 往/ 节点/ a/ (/ 图/ 9/ ④/ )/ ,/ 最后/ 节点/ a/ 根据/ PUT/ 网络/ 包/ 包头/ 所/ 携带/ 的/ 目标/ 缓冲区/ 地址/ 域/ 信息/ (/ 在/ 握手/ 过程/ 中/ 获取/ )/ 将/ 数据/ 直接/ 写入/ 目标/ 进程/ 的/ 接收缓冲区/ 中/ (/ 图/ 9/ ⑤/ )/ ;/ (/ 3/ )/ PUT/ ./ 上述/ GET/ 是/ 转换/ 为/ PUT/ 实现/ ,/ 因此/ PUT/ 的/ 主要/ 执行/ 流程/ 已/ 在/ GET/ 中/ 描述/ (/ 图/ 9/ ④/ 和/ ⑤/ )/ ./ 当/ 通信/ 双方/ 同时/ 发起/ PUT/ 或/ GET/ 操作/ 时/ ,/ 图/ 9/ 中/ 的/ 执行/ 流程/ (/ 将/ GET/ 转换/ 为/ 接收/ 方/ PUT/ 执行/ )/ 将/ 存在/ 死锁/ ./ 如图/ 9/ 中/ 的/ ②/ 至/ ④/ (/ 节点/ a/ 发起/ 的/ GET/ 操作/ )/ 和/ ⑦/ 至/ ⑨/ (/ 节点/ b/ 发起/ 的/ GET/ 操作/ )/ 形成/ 环路/ ,/ 而/ 死锁/ 的/ 原因/ 在于/ GET/ 网络/ 包与/ PUT/ 网络/ 包/ 形成/ 的/ 网络资源/ 竞争/ ./ 针对/ 该/ 问题/ ,/ 本文/ 采用/ 虚/ 通道/ 来/ 避免/ 死锁/ ,/ 如图/ 10/ 所示/ ,/ 通过/ 为/ GET/ 设置/ 独立/ 的/ 虚/ 通道/ ,/ 解除/ 了/ GET/ 网络/ 包与/ PUT/ 网络/ 包/ 的/ 网络资源/ 竞争/ ./ 3.5/ I/ // O/ 高效/ 共享/ 在/ 云/ 计算环境/ 下/ 存在/ 大量/ 并发/ 通信/ 请求/ ,/ 这/ 对/ 通信接口/ 提出/ 了/ 更/ 高/ 要求/ :/ 高性能/ 通信/ ,/ 即/ 承载/ 大量/ 通信/ 请求/ 的/ 能力/ ;/ 其次/ 是/ 高效/ 共享/ ,/ 在/ 极小/ 损失/ 通信/ 效率/ 的/ 前提/ 下/ ,/ 实现/ 多/ 虚拟机/ 对/ I/ // O/ 资源/ 的/ 共享/ 访问/ ./ 高性能/ 计算/ 与/ 云/ 计算/ 对于/ 网络接口/ 控制器/ 在/ 性能/ 方面/ 的/ 需求/ 是/ 一致/ 的/ ,/ 即/ 包括/ 更/ 高/ 的/ 峰值/ 能力/ 和/ 更/ 高/ 的/ 实际/ 通信/ 能力/ ./ 同时/ ,/ 还应/ 兼容/ 现有/ PCIe/ 设备/ ,/ 设计/ 支持/ SR/ -/ IOV/ 协议/ ,/ 实现/ 超/ 节点/ 内部/ 的/ I/ // O/ 资源/ 在/ 多/ 虚拟机/ 间/ 的/ 高效/ 共享/ ./ DMA/ 引擎/ 支持/ SR/ -/ IOV/ 规范/ ,/ 可/ 被/ 虚拟/ 为/ 若干个/ DMA/ 引擎/ ,/ 每个/ 虚拟/ DMA/ 引擎/ 都/ 可以/ 被/ 当作/ 独立/ 的/ 设备/ 分配/ 给/ 虚拟机/ 使用/ ,/ 实现/ 虚拟机/ 间/ 对/ DMA/ 引擎/ 的/ 充分/ 共享/ ./ 每个/ 虚拟机/ 可/ 通过/ DMA/ 引擎/ 直接/ 访问/ I/ // O/ 资源/ ,/ 从而/ 实现/ 多个/ 虚拟机/ 的/ 用户/ 级/ I/ // O/ 高效/ 共享/ ./ 每个/ 虚/ 功能/ (/ VF/ )/ 拥有/ 多个/ QP/ ,/ 对应/ 一个/ 独立/ 的/ 门铃/ FIFO/ 来/ 缓存/ DMA/ 请求/ ./ 通过/ 仲裁/ 模块/ 对/ 每个/ 门铃/ FIFO/ 的/ 请求/ 进行/ 仲裁/ ,/ 基于/ 优先级/ 仲裁/ 可/ 实现/ VF/ 之间/ 的/ 差异化/ 服务/ ./ 4/ 网络接口/ 控制器/ 实现/ 如图/ 11/ 所示/ ,/ 网络接口/ 控制器/ 主要/ 由/ PCIe/ 端口/ 、/ PCIe/ 配置/ 空间/ 、/ 门铃/ 启动/ 窗口/ 、/ 描述符/ 命令/ 单元/ 和/ DMA/ 数据/ 发送/ 及/ 接收/ 引擎/ 构成/ ./ PCIe/ 端口/ 采用/ 标准/ PCIeGen2/ 链路/ ./ PCIe/ 配置/ 空间/ 则/ 实现/ 了/ PCIeSR/ -/ IOV/ 功能/ ./ 门铃/ 窗口/ 模块/ 用于/ 接收/ 启动/ DMA/ 操作/ 所/ 需/ 的/ 门铃/ 信息/ ,/ 门铃/ 将/ 被/ 描述符/ 命令/ 单元/ 解析/ ,/ 并/ 根据/ 解析/ 结果/ 从/ 主存/ 中/ 获取/ DMA/ 描述符/ ./ DMA/ 描述符/ 则/ 提交/ 给/ DMA/ 发送/ 引擎/ ,/ DMA/ 发送/ 引擎/ 根据/ 描述符/ 获取/ 源/ 目的/ 信息/ 后/ ,/ 进行/ 数据/ 的/ 读取/ 及/ 发送/ ./ DMA/ 接收/ 引擎/ 则/ 负责/ 数据/ 的/ 接收/ ,/ 解/ 封装/ 并/ 将/ 数据/ 送到/ 相应/ 的/ 内存空间/ ./ DMA/ 操作/ 需要/ 在/ 内存空间/ 开辟/ 3/ 块/ 缓冲区/ :/ 发送/ 完成/ 事件/ 缓存/ 用于/ 通知/ 主机/ 本次/ 数据/ 发送/ 完毕/ ;/ 接收/ 完成/ 事件/ 缓存/ 用于/ 通知/ 数据/ 接收/ 成功/ ;/ 而/ 接收数据/ 缓存/ 用于/ 存放/ 接收/ 的/ 数据/ ,/ 只/ 用于/ NAP/ 操作/ ./ 每个/ 缓存/ 都/ 是/ Page8/ 逻辑/ 上/ 的/ 环形/ 结构/ ,/ 每/ 完成/ 一次/ DMA/ 操作/ 都/ 会/ 将/ 指针/ 指向/ 下/ 一项/ ./ 4.1/ DMA/ 发送/ 引擎/ DMA/ 发送/ 引擎/ 整体/ 框图/ 如图/ 12/ 所示/ ,/ 它/ 负责/ 接收/ 主机/ 发来/ 的/ 门铃/ ,/ 并/ 对/ 门铃/ 请求/ 进行/ 响应/ ,/ 根据/ 门铃/ 读取/ 描述符/ ,/ 然后/ 再/ 根据/ 描述符/ 的/ 相关/ 信息/ 读取数据/ ,/ 并/ 打包/ 成/ 网络/ 包/ 进行/ 传输/ ./ 门铃/ 模块/ 包含/ 8/ 个/ FIFO/ 分别/ 用于/ 存储/ 8/ 个/ 功能/ (/ PF/ 和/ 7/ 个/ VF/ )/ 对应/ 的/ 门铃/ ,/ 每个/ FIFO/ 深度/ 为/ 32/ ,/ 即/ 每个/ 功能/ 最多能/ 同时/ 支持/ 32/ 个/ DMA/ 请求/ ./ DMA/ 发送/ 端的/ 流控/ 则/ 交由/ 软件/ 负责/ ,/ 保证/ 每个/ 功能/ 同时/ 发起/ 的/ DMA/ 请求/ 不/ 超过/ 32/ 个/ ./ 描述符/ 读取/ 模块/ 负责/ 读取/ 门铃/ 模块/ 中/ 的/ FIFO/ ,/ 并/ 根据/ 门铃/ 中/ 的/ 内容/ 生成/ 读取/ 描述符/ 的/ PCIe/ 读包/ ./ 描述符/ 提取/ 模块/ 负责/ 接收/ 并/ 处理/ 读取/ 描述符/ 的/ 返回/ 包/ ,/ 在/ 接收/ 到/ 返回/ 包后/ ,/ 首先/ 从/ PCIe/ 包中/ 提取/ 出/ 描述符/ ,/ 在/ 描述符/ 提取/ 出来/ 后/ 再/ 根据/ 描述符/ 的/ 种类/ 进行/ 处理/ ./ 对于/ PUT/ 描述符/ 和/ NAP/ 描述符/ ,/ 模块/ 将/ 描述符/ 中/ 的/ 控制/ 信息/ 、/ 源/ 数据/ 信息/ 和/ 目的/ 信息/ 分别/ 提取/ 出来/ 并/ 缓存/ 到/ 相应/ 的/ FIFO/ 中/ ./ 对于/ GET/ 描述符/ ,/ 模块/ 则/ 直接/ 将/ 其/ 打包/ 为/ GET/ 网络/ 包/ 并发/ 往/ GET/ 交叉开关/ 传输/ ./ 除了/ 本地/ 的/ 描述符/ 外/ ,/ 描述符/ 读取/ 模块/ 还/ 接收/ 由/ 上传/ 模块/ 接收/ 到/ 的/ GET/ 描述符/ ,/ 并/ 将/ GET/ 描述符/ 转换/ 为/ PUT/ 描述符/ 后/ 存入/ 相应/ 的/ FIFO/ 中/ ./ 数据/ 读取/ 模块/ 根据/ 从/ 描述符/ 中/ 提取/ 出来/ 的/ 源/ 数据/ 信息/ ,/ 生成/ 相应/ 的/ PCIe/ 内存/ 读包/ ./ 具体/ 的/ 生成/ 规则/ 同/ 描述符/ 读取/ 模块/ ./ 数据/ 提取/ 模块/ 负责/ 接收/ 返回/ 的/ 包含/ 源/ 数据/ 的/ PCIe/ 返回/ 包/ ,/ 并/ 从中/ 提取/ 出/ 相应/ 的/ 源/ 数据/ ./ 数据/ 重/ 排序/ 模块/ 负责/ 数据/ 整合/ ,/ 方便/ 网络/ 包/ 打包/ 模块/ 使用/ ./ 由于/ PCIe/ 协议/ 对于/ 单次/ 内存/ 读取/ 的/ 限制/ ,/ 一个/ 源/ 数据项/ 可能/ 会/ 对应/ 多次/ 内存/ 读取/ ,/ 而/ 一次/ 内存/ 读取/ 也/ 可能/ 对应/ 多次/ 返回/ ./ 因此/ 数据/ 提取/ 模块/ 提取/ 出/ 的/ 数据/ 是/ 不规则/ 的/ ,/ 为了/ 减轻/ 网络/ 包/ 打包/ 模块/ 的/ 负担/ ,/ 加快/ 打包/ 速度/ ,/ 源/ 数据/ 重/ 排序/ 模块/ 负责/ 将/ 提取/ 出/ 的/ 源/ 数据/ 按照/ 网络/ 包/ 打包/ 模块/ 的/ 要求/ 进行/ 重/ 排序/ ./ 网络/ 包/ 打包/ 模块/ 负责/ 根据/ 描述符/ 控制/ 项/ 和/ 目的/ 项/ 的/ 信息/ ,/ 将/ 读取/ 的/ 源/ 数据/ 进行/ 打包/ 并发/ 往/ 交叉开关/ (/ intraDMA/ 交叉开关/ )/ ./ 4.2/ DMA/ 接收/ 引擎/ DMA/ 接收/ 引擎/ 主要/ 负责/ 接收/ 网络/ 包/ ,/ 并/ 根据/ 网络/ 包/ 的/ 类型/ 进行/ 相应/ 的/ 处理/ ./ 主要/ 由/ 接收/ 分发/ 模块/ ,/ 数据/ 上传/ 模块/ 和/ 缓冲区/ 管理/ 模块/ 等/ 模块/ 构成/ ,/ 具体/ 结构/ 如图/ 13/ 所示/ ./ 接收/ 分发/ 模块/ 负责/ 从/ IntraDMA/ 交叉开关/ 中/ 读取/ 网络/ 包/ ,/ 并/ 根据/ 网络/ 包/ 类型/ 交给/ 相应/ 的/ 模块/ 进行/ 数据处理/ ./ 对于/ PUT/ 包/ 和/ NAP/ 包/ ,/ 分发/ 模块/ 将/ 其/ 交给/ 上传/ 模块/ 中/ 的/ PUT/ 包/ 处理/ 模块/ 和/ NAP/ 包/ 处理/ 模块/ 进行/ 处理/ ,/ 而/ 对于/ GET/ 包/ ,/ 则/ 从中/ 提取/ 出/ 描述符/ 并/ 写/ 往/ 本地/ 发送/ 引擎/ 的/ 描述符/ 提取/ 模块/ 进行/ 处理/ ./ PUT/ 数据/ 上传/ 模块/ 负责/ 处理/ PUT/ 网络/ 包/ ,/ 从/ 缓冲区/ 中/ 读取/ PUT/ 包将/ 其/ 拆/ 分成/ 多个/ PCIe/ 内存/ 写包/ 上传/ ./ 在/ 遵循/ PCIe/ 协议/ 规定/ 及/ 地址/ 对齐/ 要求/ 等/ 前提/ 下/ ,/ 用/ 尽可能少/ 的/ 逻辑/ 资源/ 实现/ 高效/ 上传/ ./ PUT/ 包/ 和/ NAP/ 包/ 共用/ 接收/ 完成/ 事件/ 环/ ,/ 若/ 需要/ 上传/ 接收/ 完成/ 事件/ ,/ 则/ 需向/ 缓存/ 管理/ 模块/ 申请/ 接收/ 完成/ 事件/ 环/ 地址/ ;/ 如果/ 不/ 要求/ ,/ 则/ 可以/ 直接/ 将/ PUT/ 包/ 转换/ 为/ PCIe/ 内存/ 写包/ 上传/ ./ NAP/ 数据/ 上传/ 模块/ 负责/ 处理/ NAP/ 网络/ 包/ ,/ 并/ 将/ 其/ 转换/ 为/ PCIe/ 标准/ 写包/ 发往/ 缓存/ 空间/ ./ NAP/ 包/ 上传/ 过程/ 与/ PUT/ 包/ 上传/ 过程/ 相似/ ,/ 只是/ NAP/ 包中/ 无地址/ 信息/ ,/ 因此/ 需要/ 从/ 缓冲区/ 管理/ 模块/ 中/ 读取/ 相应/ 的/ 缓存/ 地址/ ./ NAP/ 包/ 必须/ 上传/ 接收/ 完成/ 事件/ ,/ 因此/ 必须/ 先/ 申请/ 得到/ 接收/ 完成/ 事件/ 地址/ 才/ 可以/ 开始/ 上/ Page9/ 传/ ;/ PUT/ 包/ 目的/ 地址/ 可以/ 是/ 任意/ 的/ ,/ NAP/ 包/ 目的/ 地址/ 必须/ 是/ 2KB/ 对齐/ 的/ ./ 缓冲区/ 管理/ 模块/ 负责/ 仲裁/ 所有/ NAP/ 数据/ 接收缓冲区/ 地址/ 的/ 读/ 请求/ ,/ 采用/ 请求/ // 应答/ 机制/ ./ 系统/ 初始化/ 期间/ 分配/ 一定/ 数目/ 的/ 缓冲区/ 地址/ 进行/ 缓存/ ,/ 当/ 接收缓冲区/ 不足/ 时/ ,/ 向/ 缓冲区/ 管理/ 模块/ 发出/ 读/ 地址/ 请求/ ,/ 从/ 内存/ 中/ 读取/ 新/ 的/ NAP/ 接收缓冲区/ 地址/ 并/ 更新/ RAM/ 中/ 缓存/ 的/ NAP/ 地址/ ./ 缓冲区/ 管理/ 模块/ 还/ 负责/ 维护/ 接收/ 完成/ 事件/ 环/ 的/ 流控/ ./ 完成/ 事件/ 缓冲区/ :/ NAP/ 与/ PUT/ 共享/ 一个/ 接收/ 完成/ 事件/ 缓冲区/ ,/ 缓冲区/ 首/ 地址/ 在/ 初始化/ 时/ 由/ 主机/ 告知/ 网络接口/ 控制器/ ,/ 缓冲区/ 逻辑/ 上/ 呈/ 环形/ 结构/ ,/ 可/ 循环/ 使用/ ./ 完成/ 事件队列/ 是/ 一个/ 有限/ 项数/ 的/ 环形/ 缓冲区/ ,/ 每/ 上传/ 一个/ NAP/ 消息/ ,/ 需要/ 向/ 接收/ 完成/ 事件/ 缓冲区/ 写入/ 一个/ 接收/ 完成/ 事件/ ;/ 每/ 上传/ 一个/ PUT/ 包则/ 根据/ 标志/ 位/ 决定/ 是否/ 向/ 接收/ 事件/ 缓冲区/ 写入/ 接收/ 完成/ 事件/ ,/ 通过/ 流控/ 计数器/ 防止/ 接收/ 事件队列/ 的/ 溢出/ ./ 4.3/ I/ // O/ 共享/ 实现/ DMA/ 引擎/ 支持/ SR/ -/ IOV/ 功能/ :/ DMA/ 引擎/ 将/ 虚拟/ 出/ 1/ 个/ 物理/ 功能/ (/ PF/ )/ 和/ 7/ 个/ 虚/ 功能/ (/ VF/ )/ 与/ 相应/ 的/ 配置/ 空间/ 和/ 用于/ 通信/ 所/ 需/ 的/ 资源/ “/ QueuePair/ ”/ (/ QP/ )/ 建立/ 映射/ 关系/ (/ 每个/ 功能/ 对应/ 4/ 个/ QP/ )/ ,/ 使/ 每个/ 虚/ 功能/ 可以/ 分配/ 给/ 不同/ 的/ 虚拟机/ 使用/ ,/ 实现/ I/ // O/ 设备/ 的/ 高效/ 共享/ ,/ 如图/ 14/ 所示/ ./ 同时/ 通过/ QP/ 和/ 虚/ 功能/ 配置/ 空间/ 使/ 处理器/ 获取/ 若干/ “/ 虚拟/ DMA/ 引擎/ ”/ ,/ 这些/ 虚拟/ DMA/ 可供/ 不同/ 虚拟机/ 使用/ ,/ 并/ 实现/ 安全/ 隔离/ 功能/ ,/ 也/ 即/ 实现/ 了/ “/ DMA/ 虚拟化/ ”/ 功能/ ./ 处理器/ 可以/ 通过/ DMA/ 引擎/ 对/ 全局/ 统一/ 编址/ 的/ I/ // O/ 资源/ 进行/ 直接/ 访问/ ,/ 实现/ I/ // O/ 设备/ 的/ 高效/ 共享/ ./ 5/ 原型/ 系统/ 和/ 性能/ 评测/ 实验/ 基于/ XilinxVirtex6X365T/ 实现/ 了/ 原型/ 系统/ ,/ 如图/ 15/ 所示/ ,/ 中间/ 风扇/ 下/ 即/ 为/ 网络接口/ 控制器/ 的/ FPGA/ 原型/ 芯片/ ,/ 顶部/ 插卡/ 为/ 具备/ SR/ -/ IOV/ 功能/ 的/ Intel82599/ 以太网卡/ ,/ 紧邻/ FPGA/ 的/ PCIe/ 线缆/ 连接/ 到/ 另/ 一个/ 主机/ ,/ 作为/ 处理器/ 节点/ 与/ 控制器/ 连接/ ./ 为了/ 平衡/ 网络接口/ 控制器/ 的/ 逻辑/ 规模/ 和/ 工作频率/ ,/ 根据/ FPGA/ 的/ 结构/ 特点/ ,/ 网络接口/ 控制器/ 的/ 内部/ 总线/ 设为/ 128bits/ ,/ 工作频率/ 250MHz/ ./ 为/ 与/ 内部/ 总线/ 带宽/ 相匹配/ ,/ 原型/ 系统/ 的/ 所有/ PCIe/ 接口/ 均/ 选择/ PCIe2/ ./ 0/ ×/ 8/ (/ 峰值/ 5GB/ // s/ ,/ 使用/ 8b/ // 10b/ 编码/ 机制/ ,/ 即/ 有效/ 传输/ 带宽/ 为/ 4GB/ // s/ )/ ./ XilinxVirtex6Lx365t/ 芯片/ 共/ 包含/ 56880/ 个/ Slice/ ,/ 416/ 块/ BlockRAM/ ,/ 网络接口/ 控制器/ 实际/ 设计/ 消耗/ 资源/ :/ Slice/ 消耗量/ 为/ 总量/ 的/ 8/ %/ ,/ BlockRAM/ 为/ 总量/ 的/ 6/ %/ ./ 5.1/ 峰值/ 带宽/ 测试/ 在/ PCIe/ 协议/ 中/ ,/ 单个/ PCIe/ 包所能/ 携带/ 的/ 数据量/ ,/ 即/ 最大/ 传输/ 单元/ MTU/ (/ MaximumTransferUnit/ )/ ,/ 是/ 受/ 设备/ 限制/ 的/ ./ 对于/ 不同/ 的/ MTU/ ,/ PCIe/ 包头/ 所/ 占据/ 的/ 开销/ 比例/ 会受/ 影响/ ./ 在/ 本/ 实验/ 中/ ,/ 将/ 单次/ 内存/ 读取/ 所/ 能/ 请求/ 的/ 数据量/ 大小/ 与/ MTU/ 设为/ 相同/ ,/ 即/ 通过/ 改变/ MTU/ 的/ 值/ ,/ 研究/ PCIe/ 协议/ 对于/ DMA/ 引擎/ 峰值/ 带宽/ 的/ 影响/ ./ 测试/ 在/ 请求/ 2MB/ 数据/ 情况/ 下/ ,/ MTU/ 变化/ 对/ 带宽/ 的/ 影响/ ,/ 采用/ 带宽/ 效率/ 最高/ 的/ PUT/ 包/ 测试/ 峰值/ 性能/ ./ 从图/ 16/ 中/ 可以/ 看出/ ,/ 在/ MTU/ </ 128Bytes/ 时/ ,/ DMA/ 引擎/ 的/ 峰值/ 带宽/ 随着/ MTU/ 呈/ 线性/ 增加/ ,/ 而/ 在/ MTU/ 为/ 128Bytes/ 时/ 达到/ 顶峰/ ,/ 接近/ 3.2/ GB/ // s/ 的/ 实际/ 带宽/ 性能/ 上限/ :/ 16BytesPCIe/ 包头/ 占/ 1/ 个/ 周期/ ,/ 返回/ 最大/ 传输数据/ 128Bytes/ 占/ 8/ 个/ 周期/ ,/ DMA/ 引擎/ 处理/ 需要/ 1/ 个/ 周期/ ,/ 数据/ 返回/ 的/ 效率/ 为/ 80/ %/ ./ 因此/ PCIe/ 协议/ 的/ 限制/ 传输/ 带宽/ 上限/ 为/ 4GB/ // s/ ×/ 80/ %/ ,/ 即/ 3.2/ GB/ // s/ ./ 而/ 当/ MTU/ >/ 128Bytes/ 之后/ ,/ 峰值/ 带宽/ 却/ 不再/ 增加/ ,/ 这是/ 由于/ PCIe/ 协议/ 是/ 通过/ 包头/ Tag/ 域来/ 分辨/ 返回/ 包所/ 对应/ 的/ 是/ 哪个/ 内存/ 读/ 请求/ 所/ 发起/ 的/ ,/ 因此/ ,/ 在/ 发送/ 前要/ 申请/ 到/ Tag/ 号/ 才能/ 上传/ ./ PCIe/ 协议/ 默认/ Tag/ 域/ 的/ 高/ 3bits/ 是/ 保留/ 的/ ,/ 只有/ 低/ 5bits/ 有效/ ,/ 即可/ 用/ 的/ Tag/ 数目/ 为/ 32/ 个/ ,/ 不能/ 随着/ MTUPage10/ 的/ 增大/ 而/ 相应/ 增加/ ,/ 进而/ 导致/ 带宽/ 难以达到/ 设计/ 的/ 理论/ 性能/ ./ DMA/ 发送/ 引擎/ 的/ 描述符/ 读取/ 模块/ ,/ 数据/ 读取/ 模块/ 和/ 接收/ 引擎/ 中/ 用于/ 读取/ 缓存/ 区/ 地址/ 的/ 模块/ 需要/ 申请/ Tag/ 号/ 进行/ 内存/ 读取/ 操作/ ./ 因此/ 在/ 设计/ 中/ ,/ 需要/ 对/ Tag/ 进行/ 统一/ 的/ 分配/ 和/ 调度/ 等/ 方式/ 以免/ Tag/ 被/ 重用/ ./ 对于/ 读取/ 内存/ 的/ 模块/ ,/ 满足/ 持续/ 提交/ PCIe/ 内存/ 读/ 请求/ 所/ 需/ 的/ 最少/ Tag/ 数目/ 如式/ (/ 2/ )/ 所示/ ./ n/ =/ mintSizepayload16/ +/ 2/ ×/ SizepayloadMTU/ 其中/ t/ 为/ 从/ 内存/ 读取/ 模块/ 得到/ Tag/ 号/ 并/ 生成/ PCIe/ 读/ 内存/ 包/ 开始/ ,/ 到/ 响应/ 数据包/ 完全/ 返回/ 所/ 需要/ 的/ 时间/ ,/ 单位/ 为/ 时钟/ 周期/ ,/ 此后/ ,/ 相应/ 的/ Tag/ 号/ 被/ 释放/ ,/ 回收/ 后/ 可/ 循环/ 使用/ ./ Sizepayload/ 则/ 为/ 单个/ PCIe/ 包所能/ 请求/ 的/ 最大/ 数据量/ (/ 单位/ 为/ Byte/ )/ ,/ Sizepayload/ // 16/ 是/ 所/ 请求/ 的/ 数据/ 返回/ 所/ 需要/ 的/ 时钟/ 周期/ 数/ (/ 位/ 宽/ 16Bytes/ )/ ,/ Sizepayload/ // MTU/ 是/ 指/ 所/ 请求/ 的/ 数据/ 被/ 封装/ 为/ PCIe/ 包/ 的/ 个数/ ,/ 而/ 每个/ 数据包/ 都/ 需要/ 一个/ 时钟/ 周期/ 封装/ 包头/ 和/ 一个/ 周期/ 的/ 处理/ 时间/ ,/ 因而/ 要/ 乘以/ 系数/ 2/ ./ 公式/ 表明/ 在/ 这/ 段时间/ 内/ n/ 个/ Tag/ 足够/ 使用/ ,/ 可以/ 连续/ 发送数据/ 请求/ 而/ 不必/ 等待/ Tag/ 返回/ ./ 当/ Sizepayload/ 增加/ 时/ ,/ 维持/ 最高/ 性能/ 所/ 需/ 的/ Tag/ 数目/ 也/ 会/ 随之/ 减少/ ,/ 但是/ 在/ 实际/ 的/ 系统/ 中/ ,/ 由于/ 内存/ 返回/ 数据/ 是/ 与/ Sizepayload/ 成/ 比例/ 的/ ,/ 且/ 存在/ 竞争/ 等/ 不确定性/ 因素/ ,/ t/ 可能/ 会/ 需要/ 成百上千/ 个/ 周期/ ,/ 仅/ 32/ 个/ Tag/ 很难/ 满足/ 持续/ 高效/ 的/ 数据/ 读取/ ./ 为了/ 增加/ 读取/ 效率/ ,/ 可以/ 通过/ 两种/ 方法/ 提高/ 性能/ :/ (/ 1/ )/ 将/ PCIe/ 设备/ 中/ 的/ PCIExpressCapabilityStructure/ 中/ DeviceControl/ 寄存器/ 的/ ExtendedTagFieldEnable/ 位置/ 位/ ,/ 使/ Tag/ 数目/ 扩大/ 为/ 256/ 个/ ,/ 但是/ 该/ 功能/ 需要/ PCIe/ 核/ 的/ 支持/ ;/ (/ 2/ )/ 增/ 大S/ izepayload/ ,/ 该/ 功能/ 需要/ 设备/ 本身/ 及其/ PCIe/ 桥/ 支持/ 大/ 的/ 数据/ 请求/ 量/ ./ 5.2/ 带宽/ 随/ 负载/ 变化/ 情况/ 由于/ 实际/ 系统/ MTU/ 为/ 128Bytes/ ,/ Tag/ 数目/ 为/ 32/ 个/ ,/ 因此/ ,/ 在/ 该/ 条件/ 下/ 测试/ 了/ 带宽/ 随/ 负载/ 变化/ 的/ 情况/ ./ 如图/ 17/ 所示/ ,/ 当/ 数据量/ 较/ 小时/ ,/ PUT/ 包/ 的/ 带宽/ 随/ 负载/ 的/ 增加/ 而/ 显著/ 增加/ ,/ 这/ 是因为/ 包头/ 所/ 占/ 比例/ 迅速/ 减小/ 所致/ ./ 但/ 当/ 负载/ 增加/ 到/ 一定/ 的/ 程度/ 之后/ ,/ 带宽/ 增加/ 变缓/ ,/ 此时/ 负载/ 所/ 占/ 比例/ 的/ 增加/ 已经/ 不够/ 明显/ ./ 当/ 负载/ 的/ 数据量/ 增加/ 到/ 512KB/ 时/ ,/ 实际/ 速度/ 已达/ 3.19/ GB/ // s/ ./ GET/ 包/ 类似/ PUT/ ,/ 但/ 数值/ 略/ 小/ ,/ 是因为/ 接收/ 方会/ 将/ GET/ 描述符/ 提取/ 给/ 发送/ DMA/ 引擎/ ,/ 转化/ 为/ 本地/ PUT/ 操作/ ,/ 延迟/ 会/ 增加/ ./ 由于/ NAP/ 包长/ 最大/ 为/ 2KB/ ,/ 因此/ 只/ 分析/ 到/ 传输数据/ 最大/ 2KB/ ./ 如图/ 17/ 所示/ ,/ NAP/ 包/ 的/ 带宽/ 均/ 随/ 负载/ 而/ 增加/ ./ 在/ 负载/ 很/ 小时/ NAP/ 立即/ 数包/ 的/ 带宽/ 最大/ ,/ 这/ 是因为/ NAP/ 立即/ 数将/ 待/ 传输/ 的/ 源/ 数据/ 封装/ 在/ 描述符/ 中/ ,/ 可/ 直接/ 提取/ 发送/ ,/ 相比/ 其他/ 操作/ 减少/ 一次/ 内存/ 读取/ ./ 而/ 随着/ 负载/ 的/ 增加/ ,/ 两种/ NAP/ 操作/ 的/ 差距/ 开始/ 减小/ ,/ 这/ 是因为/ 描述符/ 为/ 64bits/ ,/ 每/ 两个/ 周期/ 才能/ 提取/ 一个/ 周期/ 的/ NAP/ 立即/ 数/ ,/ 而/ 对于/ NAP/ 间接/ 数/ ,/ 内存/ 直接/ 返回/ 128bits/ 的/ PCIe/ 包/ ,/ 二者/ 开销/ 近似/ ,/ 因而/ 带宽/ 效率/ 开始/ 接近/ ./ 5.3/ 延迟/ 随/ 负载/ 的/ 变化/ 情况表/ 2/ 描述/ 了/ 各种/ 网络/ 包/ 在/ 不同/ 负载/ 情况/ 下/ 的/ 延迟/ 情况/ ./ 考虑/ 到/ 描述符/ 大小/ 对于/ 延迟/ 的/ 影响/ ,/ 将源/ 数据/ 以/ 2MB/ 为/ 单位/ 传输/ ,/ 即/ 使用/ 最小/ 的/ 描述符/ ./ 从表/ 2/ 可知/ ,/ 对于/ PUT/ 包/ ,/ 当/ 数据量/ 最小/ 1Byte/ 时/ ,/ 延迟/ 最小/ ,/ 达到/ 1.762/ μ/ s/ ,/ 而/ 延迟/ 随着/ 数据量/ 的/ 增加/ 而/ 线性/ 增加/ ./ 对于/ GET/ 请求/ ,/ 最小/ 延迟/ 为/ 1.838/ μ/ s/ ,/ 是因为/ 接收/ 方会/ 将/ GET/ 请求/ 转化/ 为/ 本地/ 的/ PUT/ 操作/ ,/ 因而/ 延迟/ 略大于/ PUT/ 包/ ./ NAP/ 立即/ 数包/ 的/ 延迟/ 最小/ 仅/ 1.242/ μ/ s/ ,/ 这/ 是因为/ 其源/ 数据/ 直接/ 包含/ 在/ 描述符/ 中/ ,/ 相对/ 其他/ 类型/ 减少/ 了/ 一次/ 内存/ 读取/ ,/ 因此/ 延迟/ 最小/ ./ 而/ NAP/ 间接/ 数/ 最小/ 延迟/ 为/ 1.846/ μ/ s/ ./ 延迟/ 略/ Page11Payload/ // BytesNAP/ 立即/ 数/ NAP/ 间接/ 数/ PUTGET11/ ./ 2421.8461/ ./ 7621.838641/ ./ 2541.8621/ ./ 7781.8542561/ ./ 4582.0181/ ./ 8381.91410242/ ./ 5142.8582/ ./ 0782.15420483/ ./ 9223.9782/ ./ 3982.474409616384655362621441048576/ 大于/ PUT/ 包/ ,/ 这/ 是因为/ NAP/ 包/ 没有/ 指定/ 目标/ 地址/ ,/ 在/ 接收端/ 需要/ 申请/ 缓存/ 地址/ ,/ 也/ 需要/ 一次/ 内存/ 读取/ ,/ 因此/ 延迟/ 会/ 大于/ PUT/ 包/ ./ 图/ 18/ 显示/ 了/ NAP/ 立即/ 数/ 和/ PUT/ 两种/ 网络/ 包/ 在/ 发送/ 1Byte/ 数据/ 负载/ 条件/ 下/ 的/ 延迟/ 情况/ ,/ 时间/ 从图/ 18/ 传输/ 延迟/ 分析/ 5.4/ 吞吐/ 率/ 分析/ 在/ 不同/ 端口/ 相互/ 通信/ 的/ 过程/ 中/ ,/ 随着/ 通信量/ 的/ 增加/ ,/ 连接/ 通信接口/ 的/ 交叉开关/ 会/ 变得/ 拥堵/ ,/ 进而/ 导致/ DMA/ 传输/ 请求/ 响应/ 时间/ 变/ 长/ ,/ 即/ 延迟/ 增加/ ./ 增加/ 虚/ 通道/ 数量/ 可/ 有效/ 缓解/ “/ 队头/ 阻塞/ ”/ 的/ 影响/ ,/ 因此/ ,/ 实验/ 测试/ 了/ 不同/ 端口数/ 配置/ 不同/ 虚/ 通道/ 条件/ 下/ 的/ 通信/ 吞吐/ 率/ 情况/ ./ 为/ 避免/ 请求/ 和/ 响应/ 导致/ 的/ 死锁/ ,/ 规定/ GET/ 数据包/ 需要/ 走/ 专用/ 虚/ 通道/ ./ 为/ 提高/ 吞吐/ 率/ ,/ 本/ 节/ 采用/ 传输/ 效率高/ 的/ PUT/ 数据包/ 进行/ 测试/ ,/ 因此/ ,/ 这里/ 不/ 考虑/ GET/ 专用/ 虚/ 通道/ ./ 定义/ 虚/ 通道/ 的/ 使用/ 策略/ Dest/ -/ Mod/ :/ 如果/ 虚/ 通道/ 数量/ 为/ n/ ,/ 而/ 消息/ 的/ 目的/ 端口/ 是/ d/ ,/ 则/ 消息/ 将会/ 被/ 缓存/ 在/ 虚/ 通道/ [/ dmodn/ ]/ 内/ ./ 图/ 19/ 为/ 4/ 端口/ 2VC/ (/ VirtualChannel/ :/ 虚/ 通道/ )/ (/ 每个/ 端口/ 2VC/ ,/ 下同/ )/ 、/ 4/ 端口/ 4VC/ 、/ 8/ 端口/ 2VC/ 、/ 8/ 端口/ 4VC/ 和/ 16/ 端口/ 4VC/ 条件/ 下/ 采用/ 所/ 设计/ 的/ 基于/ PCIe/ 标准/ 的/ 通信协议/ 进行/ 互连/ 通信/ 时/ ,/ 延迟/ 和/ 带宽/ 的/ 关系/ ./ 延迟/ 从/ 写/ 门铃/ 开始/ 计算/ ,/ 到/ 接收端/ 全部/ DMA/ 接收/ 引擎/ 接收/ 到/ 主机/ 发起/ 的/ 门铃/ 开始/ ,/ 到/ DMA/ 发送/ 引擎/ 将/ 网络/ 包/ 转换/ 为/ PCIe/ 数据包/ 上传/ 数据/ 为止/ ,/ 其/ 数据/ 流程/ 与/ 第/ 4/ 节所/ 描述/ 的/ 一致/ ,/ 其中/ Crossbar/ 延迟/ 是/ 节点/ 内/ 的/ 交换/ 延迟/ ./ 对于/ NAP/ 数据包/ 需要/ 请求/ 缓存/ 地址/ ,/ 而/ 虚/ 线框/ 表示/ DMA/ 会/ 预先/ 读取/ 并/ 保存/ 一定/ 数量/ 的/ 缓存/ 地址/ ,/ 以/ 避免/ 频繁/ 发起/ 地址/ 请求/ ./ 其中/ 描述符/ 和/ 数据/ 读取/ 和/ 返回/ 的/ 延迟/ 受/ 内存/ 控制器/ 的/ 响应速度/ 和/ 资源/ 竞争/ 的/ 影响/ 是/ 动态变化/ 的/ ,/ 图/ 18/ 中/ 的/ “/ 130/ ”/ 是/ 经验值/ ./ 若/ 不/ 考虑/ 内存/ 数据/ 读取/ 的/ 延迟/ ,/ DMA/ 引擎/ 的/ 处理/ 延迟/ 仅/ 35/ (/ NAP/ )/ 到/ 48/ (/ PUT/ )/ 时钟/ 周期/ ./ 尤其/ 在/ Crossbar/ 后/ 的/ DMA/ 上传/ 阶段/ ,/ NAP/ 操作/ (/ 当/ 缓存/ 地址/ 足够/ 时/ )/ 和/ PUT/ 操作/ 均/ 将/ 数据/ 直接/ 打包/ 为/ 标准/ PCIe/ 数据包/ 上传/ ,/ 简化/ 了/ 繁琐/ 的/ 协议/ 转换/ ./ 接收/ 并/ 处理/ 完/ 网络/ 包/ 结束/ ,/ 最后/ 对/ 所有/ 请求/ 的/ 延迟/ 进行/ 平均/ ./ 吞吐/ 率/ 定义/ 为/ 传输/ 稳定/ 之后/ ,/ 对/ 每个/ 端口/ 的/ 带宽/ 求/ 平均/ 并/ 除以/ 理论/ 峰值/ ./ 从图/ 19/ 中/ 可以/ 看出/ ,/ 在/ 端口/ 数目/ 一定/ 的/ 情况/ 下/ ,/ 吞吐/ 率/ 随着/ 虚/ 通道/ 数目/ 增加/ 而/ 增大/ ;/ 在/ 虚/ 通道/ 数目/ 一定/ 的/ 情况/ 下/ ,/ 吞吐/ 率则/ 随着/ 端口数/ 目的/ 增加/ 而/ 减小/ ./ 4/ 端口/ 时/ ,/ 在/ 每/ 端口/ 2VC/ 的/ 情况/ 下/ ,/ 当/ 吞吐/ 率/ 接近/ 最大/ 理论/ 带宽/ 的/ 65/ %/ 时/ ,/ 延迟/ 开始/ 迅速/ 上升/ ,/ 此时/ 通信/ 系统/ 的/ 吞吐/ 率为/ 2.6/ GB/ // s/ ./ 在/ 每/ 端口/ 4VC/ 的/ 情况/ 下/ ,/ 系统/ 的/ 吞吐/ 率/ 达到/ 2.8/ GB/ // s/ ./ 带宽/ 较/ 2VC/ 有所提高/ 是/ 由于/ 在/ 4/ 端口/ 4VC/ 的/ 情况/ 下/ 构成/ VOQ/ 结构/ [/ 9/ ]/ ,/ 可/ 消除/ 队头/ 阻塞/ 问题/ ./ 没有/ 达到/ 实际/ 性能/ 上限/ ,/ 是因为/ 仿真/ 环境/ 的/ 随机性/ 不足/ ,/ 没有/ 达到/ 完全/ 均匀/ 随机/ 分布/ ./ 考虑/ 到/ 4VC/ 相对/ 于/ 2VC/ 仅/ 提高/ 了/ 5/ %/ 的/ 性能/ ,/ 综合/ 资源/ 消耗/ 和/ 性能/ 的/ 考虑/ ,/ 4/ 端口/ 下/ 2VC/ 的/ 设计/ 在/ 性能/ 和/ 资源/ 上/ 都/ 能/ 达到/ 较/ 好/ 的/ 平衡/ ./ Page125/ ./ 5I/ // O/ 虚拟化/ 性能/ 分析/ 在/ 设计/ 中/ ,/ 虚拟机/ 通过/ QP/ 可以/ 同时/ 使用/ 多个/ 网卡/ ,/ 为/ 实现/ 合理/ 高效/ 的/ 资源共享/ ,/ 使/ 每个/ 虚/ 功能/ 都/ 能/ 公平/ 的/ 使用/ 系统资源/ ,/ 采取/ 了/ 公平/ 的/ 仲裁/ 策略/ ./ 针对/ 各个/ 虚/ 功能/ 的/ 门铃/ 请求/ ,/ 实现/ 了/ 公平/ 的/ 调度/ 策略/ ./ 图/ 20/ 为/ 各个/ VF/ 分配/ 的/ 带宽/ 随/ 时间/ 的/ 变化/ 情况/ ./ 测试/ 中/ 使用/ 随机/ 大小/ 的/ PUT/ 包/ ,/ 并/ 随机/ 的/ 将/ 请求/ 分配/ 给/ 各个/ 功能/ ./ 测试/ 结果/ 如图/ 20/ 所示/ ,/ 不同/ 曲线/ 描述/ 的/ 是/ VF0/ ~/ VF6/ 和/ PF/ 所/ 分配/ 到/ 的/ 带宽/ 情况/ ./ 纵轴/ 曲线/ 之间/ 的/ 间隔/ 表示/ 不同/ 功能/ 所/ 占/ 带宽/ 的/ 百分比/ ,/ 横轴/ 为/ 仿真/ 时间/ ,/ 单位/ 为/ 时钟/ 周期/ 右侧/ 标名/ 曲线/ ./ 从图/ 中/ 可以/ 看出/ ,/ 传输/ 刚/ 开始/ 时/ ,/ 带宽/ 的/ 分配/ 变化/ 很/ 剧烈/ ,/ 这/ 是因为/ DMA/ 引擎/ 刚/ 开始/ 工作/ 时/ ,/ 已/ 传输/ 的/ 数据量/ 较/ 小/ ,/ 因此/ 单次/ 请求/ 传输/ 的/ 数据/ 占/ 总/ 数据量/ 的/ 比例/ 比较/ 大/ ,/ 因而/ 对/ 带宽/ 的/ 分配/ 有着/ 很大/ 的/ 影响/ ,/ 但/ 很快/ 各个/ VF/ 之间/ 带宽/ 分配/ 的/ 比例/ 就/ 基本/ 达到/ 均衡/ ./ 大概/ 在/ 150/ 万个/ 时钟/ 周期/ 带宽/ 分配/ 达到/ 稳定/ 状态/ ,/ 很/ 好/ 的/ 达到/ 了/ 公平/ 分配/ 带宽/ 的/ 目的/ ./ 6/ 相关/ 研究/ 节点/ 控制器/ 是/ 高性能/ 互连/ 网络/ 的/ 核心/ 组件/ ,/ 是/ 提供/ 高性能/ 通信/ 的/ 重要/ 保障/ ./ 全局/ 地址/ 统一/ 编址/ 、/ 提供/ 高效/ 通信/ 原语/ 、/ 用户/ 级/ 通信/ 等/ 多种/ 关键技术/ 被/ 许多/ 高性能/ 计算机/ 的/ 大规模/ 互连/ 网络/ 所/ 采用/ ,/ 例如/ :/ IBMBlueGene/ 系列/ [/ 5/ -/ 6/ ,/ 10/ ]/ 在/ 处理器/ 内/ 集成/ 了/ 高性能/ 网络/ 路由器/ ,/ 其/ 网络接口/ 支持/ 直接/ PUT/ 和/ 远程/ GET/ ;/ Cray/ 的/ Gemini/ [/ 11/ ]/ 和/ Aries/ [/ 12/ ]/ 系列/ 互连/ 网络/ 的/ 网络接口/ 也/ 在/ 硬件/ 层次/ 提供/ 了/ 支持/ 小/ 消息/ 传输/ 的/ 快速/ 消息/ 访问/ (/ FMA/ )/ 和/ 长/ 消息/ 传输/ 的/ 块/ 传输/ 引擎/ (/ BTE/ )/ ;/ 商业/ 网络/ 如/ Infiniband/ [/ 4/ ]/ ,/ 其/ 主机/ 通道/ 适配器/ (/ HCA/ )/ 提供/ 硬件/ 支持/ 的/ RDMAPUT/ // GET/ 操作/ 、/ 采用/ IPV6/ 兼容/ 的/ 128bits/ 全局/ 地址/ 以/ 支持/ 远程/ 直接/ 内存/ 访问/ ./ 由于/ 这些/ 网络接口/ 控制器/ 的/ 设计/ 均/ 面向/ 大规模/ 互连/ 网络/ ,/ 因此/ 一次/ 端/ 对/ 端/ 通信/ 要/ 涉及/ I/ // O/ 总线/ 协议/ -/ 网络协议/ -/ I/ // O/ 总线/ 协议/ 间/ 的/ 转换/ 流程/ (/ 解析/ →/ 打包/ →/ 解析/ →/ 拆包/ )/ ,/ 其/ 消息/ 格式/ 定义/ 也/ 需要/ 涵盖/ 大规模/ 数据传输/ 所/ 需/ 的/ 子网/ 管理/ 、/ 路由/ 、/ 拥塞/ 控制/ 等/ 信息/ (/ 例如/ Infiniband/ 的/ 包头/ 最大/ 可/ 达到/ 94Bytes/ )/ ,/ 限制/ 了/ 网络/ 处理/ 的/ 效率/ 和/ 有效/ 负载/ 带宽/ 的/ 提升/ ./ 然而/ 在/ 规模/ 限定/ 的/ 局部/ 互连/ 中/ ,/ 上述/ 开销/ 均/ 为/ 无效/ 开销/ ,/ 本文/ 正是/ 通过/ 面向/ 局部/ 通信/ 的/ 互连/ 协议/ 和/ DMA/ 引擎/ 结构设计/ ,/ 实现/ 对/ 上述/ 开销/ 的/ 优化/ ./ 此外/ ,/ 在/ 局部/ 互连/ 网络/ 中/ ,/ 存在/ 着/ 网络/ 互连/ 和/ I/ // O/ 设备/ 扩展/ 两种/ 功能/ 需求/ ,/ 本文/ 通过/ 扩展/ PCIe/ 协议/ 实现/ 的/ 互连/ 网络/ ,/ 实现/ 了/ 在/ 物理层/ 上/ 两种/ 功能/ 的/ 融合/ ,/ 这是/ 面向/ 大规模/ 互连/ 网络/ 的/ 网络接口/ 控制器/ 所/ 不/ 具备/ 的/ ./ 正是/ 上述/ 功能/ 融合/ 的/ 需求/ ,/ 国际/ 上/ 已有/ 利用/ PCIe/ 实现/ 处理器/ 间/ 互连/ 的/ 工作/ ,/ 例如/ 日本/ 瑞萨/ 公司/ 的/ PERAL/ [/ 13/ ]/ 使用/ PCIe/ 作为/ 通信/ 链路/ 设计/ 了/ 功耗/ 感知/ 的/ ,/ 高/ 可靠/ 和/ 高性能/ 的/ 互连/ 芯片/ ,/ 最低/ 传输/ 延迟/ 1.2/ μ/ s/ ,/ 但/ 带宽/ 仅为/ 1.1/ GB/ // s/ (/ 理论/ 峰值/ 的/ 55/ %/ )/ ./ Dolphin/ 公司/ 使用/ 增强/ 的/ PCIe/ 互连/ 方案/ [/ 14/ ]/ 来/ 实现/ 多/ 主机/ 间通信/ 和/ 主机/ 到/ I/ // O/ 通信/ 的/ 功能/ ,/ 由于/ 同时/ 面向/ 大规模/ 机群系统/ ,/ 因此/ 它/ 并/ 没有/ 进行/ 针对/ 局部/ 互连/ 的/ 优化/ ,/ 其/ 传输/ 延迟/ 为/ 14/ μ/ s/ ,/ 带宽/ 为/ 1.27/ GB/ // s/ ./ 表/ 3/ 比较/ 了/ cHPP/ 网络接口/ 控制器/ 与/ 上述/ 经典/ 互连/ 网络/ ①/ 以及/ 采用/ PCIe/ 标准/ 的/ 商业/ 网络/ [/ 13/ -/ 14/ ]/ 的/ 性能/ ./ 从表/ 3/ 可以/ 看出/ ,/ cHPP/ 网络接口/ 控制器/ 的/ FPGA/ 原型/ 系统/ 在/ 带宽/ 的/ 绝对/ 性能/ 方面/ (/ 绝对/ 带宽/ 与/ 实现/ 工艺/ 相关/ )/ 低于/ Cray/ 的/ 高性能/ 互连/ 网络/ ,/ 但/ 超过/ 同/ 类型/ 的/ PCIe/ 互连/ 网络/ (/ PERAL/ 和/ DolphinExpress/ )/ ,/ 同时/ 可以/ 获得/ 与/ 峰值/ 带宽/ 相同/ 的/ Infini/ -/ Band/ (/ QDR/ )/ 相近/ 的/ 性能/ ./ 上述/ 网络接口/ 控制器/ 在/ 延/ ①/ Networkbandwidthsandlatenciesforsomenetworks/ ./ Page13/ 迟/ 方面/ 的/ 性能/ 相似/ ,/ 根据/ 图/ 18/ 的/ 延迟/ 分析/ 可知/ ,/ 若/ 提高/ cHPP/ 控制器/ 的/ 工作频率/ ,/ 其/ 硬件/ 延迟/ 还/ 可/ 进一步/ 降低/ ./ NetworksBandwidth/ // (/ GB/ // s/ )/ Latency/ // μ/ sInfiniBandQDRCrayGeminiCrayAriesBlueGene/ // QDolphinExpress7/ 结论/ 和/ 展望/ cHPP/ 体系结构/ 采用/ 超/ 节点/ 设计/ 可/ 支持/ 多种/ 网络拓扑/ ,/ 超/ 节点/ 结构/ 可/ 减少/ 通信/ 路径/ ,/ 缩减/ 网络/ 规模/ ,/ 并且/ 充分利用/ 了/ 超/ 节点/ 内部/ 通信/ 的/ 局部性/ ,/ 可/ 有效/ 降低/ 通信/ 延迟/ ./ 超/ 节点/ 控制器/ 采用/ PCIe/ 高速接口/ 提供/ 高带宽/ 和/ 高/ 可靠/ 的/ 链路/ 性能/ ./ 网络接口/ 控制器/ 支持/ 用户/ 级/ 通信/ 和/ 高效/ 通信/ 原语/ 加速/ 大/ 数据量/ 传输/ ,/ 降低/ 处理器/ 的/ 占用率/ ,/ 进一步提高/ 计算能力/ ./ 基于/ 硬件/ 支持/ 超/ 节点/ 内/ I/ // O/ 资源/ 的/ 高效/ 共享/ ,/ 在/ 极小/ 性能/ 损失/ 的/ 条件/ 下/ 可/ 满足/ 大量/ 并发/ 的/ 通信/ 请求/ ./ 下/ 一步/ 工作/ 主要/ 是/ 在/ 节点/ 内/ 通信/ 的/ 基础/ 上/ 进行/ 系统/ 的/ 级联/ 扩展/ ,/ 支持/ 高/ 维度/ 网络拓扑/ 的/ 大规模/ 直接/ 网络/ ,/ 实现/ 全/ 系统/ 节点/ 间/ 处理器/ 的/ 高速通信/ ;/ 针对/ MTU/ 和/ Tag/ 等/ 限制/ 因素/ 进一步/ 优化/ DMA/ 引擎/ 结构/ ,/ 改善/ 通信/ 性能/ ;/ 拓展/ I/ // O/ 虚拟化/ 性能/ ,/ 实现/ 不同/ 节点/ 间/ 的/ 处理器/ 对/ 全局/ I/ // O/ 资源/ 的/ 高效/ 共享/ ;/ 并/ 根据/ 不同/ 层次/ 的/ 通信/ 需求/ ,/ 通过/ 调整/ QP/ 资源/ 在/ 虚拟机/ 和/ VF/ 之间/ 的/ 动态分配/ 来/ 实现/ 丰富/ 的/ QoS/ 服务/ ./ 致谢/ 感谢/ 李强/ 博士/ 在/ 文章/ 撰写/ 过程/ 中/ 关于/ 通信/ 原语/ 和/ 上层/ 软件/ 库/ 之间/ 关系/ 的/ 深入/ 有益/ 的/ 讨论/ !/ 

