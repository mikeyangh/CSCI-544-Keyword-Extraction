Page1/ 基于/ 线程/ 的/ MPI/ 通信/ 加速器/ 技术/ 研究/ 刘志强/ 宋君强/ 卢风顺/ 赵娟/ (/ 国防科学技术大学/ 计算机/ 学院/ 长沙/ 410073/ )/ 摘要/ 为了/ 针对/ 多核/ 系统/ 构建/ 更/ 高效/ 的/ MPI/ 支撑/ 环境/ ,/ 文中/ 提出/ 了/ 一种/ 基于/ 线程/ 的/ MPI/ 加速器/ ,/ 称作/ MPIActor/ ./ MPIActor/ 是/ 一种/ 用于/ 协助/ 传统/ MPI/ 库/ 的/ 透明/ 中间件/ ,/ 用户/ 可以/ 在/ 编译/ 期/ 选择/ 是否/ 在/ 单线程/ MPI/ 程序/ 中/ 采用/ 该/ 中间件/ ./ 加入/ MPIActor/ 后/ ,/ 每个/ 节点/ 内/ 的/ MPI/ 进程/ 都/ 被/ 映射/ 成/ 同一/ 进程/ 中/ 的/ 多个/ 线程/ ,/ 从而/ 节点/ 内/ 的/ 通信/ 可/ 通过/ 轻量级/ 的/ 线程/ 通信/ 机制/ 实现/ ./ 作者/ 给出/ 了/ MPIActor/ 的/ 基本/ 设计/ ,/ 详细/ 阐述/ 了/ 其/ 工作/ 机制/ 、/ 通信/ 体系结构/ 及/ 关键技术/ ,/ 并/ 在/ 真实/ 系统/ 上/ 分别/ 针对/ MVAPICH2/ 和/ OpenMPI/ 并行/ 环境/ 利用/ OSULATENCY/ 基准/ 测试/ 进行/ 了/ 性能/ 评测/ ./ 实验/ 结果表明/ 在/ 两种/ MPI/ 环境/ 上/ 进行/ 节点/ 内/ 8KB/ ~/ 4MB/ 数据通信/ 时/ MPIActor/ 都/ 能/ 使/ 通信/ 性能/ 平均/ 提高/ 一倍/ 左右/ ./ 关键词/ MPI/ 软件结构/ ;/ 线程/ MPI/ ;/ MPI/ 加速器/ ;/ MPIActor1/ 引言/ 多/ 核技术/ ,/ 即/ 在/ 同一/ 芯片/ 内/ 包含/ 更/ 多/ 的/ 核心/ ,/ 被/ 认为/ 是/ 一种/ 能够/ 继续/ 保持/ 处理器/ 性能/ 发展/ 遵循/ 摩尔定律/ 的/ 有效/ 手段/ ./ 在/ 过去/ 的/ 几年/ 中/ ,/ 多核/ 处理器/ 的/ 发展/ 验证/ 了/ 该/ 手段/ 的/ 有效性/ ,/ 其间/ 处理器/ 内/ 的/ 核心/ 数目/ 也/ 基本/ 保持/ 每/ 18/ 个/ 月/ 左右/ 增长/ 一倍/ ①/ ②/ ./ 可以/ 预/ Page2/ 见/ ,/ 在/ 相当/ 的/ 一段时间/ 内多核/ 处理器/ 的/ 核心/ 数目/ 还会/ 持续增长/ ./ 在/ 多/ 核/ 处理器/ 高速/ 发展/ 的/ 背景/ 下/ ,/ 近年来/ ,/ 采用/ 多核/ 处理器/ 的/ 集群/ 系统/ (/ 简称/ 多核/ 集群/ )/ 主导/ 了/ 高性能/ 计算/ (/ HPC/ )/ 领域/ 的/ 主流/ 硬件平台/ ./ 消息传递/ 接口/ (/ MPI/ )/ 是/ 该/ 领域/ 并行程序/ 开发/ 的/ 事实标准/ ./ 基于/ MPI/ 的/ 并行程序/ 通过/ MPI/ 运行/ 时库/ 提供/ 的/ 消息传递/ 接口函数/ 进行/ 通信/ ;/ 随着/ 节点/ 内/ 核心/ 数目/ 的/ 增多/ ,/ 节点/ 内/ 通信/ 的/ 性能/ 将会/ 对/ MPI/ 程序/ 的/ 整体/ 性能/ 产生/ 越来越/ 重要/ 的/ 影响/ ./ 例如/ 当/ 64/ 个/ MPI/ 进程/ 平均/ 分布/ 在/ 16/ 个/ 节点/ 上/ 进行/ HPL/ 基准/ 测试/ 时/ ,/ 约/ 有/ 57/ %/ 的/ 通信/ 发生/ 在/ 节点/ 内/ [/ 1/ ]/ ./ 传统/ MPI/ 基础/ 软件/ (/ 如/ MPICH2/ ①/ 、/ OpenMPI/ ②/ 和/ MVAPICH/ ③/ )/ 的/ 节点/ 内/ 通信/ 性能/ 受限于/ 低效/ 的/ 操作系统/ 进程/ 间通信/ ./ 这一/ 问题/ 归因于/ 传统/ MPI/ 基础/ 软件/ 将/ 每个/ MPI/ 逻辑/ 进程/ 映射/ 为/ 一个/ 操作系统/ 进程/ ;/ 相应/ 地/ MPI/ 逻辑/ 进程/ 间通信/ 只能/ 通过/ 高/ 代价/ 的/ 操作系统/ 进程/ 间通信/ 完成/ ,/ 由此/ 带来/ 的/ 性能/ 缺陷/ 难以/ 通过/ 单纯/ 地/ 改进/ 通信/ 算法/ 来/ 克服/ ./ 在/ 20/ 世纪/ 90/ 年代/ 末/ 大规模/ SMP/ 系统/ 发展/ 的/ 顶峰/ 时期/ ,/ 一些/ 工作/ [/ 2/ -/ 4/ ]/ 曾/ 针对/ 大规模/ SMP/ 系统对/ 线程/ MPI/ (/ ThreadedMPI/ )/ 进行/ 过/ 研究/ ./ 线程/ MPI/ 即/ :/ 在/ 共享内存/ 系统/ 中/ ,/ 将/ MPI/ 逻辑/ 进程/ 映射/ 为/ 同一/ 操作系统/ 进程/ 中/ 的/ 多个/ 线程/ ./ 虽然/ 这些/ 早期/ 的/ 工作/ 随着/ 大规模/ SMP/ 系统/ 淡出/ 市场/ 没有/ 得到/ 延续/ ,/ 但/ 这些/ 工作/ 的/ 结果表明/ :/ 线程/ MPI/ 可以/ 通过/ 改进/ MPI/ 程序/ 的/ 运行机制/ 来/ 提供/ 高效/ 的/ 节点/ 内/ 通信/ ./ 理论/ 上线/ 程/ MPI/ 同样/ 是/ 一种/ 可以/ 用于/ 改进/ 多核/ 系统/ 上/ 节点/ 内/ 通信/ 性能/ 的/ 有效途径/ ./ 但/ MPI/ 多年/ 来/ 的/ 发展/ 和/ 用户/ 对/ MPI/ 应用/ 需求/ 的/ 提高/ 使得/ 线程/ MPI/ 面临/ 两/ 方面/ 的/ 问题/ ./ 首先/ ,/ 线程/ MPI/ 难以/ 支持/ MPI/ 逻辑/ 进程/ 内/ 的/ 多线程/ ./ 支持/ MPI/ 逻辑/ 进程/ 内/ 的/ 多线程/ 是/ MPI2/ ./ 0/ 标准/ 中/ 的/ 一项/ 重要/ 内容/ ④/ ,/ 它/ 的/ 缺失/ 将会/ 导致/ 多线程/ 应用程序/ (/ 如/ 基于/ MPI/ +/ OpenMP/ 的/ 应用程序/ )/ 无法/ 在线/ 程/ MPI/ 环境/ 中/ 正常/ 运行/ ./ 其次/ ,/ 传统/ 的/ MPI/ 基础/ 软件/ 历经/ 数年/ 的/ 发展/ ,/ 规模/ 已/ 非常/ 庞大/ ,/ 由于/ 底层/ 机制/ 的/ 改变/ ,/ 即使/ 在/ 已有/ 代码/ 的/ 基础/ 上/ 开发/ 一套/ 完整/ 的/ 线程/ MPI/ 基础/ 软件/ 也/ 需要/ 庞大/ 的/ 工作量/ ./ 据/ 我们/ 所知/ ,/ 目前/ 仍/ 无/ 成熟/ 的/ 线程/ MPI/ 支撑/ 环境/ 供/ 用户/ 使用/ ./ 为了/ 实现/ 高效/ 的/ 线程/ MPI/ 支撑/ 环境/ ,/ 本文/ 提出/ 了/ 一种/ 基于/ 线程/ 的/ MPI/ 加速器/ 技术/ ,/ 称作/ MPIActor/ ./ MPIActor/ 是/ 一种/ 可/ 协助/ 传统/ MPI/ 软件/ 的/ 透明/ 中间件/ ,/ 它/ 位于/ 用户/ MPI/ 程序/ 和/ MPI/ 运行/ 时库/ 之间/ 并/ 对/ 用户/ MPI/ 程序/ 提供/ 线程/ MPI/ 支撑/ 环境/ ./ 传统/ 的/ MPI/ 支撑/ 环境/ 由/ 单一/ MPI/ 运行/ 时库/ 实现/ 所有/ 通信/ ;/ 引入/ MPIActor/ 后/ ,/ MPI/ 支撑/ 环境/ 则/ 由/ 独立/ 并/ 低/ 耦合/ 的/ 两/ 部分/ 运行/ 时库/ 分别/ 完成/ 节点/ 内/ 通信/ 和/ 节点/ 间通信/ ./ MPIActor/ 的/ 贡献/ 在于/ :/ (/ 1/ )/ 以低/ 耦合/ 的/ 软件结构/ 化解/ 了/ 线程/ MPI/ 和/ MPI/ 逻辑/ 进程/ 内/ 多线程/ 之间/ 的/ 矛盾/ ./ 程序员/ 可以/ 根据/ 需要/ 在/ 编译/ 期/ 选择/ 是否/ 加入/ MPIActor/ ,/ 编译/ 时/ 加入/ MPIActor/ 得到/ 的/ MPI/ 可执行程序/ 是/ 线程/ MPI/ 的/ ,/ 编译/ 时/ 不/ 加入/ MPIActor/ 得到/ 的/ MPI/ 可执行程序/ 能够/ 支持/ MPI/ 逻辑/ 进程/ 内/ 多线程/ ./ (/ 2/ )/ 实现/ 了/ 线程/ MPI/ 支撑/ 环境/ 并/ 充分利用/ 了/ 传统/ 的/ MPI/ 基础/ 软件/ ,/ 降低/ 了/ 开发/ 线程/ MPI/ 的/ 难度/ 和/ 工作量/ ./ 在/ 基于/ MPI/ +/ MPIActor/ 的/ 并行程序/ 中/ ,/ 节点/ 内/ 通信/ 由/ MPIActor/ 完成/ ,/ 节点/ 间通信/ 由/ MPIActor/ 转发/ 到/ 下层/ 的/ MPI/ 库/ 完成/ ./ 因此/ ,/ MPIActor/ 可以/ 专注/ 于/ 实现/ 高效/ 的/ 节点/ 内/ 通信/ ,/ 而/ 对/ 节点/ 间通信/ 只/ 需/ 实现/ 简单/ 的/ 转发/ ./ (/ 3/ )/ 不/ 依赖于/ 特定/ 的/ 传统/ MPI/ 基础/ 软件/ 提高/ 节点/ 内/ 通信/ 性能/ ,/ 即/ MPIActor/ 即可/ 用于/ MVAPICH2/ 也/ 可/ 用于/ OpenMPI/ 或/ 其它/ 符合/ MPI2/ ./ 0/ 标准/ 的/ 基础/ 软件/ ./ 我们/ 目前/ 已/ 实现/ 了/ MPIActor/ 的/ 基本/ 框架/ 和/ 节点/ 内/ 点对点/ 通信/ 模块/ ./ 本文/ 在/ 一个双/ 路四核/ Nehalem/ 系统/ [/ 5/ ]/ 上/ 利用/ OSULATENCY/ 基准/ 测试/ ③/ 分别/ 在/ MVAPICH2/ 和/ OpenMPI/ 环境/ 中/ 对/ MPIActor/ 进行/ 了/ 性能/ 评测/ ,/ 实验/ 结果表明/ MPIActor/ 能够/ 在/ 不/ 影响/ 节点/ 间通信/ 的/ 情况/ 下/ 有效/ 地/ 提高/ MPI/ 运行/ 环境/ 的/ 节点/ 内/ 通信/ 性能/ ./ 在/ 两种/ MPI/ 环境/ 上/ 传输/ 8KB/ ~/ 4MB/ 之间/ 的/ 数据/ 时/ ,/ MPIActor/ 能够/ 使/ 节点/ 内/ 通信/ 性能/ 平均/ 提高/ 一倍/ 左右/ ./ 这些/ 数据/ 表明/ MPI/ 加速器/ 技术/ 是/ 一种/ 对/ 传统/ MPI/ 基础/ 软件/ 有效/ 的/ 节点/ 内/ 通信/ 性能/ 优化/ 技术/ ./ 本文/ 第/ 2/ 节/ 介绍/ 相关/ 工作/ ;/ 第/ 3/ 节/ 阐述/ MPI/ -/ Actor/ 的/ 工作/ 机制/ ;/ 第/ 4/ 节/ 介绍/ MPIActor/ 的/ 通信/ 体系结构/ ;/ 第/ 5/ 节/ 和/ 第/ 6/ 节/ 介绍/ 实现/ MPIActor/ 两个/ 方面/ 的/ 关键技术/ :/ 通信/ 操作/ 聚合/ 和/ 基于/ 单次/ 内存/ 拷贝/ 的/ 节点/ 内/ 点对点/ 通信/ ;/ 第/ 7/ 节在/ 真实/ 平台/ 上/ 进行/ ①/ ②/ ③/ ④/ Page3/ 性能/ 评测/ ;/ 最后/ 一节/ 对/ 本文/ 的/ 工作/ 进行/ 总结/ 并/ 提出/ 对/ 未来/ 工作/ 的/ 展望/ 和/ 设想/ ./ 2/ 相关/ 工作/ MPI/ 标准/ 的/ 制定/ 计划/ 诞生/ 于/ 1992/ 年/ ,/ 并于/ 1994/ 年/ 发布/ 第一个/ 版本/ ./ MPI/ 的/ 初衷/ 是/ 为/ 分布/ 内存/ 系统/ 上/ 的/ 消息传递/ 并行/ 编程/ 模式/ 提出/ 统一/ 的/ 标准/ ①/ ,/ 因此/ ,/ 在/ 基于/ 传统/ MPI/ 基础/ 软件/ 的/ 程序/ 中/ ,/ 每个/ 逻辑/ 上/ 的/ MPI/ 进程/ 在/ 运行/ 时/ 都/ 将/ 被/ 映射/ 成/ 一个/ 操作系统/ 进程/ ./ MPI/ 具有/ 直观/ 的/ 编程/ 模式/ 和/ 良好/ 的/ 可扩展性/ ,/ 因此/ 它/ 迅速/ 成为/ 高性能/ 计算/ 领域/ 并行程序/ 设计/ 的/ 事实标准/ ,/ 并/ 广泛/ 地/ 应用/ 于/ 共享内存/ 系统/ ,/ 如/ 大规模/ SMP/ 系统/ ./ 但/ 在/ 共享内存/ 系统/ 中/ ,/ 进程/ 级/ 并行/ 相对/ 于/ 线程/ 级/ 并行/ 的/ 交互/ 开销/ 往往/ 要/ 大/ 数倍/ ./ 传统/ 上/ 基于/ 进程/ 的/ MPI/ 难以/ 发挥/ 出/ 共享内存/ 系统/ 的/ 最佳/ 性能/ ,/ 于是/ ,/ 早期/ 也/ 有/ 多个/ 工作/ 关注/ 于/ 线程/ MPI/ [/ 2/ -/ 4/ ]/ ./ TOMPI/ [/ 3/ ]/ 和/ MPI/ -/ Lite/ [/ 4/ ]/ 研究/ 在/ 单台/ 共享内存/ 系统/ 中以/ 线程/ 为/ 单位/ 运行/ MPI/ 程序/ ./ TMPI/ [/ 2/ ]/ 是/ 公开/ 发布/ 的/ 第一个/ 可以/ 用于/ 共享/ 、/ 分布/ 内存/ 混合系统/ 上/ 的/ 线程/ MPI/ ./ 但/ 随着/ 2000/ 年/ 左右/ 大规模/ SMP/ 系统/ 淡出/ 市场/ ,/ 这/ 几项/ 线程/ MPI/ 的/ 工作/ 都/ 没有/ 继续下去/ ./ 近年来/ ,/ 采用/ 多核/ 处理器/ 的/ 集群/ 系统/ (/ 简称/ 多核/ 集群/ )/ 主导/ 了/ 高性能/ 计算/ 领域/ 的/ 主流/ 硬件平台/ ②/ ./ 随着/ 多核/ 处理器/ 的/ 发展/ ,/ 主流/ 集群/ 系统/ 各/ 节点/ 内/ 的/ 计算/ 核心/ 数目/ 不断/ 增长/ ,/ 节点/ 内/ 通信/ 性能/ 的/ 重要性/ 不断/ 增加/ ,/ 线程/ MPI/ 又/ 再次/ 被/ 关注/ ./ AzequiaMPI/ [/ 6/ ]/ 是/ 一项/ 目前/ 正在/ 进行/ 中/ 的/ 工作/ ,/ 它/ 是/ 西班牙/ “/ Ingenio2010/ ”/ 政府/ 计划/ ③/ 中/ 的/ 一个/ 子项目/ ,/ 其/ 目标/ 是/ 建立/ 一套/ 完全/ 支持/ MPI1/ ./ 3/ 标准/ 的/ 线程/ MPI/ 基础/ 软件/ ./ 不同于/ TMPI/ 和/ AzequiaMPI/ ,/ MPIActor/ 以/ 透明/ 中间件/ 的/ 方式/ 基于/ 传统/ MPI/ 基础/ 软件/ 提供/ 线程/ MPI/ 环境/ ,/ 从而/ 解/ 耦/ 了/ 线程/ MPI/ 与/ MPI/ 进程/ 内/ 多线程/ 之间/ 的/ 矛盾/ ./ 引入/ MPIActor/ 后/ ,/ 传统/ MPI/ 基础/ 软件/ 的/ 结构/ 得到/ 改进/ ,/ 节点/ 内/ 通信/ 和/ 节点/ 间通信/ 分别/ 由/ 低/ 耦合/ 的/ 独立/ 模块/ 完成/ ,/ 即/ :/ 传统/ MPI/ 库/ 完成/ 节点/ 间通信/ ;/ MPIActor/ 专注/ 于/ 实现/ 节点/ 内/ 通信/ ./ 3/ 工作/ 机制/ 如图/ 1/ 所示/ ,/ MPIActor/ 通过/ 编译/ 期/ 和/ 运行/ 期/ 的/ 相关/ 支持/ 使/ 传统/ 的/ 用户/ MPI/ 程序/ 在线/ 程/ MPI/ 环境/ 中/ 执行/ ,/ 期间/ 不/ 需要/ 用户/ 专门/ 为/ MPIActor/ 对/ 源代码/ 做/ 任何/ 手工/ 改动/ ./ 我们/ 在下文/ 将/ 基于/ MPI/ +/ MPIActor/ 的/ 应用程序/ 简称/ 为/ MPIActor/ 程序/ ,/ 相应/ 的/ 并行/ 作业/ 称为/ MPIActor/ 并行/ 作业/ ./ 本节/ 的/ 以下内容/ 首先/ 阐述/ MPIActor/ 程序/ 的/ 运行/ 期/ 机制/ ,/ 其次/ 介绍/ 将/ MPIActor/ 加入/ 传统/ MPI/ 程序/ 的/ 编译/ 期/ 机制/ ./ 图/ 1/ 用户/ MPI/ 程序/ 在/ 编译/ 期/ 被/ 转换/ 为/ 基于/ MPI/ +/ MPIActor/ 的/ 可执行程序/ ,/ 运行/ 期/ 在线/ 程/ MPI/ 环境/ 中/ 执行/ 3.1/ 运行/ 期/ 机制/ 传统/ MPI/ 程序/ 的/ 运行机制/ 如图/ 2/ (/ a/ )/ 所示/ ,/ 在/ 此/ 运行机制/ 下/ ,/ 每个/ MPI/ 逻辑/ 进程/ 在/ 运行/ 时/ 都/ 被/ 映射/ 为/ 一个/ 操作系统/ 进程/ ,/ 不论/ 节点/ 内/ 通信/ 还是/ 节点/ 间通信/ 都/ 通过/ MPI/ 运行/ 时库/ 完成/ 并/ 最终/ 通过/ 某种/ 进程/ 间通信/ 机制/ 实现/ ./ 图/ 2/ 传统/ MPI/ 程序/ 与/ MPIActor/ 程序/ 的/ 运行机制/ 比较/ ①/ ②/ ③/ Page4MPIActor/ 程序/ 的/ 运行机制/ 如图/ 2/ (/ b/ )/ 所示/ ,/ 在/ MPIActor/ 的/ 支持/ 下/ ,/ 同/ 节点/ 的/ MPI/ 逻辑/ 进程/ 被/ 映射/ 为/ 同一/ 进程/ (/ 称/ 容器/ 进程/ )/ 中/ 的/ 多个/ 线程/ ./ 这些/ 线程/ 由/ 容器/ 进程/ 的/ 主线/ 程/ 根据/ MArun/ 传入/ 的/ 参数/ 派生/ ,/ 其中/ MArun/ 是/ 用来/ 提交/ MPIActor/ 并行/ 作业/ 的/ 脚本/ 程序/ ,/ 它/ 将/ 用户/ 命令/ 参数/ 分解/ ,/ 一部分/ 传递/ 给/ MPI/ 作业/ 提交/ 器/ ,/ 另/ 一部分/ 传递/ 给/ 容器/ 进程/ ./ MPIActor/ 运行/ 时库/ 为/ MPI/ 逻辑/ 进程/ 构建/ 了/ 以/ 线程/ 为/ 单位/ 执行/ 的/ 基础/ 结构/ ,/ 它/ 位于/ 用户/ MPI/ 程序/ 和/ MPI/ 运行/ 时库/ 之间/ ,/ 处理/ 运行/ 时/ 用户/ MPI/ 程序/ 中/ 的/ MPI/ 调用/ ,/ 其中/ 节点/ 内/ 通信/ 由/ MPIActor/ 运行/ 时库/ 单独/ 完成/ ,/ 节点/ 间通信/ 由/ MPIActor/ 转发/ 到/ 底层/ 的/ MPI/ 库/ 完成/ ./ 我们/ 将/ 这种/ 由/ MPIActor/ 协同/ 标准/ MPI/ 库/ 处理/ MPI/ 通信/ 调用/ 的/ 方法/ 称为/ 通信/ 操作/ 聚合/ 技术/ ,/ 它/ 是/ MPIActor/ 成为/ 透明/ 中间件/ 的/ 关键技术/ 之一/ ,/ 将/ 在/ 本文/ 的/ 第/ 5/ 节中/ 详细/ 阐述/ ./ 3.2/ 编译/ 期/ 机制/ 为了/ 不/ 给/ 用户/ 增加/ 额外/ 的/ 工作量/ ,/ MPIActor/ 在/ 改变/ MPI/ 程序运行/ 方式/ 的/ 同时/ 保持/ 了/ 传统/ 的/ MPI/ 编程/ 模式/ ./ 但/ 在/ 传统/ 的/ MPI/ 编程/ 模式/ 中/ 存在/ 一个/ 潜在/ 假设/ :/ 每个/ MPI/ 逻辑/ 进程/ 都/ 有/ 独立/ 的/ 内存地址/ 空间/ ./ 传统/ 基于/ 进程/ 的/ MPI/ 恰好/ 自然/ 地/ 满足/ 这一/ 假设/ ,/ 因此/ 用户/ 在/ 编写/ MPI/ 程序/ 时/ 往往/ 习惯于/ 使用/ 全局变量/ 实现/ MPI/ 逻辑/ 进程/ 全局/ 范围/ 内/ 的/ 数据共享/ 或者/ 使用/ 静态/ 变量/ 实现/ 对/ 数据/ 的/ 静态/ 访问/ ./ 不幸/ 的/ 是/ 基于/ 线程/ 的/ MPI/ 无法/ 满足/ 这一/ 假设/ ,/ 若/ 不/ 改变/ 用户/ 的/ 编程/ 习惯/ ,/ 就/ 需要/ 对/ 线程/ MPI/ 提供/ 编译/ 期/ 支持/ ./ 在/ MPIActor/ 的/ 支撑/ 软件/ 中/ ,/ MAcc/ (/ MPIActorCCompiler/ )/ 为/ 基于/ MPIActor/ 的/ C语言/ 程序/ 提供/ 编译/ 期/ 支持/ ./ 它/ 的/ 工作/ 原理/ 如图/ 3/ 所示/ ,/ 由/ 用户/ MPI/ 程序/ 源代码/ 到/ MPIActor/ 可执行程序/ 需要/ 经过/ 3/ 个/ 阶段/ :/ 预/ 编译/ 、/ 编译/ 和/ 链接/ ./ 在/ 预/ 编译/ 阶段/ ,/ MAcc/ 将/ 用户/ MPI/ 程序代码/ 转换/ 为/ 线程/ 安全/ 的/ 程序代码/ ,/ 确保/ 用户/ MPI/ 程序/ 在/ 映射/ 为/ 同一/ 进程/ 中/ 多个/ 线程/ 执行/ 时/ 彼此/ 的/ 数据/ 空间/ 无/ 重叠/ ./ MAcc/ 主要/ 完成/ 3/ 项/ 工作/ :/ (/ 1/ )/ 将/ 程序/ 的/ 入口/ 函数/ main/ 改名/ 为/ mpi/ _/ main/ ;/ (/ 2/ )/ 将/ 全局变量/ 和/ 静态/ 变量/ 改为/ 线程/ 局部/ 存储/ (/ threadlocalstorage/ )/ [/ 7/ ]/ 的/ 变量/ ;/ (/ 3/ )/ 将/ 程序/ 中/ 调用/ 的/ 非/ 线程/ 安全/ 的/ 函数/ (/ 如/ getopt/ )/ 改为/ 调用/ MPIActor/ 运行/ 时库/ 中/ 相应/ 线程/ 安全/ 的/ 函数/ ./ 在/ 编译/ 和/ 链接/ 阶段/ ,/ MAcc/ 利用/ mpicc/ 完成/ 对预/ 编译/ 转换/ 后/ 代码/ 的/ 编译/ 和/ 链接/ 工作/ ./ 在/ 编译/ 阶段/ ,/ MAcc/ 指定/ mpi/ ./ h/ 的/ 路径/ 到/ MPIActor/ 提供/ 的/ mpi/ ./ h/ ./ 在/ 链接/ 阶段/ ,/ MAcc/ 添加/ MPIActor/ 的/ 运行/ 时库/ ./ 目前/ 我们/ 正在/ 设计/ 用于/ 编译/ Fortran/ 语言/ 和/ C++/ 语言/ 程序/ 的/ 编译器/ ,/ 其/ 工作/ 步骤/ 与/ MAcc/ 类似/ ,/ 但/ 需要/ 注意/ 的/ 是/ Fortran/ 语言/ 目前/ 还/ 不/ 支持/ 线程/ 局部/ 存储/ ,/ 支持/ Fortran/ 语言/ 程序/ 的/ 预/ 编译器/ 要/ 比/ 用于/ C/ 和/ C++/ 语言/ 的/ 预/ 编译器/ 复杂/ 得/ 多/ ./ 通过/ 编译/ 期/ 的/ 支持/ ,/ 用户/ 可不/ 对原/ 程序代码/ 作/ 任何/ 修改/ 将/ 基于/ 传统/ MPI/ 软件/ 的/ 程序代码/ 编译成/ 基于/ MPI/ +/ MPIActor/ 的/ 可执行程序/ ./ 4/ 通信/ 体系结构/ MPIActor/ 的/ 通信/ 体系结构/ 是/ 实现/ 通信/ 操作/ 聚合/ 和/ 节点/ 内/ 通信/ 的/ 基础/ ./ 如图/ 4/ 所示/ ,/ 在/ 此/ 通信/ 体系结构/ 中/ ,/ N/ 个/ MPI/ 逻辑/ 进程/ 间/ 通过/ N2/ 个/ 虚拟/ 通道/ 实现/ 点对点/ 通信/ ,/ 即/ 任意/ 一对/ MPI/ 逻辑/ 进程/ PA/ 和/ PB/ 对应/ 两个/ 独立/ 的/ 虚拟/ 通道/ CAB/ 和/ CBA/ ./ 若/ PA/ 和/ PB/ 属于/ 同一/ 节点/ ,/ 则/ CAB/ 和/ CBA/ 为/ 节点/ 内/ 通信/ 虚拟/ 通道/ (/ ON/ -/ hostVirtualChannel/ ,/ ONVC/ )/ ,/ 它/ 是/ MPIActor/ 实现/ 节点/ 内/ 通信/ 的/ 物理/ 基础/ 结构/ ,/ 节点/ 内/ 通信/ 操作/ 在/ 此基础/ 上/ 实施/ ./ 图/ 4/ 中/ 的/ 每个/ 灰色/ 圆形/ 都/ 表示/ 一个/ ONVC/ ,/ ONVC/ 是/ 单向/ 通信/ 通道/ ,/ 它/ 被/ 接收端/ MPI/ 逻辑/ 进程/ 创建/ 并/ 由/ 发送/ 端/ MPI/ 逻辑/ 进程/ 持有/ 引用/ 指针/ ,/ 即/ CAB/ 用于/ PA/ 发送/ 消息/ 到/ Page5PB/ ,/ CBA/ 用于/ 相反/ 方向/ 通信/ ./ 另外/ ,/ ONVC/ 内/ 包含/ 3/ 个/ 用于/ 实现/ 节点/ 内/ 点对点/ 通信/ 的/ 先入/ 先出/ (/ FIFO/ )/ 队列/ :/ 接收/ 请求/ 队列/ 、/ 发送/ 请求/ 队列/ 和/ anysource/ 接收/ 请求/ 队列/ ./ 节点/ 内/ 通信/ 的/ 细节/ 部分/ 将/ 在/ 本文/ 第/ 6/ 节中/ 详细/ 讨论/ ./ 若/ PA/ 和/ PB/ 属于/ 不同/ 节点/ ,/ 则/ CAB/ 和/ CBA/ 为/ 节点/ 间通信/ 虚拟/ 通道/ (/ OFf/ -/ hostVirtualChannel/ ,/ OFVC/ )/ ./ 与/ ONVC/ 不同/ ,/ OFVC/ 并/ 不/ 作为/ 节点/ 间通信/ 的/ 物理/ 基础/ 结构/ ,/ 仅/ 缓存/ 用于/ 转发/ 节点/ 间通信/ 所/ 需/ 的/ 一些/ 必要/ 信息/ ,/ 包括/ 远端/ 节点/ rank/ (/ RemoteNodeRank/ ,/ RNR/ )/ 、/ 发送/ 标记/ 基数/ (/ SendTagBase/ ,/ STB/ )/ 和/ 接收/ 标记/ 基数/ (/ ReceiveTagBase/ ,/ RTB/ )/ ./ 图/ 4/ 中/ 的/ 白色/ 圆形/ 表示/ OFVC/ ,/ OFVC/ 是/ 双向通信/ 通道/ ,/ 它/ 由/ 各个/ 进程/ 创建/ 并/ 独立/ 持有/ ,/ 即/ CAB/ 由/ PA/ 持有/ 并/ 缓存/ 与/ PB/ 通信/ 的/ 相关/ 信息/ ,/ CBA/ 由/ PB/ 持有/ 并/ 存放/ 与/ PA/ 通信/ 的/ 相关/ 信息/ ./ 5/ 通信/ 操作/ 聚合/ 技术/ 通信/ 操作/ 聚合/ 是/ MPIActor/ 整合/ 自身/ 运行/ 时库/ 与/ 底层/ 标准/ MPI/ 库/ 实现/ MPI/ 通信/ 语义/ 的/ 关键技术/ ./ 其/ 基本/ 过程/ 如图/ 5/ 所示/ ,/ 我们/ 将/ 每/ 一次/ MPI/ 通信接口/ 调用/ 作为/ 一次/ 通信/ 请求/ ,/ MPIActor/ 程序/ 发出/ 的/ 通信/ 请求/ 首先/ 经由/ “/ 通信/ 请求/ 分离/ ”/ 过程/ 判断/ 通信/ 属于/ 何种/ 类型/ ,/ 再/ 根据/ 通信/ 的/ 类型/ 将/ 请求/ 递交/ 给/ 相应/ 的/ 聚合/ 过程/ 处理/ ./ 本节/ 将/ 详细/ 讨论/ 这一/ 关键技术/ ./ 5.1/ 通信/ 请求/ 分离/ 在/ 多/ 核/ 集群/ 系统/ 上/ ,/ MPI/ 消息传递/ 通信/ 究竟/ 发生/ 在/ 节点/ 间/ 还是/ 在/ 节点/ 内/ 是/ 一个/ 运行/ 时/ 特征/ ,/ 我们/ 称此/ 特征/ 为/ 通信/ 位置/ 特征/ ,/ MPI/ 标准/ 中/ 定义/ 的/ 通信接口/ 语义/ 并/ 不/ 区分/ 类似/ 的/ 运行/ 时/ 特征/ ./ 根据/ MPI/ -/ Actor/ 的/ 运行机制/ ,/ MPI/ 通信/ 请求/ 须/ 根据/ 其/ 通信/ 位置/ 特征/ 由/ 不同/ 的/ 运行/ 时库/ 处理/ ,/ 因此/ MPIActor/ 在/ 重载/ MPI/ 接口/ 时/ 需要/ 在/ 过程/ 中/ 辨别/ 通信/ 的/ 类型/ ,/ 我们/ 称/ 这/ 一/ 过程/ 为/ 通信/ 请求/ 分离/ ,/ 简称/ CRS/ (/ Commu/ -/ nicationRequestSeparating/ )/ ,/ 它/ 是/ 通信/ 操作/ 聚合/ 过程/ 中/ 的/ 第一个/ 关键步骤/ ./ 从/ CRS/ 的/ 输入/ 角度看/ 可/ 将/ MPI/ 通信/ 请求/ 分为/ 两类/ ,/ 具体/ 分类/ 如表/ 1/ 所示/ ./ 阻塞/ 通信接口/ 非/ 阻塞/ 通信接口/ 第/ 1/ 类/ 通信/ 请求/ 须/ 根据/ 接口/ 参数/ 进行/ CRS/ ./ MPIActor/ 通过/ 虚拟/ 通道/ 通信/ 体系结构/ 建立/ 了/ MPI/ 逻辑/ 进程/ 间/ 的/ 通信/ 关系/ ,/ 在/ 此基础/ 上/ 第/ 1/ 类/ 通信/ 请求/ 通过/ 输入/ 参数/ 获取/ 虚拟/ 通道/ ,/ 并/ 根据/ 通道/ 的/ 类型/ 判断/ 通信/ 请求/ 的/ 通信/ 位置/ 特征/ ./ 另一类/ 通信/ 请求/ 根据/ 通信/ 请求/ 对象/ 中/ 的/ 通信/ 位置/ 特征/ 进行/ 通信/ 请求/ 分离/ ,/ 其中/ 这些/ 通信/ 请求/ 对象/ 由/ 第/ 1/ 类/ 通信/ 请求/ 的/ 非/ 阻塞/ 通信/ 过程/ 返回/ ./ 经由/ CRS/ 过程/ ,/ MPI/ 通信/ 请求/ 将会/ 被/ 传递/ 给/ MPIActor/ 的/ 通信/ 过程/ 、/ 标准/ MPI/ 库/ 的/ 通信/ 过程/ 或/ MPI/ _/ ANY/ _/ SOURCE/ 处理过程/ 作/ 进一步/ 处理/ ./ 5.2/ 节点/ 间通信/ 请求/ 的/ 转发/ 方法/ 设/ r/ 为/ 输入/ MPIActor/ 的/ 通信/ 请求/ ,/ 若/ r/ 对应/ 的/ 通信/ 发生/ 在/ 节点/ 间/ ,/ 则/ r/ 会/ 被/ MPIActor/ 转发/ 到/ 底层/ 的/ 标准/ MPI/ 库/ 处理/ ,/ 在/ 此/ 过程/ 中/ MPI/ 库/ 需要/ 知道/ 与/ 对方/ 节点/ 容器/ 进程/ 中/ 的/ 哪个/ 线程/ 进行/ 消息/ 交互/ ,/ 这是/ 节点/ 间通信/ 请求/ 转发/ 需要/ 解决/ 的/ 问题/ ./ 本/ 小节/ 将/ 以/ 转发/ 节点/ 间/ MPI/ _/ Irecv/ 请求/ 为例/ 详细/ 讨论/ 节点/ 间通信/ 请求/ 转发/ 的/ 方法/ 及/ 规则/ ./ 我们/ 有/ 以下/ 定义/ ./ 定义/ 1/ (/ 通信/ 请求/ 对象/ )/ ./ 通信/ 请求/ 对象/ 是/ 一个/ 三元组/ CRO/ =/ 〈/ S/ ,/ D/ ,/ T/ 〉/ ,/ 其中/ :/ S/ =/ {/ i/ |/ i/ ∈/ N/ ,/ 0/ / i/ </ Pmax/ }/ ∪/ {/ MPI/ _/ ANY/ _/ SOURCE/ }/ 表示/ 通信/ 源/ MPI/ 进程/ ,/ 其中/ Pmax/ 为/ MPI/ 辑/ 进程/ 个数/ ./ D/ =/ {/ i/ |/ i/ ∈/ N/ ,/ 0/ / i/ </ Pmax/ }/ 表示/ 通信/ 目的/ MPIT/ =/ {/ t/ |/ t/ ∈/ N/ }/ ∪/ {/ MPI/ _/ ANY/ _/ TAG/ }/ 表示/ 通信/ 进程/ ./ 标记/ ./ Page6/ 定义/ 2/ (/ 接收/ 请求/ )/ ./ 接收/ 请求/ 是/ 一个二元/ 组/ RR/ =/ 〈/ S/ ,/ T/ 〉/ ,/ 表示/ 从源/ MPI/ 进程/ 接收/ 某/ 标记/ 的/ 数据/ ,/ 其中/ S/ 和/ T/ 的/ 定义/ 同/ 定义/ 1/ ./ 定义/ 3/ (/ MPI/ 进程/ 的/ 节点/ 位置/ )/ ./ 称/ MPI/ 进程/ 所在/ 的/ 节点/ 为/ 节点/ 位置/ :/ 其中/ Proc/ ,/ Node/ / N/ ,/ npos/ (/ p/ )/ 能够/ 得到/ MPI/ 进程/ p/ 的/ 节点/ 位置/ ./ 定义/ 4/ (/ 非/ 阻塞/ 接收/ )/ ./ 非/ 阻塞/ 接收/ 以非/ 阻塞/ 方式/ 处理/ 接收/ 请求/ 并/ 返回/ 一个/ 通信/ 请求/ 对象/ :/ MPI/ _/ Irecv/ (/ p/ ,/ 〈/ s/ ,/ t/ 〉/ )/ =/ 〈/ p/ ,/ s/ ,/ t/ 〉/ ,/ 其中/ p/ 为/ 当前/ MPI/ 进程/ 的/ 进程/ 序列号/ (/ rank/ )/ ./ 对非/ 阻塞/ 接收/ MPI/ _/ Irecv/ (/ p/ ,/ 〈/ s/ ,/ t/ 〉/ )/ ,/ 在/ s/ 不/ 等于/ MPI/ _/ ANY/ _/ SOURCE/ 的/ 情况/ 下/ ,/ 如果/ npos/ (/ p/ )/ 不/ 等于/ npos/ (/ s/ )/ ,/ 则/ 〈/ s/ ,/ t/ 〉/ 需要/ 被/ 转换/ 为/ 〈/ s/ ,/ t/ 〉/ 并/ 通过/ 底层/ MPI/ 库中/ 的/ MPI/ _/ Irecv/ (/ p/ ,/ 〈/ s/ ,/ t/ 〉/ )/ 操作/ 完成/ 通信/ ,/ 其中/ :/ 当/ t/ 不/ 等于/ MPI/ _/ ANY/ _/ TAG/ 时/ :/ t/ =/ L/ (/ s/ )/ / offsets/ +/ L/ (/ p/ )/ / offsetr/ +/ t/ ,/ Cps/ ./ rtb/ =/ s/ / offsets/ +/ p/ / offsetr/ +/ t/ ,/ 其中/ offsets/ 和/ offsetr/ 分别/ 表示/ 发送/ 位置/ 偏移/ 和/ 接收/ 位置/ 偏移/ ,/ L/ (/ x/ )/ 表示/ MPI/ 进程/ x/ 的/ 本地/ 进程/ 号/ ./ 可知/ MPIActor/ 程序/ 进行/ 节点/ 通信/ 时源/ 进程/ 和/ 目的/ 进程/ 通过/ 底层/ MPI/ 接口/ 中/ 的/ 通信/ 标记/ 参数/ 识别/ ./ 另外/ ,/ 若/ Cps/ 为/ p/ 到/ s/ 的/ 通信/ 虚拟/ 通道/ ,/ 则/ 有/ 其中/ Cps/ / rtb/ 为/ Cps/ 中/ 的/ 接收/ 标记/ 基数/ ./ 在/ 实现/ 节点/ 间通信/ 请求/ 转发/ 时/ 可/ 利用/ 此/ 基数/ 转换/ 通信/ 标记/ ./ 当/ t/ =/ MPI/ _/ ANY/ _/ TAG/ 时/ ,/ t/ =/ t/ ,/ 此/ 类型/ 的/ 节点/ 间通信/ 将/ 通过/ MPIActor/ 内置/ 的/ 特殊/ 过程/ 处理/ ./ s/ =/ MPI/ _/ ANY/ _/ SOURCE/ 情况/ 下/ 的/ 通信/ 请求/ 处理/ 方法/ 将/ 在/ 5.3/ 节/ 讨论/ ./ 5.3/ 对/ MPI/ _/ ANY/ _/ SOURCE/ 类型/ 请求/ 的/ 处理/ 方法/ MPI/ _/ ANY/ _/ SOURCE/ 可/ 作为/ MPI/ 接收/ (/ Recv/ )/ 或/ 检查/ (/ Probe/ )/ 接口/ 中/ “/ 数据源/ 进程/ 号/ ”/ 的/ 参数值/ ,/ 表示/ 任意/ 数据源/ ./ 当/ MPI/ 程序/ 调用/ 接收/ 或/ 检查/ 操作/ 时/ 的/ “/ 数据源/ 进程/ 号/ ”/ 参数值/ 为/ MPI/ _/ ANY/ _/ SOURCE/ 时/ ,/ 算法/ 无法/ 通过/ 输入/ 参数/ 的/ 静态/ 值/ 确定/ 其/ 发出/ 的/ 通信/ 请求/ 对应/ 节点/ 间通信/ 还是/ 节点/ 内/ 通信/ ,/ 我们/ 称/ 此类/ 通信/ 请求/ 为/ MPI/ _/ ANY/ _/ SOURCE/ 类型/ 的/ 请求/ ,/ 或/ 简称/ ASR/ (/ AnySourceRequests/ )/ 类型/ 的/ 请求/ ./ ASR/ 通过/ 专门/ 的/ 处理过程/ 协同/ MPIActor/ 通信/ 库/ 和/ MPI/ 通信/ 库中/ 的/ 通信/ 操作/ 完成/ ,/ 涉及/ 此类/ 通信/ 的/ MPI/ 接口/ 有/ MPI/ _/ Recv/ 、/ MPI/ _/ Irecv/ 、/ MPI/ _/ Iprobe/ 、/ MPI/ _/ Probe/ 和/ 所有/ 第/ 2/ 类/ 通信/ 请求/ 接口/ ./ 本/ 小节/ 的/ 以下内容/ 将/ 以/ MPI/ _/ Iprobe/ 为例/ 讨论/ 针对/ ASR/ 的/ 处理/ 方法/ ./ MPI/ _/ Iprobe/ 操作/ 的/ 定义/ 为/ MPI/ _/ Iprobe/ (/ source/ ,/ tag/ ,/ comm/ ,/ flag/ ,/ status/ )/ ,/ 其/ 语义/ 是/ 在/ 不/ 实际/ 接收/ 输入/ 消息/ 的/ 情况/ 下/ 检查/ 是否/ 收到/ comm/ 通信/ 域/ 中源/ 为/ source/ 标记/ 为/ tag/ 的/ 消息/ ./ 当/ 调用/ MPI/ _/ Iprobe/ “/ 数据源/ 进程/ 号/ ”/ 参数/ 的/ 值/ 为/ MPI/ _/ ANY/ _/ SOURCE/ 时/ ,/ 通信/ 请求/ 由/ CRS/ 传给/ ASR/ _/ MPI/ _/ Iprobe/ 处理/ ,/ 其/ 定义/ 和/ 算法/ 如图/ 6/ 所示/ ./ ASR/ _/ MPI/ _/ Iprobe/ 算法/ 通过/ MPI/ _/ Iprobe/ 检查/ 并/ 获取/ 发送给/ 所属/ 容器/ 进程/ 的/ 节点/ 间通信/ 消息/ ,/ 若/ 收到/ 则/ 根据/ 获取/ 到/ 的/ 信息/ 检查/ 消息/ 是否/ 为/ 发送给/ 本/ MPI/ 逻辑/ 进程/ 的/ 消息/ ;/ 另一方面/ ,/ ASR/ _/ MPI/ _/ Iprobe/ 通过/ 检查/ 虚拟/ 通道/ 的/ 发送/ 队列/ 检查/ 是否/ 有/ 节点/ 内/ 通信/ ./ 算法/ ./ ASR/ _/ MPI/ _/ Iprobe/ ./ 输入/ :/ tag/ ,/ comm/ ;/ 输出/ :/ flag/ ,/ statusBEGIN/ // / 检查/ 是否/ 收到/ 节点/ 间/ 消息/ / // MPI/ _/ Iprobe/ (/ MPI/ _/ ANY/ _/ SOURCE/ ,/ MPI/ _/ ANY/ _/ TAG/ ,/ // / 检查/ 所/ 收到/ 节点/ 间/ 消息/ 的/ 目的地/ 是否/ 为本/ 进程/ / // IF/ (/ tflag/ =/ =/ 1/ )/ andismymsg/ (/ ts/ ,/ tag/ )/ flag/ =/ 1/ ;/ 将/ ts/ 转换/ 到/ status/ ;/ return/ ;/ ELSE/ 检查/ 虚拟/ 通道/ 发送/ 队列/ 中/ 是否/ 已/ 收到/ 匹配/ 的/ 节点/ 内/ 消息/ ENDEND/ 并/ 返回/ 结果/ ;/ 除/ ASRMPI/ _/ Irecv/ 外/ ,/ 其余/ ASR/ 通信/ 请求/ 对应/ 的/ 处理/ 方法/ 在/ 基本原理/ 上/ 都/ 与/ ASR/ _/ MPI/ _/ Iprobe/ 算法/ 相似/ ./ ASRMPI/ _/ Irecv/ 较为/ 特殊/ 的/ 原因/ 是/ 它/ 的/ 非/ 阻塞/ 数据/ 接收/ 机制/ ,/ 如何/ 协同/ MPIActor/ 通信/ 库/ 和/ MPI/ 通信/ 库/ 有效/ 实现/ 这一/ 机制/ 是/ 一个/ 难点/ ./ ASRMPI/ _/ Irecv/ 的/ 基本原理/ 是/ 若/ 通过/ MPI/ _/ Iprobe/ 未/ 发现/ 匹配/ 的/ 请求/ 则/ 同时/ 启动/ 节点/ 内/ MPI/ _/ Irecv/ 通信/ 和/ 节点/ 间/ MPI/ _/ Irecv/ 通信/ ,/ 然后/ 在/ 接收/ 到/ 数据/ 后/ 再/ 对/ 其中/ 一个/ 选择性/ 取消/ ./ 为此/ 我们/ 在/ MPIActor/ 的/ 通信/ 体系结构/ 中为/ 每/ 一个/ MPI/ 逻辑/ 进程/ 增加/ 了/ 一个/ any/ -/ source/ 请求/ 队列/ ,/ 并/ 将/ 其/ 连入/ 虚拟/ 通道/ ./ ASRMPI/ _/ Irecv/ 的/ 实现/ 方法/ 我们/ 将/ 在/ 后续/ 工作/ 中/ 详细/ 介绍/ ./ 6/ 基于/ 单次/ 内存/ 拷贝/ 的/ 节点/ 内/ 点对点/ 通信/ 基于/ 单次/ 内存/ 拷贝/ 的/ 节点/ 内/ 点对点/ 通信/ 是/ Page7MPIActor/ 提高/ 通信/ 性能/ 最/ 重要/ 的/ 技术手段/ 之一/ ./ 在/ MPIActor/ 程序/ 的/ 运行机制/ 下/ ,/ 若/ 点对点/ 通信/ 发生/ 在/ 节点/ 内则源/ MPI/ 进程/ 中/ 的/ 发送缓冲区/ 和/ 目的/ MPI/ 进程/ 中/ 的/ 接收缓冲区/ 都/ 属于/ 同一/ 进程/ 地址/ 空间/ ,/ 进而/ 节点/ 内/ 点对点/ 通信/ 可/ 自然/ 地/ 通过/ 单次/ 内存/ 拷贝/ 实现/ ./ 本节/ 的/ 以下内容/ 主要/ 讨论/ MPIActor/ 中/ 基于/ 单次/ 内存/ 拷贝/ 的/ 点对点/ 通信/ 算法/ ./ 另外/ ,/ 为了/ 更/ 清晰/ 地/ 说明/ MPIActor/ 节点/ 内/ 通信/ 的/ 性能/ 优势/ ,/ 我们/ 将/ 在/ 最后/ 一个/ 小节/ 详细/ 介绍/ 基于/ 核心/ 态/ 内存/ 映射/ 的/ 单次/ 内存/ 拷贝/ 机制/ 并/ 与/ 之/ 进行/ 比较/ ./ 6.1/ 节点/ 内/ 单次/ 拷贝/ 点对点/ 通信/ 算法/ MPIActor/ 中/ 的/ 节点/ 内/ 点对点/ 通信/ 算法/ 如图/ 7/ 所示/ ,/ 包括/ 相互配合/ 的/ 发送/ 算法/ 和/ 接收/ 算法/ ./ 接收/ 和/ 发送/ 算法/ 的/ 基本/ 结构/ 类似/ ,/ 均/ 包含/ 两个/ 步骤/ :/ 第/ 1/ 个/ 算法/ ./ Intra/ _/ MPI/ _/ Send/ ./ 输入/ :/ buf/ ,/ bsize/ ,/ dest/ ,/ tag/ ,/ comm/ 输出/ :/ bufBEGIN/ // / 步骤/ 1/ / // 通过/ 本/ 进程/ 号/ (/ rank/ )/ 和/ dest/ 从/ comm/ 对应/ 的/ 通信/ 域/ 对象/ 中/ 得到/ 通信/ 虚拟/ 通道/ CVC/ ;/ mutex/ _/ lock/ (/ CVC/ ./ mutex/ )/ ;/ // / 加锁/ / // 狉/ 犲/ 狇/ =/ 犆/ 犞/ 犆/ ./ 狉/ 犲/ 犮/ 狏/ 狉/ 犲/ 狇/ _/ 狇/ 狌/ 犲/ 狌/ 犲/ ./ 犺/ 犲/ 犪/ 犱/ ;/ IFreq/ !/ =/ NULLWHILE/ (/ req/ !/ =/ NULLreq/ =/ req/ -/ >/ next/ ;/ END/ // / 步骤/ 2/ / // IFreq/ !/ =/ NULL/ 犱/ 犲/ 狇/ 狌/ 犲/ 狌/ 犲/ (/ 犆/ 犞/ 犆/ ./ 狉/ 犲/ 犮/ 狏/ 狉/ 犲/ 狇/ _/ 狇/ 狌/ 犲/ 狌/ 犲/ ,/ 狉/ 犲/ 狇/ )/ ;/ // / 将/ req/ 出列/ / // 犿/ 犲/ 犿/ 犮/ 狆/ 狔/ (/ 狉/ 犲/ 狇/ -/ >/ 狉/ 犲/ 犮/ 狏/ 犫/ 狌/ 犳/ ,/ 犫/ 狌/ 犳/ ,/ 狉/ 犲/ 狇/ -/ >/ 犫/ 狊/ 犻/ 狕/ 犲/ )/ ;/ ELSE/ 申请/ 一个/ 新/ 的/ 通信/ 请求/ 对象/ req/ ,/ 并/ 初始化/ ;/ 狉/ 犲/ 狇/ -/ >/ 狊/ 犲/ 狀/ 犱/ 犫/ 狌/ 犳/ =/ 犫/ 狌/ 犳/ ;/ 犲/ 狀/ 狇/ 狌/ 犲/ 狌/ 犲/ (/ 犆/ 犞/ 犆/ ./ 狊/ 犲/ 狀/ 犱/ 狉/ 犲/ 狇/ _/ 狇/ 狌/ 犲/ 狌/ 犲/ ,/ 狉/ 犲/ 狇/ )/ ;/ // / 将/ req/ 入列/ / // ENDmutex/ _/ unlock/ (/ CVC/ ./ mutex/ )/ ;/ // / 解锁/ / // END/ 图/ 7/ 节点/ 内/ 点对点/ 通信/ 算法/ 6.2/ 进程/ 间/ 单次/ 内存/ 拷贝/ 的/ 点对点/ 通信/ 传统/ 的/ MPI/ 软件/ 通常/ 基于/ 用户/ 态/ 共享内存/ 实现/ 节点/ 内/ 点对点/ 通信/ ,/ 通信/ 的/ 基本原理/ 是/ 将源/ MPI/ 进程/ 中/ 的/ 消息/ 拷贝到/ 共享内存/ 的/ 缓冲区/ ,/ 再/ 从/ 共享内存/ 缓冲区/ 拷贝到/ 目的/ MPI/ 进程/ 中/ 的/ 接收缓冲区/ ,/ 整个/ 通信/ 过程/ 需要/ 通过/ 两次/ 内存/ 拷贝/ ./ 近年来/ 的/ 一些/ 工作/ (/ 如/ MVAPICH2/ 中/ 的/ LiMiC/ [/ 8/ ]/ 和/ OpenMPI/ 中/ 的/ KNEM/ [/ 9/ ]/ )/ 为/ 传统/ 的/ 进程/ MPI/ 提出/ 了/ 一种/ 新/ 的/ 节点/ 内/ 点对点/ 通信/ 机制/ :/ 基于/ 核心/ 态/ 内存/ 映射/ 的/ 通信/ 机制/ (/ Kernel/ -/ BasedMemoryMap/ -/ pingCommunicationMechanism/ ,/ KBMMCM/ )/ ,/ 基/ 步骤/ 进行/ 通信/ 请求/ 匹配/ ,/ 第/ 2/ 个/ 步骤/ 根据/ 第/ 1/ 个/ 步骤/ 的/ 运行/ 时/ 结果/ 选择/ 创建/ 或/ 处理/ 通信/ 请求/ ./ 发送/ 算法/ 的/ 第/ 1/ 个/ 步骤/ 遍历/ 虚拟/ 通道/ 的/ 接收/ 请求/ 队列/ 并/ 获取/ 第/ 1/ 个/ 匹配/ 的/ 通信/ 请求/ 对象/ req/ ,/ 此/ 过程/ 的/ 时间/ 复杂度/ 为/ O/ (/ n/ )/ ./ 算法/ 第/ 2/ 个/ 步骤/ 的/ 两个/ 分支/ 分别/ 对应/ 成功/ 获取/ 匹配/ 的/ 通信/ 请求/ 对象/ 和/ 未/ 获取/ 通信/ 请求/ 对象/ 的/ 情况/ ,/ 如果/ 成功/ 获取/ 匹配/ 的/ 通信/ 请求/ 对象/ req/ 则/ 将/ req/ 从/ 接收/ 请求/ 对列/ 中/ 移出/ 并/ 根据/ 请求/ 中/ 的/ 接收/ 缓存/ 地址/ 和/ 大小/ 完成/ 数据/ 拷贝/ ;/ 如果/ 未/ 获取/ 匹配/ 的/ 通信/ 请求/ 对象/ 则/ 申请/ 一个/ 新/ 的/ 通信/ 请求/ 对象/ 并/ 填充/ 本次/ 发送/ 请求/ 的/ 信息/ ,/ 之后/ 加入/ 发送/ 请求/ 队列/ ./ 两个/ 步骤/ 都/ 在/ 虚拟/ 通道/ 访问/ 锁/ 的/ 保护/ 下/ 进行/ ./ 接收/ 算法/ 与/ 发送/ 算法/ 的/ 结构/ 相同/ ,/ 具体/ 过程/ 对偶/ ./ 图/ 7/ 用/ 黑体/ 标记/ 了/ 两种/ 算法/ 的/ 区别/ ./ 算法/ ./ Intra/ _/ MPI/ _/ Recv/ ./ 输入/ :/ count/ ,/ src/ ,/ tag/ ,/ comm/ 输出/ :/ bufBEGIN/ // / 步骤/ 1/ / // 通过/ 本/ 进程/ 号/ (/ rank/ )/ 和/ src/ 从/ comm/ 对应/ 的/ 通信/ 域/ 对象/ 中/ 得到/ 通信/ 虚拟/ 通道/ CVC/ ;/ mutex/ _/ lock/ (/ CVC/ ./ mutex/ )/ ;/ // / 加锁/ / // 狉/ 犲/ 狇/ =/ 犆/ 犞/ 犆/ ./ 狊/ 犲/ 狀/ 犱/ 狉/ 犲/ 狇/ _/ 狇/ 狌/ 犲/ 狌/ 犲/ ./ 犺/ 犲/ 犪/ 犱/ ;/ IFreq/ !/ =/ NULLWHILE/ (/ req/ !/ =/ NULLreq/ =/ req/ -/ >/ next/ ;/ END/ // / 步骤/ 2/ / // IFreq/ !/ =/ NULL/ 犱/ 犲/ 狇/ 狌/ 犲/ 狌/ 犲/ (/ 犆/ 犞/ 犆/ ./ 狊/ 犲/ 狀/ 犱/ 狉/ 犲/ 狇/ _/ 狇/ 狌/ 犲/ 狌/ 犲/ ,/ 狉/ 犲/ 狇/ )/ ;/ // / 将/ req/ 出列/ / // 犿/ 犲/ 犿/ 犮/ 狆/ 狔/ (/ 犫/ 狌/ 犳/ ,/ 狉/ 犲/ 狇/ -/ >/ 狊/ 犲/ 狀/ 犱/ 犫/ 狌/ 犳/ ,/ 犫/ 狊/ 犻/ 狕/ 犲/ )/ ;/ ELSE/ 申请/ 一个/ 新/ 的/ 通信/ 请求/ 对象/ req/ ,/ 并/ 初始化/ ;/ 狉/ 犲/ 狇/ -/ >/ 狉/ 犲/ 犮/ 狏/ 犫/ 狌/ 犳/ =/ 犫/ 狌/ 犳/ ;/ 犲/ 狀/ 狇/ 狌/ 犲/ 狌/ 犲/ (/ 犆/ 犞/ 犆/ ./ 狉/ 犲/ 犮/ 狏/ 狉/ 犲/ 狇/ _/ 狇/ 狌/ 犲/ 狌/ 犲/ ,/ 狉/ 犲/ 狇/ )/ ;/ // / 将/ req/ 入列/ / // ENDmutex/ _/ unlock/ (/ CVC/ ./ mutex/ )/ ;/ // / 解锁/ / // END/ 于/ 该/ 通信/ 机制/ 的/ 通信/ 过程/ 仅/ 需/ 单次/ 内存/ 拷贝/ ./ KBMMCM/ 的/ 基本原理/ 如图/ 8/ (/ a/ )/ 所示/ ./ KBMMCM/ 需要/ 在/ 系统/ 中/ 建立/ 一个/ 内核模块/ ,/ 此/ 模块/ 可/ 通过/ 相应/ 的/ KBMM/ 驱动/ 进行/ 使用/ ./ 基于/ KBMMCM/ 的/ 点对点/ 通信/ 过程/ 主要/ 通过/ 6/ 步/ 完成/ :/ 1/ ./ 发送/ 端/ 将/ 发送缓冲区/ 的/ 起始/ 地址/ buff/ 和/ 缓冲区/ 大小/ size/ 发送给/ KBMM/ 驱动/ ./ 2/ ./ KBMM/ 接收/ 到/ buff/ 和/ size/ 后/ 将/ 发送/ 进程/ 的/ 进程/ 指针/ (/ current/ )/ 、/ 内存/ 指针/ (/ current/ -/ >/ mm/ )/ 、/ buff/ 占用/ 的/ 页/ 大小/ 等/ 信息/ 存在/ 一个/ 数据结构/ kbmm/ _/ s/ 中/ 返回/ 给/ 发送/ 进程/ ./ Page8/ 送给/ 接收端/ ./ 4/ ./ 接收端/ 将/ kbmm/ _/ s/ 发送给/ KBMM/ 驱动/ ,/ KBMM/ 驱动/ 根据/ kbmm/ _/ s/ 得到/ 发送缓冲区/ 的/ 页/ 列表/ pages/ _/ list/ ./ 5/ ./ KBMM/ 驱动/ 通过/ kmap/ 将/ pages/ _/ list/ 映射/ 到/ 一个/ 核/ 3/ ./ 发送/ 进程/ 将/ kbmm/ _/ s/ 打包/ 并/ 通过/ MPI/ 点对点/ 通信/ 发/ 心态/ 内存地址/ kbuff/ ./ 6/ ./ 将/ kbuff/ 拷贝到/ 接收缓冲区/ ./ 此/ 过程/ 只/ 需/ 一次/ 内存/ 拷贝/ ,/ 但/ 同时/ 会/ 产生/ 较大/ 的/ 处理/ 开销/ ,/ 在/ 某些/ 情况/ 下/ 处理/ 开销/ 甚至/ 会/ 超过/ 减少/ 一次/ 内存/ 拷贝/ 带来/ 的/ 收益/ ,/ 造成/ 性能/ 下降/ ,/ 因此/ 基于/ 核心/ 态/ 内存/ 映射/ 是/ 一种/ 重量级/ 的/ 单次/ 内存/ 拷贝/ 机制/ ./ 与/ 之/ 相比/ MPIActor/ 中/ 的/ 单次/ 内存/ 拷贝/ 机制/ 不/ 需要/ 进行/ 繁琐/ 的/ 内存/ 转换/ 过程/ ,/ 是/ 一种/ 轻量级/ 的/ 通信/ 机制/ ./ 7/ 实验/ 与/ 结果/ 7.1/ 实验/ 环境/ 与/ 方法/ 我们/ 的/ 实验/ 环境/ 是/ 一个/ 128/ 个/ 节点/ 的/ 双路/ 四核/ Nehalem/ 集群/ (/ 本文/ 实验/ 只/ 需要/ 其中/ 的/ 两个/ 节点/ )/ ,/ 每个/ 节点/ 都/ 包含/ 两颗/ XeonE5540/ 处理器/ 和/ 32GB/ 内存/ ,/ 测试/ 时/ 处理器/ 主频/ 为/ 2.53/ GHz/ ,/ 外频/ 为/ 1066MHz/ ./ 操作系统/ 采用/ RedhatLinuxEnterprise4/ ./ 我们/ 实验/ 的/ 性能/ 测试工具/ 采用/ OSUBench/ -/ mark/ 中/ 的/ OSU/ _/ LATENCY/ ,/ 其/ 内部/ 采用/ ping/ -/ pong/ 测试方法/ ,/ 是/ 一种/ 用于/ 测试/ MPI/ 点对点/ 通信/ 性能/ 的/ 通用/ 工具/ ./ 通过/ OSU/ _/ LATENCY/ ,/ 我们/ 的/ 实验/ 分为/ 两个/ 部分/ :/ 第/ 1/ 部分/ 的/ 实验/ 用于/ 评测/ 在/ MVAPICH2/ 基础/ 上/ MPIActor/ 节点/ 内/ 通信/ 的/ 性能/ ,/ 对比/ 以下/ MPI/ 通信/ 配置/ 的/ 性能/ :/ (/ 1/ )/ 基于/ 用户/ 级/ 共享内存/ 的/ MVAPICH21/ ./ 4/ ;/ (/ 2/ )/ 基于/ LiMiC/ 的/ MVAPICH21/ ./ 4/ ;/ (/ 3/ )/ MVAPICH21/ ./ 4/ +/ MPIActor/ ./ 另外/ ,/ 在/ 这/ 一部分/ 实验/ 中/ 我们/ 还/ 将/ 对比/ 节点/ 间通信/ 的/ 性能/ ,/ 目的/ 是/ 为了/ 测试/ 经/ MPIActor/ 转发/ 通信/ 后/ 性能/ 是否/ 会/ 受到/ 显著/ 影响/ ./ 第/ 2/ 部分/ 的/ 实验/ 用于/ 评测/ 在/ OPENMPI/ 基础/ 上/ MPIActor/ 节点/ 内/ 通信/ 的/ 性能/ ,/ 对比/ 以下/ MPI/ 通信/ 配置/ 的/ 性能/ :/ (/ 1/ )/ 基于/ 用户/ 级/ 共享内存/ 的/ OPENMPI1/ ./ 5a1/ ;/ (/ 2/ )/ 基于/ KNEM/ 的/ OPENMPI1/ ./ 5a1/ ;/ (/ 3/ )/ OPENMPI1/ ./ 5a1/ +/ MPIActor/ ./ 对/ 以上/ 每/ 一种/ 配置/ 我们/ 都/ 分别/ 测试/ 了/ 处理器/ 内/ 通信/ 和/ 处理器/ 间通信/ ./ 7.2/ 实验/ 结果/ 与/ 分析/ 将/ 两/ 部分/ 实验/ 结果/ 综合/ ,/ 可以/ 得出/ 如图/ 9/ ~/ 图/ 11/ 的/ 实验/ 结果/ ./ 可以/ 发现/ ,/ MPIActor/ 有效/ 的/ 提高/ 了/ MVAPICH2/ 和/ OpenMPI/ 的/ 节点/ 内/ 点对点/ 通信/ 的/ 性能/ ,/ 同时/ 对/ 节点/ 间通信/ 性能/ 几乎/ 不/ 造成/ 影响/ ./ 处理器/ 内/ 的/ 通信/ 性能/ 评测/ 结果/ 如图/ 9/ 所示/ ,/ 相对/ 于/ 基于/ 用户/ 级/ 共享内存/ 的/ MVAPICH2/ ,/ MPIAc/ -/ tor/ 在/ 传输/ 大小/ 为/ 8KB/ ~/ 256KB/ 的/ 消息/ 时/ 通信/ 性能/ 提高/ 了/ 37/ %/ ~/ 114/ %/ ;/ 在/ 传输/ 256KB/ ~/ 2MB/ 的/ 消息/ 时/ 性能/ 提高/ 了/ 68/ %/ ~/ 111/ %/ ;/ 在/ 传输/ 4MB/ ~/ 8MB/ 的/ 消息/ 时/ 性能/ 提高/ 了/ 60/ %/ ~/ 12/ %/ ./ 相对/ 于/ OpenMPI/ ,/ MPIActor/ 在/ 传输/ 大小/ 为/ 8KB/ ~/ 256KB/ 的/ 消息/ 时/ 通信/ 性能/ 提高/ 了/ 48/ %/ ~/ 91/ %/ ;/ 在/ 传输/ 256KB/ ~/ 2MB/ 字节/ 的/ 消息/ 时/ 性能/ 提高/ 了/ 47/ %/ ~/ 106/ %/ ;/ 在/ 传输/ 4MB/ ~/ 8MB/ 的/ 消息/ 时/ 性能/ 提高/ 了/ 56/ %/ ~/ 10/ %/ ./ 另外/ ,/ LiMiC/ 在/ 传输/ 大小/ 为/ 16KB/ ~/ 256KB/ 数据/ 时/ 性能/ 提高/ 了/ 5/ %/ ~/ 35/ %/ ,/ 但/ 在/ 传输/ 其余/ 大小/ 区间/ 的/ 消息/ 时/ 通信/ 性能/ 较/ 使用/ 用户/ 态/ 共享内存/ 的/ MVAPI/ -/ CH2/ 有/ 明显/ 下降/ ;/ KNEM/ 对/ 处理器/ 内/ 点对点/ 通信/ 的/ 性能/ 没有/ 任何/ 提高/ ,/ 我们/ 进行/ 多次/ 实验/ 都/ 得到/ 了/ 相似/ 的/ 结果/ ./ 处理器/ 间/ 的/ 通信/ 性能/ 评测/ 结果/ 如图/ 10/ 所示/ ,/ 相对/ 于/ 基于/ 用户/ 级/ 共享内存/ 的/ MVAPICH2/ ,/ MPIAc/ -/ tor/ 在/ 传输/ 大小/ 为/ 8KB/ ~/ 256KB/ 的/ 消息/ 时/ 通信/ 性能/ 提高/ 了/ 30/ %/ ~/ 144/ %/ ;/ 在/ 传输/ 256KB/ ~/ 2MB/ 的/ 消息/ 时/ 性能/ 提高/ 了/ 74/ %/ ~/ 117/ %/ ;/ 在/ 传输/ 2MB/ ~/ 16MB/ 的/ 消息/ 时/ 性能/ 提高/ 了/ 44/ %/ ~/ 64/ %/ ./ 相对/ 于/ OpenMPI/ ,/ MPIActor/ 在/ 传输/ 大小/ 为/ 8KB/ ~/ 256KB/ 的/ 消息/ 时/ 通信/ 性能/ 提高/ 了/ 46/ %/ ~/ 98/ %/ ;/ 在/ 传输/ 256KB/ ~/ 2MB/ 的/ 消息/ 时/ 性能/ 提高/ 了/ 70/ %/ ~/ 72/ %/ ;/ 传输/ 2MB/ ~/ 16MB/ 的/ 消息/ 时/ 性能/ 提高/ 了/ 38/ %/ ~/ 70/ %/ ./ 另外/ ,/ LiMiC/ 在/ 传输/ 32KB/ ~/ 4MB/ 的/ 数据/ 时有/ Page937/ %/ ~/ 98/ %/ 的/ 性能/ 提升/ ,/ 在/ 传输/ 更大/ 的/ 消息/ 时/ 性能/ 有/ 5/ %/ 左右/ 的/ 提升/ ./ 图/ 9/ 处理器/ 内/ 点对点/ 通信/ 延迟/ 图/ 10/ 处理器/ 间/ 点对点/ 通信/ 延迟/ 节点/ 间通信/ 的/ 性能/ 比较/ 如图/ 11/ 所示/ ,/ 可以/ 发现/ 节点/ 间通信/ 经由/ MPIActor/ 转发/ 性能/ 几乎/ 不/ 受/ 影响/ ./ Page10/ 图/ 11/ 节点/ 间/ 点对点/ 通信/ 延迟/ 8/ 总结/ 和/ 未来/ 的/ 工作/ MPI/ 加速器/ 是/ 一种/ 对/ 传统/ MPI/ 基础/ 软件/ 有效/ 的/ 通信/ 性能/ 优化/ 技术/ ./ 它/ 以/ 透明/ 中间件/ 的/ 形式/ 提供/ 了/ 一个/ 线程/ MPI/ 运行/ 环境/ ./ 采用/ 这种/ 形式/ ,/ MPI/ -/ Actor/ 一方面/ 解/ 耦/ 了/ 线程/ MPI/ 与/ MPI/ 进程/ 内/ 多线程/ 之间/ 的/ 矛盾/ ,/ 另一方面/ 简化/ 了/ 线程/ MPI/ 软件/ 的/ 开发/ 工作量/ ./ 实验/ 表明/ :/ (/ 1/ )/ MPIActor/ 能够/ 有效/ 地/ 全面提高/ 传统/ MPI/ 软件/ 的/ 节点/ 内/ 通信/ 性能/ ,/ 同时/ 不/ 影响/ 节点/ 间通信/ 性能/ ./ (/ 2/ )/ 线程/ MPI/ 的/ 点对点/ 通信/ 比/ LiMiC/ 和/ KNEM/ 等/ 基于/ 核心/ 级/ 内存/ 映射/ 机制/ 的/ 点对点/ 通信/ 优化/ 方法/ 更/ 高效/ ./ 下/ 一步/ 我们/ 将/ 在/ MPIActor/ 的/ 基础/ 上/ 通过/ 基于/ 共享内存/ 的/ 算法/ 进一步/ 优化/ 集合/ 通信/ 性能/ ./ 

