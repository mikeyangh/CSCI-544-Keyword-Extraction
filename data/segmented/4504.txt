Page1/ 基于/ 自/ 适应/ 归一化/ RBF/ 网络/ 的/ 犙/ -/ 犞/ 值/ 函数/ 协同/ 逼近/ 模型/ 1/ )/ (/ 苏州大学/ 计算机科学/ 与/ 技术/ 学院/ 江苏/ 苏州/ 215006/ )/ 2/ )/ (/ 吉林大学/ 符号计算/ 与/ 知识/ 工程/ 教育部/ 重点/ 实验室/ 长春/ 130012/ )/ 3/ )/ (/ 江苏省/ 软件/ 新/ 技术/ 与/ 产业化/ 协同/ 创新/ 中心/ 南京/ 210046/ )/ 摘要/ 径向/ 基/ 函数/ 网络/ 逼近/ 模型/ 可以/ 有效/ 地/ 解决/ 连续/ 状态/ 空间/ 强化/ 学习/ 问题/ ./ 然而/ ,/ 强化/ 学习/ 的/ 在线/ 特性/ 决定/ 了/ RBF/ 网络/ 逼近/ 模型/ 会/ 面临/ “/ 灾难性/ 扰动/ ”/ ,/ 即/ 新/ 样本/ 作用/ 于/ 学习/ 模型/ 后/ 非常容易/ 对/ 先前/ 学习/ 到/ 的/ 输入输出/ 映射/ 关系/ 产生/ 破坏/ ./ 针对/ RBF/ 网络/ 逼近/ 模型/ 的/ “/ 灾难性/ 扰动/ ”/ 问题/ ,/ 文中/ 提出/ 了/ 一种/ 基于/ 自/ 适应/ 归一化/ RBF/ (/ ANRBF/ )/ 网络/ 的/ Q/ -/ V/ 值/ 函数/ 协同/ 逼近/ 模型/ 及/ 对应/ 的/ 协同/ 逼近/ 算法/ —/ —/ —/ QV/ (/ λ/ )/ ./ 该/ 算法/ 对/ 由/ RBFs/ 提取/ 得到/ 的/ 特征向量/ 进行/ 归一化/ 处理/ ,/ 并/ 在线/ 自/ 适应/ 地/ 调整/ ANRBF/ 网络/ 隐藏/ 层/ 节点/ 的/ 个数/ 、/ 中心/ 及/ 宽度/ ,/ 可以/ 有效/ 地/ 提高/ 逼近/ 模型/ 的/ 抗干扰性/ 和/ 灵活性/ ./ 协同/ 逼近/ 模型/ 中/ 利用/ Q/ 和/ V/ 值/ 函数/ 协同/ 塑造/ TD/ 误差/ ,/ 在/ 一定/ 程度/ 上/ 利用/ 了/ 环境/ 模型/ 的/ 先验/ 知识/ ,/ 因此/ 可以/ 有效/ 地/ 提高/ 算法/ 的/ 收敛/ 速度/ 和/ 初始/ 性能/ ./ 从/ 理论/ 上/ 分析/ 了/ QV/ (/ λ/ )/ 算法/ 的/ 收敛性/ ,/ 并/ 对比/ 其他/ 的/ 函数/ 逼近/ 算法/ ,/ 通过/ 实验/ 验证/ 了/ QV/ (/ λ/ )/ 算法/ 具有/ 较优/ 的/ 性能/ ./ 关键词/ 强化/ 学习/ ;/ 函数/ 逼近/ ;/ 径向/ 基/ 函数/ ;/ 灾难性/ 扰动/ ;/ 协同/ 逼近/ 1/ 引言/ 强化/ 学习/ (/ ReinforcementLearning/ ,/ RL/ )/ 是/ 一类/ 由/ 学习/ 环境/ 状态/ 到/ 动作/ 的/ 映射/ 方法/ ./ 在/ 与/ 环境/ 交互/ 中/ ,/ Agent/ 选择/ 动作/ ,/ 环境/ 作出反应/ ,/ 到达/ 新/ 的/ 状态/ ,/ 并/ 对/ 每个/ 状态/ 或/ 状态/ 动作/ 对/ 通过/ 值/ 函数/ 评价/ 其/ 好坏/ ,/ 最终/ 通过/ 值/ 函数/ 确定/ 到达/ 目标/ 的/ 最优/ 策略/ ./ 目前/ 强化/ 学习/ 方法/ 被/ 广泛/ 地/ 应用/ 于/ 工业/ 控制/ 、/ 仿真/ 、/ 博弈/ 等/ 领域/ [/ 1/ -/ 4/ ]/ ./ 在/ 离散/ 状态/ 空间/ 强化/ 学习/ 系统/ 中/ ,/ 值/ 函数/ 通常/ 利用/ 查询/ 表/ (/ Lookup/ -/ Table/ )/ 的/ 方式/ 进行/ 存储/ ,/ 状态/ 和/ 动作/ 是/ 表/ 的/ 两个/ 维度/ ,/ 其/ 估计值/ 与/ 表格/ 中/ 的/ 表项/ 相对/ 应/ ./ 这种/ 方法/ 非常适合/ 于/ 离散/ 的/ 小/ 状态/ 空间/ 任务/ ,/ 而/ 对于/ 大/ 的/ 或/ 连续/ 空间/ 任务/ ,/ 会/ 面临/ “/ 维数灾/ ”/ ,/ 从而/ 导致/ 收敛/ 速度慢/ 甚至/ 无法/ 收敛/ ./ 目前/ 解决/ “/ 维数灾/ ”/ 问题/ 主要/ 采取/ 3/ 种/ 方法/ :/ (/ 1/ )/ 采取/ 聚类/ 等/ 编码方式/ 将/ 任务/ 转化成/ 查询/ 表/ 强化/ 学习/ 可以/ 解决/ 的/ 问题/ [/ 5/ ]/ ;/ (/ 2/ )/ 对/ 任务/ 进行/ 分解/ ,/ 采用/ 分层/ 、/ 并行/ 等/ 技术/ 来/ 提高/ 强化/ 学习/ 方法/ 的/ 执行/ 效率/ [/ 6/ -/ 8/ ]/ ;/ (/ 3/ )/ 采取/ 函数/ 逼近/ 方法/ 对/ 强化/ 学习/ 中/ 的/ 值/ 函数/ 或/ 策略/ 进行/ 建模/ ,/ 利用/ 样本/ 来/ 不断/ 调整/ 参数/ ,/ 逐渐/ 逼近/ 真实/ 的/ 问题/ 模型/ [/ 9/ ]/ ./ 函数/ 逼近/ 通常/ 包括/ 带参/ 函数/ 逼近/ 和/ 无参/ 函数/ 逼近/ 两种/ [/ 10/ -/ 11/ ]/ ./ 带/ 参数/ 函数/ 逼近/ 其/ 函数/ 形式/ 和/ 参数/ 个数/ 事先/ 预定/ ,/ 初始模型/ 限制/ 了/ 学习效果/ ,/ 这种/ 逼近/ 方法/ 非常容易/ 陷入/ 局部/ 极小值/ ./ 相比而言/ ,/ 无参/ 函数/ 逼近/ 方法/ 其/ 模型/ 根据/ 样本/ 的/ 个数/ 和/ 形式/ 不断/ 调整/ ,/ 因此/ 灵活性/ 和/ 逼近/ 精度/ 都/ 大大提高/ ./ 由于/ 带参/ 的/ 函数/ 逼近/ 模型/ 方法/ 简单/ ,/ 容易/ 在/ 理论/ 上/ 保证/ 其/ 收敛性/ ,/ 使得/ 其/ 应用/ 广泛/ ./ Tadic/ [/ 12/ ]/ 提出/ 了/ 一种/ 与/ 线性/ 函数/ 逼近/ 器/ 相结合/ 的/ 时间/ 差分/ (/ TemporalDifference/ ,/ TD/ )/ 学习/ 方法/ ./ Sherstov/ 等/ 人/ [/ 13/ ]/ 提出/ 了/ 一种/ 基于/ 粗糙/ 编码/ 的/ 在线/ 自/ 适应/ 线性/ 函数/ 逼近/ 方法/ ./ Sutton/ 等/ 人/ [/ 14/ ]/ 将/ 梯度/ 下降/ 线性/ 函数/ 逼近/ 器/ 结合/ 于/ 时间/ 差分/ 算法/ 中/ ,/ 提出/ 了/ 一种/ 梯度/ 时间/ 差分/ 学习/ 方法/ ./ 而后/ ,/ Sutton/ 等/ 人/ [/ 15/ ]/ 对/ 梯度/ 时间/ 差分/ 算法/ 做/ 进一步/ 改进/ ,/ 提出/ 了/ GTD2/ 及/ 带有/ 梯度/ 校正/ 的/ 线性/ 时间/ 差分/ 方法/ (/ linearTDwithgradientCorrection/ ,/ TDC/ )/ ./ Sutton/ 等/ 人/ 提出/ 的/ 系列/ 时间/ 差分/ 方法/ 使得/ 离/ 策略/ 时间/ 差分/ 学习/ 算法/ 的/ 不/ 稳定/ 问题/ ,/ 在/ 一定/ 程度/ 上/ 得到/ 解决/ ./ 另外/ ,/ Maei/ 等/ 人/ 提出/ 了/ GQ/ (/ λ/ )/ (/ GeneralQ/ (/ λ/ )/ )/ 算法/ [/ 16/ ]/ 和/ Greedy/ -/ GQ/ 算法/ [/ 17/ ]/ ,/ 并于/ 2009/ 年/ 将/ 文献/ [/ 14/ -/ 15/ ]/ 中/ 的/ 线性/ 函数/ 逼近/ 模型/ 扩展/ 到/ 非线性/ 函数/ 逼近/ 模型/ [/ 18/ ]/ ./ Bonarini/ 等/ 人/ [/ 19/ ]/ 提出/ 了/ 一种/ 基于/ 模糊/ 逻辑/ 的/ Q/ 学习/ 算法/ ,/ 并/ 验证/ 了/ 算法/ 的/ 有效性/ ./ Heinen/ 等/ 人/ [/ 20/ ]/ 提出/ 了/ 一种/ 基于/ 概率/ 神经网络/ 的/ 强化/ 学习/ 方法/ ,/ 该/ 方法/ 增量/ 式/ 地/ 逼近/ 问题/ 的/ 值/ 函数/ ./ 目前/ 无参/ 函数/ 逼近/ 模型/ 主要/ 包括/ 高斯/ 过程/ 函数/ 逼近/ 模型/ 和/ 核/ 函数/ 逼近/ 模型/ 两种/ ./ Engel/ 等/ 人/ [/ 21/ ]/ 在/ 无参/ 函数/ 逼近/ 器/ 的/ 基础/ 上/ ,/ 提出/ 了/ 高斯/ 过程/ 时间/ 差分/ 算法/ ,/ 利用/ 高斯/ 过程/ 来/ 逼近/ 强化/ 学习/ 中/ 的/ 值/ 函数/ ./ Ormoneit/ 等/ 人/ [/ 22/ ]/ 提出/ 了/ 一种/ 基于/ 核/ 的/ 强化/ 学习/ 方法/ ./ Xu/ 等/ 人/ [/ 23/ ]/ 提出/ 了/ 基于/ 核/ 的/ 最小/ 二乘/ 时间/ 差分/ 方法/ (/ Kernel/ -/ basedLeastSquaresTD/ ,/ KLSTD/ )/ ,/ 将/ 基于/ 核/ 的/ 逼近/ 器/ 与/ 最小/ 二乘/ 相结合/ ,/ 取得/ 了/ 一定/ 的/ 效果/ ./ 在/ KLSTD/ 基础/ 上/ ,/ Xu/ 等/ 人/ [/ 24/ ]/ 提出/ 了/ KLSPI/ 及/ KLSTD/ -/ Q/ 算法/ ,/ 并/ 证明/ 了/ 算法/ 的/ 有效性/ ./ 而后/ Taylor/ 等/ 人/ [/ 25/ ]/ 证明/ 了/ KLSPI/ 及/ KLSTD/ -/ Q/ 算法/ 的/ 等价/ 性/ ./ 对于/ 无参/ 函数/ 逼近/ 模型/ 来说/ ,/ 由于/ 强化/ 学习/ 的/ 样本/ 是/ 在线/ 获得/ 的/ ,/ 因此/ 难以/ 保证/ 其/ 收敛/ ./ 径向/ 基/ 函数/ RBF/ (/ RadialBasisFunction/ )/ 网络/ 逼近/ 模型/ 是/ 一种/ 局部/ 逼近/ 神经网络/ ,/ 可以/ 用来/ 解决/ 连续/ 空间/ 强化/ 学习/ 问题/ ./ 该/ 模型/ 既/ 利用/ 了/ 核/ 函数/ 等/ 机制/ 来/ 提高/ 模型/ 的/ 表达能力/ ,/ 又/ 具有/ 线性/ 逼近/ 模型/ 的/ 简单/ 性/ ./ 针对/ RBF/ 网络/ 逼近/ 模型/ 应用/ 于/ 强化/ 学习/ 会/ 出现/ “/ 灾难性/ 扰动/ ”/ 等/ 问题/ ,/ Barreto/ 等/ 人/ [/ 26/ ]/ 提出/ 了/ 基于/ RBF/ 网络/ 的/ RGD/ -/ TD/ (/ 0/ )/ 算法/ ,/ 经/ 扩展/ 得到/ RGD/ -/ Sarsa/ (/ λ/ )/ 等/ 系列/ 算法/ ,/ 并/ 从/ 理论/ 和/ 实验/ 两/ 方面/ 验证/ 了/ 算法/ 的/ 有效性/ ./ 这些/ 算法/ 一定/ 程度/ 上/ 解决/ 了/ RBF/ 网络/ 逼近/ 模型/ 中/ 的/ “/ 灾难性/ 扰动/ ”/ 问题/ ,/ 但是/ 算法/ 的/ 收敛/ 速度/ 却/ 不/ 理想/ ./ Page3/ 针对/ RBF/ 网络/ 逼近/ 模型/ 的/ 有效性/ 及/ 存在/ 的/ 问题/ ,/ 本文/ 提出/ 了/ 一种/ 基于/ 自/ 适应/ 归一化/ 径向/ 基/ 函数/ RBF/ (/ AdaptiveNormalizedRBF/ ,/ ANRBF/ )/ 网络/ 的/ Q/ -/ V/ 值/ 函数/ 协同/ 逼近/ 模型/ 及/ 对应/ 的/ 协同/ 逼近/ 算法/ —/ —/ —/ QV/ (/ λ/ )/ ./ 对于/ 连续/ 状态/ ,/ 利用/ ANRBF/ 网络/ 来/ 进行/ 编码/ 和/ 特征提取/ ,/ 得到/ 特征向量/ ./ 进一步/ 经过/ 归一化/ 处理/ ,/ 能够/ 得到/ 平滑/ 的/ 逼近/ 模型/ ,/ 在/ 一定/ 程度/ 上/ 可/ 提高/ 模型/ 的/ 抗干扰性/ ./ 为了/ 有效/ 地/ 提高/ 逼近/ 模型/ 的/ 灵活性/ ,/ 该/ 网络/ 逼近/ 模型/ 可以/ 自/ 适应/ 地/ 调整/ 隐藏/ 层/ 节点/ 的/ 个数/ 、/ 宽度/ 和/ 中心/ ./ 另外/ ,/ Q/ -/ V/ 值/ 函数/ 协同/ 逼近/ 模型/ 利用/ 状态值/ 函数/ 来/ 塑造/ 奖赏/ ,/ 将/ 环境/ 模型/ 知识/ 以/ 奖赏/ 的/ 形式/ 传递/ 给/ 学习/ 器/ ,/ 从而/ 有效/ 地/ 提高/ 了/ 算法/ 的/ 收敛/ 速度/ 和/ 初始/ 性能/ ./ 将/ QV/ (/ λ/ )/ 算法/ 应用/ 于/ 连续/ 状态/ 空间/ 强化/ 学习/ 标准/ 验证/ 仿真/ 平台/ MountainCar/ ,/ 实验/ 表明/ ,/ QV/ (/ λ/ )/ 算法/ 在/ 时间/ 、/ 空间/ 及/ 收敛性/ 上/ 都/ 具有/ 较优/ 的/ 性能/ ./ 2/ 相关/ 理论/ 2.1/ 问题/ 描述/ 强化/ 学习/ 问题/ 通常/ 可以/ 用/ 马尔可夫/ 决策/ 过程/ 来/ 建模/ ./ 定义/ 1/ ./ 在/ t/ 时刻/ ,/ 系统/ 的/ n/ 个/ 状态/ 可以/ 用/ 实数/ 向量/ 表示/ 为/ 狓/ t/ =/ [/ 狓/ 1/ (/ t/ )/ ,/ 狓/ 2/ (/ t/ )/ ,/ …/ ,/ 狓/ n/ (/ t/ )/ ]/ T/ ∈/ / 不/ 考虑/ 时刻/ t/ ,/ 则/ 可以/ 表示/ 为/ 狓/ =/ [/ 狓/ 1/ ,/ 狓/ 2/ ,/ …/ ,/ 狓/ n/ ]/ T/ ./ 定义/ 2/ ./ 设/ P/ 为/ 强化/ 学习/ 任务/ ,/ P/ 的/ 状态/ 空间/ 是/ 由/ n/ 维/ 状态/ 向量/ 狓/ =/ [/ 狓/ 1/ ,/ 狓/ 2/ ,/ …/ ,/ 狓/ n/ ]/ T/ 各/ 分量/ 而/ 构成/ 的/ 子/ 状态/ 空间/ X/ (/ P/ )/ =/ {/ 狓/ |/ 狓/ =/ [/ 狓/ 1/ ,/ …/ ,/ 狓/ n/ ]/ T/ ,/ 狓/ i/ ∈/ Di/ ,/ i/ =/ 1/ ,/ …/ ,/ n/ }/ ,/ 其中/ Di/ 是/ 状态/ 向量/ 第/ i/ 个/ 分量/ 的/ 值域/ ./ 定义/ 3/ ./ 一个/ 四元组/ M/ =/ {/ X/ ,/ U/ ,/ R/ ,/ T/ }/ 为/ 马尔可夫/ 决策/ 过程/ ,/ 其中/ X/ =/ {/ 狓/ t/ |/ t/ ∈/ / (/ 可以/ 为/ 连续/ 的/ )/ ,/ U/ =/ {/ 狌/ i/ }/ k/ 间/ ,/ R/ :/ X/ ×/ U/ ×/ X/ / / 是/ 状态/ 迁移/ 函数/ ./ X/ / / 2.2/ GD/ -/ Sarsa/ (/ λ/ )/ 线性/ 带参/ Q/ 值/ 函数/ 逼近/ 器是/ 通过/ n/ 维基/ 函数/ 向量/ / (/ 狓/ ,/ 狌/ )/ 与/ n/ 维/ 参数/ 向量/ 狑/ 建模/ 得到/ ,/ 其/ 计算公式/ 为/ 其中/ / (/ 狓/ ,/ 狌/ )/ 为/ 状态/ 动作/ 对/ 〈/ 狓/ ,/ 狌/ 〉/ 的/ 特征向量/ ,/ 狑/ 为/ 参数/ 向量/ ./ Sutton/ 等/ 人/ 在/ 式/ (/ 1/ )/ 的/ 基础/ 上/ ,/ 提出/ 了/ GD/ -/ Sarsa/ (/ λ/ )/ 算法/ [/ 1/ ]/ ,/ 该/ 方法/ 利用/ 梯度/ 下降/ 方法/ 来/ 迭代/ 更新/ 值/ 函数/ 的/ 参数/ 向量/ ,/ 其/ 迭代/ 公式/ 如式/ (/ 2/ )/ :/ 其中/ 狑/ t/ 和/ 狑/ t/ +/ 1/ 为/ 参数/ 向量/ ./ α/ ∈/ [/ 0/ ,/ 1/ ]/ 为/ 学习/ 率/ ./ 时间/ 差分/ TD/ 误差/ 为/ δ/ t/ =/ 狉/ t/ +/ 1/ +/ γ/ Qt/ (/ 狓/ t/ +/ 1/ ,/ 狌/ t/ +/ 1/ )/ -/ Qt/ (/ 狓/ t/ ,/ 狌/ t/ )/ ,/ 狉/ t/ +/ 1/ 为/ 立即/ 奖赏/ ,/ γ/ ∈/ [/ 0/ ,/ 1/ ]/ 为/ 折扣率/ ,/ Qt/ (/ 狓/ t/ ,/ 狌/ t/ )/ 和/ Qt/ (/ 狓/ t/ +/ 1/ ,/ 狌/ t/ +/ 1/ )/ 分别/ 为/ 在/ t/ 时刻/ 状态/ 动作/ 对/ 〈/ 狓/ t/ ,/ 狌/ t/ 〉/ 及/ 〈/ 狓/ t/ +/ 1/ ,/ 狌/ t/ +/ 1/ 〉/ 的/ 估计值/ ./ 向量/ 犲/ t/ 为/ t/ 时刻/ 的/ 资格/ 迹/ ,/ 具体/ 更新/ 如式/ (/ 3/ )/ :/ 式/ (/ 3/ )/ 采用/ 累加/ 迹/ (/ accumulatingtrace/ )/ 的/ 更新/ 方式/ ./ λ/ ∈/ [/ 0/ ,/ 1/ ]/ 为/ 资格/ 迹/ 衰减/ 因子/ ./ 通过/ 对/ 函数/ f/ 的/ 参数/ 向量/ 狑/ t/ 的/ 每/ 一个/ 分量/ 求/ 偏导/ ,/ 得到/ 梯度/ 向量/ ,/ 如式/ (/ 4/ )/ :/ / 狑/ tf/ (/ 狑/ t/ )/ =/ 3/ 犙/ -/ 犞/ 值/ 函数/ 协同/ 逼近/ 模型/ 3.1/ 犙/ -/ 犞/ 值/ 函数/ 协同/ 机制/ 在/ 强化/ 学习/ 中/ ,/ 通常/ 利用/ 值/ 函数/ 来/ 评估/ 策略/ ,/ 具体/ 可以/ 分为/ 动作/ 值/ 函数/ 和/ 状态值/ 函数/ 两种/ ./ 根据/ 两类/ 值/ 函数/ 的/ 不同/ 特性/ ,/ 提出/ 一种/ 协同/ 机制/ 来/ 解决/ 连续/ 状态/ 空间/ 的/ 强化/ 学习/ 问题/ ./ 针对/ 连续/ 状态/ 空间/ 的/ 强化/ 学习/ 任务/ ,/ 其中/ X/ =/ {/ 狓/ |/ 狓/ =/ [/ 狓/ 1/ ,/ …/ ,/ 狓/ n/ ]/ T/ ,/ 狓/ i/ ∈/ Di/ ,/ i/ =/ 1/ ,/ …/ ,/ n/ }/ 为/ 连续/ 状态/ 空间/ ,/ U/ =/ {/ 狌/ i/ }/ kQ/ -/ V/ 值/ 函数/ 协同/ 模型/ ,/ 如式/ (/ 5/ )/ :/ 烄/ Qi/ (/ 狓/ )/ =/ 狑/ T/ 烅/ V/ (/ 狓/ )/ =/ 狑/ T/ 烆/ 其中/ 包含/ k/ 个/ Q/ 值/ 函数/ 和/ 1/ 个/ V/ 值/ 函数/ ,/ / (/ 狓/ )/ =/ [/ / 1/ (/ 狓/ )/ ,/ …/ ,/ / m/ (/ 狓/ )/ ]/ T/ ∈/ / 征/ 向量/ ,/ 狑/ i/ =/ [/ 狑/ i1/ ,/ …/ ,/ 狑/ im/ ]/ T/ ∈/ / 值/ 函数/ 的/ 权值/ 向量/ ,/ 狑/ l/ =/ [/ 狑/ l1/ ,/ …/ ,/ 狑/ lm/ ]/ T/ ∈/ / 函数/ 的/ 权值/ 向量/ ./ 在/ 强化/ 学习/ 算法/ 中/ ,/ 通常/ 利用/ TD/ 误差/ 来/ 修正/ 值/ 函数/ 的/ 参数/ 向量/ ,/ 动态/ 有效/ 地/ 降低/ 函数/ 逼近/ 模型/ 的/ “/ 灾难性/ 扰动/ ”/ ./ 为了/ 使得/ TD/ 误差/ 相对/ 稳定/ ,/ Ng/ 等/ 人/ [/ 27/ ]/ 利用/ 塑造/ 奖赏/ 机制/ 来/ 重塑/ TD/ 误差/ ./ 通过/ 将/ 模型/ 知识/ 以/ 奖赏/ 的/ 形式/ 调整/ TD/ 误差/ ,/ 使得/ Agent/ 根据/ 新/ TD/ 误差/ 更新/ 值/ 函数/ ,/ 最终/ 能够/ 减少/ 次优/ 动/ Page4/ 作/ 的/ 选择/ 次数/ ,/ 从而/ 加快/ 算法/ 的/ 收敛/ 速度/ ./ 另外/ ,/ Randlv/ 等/ 人/ [/ 28/ ]/ 同时/ 也/ 指出/ ,/ 如果/ 塑造/ 奖赏/ 机制/ 使用不当/ ,/ 将会/ 减缓/ 算法/ 收敛/ 速度/ ./ 本文/ 在/ 文献/ [/ 27/ -/ 28/ ]/ 的/ 基础/ 上/ ,/ 提出/ 了/ 一种/ 利用/ 状态值/ 函数/ 对/ 奖赏/ 进行/ 塑造/ 的/ 方法/ ./ 基于/ 状态值/ 函数/ 的/ 塑造/ 奖赏/ 定义/ 如下/ ./ 定义/ 4/ ./ 基于/ 状态值/ 函数/ V/ :/ X/ / / 有界/ 实值/ 映射/ F/ :/ X/ ×/ U/ ×/ X/ / / 态/ 空间/ ,/ U/ 为/ 离散/ 动作/ 空间/ ,/ 状态值/ 函数/ 塑造/ 奖赏/ (/ ShapingRewardbasedontheStateValueFunction/ ,/ SR/ -/ SVF/ )/ 定义/ 如下/ :/ 其中/ ,/ γ/ ∈/ [/ 0/ ,/ 1/ ]/ 为/ 折扣率/ ,/ V/ (/ 狓/ t/ )/ 和/ V/ (/ 狓/ t/ +/ 1/ )/ 分别/ 为/ 状态/ 狓/ t/ 和/ 狓/ t/ +/ 1/ 的/ 状态值/ 函数/ ./ 定义/ 5/ ./ 考虑/ t/ 时刻/ 的/ 状态/ 狓/ t/ ,/ Agent/ 根据/ 当前/ 行为/ 策略/ 选择/ 一个/ 动作/ 狌/ t/ =/ 狌/ i/ ∈/ U/ (/ 1/ / i/ / k/ )/ 作用/ 于/ 环境/ ,/ 状态/ 转移/ 至/ 狓/ t/ +/ 1/ ,/ 立即/ 奖赏/ 值/ 狉/ t/ +/ 1/ ,/ 并/ 根据/ 行为/ 策略/ 选择/ 状态/ 狓/ t/ +/ 1/ 下/ 的/ 动作/ 狌/ t/ +/ 1/ =/ 狌/ j/ ∈/ U/ (/ 1/ / j/ / k/ )/ ./ 在/ t/ 时刻/ ,/ 利用/ 塑造/ 奖赏/ 机制/ 的/ 协同/ TD/ 误差/ (/ CollaborativeTemporalDifferenceError/ ,/ C/ -/ TDE/ )/ 计算方法/ 如式/ (/ 7/ )/ :/ δ/ t/ =/ 狉/ t/ +/ 1/ +/ γ/ Qj/ (/ 狓/ t/ +/ 1/ )/ -/ Qi/ (/ 狓/ t/ )/ +/ F/ (/ 狓/ t/ ,/ 狌/ t/ ,/ 狓/ t/ +/ 1/ )/ =/ 狉/ t/ +/ 1/ +/ γ/ Qj/ (/ 狓/ t/ +/ 1/ )/ -/ Qi/ (/ 狓/ t/ )/ +/ γ/ V/ (/ 狓/ t/ +/ 1/ )/ -/ V/ (/ 狓/ t/ )/ =/ 狉/ t/ +/ 1/ +/ γ/ (/ Qj/ (/ 狓/ t/ +/ 1/ )/ +/ V/ (/ 狓/ t/ +/ 1/ )/ )/ -/ (/ Qi/ (/ 狓/ t/ )/ +/ V/ (/ 狓/ t/ )/ )/ 为了/ 简化/ 表示/ ,/ 式/ (/ 7/ )/ 中/ 值/ 函数/ Qi/ 、/ Qj/ 以及/ V/ 都/ 省略/ 时间/ 步/ 下标/ t/ ./ 由于/ 协同/ 逼近/ 模型/ 的/ 特殊性/ ,/ 本文/ 对/ 资格/ 迹/ 重新/ 定义/ ,/ 如式/ (/ 8/ )/ :/ 犲/ j/ (/ t/ )/ =/ 其中/ j/ =/ 1/ ,/ 2/ ,/ …/ ,/ l/ ,/ 犲/ j/ (/ t/ )/ 是/ 在/ t/ 时刻/ 第/ j/ 个子/ 模型/ 的/ 权值/ 向量/ 的/ 资格/ 迹/ ./ 3.2/ 基于/ ANRBF/ 网络/ 的/ 犙/ -/ 犞/ 值/ 函数/ 协同/ 逼近/ 模型/ 在/ 基于/ 函数/ 近似/ 的/ 强化/ 学习/ 算法/ 中/ ,/ 状态/ 特征/ 的/ 提取/ 直接/ 影响/ 着/ 所/ 逼近/ 的/ 值/ 函数/ 模型/ 的/ 质量/ ./ 本文/ 利用/ 高斯/ 径向/ 基/ 函数/ 构建/ RBF/ 网络/ ,/ 用于/ 对/ 状态/ 特征/ 进行/ 提取/ ,/ 并/ 对/ 基/ 函数/ 进行/ 归一化/ 处理/ ,/ 通过/ 引入/ 自/ 适应/ 机制/ ,/ 最终/ 构建/ 一个/ ANRBF/ 网络/ ./ 下面/ 给出/ 基于/ ANRBF/ 网络/ 的/ Q/ -/ V/ 值/ 函数/ 协同/ 逼近/ 模型/ 的/ 结构图/ 及/ 描述/ ,/ 如图/ 1/ 所示/ ./ 图/ 1/ 给出/ 的/ Q/ -/ V/ 值/ 函数/ 协同/ 逼近/ 模型/ 是/ 一个/ n/ ×/ m/ ×/ (/ k/ +/ 1/ )/ 的/ 三层/ 归一化/ 带/ 反馈/ 机制/ 的/ 神经网络/ ,/ 该/ 网络/ 可以/ 在线/ 自/ 适应/ 地/ 调整/ 隐藏/ 层/ 各/ 节点/ 的/ 图/ 1/ 基于/ ANRBF/ 网络/ 的/ Q/ -/ V/ 值/ 函数/ 协同/ 逼近/ 模型/ 结构图/ 权值/ ./ 描述/ 如下/ :/ (/ 1/ )/ 左边/ 层为/ 输入/ 层/ ,/ 输入/ 一个/ n/ 维/ 状态/ 向量/ 狓/ =/ [/ 狓/ 1/ ,/ 狓/ 2/ ,/ …/ ,/ 狓/ n/ ]/ T/ ∈/ / (/ 2/ )/ 中间层/ 为/ 隐藏/ 层/ ,/ 包括/ 激活/ 函数/ 、/ 归一化/ 、/ 特征向量/ 3/ 个/ 部分/ ./ 激活/ 函数/ 共有/ m/ 个/ 隐/ 节点/ ,/ 采用/ 高斯/ 径向/ 基/ 函数/ 为/ 激活/ 函数/ ./ 激活/ 函数/ 定义/ 如式/ (/ 9/ )/ :/ ψ/ i/ (/ 狓/ )/ =/ exp/ -/ ∑/ 其中/ n/ 维/ 向量/ 犮/ i/ =/ [/ 犮/ i1/ ,/ …/ ,/ 犮/ in/ ]/ T/ 、/ σ/ i/ =/ [/ σ/ i1/ ,/ …/ ,/ σ/ in/ ]/ T/ 分别/ 表示/ 第/ i/ 个/ 高斯/ 径向/ 基/ 函数/ 的/ 中心/ 及/ 宽度/ ./ 通过/ 归一化/ 方法/ 对/ 激活/ 函数/ 的/ 输出/ 进行/ 归一化/ 处理/ 得到/ 状态/ 的/ 特征向量/ ,/ 其中/ 特征向量/ 的/ 第/ i/ 个/ 分量/ 如式/ (/ 10/ )/ :/ / i/ (/ 狓/ )/ =/ ψ/ i/ (/ 狓/ )/ ∑/ (/ 3/ )/ 右边/ 层为/ 输出/ 层/ ,/ 包含/ l/ =/ k/ +/ 1/ 个/ 输出/ ,/ 分别/ 是/ k/ 个/ 动作/ 值/ 函数/ 和/ 1/ 个/ 状态值/ 函数/ ,/ 值/ 函数/ 模型/ 构建/ 如式/ (/ 5/ )/ 所示/ ,/ 其中/ 式/ (/ 5/ )/ 中/ 的/ 狑/ ij/ (/ i/ =/ 1/ ,/ 2/ ,/ …/ ,/ l/ ,/ j/ =/ 1/ ,/ 2/ ,/ …/ ,/ m/ )/ 是/ 输出/ 层/ 的/ 权值/ ./ 基于/ ANRBF/ 网络/ 的/ Q/ -/ V/ 值/ 函数/ 协同/ 逼近/ 模型/ 的/ 描述/ 如下/ :/ (/ 1/ )/ 将/ 状态/ 向量/ 狓/ =/ [/ 狓/ 1/ ,/ …/ ,/ 狓/ n/ ]/ T/ ∈/ / 入/ ,/ 经过/ 式/ (/ 9/ )/ 所示/ 的/ 激活/ 函数/ 计算/ 得到/ 特征向量/ ψ/ (/ 狓/ )/ ∈/ / (/ 2/ )/ 将/ ψ/ (/ 狓/ )/ 代入/ 式/ (/ 10/ )/ 所示/ 的/ 归一化/ 公式/ 进行/ 处理/ ,/ 输出/ 特征向量/ / (/ 狓/ )/ ∈/ / (/ 3/ )/ 将/ / (/ 狓/ )/ 代入/ 式/ (/ 5/ )/ 所示/ 的/ 值/ 函数/ 模型/ ,/ 得到/ Q/ 和/ V/ 值/ 函数/ 的/ 近似值/ ./ (/ 4/ )/ 根据/ Q/ 值/ 函数/ 的/ 估计值/ ,/ 结合/ 动作/ 选择/ 策略/ 选择/ 一个/ 动作/ ,/ 获得/ 后续/ 状态/ 并/ 立即/ 奖赏/ ./ 利用/ 式/ (/ 6/ )/ 、/ (/ 7/ )/ 来/ 计算/ 协同/ TD/ 误差/ δ/ ./ 然后/ 根据/ 式/ (/ 8/ )/ 所/ 定义/ 的/ 资格/ 迹/ 将/ 当前/ 得到/ 的/ 协同/ TD/ 误差/ 反向/ 传播/ 至/ 整个/ 状态/ 及/ 状态/ 动作/ 空间/ ,/ 最后/ 根据/ 式/ (/ 2/ )/ 自/ 适应/ 地/ 调整/ 输出/ 层权值/ 及/ 隐藏/ 层/ 各/ 激活/ 函数/ 的/ 中心/ 和/ 宽度/ ./ Page5/ 协同/ 逼近/ 模型/ 的/ 动态/ 结构/ 主要/ 包括/ 4/ 个/ 方面/ 的/ 调整/ :/ (/ 1/ )/ 调整/ 输出/ 层权值/ 向量/ 狑/ i/ =/ [/ 狑/ i1/ ,/ 狑/ i2/ ,/ …/ ,/ 狑/ im/ ]/ T/ ,/ i/ =/ 1/ ,/ 2/ ,/ …/ ,/ l/ ./ 方法/ 为/ 狑/ i/ =/ 狑/ i/ +/ α/ δ/ 犲/ i/ ,/ 其中/ 犲/ i/ =/ [/ 犲/ i1/ ,/ 犲/ i2/ ,/ …/ ,/ 犲/ im/ ]/ T/ ,/ 资格/ 迹/ 的/ 更新/ 策略/ 为/ / i/ =/ 1/ ,/ 2/ ,/ …/ ,/ l/ ,/ 如果/ i/ =/ ID/ (/ 狌/ t/ )/ 或者/ i/ =/ l/ ,/ 那么/ 犲/ i/ =/ max/ (/ γ/ λ/ 犲/ i/ ,/ / (/ 狓/ )/ )/ ,/ 否则/ 犲/ i/ =/ γ/ λ/ 犲/ i/ ./ 其中/ ID/ (/ 狌/ t/ )/ 表示/ 当前/ 动作/ 狌/ t/ 在/ 动作/ 集合/ 中/ 的/ 编号/ ./ (/ 2/ )/ 如果/ δ/ >/ η/ 1/ 且/ / j/ =/ 1/ ,/ 2/ ,/ …/ ,/ m/ ,/ max/ (/ / j/ (/ 狓/ )/ )/ </ η/ 2/ ,/ 则/ 增加/ 一个/ 新/ 的/ 隐/ 节点/ ,/ m/ =/ m/ +/ 1/ ./ 初始化/ 该隐/ 节点/ 所含/ 的/ RBF/ 激活/ 函数/ 的/ 中心/ 向量/ 、/ 宽度/ 向量/ 以及/ 对应/ 的/ 输出/ 层权值/ 向量/ :/ / 狌/ =/ 1/ ,/ …/ ,/ n/ ,/ 犮/ mu/ =/ 狓/ u/ ,/ σ/ mu/ =/ 0.1/ ,/ / i/ =/ 1/ ,/ …/ ,/ l/ ,/ 狑/ im/ =/ 0/ ,/ 犲/ im/ =/ 0/ ./ 其中/ η/ 1/ 和/ η/ 2/ 为/ 预先/ 设定/ 的/ 阈值/ ./ (/ 3/ )/ 调整/ 中心/ 犮/ j/ =/ [/ 犮/ j1/ ,/ …/ ,/ 犮/ jn/ ]/ T/ ,/ j/ =/ 1/ ,/ 2/ ,/ …/ ,/ m/ ./ 假设/ / j/ =/ 1/ ,/ 2/ ,/ …/ ,/ m/ ,/ 犮/ z/ =/ argmax1/ ,/ …/ ,/ n/ ,/ 犮/ zu/ =/ 犮/ zu/ -/ =/ 犮/ zu/ +/ β/ 1/ δ/ 狑/ iz/ (/ 1/ -/ / z/ (/ 狓/ )/ )/ / z/ (/ 狓/ )/ 其中/ β/ 1/ 为/ 预先/ 设定/ 的/ 参数/ ./ (/ 4/ )/ 调整/ 宽度/ σ/ j/ =/ [/ σ/ j1/ ,/ …/ ,/ σ/ jn/ ]/ T/ ,/ j/ =/ 1/ ,/ 2/ ,/ …/ ,/ m/ ./ 假设/ / j/ =/ 1/ ,/ 2/ ,/ …/ ,/ m/ ,/ σ/ z/ =/ argmax/ / u/ =/ 1/ ,/ …/ ,/ n/ ,/ σ/ zu/ =/ σ/ zu/ -/ =/ σ/ zu/ +/ β/ 2/ δ/ 狑/ iz/ (/ 1/ -/ / z/ (/ 狓/ )/ )/ / z/ (/ 狓/ )/ 其中/ β/ 2/ 为/ 预先/ 设定/ 的/ 参数/ ./ 上述/ ANRBF/ 协同/ 逼近/ 模型/ 的/ 动态/ 结构调整/ 中/ ,/ 第/ (/ 1/ )/ 、/ (/ 3/ )/ 和/ (/ 4/ )/ 步/ 都/ 是/ 采用/ 梯度/ 下降/ 的/ 方法/ 来/ 进行/ 调整/ 的/ ,/ 第/ (/ 1/ )/ 步/ 的/ 调整/ 策略/ 如式/ (/ 4/ )/ 和/ (/ 8/ )/ 所示/ ,/ 第/ (/ 3/ )/ 和/ (/ 4/ )/ 步/ 调整/ 策略/ 的/ 详细/ 推导/ 如下/ :/ 犮/ zu/ =/ 犮/ zu/ -/ =/ 犮/ zu/ +/ β/ 1/ δ/ 狑/ iz/ =/ 犮/ zu/ +/ β/ 1/ δ/ 狑/ iz/ 同理/ ,/ σ/ zu/ =/ σ/ zu/ -/ =/ σ/ zu/ +/ β/ 2/ δ/ 狑/ iz/ (/ 1/ -/ / z/ (/ 狓/ )/ )/ / z/ (/ 狓/ )/ i/ / (/ 狓/ )/ 为/ 当前/ 选择/ 的/ 第/ i/ 个/ 动作/ 的/ Q/ 值/ 函数/ ,/ 其中/ 狑/ T/ / z/ (/ 狓/ )/ 形式/ 如式/ (/ 10/ )/ 所示/ ./ 4/ 犙/ -/ 犞/ 值/ 函数/ 协同/ 逼近/ 算法/ 4.1/ 犙/ 犞/ (/ λ/ )/ 根据/ Q/ -/ V/ 值/ 函数/ 协同/ 逼近/ 模型/ ,/ 提出/ 了/ 一种/ 基于/ ANRBF/ 网络/ 的/ 协同/ 逼近/ 算法/ QV/ (/ λ/ )/ ,/ 假设/ A/ =/ δ/ new/ >/ η/ 1/ ∧/ / j/ =/ 1/ ,/ 2/ ,/ …/ ,/ m/ ,/ max/ (/ / j/ (/ 狓/ )/ )/ </ η/ 2/ ,/ B/ =/ δ/ 狑/ iz/ (/ 1/ -/ / z/ (/ 狓/ )/ )/ / z/ (/ 狓/ )/ ,/ 算法/ 的/ 详细描述/ 如下/ ./ 算法/ 1/ ./ Q/ -/ V/ 值/ 函数/ 协同/ 逼近/ 算法/ QV/ (/ λ/ )/ ./ 输入/ :/ 任务/ 环境/ 模型/ 输出/ :/ 各/ 状态/ 对应/ 的/ 策略/ 1/ ./ 初始化/ ANRBF/ 网络/ 隐藏/ 层/ 各/ 节点/ 激活/ 函数/ ;/ 2/ ./ 构建/ 如式/ (/ 5/ )/ 所示/ 的/ Q/ -/ V/ 值/ 函数/ 协同/ 逼近/ 模型/ ;/ 3/ ./ / i/ =/ 1/ ,/ 2/ ,/ …/ ,/ l/ ,/ 狑/ i/ =/ 0/ ∈/ / 4/ ./ repeat/ (/ 对/ 每/ 一个/ 情节/ )/ 5/ ./ 当前/ 状态/ 狓/ 及/ 动作/ 狌/ ;/ 6/ ./ repeat/ (/ 对/ 该/ 情节/ 中/ 的/ 每/ 一步/ )/ 7/ ./ 执行/ 动作/ 狌/ ,/ 观察/ 狉/ ,/ 狓/ ;/ 8/ ./ 将/ 数据/ 〈/ 狓/ ,/ U/ (/ 狓/ )/ 〉/ 输入/ 当前/ 值/ 函数/ 模型/ ;/ 9/ ./ 狌/ ←/ 通过/ ε/ -/ greedy/ 等/ 策略/ 选择/ 狓/ 下/ 的/ 动作/ ;/ 10/ ./ 收集/ 数据/ 〈/ 狓/ ,/ 狌/ ,/ 狉/ ,/ 狓/ ,/ 狌/ 〉/ ;/ 11/ ./ 计算/ TDE/ ,/ δ/ old/ =/ 狉/ +/ γ/ QID/ (/ 狌/ )/ (/ 狓/ )/ -/ QID/ (/ 狌/ )/ (/ 狓/ )/ ;/ 12/ ./ 计算/ SR/ -/ SVF/ ,/ F/ (/ 狓/ ,/ 狌/ ,/ 狓/ )/ =/ γ/ V/ (/ 狓/ )/ -/ V/ (/ 狓/ )/ ;/ 13/ ./ 计算/ C/ -/ TDE/ ,/ δ/ new/ =/ δ/ old/ +/ F/ (/ 狓/ ,/ 狌/ ,/ 狓/ )/ ;/ 14/ ./ 更新/ 资格/ 迹/ ,/ / i/ =/ 1/ ,/ 2/ ,/ …/ ,/ l/ ,/ 15.16/ ./ 17/ ./ 将/ δ/ new/ 反馈/ 给/ Q/ -/ V/ 值/ 函数/ 协同/ 逼近/ 模型/ ;/ 18/ ./ / i/ =/ 1/ ,/ 2/ ,/ …/ ,/ l/ ,/ 狑/ i/ =/ 狑/ i/ +/ α/ δ/ new/ 犲/ i/ ;/ 19/ ./ 调整/ 协同/ 逼近/ 模型/ 的/ 隐藏/ 层/ 结构/ ,/ 20.21/ ./ 22.23/ ./ 24.25/ ./ 26.27/ ./ 狓/ =/ 狓/ ,/ 狌/ =/ 狌/ ;/ 28/ ./ 直到/ 狓/ 是/ 终止/ 状态/ ;/ 29/ ./ 直到/ 运行/ 完/ 设定/ 情节/ 数/ 或/ 满足/ 其他/ 终止/ 条件/ ./ 算法/ 对/ Q/ -/ V/ 值/ 函数/ 协同/ 逼近/ 模型/ 的/ 调整/ 主要/ Page6/ 包含/ 两个/ 方面/ :/ (/ 1/ )/ 对/ 输出/ 层/ 权重/ 的/ 调整/ ;/ (/ 2/ )/ 对/ 隐藏/ 层/ 结构/ 的/ 调整/ ./ 其中/ ,/ 调整/ 策略/ 是/ 文献/ [/ 26/ ]/ 中/ 所/ 给出/ 的/ RGD/ 策略/ ./ 假设/ 1/ ./ C/ -/ TDE/ 满足/ δ/ new/ / η/ 1/ 或者/ / j/ =/ 1/ ,/ 2/ ,/ …/ ,/ m/ ,/ max/ (/ / j/ (/ 狓/ )/ )/ / η/ 2/ ,/ 即/ C/ -/ TDE/ 不/ 大于/ 阈值/ η/ 1/ ,/ 或者/ 逼近/ 模型/ 隐藏/ 层/ 输出/ 的/ 最大/ 分量/ 不/ 小于/ 阈值/ η/ 2/ ,/ 其中/ η/ 1/ 和/ η/ 2/ 为/ 事先/ 设定/ 的/ 阈值/ ./ 假设/ 2/ ./ 令/ 犮/ z/ =/ argmax/ (/ / j/ (/ 狓/ )/ )/ ,/ 即/ 犮/ z/ 和/ σ/ z/ 分别/ 是/ 隐藏/ 层/ 各/ 激活/ 函数/ 中离/ 狓/ 最近/ 的/ 一个/ 基/ 函数/ 的/ 中心/ 向量/ 和/ 宽度/ 向量/ ./ 定理/ 1/ ./ 在/ 假设/ 1/ 和/ 假设/ 2/ 成立/ 的/ 前提/ 下/ ,/ RGD/ 调整/ 策略/ 有效/ ,/ 即/ 逼近/ 模型/ 关于/ 状态/ 向量/ 狓/ 收缩/ ./ 证明/ ./ 下面/ 从/ 中心/ 向量/ 和/ 宽度/ 向量/ 的/ 更新过程/ 来/ 分别/ 证明/ 定理/ 1/ 的/ 正确性/ :/ ①/ 由于/ / (/ 狓/ )/ 经过/ 归一化/ 处理/ ,/ / z/ =/ 1/ ,/ 2/ ,/ …/ ,/ n/ ,/ / z/ (/ 狓/ )/ ∈/ [/ 0/ ,/ 1/ ]/ ,/ 有/ (/ 1/ -/ / z/ (/ 狓/ )/ )/ / z/ (/ 狓/ )/ σ/ -/ 2/ ②/ 根据/ RGD/ 调整/ 策略/ ,/ 在/ 状态/ 狓/ 下/ 选择/ 的/ 动作/ 为/ 狌/ (/ 在/ 动作/ 集合/ 中/ 的/ 编号/ 记为/ i/ =/ ID/ (/ 狌/ )/ )/ ,/ 只有/ 当/ δ/ 狑/ iz/ >/ 0/ 时/ ,/ 才/ 调整/ 犮/ zu/ 的/ 大小/ ./ 又/ 因为/ Δ/ 犮/ zu/ =/ β/ 1/ δ/ 狑/ iz/ (/ 1/ -/ / z/ (/ 狓/ )/ )/ / z/ (/ 狓/ )/ (/ 狓/ u/ -/ 犮/ zu/ )/ σ/ -/ 2/ 负/ 取决于/ (/ 狓/ u/ -/ 犮/ zu/ )/ 的/ 正负/ ;/ ③/ 根据/ ①/ 和/ ②/ 可知/ :/ 犮/ z/ 的/ 狌/ 分量/ 犮/ zu/ 向/ 状态/ 向量/ 狓/ 的/ 狌/ 分量/ 狓/ u/ 方向/ 调整/ ;/ ④/ 在/ ③/ 的/ 基础/ 上/ ,/ 考虑/ 中心/ 向量/ 的/ 每/ 一维/ ,/ 可/ 得/ 中心/ 向量/ 犮/ z/ 始终/ 向/ 状态/ 向量/ 狓/ 的/ 方向/ 调整/ ;/ ⑤/ 因为/ / z/ (/ 狓/ )/ ∈/ [/ 0/ ,/ 1/ ]/ 且/ σ/ zu/ >/ 0/ ,/ 则/ (/ 1/ -/ / z/ (/ 狓/ )/ )/ / z/ (/ 狓/ )/ (/ 狓/ u/ -/ 犮/ zu/ )/ 2/ σ/ -/ 3RBF/ 的/ 宽度/ 向量/ 的/ 狌/ 分量/ σ/ zu/ 将/ 被/ 缩小/ ./ ⑥/ 由/ ③/ 可知/ ,/ 激活/ 函数/ 的/ 中心/ 向量/ 调整/ 的/ 方向/ 与/ 状态/ 向量/ 狓/ 保持一致/ ;/ 由/ ⑤/ 可知/ 激活/ 函数/ 的/ 宽度/ 向量/ 调整/ 是/ 收缩/ 的/ ./ 综上所述/ ,/ RGD/ 调整/ 策略/ 有效/ ,/ 且/ 逼近/ 模型/ 关于/ 状态/ 向量/ 狓/ 收缩/ ./ 4.2/ 算法/ 收敛性/ 分析/ 下面/ 从/ Q/ -/ V/ 值/ 函数/ 逼近/ 模型/ 的/ 结构/ 和/ 权值/ 调整/ 两个/ 方面/ 来/ 分析/ Q/ -/ V/ 逼近/ 模型/ 的/ 收敛性/ ./ 参考文献/ [/ 23/ ,/ 29/ ]/ 给出/ 了/ 引理/ 1/ ,/ 该/ 引理/ 主要/ 用来/ 辅助/ 选择/ Q/ -/ V/ 逼近/ 模型/ 隐藏/ 层/ 激活/ 函数/ 的/ 中心/ 向量/ ./ 引理/ 1/ [/ 23/ ,/ 29/ ]/ ./ 假设/ Q/ -/ V/ 逼近/ 模型/ 隐藏/ 层/ 激活/ 函数/ 的/ 候选/ 中心/ 向量/ 集/ 为/ 犡/ =/ {/ 狓/ 1/ ,/ 狓/ 2/ ,/ …/ ,/ 狓/ n/ }/ ,/ 基于/ ALD/ 方法/ 得到/ t/ -/ 1/ 时刻/ 的/ 中心/ 向量/ 集/ 为/ 犡/ t/ -/ 1/ =/ {/ 狓/ 1/ ,/ 狓/ 2/ ,/ …/ ,/ 狓/ m/ }/ ,/ 1/ </ m/ / n/ ./ 考虑/ 一个/ 新/ 样本/ 狓/ t/ ,/ 采用/ 文献/ [/ 23/ ]/ 中/ 所/ 给出/ 的/ ALD/ 特征选择/ 方法/ ,/ 当/ min/ 犮/ ∑/ j/ 则/ 犡/ t/ =/ 犡/ t/ -/ 1/ ∪/ {/ 狓/ t/ }/ ,/ 其中/ 犮/ =/ [/ 犮/ 1/ ,/ …/ ,/ 犮/ j/ ]/ ,/ μ/ 为/ 预先/ 设定/ 的/ 阈值/ ,/ 则/ Q/ -/ V/ 逼近/ 模型/ 隐藏/ 层/ 的/ 中心/ 向量/ 能够/ 稳定/ ./ 文献/ [/ 23/ ,/ 29/ ]/ 给出/ 了/ 引理/ 1/ 的/ 证明/ ./ 根据/ 引理/ 1/ 所/ 给出/ 的/ 策略/ ,/ Q/ -/ V/ 逼近/ 模型/ 选择/ 隐藏/ 层/ 激活/ 函数/ 的/ 中心/ 向量/ ,/ 则/ 可以/ 保证/ 模型/ 的/ 隐藏/ 层/ 结点/ 数及/ 中心/ 向量/ 能够/ 稳定/ ./ 基于/ 引理/ 1/ 及/ 定理/ 1/ ,/ 本文/ 给出/ 了/ 定理/ 2/ ,/ 其/ 证明/ 在/ Q/ -/ V/ 逼近/ 模型/ 的/ 隐藏/ 层/ 结构/ 趋于稳定/ 的/ 情况/ 下/ ,/ QV/ (/ λ/ )/ 算法/ 能够/ 收敛/ ./ 定理/ 2/ ./ 假设/ Q/ -/ V/ 逼近/ 模型/ 的/ 隐藏/ 层/ 趋于稳定/ ,/ 在/ 确定性/ MDP/ 中/ ,/ 若/ 算法/ 具有/ 相同/ 的/ 样本/ 序列/ ,/ 则/ 根据/ C/ -/ TDE/ 更新/ 值/ 函数/ 的/ QV/ (/ λ/ )/ 算法/ 与/ 利用/ TDE/ 结合/ V/ 值/ 函数/ 进行/ 更新/ 的/ GD/ -/ Sarsa/ (/ λ/ )/ 算法/ 具有/ 相同/ 的/ 收敛性/ ./ 证明/ ./ 考虑/ 具有/ 连续/ 状态/ 空间/ X/ 和/ 离散/ 动作/ 空间/ U/ 的/ 强化/ 学习/ 问题/ ,/ 分别/ 用/ L/ 和/ L/ 表示/ GD/ -/ Sarsa/ (/ λ/ )/ 算法/ 和/ QV/ (/ λ/ )/ 算法/ 的/ 学习/ 器/ ./ / 狓/ ∈/ X/ ,/ 狌/ ∈/ U/ ,/ Q/ (/ 狓/ ,/ 狌/ )/ =/ 狑/ TQ0/ (/ 狓/ ,/ 狌/ )/ +/ V/ (/ 狓/ )/ =/ ω/ TQ/ (/ 狓/ ,/ 狌/ )/ =/ ω/ TQ/ (/ 狓/ ,/ 狌/ )/ =/ Q0/ (/ 狓/ ,/ 狌/ )/ =/ ω/ TQ/ (/ 狓/ ,/ 狌/ )/ 表示/ QID/ (/ 狌/ )/ (/ 狓/ )/ ./ 假设/ 在/ 状态/ 狓/ 下/ 采取/ 动作/ 狌/ 转移/ 到/ 状态/ 狓/ ,/ 立即/ 奖赏/ 为/ 狉/ ,/ 在/ 狓/ 下/ 根据/ 行为/ 策略/ 选择/ 动作/ 狌/ ,/ 则/ 该/ 样本/ 数据/ 可以/ 用/ 五元/ 组/ 〈/ 狓/ ,/ 狌/ ,/ 狓/ ,/ 狉/ ,/ 狌/ 〉/ 表示/ ./ 基于/ 此/ 五元/ 组/ 样本/ 数据/ ,/ L/ 和/ L/ 分别/ 更新/ 权值/ ,/ 更新/ 公式/ 如式/ (/ 11/ )/ 和/ 式/ (/ 12/ )/ :/ 其中/ δ/ Q/ (/ 狓/ ,/ 狌/ )/ =/ 狉/ +/ γ/ Q/ (/ 狓/ ,/ 狌/ )/ -/ Q/ (/ 狓/ ,/ 狌/ )/ 为/ TDE/ ,/ δ/ Q/ (/ 狓/ ,/ 狌/ )/ =/ 狉/ +/ γ/ Q/ (/ 狓/ ,/ 狌/ )/ -/ Q/ (/ 狓/ ,/ 狌/ )/ +/ F/ (/ 狓/ ,/ 狌/ ,/ 狓/ )/ 为/ C/ -/ TDE/ ./ 犲/ ID/ (/ 狌/ )/ 和/ 犲/ ID/ (/ 狌/ )/ 分别/ 是/ 值/ 函数/ 模型/ 中/ 的/ 第/ ID/ (/ 狌/ )/ 个子/ 模型/ 的/ 资格/ 迹/ ./ 状态/ 动作/ 对/ 〈/ 狓/ ,/ 狌/ 〉/ 先后/ 两次/ 值/ 函数/ 的/ 差/ 分别/ 记/ 为/ Δ/ Q/ (/ 狓/ ,/ 狌/ )/ ,/ Δ/ Q/ (/ 狓/ ,/ 狌/ )/ ./ 根据/ 式/ (/ 11/ )/ 和/ 式/ (/ 12/ )/ 的/ 权值/ 更新/ 调整/ Q/ 值/ 函数/ 的/ 更新/ ,/ Q/ 值/ 函数/ 的/ 更新/ 量计算/ 如式/ (/ 13/ )/ 和/ 式/ (/ 14/ )/ :/ 给定/ 相同/ 的/ 学习/ 样本/ 序列/ ,/ 接下来/ 利用/ 归纳法/ 证明/ L/ 和/ L/ 的/ 等价/ 性/ ,/ 即/ 证明/ / 狓/ ∈/ X/ ,/ 狌/ ∈/ U/ ,/ Δ/ Q/ (/ 狓/ ,/ 狌/ )/ =/ Δ/ Q/ (/ 狓/ ,/ 狌/ )/ ./ 证明/ 过程/ 如下/ :/ ①/ 当/ Q/ (/ 狓/ ,/ 狌/ )/ 和/ Q/ (/ 狓/ ,/ 狌/ )/ 都/ 是/ 初始值/ 时/ ,/ 则/ Page7/ Δ/ Q/ (/ 狓/ ,/ 狌/ )/ =/ Δ/ Q/ (/ 狓/ ,/ 狌/ )/ =/ 0/ ;/ ②/ 假设/ 当前/ 存在/ / 狓/ ∈/ X/ ,/ 狌/ ∈/ U/ ,/ Δ/ Q/ (/ 狓/ ,/ 狌/ )/ =/ Δ/ Q/ (/ 狓/ ,/ 狌/ )/ ,/ 则/ 在/ 此基础/ 上/ ,/ 根据/ 新/ 的/ 样本/ 数据/ 〈/ 狓/ ,/ 狌/ ,/ 狓/ ,/ 狉/ ,/ 狌/ 〉/ ,/ 利用/ TDE/ 及/ C/ -/ TDE/ 可得/ δ/ Q/ (/ 狓/ ,/ 狌/ )/ =/ 狉/ +/ γ/ Q/ (/ 狓/ ,/ 狌/ )/ -/ Q/ (/ 狓/ ,/ 狌/ )/ δ/ Q/ (/ 狓/ ,/ 狌/ )/ =/ 狉/ +/ γ/ Q/ (/ 狓/ ,/ 狌/ )/ -/ Q/ (/ 狓/ ,/ 狌/ )/ +/ F/ (/ 狓/ ,/ 狌/ ,/ 狓/ )/ 根据上述/ 计算/ ,/ 可/ 得/ δ/ Q/ (/ 狓/ ,/ 狌/ )/ =/ δ/ Q/ (/ 狓/ ,/ 狌/ )/ ,/ 即/ 在/ 新/ 样本/ 数据/ 下/ ,/ L/ 和/ L/ 所/ 计算/ 的/ TDE/ 和/ C/ -/ TDE/ 相同/ ./ 此外/ ,/ 由于/ 具有/ 相同/ 的/ 样本/ 序列/ ,/ L/ 和/ L/ 中/ 资格/ 迹/ 更新/ 也/ 是/ 相同/ 的/ ,/ 又/ 根据/ 每个/ 时间/ 步/ L/ 和/ L/ 获得/ 相同/ 的/ TDE/ 和/ C/ -/ TDE/ ,/ 因此/ ,/ L/ 和/ L/ 资格/ 迹/ 在/ 每个/ 时间/ 步/ 具有/ 相同/ 的/ 更新/ ,/ 即/ 对于/ 任/ 一时间/ 步/ ,/ 犲/ ID/ (/ 狌/ )/ =/ 犲/ ID/ (/ 狌/ )/ ./ 再/ 根据/ 式/ (/ 13/ )/ 和/ 式/ (/ 14/ )/ 可/ 得/ ,/ / 狓/ ∈/ X/ ,/ 狌/ ∈/ U/ ,/ Δ/ Q/ (/ 狓/ ,/ 狌/ )/ =/ Δ/ Q/ (/ 狓/ ,/ 狌/ )/ ./ 综上所述/ ,/ 在/ 相同/ 的/ 样本/ 序列/ 下/ ,/ L/ 和/ L/ 等价/ ,/ 即/ QV/ (/ λ/ )/ 算法/ 与/ GD/ -/ Sarsa/ (/ λ/ )/ 算法/ 具有/ 一致/ 的/ 收敛性/ ./ 定理/ 3/ ./ 在/ 式/ (/ 15/ )/ 所/ 给/ 的/ 条件/ 下/ ,/ 如果/ 学习/ 率/ α/ 随/ 时间/ 衰减/ ,/ 则/ 利用/ 梯度/ 下降/ 方法/ 更新/ 参数/ 的/ QV/ (/ λ/ )/ 算法/ 能够/ 收敛/ 至/ 一个/ 局部/ 最优/ 解/ ./ 证明/ ./ 根据/ 文献/ [/ 10/ ]/ 得知/ ,/ 采用/ 梯度/ 下降/ 方法/ 的/ 线性/ 学习/ 算法/ ,/ 例如/ Sutton/ 等/ 人/ 提出/ 的/ GD/ -/ Sarsa/ (/ λ/ )/ 算法/ 在/ 满足/ 式/ (/ 15/ )/ 的/ 条件/ 下/ ,/ 如果/ 学习/ 率/ α/ 随/ 时间/ 衰减/ ,/ 则/ 能够/ 保证/ 该/ 算法/ 能/ 收敛/ 至/ 一个/ 局部/ 最优/ 解/ ./ 又/ 根据/ 定理/ 2/ ,/ 在/ 具有/ 相同/ 样本/ 序列/ 的/ 情况/ 下/ ,/ QV/ (/ λ/ )/ 算法/ 与/ GD/ -/ Sarsa/ (/ λ/ )/ 算法/ 具有/ 一致/ 的/ 收敛性/ ./ 而/ 在/ 强化/ 学习/ 算法/ 中/ ,/ 任意/ 初始化/ 值/ 函数/ 都/ 不会/ 影响/ 算法/ 的/ 收敛/ 结果/ ,/ 因此/ ,/ 在/ 学习/ 率/ α/ 满足/ 式/ (/ 15/ )/ 且/ 随/ 时间/ 衰减/ 的/ 情况/ 下/ ,/ QV/ (/ λ/ )/ 算法/ 一定/ 能够/ 收敛/ 至/ 一个/ 局部/ 最优/ 解/ ./ 5/ 实验/ 及/ 结果/ 分析/ 5.1/ 实验/ 描述/ 及/ 设置/ MountainCar/ 问题/ 是/ 一个/ 经典/ 的/ 具有/ 连续/ 状态/ 空间/ 、/ 离散/ 动作/ 空间/ 的/ 强化/ 学习/ 问题/ ,/ 如图/ 2/ 所示/ ./ 图/ 2/ 描述/ 的/ 是/ 一个/ 动力/ 不足/ 的/ 小车/ ,/ 在/ 山谷/ 中/ 的/ 任何/ 一点/ 通过/ 前后/ 摆动/ 的/ 方式/ 最快/ 到达/ 山顶/ 的/ G/ 点/ 的/ 问题/ ./ 其中/ 状态/ 定义/ 如式/ (/ 16/ )/ :/ 其中/ y/ 为/ 小车/ 的/ 水平/ 位移/ (/ -/ 1.2/ / y/ / 0.5/ )/ ,/ v/ 为/ 小车/ 的/ 水平/ 速度/ (/ -/ 0.07/ / v/ / 0.07/ )/ ./ 系统/ 的/ 动力学/ 特性/ 如式/ (/ 17/ )/ :/ 其中/ g/ =/ 0.0025/ 为/ 与/ 重力/ 有关/ 的/ 常数/ ,/ 狌/ 为/ 控制/ 动作/ ,/ 取/ 3/ 个/ 离散/ 值/ ,/ +/ 1/ ,/ 0/ 和/ -/ 1/ ,/ 分别/ 代表/ 全/ 油门/ 向前/ 、/ 零/ 油门/ 和/ 全/ 油门/ 向/ 后/ ./ 另外/ ,/ 小车/ 运动/ 过程/ 中/ 的/ 奖赏/ 函数/ 如式/ (/ 18/ )/ :/ 其中/ 下标/ t/ 表示/ 时间/ 步/ ./ 实验/ 采用/ ε/ -/ greedy/ 作为/ 行为/ 策略/ [/ 1/ ]/ ,/ 每次/ 实验/ 的/ 情节/ 数为/ 1000/ ,/ 每个/ 情节/ 的/ 最大/ 时间/ 步数/ 为/ 1000/ ./ 每次/ 实验/ 随机/ 生成/ 小车/ 的/ 初始状态/ ,/ 当/ 小车/ 到达/ G/ 点/ 或者/ 时间/ 步数/ 超过/ 1000/ 时/ ,/ 情节/ 结束/ ,/ 并/ 开始/ 下/ 一个/ 情节/ 的/ 学习/ ./ 算法/ 的/ 性能指标/ 包括/ 3/ 个/ 方面/ :/ (/ 1/ )/ 收敛/ 速度/ ,/ 即/ 算法/ 能够/ 在/ 多少/ 个/ 情节/ 内/ 收敛/ ;/ (/ 2/ )/ 收敛/ 结果/ ,/ 即/ 算法/ 收敛/ 后/ ,/ 小车/ 从/ 初始/ 点/ 到达/ 目标/ 点/ G/ 所用/ 的/ 平均/ 时间/ 步/ ;/ (/ 3/ )/ 初始/ 性能/ ,/ 即/ 每次/ 实验/ 的/ 前/ 20/ 个/ 情节/ 中/ ,/ 小车/ 从/ 初始/ 点/ 到达/ 目标/ 点/ G/ 所用/ 的/ 平均/ 时间/ 步/ ./ 将/ 本文/ 所提/ 的/ QV/ (/ λ/ )/ 算法/ 与/ GD/ -/ Sarsa/ (/ λ/ )/ 算法/ [/ 1/ ]/ ,/ Greedy/ -/ GQ/ 算法/ [/ 17/ ]/ 以及/ RGD/ -/ Sarsa/ (/ λ/ )/ 算法/ [/ 26/ ]/ 在/ MountainCar/ 问题/ 中/ 进行/ 对比/ 分析/ ./ 并/ 着重/ 分析/ 资格/ 迹/ 、/ 学习/ 率/ 、/ SR/ -/ SVF/ 及/ 自/ 适应/ 机制/ 对/ 算法/ 性能/ 的/ 影响/ ./ Page85/ ./ 2/ 实验/ 分析/ 在/ QV/ (/ λ/ )/ 算法/ 及/ RGD/ -/ Sarsa/ (/ λ/ )/ 算法/ 中/ ,/ 分别/ 采用/ 8/ 个/ 等距/ 的/ 高斯/ RBFs/ 来/ 对/ 二维/ 连续/ 状态/ 空间/ 的/ 每/ 一维/ 进行/ 划分/ ,/ RBFs/ 的/ 数量/ 为/ 8/ ×/ 8/ =/ 64/ ,/ 宽度/ 向量/ 为/ σ/ =/ [/ 0.1/ ,/ 0.1/ ]/ T/ ./ 另外/ ε/ =/ 0.0/ ,/ α/ =/ 0.9/ ,/ λ/ =/ 0.9/ ,/ γ/ =/ 1.0/ ./ 在/ GD/ -/ Sarsa/ (/ λ/ )/ 中/ ,/ 采用/ 10/ 个/ 9/ ×/ 9/ 的/ Tilings/ 来/ 划分/ 状态/ 空间/ ,/ 并/ 利用/ 文献/ [/ 1/ ]/ 中/ 给出/ 的/ 最优/ 参数/ :/ 另外/ ,/ ε/ =/ 0.0/ ,/ α/ =/ 0.14/ ,/ γ/ =/ 1.0/ ,/ 在/ 采用/ 替代/ 迹/ 的/ 情况/ 下/ ,/ λ/ =/ 0.9/ ,/ 在/ 采用/ 累加/ 迹/ 的/ 情况/ 下/ ,/ λ/ =/ 0.3/ ./ 在/ Greedy/ -/ GQ/ 算法/ 中/ ,/ ε/ =/ 0.0/ ,/ α/ =/ 0.1/ ,/ γ/ =/ 1.0/ ./ 从图/ 3/ 和/ 图/ 4/ 中/ 可以/ 看出/ ,/ QV/ (/ λ/ )/ 算法/ 在/ 收敛/ 速度/ 和/ 初始/ 性能/ 上/ 明显/ 优于/ 其他/ 3/ 个/ 算法/ ./ 图/ 4/ 算法/ 性能/ 比较/ (/ 续/ )/ 采用/ 上述/ 给出/ 的/ RBFs/ 配置/ ,/ ε/ =/ 0.0/ ,/ α/ =/ 0.9/ ,/ λ/ =/ 0.9/ ,/ γ/ =/ 0.99/ ./ 分别/ 考虑/ 两个/ 不同/ 的/ 立即/ 奖赏/ 模型/ :/ (/ 1/ )/ 根据/ 式/ (/ 18/ )/ 给出/ 的/ 奖赏/ 模型/ ;/ (/ 2/ )/ 奖赏/ 模型/ 为/ :/ 当/ y/ </ 0.5/ 时/ ,/ 狉/ t/ =/ 0/ ;/ 当/ y/ / 0.5/ 时/ ,/ 狉/ t/ =/ 1/ ./ 实验/ 结果/ 如图/ 5/ 所示/ ,/ 左图/ 为/ 第/ 1/ 种/ 奖赏/ 设置/ 下/ 的/ 算法/ 执行/ 结果/ ,/ 右图/ 为/ 第/ 2/ 种/ 奖赏/ 设置/ 下/ 的/ 算法/ 执行/ 结果/ ./ 从图/ 5/ 可以/ 看出/ ,/ 带有/ SR/ -/ SVF/ 机制/ 的/ QV/ (/ λ/ )/ 算法/ 在/ 收敛/ 速度/ 和/ 初始/ 性能/ 方面/ 明显/ 优于/ 不带/ SR/ -/ SVF/ 图/ 5SR/ -/ SVF/ 机制/ 对/ QV/ (/ λ/ )/ 算法/ 的/ 性能/ 影响/ 分析/ 机制/ 的/ QV/ (/ λ/ )/ 算法/ ./ 在/ 采用/ 第/ 1/ 种/ 奖赏/ 模型/ 的/ 情况/ 下/ ,/ 由于/ 奖赏/ 是/ -/ 1/ 和/ 0/ ,/ 是/ 一种/ 惩罚/ 型/ 奖赏/ ,/ SR/ -/ SVF/ 机制/ 对/ 算法/ 收敛/ 速度/ 和/ 初始/ 性能/ 影响/ 较/ 小/ ;/ 当/ 采用/ 第/ 2/ 种/ 奖赏/ 模型/ 时/ ,/ 由于/ 奖赏/ 是/ 0/ 和/ 1/ ,/ 是/ 一种/ 鼓励/ 型/ 奖赏/ ,/ 只有/ 到达/ 目标/ 状态/ 时/ ,/ 才能/ 有益于/ 算法/ 的/ 学习/ ,/ 因此/ SR/ -/ SVF/ 机制/ 可以/ 在/ 学习/ 初期/ 通过/ 环境/ 模型/ 知识/ 构造/ 塑造/ 奖赏/ ,/ 并/ 传递/ 给/ 学习/ 器/ ,/ 从而/ 能够/ 有效/ 地/ 提高/ 算法/ 的/ 收敛/ 速度/ 和/ 初始/ 性能/ ./ Page9/ 下面/ 重点/ 分析/ 学习/ 率/ α/ 对/ 算法/ 性能/ 的/ 影响/ ./ 在/ QV/ (/ λ/ )/ 算法/ 中/ ,/ 本文/ 定义/ 了/ 新/ 的/ 资格/ 迹/ ,/ λ/ =/ 0.9/ ,/ α/ 分别/ 取值/ 0.1/ ,/ 0.5/ ,/ 0.9/ ,/ 1.0/ ;/ 在/ GD/ -/ Sarsa/ (/ λ/ )/ 算法/ 中/ ,/ 采用/ 替代/ 迹/ ,/ λ/ =/ 0.9/ ,/ α/ 分别/ 取值/ 0.01/ ,/ 0.05/ ,/ 0.14/ ,/ 0.18/ ,/ 其余/ 参数设置/ 不变/ ;/ 在/ Greedy/ -/ GQ/ 算法/ 中/ ,/ α/ 的/ 取值/ 分别/ 为/ 0.005/ ,/ 0.015/ ,/ 0.05/ ,/ 0.1/ ,/ 其余/ 参数/ 不图/ 6/ 算法/ 关于/ 学习/ 率/ 的/ 敏感度/ 分析/ 表/ 1/ 给出/ 了/ QV/ (/ λ/ )/ 算法/ 的/ 平均/ 性能/ ./ 当/ RBFs/ 设置/ 为/ 5/ ×/ 5/ =/ 25/ 时/ ,/ 带有/ 自/ 适应/ 机制/ 的/ QV/ (/ λ/ )/ 算法/ 大约/ 需要/ 229/ 个/ 情节/ 收敛/ ,/ 且/ 收敛/ 后/ 大约/ 需要/ 73/ 个/ 时间/ 步/ 到达/ 目标/ 点/ ,/ 而/ 不/ 带/ 自/ 适应/ 机制/ 的/ QV/ (/ λ/ )/ 算法/ 收敛/ 后/ 大约/ 需要/ 275/ 个/ 情节/ 收敛/ ,/ 且/ 收敛/ 后/ 大约/ 需要/ 142/ 个/ 时间/ 步/ 到达/ 目标/ 点/ ;/ 当/ RBFs/ 设置/ 为表/ 1/ 自/ 适应/ 特性/ 对/ 犙/ 犞/ (/ λ/ )/ 算法/ 的/ 性能/ 影响/ 分析/ 275/ ±/ 15142/ ±/ 15/ 收敛/ 情节/ 数/ 平均/ 时间/ 步/ 6/ 结束语/ 本文/ 利用/ ANRBF/ 网络/ 将/ 状态/ 空间/ 映射/ 到/ 高维/ 特征/ 空间/ ,/ 通过/ 一组/ 高斯/ 基/ 函数/ 的/ 线性组合/ 构建/ Q/ -/ V/ 值/ 函数/ 协同/ 逼近/ 模型/ ./ 该/ 模型/ 既/ 具有/ 非线性/ 逼近/ 模型/ 的/ 强/ 表达能力/ ,/ 又/ 具有/ 线性/ 逼近/ 模型/ 的/ 简单/ 性及/ 良好/ 的/ 收敛性/ ./ ANRBF/ 网络/ 可以/ 有效/ 地/ 提高/ 算法/ 的/ 鲁棒性/ 和/ 灵活性/ ,/ 从而/ 在/ 一定/ 程度/ 上/ 解决/ 了/ 强化/ 学习/ 逼近/ 模型/ 所/ 面临/ 的/ “/ 灾难性/ 扰动/ ”/ 问题/ ./ 通过/ Q/ 值/ 函数/ 和/ V/ 值/ 函数/ 构造/ 协同/ TD/ 误差/ ,/ 从而/ 有效/ 地/ 提高/ 算法/ 的/ 收敛/ 速度/ 和/ 初始/ 性能/ ./ 此外/ ,/ 本文/ 定变/ ;/ 在/ RGD/ -/ Sarsa/ (/ λ/ )/ 算法/ 中/ ,/ 采用/ 替代/ 迹/ ,/ λ/ =/ 0.9/ ,/ α/ 的/ 取值/ 分别/ 为/ 0.1/ ,/ 0.5/ ,/ 0.9/ ,/ 1.0/ ,/ 其余/ 参数/ 不变/ ./ 实验/ 结果/ 如图/ 6/ 所示/ ,/ 与/ 基于/ 线性/ 函数/ 近似/ 的/ GD/ -/ Sarsa/ (/ λ/ )/ 、/ Greedy/ -/ GQ/ 算法/ 以及/ 基于/ 非参/ 函数/ 近似/ 的/ RGD/ -/ Sarsa/ (/ λ/ )/ 算法/ 相比/ ,/ 本文/ 所/ 给出/ 的/ QV/ (/ λ/ )/ 算法/ 对于/ 不同/ 学习/ 率/ 具有/ 较强/ 的/ 鲁棒性/ ./ 3/ ×/ 3/ =/ 9/ 时/ ,/ 带有/ 自/ 适应/ 机制/ 的/ QV/ (/ λ/ )/ 算法/ 大约/ 需要/ 252/ 个/ 情节/ 收敛/ ,/ 且/ 收敛/ 后/ 大约/ 需要/ 75/ 个/ 时间/ 步/ 到达/ 目标/ 点/ ,/ 而/ 不/ 带/ 自/ 适应/ 机制/ 的/ QV/ (/ λ/ )/ 算法/ 大约/ 需要/ 269/ 个/ 情节/ 收敛/ ,/ 且/ 收敛/ 后/ 大约/ 需要/ 276/ 个/ 时间/ 步/ 到达/ 目标/ 点/ ./ 义/ 了/ 一种/ 新/ 的/ 资格/ 迹/ 更新/ 方法/ ,/ 并/ 用来/ 处理/ 基于/ 连续/ 编码/ 的/ Q/ -/ V/ 值/ 函数/ 协同/ 逼近/ 模型/ 的/ 时间/ 信度/ 分配/ 问题/ ,/ 实验/ 结果表明/ 具有/ 较优/ 的/ 效果/ ./ 考虑/ 到/ 强化/ 学习/ 的/ 自/ 学习/ 和/ 在线/ 学习/ 特性/ ,/ 如何/ 设计/ 有效/ 的/ 逼近/ 模型/ 来/ 更好/ 的/ 适应/ 和/ 处理/ 在线/ 强化/ 学习/ 问题/ ,/ 有待/ 进一步/ 研究/ ./ 

