Page1/ 基于/ 连续/ 时间/ 半/ 马尔可夫/ 决策/ 过程/ 的/ Option/ 算法/ 唐昊/ 1/ )/ ,/ 2/ )/ 张晓艳/ 1/ )/ 韩江/ 洪/ 1/ )/ 周雷/ 1/ )/ 1/ )/ (/ 合肥工业大学/ 计算机/ 与/ 信息/ 学院/ 合肥/ 230009/ )/ 2/ )/ (/ 合肥工业大学/ 电气/ 与/ 自动化/ 工程学院/ 合肥/ 230009/ )/ 摘要/ 针对/ 大规模/ 或/ 复杂/ 的/ 随机/ 动态/ 规划系统/ ,/ 可/ 利用/ 其/ 分层/ 结构/ 特点/ 或/ 引入/ 分层/ 控制/ 方式/ ,/ 借助/ 分层/ 强化/ 学习/ (/ HierarchicalReinforcementLearning/ ,/ HRL/ )/ 来/ 解决/ 其/ “/ 维数灾/ ”/ 和/ “/ 建模/ 难/ ”/ 问题/ ./ HRL/ 归属于/ 样本/ 数据/ 驱动/ 优化/ 方法/ ,/ 通过/ 空间/ // 时间/ 抽象/ 机制/ ,/ 可/ 有效/ 加速/ 策略/ 学习/ 过程/ ./ 其中/ ,/ Option/ 方法/ 可/ 将/ 系统/ 目标/ 任务/ 分解成/ 多个/ 子目标/ 任务/ 来/ 学习/ 和/ 执行/ ,/ 层次化/ 结构/ 清晰/ ,/ 是/ 具有/ 代表性/ 的/ HRL/ 方法/ 之一/ ./ 传统/ 的/ Option/ 算法/ 主要/ 是/ 建立/ 在/ 离散/ 时间/ 半/ 马尔可夫/ 决策/ 过程/ (/ Semi/ -/ MarkovDecisionProcesses/ ,/ SMDP/ )/ 和/ 折扣/ 性能/ 准则/ 基础/ 上/ ,/ 无法/ 直接/ 用于/ 解决/ 连续/ 时间/ 无穷/ 任务/ 问题/ ./ 因此/ 本文/ 在/ 连续/ 时间/ SMDP/ 框架/ 及其/ 性能/ 势/ 理论/ 下/ ,/ 结合/ 现有/ 的/ Option/ 算法/ 思想/ ,/ 运用/ 连续/ 时间/ SMDP/ 的/ 相关/ 学习/ 公式/ ,/ 建立/ 一种/ 适用/ 于/ 平均/ 或/ 折扣/ 性能/ 准则/ 的/ 连续/ 时间/ 统一/ Option/ 分层/ 强化/ 学习/ 模型/ ,/ 并/ 给出/ 相应/ 的/ 在线/ 学习/ 优化/ 算法/ ./ 最后/ 通过/ 机器人/ 垃圾/ 收集/ 系统/ 为/ 仿真/ 实例/ ,/ 说明/ 了/ 这种/ HRL/ 算法/ 在/ 解决/ 连续/ 时间/ 无穷/ 任务/ 优化/ 控制/ 问题/ 方面/ 的/ 有效性/ ,/ 同时/ 也/ 说明/ 其/ 与/ 连续/ 时间/ 模拟退火/ Q/ 学习/ 相比/ ,/ 具有/ 节约/ 存储空间/ 、/ 优化/ 精度高/ 和/ 优化/ 速度/ 快/ 的/ 优势/ ./ 关键词/ 连续/ 时间/ 半/ Markov/ 决策/ 过程/ ;/ 分层/ 强化/ 学习/ ;/ Q/ 学习/ 1/ 引言/ 强化/ 学习/ 集/ 仿真/ 、/ 统计/ 、/ 学习/ 、/ 逼近/ 等/ 技术/ 于/ 一体/ ,/ 是/ 解决/ 随机/ 离散/ 事件/ 动态/ 系统/ (/ DEDS/ )/ 的/ 一个/ 重要/ 人工智能/ 方法/ [/ 1/ ]/ ./ 当前/ DEDS/ 的/ 学习/ 优化/ 主要/ 是/ 基于/ 状态/ 变化/ 的/ 方法/ ,/ 需要/ 考虑/ 整个/ 系统/ 每个/ 组成部分/ 的/ 详细/ 状态/ 信息/ ,/ 而待/ 学习/ 参数/ 的/ 个数/ 随/ 状态变量/ 维数/ 增长/ 成/ 指数/ 级/ 增长/ ,/ 对于/ 大规模/ 系统/ ,/ 这/ 将/ 导致/ “/ 维数灾/ ”/ 难题/ ,/ 因而/ 耗费/ 巨大/ 存储空间/ 和/ 计算/ 时间/ ,/ 影响/ 算法/ 的/ 实时性/ ,/ 制约/ 在线/ 学习/ 优化/ 方法/ 的/ 实际/ 应用/ ./ 实际/ 中/ ,/ 有些/ 系统/ 可能/ 由/ 各个/ 相互/ 联系/ 的/ 功能模块/ (/ 或子/ 决策者/ )/ 构成/ ,/ 结构/ 上/ 具有/ 分层/ 嵌套/ 等/ 特点/ ;/ 另一方面/ ,/ 为了/ 克服/ “/ 维数灾/ ”/ ,/ 复杂/ 系统/ 的/ 研究/ 有时/ 需要/ 对/ 问题/ 进行/ 分解/ 和/ 抽象/ ,/ 控制/ 方式/ 上/ 具有/ 分层/ 特点/ ./ 这时/ ,/ 系统/ 一般/ 只/ 在/ 特定/ 事件/ 发生/ 而/ 需要/ 执行/ 子/ 任务/ 或宏/ 行动/ 时才/ 触发/ 决策/ ./ 这种/ 基于/ 特定/ 事件/ 发生/ 的/ 分层/ 决策/ 方法/ ,/ 更/ 符合/ DEDS/ 的/ 特点/ ,/ 可以/ 降低/ 问题/ 研究所/ 需/ 的/ 状态/ 空间/ 和/ 策略/ 空间/ 的/ 复杂度/ 以及/ 计算/ 量/ ,/ 有利于/ 系统/ 的/ 在线/ 优化/ ./ 强化/ 学习/ 与/ 分层/ 结构/ 或/ 控制/ 相结合/ ,/ 产生/ 了/ 分层/ 强化/ 学习/ (/ HRL/ )/ 方法/ ./ HRL/ 是/ 对/ 学习/ 过程/ 进行/ 时间/ 抽象/ (/ temporalabstraction/ )/ 或/ 空间/ 抽象/ (/ spa/ -/ tialabstraction/ )/ ,/ 通过/ 引入/ 抽象/ 机制/ 将/ 学习/ 任务/ 分解/ 到/ 不同/ 层次/ 上/ 分别/ 实现/ ,/ 每层/ 上/ 的/ 学习/ 任务/ 仅/ 对应/ 较/ 小/ 的/ 状态/ 空间/ ,/ 因而/ 可/ 一定/ 程度/ 上/ 简化/ 策略/ 的/ 执行/ 空间/ ,/ 克服/ 大规模/ 系统/ 的/ “/ 维数灾/ ”/ 难题/ [/ 2/ -/ 3/ ]/ ./ 最/ 具有/ 代表性/ 的/ HRL/ 方法/ 主要/ 有/ Option/ 、/ HAM/ 和/ Max/ -/ Q/ [/ 4/ -/ 11/ ]/ ,/ 这/ 3/ 种/ 方法/ 的/ 理论/ 基础/ 都/ 是/ SMDP/ 理论/ ,/ 都/ 是/ 对/ 行动/ 的/ 抽象/ ,/ 但/ Option/ 和/ HAM/ 方法/ 转换/ 后/ 的/ 模型/ 为/ 单个/ SMDP/ ,/ 而/ Max/ -/ Q/ 方法/ 创建/ 了/ 多个/ SMDP/ 分层/ 并/ 同时/ 进行/ 学习/ ./ 另外/ ,/ Option/ 和/ HAM/ 方法/ 的/ 策略/ 收敛/ 类型/ 一般/ 是/ 分层/ 最优/ ,/ 而/ Max/ -/ Q/ 方法/ 由于/ 隔离/ 了/ 子/ 任务/ 环境/ ,/ 允许/ 使用/ 安全/ 状态/ 抽象/ ,/ 故/ 为/ 较弱/ 的/ 递归/ 最优/ [/ 12/ -/ 13/ ]/ ./ 其中/ ,/ Option/ 方法/ 通过/ 引入/ 宏/ 行动/ 概念/ ,/ 可以/ 显著/ 地/ 改变/ Agent/ 的/ 学习/ 效率/ ,/ 因而/ 获得/ 了/ 较为/ 广泛/ 的/ 应用/ ./ 当/ 考虑/ 分层/ 结构/ 或/ 分层/ 控制/ 时/ ,/ DEDS/ 的/ 决策/ 机制/ 往往/ 是/ 半/ Markov/ 型/ 的/ ,/ 例如/ ,/ Option/ 方法/ 中/ ,/ 即便/ 每个/ Option/ 是/ 马尔可夫/ 型/ 的/ ,/ 定义/ 在/ 这些/ Option/ 之上/ 的/ 决策/ 过程/ 也/ 是/ 半/ 马尔可夫/ 型/ 的/ ./ 因此/ ,/ 分层/ 强化/ 学习/ 的/ 数学/ 基础/ 是/ 半/ 马尔可夫/ 决策/ 过程/ (/ SMDP/ )/ 理论/ ,/ 但/ 国内外/ 的/ 相关/ 研究/ 主要/ 是/ 面向/ 离散/ 时间/ SMDP/ [/ 2/ ]/ ,/ 即/ 学习/ 过程/ 主要/ 关心/ 系统/ 随机/ 转移/ 的/ 离散/ 序列/ 信息/ ,/ 而/ 忽略/ 报酬/ 的/ 时间/ 累积/ 效应/ ./ 但是/ ,/ 在/ 实际/ 环境/ 中/ ,/ 状态/ 随机/ 转移/ 需要/ 一定/ 的/ 时间/ ,/ 系统/ 一般/ 将会/ 产生/ 与/ 该/ 时间/ 有关/ 的/ 报酬/ ./ 因此/ ,/ 系统/ 运行/ 的/ 性能/ 报酬/ 往往/ 不仅/ 与/ 决策/ 点/ 的/ 状态/ 和/ 行动/ 有关/ ,/ 还/ 与/ 相邻/ 决策/ 点/ 之间/ 的/ 状态/ 转移/ 过程/ 有关/ ,/ 而且/ 可能/ 是/ 按/ 时间/ 分段/ 累积/ 的/ ,/ 即/ 存在/ 按/ 时间/ 累积/ 报酬/ ./ 尽管/ 诸多/ 研究/ 把/ 决策/ 点/ 之间/ 的/ 按/ 时间/ 累积/ 报酬/ 的/ 期望值/ 等效/ 为/ 决策/ 点/ 的/ 即时/ 报酬/ ,/ 但/ 受/ “/ 建模/ 难/ ”/ 和/ “/ 维数灾/ ”/ 的/ 制约/ ,/ 这种/ 处理/ 有时/ 实际/ 不/ 可行/ ,/ 无法/ 用于/ 在线/ 学习/ 优化/ ./ Ghavamzadeh/ 和/ Mahadevan/ 基于/ 连续/ 时间/ SMDP/ ,/ 在/ 离散/ 时间/ MAXQ/ 算法/ 基础/ 上/ ,/ 分别/ 研究/ 了/ 平均/ 和/ 折扣/ 准则/ 下/ 的/ 连续/ 时间/ Max/ -/ Q/ 算法/ [/ 5/ ]/ ,/ 但是/ 面向/ 连续/ 时间/ SMDP/ 的/ Option/ 方法/ ,/ 还/ 留有/ 诸多/ 问题/ 有待/ 深入研究/ 和/ 发展/ ./ Option/ 算法/ 的/ 特点/ 是/ 把/ 目标/ 划分/ 为/ 多个/ 子目标/ ,/ 并且/ 只/ 在/ 子目标/ 点/ 进行/ 决策/ ,/ 其它/ 时刻/ 按照/ Option/ 已经/ 学习/ 好/ 的/ 策略/ 或/ 根据/ 先验/ 知识/ 确定/ 的/ 策略/ 执行/ ./ 换言之/ ,/ Option/ 方法/ 的/ 策略/ 可/ 分为/ 宏观/ 、/ 微观/ 两个/ 层面/ ,/ 而/ 微观/ 策略/ 可/ 由/ 专家/ 直接/ 设计/ 确定/ 或/ 使用/ 现成/ 的/ 策略/ ,/ 使得/ 学习/ 任务/ 只是/ 优化/ 宏观/ 策略/ ,/ 因而/ 简单易行/ ./ 但是/ ,/ 现有/ HRL/ 方法/ 中/ 的/ Option/ 算法/ 主要/ 建立/ 在/ 离散/ 时间/ 折扣/ 准则/ SMDP/ 模型/ 上/ [/ 12/ ,/ 14/ ]/ ,/ 不能/ 直接/ 用于/ 解决/ 连续/ 时间/ 大规模/ 无穷/ 任务/ 问题/ ./ 文献/ [/ 15/ ]/ 基于/ 性能/ 势/ 理论/ 和/ 连续/ 时间/ SMDP/ (/ CT/ -/ SMDP/ )/ 数学模型/ ,/ 建立/ 了/ 一种/ 适用/ 于/ 平均/ 或/ 折扣/ 准则/ 且/ 与/ 模型/ 无关/ (/ model/ -/ free/ )/ 的/ 统一/ 学习/ 优化/ 方法/ ,/ 可/ 在线/ 实现/ ./ 在/ 这些/ 工作/ 基础/ 上/ ,/ 本文/ 将/ 根据/ 现有/ HRL/ 方法/ 中/ 的/ Option/ 算法/ ,/ 研究/ 提出/ Page3/ 一种/ 适用/ 于/ 平均/ 或/ 折扣/ 性能/ 准则/ 的/ 连续/ 时间/ 统一/ Option/ 算法/ (/ 非/ 全新/ 结构/ 的/ HRL/ 模型/ ,/ 结构/ 上/ 仍/ 属/ Option/ 方法/ )/ ,/ 可/ 同步进行/ 宏观/ 策略/ 、/ 微观/ 策略/ 学习/ ,/ 以/ 实现/ 在线/ 优化/ ./ 2/ 连续/ 时间/ SMDP/ 数学模型/ 在/ SMDP/ 中/ ,/ 系统/ 的/ 状态/ 在/ 决策/ 点/ 之间/ 也/ 可能/ 发生/ 转移/ ,/ 其中/ 的/ 状态/ 称为/ 自然/ 状态/ ./ 根据/ 代价/ 的/ 时间/ 累积/ 特性/ ,/ SMDP/ 可以/ 分为/ 离散/ 时间/ SMDP/ (/ DT/ -/ SMDP/ )/ 和/ CT/ -/ SMDP/ ,/ 两者/ 的/ 主要/ 区别/ 是/ 前者/ 的/ 代价/ 累积/ 只/ 依赖/ 决策/ 点/ 之间/ 自然/ 状态/ 的/ 离散/ 转移/ 序列/ 和/ 相关/ 状态/ 报酬/ 信息/ ,/ 而/ 不/ 考虑/ 自然/ 状态/ 的/ 实际/ 逗留/ 时间/ 和/ 累积/ 报酬/ ./ 在/ CT/ -/ SMDP/ 模型/ 中/ ,/ 令/ X/ (/ t/ )/ 表示/ t/ 时刻/ 的/ 状态/ ,/ t/ 属于/ 非负/ 实数/ 集/ ,/ X/ (/ t/ )/ 属于/ 一个/ 有限/ 状态/ 集/ Φ/ =/ {/ 1/ ,/ 2/ ,/ …/ ,/ M/ }/ ./ 记/ ti/ 为/ 系统/ 第/ i/ 个/ 决策/ 时刻/ ,/ i/ 属于/ 自然数/ 集/ N/ ,/ 且/ t1/ =/ 0/ ;/ a/ (/ ti/ )/ 表示/ ti/ 时刻/ 系统/ 在/ 状态/ X/ (/ ti/ )/ 处/ 采取/ 的/ 行动/ ,/ 且/ a/ (/ ti/ )/ 属于/ 一个/ 有限/ 行动/ 集/ D/ ;/ 此时/ ,/ 状态/ 到/ 行动/ 集/ D/ 的/ 一个/ 映射/ π/ 就/ 可以/ 构成/ 系统/ 的/ 一个/ 平稳/ 控制策略/ ,/ 即/ π/ :/ Φ/ →/ D/ ;/ f/ (/ X/ (/ t/ )/ ,/ X/ (/ ti/ )/ ,/ a/ (/ ti/ )/ ,/ X/ (/ ti/ +/ 1/ )/ )/ ,/ ti/ / t/ </ ti/ +/ 1/ ,/ 表示/ 系统/ 在/ 状态/ X/ (/ ti/ )/ 处/ 选择/ 行动/ a/ (/ ti/ )/ 后/ ,/ 转移/ 到/ 下/ 一/ 决策/ 状态/ X/ (/ ti/ +/ 1/ )/ 前/ ,/ 处于/ 状态/ X/ (/ t/ )/ 时/ 的/ 单位/ 时间/ 期望/ 报酬/ (/ 或/ 代价/ )/ 函数/ ./ 在/ 上述/ 定义/ 下/ ,/ 根据/ 文献/ [/ 16/ -/ 17/ ]/ ,/ 有限/ 状态/ 集/ Φ/ 中/ 任意/ 状态/ x/ 的/ 无穷/ 时段/ 折扣/ 性能/ 准则/ 为/ α/ (/ x/ )/ =/ E/ ∑/ η/ π/ a/ (/ ti/ )/ ,/ X/ (/ ti/ +/ 1/ )/ )/ dtX/ (/ 0/ )/ =/ ]/ x/ (/ 1/ )/ 这里/ ,/ 0/ </ α/ </ 1/ 是/ 折扣/ 因子/ ;/ 当/ α/ →/ 0/ 时/ ,/ 其/ 极限/ α/ (/ x/ )/ 表示/ 任意/ 状态/ 下/ 的/ 平均/ 代价/ η/ π/ [/ 16/ ]/ ,/ 其/ 计算/ 公/ η/ π/ 式/ 可/ 另/ 表示/ 为/ η/ π/ =/ limT/ →/ t/ (/ T/ +/ 1/ )/ E/ ∑/ T/ 根据上述/ 式/ (/ 1/ )/ 与/ (/ 2/ )/ ,/ 在/ 获得/ 一次/ 转移/ 样本/ 情况/ 下/ ,/ 可以/ 构造/ 折扣/ 或/ 平均/ 性能/ 准则/ 下/ 性能/ 势/ 的/ 统一/ 即时/ 差分/ (/ temporaldifference/ )/ 学习/ 公式/ ,/ 详见/ 文献/ [/ 15/ ]/ ,/ 此处/ 不再/ 赘述/ ./ 3/ 连续/ 时间/ 统一/ Option/ 算法/ Option/ 是/ Sutton/ 等/ 人/ [/ 10/ ]/ 提出/ 的/ 一种/ HRL/ 算法/ ,/ 其中/ ,/ 系统/ 的/ 学习/ 任务/ 被/ 抽象/ 成/ 若干个/ Option/ ,/ 并且/ 这些/ Option/ 被/ 作为/ 特殊/ 的/ “/ 行动/ ”/ (/ 宏/ 行动/ )/ 加入/ 到/ 原有/ 的/ 动作/ (/ 称/ 动作/ 原语/ )/ 集中/ ,/ 动作/ 原语/ 可以/ 视为/ Option/ 的/ 特例/ ./ 为/ 方便/ 定义/ ,/ 一般/ 将宏/ 行动/ 和/ 动作/ 原语/ 统称/ 为宏/ 行动/ ./ 在/ Option/ 算法/ 中/ ,/ 每个/ Option/ 可/ 视为/ 完成/ 某项/ 子/ 任务/ 而/ 定义/ 在子/ 状态/ 空间/ 上/ 并/ 按/ 一定/ 策略/ 执行/ 的/ 宏/ 行动/ 序列/ ./ 与/ Max/ -/ Q/ 等/ HRL/ 方法/ 相比/ ,/ Option/ 方法/ 的/ 层次化/ 结构/ 较为/ 清晰/ ,/ 因而/ 易于/ 自动/ 生成/ ,/ 即子/ 任务/ 生成/ 、/ 子/ 任务/ 交互/ 和子/ 任务/ 抽象/ 等/ 分层/ 工作/ 都/ 可/ 通过/ 算法/ 自动/ 完成/ ./ Sutton/ 等/ 人/ 提出/ 的/ 离散/ 时间/ 折扣/ 准则/ Option/ 算法/ 是/ 以/ DT/ -/ SMDP/ 为/ 数学模型/ 和/ 理论/ 基础/ 的/ ,/ 主要/ 考虑/ Option/ 内部/ 状态/ 的/ 逻辑/ 转移/ 序列/ 和/ 离散/ 决策/ 点/ 代价/ ./ 但是/ ,/ 在/ 实际/ 环境/ 中/ ,/ 不但/ 系统/ 的/ 状态/ 按/ 概率/ 随机/ 转移/ ,/ 而且/ 系统/ 在/ 一个/ 状态/ 所/ 逗留/ 的/ 时间/ 一般/ 也/ 服从/ 一个/ 随机/ 分布/ ,/ 即/ 一次/ 状态/ 转移/ 需要/ 耗费/ 一定/ 的/ 随机/ 时间/ ,/ 特别/ 地/ ,/ 在/ 相邻/ 的/ 两个/ 决策/ 点/ 之间/ 可能/ 发生/ 状态/ 转移/ 或/ 有/ 多次/ 状态/ 转移/ ./ 因此/ ,/ 系统/ 运行/ 获得/ 的/ 性能/ 报酬/ 往往/ 不仅/ 与/ 决策/ 点/ 的/ 状态/ 和/ 行动/ 有关/ ,/ 还/ 与/ 相邻/ 决策/ 点/ 之间/ 的/ 状态/ 转移/ 有关/ ,/ 而且/ 往往/ 是/ 按/ 时间/ 分段/ 累积/ 的/ ,/ 即/ 存在/ 按/ 时间/ 累积/ 报酬/ ,/ 例如/ 文献/ [/ 15/ ]/ 考虑/ 的/ 生产/ 加工/ 系统/ 即/ 为/ 此例/ ./ 因此/ ,/ 很多/ 实际/ 复杂/ 的/ 系统/ 需要/ 考虑/ 以/ CT/ -/ SMDP/ 为/ 理论/ 基础/ 的/ 连续/ 时间/ Option/ 方法/ ./ 本文/ 将/ 以/ 前述/ CT/ -/ SMDP/ 模型/ 为/ 基础/ ,/ 运用/ 性能/ 势/ 理论/ [/ 17/ -/ 18/ ]/ ,/ 给出/ 一种/ 连续/ 时间/ 统一/ Option/ 算法/ ./ 首先/ ,/ 一个/ 任务/ 或子/ 任务/ 可/ 由/ 三元组/ o/ =/ 〈/ Io/ ,/ μ/ o/ ,/ β/ o/ 〉/ 表示/ ,/ 称之为/ 一个/ Option/ [/ 2/ ]/ ./ 其中/ ,/ Io/ / Φ/ 是/ 该/ Option/ 的/ 状态/ 集/ ,/ Io/ 包含/ 且/ 仅/ 包含/ 该/ Option/ 所/ 需/ 的/ 状态/ ;/ μ/ o/ :/ Io/ ×/ OIo/ →/ [/ 0/ ,/ 1/ ]/ 为此/ Option/ 的/ 内部/ 随机/ 策略/ ,/ OIo/ 为/ 状态/ 集/ Io/ 上/ 可/ 执行/ 的/ 宏/ 行动/ 集/ ;/ β/ o/ (/ X/ (/ t/ )/ )/ ∈/ [/ 0/ ,/ 1/ ]/ ,/ X/ (/ t/ )/ ∈/ Io/ ,/ 为此/ Option/ 的/ 结束/ 条件/ ,/ 表示/ 该/ Option/ 中/ 状态/ X/ (/ t/ )/ 以/ 概率/ β/ o/ (/ X/ (/ t/ )/ )/ 结束/ ,/ 若令/ 该/ Option/ 的/ 子目标/ 状态/ 集/ 为/ Φ/ G/ ,/ 则/ 对/ / X/ (/ t/ )/ ∈/ Φ/ G/ ,/ 有/ β/ o/ (/ X/ (/ t/ )/ )/ =/ 1/ ./ 在/ 本文/ 提出/ 的/ 连续/ 时间/ 统一/ Option/ 算法/ 中/ ,/ 假设/ 其/ 结构/ 分为/ n/ 层/ ,/ 其中/ 最上层/ 为/ 第/ 1/ 层/ ,/ 最底层/ 为/ 第/ n/ 层/ ./ 图/ 1/ 为/ 该/ 算法/ 第/ u/ 层/ 、/ 第/ u/ +/ 1/ 层/ 、/ 第/ u/ +/ 2/ 层间/ 的/ 切换/ 及其/ 内部/ 部分/ 状态/ 转移/ 图例/ ./ 记/ tui/ 为/ 第/ u/ 层中/ 当前/ 考查/ 的/ 某个/ 任务/ (/ 或子/ 任务/ )/ 的/ 第/ i/ 个/ 决策/ 时刻/ ,/ 其中/ u/ ∈/ {/ 1/ ,/ 2/ ,/ …/ ,/ n/ }/ ,/ i/ ∈/ {/ 1/ ,/ 2/ ,/ 3/ ,/ …/ }/ ,/ 并且/ t11/ =/ 0/ ./ Xu/ (/ tui/ )/ 表示/ 该/ 当前任务/ 在/ tui/ 时/ 对应/ 的/ 内部/ 状态/ ,/ ou/ (/ tui/ )/ 表示/ 系统/ 在/ 该/ 状态/ 处/ 根据/ 当前任务/ 的/ 随机/ 策略/ μ/ o/ 随机/ 选择/ 的/ 宏/ 行动/ ./ Page4/ 若/ ou/ (/ tui/ )/ 为/ 动作/ 原语/ ,/ 则/ 该/ 动作/ 原语/ 执行/ 完毕/ 就/ 立即/ 结束/ ,/ 系统/ 在/ tui/ +/ 1/ 时/ 转移/ 到/ 当前任务/ 的/ 下/ 一/ 状态/ Xu/ (/ tui/ +/ 1/ )/ ,/ 则/ tui/ +/ 1/ -/ tui/ 为/ 执行/ 该/ 动作/ 原语/ 的/ 逗留/ 时间/ ./ 对/ 任意/ α/ / 0/ ,/ 令该/ 次/ 转移/ 对应/ 的/ 随/ 时间/ 累积/ 报酬/ f/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ ,/ Xu/ (/ tui/ +/ 1/ )/ )/ 为/ f/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ ,/ Xu/ (/ tui/ +/ 1/ )/ )/ =/ k1/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ )/ +/ ∫/ tu/ 图/ 1/ 连续/ 时间/ 统一/ Option/ 状态/ 转移/ 图例/ 其中/ ,/ k1/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ )/ 为/ 系统/ 在/ 状态/ Xu/ (/ tui/ )/ 处/ 执行/ 动作/ 原语/ ou/ (/ tui/ )/ 后/ 所/ 获得/ 的/ 立即/ 报酬/ ,/ 该/ 报酬/ 只/ 与/ 所在/ 的/ 状态/ 和/ 执行/ 的/ 动作/ 原语/ 有关/ ,/ 而/ 与/ 时间/ 无关/ ;/ k2/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ ,/ Xu/ (/ tui/ +/ 1/ )/ )/ 为/ 系统/ 在/ 状态/ Xu/ (/ tui/ )/ 处/ 执行/ 动作/ 原语/ ou/ (/ tui/ )/ 后/ ,/ 并于/ 下/ 一/ 决策/ 时刻/ tui/ +/ 1/ 转移/ 到/ 状态/ Xu/ (/ tui/ +/ 1/ )/ 之前/ 的/ 单位/ 时间/ 代价/ 率/ (/ 不失/ 一般性/ 的/ ,/ 此处/ 假设/ k2/ (/ ·/ )/ 在/ 一次/ 转移/ 中为/ 常数/ )/ ./ 若/ ou/ (/ tui/ )/ 为/ 非/ 动作/ 原语/ ,/ 则/ 执行/ 该/ Option/ (/ 或宏/ 行动/ )/ ,/ 系统/ 进入/ 第/ u/ +/ 1/ 层/ ./ 其中/ ,/ Optionou/ (/ tui/ )/ 内/ 的/ 初始状态/ 由/ 第/ u/ 层/ 状态/ Xu/ (/ tui/ )/ 决定/ ,/ 记为/ Xu/ +/ 1/ (/ tu/ +/ 11/ )/ ,/ 且/ tu/ +/ 11/ 为/ tui/ 映射/ 到/ 该/ Option/ 内/ 的/ 初始/ 决策/ 时刻/ ,/ 可设/ tu/ +/ 11/ =/ tui/ ./ 如图/ 1/ 所示/ ,/ ou/ (/ tui/ )/ 的/ 初始状态/ Xu/ +/ 1/ (/ tu/ +/ 11/ )/ 是/ 系统/ 从/ 第/ u/ 层/ 状态/ Xu/ (/ tui/ )/ 进入/ 到/ 该/ Option/ 内/ 的/ 跨层/ 物理/ 映射/ ,/ 但/ 两者/ 数学/ 表达/ 或/ 定义/ 一般/ 不同/ ./ 在/ 第/ u/ +/ 1/ 层/ 的/ ou/ (/ tui/ )/ 内/ ,/ 系统/ 按照/ 策略/ i/ )/ 执行/ 一个/ 子/ 任务/ 序列/ ,/ 直到/ 该/ Option/ 的/ 结束/ μ/ ou/ (/ tu/ 条件/ 在/ tu/ +/ 1m/ +/ 1/ 时刻/ 满足/ ,/ 并/ 在/ 内部/ 状态/ Xu/ +/ 1/ (/ tu/ +/ 1m/ +/ 1/ )/ 处/ 回到/ 第/ u/ 层/ ./ Xu/ +/ 1/ (/ tu/ +/ 1m/ +/ 1/ )/ 对应/ 的/ 第/ u/ 层/ 状态/ Xu/ (/ tui/ +/ 1/ )/ 亦/ 是/ 系统/ 在/ 状态/ Xu/ (/ tui/ )/ 处/ 执行/ 完宏/ 行动/ ou/ (/ tui/ )/ 后/ 的/ 下/ 一/ 决策/ 状态/ ./ 显然/ ,/ tui/ +/ 1/ =/ tu/ +/ 1m/ +/ 1/ ,/ 且/ tu/ +/ 1m/ +/ 1/ -/ tu/ +/ 11/ ,/ 即/ tui/ +/ 1/ -/ tui/ 为/ 执行/ 宏/ 行动/ ou/ (/ tui/ )/ 的/ 累积/ 时间/ ,/ 且/ Xu/ (/ tui/ +/ 1/ )/ 是/ 系统/ 从/ 第/ u/ +/ 1/ 层/ 状态/ Xu/ +/ 1/ (/ tu/ +/ 1m/ +/ 1/ )/ 回到/ 第/ u/ 层/ 的/ 跨层/ 物理/ 映射/ ./ 于是/ ,/ 可/ 按/ 下式/ 计算/ 第/ u/ 层/ 在/ 执行/ 完宏/ 行动/ ou/ (/ tui/ )/ 时/ 对应/ 的/ 时间/ 累积/ 报酬/ f/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ ,/ Xu/ (/ tui/ +/ 1/ )/ )/ =/ f/ (/ Xu/ +/ 1/ (/ tu/ +/ 11/ )/ ,/ 这里/ ,/ bu/ +/ 1Xu/ +/ 1/ (/ tu/ +/ 1/ 策/ 状态/ Xu/ +/ 1/ (/ tu/ +/ 1/ {/ Xu/ +/ 1/ (/ tu/ +/ 1/ 执行/ 宏/ 行动/ ou/ (/ tui/ )/ 直到/ 结束/ 时/ ,/ 第/ u/ +/ 1/ 层/ 经历/ 的/ 决策/ 状态/ 序列/ 和/ 决策/ 状态/ 数/ ./ 根据/ 式/ (/ 3/ )/ 和/ (/ 4/ )/ 以及/ 性能/ 势/ 理论/ [/ 15/ -/ 16/ ]/ ,/ 可/ 计算/ 对应/ 第/ u/ 层/ 的/ 一次/ 转移/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ ,/ Xu/ (/ tui/ +/ 1/ )/ )/ 的/ 即时/ 差分/ 公式/ i/ =/ f/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ ,/ Xu/ (/ tui/ +/ 1/ )/ )/ -/ dtu/ 这里/ ,/ Q/ α/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ )/ 是/ 折扣/ 因子/ 为/ α/ 时/ 的/ 状态/ -/ 宏/ 行动/ 对/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ )/ 的/ 性能/ 势/ Q/ 因子/ ;/ 另外/ ,/ T/ α/ (/ tui/ +/ 1/ -/ tui/ )/ =/ ∫/ tuT0/ (/ z/ )/ =/ lim/ α/ →/ 0T/ α/ (/ z/ )/ =/ z/ ,/ z/ / 0/ ,/ 且/ Ou/ (/ Xu/ (/ tui/ +/ 1/ )/ )/ 为/ 状态/ Xu/ (/ tui/ +/ 1/ )/ 所/ 对应/ 的/ 宏/ 行动/ 集/ ./ 于是/ ,/ 上述/ 式/ (/ 5/ )/ 对/ 任意/ α/ / 0/ 都/ 成立/ ,/ 即/ 适用/ 于/ 平均/ 或/ 折扣/ 性能/ 准则/ ./ 在/ 连续/ 时间/ Option/ 算法/ 中/ ,/ 单位/ 时间/ 平均/ 报酬/ 珔/ η/ 通过/ 系统/ 学习/ 过程/ 中/ 的/ 实际/ 学习/ 时间/ 和/ 实际/ 累积/ 报酬/ 计算/ 得到/ ,/ 而/ 系统/ 执行/ 非/ 动作/ 原语/ 的/ 累积/ 时间/ 和/ 累积/ 报酬/ 值/ ,/ 均/ 通过/ 累计/ 执行/ 该宏/ 行动/ 所/ 包含/ 的/ 动作/ 原语/ 的/ 逗留/ 时间/ 和/ 累积/ 报酬/ 得到/ ./ 因此/ ,/ 珔/ η/ 的/ 值/ 更新/ 只/ 与/ 动作/ 原语/ 的/ 执行/ 有关/ ,/ 即仅/ 当/ 执行/ 的/ 宏/ 行动/ 为/ Page5/ 动作/ 原语/ 时才/ 对/ 其值/ 进行/ 更新/ ./ 其/ 更新/ 公式/ 为/ 珔/ η/ =/ 珔/ η/ +/ ζ/ tu/ 其中/ ,/ f/ α/ =/ 0/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ ,/ Xu/ (/ tui/ +/ 1/ )/ )/ 表示/ 平均/ 准则/ 下/ 对应/ 动作/ 原语/ ou/ (/ tui/ )/ 的/ 转移/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ ,/ Xu/ (/ tui/ +/ 1/ )/ )/ 的/ 时间/ 累积/ 报酬/ ,/ ζ/ tu/ 为/ 学习/ 时间/ tui/ 的/ 倒数/ ./ 每/ 执行/ 完/ 一个/ 宏/ 行动/ ,/ 就/ 可/ 对/ 状态/ -/ 宏/ 行动/ 对/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ )/ 的/ Q/ 值/ 进行/ 迭代/ 学习/ ,/ 如式/ (/ 7/ )/ 所示/ Q/ α/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ )/ =/ Q/ α/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ )/ +/ 式/ (/ 7/ )/ 中/ γ/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ )/ 表示/ 学习/ 步长/ ,/ 在/ 这里/ 可以/ 取为/ γ/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ )/ =/ 1/ // N/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ )/ c/ ./ 其中/ ,/ N/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ )/ 为/ 状态/ -/ 宏/ 行动/ 对/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ )/ 的/ 访问/ 次数/ ,/ c/ 为/ 常数/ ,/ 且/ 0/ </ c/ </ 1/ ./ 式/ (/ 7/ )/ 为/ 折扣/ 和/ 平均/ 两种/ 性能/ 准则/ 下/ 统一/ 的/ 连续/ 时间/ Option/ 优化/ 算法/ 的/ 迭代/ 公式/ ./ 在/ Q/ 算法/ 中/ ,/ 常/ 采用/ ε/ -/ greedy/ 机制/ ,/ 但/ ε/ 的/ 初始值/ 和/ 衰减/ 速率/ 一般/ 只能/ 通过/ 经验/ 确定/ ,/ 若/ 初始值/ 图/ 2SA/ -/ Option/ 算法/ 流程/ 根据上述/ 的/ 流程图/ ,/ 我们/ 可以/ 得出/ SA/ -/ Option/ 算法/ 具体步骤/ 如下/ ./ 1/ ./ 初始化/ ./ 令/ 所有/ 状态/ -/ 宏/ 行动/ 对/ 的/ Q/ 值为/ 0/ ;/ 令/ 珔/ η/ =/ 0/ ;/ 生成/ 一空栈/ stack/ {/ u/ ,/ i/ ,/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ ,/ bui/ ,/ 累积/ 报酬/ 值/ f/ }/ ,/ 初始化/ 其他/ 控制/ 条件/ ./ 2/ ./ 选择/ 宏/ 行动/ ./ 在/ 状态/ Xu/ (/ tui/ )/ 处/ ,/ 根据/ SA/ 机制/ 选择/ 宏/ 太小/ 或/ 衰减/ 速率/ 太快/ ,/ 则/ 不能/ 进行/ 充分/ 的/ 探索/ (/ ex/ -/ ploration/ )/ ,/ 得到/ 的/ 策略/ 往往/ 只是/ 次优/ 策略/ ;/ 相反/ ,/ 若/ 初始值/ 太大/ 或/ 衰减/ 速率/ 太慢/ ,/ 虽然/ 有利于/ 探索/ ,/ 但/ 却/ 增加/ 了/ 学习/ 代价/ ,/ 影响/ 算法/ 的/ 收敛性/ ,/ 不利于/ 学习/ 利用/ (/ exploitation/ )/ ./ 模拟退火/ (/ SimulatedAnnealing/ ,/ SA/ )/ 机制/ 利用/ 学习/ 过程/ 中/ 的/ Q/ 值/ 变化/ 来/ 平衡/ 探索/ 和/ 利用/ 之间/ 的/ 关系/ ,/ 可/ 克服/ ε/ -/ greedy/ 机制/ 的/ 缺点/ [/ 20/ ]/ ./ 因此/ ,/ 本文/ 将/ 模拟退火/ 机制/ 引入/ 到/ 分层/ 强化/ 学习/ 中/ ,/ 并称/ 对应/ 的/ 算法/ 为/ SA/ -/ Option/ 算法/ ./ SA/ 机制/ 运行/ 规则/ 表述/ 如下/ :/ 根据/ 当前/ Q/ 值表/ ,/ 在/ 状态/ Xu/ (/ tui/ )/ 处/ 选择/ 一个/ 如下/ 描述/ 的/ 宏/ 行动/ :/ omaxu/ (/ tui/ )/ =/ argmax/ 所/ 对应/ 的/ 宏/ 行动/ 集中/ 随机/ 选取/ 一个/ 宏/ 行动/ ou/ (/ tui/ )/ ,/ 若/ exp/ [/ -/ (/ Qu/ (/ Xu/ (/ tui/ )/ ,/ omaxu/ (/ tui/ )/ )/ -/ Qu/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ )/ )/ // (/ K/ ·/ Tm/ )/ ]/ / random/ [/ 0/ ,/ 1/ )/ 成立/ ,/ 则/ 取/ ou/ (/ tui/ )/ =/ ou/ (/ tui/ )/ ;/ 否则/ ,/ 令/ ou/ (/ tui/ )/ =/ omaxu/ (/ tui/ )/ ./ 其中/ ,/ random/ [/ 0/ ,/ 1/ )/ 为/ 在/ 区间/ [/ 0/ ,/ 1/ )/ 中/ 产生/ 的/ 随机数/ ,/ K/ 为/ Boltzman/ 常数/ ,/ Tm/ 为/ 温度/ ./ 本文/ 定义/ 的/ SA/ -/ Option/ 算法/ 流程/ 如图/ 2/ 所示/ ./ 行动/ ou/ (/ tui/ )/ ./ 3/ ./ 执行/ 宏/ 行动/ ou/ (/ tui/ )/ ./ 3.1/ 若/ ou/ (/ tui/ )/ 为/ 非/ 动作/ 原语/ ./ 令/ bui/ =/ 0/ ,/ f/ =/ 0/ ,/ 把/ 记录/ 的/ 样本/ 入栈/ ,/ 即/ stack/ {/ }/ ←/ 〈/ u/ ,/ i/ ,/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ ,/ bui/ ,/ f/ 〉/ ./ 在/ Xu/ (/ tui/ )/ 对应/ 的/ 第/ u/ +/ 1/ 层/ 状态/ Xu/ +/ 1/ (/ tu/ +/ 11/ )/ 处/ ,/ 根据/ 策略/ μ/ ou/ (/ tui/ )/ 来/ 选择/ 宏/ 行动/ ou/ +/ 1/ (/ tu/ +/ 11/ )/ ,/ 并令/ u/ ·/ ·/ =/ u/ +/ 1/ 和/ i/ =/ 1/ ,/ 转到/ 步/ 3/ ./ Page63/ ./ 2/ 若/ ou/ (/ tui/ )/ 为/ 动作/ 原语/ ./ 执行/ ou/ (/ tui/ )/ ,/ 得到/ 转移/ 间隔/ 逗留/ 时间/ bu1/ ,/ 观察/ 下/ 一/ 决策/ 状态/ Xu/ (/ tui/ +/ 1/ )/ ,/ 根据/ 式/ (/ 3/ )/ 计算/ 累积/ 报酬/ 值/ f/ ,/ 把/ 记录/ 的/ 样本/ 入栈/ ;/ 根据/ 式/ (/ 6/ )/ 计算/ 平均/ 代价/ 的/ 学习/ 值/ η/ -/ ./ 令/ h/ =/ u/ ./ 3.3/ 若/ 状态/ Xh/ (/ thh/ >/ 1/ ,/ 则/ h/ ·/ ·/ =/ h/ -/ 1/ ,/ 并/ 转到/ 步/ 3.3/ ;/ 否则/ ,/ 转到/ 步/ 4/ ./ 其中/ Xh/ (/ th/ 为/ Xu/ (/ tui/ +/ 1/ )/ 对应/ 的/ 第/ h/ 层/ 第/ m/ 个/ 决策/ 状态/ ./ 4/ ./ 值/ 更新/ ./ 4.1/ 根据/ 栈/ 的/ 后进先出/ 特性/ ,/ 进行/ 出栈/ 操作/ ,/ 即/ tui/ =/ 0/ ,/ u/ =/ 1/ ,/ i/ =/ 1/ ,/ 〈/ u/ ,/ t/ ,/ Xu/ (/ tu/ 据式/ (/ 5/ )/ 和/ (/ 7/ )/ 计算/ 即时/ 差分/ dtuu/ =/ h/ ,/ 状态/ 为/ Xu/ (/ tui/ )/ ,/ 转到/ 步/ 2.4/ ./ 2/ 若栈/ 为/ 空/ ,/ 转到/ 步/ 5/ ;/ 否则/ ,/ 对栈/ 顶/ 记录/ 的/ 累积/ 时间/ 和/ 累积/ 报酬/ 进行/ 更新/ ./ 即先/ 出栈/ 〈/ u/ ,/ i/ ,/ S/ ,/ action/ ,/ time/ ,/ R/ 〉/ ←/ stack/ {/ }/ ,/ 然后/ 计算/ (/ Xu/ (/ tue/ -/ α/ ×/ time/ ×/ f/ 与/ 转移/ 过程/ 中/ 的/ 累积/ 时间/ time/ ·/ ·/ =/ time/ +/ bu/ 后/ 进行/ 入栈/ 操作/ ,/ 即/ stack/ {/ }/ ←/ 〈/ u/ ,/ i/ ,/ S/ ,/ action/ ,/ time/ ,/ R/ 〉/ ./ 4.3/ 若/ u/ =/ h/ ,/ 令/ 珔/ η/ =/ 珔/ η/ ,/ 转到/ 步/ 5/ ;/ 否则/ ,/ 转到/ 步/ 4.5/ ./ 若/ 算法/ 终止/ 条件/ 满足/ ,/ 学习/ 结束/ ;/ 否则/ ,/ i/ =/ i/ +/ 1/ ,/ Singh/ 等/ 人/ 已/ 证明/ ,/ 在/ 标准/ Q/ -/ 学习/ 收敛/ 条件/ 下/ ,/ 以/ Q/ -/ 学习/ 算法/ 为/ 基础/ 的/ Option/ 方法/ 以/ 概率/ 1/ 收敛/ 到/ 与/ 任务/ 分层/ 结构/ 兼容/ 的/ 分层/ 最优/ [/ 20/ ]/ ,/ 此/ 结果/ 很/ 容易/ 推广/ 到/ 上述/ 连续/ 时间/ Option/ 方法/ ./ 强化/ 学习/ 算法/ 的/ 复杂度/ 主要/ 涉及/ 空间/ 复杂度/ 、/ 样本/ 复杂度/ 和/ 时间/ 复杂度/ ,/ 一般/ 情况/ 下/ 样本/ 复杂度/ 和/ 时间/ 复杂度/ 等同/ ./ 对于/ 标准/ Q/ -/ 学习/ 算法/ ,/ 空间/ 复杂度/ 即/ 为/ 状态/ -/ 行动/ 对/ 数据/ 所/ 需/ 存储空间/ ,/ 若/ 假设/ MDP/ 或/ SMDP/ 模型/ 的/ 状态/ 个数/ 为/ N/ ,/ 行动/ 个数/ 为/ A/ ,/ 则/ 其/ 所/ 需/ 存储空间/ 为/ NA/ ,/ 因而/ 空间/ 复杂度/ 为/ Θ/ (/ NA/ )/ ,/ 也/ 可/ 记为/ Θ/ (/ N/ )/ ./ 若/ Option/ 算法/ 分层/ 后/ 的/ 系统/ 状态/ 个数/ 为/ N/ ,/ 行动/ 个数/ 为/ A/ ,/ 则/ 所/ 需/ 存储空间/ 为/ NA/ (/ 由于/ Option/ 算法/ 是/ 为/ 解决/ 复杂/ 问题/ 而/ 引入/ 抽象/ 机制/ 来/ 实现/ 空间/ 降维/ ,/ 其/ 存储空间/ 一般/ 小于/ 原/ MDP/ 或/ SMDP/ ,/ 见/ 本文/ 实例/ )/ ,/ 因而/ 空间/ 复杂度/ 为/ Θ/ (/ NA/ )/ ./ 根据/ 文献/ [/ 21/ -/ 22/ ]/ ,/ 标准/ Q/ -/ 学习/ 算法/ 的/ 样本/ 复杂度/ 可以/ 简记/ 为/ O/ (/ Nlog/ (/ N/ )/ )/ ./ 而/ Option/ 算法/ 中/ ,/ 考虑/ 到/ 进行/ 抽象/ 而/ 产生/ 的/ 各层/ 之间/ 的/ 相关性/ ,/ 一般/ 难以/ 给出/ 该类/ 算法/ 统一/ 的/ 样本/ 复杂度/ 公式/ ./ 当然/ ,/ 对于/ 非/ 嵌套/ Option/ ,/ 若/ 其子/ 空间/ 的/ 状态/ 个数/ 为/ Ns/ ,/ 则/ 其/ 样本/ 复杂度/ 可记/ 为/ O/ (/ Nslog/ (/ Ns/ )/ )/ ./ 4/ 实验/ 仿真/ 本/ 节/ 利用/ 单个/ 机器人/ (/ Agent/ )/ 进行/ 垃圾/ 收集/ 的/ 系统/ 为例/ 验证/ 第/ 3/ 节/ 构造/ 的/ 连续/ 时间/ 统一/ Option/ 算法/ (/ SA/ -/ Option/ )/ 的/ 有效性/ [/ 9/ ]/ ,/ 并/ 与/ 基于/ 性能/ 势/ 的/ 连续/ 时间/ 统一/ SA/ -/ Q/ 算法/ 进行/ 性能/ 比较/ (/ 具体/ 算法/ 见/ 附录/ )/ ,/ 以/ 说明/ 其/ 优越性/ ./ 4.1/ 仿真/ 模型/ 如图/ 3/ 所示/ ,/ Agent/ 垃圾/ 收集/ 系统/ 由/ 3/ 个/ 房间/ 和/ 1/ 个/ 走廊/ 组成/ ./ 现有/ 1/ 个/ Agent/ 负责/ 从/ Room1/ 和/ Room2/ 的/ 垃圾箱/ (/ T1/ 或/ T2/ )/ 中/ 收集/ 垃圾/ ,/ 并/ 把/ 收集/ 到/ 的/ 垃圾/ 放入/ Room3/ 中/ 的/ 垃圾场/ (/ Dump/ )/ ./ 垃圾箱/ 中/ 的/ 垃圾/ 按/ 一定/ 的/ Poisson/ 流/ 产生/ ,/ 假定/ 两个/ 垃圾箱/ 的/ 垃圾/ 产生/ 率/ 相等/ ;/ 机器人/ 一次/ 只能/ 在/ 一个/ 垃圾箱/ 处/ 收集/ 垃圾/ ,/ 并/ 假设/ 机器人/ 一次/ 能/ 把/ 一个/ 垃圾箱/ 中/ 的/ 垃圾/ 全部/ 收集/ 并/ 送往/ Dump/ (/ 假设/ 垃圾箱/ 和/ Dump/ 的/ 容量/ 无限大/ )/ ./ Agent/ 可/ 执行/ 7/ 种/ 行动/ :/ 上/ 、/ 下/ 、/ 左/ 、/ 右/ 、/ 等待/ 、/ 捡取/ 、/ 放置/ ,/ 且/ 令/ Agent/ 执行/ 每个/ 行动/ 的/ 逗留/ 时间/ 均/ 服从/ 均值/ 为/ 0.5/ 的/ 指数分布/ ./ 定义/ Agent/ 从/ 任一/ 位置/ 出发/ ,/ 转移/ 到任/ 一/ 垃圾箱/ 并/ 正确/ 捡取/ 垃圾/ ,/ 然后/ 转移/ 到/ Dump/ 并/ 正确/ 放置/ 垃圾/ 这一/ 过程/ ,/ 称为/ 正确/ 收集/ 一次/ 垃圾/ ./ Agent/ 在/ T1/ 或/ T2/ 处/ 正确/ 捡取/ 垃圾/ 以及/ 在/ Dump/ 处/ 正确/ 放置/ 垃圾/ ,/ 分别/ 会/ 得到/ 相应/ 的/ 立即/ 报酬/ ,/ 其值/ 为/ 1/ ;/ 若/ Agent/ 错误/ 捡取/ 或/ 放置/ 垃圾/ ,/ 会/ 得到/ 值为/ 负/ 的/ 报酬/ 作为/ 惩罚/ ,/ 其值/ 为/ -/ 1/ ;/ 若/ Agent/ 执行/ 上/ 、/ 下/ 、/ 左/ 、/ 右/ 或/ 等待/ 这/ 5/ 种/ 行动/ ,/ 其/ 立即/ 报酬/ 为/ Page70/ ./ 另外/ ,/ Agent/ 执行/ 每个/ 行动/ 都/ 会/ 产生/ 时间/ 累积/ 报酬/ ,/ 其/ 单位/ 时间/ 报酬率/ 为/ -/ 0.025/ ./ 当/ Agent/ 在/ 垃圾箱/ T1/ 或/ T2/ 处/ ,/ 若/ 其/ 未/ 持有/ 垃圾/ ,/ 并且/ 所在/ 的/ 垃圾箱/ 中有/ 垃圾/ ,/ 则/ 此时/ 进行/ 垃圾/ 捡取/ 为/ 正确/ 行动/ ,/ 否则/ 执行/ 垃圾/ 捡取/ 为/ 错误/ 行动/ ;/ 同样/ ,/ 当/ Agent/ 在/ Dump/ 处/ ,/ 若/ 其/ 持有/ 垃圾/ ,/ 则/ 此时/ Agent/ 进行/ 垃圾/ 放置/ 为/ 正确/ 行动/ ,/ 否则/ 执行/ 垃圾/ 放置/ 为/ 错误/ 行动/ ./ 假设/ A/ -/ gent/ 错误/ 地/ 执行/ 行动/ ,/ 即/ 错误/ 捡取/ 或/ 放置/ 以及/ 导致/ 碰壁/ ,/ 则/ 系统/ 状态/ 不变/ ,/ 否则/ 系统/ 状态/ 做出/ 相应/ 转变/ ./ 该/ Agent/ 垃圾/ 收集/ 系统/ 为/ 逗留/ 时间/ 服从/ 指数分布/ 的/ 连续/ 时间/ MDP/ 模型/ ,/ 其/ 优化/ 目标/ 为/ Agent/ 根据/ 决策/ 时刻/ 的/ 状态/ ,/ 采取/ 最优/ 行动/ ,/ 使/ 系统/ 报酬/ 准则/ 函数/ 值/ 在/ 无穷/ 时间/ 水平/ 下/ 期望值/ 最大/ ./ 现把/ 第/ 3/ 节/ 介绍/ 的/ SA/ -/ Option/ 算法/ 运用/ 到/ 此/ 系统/ 中/ ,/ 为了/ 实现/ 分层/ 强化/ 学习/ 方法/ 中/ 状态/ 和/ 宏/ 行动/ 的/ 合理/ 抽象/ ,/ 首先/ 把/ 系统/ 环境/ 离散/ 化成/ 8/ ×/ 8/ 的/ 栅格/ ,/ 走廊/ 标记/ 成/ 两个/ 房间/ 并/ 对/ 房间/ 号/ 重新/ 编排/ ,/ 如图/ 4/ 所示/ ./ 然后/ ,/ 对/ 该/ Agent/ 进行/ 分层/ 控制/ ,/ 总共/ 可以/ 分为/ 3/ 层/ ,/ 即/ 顶层/ (/ 1/ )/ ,/ 中间层/ (/ 2/ )/ ,/ 最底层/ (/ 3/ )/ ./ 于是/ 有/ 下列/ 定义/ :/ X1/ (/ t1i/ )/ =/ (/ XRoom/ (/ t1i/ )/ ,/ XA/ (/ t1i/ )/ ,/ XT/ (/ t1i/ )/ )/ :/ 系统/ 顶层/ 在/ 其/ 第/ i/ 个/ 决策/ 时刻/ 的/ 状态/ ./ 其中/ ,/ XRoom/ (/ t1i/ )/ 表示/ 系统/ 在/ t1i/ 决策/ 时刻/ 所处/ 的/ 房间/ 号/ 状态/ ,/ XRoom/ (/ tci/ )/ ∈/ {/ 1/ ,/ 2/ ,/ …/ ,/ 5/ }/ ,/ 数字/ 表示/ 几号/ 房间/ ;/ XA/ (/ t1i/ )/ 表示/ 系统/ 在/ t1i/ 决策/ 时刻/ Agent/ 的/ 自身/ 状态/ ,/ XA/ (/ t1i/ )/ ∈/ {/ 0/ ,/ 1/ }/ ,/ 0/ 状态/ 表示/ Agent/ 没有/ 运载/ 垃圾/ ,/ 1/ 状态/ 表示/ 运载/ 垃圾/ ;/ XT/ (/ t1i/ )/ =/ (/ XT1/ (/ t1i/ )/ ,/ XT2/ (/ t1i/ )/ )/ 表示/ 系统/ 在/ t1i/ 时刻/ 两/ 垃圾箱/ 的/ 状态/ ;/ XTq/ (/ t1i/ )/ 表示/ 为/ 垃圾箱/ Tq/ ,/ q/ ∈/ {/ 1/ ,/ 2/ }/ ,/ 在/ 决策/ 时刻/ t1i/ 的/ 状态/ ,/ XTq/ (/ t1i/ )/ ∈/ {/ 0/ ,/ 1/ }/ ,/ 0/ 表示/ 垃圾箱/ Tq/ 处/ 没有/ 垃圾/ ,/ 1/ 表示/ 有/ 垃圾/ ;/ X2/ (/ t2i/ )/ =/ (/ Xs/ (/ t2i/ )/ ,/ XA/ (/ t2i/ )/ ,/ XTq/ (/ t2i/ )/ )/ :/ 系统/ 中间层/ 在/ 其/ 第/ i/ 个/ 决策/ 时刻/ 的/ 状态/ ./ 其中/ ,/ Xs/ (/ t2i/ )/ =/ (/ x/ (/ t2i/ )/ ,/ y/ (/ t2i/ )/ )/ 表示/ Agent/ 所在/ 的/ 位置/ ,/ x/ (/ t2i/ )/ 和/ y/ (/ t2i/ )/ 则/ 分别/ 为/ t2i/ 时刻/ Agent/ 所处/ 栅格/ 的/ 横坐标/ 和/ 纵坐标/ ,/ 其中/ x/ (/ t2i/ )/ ,/ y/ (/ t2i/ )/ ∈/ {/ 0/ ,/ 1/ ,/ …/ ,/ 7/ }/ ,/ 以图/ 4/ 的/ 左下方/ 为/ 坐标/ 原点/ ;/ XA/ (/ t2i/ )/ 同/ XA/ (/ t1i/ )/ ,/ XTq/ (/ t2i/ )/ 同/ XTq/ (/ t1i/ )/ 分别/ 是/ 一一对应/ 关系/ ;/ 若/ Agent/ 在/ 上层/ 选择/ 处理/ T1/ 的/ 垃圾/ ,/ 则/ q/ =/ 1/ ;/ 若/ Agent/ 在/ 上层/ 选择/ 处理/ T2/ 的/ 垃圾/ ,/ 则/ q/ =/ 2/ ./ X3/ (/ t3i/ )/ =/ (/ x/ (/ t3i/ )/ ,/ y/ (/ t3i/ )/ )/ :/ 系统/ 最底层/ 在/ 其/ 第/ i/ 个/ 决策/ 时刻/ Agent/ 所在/ 的/ 位置/ ./ X3/ (/ t3i/ )/ 与/ Xs/ (/ t2i/ )/ 是/ 一一对应/ 关系/ ./ o1/ (/ t1i/ )/ :/ t1i/ 时刻/ Agent/ 在/ 顶层/ 采取/ 的/ 宏/ 行动/ ,/ o1/ (/ t1i/ )/ ∈/ {/ 处理/ T1/ 的/ 垃圾/ ,/ 处理/ T2/ 的/ 垃圾/ ,/ 等待/ }/ ./ o2/ (/ t2i/ )/ :/ t2i/ 时刻/ Agent/ 在/ 中间层/ 采取/ 的/ 宏/ 行动/ ,/ o2/ (/ t2i/ )/ ∈/ {/ 移动/ 到/ Tq/ ,/ 垃圾/ 捡取/ ,/ 移动/ 到/ Dump/ ,/ 垃圾/ 放置/ }/ ,/ 其中/ q/ ∈/ {/ 1/ ,/ 2/ }/ ./ o3/ (/ t3i/ )/ :/ t3i/ 时刻/ Agent/ 在/ 最底层/ 采取/ 的/ 宏/ 行动/ ,/ o3/ (/ t3i/ )/ ∈/ {/ 上/ ,/ 下/ ,/ 左/ ,/ 右/ }/ ./ 在/ 该/ 仿真/ 实例/ 中/ ,/ 若/ 系统/ 在/ 状态/ Xu/ (/ tui/ )/ 处/ ,/ 选择/ 的/ 宏/ 行动/ ou/ (/ tui/ )/ 为/ 动作/ 原语/ ,/ 则/ 时间/ 累积/ 报酬/ 值/ f/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ ,/ Xu/ (/ tui/ +/ 1/ )/ )/ 按式/ (/ 3/ )/ 计算/ ,/ 有/ f/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ ,/ Xu/ (/ tui/ +/ 1/ )/ )/ =/ 其中/ ,/ 当/ 执行/ 捡取/ 或/ 放置/ 动作/ 时/ ,/ 正确/ 的/ 行动/ 对应/ 为/ k1/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ )/ =/ 1/ ,/ 而/ 当/ 其/ 是/ 错误/ 行动/ 时则/ 对应/ 为/ k1/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ )/ =/ -/ 1/ ;/ 当/ Agent/ 采取/ 上/ 、/ 下/ 、/ 左/ 、/ 右/ 行动/ 时/ ,/ k1/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ )/ =/ 0/ ;/ k2/ 表示/ 单位/ 时间/ 报酬率/ ./ 若/ 选择/ 的/ 宏/ 行动/ ou/ (/ tui/ )/ 为/ 非/ 动作/ 原语/ ,/ 则/ f/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ ,/ Xu/ (/ tui/ +/ 1/ )/ )/ 按式/ (/ 4/ )/ 进行/ 计算/ ./ 4.2/ 实验/ 结果/ 仿真/ 初始条件/ 如下/ :/ 对/ 任意/ 状态/ -/ 宏/ 行动/ 对/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ )/ ,/ 令/ 初始/ 学习/ 步长/ γ/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ )/ =/ 1/ // 8/ ,/ 其/ 衰减/ 规律/ 为/ 1/ // (/ 8/ ×/ N/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ )/ 0.2/ )/ ;/ 模拟退火/ 的/ 初始/ 温度/ 为/ T0/ =/ 1.0/ ×/ 1020/ ,/ 且/ 每/ 5000/ 步/ 降温/ 一次/ ,/ 降温/ 系数/ 为/ 0.9/ ./ 从图/ 4/ 可以/ 直接/ 看出/ ,/ 若/ Agent/ 从/ Dump/ 处/ 出发/ ,/ 正确/ 收集/ 一次/ T1/ 处/ 垃圾/ 最少/ 需要/ 执行/ 34/ 个/ 动作/ 原语/ ,/ 正确/ 收集/ T2/ 处/ 的/ 垃圾/ 最少/ 需要/ 执行/ 48/ 个/ 动作/ 原语/ ./ 若以/ 相等/ 的/ 概率/ 选择/ 收集/ T1/ 和/ T2/ 处/ 的/ 垃圾/ ,/ 则/ Agent/ 平均/ 收集/ 一次/ 垃圾/ 最少/ 需/ 执行/ 41/ 个/ 动作/ 原语/ ./ 由于/ 执行/ 动作/ 原语/ 的/ 时间/ 均值/ 为/ 0.5/ s/ ,/ 则/ 正确/ 收集/ 一次/ 垃圾/ 平均/ 最少/ 需要/ 20.5/ s/ ./ 若/ 每个/ 垃圾箱/ 垃圾/ 产生/ 率/ λ/ =/ 0.01/ ,/ 则/ 系统/ 平均/ 50s/ 生成/ 一个/ 垃圾/ ,/ 若/ λ/ =/ 0.03/ ,/ 平均/ 约/ 17s/ 生成/ 一个/ 垃圾/ ./ 因此/ ,/ 在/ 下面/ 仿真/ 实验/ 中/ ,/ 当/ λ/ =/ 0.01/ 时/ ,/ 表示/ 系统/ 中/ 的/ 垃圾/ 产生/ 率/ 不是/ 很大/ ,/ 系统/ 中/ 的/ 垃圾箱/ 有/ 较大/ 机会/ 出现/ 空/ 状态/ ;/ 当/ λ/ =/ 0.03/ 时/ ,/ 表示/ 系统/ 中/ 的/ 垃圾/ 产生/ 率/ 相对/ 较大/ ,/ 垃圾箱/ 大部分/ 情况/ 下/ 处于/ 有/ 垃圾/ 状态/ ./ 采用/ 这/ 两种/ 取值/ ,/ 可以/ 更好/ 地/ 将/ SA/ -/ Option/ 算法/ 与/ SA/ -/ Q/ 算法/ 进行/ 比较/ ./ 图/ 5/ 和/ 图/ 6/ 分别/ 为/ λ/ =/ 0.01/ 和/ λ/ =/ 0.03/ 时/ ,/ 两种/ 算法/ 的/ 平均/ 报酬/ 学习曲线/ ./ 这里/ 每/ 学习/ 10/ 步/ 记录/ 一次/ 平均/ 报酬/ 的/ 学习/ 值/ ./ 对比/ 两图/ 可见/ ,/ SA/ -/ Option/ 算法/ 的/ 平均/ 报酬/ 均/ 优于/ SA/ -/ Q/ 算法/ ./ 图/ 7/ 和/ 图/ 8/ 分别/ 为/ 图/ 5/ 和/ 图/ 6/ 在/ 学习/ 初始/ 时/ 的/ 截图/ ,/ 可见/ 从/ 初始/ 阶段/ 开始/ ,/ SA/ -/ Option/ 算法/ 的/ 平均/ 报酬/ 就/ 明显/ 优于/ SA/ -/ Q/ 算法/ ./ 这/ 是因为/ 前者/ 通过/ 分层/ 和/ 行动/ 抽象/ ,/ 在/ Page8/ 图/ 5/ α/ =/ 0/ ,/ λ/ =/ 0.01/ 时/ ,/ 两种/ 算法/ 的/ 平均/ 报酬/ 优化/ 曲线图/ 6/ α/ =/ 0/ ,/ λ/ =/ 0.03/ 时/ ,/ 两种/ 算法/ 的/ 平均/ 报酬/ 优化/ 曲/ 上层/ 进行/ 宏/ 决策/ ,/ 可/ 加快/ 算法/ 的/ 学习/ 速度/ ,/ 并/ 提高/ 其/ 优化/ 精度/ ./ 图/ 7/ 中/ SA/ -/ Option/ 算法/ 的/ 优化/ 曲线/ 在/ 200/ ×/ 10/ 步/ 左右/ 时/ 达到/ 一个/ 峰值/ ,/ 然后/ 有/ 一个/ 大/ 的/ 下降/ ./ 这是/ 由于/ 初始/ 学习/ 时/ ,/ 机器人/ 正确/ 收集/ 一次/ 垃圾/ 需要/ 很多/ 仿真/ 步数/ ,/ 导致/ 在/ 仿真/ 的/ 初始/ 阶段/ 垃圾箱/ 一直/ 处于/ 有/ 垃圾/ 的/ 状态/ ,/ 得到/ 该种/ 情况/ 下/ 的/ 较优/ 策略/ ./ 但/ 由于/ SA/ -/ Option/ 算法/ 收敛/ 很快/ ,/ 在/ 学到/ 200/ ×/ 10/ 步/ 左右/ 时/ ,/ SA/ -/ Option/ 算法/ 中/ 垃圾/ 收集/ 策略/ 就/ 渐渐/ 成熟/ ,/ 成功/ 收集/ 一次/ 垃圾/ 的/ 时间/ 也/ 随之/ 变短/ ,/ 导致/ 垃圾箱/ 有时/ 处于/ 空/ 状态/ ,/ 因此/ Agent/ 需要/ 重新学习/ 此种/ 情况/ 下/ 的/ 策略/ ,/ 因而/ 导致/ 平均/ 报酬/ 下降/ ./ 当/ 垃圾/ 产生/ 率/ 很大/ 时/ ,/ 无论是/ 在/ 学习/ 的/ 初始/ 阶段/ 还是/ 后期/ ,/ Agent/ 学习/ 较/ 多/ 的/ 是/ 系统/ 处于/ 有/ 垃圾/ 的/ 状态/ ,/ 因此/ 不会/ 出现/ 图/ 7/ 中/ 的/ 明显/ 峰值/ 现象/ ,/ 具体/ 可/ 参见/ 图/ 8/ 中/ 的/ SA/ -/ Option/ 曲线/ ./ 而图/ 7/ 和/ 图/ 8/ 中/ 的/ SA/ -/ Q/ 优化/ 曲线/ 基本上/ 没有/ 出现/ 明显/ 的/ 峰值/ 现象/ ,/ 皆/ 呈/ 平稳/ 的/ 上升/ 趋势/ ./ 这是/ 由于/ 该/ 算法/ 的/ 学习/ 优化/ 速度/ 比/ SA/ -/ Option/ 算法/ 要慢/ ,/ 不/ 存在/ 初始/ 阶段/ 很快/ 学习/ 收敛/ 的/ 情形/ ./ 图/ 7/ α/ =/ 0/ ,/ λ/ =/ 0.01/ 时/ ,/ 两种/ 算法/ 的/ 平均/ 报酬/ 优化/ 曲线图/ 8/ α/ =/ 0/ ,/ λ/ =/ 0.03/ 时/ ,/ 两种/ 算法/ 的/ 平均/ 报酬/ 优化/ 曲线/ 表/ 1/ 给出/ SA/ -/ Option/ 算法/ 与/ SA/ -/ Q/ 算法/ 在/ 平均/ 准则/ 下/ 的/ 其它/ 性能/ 值/ 比较/ ./ 在/ λ/ =/ 0.01/ 时/ ,/ SA/ -/ Q/ 算法/ 的/ 平均/ 报酬/ 为/ 0.0497/ ,/ 而/ SA/ -/ Option/ 算法/ 的/ 平均/ 报酬/ 为/ 0.1702/ ,/ 相差/ 0.1205/ ./ 这/ 是因为/ SA/ -/ Option/ 算法/ 通过/ 分层/ ,/ 在/ 上层/ 进行/ 宏/ 决策/ ,/ 可以/ 有效/ 减少/ 微观/ 决策/ 的/ 失误/ 性/ ,/ 从而/ 减少/ 了/ 惩罚/ 次数/ ,/ 可以/ 明显提高/ 学习/ 值/ ,/ 即/ 增加/ 了/ 优化/ 精度/ ./ 表/ 1/ 两种/ 学习/ 算法/ 平均/ 报酬/ 和/ 所/ 需/ 存储空间/ 比较/ 算法/ 平均/ 报酬/ (/ λ/ =/ 0.01/ )/ 所/ 需/ 存储单元/ SA/ -/ QSA/ -/ Option/ 另外/ ,/ SA/ -/ Q/ 算法/ 在/ 学习/ 过程/ 中需/ 存储/ 的/ 状态/ -/ 行动/ 对/ (/ X/ (/ ti/ )/ ,/ a/ (/ ti/ )/ )/ 为/ 512/ ×/ 7/ =/ 3584/ 个/ ./ 在/ Option/ 算法/ 中/ ,/ 若/ 一个/ 子目标/ 只/ 包含/ 动作/ 原语/ ,/ 则/ 其/ 对应/ 的/ 子/ 策略/ 可以/ 根据/ 先验/ 经验/ 得到/ ./ 根据/ 先验/ 经验/ 得到/ 某些/ 子目标/ 的/ 策略/ ,/ 不仅/ 由于/ 无需/ 子/ 策略/ 的/ 学习/ 而/ 节约/ 存储空间/ ,/ 还/ 可以/ 加快/ 整个/ 算法/ 的/ 学习/ 速度/ 并/ 有/ 可能/ 提高/ 学习/ 精度/ ./ 但是/ 先验/ 经验/ 有时/ 难以/ 得到/ ,/ 这/ 也/ Page9/ 是/ Option/ 算法/ 的/ 不足之处/ ./ 因此/ ,/ 在/ Option/ 算法/ 中/ ,/ 一般/ 可/ 通过/ 同步/ 学习/ 来/ 确定/ 子目标/ 的/ 策略/ ./ 此时/ ,/ 本/ 仿真/ 实验/ 系统/ 需/ 存储/ 的/ 状态/ -/ 宏/ 行动/ 对/ (/ Xu/ (/ tui/ )/ ,/ ou/ (/ tui/ )/ )/ 的/ 个数/ 为/ 40/ ×/ 3/ +/ (/ 256/ ×/ 4/ )/ ×/ 2/ +/ (/ 64/ ×/ 4/ )/ ×/ 3/ =/ 2936/ 个/ ,/ 则/ SA/ -/ Option/ 算法/ 比/ SA/ -/ Q/ 算法/ 节约/ 648/ 个/ 存储单元/ ,/ 空间/ 节约/ 率为/ 18.08/ %/ ./ 若/ 在/ SA/ -/ Option/ 算法/ 中/ 采用/ 先验/ 经验/ 确定/ 部分/ 策略/ ,/ 则/ 学习/ 过程/ 中需/ 存储/ 的/ 状态/ -/ 宏/ 行动/ 对/ 个数/ 为/ 40/ ×/ 3/ +/ (/ 256/ ×/ 4/ )/ ×/ 2/ =/ 2168/ 个/ ,/ 比/ SA/ -/ Q/ 算法/ 节约/ 1416/ 个/ 存储单元/ ,/ 节约/ 率约/ 39.51/ %/ ./ 其中/ 节约/ 率/ κ/ 定义/ 为/ 其中/ ,/ B/ 表示/ SA/ -/ Q/ 所需/ 存储空间/ ,/ C/ 表示/ SA/ -/ Option/ 所需/ 存储空间/ ./ 图/ 9/ 为/ α/ =/ 0.1/ ,/ λ/ =/ 0.01/ 时/ ,/ SA/ -/ Option/ 算法/ 下/ 状态/ X11/ (/ t11/ )/ =/ (/ 3/ ,/ 0/ ,/ 0/ ,/ 1/ )/ 、/ X21/ (/ t11/ )/ =/ (/ 1/ ,/ 0/ ,/ 1/ ,/ 0/ )/ 和/ X31/ (/ t11/ )/ =/ (/ 4/ ,/ 0/ ,/ 0/ ,/ 1/ )/ 的/ 折扣/ 报酬/ 学习曲线/ ;/ 图/ 10/ 为/ α/ =/ 0.1/ ,/ λ/ =/ 0.01/ 时/ ,/ SA/ -/ Q/ 算法/ 下/ 状态/ X1/ (/ t1/ )/ =/ (/ 2/ ,/ 6/ ,/ 0/ ,/ 0/ ,/ 1/ )/ 、/ X2/ (/ t1/ )/ =/ (/ 6/ ,/ 0/ ,/ 0/ ,/ 1/ ,/ 0/ )/ 和/ X3/ (/ t1/ )/ =/ (/ 3/ ,/ 0/ ,/ 图/ 9/ α/ =/ 0.1/ ,/ λ/ =/ 0.01/ 时/ ,/ SA/ -/ Option/ 算法/ 不同/ 图/ 10/ α/ =/ 0.1/ ,/ λ/ =/ 0.01/ 时/ ,/ SA/ -/ Q/ 算法/ 不同/ 初始/ 位置/ 的/ 0/ ,/ 0/ ,/ 1/ )/ 的/ 折扣/ 报酬/ 学习曲线/ ./ 其中/ Xj1/ (/ t11/ )/ 表示/ SA/ -/ Option/ 算法/ 顶层/ 在/ 第一个/ 决策/ 时刻/ t11/ 的/ 初始状态/ j/ ,/ Xj/ (/ t1/ )/ 表示/ SA/ -/ Q/ 算法/ 在/ 第/ 1/ 个/ 决策/ 时刻/ t1/ 的/ 初始状态/ j/ ,/ j/ ∈/ {/ 1/ ,/ 2/ ,/ 3/ }/ ./ 在/ 这/ 两种/ 算法/ 中/ ,/ 初始状态/ 是/ 一一对应/ 的/ ,/ 如/ X11/ (/ t11/ )/ 对应/ X1/ (/ t1/ )/ ./ 对比/ 两图/ 可得/ ,/ 折扣/ 情况/ 下/ ,/ SA/ -/ Option/ 算法/ 的/ 优化/ 精度/ 也/ 比/ SA/ -/ Q/ 算法/ 高/ ,/ 并且/ SA/ -/ Option/ 算法/ 的/ 折扣/ 报酬/ 在/ 学习/ 的/ 初始/ 阶段/ 迅速/ 提高/ ,/ SA/ -/ Q/ 算法/ 的/ 折扣/ 报酬/ 上升/ 则/ 比较/ 平缓/ ./ 这/ 表明/ 在/ 折扣/ 准则/ 下/ ,/ 与/ SA/ -/ Q/ 学习/ 相比/ ,/ SA/ -/ Option/ 算法/ 的/ 学习/ 速度/ 和/ 学习/ 优化/ 结果/ 也/ 都/ 更/ 有/ 优势/ ,/ 表/ 2/ 为/ 部分/ 比较/ 结果/ ./ 这是/ 由于/ SA/ -/ Option/ 算法/ 通过/ 分层/ 和/ 行动/ 抽象/ ,/ 减少/ 了/ 微观/ 决策/ 的/ 失误/ 性/ ,/ 在/ 仿真/ 的/ 初始/ 阶段/ 就/ 能/ 得到/ 较/ 好/ 的/ 策略/ ,/ 其/ 折扣/ 报酬/ 基本上/ 不/ 受/ 初始状态/ 的/ 影响/ ;/ 而/ SA/ -/ Q/ 算法/ 通过/ 每个/ 行动/ 的/ 选择/ 来/ 学习策略/ ,/ 学习/ 速度/ 比/ SA/ -/ Option/ 算法/ 慢/ ,/ 其/ 初始状态/ 将/ 很大/ 地/ 影响/ 折扣/ 报酬/ ./ 因此/ ,/ 与/ SA/ -/ Q/ 算法/ 明显/ 不同/ 的/ 是/ ,/ SA/ -/ Option/ 算法/ 的/ 折扣/ 报酬/ 学习/ 值/ 对/ 初始状态/ 很/ 不/ 敏感/ ./ 表/ 2/ 两种/ 学习/ 算法/ 不同/ 初始状态/ 的/ 折扣/ 报酬/ 比较/ 算法/ 初始状态/ 1/ 初始状态/ 2/ 初始状态/ 3SA/ -/ Q0/ ./ 04050.1166/ -/ 0.9491/ SA/ -/ Option0/ ./ 28150.30710/ ./ 33495/ 总结/ 本文/ 针对/ 连续/ 时间/ 和/ 无穷/ 任务/ 问题/ ,/ 在/ 性能/ 势/ 理论/ 和/ CT/ -/ SMDP/ 模型/ 下/ ,/ 结合/ 分层/ 强化/ 学习/ 中/ 现有/ 的/ Option/ 算法/ ,/ 提出/ 了/ 一种/ 适用/ 于/ 平均/ 或/ 折扣/ 准则/ 的/ 连续/ 时间/ 统一/ 分层/ 强化/ 学习/ 优化/ 算法/ (/ SA/ -/ Option/ )/ ./ 为/ 验证/ 这种/ 算法/ 的/ 有效性/ ,/ 利用/ 机器人/ 垃圾/ 收集/ 系统/ 为例/ ,/ 建立/ 了/ 相应/ 的/ 分层/ 优化/ 模型/ 和/ Option/ 分层/ 算法/ ./ 仿真/ 结果表明/ ,/ 构造/ 的/ 统一/ SA/ -/ Option/ 算法/ 在/ 求解/ 连续/ 时间/ 和/ 无穷/ 任务/ 问题/ 时/ ,/ 比/ SA/ -/ Q/ 算法/ 具有/ 明显/ 的/ 优势/ ./ 一方面/ ,/ 该/ 算法/ 能够/ 节约/ 存储空间/ ,/ 另一方面/ ,/ 它/ 能够/ 提高/ 学习/ 速度/ ,/ 并/ 在/ 一定/ 程度/ 上/ 提高/ 了/ 学习/ 精度/ ./ 此外/ ,/ 本文/ 研究/ 的/ 算法/ 可/ 进一步/ 扩展/ 到/ 解决/ 连续/ 时间/ 无穷/ 任务/ 的/ 多/ 机器人/ 协作/ 问题/ ./ 

