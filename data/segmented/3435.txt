Page1/ 自然语言/ 处理/ 中/ 主题/ 模型/ 的/ 发展/ 徐戈/ 王厚峰/ (/ 北京大学/ 计算/ 语言学/ 研究所/ ,/ 北京大学/ 计算/ 语言学/ 教育部/ 重点/ 实验室/ 北京/ 100871/ )/ 摘要/ 主题/ 模型/ 在/ 自然语言/ 处理/ 领域/ 受到/ 了/ 越来越/ 多/ 的/ 关注/ ./ 在/ 该/ 领域/ 中/ ,/ 主题/ 可以/ 看成/ 是/ 词项/ 的/ 概率分布/ ./ 主题/ 模型/ 通过/ 词项/ 在/ 文档/ 级/ 的/ 共现/ 信息/ 抽取/ 出/ 语义/ 相关/ 的/ 主题/ 集合/ ,/ 并/ 能够/ 将/ 词项/ 空间/ 中/ 的/ 文档/ 变换/ 到/ 主题/ 空间/ ,/ 得到/ 文档/ 在/ 低/ 维空间/ 中/ 的/ 表达/ ./ 作者/ 从/ 主题/ 模型/ 的/ 起源/ 隐性/ 语义/ 索引/ 出发/ ,/ 对/ 概率/ 隐性/ 语义/ 索引/ 以及/ LDA/ 等/ 在/ 主题/ 模型/ 发展/ 中/ 的/ 重要/ 阶段性/ 工作/ 进行/ 了/ 介绍/ 和/ 分析/ ,/ 着重/ 描述/ 这些/ 工作/ 之间/ 的/ 关联性/ ./ LDA/ 作为/ 一个/ 概率/ 生成/ 模型/ ,/ 很/ 容易/ 被/ 扩展/ 成/ 其它/ 形式/ 的/ 概率模型/ ./ 作者/ 对/ 由/ LDA/ 派/ 生出/ 的/ 各种/ 模型/ 作/ 了/ 粗略/ 分类/ ,/ 并/ 选择/ 了/ 各类/ 的/ 代表性/ 模型/ 简单/ 介绍/ ./ 主题/ 模型/ 中/ 最/ 重要/ 的/ 两组/ 参数/ 分别/ 是/ 各/ 主题/ 下/ 的/ 词项/ 概率分布/ 和/ 各/ 文档/ 的/ 主题/ 概率分布/ ,/ 作者/ 对/ 期望/ 最大化/ 算法/ 在/ 主题/ 模型/ 参数估计/ 中/ 的/ 使用/ 进行/ 了/ 分析/ ,/ 这/ 有助于/ 更/ 深刻理解/ 主题/ 模型/ 发展/ 中/ 各项/ 工作/ 的/ 联系/ ./ 关键词/ 自然语言/ 处理/ ;/ 主题/ 模型/ ;/ 隐性/ 语义/ 索引/ ;/ LDA/ ;/ 期望/ 最大化/ 算法/ ;/ Gibbs/ 采样/ 1/ 引言/ 在/ 自然语言/ 处理/ 中/ ,/ 主题/ (/ topic/ )/ ①/ 可以/ 看成/ 是/ 词项/ 的/ 概率分布/ ./ 我们/ 使用/ 主题/ 模型/ 对/ 文档/ 的/ 生成/ 过程/ 进行/ 模拟/ ,/ 再/ 通过/ 参数估计/ 得到/ 各个/ 主题/ ./ 当以/ 词袋/ (/ bagofwords/ )/ 形式/ 表示/ 文档/ 时/ ,/ 其/ 维度/ 可能/ 是/ 数万/ ./ 若/ 指定/ 主题/ 模型/ 的/ 主题/ 个数/ 为/ K/ ,/ 通过/ 主题/ 模型/ 的/ 训练/ ,/ 最终/ 形成/ 了/ K/ 个/ 主题/ ,/ 则/ 可以/ 将/ 词项/ 空间/ 中/ 的/ 文档/ 变换/ 到/ 主题/ 空间/ ,/ 得到/ 文档/ 新/ 的/ 表达/ ./ 由于/ 通常/ 主题/ 的/ 个数/ K/ 远/ 小于/ 词项/ 的/ 个数/ ,/ 常/ 使用/ 主题/ 模型/ 进行/ 降维/ ./ 在/ 以/ 文本/ 为/ 处理/ 对象/ 的/ 领域/ 中/ ,/ 降维后/ 的/ 新/ 坐标/ (/ 即/ 在/ K/ 个/ 主题/ 上/ 的/ 分量/ )/ 往往/ 具有/ 语义上/ 的/ 特征/ ./ 图/ 1/ 是/ 在/ 人民日报/ 语料/ 上/ 通过/ LDA/ (/ LatentDirichletAllocation/ )/ 模型/ 训练/ 得到/ 的/ 一部分/ 主题/ ./ 每个/ 主题/ 中/ 的/ 词项/ 按照/ 在/ 该/ 主题/ 中/ 的/ 概率/ 降序/ 排列/ ./ 其中/ 主题/ 1/ 表示/ “/ 国家/ ”/ 相关/ 的/ 概念/ ,/ 主题/ 2/ 表示/ 了/ “/ 中国/ 人民代表大会/ ”/ 相关/ 的/ 概念/ 等等/ ./ 法国/ 欧洲/ 德国/ 欧盟/ 巴黎/ 希拉克/ 瑞典/ 主题/ 1/ 主题/ 2/ 主题/ 3/ 主题/ 4/ 主题/ 5/ 图/ 1/ 人民日报/ 语料/ 在/ LDA/ 模型/ 上/ 的/ 训练/ 结果/ (/ 部分/ )/ 主题/ 模型/ 的/ 起源/ 是/ 隐性/ 语义/ 索引/ (/ LatentSemanticIndexing/ ,/ LSI/ )/ [/ 1/ ]/ ./ 隐性/ 语义/ 索引/ 并/ 不是/ 概率模型/ ,/ 因此/ 也/ 算不上/ 一个/ 主题/ 模型/ ,/ 但是/ 其/ 基本/ 思想/ 为/ 主题/ 模型/ 的/ 发展/ 奠定/ 了/ 基础/ ./ 在/ LSI/ 的/ 基础/ 上/ ,/ Hofmann/ [/ 2/ ]/ 提出/ 了/ 概率/ 隐性/ 语义/ 索引/ (/ probabi/ -/ listicLatentSemanticIndexing/ ,/ pLSI/ )/ ,/ 该/ 模型/ 被/ 看成/ 是/ 一个/ 真正/ 意义/ 上/ 的/ 主题/ 模型/ ./ 而/ Blei/ 等/ 人/ [/ 3/ ]/ 提出/ 的/ LDA/ (/ LatentDirichletAllocation/ )/ 又/ 在/ pLSI/ 的/ 基础/ 上/ 进行/ 了/ 扩展/ 得到/ 一个/ 更为/ 完全/ 的/ 概率/ 生成/ 模型/ ./ 近几年来/ ,/ 与/ 特定/ 的/ 任务/ 相结合/ ,/ 出现/ 了/ 越来越/ 多/ 的/ 基于/ LDA/ 的/ 概率模型/ ./ 本文/ 第/ 2/ 节对/ 主题/ 模型/ 的/ 主要/ 内容/ 进行/ 归纳/ ;/ 第/ 3/ 节/ 简单/ 介绍/ EM/ 算法/ ;/ 第/ 4/ 节到/ 第/ 8/ 节/ 按照/ 主题/ 模型/ 的/ 发展/ 过程/ 依次/ 介绍/ LSI/ ,/ pLSI/ ,/ LDA/ 以及/ LDA/ 的/ 扩展/ 模型/ ;/ 最后/ 第/ 9/ 节/ 总结/ 全文/ 并/ 展望/ 下/ 一步/ 的/ 工作/ ./ 2/ 主题/ 模型/ 的/ 主要/ 内容/ 一个/ 主题/ 模型/ 通常/ 包括/ 5/ 项/ 内容/ (/ 见/ 2.1/ 节/ ~/ 2.5/ 节/ )/ ./ 一般/ ,/ 主题/ 模型/ 的/ 输入/ 和/ 基本/ 假设/ 这/ 两/ 部分/ 对/ 大部分/ 主题/ 模型/ 都/ 是/ 相同/ 的/ ,/ 因此/ 针对/ 具体/ 的/ 主题/ 模型/ 分析/ 时/ 一般/ 不再/ 涉及/ ./ 主题/ 模型/ 的/ 表示/ 、/ 参数估计/ 和/ 新/ 样本/ 推断/ 3/ 个/ 部分/ 在/ 不同/ 的/ 主题/ 模型/ 中/ 有所不同/ ,/ 我们/ 将/ 在/ 具体/ 的/ 主题/ 模型/ 中/ 分别/ 介绍/ ./ 2.1/ 主题/ 模型/ 的/ 输入/ 主题/ 模型/ 的/ 主要/ 输入/ 是/ 文档/ 集合/ ,/ 由于/ 交换/ 性/ 的/ 假设/ (/ 见/ 2.2/ 节/ )/ ,/ 等价/ 于/ 词项/ 文档/ (/ term/ -/ docu/ -/ ment/ )/ 矩阵/ ,/ 图/ 2/ 是/ 词项/ 文档/ 矩阵/ 的/ 一个/ 实例/ ./ 从/ 该词/ 项/ 文档/ 矩阵/ 可以/ 看出/ ,/ 语料/ 包括/ 6/ 篇/ 文档/ ,/ 整个/ 语料/ 中共/ 有/ 5/ 个/ 词项/ ②/ ,/ 文档/ d1/ 中/ ship/ 和/ ocean/ ,/ voyage/ 三个/ 词项/ 各/ 出现/ 一次/ ./ 注意/ 同一个/ 词项/ 在/ 一篇/ 文档/ 中/ 可以/ 出现/ 多次/ ./ 另外/ 还有/ 一个/ 重要/ 输入/ 就是/ 主题/ 个数/ K/ ./ 通常/ ,/ K/ 的/ 大小/ 需要/ 在/ 模型/ 训练/ 前/ 指定/ ,/ 而且/ 存在/ 一定/ 的/ 经验性/ ./ 确定/ 最优/ K/ 的/ 简单/ 方法/ 是/ 用/ 不同/ 的/ K/ 重复/ 实验/ ,/ 当/ 评价/ 指标/ 如/ 困惑/ 度/ (/ perplexity/ )/ 、/ 语料/ 似然值/ 、/ 分类/ 正确率/ 等/ 最优/ 时/ 认为/ 此时/ 的/ K/ 是/ 模型/ 的/ 最佳/ 选择/ [/ 3/ -/ 6/ ]/ ./ 也/ 有/ 作者/ 用非/ 参数/ 贝叶斯/ 的/ 方法/ 来/ 选择/ 模型/ 的/ 合适/ 主题/ 数目/ [/ 7/ -/ 8/ ]/ ,/ 该/ 方法/ 假设/ 主题/ 个数/ 为/ 无穷/ 多/ ,/ 实际/ 主题/ 个数/ 可以/ 随着/ 语料/ 的/ 规模/ 而/ 变化/ ,/ 训练/ 结束/ 时/ 的/ 主题/ 个数/ 即/ K/ 的/ 最佳/ 选择/ ./ 2.2/ 主题/ 模型/ 中/ 的/ 基本/ 假设/ 主题/ 模型/ 中/ 的/ 一个/ 重要/ 假设/ 是/ 词袋/ (/ bagofwords/ )/ 假设/ ,/ 即/ 一篇/ 文档/ 内/ 的/ 单词/ 可以/ 交换/ 次序/ 而/ 不/ 影响/ 模型/ 的/ 训练/ 结果/ ./ 可/ 交换/ (/ exchangeability/ )/ 可以/ 简单/ 理解/ 为/ 与/ 顺序/ 无关/ ,/ 和/ 条件/ 独立/ 同/ 分布/ 等价/ ./ 事实上/ ,/ 通过观察/ 2.4/ 节中/ 的/ 似然/ 函数/ ,/ 我们/ 可以/ 看出/ 文档/ 也/ 是/ 可/ 交换/ 的/ ,/ 即/ 语料/ 中/ 文档/ 的/ 次序/ 也/ ①/ ②/ Page3/ 不/ 影响/ 模型/ 的/ 训练/ 结果/ [/ 3/ ]/ ./ 需要/ 指出/ 的/ 是/ ,/ 在/ LDA/ 的/ 一些/ 派生/ 模型/ 中/ ,/ 一些/ 可/ 交换/ 性会/ 被/ 打破/ ,/ 以便/ 构造/ 相应/ 的/ 模型/ ,/ 读者/ 可以/ 参考/ 本文/ 7.2/ 小节/ 中/ 的/ 有关/ 实例/ ./ 2.3/ 主题/ 模型/ 的/ 表示/ 生成/ 过程/ ./ 对/ LDA/ 模型/ 的/ 表示/ ./ 以/ LDA/ 模型/ 为例/ [/ 3/ ]/ ,/ 图/ 3/ 是/ 使用/ 图/ 模型/ 的/ 方法/ 主题/ 模型/ 的/ 表示/ 有/ 两种/ ,/ 分别/ 是/ 使用/ 图/ 模型/ 和/ 方框/ 表示/ 其中/ 的/ 内容/ 进行/ 重复/ ,/ 右下角/ 是/ 重复/ 的/ 次数/ ;/ 灰色/ 节点/ 表示/ 观测/ 值/ ,/ 空心/ 节点/ 表示/ 隐含/ 随机变量/ 或者/ 参数/ ,/ 箭头/ 代表/ 依赖/ 关系/ ./ α/ 是/ θ/ 的/ 超/ 参数/ ,/ β/ 是/ K/ ×/ V/ 的/ 参数/ 集合/ ,/ 每行/ 代表/ 某个/ 主题/ 中/ 的/ 词项/ 概率分布/ ,/ K/ 是/ 主题/ 个数/ ,/ V/ 是/ 词项/ 个数/ ;/ θ/ 表示/ 某/ 文档/ 的/ 主题/ 概率分布/ ,/ 共/ M/ 个/ ,/ M/ 为/ 文档/ 个数/ ./ w/ 为/ 单词/ ,/ z/ 为/ w/ 的/ 主题/ 标号/ ./ 我们/ 也/ 可以/ 通过/ 生成/ 过程/ 来/ 对/ 主题/ 模型/ 进行/ 描述/ ,/ 即/ LDA/ 模型/ 是/ 按照/ 如图/ 4/ 所示/ 的/ 方式/ 生成/ 一篇/ 文档/ ,/ 重复/ M/ 次则/ 生成/ 整个/ 语料/ ./ 2.4/ 参数估计/ 过程/ 在/ 主题/ 模型/ 中/ ,/ 最/ 重要/ 的/ 两组/ 参数/ 分别/ 是/ 各/ 主题/ 下/ 的/ 词项/ 概率分布/ 和/ 各/ 文档/ 的/ 主题/ 概率分布/ ①/ ./ 参数估计/ 可以/ 看成/ 是/ 生成/ 过程/ 的/ 逆/ 过程/ :/ 即/ 在/ 已知/ 文档/ 集/ (/ 即/ 生成/ 的/ 结果/ )/ 的/ 情况/ 下/ ,/ 通过/ 参数估计/ ,/ 得到/ 参数值/ ./ 这些/ 估计值/ 也/ 就是/ 我们/ 整个/ 训练/ 过程/ 的/ 输出/ 结果/ ./ 针对/ 参数估计/ 我们/ 需要/ 选择/ 最优化/ 的/ 目标/ 函数/ ,/ 在/ 主题/ 模型/ 中/ 通常/ 是/ 整个/ 语料/ 的/ 概率/ 值/ ./ 以/ LDA/ 模型/ 为例/ [/ 3/ ]/ ,/ 根据/ 其图/ 模型/ 很/ 容易/ 得到/ 语料/ 概率/ 值/ p/ (/ D/ |/ α/ ,/ β/ )/ 为/ d/ =/ 1/ ∫/ p/ (/ θ/ d/ α/ )/ ∏/ Nd/ ∏/ M/ 其中/ ,/ D/ 代表/ 整个/ 语料/ ,/ 也/ 就是/ 所有/ 文档/ 的/ 集合/ ;/ Nd/ 表示/ 第/ d/ 篇/ 文档/ 长度/ ;/ θ/ d/ 表示/ 第/ d/ 篇/ 文档/ 的/ 主题/ 概率分布/ ;/ wdn/ 表示/ 第/ d/ 篇/ 文档/ 的/ 第/ n/ 个/ 单词/ ;/ zdn/ 表示/ wdn/ 主题/ ./ 该/ 函数/ 以/ α/ 和/ β/ 作为/ 参数/ ,/ 通过/ 对/ 目标/ 函数/ 进行/ 最大化/ 来/ 估计/ α/ 和/ β/ 的/ 值/ ./ 2.5/ 新/ 样本/ 的/ 推断/ 主题/ 模型/ 训练/ 完成/ 后/ ,/ 我们/ 便/ 可以/ 使用/ 训练/ 好/ 的/ 主题/ 模型/ 对/ 新/ 的/ 样本/ 进行/ 推断/ ,/ 通过/ 主题/ 模型/ 将/ 以/ 词项/ 空间/ 表达/ 的/ 文档/ 变换/ 到/ 新/ 的/ 主题/ 空间/ ,/ 得到/ 一个/ 以/ 主题/ 为/ 坐标/ 的/ 低维/ 表达/ ,/ 该/ 表达/ 也/ 就是/ 文档/ 的/ 主题/ 概率分布/ ./ 新/ 样本/ 的/ 推断/ 不仅/ 可以/ 针对/ 新/ 的/ 文档/ ,/ 还/ 可以/ 针对/ 查询/ ,/ 以便/ 应用/ 于/ 信息检索/ 之中/ ./ 3/ 期望/ 最大化/ 算法/ 和/ 参数估计/ 期望/ 最大化/ 算法/ (/ ExpectationMaximization/ ,/ EM/ )/ 由/ Dempster/ 等/ 人/ [/ 9/ ]/ 于/ 1977/ 年/ 提出/ ,/ 是/ 一种/ 对/ 具有/ 隐/ 变量/ (/ 缺失/ 数据/ )/ 的/ 概率模型/ 寻找/ 极大/ 似然/ 估计/ 的/ 一般性/ 方法/ ./ 该/ 算法/ 通过/ 迭代/ 不断/ 修改/ 模型/ 参数/ 直到/ 达到/ 局部/ 最/ 优点/ ,/ 即/ 每次/ 都/ 用/ 现有/ 的/ 模型/ 推断/ 隐/ 变量/ 的/ 后验/ 概率分布/ ,/ 然后/ 对/ 参数/ 重新/ 估计/ 得到/ 一个/ 新/ 的/ 模型/ ,/ 如此/ 反复/ 直到/ 满足/ 终止/ 条件/ ./ 由于/ EM/ 算法/ 不能/ 保证/ 全局/ 最优/ 解/ ,/ 因此/ 有/ 的/ 时候/ 需要/ 变换/ 参数/ 的/ 初始值/ ,/ 或者/ 选择/ 较/ 多/ 的/ 迭代/ 次数/ ,/ 才能/ 得到/ 较为理想/ 的/ 参数/ 估计值/ ./ 在/ 自然语言/ 处理/ 中/ ,/ 常见/ 的/ 诸如/ 隐/ 马尔可夫/ 模型/ (/ HMM/ )/ 、/ 高斯/ 混合/ 模型/ (/ GMM/ )/ 、/ k/ -/ 均值/ 算法/ (/ k/ -/ means/ )/ 、/ 主/ 成分/ 分析/ (/ PCA/ )/ 等/ 都/ 可以/ 用/ EM/ 算法/ 的/ 思想/ 来/ 解释/ ./ 一般/ 情况/ 下/ ,/ 主题/ 模型/ 中/ 的/ 参数估计/ 问题/ 很难/ 得到/ 精确/ 解/ ,/ 可以/ 使用/ EM/ 算法/ 来/ 得到/ 近似/ 解/ ./ EM/ 算法/ 简介/ 如下/ [/ 10/ ]/ :/ 已知/ 一个/ 概率模型/ ,/ 包括/ :/ 1/ ./ 隐/ 变量/ 集/ Z/ ;/ 2/ ./ 观测/ 值集/ X/ ;/ 3/ ./ 参数/ 集/ θ/ 目标/ :/ 得到/ p/ (/ X/ |/ θ/ )/ 最大化/ 时/ 的/ θ/ EM/ 算法/ 过程/ :/ 初始化/ θ/ E/ 步骤/ :/ 以/ 当前/ θ/ old/ 估计/ p/ (/ Z/ |/ X/ ,/ θ/ old/ )/ M/ 步骤/ :/ 利用/ 前/ 一步/ 的/ 结果/ ,/ 对/ θ/ 最大化/ 如下/ 式子/ :/ ∑/ Z/ 重复/ E/ ,/ M/ 步骤/ 直到/ 满足/ 结束/ 条件/ ./ 在/ 主题/ 模型/ 中/ ,/ 主题/ 通常/ 表示/ 为/ 隐/ 变量/ ,/ 单词/ 为/ ①/ 各个/ 模型/ 在/ 表示/ 这/ 两组/ 参数/ 的/ 时候/ 所用/ 符号/ 可能/ 会/ 不同/ ,/ Page4/ 观测/ 值/ ,/ 而/ 参数/ 集/ 通常/ 就是/ 各/ 主题/ 下/ 的/ 词项/ 概率分布/ 和/ 各/ 文档/ 的/ 主题/ 概率分布/ ./ 不同/ 的/ 主题/ 模型/ 中/ 的/ 观测/ 值/ 、/ 隐/ 变量/ 和/ 参数/ 集都/ 不尽相同/ ,/ 辨识/ 这些/ 元素/ 有助于/ 正确/ 和/ 快速/ 理解/ 主题/ 模型/ ./ 在/ 以下/ 几个/ 主要/ 的/ 主题/ 模型/ 的/ 分析/ 中/ ,/ 我们/ 将/ 以/ EM/ 算法/ 的/ 框架/ 来/ 理解/ 参数估计/ 过程/ ./ 4/ 隐性/ 语义/ 索引/ 隐性/ 语义/ 索引/ 中/ 的/ 奇异/ 值/ 分解/ (/ SingularValueDecomposition/ ,/ SVD/ )/ 与/ 主/ 成分/ 分析/ (/ PrincipalComponentAnalysis/ ,/ PCA/ )/ 有着/ 紧密/ 的/ 联系/ ,/ 在/ 介绍/ 隐性/ 语义/ 索引/ 之前/ ,/ 有/ 必要/ 先对主/ 成分/ 分析/ 作/ 简单/ 的/ 介绍/ ./ 4.1/ 主/ 成分/ 分析/ 主/ 成分/ 分析/ 将/ 高维/ 的/ 向量/ 变换/ 到/ 低/ 维空间/ ,/ 而且/ 低/ 维空间/ 中/ 各个/ 维度/ 不/ 相关/ ,/ 基本/ 过程/ 是/ 取/ 协方差/ 矩阵/ 犛/ (/ 见式/ (/ 1/ )/ )/ 的/ 前/ m/ 个/ 最大/ 的/ 特征值/ 对应/ 的/ 特征向量/ 来/ 构造/ 一个/ m/ 维/ 的/ 新/ 空间/ ./ 此处/ m/ 可以/ 理解/ 为/ 主题/ 模型/ 中/ 的/ 主题/ 个数/ K/ ,/ 也/ 需要/ 人为/ 指定/ ./ 对/ 原始/ 样本/ 作/ 近似/ 时/ ,/ 可以/ 证明/ 该/ 方法/ 产生/ 的/ 误差/ 最小/ [/ 10/ ]/ ./ 其中/ ,/ N/ 为/ 样本/ 个数/ ;/ 狓/ i/ 为/ 第/ i/ 个/ 样本/ ;/ μ/ 是/ 样本均值/ ./ 矩阵/ 犛/ 揭示/ 坐标/ 间/ 的/ 相关性/ ,/ 而/ 变换/ 后/ 的/ 样本/ 在/ 新/ 坐标/ 空间/ 中/ 的/ 协方差/ 矩阵/ 是/ 一个/ 以/ 降序/ 特征值/ 为主/ 对角线/ 元素/ 的/ 对角/ 阵/ ,/ 因此/ 在/ 新/ 的/ 空间/ 中/ 各个/ 坐标/ 统计/ 不/ 相关/ ./ 主/ 成分/ 分析/ 的/ 思想/ 在/ LSI/ 中有/ 充分/ 的/ 体现/ ,/ 即/ 构造/ 原/ 坐标/ 间/ 的/ 相似/ 度/ 矩阵/ ,/ 通过/ 特征向量/ 对/ 样本/ 进行/ 变换/ ,/ 在/ 新/ 的/ 坐标/ 空间/ 中/ 各个/ 坐标/ 间/ 统计/ 不/ 相关/ ,/ 且/ 新/ 空间/ 维度/ 一般/ 远/ 小于/ 原/ 空间/ 的/ 维度/ ./ 4.2/ 隐性/ 语义/ 索引/ 隐性/ 语义/ 索引/ 通过/ 奇异/ 值/ 分解/ 构造/ 一个/ 新/ 的/ 隐性/ 语义/ (/ LatentSemantic/ )/ 空间/ [/ 1/ ]/ ./ 该/ 空间/ 通常/ 比原/ 空间/ 维度/ 低/ ,/ 文档/ 或者/ 单词/ 可以/ 变换/ 到/ 这个/ 新/ 的/ 空间/ ,/ 找到/ 更/ 简单/ 的/ 表达/ ./ SVD/ 示意图/ 如图/ 5/ 所示/ [/ 1/ ]/ ./ 其中/ ,/ 犡/ 是/ 词/ 项/ 文档/ 矩阵/ ;/ t/ 是/ 词项/ 空间/ 的/ 维度/ ;/ d/ 是/ 文档/ 个数/ ;/ 犝/ ,/ 犞/ 都/ 是/ 正交/ 单位矩阵/ ;/ Σ/ 是/ 对角/ 阵且/ 主/ 对角线/ 上/ 的/ 元素/ 值/ 降序/ 排列/ ;/ m/ 是/ 犡/ 的/ 秩/ ;/ 犝/ 是/ 犡/ 犡/ T/ 的/ 特征向量/ 集/ ;/ 犞/ 是/ 犡/ T/ 犡/ 的/ 特征向量/ 集/ ,/ 犡/ 犡/ T/ 和/ 犡/ T/ 犡/ 的/ 特征值/ 相同/ ./ 犡/ 犡/ T/ 的/ 元素/ (/ i/ ,/ j/ )/ 代表/ 了/ 词项/ i/ 和/ 词项/ j/ 的/ 共现/ 次数/ (/ 以/ 文档/ 为/ 窗口/ 范围/ )/ ./ 这个/ 矩阵/ 反映/ 了/ 任意/ 两个/ 词项/ 之间/ 的/ 相似/ 度/ ./ 犝/ 代表/ 了/ 词项/ 空间/ 到/ 主题/ 空间/ 的/ 转换/ ./ 在/ LSI/ 的/ 介绍/ 中/ 一般/ 没有/ 提及/ 参数估计/ 的/ 问题/ ,/ 但/ 通过/ 主/ 成分/ 分析/ 我们/ 仍然/ 可以/ 把/ LSI/ 与/ EM/ 算法/ 联系/ 起来/ ./ LSI/ 可以/ 看成/ 是/ 对/ 两个/ 相似/ 度/ 矩阵/ 分别/ 做/ 了/ 主/ 成分/ 分析/ ,/ 而主/ 成分/ 分析/ 可以/ 通过/ EM/ 算法/ 进行/ 解释/ ./ 我们/ 可以/ 把/ 特征向量/ 看成/ 是/ 待/ 估计/ 的/ 参数/ ,/ 样本/ 在/ 新/ 空间/ 的/ 坐标/ (/ 隐性/ 语义/ )/ 看成/ 是/ 隐/ 变量/ ,/ 套用/ EM/ 算法/ 的/ 框架/ 来/ 迭代/ 求解/ ./ 这种/ 方法/ 尤其/ 适合/ 相似/ 度/ 矩阵/ 维度/ 很/ 高/ 无法/ 直接/ 处理/ ,/ 或者/ 存在/ 数据/ 缺失/ 的/ 情况/ [/ 11/ ]/ ./ 在/ 文献/ [/ 10/ ]/ 中用/ 了/ 一个/ 形象/ 的/ 实例/ 解释/ 主/ 成分/ 分析/ 中/ EM/ 算法/ 的/ 过程/ ./ 无论是/ 训练/ 集中/ 的/ 文档/ ,/ 还是/ 一篇/ 新/ 的/ 文档/ ,/ 都/ 可以/ 通过/ SVD/ 分解/ 后/ 得到/ 的/ 矩阵/ 把/ 文档/ 变换/ 到/ 隐性/ 语义/ 空间/ ,/ 公式/ 如下/ 其中/ ,/ 狓/ 为/ 词项/ 空间/ 的/ 文档/ ;/ 狔/ 为/ 狓/ 在/ 主题/ 空间/ 的/ 表示/ ,/ 均/ 为/ 列/ 向量/ ./ 在/ 信息检索/ 中/ ,/ 可以/ 把/ 一个/ 查询/ 请求/ 看成/ 是/ 一篇/ 文档/ ,/ 从而/ 将/ 其/ 变换/ 到/ 主题/ 空间/ ,/ 并/ 在/ 该/ 空间/ 寻找/ 与/ 之/ 匹配/ 的/ 文档/ ./ 类似/ 地/ ,/ 对于/ 犡/ T/ 犡/ 可以/ 理解/ 为/ 文档/ 间/ 的/ 相似/ 度/ 矩阵/ ,/ 得到/ 它/ 的/ 特征向量/ 集/ 犞/ 后/ ,/ 可以/ 把/ 一个/ 单词/ 变换/ 到/ 新/ 的/ 主题/ 空间/ ./ LSI/ 的/ 详细/ 例子/ 可以/ 参考文献/ [/ 1/ ]/ ./ 5/ 概率/ 隐性/ 语义/ 索引/ 概率/ 隐性/ 语义/ 索引/ (/ probabilisticLatentSe/ -/ manticIndexing/ ,/ pLSI/ )/ ①/ 是/ Hofmann/ [/ 2/ ]/ 在/ 1999/ 年/ 提出/ 的/ 一个/ 主题/ 模型/ ./ 同/ LSI/ 相似/ ,/ pLSI/ 寻找/ 一个/ 从/ 词项/ 空间/ 到/ 隐性/ 语义/ (/ 主题/ )/ 空间/ 的/ 变换/ ,/ 但/ pLSI/ 是/ 一个/ 概率/ 生成/ 模型/ ,/ 而且/ 选择/ 了/ 不同/ 的/ 最优化/ 目标/ 函数/ ./ 5.1/ 模型表示/ 图/ 6/ 中/ ,/ d/ 代表/ 文档/ 标号/ ,/ z/ 是/ 主题/ ,/ w/ 是/ 单词/ ,/ ①/ 在/ 该文/ 中/ ,/ 作者/ 将/ 该/ 模型/ 称之为/ AspectModel/ ,/ 在/ 不/ 引起/ 混/ Page5/ 其中/ 只有/ z/ 是/ 隐含/ 变量/ ,/ M/ 代表/ 文档/ 数目/ ,/ N/ 表示/ 文档/ 的/ 长度/ ./ 观察/ 该/ 模型/ 的/ 生成/ 过程/ 描述/ (/ 见图/ 7/ )/ ,/ 容易/ 得到/ 模型/ 的/ 两组/ 主要参数/ :/ p/ (/ w/ |/ z/ )/ 和/ p/ (/ z/ |/ d/ )/ ,/ 即/ 各/ 主题/ 下/ 的/ 词项/ 概率分布/ 和/ 各/ 文档/ 的/ 主题/ 概率分布/ ./ 由于/ 没有/ 指定/ 概率分布/ 的/ 类型/ ,/ 这/ 两组/ 参数/ 其实/ 就是/ 两张/ 二维/ 的/ 参数表/ ,/ 需要/ 通过/ 参数估计/ 确定/ 二维/ 表中/ 每个/ 参数/ 的/ 值/ ./ 5.2/ 参数估计/ 根据/ 模型/ 的/ 表示/ ,/ 我们/ 可以/ 按照/ EM/ 的/ 框架/ 找出/ 模型/ 中/ 的/ 各个/ 对应/ 成分/ ,/ 分别/ 是/ :/ pLSI/ 模型/ 中/ 的/ w/ ,/ d/ 为/ 观测/ 值/ ,/ z/ 是/ 隐/ 变量/ ,/ p/ (/ w/ |/ z/ )/ 和/ p/ (/ z/ |/ d/ )/ 为/ 待/ 估计/ 的/ 参数/ ./ 不难看出/ p/ (/ w/ |/ z/ )/ 相当于/ 某/ 主题/ 下/ 的/ 词项/ 概率分布/ ;/ p/ (/ z/ |/ d/ )/ 相当于/ 某/ 文档/ 的/ 主题/ 概率分布/ ./ 整个/ 语料/ 的/ 概率/ 对/ 数值/ 定义/ 如下/ :/ 其中/ ,/ n/ (/ d/ ,/ w/ )/ 是/ d/ 文档/ 中/ w/ 出现/ 的/ 次数/ ;/ p/ (/ d/ ,/ w/ )/ 是/ (/ d/ ,/ w/ )/ 对/ 的/ 概率/ ./ 参数估计/ 的/ EM/ 过程/ 如下/ :/ E/ 步骤/ ./ 在/ 当前/ 的/ 参数估计/ 下/ ,/ 隐/ 变量/ z/ 的/ 后/ p/ (/ zd/ ,/ w/ )/ =/ p/ (/ z/ )/ p/ (/ dz/ )/ p/ (/ wz/ )/ 验/ 概率/ 表示/ 为/ M/ 步骤/ ./ 根据/ 上/ 一步/ 的/ 结果/ 对/ 完整/ 数据/ 的/ 期望值/ 进行/ 最大化/ ,/ 得到/ 更新/ 参数/ 的/ 公式/ p/ (/ wz/ )/ =/ ∑/ dp/ (/ dz/ )/ =/ ∑/ w/ 其中/ ,/ R/ ≡/ ∑/ d/ ,/ w/ 易算出/ p/ (/ z/ |/ d/ )/ ,/ 从而/ 得到/ 了/ p/ (/ w/ |/ z/ )/ 和/ p/ (/ z/ |/ d/ )/ 两组/ 参数/ ./ 为了/ 防止/ 过/ 拟合/ ,/ 在/ E/ 步骤/ 中/ ,/ 可以/ 引入/ 控制参数/ b/ ,/ 且/ b/ </ 1/ ./ 详细/ 内容/ 可以/ 参考文献/ [/ 2/ ]/ ./ 5.3/ 新/ 样本/ 的/ 推断/ 在/ pLSI/ 中/ ,/ 对于/ 新/ 样本/ 的/ 推断/ 仍然/ 采用/ EM/ 算法/ 完成/ ./ 不过/ 由于/ 我们/ 只/ 需要/ 得到/ 新/ 样本/ dnew/ 在/ 主题/ 空间/ 的/ 表达/ p/ (/ z/ |/ dnew/ )/ ,/ 而/ 不/ 需要/ 修改/ p/ (/ w/ |/ z/ )/ ,/ 因此/ 只/ 在/ EM/ 算法/ 中/ M/ 步骤/ 更新/ p/ (/ z/ |/ dnew/ )/ 而/ 保持/ p/ (/ w/ |/ z/ )/ 不变/ ./ 这/ 和/ LSI/ 的/ 处理/ 不同/ ,/ 因为/ LSI/ 在/ 对/ 新/ 样本/ 向/ 低维/ 的/ 隐性/ 语义/ 空间/ 变换/ 的/ 时候/ 只/ 需要/ 作/ 矩阵/ 运算/ ./ 5.4/ pLSI/ 和/ LSI/ 的/ 关系/ 两者/ 的/ 差异/ 是/ 很/ 明显/ 的/ ./ LSI/ 不是/ 概率/ 生成/ 模型/ ,/ 因此/ 无法/ 用/ 文档/ 的/ 生成/ 过程/ 来/ 解释/ LSI/ ,/ 从而/ 也/ 无法/ 将/ 不同/ 类型/ 的/ 语义/ 结构/ 和/ 语法/ 角色/ 引入/ 到/ LSI/ 中/ ./ pLSI/ 作为/ 生成/ 模型/ ,/ 具有/ 概率/ 基础/ ,/ 也/ 容易/ 进行/ 模型/ 扩展/ ./ 此外/ ,/ LSI/ 和/ pLSI/ 最优化/ 的/ 目标/ 函数/ 不同/ :/ LSI/ 以/ 最优/ 低/ 秩/ 逼近/ 为/ 优化/ 的/ 目标/ 函数/ ,/ 而/ pL/ -/ SI/ 以/ 观测/ 值/ 的/ 似然值/ 为/ 优化/ 目标/ 函数/ ./ 另外/ ,/ LSI/ 的/ SVD/ 分解/ 得到/ 的/ 是/ 全局/ 最优/ 解/ ,/ 而/ pLSI/ 得到/ 的/ 是/ 局部/ 最优/ 解/ ./ 即便如此/ ,/ pLSI/ 模型/ 仍然/ 取得/ 了/ 比/ LSI/ 更好/ 的/ 效果/ [/ 2/ ,/ 12/ -/ 13/ ]/ ./ 尽管/ 两者/ 存在/ 差别/ ,/ 但是/ ,/ 如果/ 我们/ 仅/ 考虑/ 从/ 词项/ 空间/ 向/ 主题/ 空间/ 转换/ ,/ 那么/ 两者/ 又/ 是/ 十分相似/ 的/ ./ 我们/ 可以/ 找出/ 以下/ 的/ 对应/ 关系/ [/ 2/ ]/ ,/ 比如/ :/ LSI/ 的/ 犝/ 矩阵/ 对应/ pLSI/ 中/ 的/ p/ (/ wj/ |/ zk/ )/ j/ ,/ k/ ;/ 犞/ 对应/ p/ (/ di/ |/ zk/ )/ i/ ,/ k/ ;/ 而/ Σ/ 对应/ diag/ (/ p/ (/ zk/ )/ )/ k/ ./ 当然/ ,/ 犝/ ,/ 犞/ 矩阵/ 中/ 的/ 元素/ 取值/ 可以/ 为/ 负/ ,/ 这/ 也/ 是/ LSI/ 缺乏/ 概率/ 基础/ 的/ 一个/ 表现/ ./ 而/ 在/ pLSI/ 中/ 对应/ 的/ 元素/ 是非/ 负/ 的/ 概率/ 值/ ./ 正如/ pLSI/ 的/ 命名/ ,/ 它/ 在/ 概率/ 化/ 的/ LSI/ ,/ 而/ 基本/ 思想/ 却是/ 源自/ LSI/ ./ 值得一提的是/ ,/ 在/ pLSI/ 提出/ 的/ 同年/ (/ 1999/ )/ ,/ Lee/ 等/ 人/ [/ 14/ -/ 15/ ]/ 提出/ 了/ 非负/ 矩阵/ 分解/ (/ Non/ -/ NegativeMatrixFactorization/ ,/ NMF/ )/ ,/ 在/ 某些/ 条件/ 下/ 被/ 证明/ 和/ pLSI/ 等价/ [/ 16/ -/ 17/ ]/ ./ 6LDA/ 模型/ Blei/ 等/ 人/ [/ 3/ ]/ 在/ 2003/ 年/ 提出/ 了/ LDA/ (/ LatentDirichletAllocation/ )/ ./ 他们/ 在/ pLSI/ 的/ 基础/ 上/ ,/ 用/ 一个/ 服从/ Dirichlet/ 分布/ 的/ K/ 维/ 隐含/ 随机变量/ 表示/ 文/ Page6/ 档/ 的/ 主题/ 概率分布/ ,/ 模拟/ 文档/ 的/ 产生/ 过程/ (/ 见图/ 3/ )/ ./ Griffiths/ 等/ 人/ [/ 4/ ]/ 又/ 对/ β/ 参数/ 施加/ Dirichlet/ 先验/ 分布/ ,/ 使得/ LDA/ 模型/ 成为/ 一个/ 完整/ 的/ 生成/ 模型/ (/ 见图/ 8/ )/ ./ LDA/ 主题/ 模型/ 及其/ 扩展/ 正/ 被/ 越来越/ 多地/ 应用/ 于/ 图像处理/ 、/ 自然语言/ 处理/ 等/ 领域/ ./ 近些年/ 出现/ 的/ 主题/ 模型/ 或多或少/ 与/ LDA/ 模型/ 存在/ 联系/ ,/ 因此/ ,/ 理解/ LDA/ 模型/ 对于/ 把握/ 主题/ 模型/ 的/ 发展/ 是/ 十分/ 有/ 意义/ 的/ ./ 6.1/ 模型表示/ 图/ 8/ 中/ ,/ φ/ k/ 表示/ 主题/ k/ 中/ 的/ 词项/ 概率分布/ ;/ θ/ m/ 表示/ 第/ m/ 篇/ 文档/ 的/ 主题/ 概率分布/ ./ θ/ m/ ,/ φ/ k/ 又/ 作为/ 多项式/ 分布/ 的/ 参数/ 分别/ 用于/ 生成/ 主题/ 和/ 单词/ ./ K/ 代表/ 主题/ 数目/ ,/ M/ 代表/ 文档/ 数目/ ,/ Nm/ 表示/ 第/ m/ 篇/ 文档/ 的/ 长度/ ,/ wm/ ,/ n/ 和/ zm/ ,/ n/ 分别/ 表示/ 第/ m/ 篇/ 文档/ 中/ 第/ n/ 个/ 单词/ 及其/ 主题/ ./ α/ 和/ β/ 是/ Dirichlet/ 分布/ 的/ 参数/ ,/ 通常/ 是/ 固定值/ 且/ 对称/ 分布/ (/ symmetric/ )/ ①/ ,/ 因此/ 用/ 标量/ 表示/ ./ θ/ m/ ,/ φ/ k/ 均/ 服从/ Dirichlet/ 分布/ ,/ 该/ 分布/ 函数/ 如下/ 式/ 所示/ 其中/ ,/ 0/ / μ/ k/ / 1/ ,/ ∑/ k/ μ/ k/ =/ 1/ ;/ α/ 0/ =/ ∑/ K/ 函数/ ./ LDA/ 的/ 生成/ 过程/ 如图/ 9/ 所示/ ./ 对于/ 多项式/ 分布/ 函数/ 而言/ ,/ Dirichlet/ 是/ 其/ 共轭/ 先验/ 分布/ ,/ 可以/ 简化/ 模型/ 中/ 的/ 计算/ ./ 其中/ Dirichlet/ 的/ 先验/ α/ 和/ β/ 的/ 经验值/ 取值/ 一般/ 为/ α/ =/ 50/ // K/ ,/ β/ =/ 0.01/ ,/ 起到/ 平滑/ 数据/ 的/ 作用/ ./ 在/ 一些/ 情况/ 下/ ,/ 也/ 可以/ 使用/ 语料/ 对/ α/ 和/ β/ 进行/ 经验/ 贝叶斯/ 估计/ ./ 根据/ Dirichlet/ 分布/ 函数/ 的/ 性质/ 可知/ ,/ 先验/ 变/ 大/ 表示/ 概率密度/ 越/ 集中/ 于/ K/ -/ 1/ 维/ Simplex/ 的/ 中间/ 区域/ ,/ 可以/ 得到/ 更为/ 均匀/ 的/ 概率分布/ [/ 5/ ]/ ./ 本节/ 中/ 选用/ 的/ 模型表示/ 参照/ 文献/ [/ 18/ ]/ ,/ 与/ Blei/ 提出/ 的/ LDA/ 模型表示/ 略有/ 差别/ ,/ 但/ 实际/ 需要/ 估计/ 的/ 参数/ 相同/ ,/ 并/ 无/ 本质/ 差异/ ./ 6.2/ 参数估计/ LDA/ 的/ 参数估计/ 方法/ 有变/ 分/ 贝叶斯/ 推断/ (/ Var/ -/ iationalBayesianInference/ ,/ VB/ )/ [/ 3/ ]/ 、/ 期望/ 传播/ (/ Expectation/ -/ Propagation/ ,/ EP/ )/ [/ 19/ ]/ 和/ CollapsedGibbsSampling/ [/ 4/ ]/ 等/ ./ 此外/ ,/ Teh/ 等/ 人/ [/ 20/ ]/ 提出/ 了/ CollapsedVariationalBayesian/ (/ CVB/ )/ 方法/ ,/ 结合/ 了/ CollapsedGibbsSampling/ 和/ VariationalInference/ 两种/ 方法/ ./ 每种/ 参数估计/ 方法/ 都/ 各有利弊/ ,/ 选择/ 一个/ 合适/ 的/ 近似算法/ 要/ 在/ 效率/ 、/ 复杂性/ 、/ 准确性/ 和/ 概念/ 简洁性/ 之间/ 综合/ 考虑/ [/ 20/ -/ 21/ ]/ ./ 无论/ 哪/ 种/ 方法/ ,/ 我们/ 所/ 要/ 处理/ 的/ 任务/ 是/ 相同/ 的/ ,/ 即/ 根据/ 给定/ 的/ 最优化/ 目标/ 函数/ ,/ 得到/ 对/ 参数/ 的/ 估计值/ ./ 由于/ Gibbs/ 方法/ 描述/ 简单/ 且/ 更/ 容易/ 实现/ ,/ 成为/ 主题/ 模型/ 中/ 最常/ 采用/ 的/ 参数估计/ 方法/ ./ 本文/ 选择/ CollapsedGibbsSampling/ 方法/ 进行/ 介绍/ [/ 18/ ]/ ./ 所谓/ “/ Collapsed/ ”/ 的/ 含义/ 是/ 指/ 通过/ 积分/ 避开/ 了/ 实际/ 待/ 估计/ 的/ 参数/ θ/ m/ 和/ φ/ k/ ,/ 转而/ 对/ 每个/ 单词/ w/ 的/ 主题/ z/ 进行/ 采样/ ,/ 一旦/ 每个/ w/ 的/ z/ 确定/ 下来/ ,/ θ/ m/ 和/ φ/ k/ 的/ 值/ 可以/ 在/ 统计/ 频次/ 后/ 计算出来/ ./ 因此/ ,/ 问题/ 转为/ 计算/ 单词/ 序列/ 下/ 主题/ 序列/ 的/ 条件/ 概率/ ,/ 然后/ 进行/ 主题/ 序列/ 的/ 采样/ ,/ 公式/ 如下/ 其中/ ,/ 狑/ 表示/ 所有/ 文档/ 首尾相接/ 而成/ 的/ 单词/ 向量/ ;/ 狕/ 是/ 其/ 对应/ 主题/ 向量/ ./ 由于/ 狕/ 的/ 序列/ 通常/ 较长/ ,/ 可能/ 取值/ 随/ 向量/ 长度/ 指数/ 增长/ ,/ 一般/ 无法/ 直接/ 计算/ ./ 这时/ 我们/ 可以/ 考虑/ 使用/ Gibbs/ 采样/ 把/ 问题/ 进行/ 分解/ ,/ 每次/ 对/ 一个/ 隐/ 变量/ (/ 主题/ )/ 进行/ 采样/ ./ Gibbs/ 采样/ 是/ 马尔可夫/ 链/ 蒙特卡洛/ 方法/ (/ Markov/ -/ chainMonteCarlo/ ,/ MCMC/ )/ 的/ 特例/ ,/ 每次/ 对/ 联合/ 分布/ 的/ 一个/ 分量/ 进行/ 采样/ ,/ 而/ 保持/ 其它/ 分量/ 的/ 值/ 不变/ [/ 10/ ]/ ./ 对于/ 联合/ 分布/ 维度/ 较/ 高/ 的/ 情况/ 使用/ Gibbs/ 采样/ 可以/ 产生/ 比较简单/ 的/ 算法/ ./ ①/ 注意/ 此处/ 的/ β/ 与/ 图/ 3/ 中/ 的/ β/ 含义/ 不同/ ,/ 图/ 3/ 中/ 的/ β/ 对应/ 图/ 8Page7/ 经过/ 推导/ ,/ 最终/ 的/ 采样/ 公式/ 如下/ p/ (/ zi/ =/ k/ 狕/ / i/ ,/ 狑/ )/ ∝/ 其中/ ,/ 假设/ wi/ =/ t/ ;/ zi/ 表示/ 第/ i/ 个/ 单词/ 对应/ 的/ 主题/ 变量/ ;/ / i/ 表示/ 剔除/ 其中/ 的/ 第/ i/ 项/ ;/ n/ (/ v/ )/ 现词/ 项/ v/ 的/ 次数/ ;/ β/ v/ 是/ 词项/ v/ 的/ Dirichlet/ 先验/ ;/ n/ (/ z/ )/ 表示/ 文档/ m/ 中/ 出现/ 主题/ z/ 的/ 次数/ ;/ α/ z/ 是/ 主题/ z/ 的/ Dirichlet/ 先验/ ./ 一旦/ 获得/ 每个/ 单词/ w/ 的/ 主题/ z/ 的/ 标号/ ,/ 我们/ 需要/ 的/ 参数/ 计算公式/ 表示/ 如下/ 式/ (/ 11/ )/ :/ 其中/ ,/ φ/ k/ ,/ t/ 表示/ 主题/ k/ 中词/ 项/ t/ 的/ 概率/ ;/ θ/ m/ ,/ k/ 表示/ 文档/ m/ 中/ 主题/ k/ 的/ 概率/ ./ 因此/ ,/ 只要/ 知道/ 了/ 每个/ 单词/ 的/ 主题/ 标号/ ,/ 那么/ 我们/ 就/ 可以/ 通过/ 简单/ 计数/ 的/ 方式/ 对/ 参数/ 进行/ 估计/ ./ 6.3/ 新/ 样本/ 的/ 推断/ 单词/ 的/ 隐含/ 主题/ 采样/ 公式/ 如下/ 已知/ 训练/ 好/ 的/ 模型/ M/ ,/ 任给/ 新/ 文档/ 狑/ ~/ ,/ 其中/ 每个/ p/ (/ z/ ~/ i/ =/ kw/ ~/ n/ (/ t/ )/ 其中/ ,/ 狕/ ~/ 代表/ 新/ 文档/ 狑/ ~/ 对应/ 的/ 主题/ 向量/ ,/ 其余/ 符号/ 含义/ 请/ 参考/ 式/ (/ 9/ )/ ~/ (/ 11/ )/ 的/ 解释/ ./ 通过/ 前面/ 提到/ 的/ Gibbs/ 采样/ 方法/ ,/ 最终/ 我们/ 可以/ 得到/ 每个/ 单词/ 的/ 主题/ 标号/ ,/ 然后/ 套用/ 公式/ 计算/ 出该/ 文档/ 在/ 各个/ 主题/ 分量/ 上/ 的/ 值/ 后/ ,/ 一篇/ 词项/ 空间/ 的/ 文档/ 就/ 获得/ 了/ 在/ 主题/ 空间/ 中/ 的/ 表示/ ./ 6.4/ LDA/ 参数估计/ 与/ EM/ 算法/ 联系/ LDA/ 的/ 参数估计/ 方法/ 有/ 多种/ ,/ 我们/ 也/ 可以/ 套用/ EM/ 算法/ 的/ 框架/ 来/ 进行/ 理解/ ./ 在/ Collapsed/ 的/ Gibbs/ 采样/ 中/ ,/ 由于/ 将/ 参数/ θ/ m/ 和/ φ/ k/ 通过/ 积分/ 消/ 去/ ,/ 所以/ 上述/ EM/ 算法/ 中/ 每次/ 迭代/ 的/ M/ 步骤/ 被/ 省去/ ,/ 只/ 需要/ 对/ 主题/ 序列/ 进行/ 采样/ ,/ 等/ 采样/ 结束/ 再/ 根据/ 式/ (/ 11/ )/ 计算/ 参数/ ,/ 作为/ 最终/ 的/ 参数估计/ 结果/ ./ 我们/ 可以/ 这样/ 理解/ :/ 在/ E/ 步骤/ 中/ 得到/ 一个/ 后验/ 分布/ p/ (/ 狕/ |/ 狑/ )/ 的/ 采样/ ,/ 用来/ 近似计算/ 似然/ 期望值/ ,/ 并供/ M/ 步骤/ 最大化/ 使用/ ./ 需要/ 指出/ 的/ 是/ ,/ 用/ EM/ 框架/ 理解/ Collapsed/ 的/ Gibbs/ 方法/ 时/ ,/ M/ 步骤/ 的/ 参数估计/ 结果/ 在/ E/ 步骤/ 中/ 没有/ 用到/ ,/ 所以/ 不/ 需要/ 重复/ 多余/ 的/ M/ 步骤/ ,/ 只/ 需/ 在/ 最后/ 进行/ 一次/ M/ 步骤/ ,/ 得到/ 所/ 需要/ 的/ 参数/ θ/ m/ 和/ φ/ k/ 即可/ ./ 这种/ 在/ E/ 步骤/ 中用/ 后验/ 分布/ 的/ 采样/ 代替/ 后验/ 分布/ 并/ 用于/ 近似/ 数据/ 似然值/ 的/ 处理/ 称为/ 随机/ (/ stochastic/ )/ EM/ ,/ 是/ 蒙特卡洛/ EM/ 的/ 一个/ 特例/ [/ 10/ ]/ ./ 使用/ 变分/ 贝叶斯/ 推断/ [/ 3/ ]/ 或者/ 期望/ 传播/ [/ 19/ ]/ 方法/ 来/ 对/ LDA/ 的/ 参数/ 进行/ 估计/ 时/ 也/ 采用/ 了/ EM/ 算法/ 的/ 框架/ ,/ 详细/ 内容/ 请/ 参考文献/ [/ 3/ ,/ 19/ ]/ ./ 6.5/ LDA/ 和/ pLSI/ 的/ 关系/ LDA/ 模型/ 可以/ 看成/ 是/ 对/ pLSI/ 进行/ 了/ 贝叶斯/ 化/ ,/ 使得/ 参数/ 具备/ 了/ 概率分布/ ,/ 变成/ 了/ 随机变量/ ./ 事实上/ ,/ 在/ 图/ 3/ 中/ 去掉/ α/ ,/ 或者/ 在/ 图/ 8/ 中/ 去掉/ α/ 、/ β/ ,/ 得到/ 的/ 就是/ pLSI/ 模型/ ./ 也就是说/ ,/ pLSI/ 是/ 对/ 参数/ 作/ 最大/ 似然/ 估计/ ,/ 而/ LDA/ 是/ 在/ 参数/ 有/ 先验/ 分布/ 的/ 情况/ 下/ 对/ 参数/ 作/ 最大/ 后验/ 估计/ ./ 针对/ 图/ 3/ 表示/ 的/ 主题/ 模型/ ,/ Girolami/ [/ 22/ ]/ 称/ pLSI/ 是/ LDA/ 模型/ 在/ α/ 先验/ 为/ 1/ 的/ 情况/ 下/ 的/ 最大/ 后验/ 或者/ 最大/ 似然/ 估计/ ./ 因为/ 对于/ Dirichlet/ 分布/ ,/ α/ 为/ 1/ 时/ 先验/ 失效/ ,/ 所以/ 此时/ 最大/ 后验/ 估计/ 和/ 最大/ 似然/ 估计/ 等价/ ./ 这样/ ,/ pLSI/ 可以/ 纳入/ LDA/ 的/ 框架/ ./ 之所以/ 把/ LDA/ 看成/ 是/ 比/ pLSI/ 更为/ 彻底/ 的/ 生成/ 模型/ ,/ 就是/ 因为/ 在/ LDA/ 中/ 把/ p/ (/ z/ |/ d/ )/ 和/ p/ (/ w/ |/ z/ )/ 看成/ 了/ 随机/ 向量/ (/ 见图/ 8/ 中/ θ/ 和/ φ/ )/ ,/ 指定/ 了/ 先验概率/ 分布/ ;/ 而/ 在/ pLSI/ 中仅/ 把/ 它们/ 当做/ 参数/ 来/ 估计/ ./ 这样/ 来看/ ,/ 主题/ 模型/ 从/ pLSI/ 发展/ 到/ LDA/ 是/ 非常/ 自然/ 的/ ./ 6.6/ LDA/ 模型/ 的/ 直接/ 应用/ 首先/ ,/ LDA/ 模型/ 可以/ 作为/ 一种/ 降维/ 的/ 工具/ ./ 由于/ LDA/ 模型/ 训练/ 完成/ 后/ ,/ 能够/ 得到/ 一个/ 文档/ 在/ 主题/ 空间/ 的/ 表示/ ,/ 一些/ 在/ 词项/ 空间/ 中/ 的/ 文档/ 处理/ 可以/ 通过/ LDA/ 模型/ 转而/ 在/ 主题/ 空间/ 中/ 完成/ ,/ 比如/ 文档/ 分类/ [/ 3/ ]/ 、/ 聚类/ 等/ ./ 此外/ ,/ 利用/ 主题/ 模型/ 中/ 的/ 参数/ 估计值/ ,/ 可以/ 完成/ 协同/ 过滤/ (/ collaborativefiltering/ )/ [/ 3/ ]/ 、/ 单词/ 或/ 文档/ 相似/ 度/ 计算/ [/ 5/ ]/ 、/ 文本/ 分段/ [/ 8/ ]/ 等/ 任务/ ./ 一般而言/ ,/ 直接/ 使用/ LDA/ 模型/ 只是/ 作为/ 具体任务/ 的/ 一个/ 环节/ ,/ 究竟/ 如何/ 使用/ LDA/ 模型/ 还要/ 结/ Page8/ 合/ 实际/ 情况/ ,/ 本文/ 不再/ 详述/ ./ 7LDA/ 模型/ 的/ 扩展/ 目前/ ,/ 主题/ 模型/ 相关/ 的/ 工作/ 大多/ 是/ 对/ LDA/ 模型/ 进行/ 修改/ ,/ 或者/ 是/ 将/ LDA/ 模型/ 作为/ 整个/ 概率模型/ 的/ 一个/ 部件/ ./ 虽然/ 也/ 存在/ 一些/ 和/ LDA/ 模型/ 无/ 直接/ 关系/ 的/ 主题/ 模型/ ,/ 但/ 作为/ 词项/ 概率分布/ 的/ 主题/ 贯穿/ 所有/ 的/ 主题/ 模型/ ,/ 而/ 这/ 和/ LDA/ 中/ 的/ 主题/ 并/ 无/ 实质/ 差异/ ./ 因此/ ,/ 本节/ 以/ LDA/ 模型/ 为/ 线索/ ,/ 通过/ 介绍/ 其/ 扩展/ 来/ 反映/ 主题/ 模型/ 在/ 近年/ 的/ 发展/ ./ 由于/ 针对/ LDA/ 扩展/ 的/ 研究/ 工作/ 非常/ 多/ ,/ 本文/ 中/ 难以/ 全面/ 涉及/ ./ 我们/ 对/ 这些/ 扩展/ 作/ 了/ 粗略/ 分类/ ,/ 简单/ 介绍/ 每类/ 中/ 一些/ 具有/ 代表性/ 的/ 工作/ ./ 7.1/ 对/ 参数/ 的/ 扩展/ 我们/ 知道/ ,/ 在/ 主题/ 模型/ 中/ 最/ 重要/ 的/ 两组/ 参数/ 就是/ 各/ 主题/ 下/ 的/ 词项/ 概率分布/ 和/ 各/ 文档/ 的/ 主题/ 概率分布/ ,/ 通过/ 对/ 它们/ 进行/ 扩展/ ,/ 使得/ 模型/ 更/ 接近/ 数据/ 的/ 真实情况/ ./ 在/ LDA/ 模型/ 中/ ,/ 假设/ 每个/ 文档/ 的/ 主题/ 概率分布/ θ/ 服从/ Dirichlet/ 分布/ ,/ 并/ 没有/ 对/ 不同/ 主题/ 之间/ 相关性/ 进行/ 刻画/ ./ 然而/ ,/ 在/ 真实/ 的/ 语料/ 中/ ,/ 不同/ 主题/ 之间/ 存在/ 相关性/ 的/ 现象/ 很/ 普遍/ ./ 在/ 2004/ 年/ ,/ Blei/ 等/ 人/ [/ 23/ ]/ 提出/ 了/ 主题/ 间/ 为/ 树结构/ 的/ 层级/ LDA/ (/ Hierar/ -/ chicalLDA/ )/ ./ 在/ 该/ 模型/ 中/ ,/ 树中/ 的/ 每个/ 节点/ 代表/ 一个/ 主题/ ./ 其/ 生成/ 过程/ 如下/ :/ 首先/ 针对/ 文档/ 选择/ 一条/ 从/ 根到/ 叶/ 节点/ 的/ 路径/ ;/ 然后/ 按照/ 各层/ 的/ 比重/ ,/ 选择/ 路径/ 中/ 一个/ 节点/ 作为/ 主题/ ,/ 以该/ 主题/ 的/ 词项/ 概率分布/ 生成/ 单词/ ,/ 重复/ 直到/ 生成/ 整篇/ 文档/ ./ 该/ 模型/ 还有/ 一个/ 特点/ 是/ 可以/ 从/ 语料/ 中/ 估计/ 出/ 主题/ 的/ 个数/ ,/ 并/ 与/ 使用/ LDA/ 模型/ 在/ 不同/ 主题/ 数下/ 重复/ 实验/ 得到/ 的/ 最佳/ 主题/ 个数/ 一致/ [/ 7/ ]/ ./ Blei/ 等/ 人/ [/ 24/ -/ 25/ ]/ 于/ 2006/ 年/ 又/ 在/ LDA/ 的/ 基础/ 上/ 提出/ 了/ 相关/ 主题/ 模型/ (/ CorrelatedTopicModel/ ,/ CTM/ )/ ./ 与/ LDA/ 不同/ 的/ 是/ ,/ CTM/ 从/ 对数/ 正态分布/ 中/ 对/ 主题/ 概率分布/ θ/ 进行/ 采样/ ,/ 先验/ 参数/ 包括/ 一个/ 协方差/ 矩阵/ ,/ 描述/ 每/ 对/ 主题/ 之间/ 的/ 相关性/ ./ Li/ 等/ 人/ [/ 26/ ]/ 针对/ CTM/ 只/ 考虑/ 两个/ 主题/ 间/ 关系/ 的/ 不足/ ,/ 提出/ 了/ PAM/ 模型/ (/ PachinkoAllocationModel/ )/ ,/ 该/ 模型/ 的/ 特点/ 是/ 把/ 主题/ 之间/ 的/ 关系/ 表示/ 成/ 一个/ 有/ 向/ 无/ 环图/ ,/ 其中/ 叶子/ 节点/ 是/ 单词/ ,/ 而/ 非叶/ 节点/ (/ 主题/ )/ 可以/ 看成/ 是/ 由/ 所/ 包含/ 的/ 子/ 节点/ (/ 主题/ 或/ 单词/ )/ 构成/ ./ PAM/ 和/ 层级/ LDA/ 模型/ 的/ 一个/ 区别/ 是/ :/ 前者/ 的/ 主题/ 可能/ 是/ 词项/ 的/ 概率分布/ ,/ 也/ 可能/ 是/ 其它/ (/ 子/ )/ 主题/ 的/ 概率分布/ ,/ 而/ 层级/ LDA/ 中/ 的/ 每个/ 主题/ 都/ 是/ 词项/ 概率分布/ ./ 之后/ ,/ Mimno/ 等/ 人/ [/ 27/ ]/ 又/ 在/ PAM/ 的/ 工作/ 上/ 提出/ 了/ 层级/ PAM/ 模型/ (/ hierarchicalPAM/ )/ ,/ 该/ 模型/ 可以/ 看成/ 是/ 把/ 层级/ LDA/ 和/ PAM/ 结合/ 起来/ ,/ 使得/ PAM/ 模型/ 中/ 的/ 非叶/ 节点/ 也/ 具有/ 词项/ 的/ 概率分布/ ./ Wang/ 等/ 人/ [/ 28/ ]/ 向/ LDA/ 模型/ 中/ 添加/ 了/ 一个/ 作为/ 观测/ 值/ 的/ 时间/ 随机变量/ 后/ 得到/ 了/ 主题/ 随/ 时间/ 变化/ 的/ 主题/ 模型/ (/ TopicOverTime/ ,/ TOT/ )/ ,/ 该/ 模型/ 认为/ 主题/ 概率分布/ 受到/ 时间/ 信息/ 的/ 影响/ ,/ 而/ 时间/ 变量/ 服从/ beta/ 分布/ ,/ 归一化/ 到/ [/ 0/ ,/ 1/ ]/ 之间/ ./ Blei/ 等/ 人/ [/ 29/ ]/ 在/ 2006/ 年/ 提出/ 了/ 动态/ 主题/ 模型/ (/ DynamicTopicModels/ ,/ DTM/ )/ ,/ 他们/ 认为/ 主题/ 会/ 随着/ 时间/ 变化/ ,/ 且/ 满足/ 一阶/ 马尔可夫/ 假设/ ,/ 图/ 模型/ 如图/ 10/ 所示/ ./ 可以/ 看到/ ,/ 主题/ 概率分布/ θ/ 的/ 超/ 参数/ α/ 以及/ 主题/ 中词/ 项/ 的/ 概率分布/ 参数/ β/ 随/ 时间/ 变化/ ,/ 且/ 依赖于/ 前/ 一个/ 时间/ 片/ 的/ 状态/ ./ 7.2/ 引入/ 上下文/ 信息/ 通常/ 主题/ 模型/ 假设/ 单词/ 序列/ 中/ 的/ 单词/ 是/ 可/ 交换/ 的/ ,/ 即/ 单词/ 的/ 顺序/ 和/ 模型/ 的/ 训练/ 结果/ 无关/ ./ 然而/ ,/ 有时/ 需要/ 引入/ 一些/ 上下文/ 信息/ ,/ 考虑/ 当前/ 节点/ 和/ 其它/ 节点/ 的/ 关系/ ,/ 这/ 就/ 破坏/ 了/ LDA/ 的/ 可/ 交换/ 性/ 假设/ ./ Griffiths/ 等/ 人/ [/ 30/ ]/ 认为/ 可以/ 通过/ HMM/ 来/ 捕捉/ 句法结构/ 信息/ ,/ 而/ 通过/ LDA/ 来/ 揭示/ 语义/ 关系/ ,/ 并/ 将/ 两者/ 结合/ 在/ 一起/ 提出/ 了/ HMM/ -/ LDA/ 模型/ ,/ 见图/ 11/ ./ 该/ 模型/ 把/ 主题/ 分成/ 两类/ :/ 一类/ 是/ 功能/ 主题/ ,/ 比如/ 代词/ 主题/ 、/ 介词/ 主题/ 等/ ;/ 一类/ 是/ 概念/ 词汇/ ,/ 主要/ 是/ 名词/ 和/ 动词/ 等/ 有/ 具体/ 语义/ 的/ 主题/ ./ 实验/ 的/ 结果/ 证明/ 该/ 模型/ 是/ 有效/ 的/ ,/ 能够/ 把/ 两类/ 主题/ 分开/ ,/ 并/ 可以/ 计算/ 出/ 主题/ 之间/ 的/ 转移/ 概率/ ./ Wallach/ [/ 31/ ]/ 认为/ ,/ 在/ 生成/ 过程/ 中/ ,/ 一个/ 单词/ 除了/ 依赖于/ 其/ 对应/ 的/ 主题/ 外/ ,/ 还/ 与/ 前/ 一个/ 单词/ 有关/ ,/ 提出/ 超越/ 词袋/ (/ BeyondBag/ -/ of/ -/ Words/ )/ 的/ 主题/ 模型/ ./ 这个/ 模型/ 可以/ 看成/ 是/ LDA/ 模型/ 和/ 单词/ 二元/ 组/ 模型/ 的/ 结合/ ./ Wang/ 等/ 人/ [/ 32/ -/ 33/ ]/ 将/ 搭配/ 引入/ 到/ 主题/ 模型/ 中/ 提出/ 了/ TNG/ 模型/ (/ Topicaln/ -/ gramModel/ )/ ,/ 作者/ 认为/ 两个/ 相邻/ 单词/ 之间/ 是否/ 搭配/ 不仅/ 与/ 前/ 一个/ 单/ Page9/ 词/ 有关/ 而且/ 受前/ 一个/ 单词/ 的/ 主题/ 影响/ ,/ 比如/ whitehouse/ 在/ white/ 的/ 主题/ 是/ 政治/ 时/ 应该/ 搭配/ ,/ 而/ 如果/ white/ 主题/ 是/ 颜色/ ,/ 那么/ 应该/ 分成/ 两个/ 单词/ ./ 该/ 模型/ 的/ 最/ 明显/ 的/ 特征/ 是/ :/ 主题/ 不再/ 只是/ 词项/ 的/ 概率分布/ ,/ 还/ 可以/ 是/ 词组/ (/ 词项/ 的/ 搭配/ )/ 的/ 概率分布/ ./ Gruber/ 等/ 人/ [/ 34/ ]/ 提出/ 了/ 隐性/ 主题/ 马尔可夫/ 模型/ (/ HiddenTopicMarkovModel/ ,/ HTMM/ )/ ,/ 与/ 许多/ 对/ 每个/ 单词/ 指定/ 一个/ 主题/ 的/ 模型/ 不同/ ,/ 该/ 模型/ 以/ 句子/ 为/ 单位/ 分配/ 主题/ ./ 即同/ 一句/ 话/ 内/ 所有/ 的/ 单词/ 共享/ 同一个/ 主题/ ,/ 当/ 句子/ 切换/ 时/ ,/ 按照/ Bernoulli/ 分布/ 对/ 句子/ 重新/ 选择/ 主题/ ./ Boyd/ -/ Graber/ 等/ 人/ [/ 35/ ]/ 提出/ 了/ 句法/ 主题/ 模型/ (/ Syn/ -/ tacticTopicModel/ ,/ STM/ )/ ,/ 该/ 模型/ 的/ 特色/ 是/ 在/ 选择/ 主题/ 时/ 不仅/ 要/ 考虑/ 整个/ 文档/ 的/ 主题/ 概率分布/ ,/ 而且/ 还要/ 考虑/ 句法树/ 中父/ 节点/ 的/ 主题/ 类型/ ./ 为了/ 使用/ 该/ 模型/ ,/ 要/ 先/ 对/ 语料/ 进行/ 依存/ 句法分析/ 得到/ 语法/ 树/ ./ 模型/ 训练/ 完成/ 后/ 所/ 获得/ 的/ 主题/ 同时/ 呈现/ 语义/ 和/ 句法/ 上/ 的/ 相关性/ ,/ 并且/ 主题/ 之间/ 的/ 转移/ 概率/ 也/ 被/ 估计/ 出来/ ./ 7.3/ 面向/ 特定/ 任务/ 本/ 小节/ 对/ 基于/ LDA/ 模型/ 的/ 面向/ 特定/ 任务/ 的/ 研究/ 工作/ 进行/ 介绍/ ,/ 涉及/ 分类/ 、/ 作者/ 主题/ 模型/ 、/ 词义/ 消歧/ 、/ 引用/ 链接/ 分析/ 、/ 人名/ 消歧/ 、/ 情感/ 分析/ 等/ 问题/ ./ Blei/ 等/ 人/ [/ 36/ ]/ 针对/ 文本/ 分类/ 问题/ 提出/ 了/ 有/ 监督/ LDA/ 模型/ (/ supervisedLatentDirichletAllocation/ ,/ sLDA/ )/ ,/ 该/ 模型/ 将/ 训练/ 语料/ 中/ 的/ 文档/ 类别/ 标记/ 作为/ 观测/ 值/ 加入/ LDA/ 模型/ ,/ 且/ 类别/ 标号/ 服从/ 一个/ 与/ 文档/ 主题/ 概率分布/ 有关/ 的/ 正态/ 线性/ 分布/ ./ 对于/ 新/ 文档/ ,/ 可以/ 通过/ 该/ 模型/ 判定/ 新/ 文档/ 的/ 类别/ 标号/ ./ 李文波/ 等/ 人/ [/ 37/ ]/ 在/ 2008/ 年/ 提出/ Labeled/ -/ LDA/ 模型/ ,/ 该/ 模型/ 将/ 参数/ 按照/ 类别/ 细化/ ,/ 并/ 应用/ 于/ 文本/ 分类/ 任务/ ./ Steyvers/ 等/ 人/ [/ 38/ ]/ 提出/ 作者/ 主题/ 模型/ (/ Author/ -/ Topic/ ,/ AT/ )/ ,/ 认为/ 每个/ 作者/ 有/ 一个/ 主题/ 概率分布/ ./ 文档/ 的/ 生成/ 过程/ 是/ :/ 随机/ 选择/ 一个/ 作者/ ,/ 根据/ 这个/ 作者/ 的/ 主题/ 概率分布/ ,/ 生成/ 一个/ 词/ ,/ 重复/ 该/ 过程/ 直到/ 生成/ 整个/ 文档/ ./ 注意/ 一篇/ 文档/ 可以/ 由/ 多个/ 作者/ 共同完成/ ./ McCallum/ 等/ 人/ [/ 39/ ]/ 又/ 在/ AT/ 模型/ 的/ 基础/ 上/ ,/ 提出/ 了/ 作者/ 接受者/ 主题/ 模型/ (/ Author/ -/ Recipient/ -/ Topic/ ,/ ART/ )/ ,/ 如图/ 12/ 所示/ ./ 该/ 模型/ 针对/ 具有/ 方向性/ 的/ 文档/ (/ 比如/ 电子邮件/ )/ ,/ 将/ 发送者/ 和/ 接受者/ 对/ (/ pair/ )/ 看成/ 是/ 一篇/ 文档/ 的/ 主题/ 概率分布/ 的/ 决定/ 因素/ ./ 通过/ 积分/ 或/ 求和/ 可以/ 分别/ 得到/ 同一个/ 人/ 在/ 接受者/ 和/ 发送者/ 两个/ 角色/ 时/ 的/ 主题/ 概率分布/ ./ 进而/ ,/ 我们/ 还/ 可以/ 使用/ 这些/ 主题/ 概率分布/ 进行/ 聚类/ ,/ 判定/ 哪些/ 人/ 具有/ 相同/ 的/ 社会/ 角色/ ./ 比如说/ :/ 如果/ 一些/ 人/ 作为/ 接受者/ 时/ 总是/ 收到/ 诸如/ 要求/ 复印/ 、/ 旅行/ 预约/ 、/ 安排/ 会议室/ 等/ 信息/ ,/ 那么/ 我们/ 认为/ 他们/ 具有/ “/ 行政助理/ ”/ 这样/ 的/ 社会/ 角色/ ,/ 即便/ 这些/ 人所处/ 的/ 社会关系/ 完全/ 不同/ ./ Boyd/ -/ Graber/ 等/ 人/ [/ 40/ ]/ 提出/ 了/ 一个/ 基于/ Wordnet/ 的/ LDA/ 模型/ (/ LatentDirichletAllocationwithWORDNET/ ,/ LDAWN/ )/ ./ 通常/ ,/ 我们/ 用/ 一个/ 词项/ 概率分布/ 来/ 表示/ 一个/ 主题/ ,/ 但是/ 在/ LDAWN/ 模型/ 中/ ,/ 作者/ 针对/ 每/ 一个/ 主题/ ,/ 定义/ 了/ 一个/ 同义词/ 集/ (/ synset/ )/ 的/ 转移/ 概率/ 矩阵/ ./ 在/ 生成/ 过程/ 中/ ,/ 首先/ 选择/ 一个/ 主题/ 概率分布/ ,/ 然后/ 根据/ 该/ 主题/ 概率分布/ 选择/ 一个/ 主题/ ,/ 到/ 此/ 都/ 与/ LDA/ 的/ 做法/ 相同/ ./ 接下来/ ,/ LDAWN/ 选择/ 了/ 一条/ 以/ entity/ 为根/ 节点/ ,/ 不断/ 游历/ (/ walk/ )/ 直到/ 碰到/ 一个/ 由/ 单词/ 构成/ 的/ 叶/ 节点/ ,/ 输出/ 该/ 单词/ ./ 我们/ 可以/ 这样/ 认为/ ,/ 即便/ 是/ 相同/ 的/ 一个/ 单词/ ,/ 由于/ 其/ 语义/ (/ 主题/ )/ 的/ 不同/ ,/ 在/ 生成/ 该/ 单词/ 时/ 可能/ 在/ Wordnet/ 中/ 选择/ 一条/ 不同/ 路径/ ./ Nallapati/ 等/ 人/ [/ 41/ ]/ 在/ 2008/ 年/ 提出/ 了/ Link/ -/ PL/ -/ SA/ -/ LDA/ 模型/ ,/ 对于/ 任给/ 的/ 测试/ 集中/ 的/ 文档/ 可以/ 预测/ 其/ 引用/ 其它/ 文档/ 的/ 概率/ ./ 该/ 模型/ 分两/ 部分/ ,/ 一部分/ 针对/ 所有/ 被/ 引用/ 文档/ 构造/ 一个/ pLSI/ 模型/ ;/ 另/ 一部分/ 则/ 针对/ 所有/ 引用/ 文档/ 的/ 一个/ Link/ -/ LDA/ 模型/ ,/ 对于/ 一篇/ 文档/ 而言/ ,/ 该/ 模型/ 不仅/ 生成/ 其中/ 的/ 所有/ 单词/ ,/ 而且/ 生成/ 所有/ 的/ Link/ ,/ 而/ Link/ 所/ 指向/ (/ 引用/ )/ 的/ 文/ Page10/ 档/ 就是/ 那些/ 在/ pLSI/ 模型/ 训练/ 时/ 使用/ 的/ 文档/ ./ 由于/ 重名/ 以及/ 同一个/ 人/ 的/ 姓名/ 有/ 多种/ 写法/ ,/ 存在/ 人名/ 的/ 消歧/ 问题/ ./ Bhattacharya/ 等/ 人/ [/ 42/ ]/ 提出/ 了/ 一个/ 基于/ LDA/ 的/ 无/ 监督/ 的/ 实体/ 消解/ 模型/ 来/ 处理/ 人名/ 的/ 消歧/ 问题/ ./ 该/ 模型/ 使用/ 书目/ (/ bibliography/ )/ 信息/ 作为/ 输入/ 进行/ 训练/ ,/ 完成/ 训练/ 后/ ,/ 可以/ 推测/ 一条/ 书目/ 中/ 实体/ 引用/ (/ 作者姓名/ )/ 对应/ 的/ 真实/ 实体/ (/ 作者/ 实体/ )/ ./ 该/ 模型/ 把/ 一条/ 书/ 目的/ 所有/ 作者姓名/ 看成/ 是/ 文档/ 中/ 的/ 单词/ ,/ 构建/ 了/ 称为/ 组/ (/ group/ ,/ 相当于/ LDA/ 模型/ 中/ 的/ 主题/ )/ 的/ 隐含/ 变量/ ,/ 每个/ 组/ 代表/ 一个/ 作者/ 实体/ 的/ 概率分布/ ,/ 而/ 作者/ 引用/ 的/ 生成/ 是/ 作者/ 实体/ 的/ 属性/ (/ attribute/ ,/ 可以/ 理解/ 为/ 作者/ 的/ 全名/ )/ 经过/ 噪声/ 变形/ 得到/ 的/ (/ 如/ 中间/ 名/ 缩写/ ,/ 甚至/ 省略/ 等/ )/ ./ 该/ 方法/ 还/ 能/ 从/ 数据/ 中/ 推断出/ 真实/ 实体/ 的/ 数目/ ./ Song/ 等/ 人/ [/ 43/ ]/ 提出/ 了/ 一个/ 和/ AT/ 模型/ 类似/ 的/ LDA/ 扩展/ 模型/ ,/ 用于/ 无/ 监督/ 的/ 人名/ 消解/ ,/ 该/ 方法/ 把/ 文档/ 的/ 每个/ 作者姓名/ 看成/ 是/ 文档/ 生成/ 的/ 单词/ ,/ 添加/ 了/ 一个/ 作者姓名/ 变量/ 作为/ 观测/ 值/ ./ 这样/ 不仅/ 可以/ 得到/ 某个/ 主题/ 下/ 词项/ 的/ 概率分布/ ,/ 还/ 可以/ 得到/ 该/ 主题/ 下/ 作者姓名/ 的/ 概率分布/ ./ 实际/ 作者/ 的/ 个数/ 可以/ 通过/ 聚类/ 后/ 的/ 聚类/ 个数/ 进行/ 判定/ ./ 需要/ 注意/ 的/ 是/ ,/ Song/ 的/ 方法/ 考虑/ 到/ 了/ 文档/ 的/ 内容/ ,/ 而/ Bhattacharya/ 的/ 方法/ 只是/ 关注/ 作者姓名/ 的/ 共现/ ./ Mei/ 等/ 人/ [/ 44/ ]/ 提出/ 了/ 一个/ 主题/ 情感/ 混合/ 模型/ (/ Topic/ -/ SentimentMixture/ ,/ TSM/ )/ ,/ 该/ 模型/ 把/ 单词/ 分成/ 两大类/ ,/ 一类/ 是/ 与/ 主题/ 无关/ 的/ 普通/ 单词/ (/ 如/ the/ ,/ a/ ,/ of/ )/ ,/ 另一类/ 和/ 主题/ 有关/ 的/ 单词/ 又/ 分为/ 中性/ (/ 可/ 细分/ 为/ k/ 类/ )/ 、/ 正面/ 和/ 负面/ 三大类/ ./ 单词/ 的/ 生成/ 过程/ 是/ 依照/ 概率分布/ 在/ 四个/ 大/ 类/ 之间/ 选择/ 类/ ,/ 进而/ 在/ 类/ 内/ 选择/ 单词/ ./ EM/ 算法/ 被/ 用来/ 估计/ 每个/ 类中/ 的/ 词项/ 概率分布/ ./ 此外/ ,/ 还/ 对/ 情感/ 随/ 时间/ 的/ 动态/ 变换/ 进行/ 了/ 分析/ ,/ 判断/ 出/ 某些/ 单词/ 随/ 时间/ 变化/ 出现/ 情感/ 极性/ 的/ 波动/ 和/ 爆发/ (/ burst/ )/ ./ Titov/ 等/ 人/ [/ 45/ ]/ 提出/ 了/ 一个/ 文本/ 和/ 特征/ (/ aspect/ )/ 评价/ 的/ 混合/ 模型/ ,/ 认为/ 一篇/ 文档/ 可以/ 由/ 滑动/ 窗口/ (/ slidingwindow/ )/ 的/ 集合/ 构成/ ,/ 而/ 每个/ 滑动/ 窗口/ 又/ 由/ 连续/ 的/ 若干/ 句子/ 组成/ ./ 在/ 一个/ 滑动/ 窗口/ 中/ 存在/ 局部/ 主题/ 的/ 概率分布/ ,/ 而/ 整篇/ 文档/ 对应/ 一个/ 全局/ 主题/ 的/ 概率分布/ ./ 单词/ 可以/ 从/ 局部/ 主题/ 的/ 概率分布/ 中/ 生成/ ,/ 也/ 可以/ 从/ 全局/ 主题/ 的/ 概率分布/ 中/ 产生/ ./ 在/ 有关/ 旅游/ 评价/ 的/ 语料/ 中/ ,/ 全局/ 主题/ 对应/ 于/ 实体/ ,/ 如/ Londonhotels/ ,/ seasideresorts/ ,/ 而/ 局部/ 主题/ 对应/ 于/ 特征/ ,/ 比如/ location/ ,/ service/ ,/ room/ 等/ ./ 作者/ 还/ 将/ 每个/ 特征/ 的/ 评分/ 作为/ 观测/ 值/ 加入/ 到/ 模型/ ,/ 并/ 假定/ 对/ 特征/ 讨论/ 的/ 文本/ 是/ 对/ 该/ 特征/ 评分/ 的/ 预测/ 信息/ ,/ 这样/ 将/ 所/ 需要/ 的/ 特征/ 和/ 主题/ 关联/ 起来/ ,/ 避免/ 了/ LDA/ 模型/ 这种/ 无/ 监督/ 学习/ 中/ 出现/ 的/ 主题/ 含义/ 无法/ 显式/ 确定/ 的/ 问题/ ./ Doyle/ 等/ 人/ [/ 46/ ]/ 提出/ DCMLDA/ 模型/ 来/ 检测/ 文档/ 中/ 单词/ 的/ 爆发/ (/ burstiness/ )/ 现象/ (/ 即/ 某/ 单词/ 突然/ 大量/ 出现/ )/ ./ 和/ 标准/ LDA/ 模型/ 相比/ ,/ DCMLDA/ 模型/ 中/ 每个/ 文档/ 都/ 有/ 特定/ 的/ K/ 个/ 主题/ ,/ K/ 为/ 全局/ 主题/ 个数/ ./ 训练/ 结束/ 后/ ,/ 对于/ 一个/ 文档/ ,/ 可以/ 检查/ K/ 个/ 主题/ 中/ 哪些/ 单词/ 出现/ 了/ burstiness/ 现象/ ./ 在/ del/ / icio/ / us/ 网站/ 中/ ,/ 每个/ 页面/ 对应/ 若干个/ 标记/ (/ tag/ )/ ./ 但是/ ,/ 在/ 应用/ 这些/ 标记/ 的/ 时候/ ,/ 所/ 采用/ 的/ 标准/ 并不一定/ 完全一致/ ./ 为了/ 将/ 文档/ 中/ 的/ 单词/ 和/ 标记/ 进行/ 关联/ ,/ Ramage/ 等/ 人/ [/ 47/ ]/ 提出/ 了/ 一个多/ 标记/ 文本/ 分类器/ ,/ 称为/ LabeledLDA/ 模型/ ./ 作者/ 考虑/ 文档/ 集合/ 中/ 所有/ 可能/ 的/ 标记/ ,/ 让/ 每个/ 标记/ (/ tag/ )/ 对应/ 一个/ 主题/ ./ 在/ 训练/ 时/ ,/ 一个/ 文档/ 的/ 主题/ 的/ 个数/ 就是/ 文档/ 中/ 标记/ 的/ 个数/ ./ 训练/ 结束/ 后/ ,/ 我们/ 就/ 能/ 对/ 每个/ 单词/ 知道/ 其/ 对应/ 的/ 主题/ ,/ 从而/ 知道/ 其/ 标记/ ./ 基于/ 此/ ,/ 作者/ 还/ 进行/ 了/ 文本/ 片段/ (/ snippet/ )/ 抽取/ 和/ 多/ 标记/ 文本/ 分类/ 等/ 任务/ ./ Gerrish/ 与/ Blei/ [/ 48/ ]/ 提出/ 了/ DIM/ 模型/ (/ DocumentInfluenceModel/ )/ 来/ 识别/ 文档/ 集合/ 中/ 最/ 有/ 影响力/ 的/ 文档/ ./ 该/ 方法/ 基于/ Blei/ 等/ 人/ 在/ 2006/ 年/ 提出/ 的/ DTM/ 模型/ ,/ 把/ 文档/ 集合/ 按照/ 时间/ 进行/ 切片/ ,/ 并/ 对/ 每个/ 文档/ 附加/ 一个/ 影响力/ 的/ 隐/ 变量/ ./ 计算/ 文档/ 的/ 影响力/ 并/ 不是/ 一个/ 新/ 的/ 任务/ ,/ DIM/ 模型/ 的/ 最大/ 特色/ 是/ 没有/ 使用/ 文档/ 间/ 的/ 引用/ 关系/ ./ 该/ 模型/ 假设/ :/ 一篇/ 文档/ 的/ 影响力/ 越大/ ,/ 则/ 后续/ 时间/ 片中/ 的/ 主题/ 越/ 受到/ 这个/ 文档/ 的/ 影响/ ./ 实验/ 结果表明/ ,/ DIM/ 模型/ 得到/ 的/ 文档/ 影响力/ 和/ 引用/ 率/ 有着/ 很强/ 的/ 相关性/ ./ 针对/ 具体任务/ 的/ 主题/ 模型/ 十分/ 丰富/ ,/ 即便/ 是/ 相同/ 或者/ 类似/ 的/ 任务/ ,/ 都/ 会/ 存在/ 多个/ 主题/ 模型/ ./ 其/ 区别/ 可能/ 是/ 结构/ 不同/ ,/ 隐/ 变量/ 不同/ ,/ 甚至/ 是/ 边/ 的/ 方向不同/ ./ 在/ 此/ 不/ 一一列举/ ./ 本节/ 汇总/ 见表/ 1.8/ 主题/ 模型/ 发展/ 的/ 一些/ 趋势/ 总体/ 上/ 来说/ ,/ 主题/ 模型/ 的/ 大部分/ 工作/ 集中/ 在/ 面向/ 特定/ 的/ 任务/ 之中/ ./ 对/ 参数/ 的/ 扩展/ 和/ 引入/ 上下文/ 的/ 信息/ 的/ 工作/ 相对而言/ 较/ 少/ ,/ 主要/ 原因/ 是/ 后/ 两种/ 类型/ 的/ 工作/ 是/ 针对/ 主题/ 模型/ 的/ 整体/ 修改/ ,/ 可以/ 入手/ 的/ 研究/ 点不多/ ./ 除此之外/ ,/ 尤其/ 是/ 在/ 近几年/ ,/ 我们/ 也/ 注意/ 到/ 了/ 一些/ 新/ 的/ 趋势/ ./ 首先/ ,/ 出现/ 了/ 许多/ 关注/ 主题/ 模型/ 性能/ 的/ 工作/ ./ 这/ Page11/ 说明/ 主题/ 模型/ 已经/ 不/ 局限于/ 理论/ 研究/ 阶段/ ,/ 它/ 的/ 实用性/ 得到/ 认可/ ,/ 因此/ 呼唤/ 更加/ 高效/ 的/ 训练/ 算法/ ./ Nallapati/ 等/ 人/ [/ 49/ ]/ 提出/ 了/ 并行/ 的/ 变分/ EM/ (/ Varia/ -/ tionalEM/ )/ 算法/ 来/ 对/ 训练/ 过程/ 进行/ 加速/ ,/ 以便/ 应用/ 于/ 多处理器/ 和/ 分布式/ 环境/ ./ Asuncion/ 等/ 人/ [/ 50/ ]/ 给表/ 1/ 主题/ 模型/ 扩展/ 中/ 所/ 介绍/ 的/ 主题/ 模型/ 汇总/ 2004/ 年/ ,/ Blei/ 等/ 人/ [/ 23/ ]/ 2006/ 年/ ,/ Blei/ 等/ 人/ [/ 24/ ]/ 2006/ 年/ ,/ Li/ 等/ 人/ [/ 26/ ]/ 2007/ 年/ ,/ Mimno/ 等/ 人/ [/ 27/ ]/ hierarchicalPAM2006/ 年/ ,/ Wang/ 等/ 人/ [/ 28/ ]/ 2006/ 年/ ,/ Blei/ 等/ 人/ [/ 29/ ]/ 2004/ 年/ ,/ Griffiths/ 等/ 人/ [/ 30/ ]/ HMM/ -/ LDA2005/ 年/ ,/ Wang/ 等/ 人/ [/ 32/ ]/ 2007/ 年/ ,/ Gruber/ 等/ 人/ [/ 34/ ]/ HTMM/ (/ HiddenTopicMarkovModel/ )/ 以/ 句子/ 为/ 单位/ 分配/ 主题/ ./ 2009/ 年/ ,/ Boyd/ -/ Graber/ 等/ 人/ [/ 35/ ]/ STM/ (/ SyntacticTopicModel/ )/ 2008/ 年/ ,/ Blei/ 等/ 人/ [/ 36/ ]/ 2004/ 年/ ,/ Steyvers/ 等/ 人/ [/ 38/ ]/ AT/ (/ Author/ -/ Topic/ )/ 2004/ 年/ ,/ McCallum/ 等/ 人/ [/ 39/ ]/ ART/ (/ Author/ -/ Recipient/ -/ Topic/ )/ 2007/ 年/ ,/ Boyd/ -/ Graber/ 等/ 人/ [/ 40/ ]/ LDAWN/ (/ LatentDirichletAllocation2008/ 年/ ,/ Nallapati/ 等/ 人/ [/ 41/ ]/ Link/ -/ PLSA/ -/ LDA2006/ 年/ ,/ Bhattacharya/ 等/ 人/ [/ 42/ ]/ LDA/ -/ ER2007/ 年/ ,/ Song/ 等/ 人/ [/ 43/ ]/ 2007/ 年/ ,/ Mei/ 等/ 人/ [/ 44/ ]/ sLDA/ (/ supervisedLatentDirichletAllocation/ )/ 文档/ 有类/ 标号/ ./ 2008/ 年/ ,/ Titov/ 等/ 人/ [/ 45/ ]/ 2009/ 年/ ,/ Doyle/ 等/ 人/ [/ 46/ ]/ 2009/ 年/ ,/ Ramage/ 等/ 人/ [/ 47/ ]/ LabeledLDA2010/ 年/ ,/ Gerrish/ 等/ 人/ [/ 48/ ]/ DIM/ (/ DocumentInfluenceModel/ )/ 另外/ 一个/ 较为/ 明显/ 的/ 趋势/ 是/ 主题/ 模型/ 和/ 跨/ 语言/ 的/ 结合/ ./ 其中/ 一个/ 原因/ 是/ 机器翻译/ 本身/ 是/ 自然语言/ 处理/ 的/ 热点/ ,/ 积累/ 了/ 大量/ 的/ 跨/ 语言/ 的/ 语料/ ,/ 可/ 供/ 主题/ 模型/ 使用/ ./ Ni/ 等/ 人/ [/ 56/ ]/ 针对/ Wikipedia/ 提出/ 了/ 一个/ ML/ -/ LDA/ 模型/ 来/ 从/ 跨/ 语言/ 的/ 语料/ 中/ 抽取/ 主题/ ./ 每/ 一个/ 主题/ 都/ 对应/ 多种语言/ ./ 这样/ ,/ 不同/ 语言/ 的/ 新/ 文档/ 都/ 能够/ 用/ 统一/ 的/ 主题/ 来/ 表示/ ,/ 适合/ 跨/ 语言/ 的/ 网络应用/ ./ Mimno/ 等/ 人/ [/ 57/ ]/ 提出/ 的/ PLTM/ 与/ 此/ 非常/ 相似/ ./ 以上/ 两个/ 工作/ 所/ 使用/ 的/ 语料/ 是/ 文档/ 级/ 对齐/ 的/ ,/ 因此/ 在/ 语料/ 上/ 还有/ 所/ 限制/ ./ Jagarlamudi/ 等/ 人/ [/ 58/ ]/ 提出/ 了/ JointLDA/ 模型/ ,/ 用来/ 同时/ 对/ 西班牙语/ 和/ 英语/ 语料/ 进行/ 采样/ ./ 该/ 模型/ 使用/ 了/ 一个/ 双语/ 词典/ ./ 模型/ 训练/ 结束/ 后/ ,/ 每个/ 主题/ 可以/ 是/ 不同/ 语言/ 的/ 混合/ 主题/ ./ 作者/ 将/ 该/ 模型/ 应用/ 到/ 跨/ 语言/ 的/ 信息检索/ 中/ 取得/ 了/ 较/ 好/ 的/ 效/ LDA/ 模型/ 和/ HDP/ 模型/ 提出/ 了/ 分布式/ 算法/ ,/ 在/ 保证/ 全局/ 正确性/ 的/ 前提/ 下/ ,/ 各个/ 处理单元/ 能够/ 独立/ 进行/ Gibbs/ 采样/ ./ Hoffman/ 等/ 人/ [/ 51/ ]/ 提出/ 了/ LDA/ 模型/ 的/ 在线/ (/ online/ )/ 变分/ 贝叶斯/ 方法/ (/ variationalBayesian/ )/ ./ 其它/ 关注/ 主题/ 模型/ 性能/ 的/ 工作/ 还有/ 文献/ [/ 52/ -/ 55/ ]/ 等/ ./ 果/ ./ 该/ 模型/ 的/ 最大/ 特点/ 是/ 不/ 需要/ 对齐/ 的/ 文档/ ./ 类似/ 的/ 工作/ 还有/ Boyd/ -/ Graber/ 等/ 人/ [/ 59/ ]/ 提出/ 的/ MuTo/ 模型/ ./ 除此之外/ ,/ 还有/ 一些/ 工作/ 并/ 没有/ 归入/ 上面/ 的/ 分类/ ./ 比如/ :/ Zhu/ 等/ 人/ [/ 60/ ]/ 提出/ 了/ CTRF/ 模型/ ,/ 该/ 模型/ 能够/ 融合/ 单词/ 的/ 外部/ 特征/ 和/ 单词/ 间/ 主题/ 的/ 依赖/ 关系/ ,/ 是/ 一种/ 通用/ 的/ 机器/ 学习/ 方法/ ,/ 而/ 非/ 针对/ 某个/ 具体/ 的/ 任务/ ./ 这种/ 趋势/ 值得/ 我们/ 关注/ ./ 文献/ [/ 61/ -/ 62/ ]/ 假设/ 文档/ 之间/ 是/ 有关/ 的/ ,/ 破坏/ 了/ 原始/ LDA/ 模型/ 中/ 关于/ 文档/ 独立/ 的/ 假设/ ./ 新/ 的/ 假设/ 是/ 相似/ 的/ 文档/ 具有/ 相似/ 的/ 主题/ 分布/ ./ 各种/ 有/ 新意/ 的/ 工作/ 还有/ 很多/ ,/ 不/ 一一列举/ ./ 本节/ 列出/ 的/ 这些/ 工作/ ,/ 在/ 某种意义/ 上/ 说明/ 主题/ 模型/ 在/ 深度/ 和/ 广度/ 上/ 仍/ 在/ 进行/ 着/ 渗透/ ,/ 体现/ 了/ 主题/ 模型/ 的/ 生命力/ ./ Page129/ 总结/ 和/ 展望/ 从/ 主题/ 模型/ 的/ 发展/ 脉络/ 来看/ ,/ 各个/ 工作/ 之间/ 都/ 有着/ 紧密/ 的/ 联系/ 和/ 延续性/ ./ 在/ LSI/ 中/ ,/ 出现/ 了/ 隐性/ 语义/ ,/ 而/ 这/ 实际/ 就是/ 现在/ 主题/ 模型/ 中/ 的/ 主题/ ./ LSI/ 通过/ 对/ 相似/ 度/ 矩阵/ 计算/ 特征向量/ 构造/ 了/ 一个/ 线性变换/ ,/ 将/ 词项/ 空间/ 的/ 文档/ 变换/ 到/ 了/ 隐性/ 语义/ 空间/ (/ 主题/ 空间/ )/ ./ 从/ 词项/ 空间/ 到/ 了/ 隐性/ 语义/ (/ 主题/ )/ 空间/ 变换/ 这/ 一点/ 来看/ ,/ LSI/ ,/ pLSI/ 一直/ 到/ LDA/ 是/ 一致/ 的/ ./ 它们/ 的/ 区别/ 在于/ 最优化/ 的/ 时候/ 使用/ 的/ 目标/ 函数/ 不同/ ,/ 或者/ 主题/ 模型表示/ 上/ 有所/ 差别/ ./ LDA/ 主题/ 模型/ 作为/ 概率/ 生成/ 模型/ ,/ 被/ 直接/ 或/ 扩展/ 使用/ 在/ 自然语言/ 处理/ 的/ 众多/ 任务/ 中/ ./ 对于/ 主题/ 模型/ 而言/ ,/ 最/ 重要/ 的/ 两组/ 参数/ 是/ 各/ 主题/ 下/ 的/ 词项/ 概率分布/ 和/ 各/ 文档/ 的/ 主题/ 概率分布/ ./ 由于/ 通常/ 无法/ 求得/ 精确/ 解/ ,/ EM/ 算法/ 经常/ 被/ 应用/ 在/ 主题/ 模型/ 的/ 参数估计/ 中/ ,/ 而/ 理解/ EM/ 算法/ 在/ 主题/ 模型/ 各个/ 阶段/ 的/ 具体/ 使用/ ,/ 也/ 有助于/ 了解/ 主题/ 模型/ 的/ 发展/ 中/ 各项/ 工作/ 之间/ 的/ 关联/ ./ 当然/ ,/ 关于/ 人类/ 语言/ 的/ 生成/ 本质/ ,/ 学术界/ 还/ 存在/ 争议/ ./ 作为/ 概率/ 生成/ 模型/ ,/ 主题/ 模型/ 也/ 有/ 其/ 局限性/ ./ 在/ 今后/ 的/ 主题/ 模型/ 发展/ 中/ ,/ 人们/ 需要/ 对/ 语言/ 和/ 问题/ 的/ 本质/ 进行/ 更为/ 深入/ 的/ 分析/ 和/ 观察/ ,/ 以便/ 构造/ 出/ 符合实际/ 问题/ 的/ 主题/ 模型/ ./ 

