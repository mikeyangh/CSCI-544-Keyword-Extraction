Page1/ 协同/ 过滤/ 推荐/ 系统/ 中/ 的/ 用户/ 博弈/ 1/ )/ (/ 清华大学电子工程系/ 北京/ 100084/ )/ 2/ )/ (/ 中国/ 传媒大学/ 信息/ 工程学院/ 北京/ 100024/ )/ 摘要/ 在/ 以/ 协同/ 过滤/ 算法/ 为/ 核心/ 的/ 推荐/ 系统/ 中/ ,/ 一个/ 用户/ 能否/ 获得/ 高质量/ 的/ 推荐/ 不仅/ 取决于/ 用户/ 自身/ 是否/ 积极/ 地/ 参与/ 项目/ 评分/ ,/ 还/ 取决于/ 其他/ 用户/ 是否/ 能/ 提供/ 足够/ 多/ 的/ 评分/ ./ 由于/ 对/ 项目/ 评分/ 是/ 需要/ 付出/ 成本/ 的/ ,/ 理性/ 的/ 用户/ 总是/ 希望/ 以/ 尽可能少/ 的/ 评分/ 换取/ 高质量/ 的/ 推荐/ ./ 该文/ 用/ 博弈论/ 的/ 方法/ 对/ 协同/ 过滤/ 系统/ 中/ 的/ 用户/ 评分/ 行为/ 进行/ 分析/ ./ 考虑/ 到/ 一个/ 用户/ 通常/ 无法/ 观察/ 到/ 其他/ 用户/ 的/ 评分/ 和/ 得到/ 的/ 推荐/ ,/ 该文/ 将/ 用户/ 间/ 的/ 交互/ 建模/ 为/ 不/ 完全/ 信息/ 博弈/ ,/ 并/ 引入/ “/ 满足/ 均衡/ ”/ 的/ 概念/ 来/ 分析/ 该/ 博弈/ ./ 该文/ 假定/ 每个/ 用户/ 都/ 对/ 推荐/ 质量/ 有/ 一个/ 预期/ ,/ 当/ 所有/ 用户/ 的/ 预期/ 都/ 得到/ 满足/ 时/ ,/ 博弈/ 即/ 达到/ 均衡/ ./ 针对/ 所/ 建立/ 的/ 博弈/ 模型/ ,/ 该文/ 设计/ 了/ 一种/ 均衡/ 学习/ 算法/ ,/ 该/ 算法/ 允许/ 用户/ 以/ 逐渐/ 增加/ 评分/ 数量/ 的/ 方式/ 来/ 寻找/ 均衡/ 策略/ ./ 理论/ 分析/ 和/ 仿真/ 结果/ 均/ 表明/ ,/ 当/ 所有/ 用户/ 对/ 推荐/ 质量/ 有着/ 相似/ 的/ 预期/ 时/ ,/ 所提/ 算法/ 可/ 收敛/ 到/ 满足/ 均衡/ ./ 这一/ 分析/ 结果/ 可以/ 为/ 协作/ 式/ 系统/ 中/ 激励机制/ 的/ 设计/ 提供/ 启发/ ./ 关键词/ 协同/ 过滤/ ;/ 博弈论/ ;/ 满足/ 均衡/ ;/ 均衡/ 学习/ ;/ 收敛/ 条件/ ;/ 社交/ 网络/ ;/ 社会/ 媒体/ 1/ 引言/ 作为/ 一种/ 有效/ 的/ 信息/ 过滤/ 手段/ ,/ 个性化/ 推荐/ 已/ 广泛应用/ 于/ 电子商务/ 、/ 视频/ 分享/ 、/ 在线/ 社交/ 等/ 领域/ ./ 协同/ 过滤/ (/ collaborativefiltering/ )/ 是/ 目前/ 推荐/ 系统/ 最/ 常用/ 的/ 一类/ 算法/ [/ 1/ ]/ ,/ 其/ 基本/ 思想/ 是/ 分析/ 大量/ 用户/ 对/ 项目/ 的/ “/ 评分/ ”/ 以/ 发现/ 用户/ 偏好/ 的/ 相似性/ ,/ 然后/ 向/ 用户/ 推荐/ 符合/ 其/ 偏好/ 的/ 项目/ ./ 这种/ 推荐/ 算法/ 能否/ 取得/ 良好效果/ 很大/ 程度/ 上/ 取决于/ 用户/ 是否/ 积极/ 地/ 参与/ 项目/ 评分/ ./ 用户/ 对/ 项目/ 的/ 评分/ 主要/ 基于/ 用户/ 自身/ 的/ 体验/ ./ 由于/ 给/ 项目/ 评分/ 是/ 需要/ 付出/ “/ 成本/ ”/ 的/ —/ —/ —/ 评分/ 过程/ 会/ 占用/ 一些/ 时间/ 、/ 评分/ 数据/ 可能/ 泄露/ 用户/ 的/ 隐私/ ,/ 用户/ 通常/ 不会/ 对/ 其/ 体验/ 过/ 的/ 所有/ 项目/ 都/ 进行/ 评分/ ./ 若/ 用户/ 出于/ 成本/ 考虑/ 只/ 提供/ 很少/ 的/ 评分/ 数据/ ,/ 则/ 推荐/ 服务器/ 就/ 无法/ 准确/ 计算/ 用户/ 之间/ 的/ 相似性/ ,/ 进而/ 无法/ 为/ 用户/ 提供/ 高质量/ 的/ 推荐/ ./ 为了/ 鼓励/ 用户/ 参与/ 评分/ ,/ 推荐/ 服务器/ 可/ 向/ 用户/ 提供/ 奖金/ 、/ 积分/ 等/ 形式/ 的/ 激励/ ./ 目前/ 在/ 协同/ 过滤/ 的/ 研究/ 中/ 对/ 激励机制/ 的/ 讨论/ 还/ 比较/ 少见/ ,/ 但/ 在/ P2P/ 资源共享/ [/ 2/ -/ 3/ ]/ 、/ 参与/ 感知/ [/ 4/ -/ 6/ ]/ 、/ 众包/ [/ 7/ ]/ 等/ 类似/ 的/ 问题/ 场景/ 中/ 已有/ 较/ 多/ 关于/ 激励机制/ 的/ 研究/ ./ 在/ 个性化/ 推荐/ 系统/ 中/ ,/ 除了/ 奖金/ 等/ 形式/ 的/ 外部/ 激励/ ,/ 推荐/ 结果/ 本身/ 即可/ 视为/ 一种/ 促使/ 用户/ 对/ 项目/ 评分/ 的/ 内部/ 激励/ —/ —/ —/ 用户/ 若/ 要/ 获得/ 准确/ 的/ 推荐/ ,/ 则/ 有/ 必要/ 向/ 推荐/ 服务器/ 提供/ 足够/ 的/ 评分/ 数据/ 以/ 清楚/ 地/ 表达/ 自己/ 的/ 偏好/ ./ 在/ 没有/ 外部/ 激励/ 的/ 情况/ 下/ ,/ 用户/ 可否/ 自发/ 地/ 提供/ 足够/ 多/ 的/ 评分/ 数据/ 以/ 使/ 推荐/ 服务器/ 产生/ 令人满意/ 的/ 推荐/ ?/ 如果/ 可以/ ,/ 用户/ 在/ 选择/ 项目/ 进行/ 评分/ 时应/ 如何/ 决策/ ?/ 本文/ 拟/ 通过/ 研究/ 用户/ 之间/ 的/ “/ 博弈/ ”/ 回答/ 上述/ 问题/ ./ 如前所述/ ,/ 用户/ 对/ 项目/ 评分/ 是/ 有/ 成本/ 的/ ./ 用户/ 提供/ 的/ 评分/ 越/ 多/ ,/ 付出/ 的/ 成本/ 就/ 越/ 高/ ./ 用户/ 在/ 决定/ 是否/ 对/ 一个/ 项目/ 进行/ 评分/ 时/ ,/ 需/ 仔细/ 权衡/ 评分/ 成本/ 和/ 推荐/ 质量/ ./ 此外/ ,/ 推荐/ 服务器/ 采用/ “/ 协同/ ”/ 过滤/ 算法/ 计算/ 推荐/ ,/ 这/ 意味着/ 一个/ 用户/ 获得/ 的/ 推荐/ 不仅/ 与/ 用户/ 自己/ 提供/ 的/ 评分/ 有关/ ,/ 还/ 与/ 其他/ 用户/ 提供/ 的/ 评分/ 有关/ ./ 换言之/ ,/ 不同/ 用户/ 的/ 评分/ 行为/ 是/ 相互影响/ 的/ ./ 若/ 假定/ 用户/ 是/ 理性/ 的/ ,/ 即/ 每个/ 用户/ 都/ 希望/ 以/ 尽可能/ 低/ 的/ 评分/ 成本/ 获得/ 满意/ 的/ 推荐/ ,/ 那么/ 可以/ 认为/ 推荐/ 系统/ 中/ 的/ 各/ 用户/ 是/ 以/ 推荐/ 服务器/ 为/ 媒介/ 进行/ 着/ 某种/ 形式/ 的/ 博弈/ ,/ 因此/ 本文/ 考虑/ 用/ 博弈论/ [/ 8/ ]/ 的/ 方法/ 对/ 用户/ 的/ 评分/ 行为/ 进行/ 分析/ ./ 目前/ 在/ 协同/ 过滤/ 的/ 相关/ 研究/ 中/ ,/ 博弈论/ 的/ 应用/ 并/ 不多见/ ./ Halkidi/ 等/ 人/ 在/ 文献/ [/ 9/ ]/ 中/ 用/ 博弈论/ 的/ 方法/ 分析/ 了/ 协同/ 过滤/ 推荐/ 系统/ 中/ 的/ 隐私/ 保护/ 问题/ ./ 在/ 他们/ 研究/ 的/ 博弈/ 模型/ 中/ ,/ 用户/ 向/ 推荐/ 服务器/ 提供/ 的/ 评分/ 向量/ 被/ 视为/ 用户/ 的/ 策略/ ./ 为/ 避免/ 评分/ 数据/ 泄露/ 隐私/ ,/ 用户/ 可/ 向/ 推荐/ 服务器/ 提供/ 虚假/ 评分/ ,/ 但/ 这会/ 降低/ 推荐/ 准确率/ ./ 用户/ 面临/ 的/ 问题/ 是/ 如何/ 修改/ 评分/ 才能/ 在/ 不/ 明显降低/ 推荐/ 准确率/ 的/ 前提/ 下/ 最大/ 限度/ 地/ 保留/ 隐私/ ./ 为/ 寻找/ 博弈/ 的/ 均衡/ [/ 8/ ]/ ,/ Halkidi/ 等/ 人/ 将/ 用户/ 与/ 推荐/ 服务器之间/ 的/ 交互/ 建模/ 为/ 一个/ 迭代/ 过程/ ./ 在/ 每/ 一轮/ 迭代/ 中/ ,/ 每个/ 用户/ 可/ 利用/ 其他/ 用户/ 在/ 前/ 一轮/ 迭代/ 提供/ 的/ 评分/ 来/ 确定/ 自己/ 当前/ 的/ 最优/ 策略/ ./ 但/ 在/ 实际/ 的/ 推荐/ 系统/ 中/ ,/ 用户/ 通常/ 不会/ 反复/ 更新/ 自己/ 的/ 评分/ ,/ 并且/ 用户/ 也/ 无法/ 全面/ 准确/ 地/ 获得/ 其他/ 用户/ 的/ 评分/ 信息/ ,/ 因而/ 上述/ 博弈/ 分析/ 的/ 实际意义/ 有待/ 商榷/ ./ 考虑/ 到/ 在/ 实际/ 的/ 推荐/ 系统/ 中/ ,/ 一个/ 用户/ 既/ 无法/ 完整/ 的/ 观察/ 到/ 其他/ 用户/ 的/ 评分/ ,/ 也/ 无法/ 了解/ 推荐/ 服务器/ 为/ 其他/ 用户/ 提供/ 了/ 怎样/ 的/ 推荐/ ,/ 因此/ ,/ 与/ Halkidi/ 等/ 人/ 的/ 研究/ 不同/ ,/ 本文/ 将/ 用户/ 间/ 的/ 交互/ 建模/ 为/ 一种/ 不/ 完全/ 信息/ 博弈/ ,/ 并/ 应用/ “/ 满足/ 均衡/ ”/ (/ satisfactionequilibrium/ )/ 的/ 概念/ 来/ 分析/ 该/ 博弈/ 的/ 均衡/ ./ 满足/ 均衡/ 描述/ 的/ 是/ 这样/ 一种/ 状态/ :/ 博弈/ 中/ 所有/ 参与者/ 的/ 个体/ 约束条件/ 都/ 得到/ 满足/ ./ Ross/ 和/ Chaib/ -/ draa/ [/ 10/ ]/ 最先/ 提出/ 满足/ 均衡/ 这一/ 概念/ ,/ 用于/ 解决/ 不/ 完全/ 信息/ 博弈/ 的/ 纳什/ 均衡/ (/ Nashequilibrium/ )/ 难以/ 分析/ 的/ 问题/ ./ 为/ 将/ 满足/ 均衡/ 应用/ 于/ 协同/ 过滤/ 系统/ ,/ 本文/ 假定/ 每个/ 用户/ 都/ 对/ 推荐/ 结果/ 的/ 质量/ 有/ 一个/ 预期/ ,/ 当/ 实际/ 的/ 推荐/ 质量/ 高于/ 预期/ 时/ ,/ 用户/ 即/ 感到/ “/ 满足/ ”/ ./ 不同/ 用户/ 的/ 预期/ 一般/ 是/ 不同/ 的/ ./ 当/ 所有/ 用户/ 的/ 预期/ 都/ 得到/ 满足/ 时/ ,/ 用户/ 间/ 的/ 博弈/ 达到/ 满足/ 均衡/ ./ 基于/ 所/ 建立/ 的/ 博弈/ 模型/ ,/ 本文/ 提出/ 了/ 一种/ 均衡/ 学习/ 算法/ ./ 所提/ 算法/ 参考/ 了/ perlaza/ 等/ 人/ [/ 11/ -/ 14/ ]/ 在/ 研究/ 满足/ 均衡/ 学习/ 算法/ 时/ 提出/ 的/ 基于/ 迭代/ 的/ 行动/ 规则/ ,/ 但/ 现有/ 研究/ 主要/ 是/ 针对/ 无线通信/ 网络/ 中/ QoS/ (/ QualityofService/ )/ 配置/ 问题/ 的/ ,/ 与/ 本文/ 研究/ 的/ 协同/ 过滤/ 问题/ 不同/ ,/ 因而/ 现有/ 的/ 学习/ 算法/ 不能/ 直接/ 用于/ 本文/ 构建/ 的/ 博弈/ 模型/ ./ 结合/ 推荐/ 服务器/ 的/ 工作/ 原理/ ,/ 本文/ 定义/ 了/ 如下/ 的/ 行动/ 规则/ :/ 在/ 每/ 一轮/ 迭代/ 中/ ,/ 用户/ 根据/ 当前/ 所得/ 推荐/ 是否/ 达到/ 预期/ 以/ 决定/ 是否/ 选择/ 新/ 的/ 项目/ 进行/ 评分/ ./ 换言之/ ,/ 本文/ 设计/ 的/ 均衡/ 学习/ 算法/ 是/ 让/ 用户/ 以/ 逐渐/ 增加/ 评分/ 数量/ 的/ 方式/ 来/ 寻找/ 均衡/ 策略/ ./ 在/ 适当/ 的/ 简化/ 假设/ 下/ ,/ 本文/ 对/ 所/ 提/ 算法/ 的/ 收敛性/ 进行/ 了/ 理论/ 分析/ ,/ 并/ 通过/ 一系列/ 仿真/ 实验/ 对/ 理论/ 分析/ 的/ 结果/ 进行/ 了/ 验证/ ./ 本文/ 第/ 2/ 节/ 介绍/ 本文/ 所提/ 的/ 博弈/ 模型/ 并/ 给出/ 满足/ 均衡/ 的/ 描述/ ;/ 第/ 3/ 节/ 介绍/ 均衡/ 学习/ 算法/ 并/ 给出/ 算/ Page3/ 法/ 收敛性/ 的/ 理论/ 分析/ ;/ 第/ 4/ 节/ 介绍/ 仿真/ 实验/ 的/ 设计/ 并/ 对/ 仿真/ 结果/ 进行/ 分析/ ;/ 第/ 5/ 节/ 给出/ 结论/ ./ 2/ 博弈/ 模型/ 2.1/ 场景/ 描述/ 在/ 一个/ 推荐/ 系统/ 中/ ,/ 用户/ 集合/ 为/ N/ }/ ,/ 项目/ 集合/ 为/ S/ =/ {/ s1/ ,/ s2/ ,/ …/ ,/ sM/ }/ ./ 用/ Si/ 表示/ 用户/ i/ 体验/ 过/ 的/ 项目/ 的/ 集合/ ,/ 用/ 珟/ Si/ 表示/ 用户/ i/ 给出/ 了/ 评分/ 的/ 项目/ 集合/ ,/ 则/ 有/ 珟/ Si/ / Si/ / S/ ./ 用户/ i/ 对/ 珟/ Si/ 中/ 的/ 项目/ 评分/ ,/ 相当于/ 向/ 推荐/ 服务器/ 提供/ 了/ 评分/ 向量/ 狉/ i/ =/ (/ ri1/ ,/ ri2/ ,/ …/ ,/ riM/ )/ ./ 本文/ 规定/ :/ 对/ 任意/ j/ ∈/ {/ 1/ ,/ 2/ ,/ …/ ,/ M/ }/ ,/ 若/ sj/ ∈/ 珟/ Si/ ,/ 则/ 0/ </ rij/ / rmax/ ;/ 若/ sj/ / 珟/ Si/ ,/ 则/ 有/ rij/ =/ 0/ ./ 所有/ 用户/ 提供/ 的/ 评分/ 构成/ 一个/ “/ 用户/ -/ 项目/ ”/ 评分/ 矩阵/ 犚/ =/ [/ rij/ ]/ N/ ×/ M/ ./ 在/ 得到/ 评分/ 矩阵/ 后/ ,/ 推荐/ 服务器/ 应用/ 某种/ 协同/ 过滤/ 算法/ 预测/ 矩阵/ 中/ 的/ 未知/ 评分/ (/ rij/ =/ 0/ )/ ,/ 而后/ 根据/ 预测/ 结果/ 向/ 用户/ 推荐/ 项目/ ./ 用/ 狉/ ^/ i/ =/ (/ r/ ^/ i1/ ,/ r/ ^/ i2/ ,/ …/ ,/ r/ ^/ iM/ )/ 表示/ 与/ 用户/ i/ 对应/ 的/ 预测/ 结果/ ,/ 其中/ r/ ^/ ij/ (/ j/ =/ 1/ ,/ 2/ ,/ …/ ,/ M/ )/ 定义/ 如下/ :/ CFij/ (/ 犚/ )/ 表示/ 预测/ 的/ 评分/ 由/ 全体/ 用户/ 的/ 评分/ 和/ 所采/ f/ 用/ 的/ 协同/ 过滤/ 算法/ 决定/ ./ 例如/ ,/ 若/ 采用/ 基于/ 用户/ 的/ 最近/ 邻/ 协同/ 过滤/ 算法/ ,/ f/ 式/ 中/ :/ Neighbour/ (/ i/ )/ 表示/ 与/ 用户/ i/ 最/ 相似/ 的/ 若干个/ 用户/ ;/ Fsim/ (/ i/ ,/ k/ )/ 表示/ 用户/ i/ 与/ 用户/ k/ 的/ 相似/ 度/ ,/ 可用/ Pearson/ 相关系数/ 或/ 余弦/ 相似/ 度/ 度量/ [/ 1/ ]/ ./ 式/ (/ 2/ )/ 的/ 含义/ 是/ :/ 找出/ 与/ 用户/ i/ 具有/ 相似/ 偏好/ 的/ 若干/ 用户/ ,/ 然后/ 根据/ 这些/ 用户/ 对/ 项目/ sj/ 的/ 偏好/ 程度/ 来/ 预测/ 用户/ i/ 对/ 项目/ sj/ 的/ 偏好/ 程度/ ./ 在/ 实际/ 的/ 推荐/ 系统/ 中/ ,/ 推荐/ 服务器/ 会/ 根据/ 狉/ ^/ i/ 生成/ 用户/ i/ 的/ 推荐/ 结果/ ,/ 即/ 推荐/ 服务器/ 将/ 预测/ 评分/ 最高/ 的/ 若干个/ 项目/ 推荐/ 给/ 用户/ ./ 为/ 便于/ 分析/ ,/ 本文/ 假定/ 推荐/ 服务器/ 直接/ 将/ 狉/ ^/ i/ 返回/ 给/ 用户/ ./ 用户/ i/ 在/ 得到/ 狉/ ^/ i/ 后/ ,/ 会/ 根据/ 自身/ 的/ 兴趣/ 对/ 这/ 一/ 结果/ 进行/ 评估/ ./ 用/ 狆/ i/ =/ (/ pi1/ ,/ pi2/ ,/ …/ ,/ piM/ )/ 表示/ 用户/ i/ 的/ 兴趣/ ,/ 其中/ pij/ (/ j/ ∈/ {/ 1/ ,/ 2/ ,/ …/ ,/ M/ }/ )/ 表示/ 用户/ i/ 对/ 项目/ sj/ 的/ 偏好/ 度/ ./ 规定/ :/ 0/ / pij/ / rmax/ ,/ 并且/ 若/ sj/ ∈/ 珟/ Si/ ,/ 则/ pij/ =/ rij/ ,/ 即/ 用户/ 给出/ 的/ 项目/ 评分/ 等于/ 用户/ 对/ 该项/ 目的/ 偏好/ 度/ ./ 若/ sj/ / 珟/ Si/ ,/ 可/ 将/ pij/ 理解/ 为/ 用户/ 对/ 项目/ 的/ “/ 潜在/ ”/ 评分/ ,/ 即/ 如果/ 用户/ 在/ 将来/ 的/ 某/ 一/ 时刻/ 对/ sj/ 评分/ ,/ 他/ 给出/ 的/ 评分/ 就/ 会/ 是/ pij/ ./ 为/ 每个/ 用户/ 定义/ 函数/ gi/ :/ M/ →/ [/ 0/ ,/ 1/ ]/ 来/ 度量/ 推荐/ 结果/ 狉/ ^/ i/ 的/ 质量/ :/ / 式/ 中/ ,/ 右侧/ 第/ 2/ 项中/ 的/ ∑/ 分/ 向量/ 狉/ ^/ i/ 与/ 用户/ 兴趣/ 向量/ 狆/ i/ 的/ 距离/ ,/ rmax/ 槡/ M/ 表示/ 这一/ 距离/ 可能/ 取得/ 的/ 最大值/ ./ gi/ (/ 狉/ ^/ i/ )/ 的/ 值/ 越/ 大/ 表示/ 推荐/ 结果/ 与/ 用户/ 兴趣/ 的/ 匹配/ 程度/ 越高/ ,/ 即/ 推荐/ 质量/ 越高/ ./ 从式/ (/ 1/ )/ ~/ (/ 3/ )/ 可以/ 看出/ ,/ 一个/ 用户/ 所得/ 推荐/ 的/ 质量/ 与/ 其他/ 用户/ 提供/ 的/ 评分/ 密切相关/ ./ 协同/ 过滤/ 系统/ 中/ 的/ 每个/ 用户/ 实际上/ 是/ 以/ 向/ 推荐/ 服务器/ 提供/ 评分/ 的/ 方式/ 与/ 其他/ 用户/ 进行/ 着/ 博弈/ ./ 下/ 一/ 小节/ 给出/ 该/ 博弈/ 的/ 满足/ 式/ 表述/ (/ satisfactionform/ )/ [/ 11/ ]/ ./ 2.2/ “/ 满足/ ”/ 博弈/ 2.2/ ./ 1/ 参与者/ 和/ 行动/ 本文/ 将/ 集合/ 者/ ,/ 将/ 珟/ Si/ 视为/ 用户/ i/ 的/ 行动/ (/ action/ )/ ,/ 即/ ai/ =/ 珟/ Si/ ./ 用/ 表示/ 用户/ i/ 的/ 行动/ 空间/ (/ actionspace/ )/ ./ 所有/ 用户/ 共享/ 同一个/ 行动/ 空间/ ,/ 即/ 对/ 任意/ i/ ∈/ / {/ A/ (/ 1/ )/ ,/ A/ (/ 2/ )/ ,/ …/ ,/ A/ (/ K/ )/ }/ ,/ 其中/ A/ (/ k/ )/ 1/ ,/ 2/ ,/ …/ ,/ K/ )/ ,/ K/ =/ 2/ |/ S/ |/ -/ 1/ ./ 用户/ i/ 从/ 遵循/ 特定/ 的/ 概率分布/ π/ i/ =/ (/ π/ (/ k/ )/ i/ 表示/ 用户/ i/ 选择/ 行动/ A/ (/ k/ )/ 的/ 概率/ ./ 不同/ 用户/ 的/ π/ i/ π/ 一般/ 是/ 不同/ 的/ ./ 由式/ (/ 1/ )/ 可知/ ,/ 若/ 给定/ 协同/ 过滤/ 算法/ ,/ 则/ 推荐/ 服务器返回/ 给/ 每个/ 用户/ 的/ 推荐/ 结果/ 狉/ ^/ i/ 完全/ 由/ 评分/ 矩阵/ 犚/ 确定/ ,/ 而/ 犚/ 是/ 由/ 所有/ 用户/ 的/ 行动/ 组合/ 犪/ =/ (/ a1/ ,/ a2/ ,/ …/ ,/ aN/ )/ 确定/ 的/ ./ 为/ 体现/ 用户/ 行动/ 对/ 推荐/ 质量/ 的/ 影响/ ,/ 可/ 将/ gi/ (/ 狉/ ^/ i/ )/ 改写/ 为/ gi/ (/ 狉/ ^/ i/ )/ =/ hi/ (/ ai/ ,/ 犪/ -/ i/ )/ ,/ 其中/ 犪/ -/ i/ =/ (/ a1/ ,/ …/ ,/ ai/ -/ 1/ ,/ ai/ +/ 1/ ,/ …/ ,/ aN/ )/ ,/ hi/ (/ ·/ )/ 表示/ 从/ / 2/ ×/ …/ ×/ / N/ 到/ [/ 0/ ,/ 1/ ]/ 的/ 映射/ ./ 直观/ 上/ 理解/ ,/ 无论是/ 用户/ i/ 自己/ 提供/ 更/ 多/ 的/ 评分/ 还是/ 其他/ 用户/ 提供/ 更/ 多/ 的/ 评分/ ,/ 用户/ i/ 都/ 能/ 获得/ 更好/ 的/ 推荐/ ./ 为/ 度量/ 用户/ 提供/ 评分/ 的/ 数量/ ,/ 定义/ 用户/ i/ 的/ 评分/ 完整/ 度/ 如下/ :/ 由/ ai/ / Si/ 且/ ai/ ≠/ / 可知/ 0/ </ σ/ i/ / 1/ ./ 用/ σ/ -/ i/ 表示/ 其他/ 用户/ 的/ 评分/ 完整/ 度/ 的/ 平均值/ ,/ 即/ 引入/ σ/ i/ 和/ σ/ -/ i/ 之后/ ,/ 可/ 将/ hi/ (/ ai/ ,/ 犪/ -/ i/ )/ 改写/ 为/ h/ (/ σ/ i/ ,/ σ/ -/ i/ ;/ 狆/ i/ )/ ./ 函数/ h/ (/ ·/ ;/ 狆/ i/ )/ 以/ 狆/ i/ 作为/ 参数/ ,/ 以/ σ/ i/ 和/ σ/ -/ i/ 作为/ 输/ Page4/ 入/ 变量/ ./ 本文/ 对/ h/ (/ ·/ ;/ 狆/ i/ )/ 的/ 单调/ 性/ 做出/ 如下/ 假设/ ./ 假设/ 1/ ./ 对/ 任意/ 用户/ i/ ∈/ / 对/ 所有/ σ/ i/ ∈/ (/ 0/ ,/ 1/ ]/ 和/ σ/ -/ i/ ∈/ (/ 0/ ,/ 1/ ]/ 都/ 成立/ :/ (/ 1/ )/ (/ 2/ )/ 上述/ 假设/ 表明/ ,/ 当/ 每个/ 用户/ 都/ 对/ 其/ 体验/ 过/ 的/ 所有/ 项目/ 给出/ 评分/ 时/ ,/ 用户/ 可/ 得到/ 最好/ 的/ 推荐/ ./ 用/ Γ/ max/ 表示/ 最优/ 的/ 推荐/ 质量/ ,/ 则/ 有/ Γ/ max/ 言/ 部分/ 提到/ 过/ ,/ 用户/ 对/ 项目/ 评分/ 是/ 需要/ 付出/ 成本/ 的/ ./ 用/ ci/ (/ ai/ )/ 表示/ 用户/ i/ 选择/ 行动/ ai/ 时所/ 付出/ 的/ 成本/ ./ 对/ 任意/ ai/ ∈/ Ai/ 和/ ai/ ∈/ Ai/ ,/ 若/ ai/ / ai/ ,/ 则/ 有/ ci/ (/ ai/ )/ </ ci/ (/ ai/ )/ ./ 2.2/ ./ 2/ 满足/ 式/ 表述/ 由于/ 存在/ 评分/ 成本/ ,/ 用户/ 通常/ 不会/ 对/ 其/ 体验/ 过/ 的/ 所有/ 项目/ 都/ 进行/ 评分/ ,/ 这/ 就/ 意味着/ 最优/ 推荐/ Γ/ max/ 很难/ 实现/ ./ 假定/ 每个/ 用户/ i/ 对/ 推荐/ 质量/ 有/ 一个/ 低于/ 最优/ 值/ 的/ 预期/ Γ/ i/ ./ 给定/ 用户/ 的/ 行动/ 组合/ 犪/ ,/ 只要/ hi/ (/ 犪/ )/ / Γ/ i/ ,/ 用户/ i/ 即可/ 满足/ ./ 定义/ 映射/ fi/ :/ (/ / -/ i/ / / 1/ ×/ …/ ×/ / i/ -/ 1/ ×/ / i/ +/ 1/ ×/ …/ ×/ / N/ )/ 如下/ :/ fi/ (/ 犪/ -/ i/ )/ =/ ai/ ∈/ / i/ :/ hi/ (/ ai/ ,/ 犪/ -/ i/ )/ / Γ/ 映射/ fi/ 通常/ 被/ 称为/ correspondence/ [/ 11/ ]/ ./ 基于/ 上述/ 讨论/ ,/ 本文/ 用/ 如下/ 的/ 三元组/ 描述/ 所/ 建立/ 的/ 博弈/ 模型/ :/ 上述/ 形式/ 称为/ 博弈/ 的/ 满足/ 式/ 表述/ ./ Perlaza/ 等/ 人/ [/ 11/ ]/ 在/ 研究/ 分布式/ 自/ 配置/ 网络/ 中/ 的/ QoS/ 保障/ 问题/ 时/ 最先/ 正式/ 定义/ 了/ 这/ 一/ 形式/ 的/ 博弈/ ./ 2.2/ ./ 3/ 满足/ 均衡/ 与/ 博弈/ 的/ 满足/ 式/ 表述/ 对应/ 的/ 均衡/ 称为/ 满足/ 均衡/ (/ SatisfactionEquilibrium/ ,/ SE/ )/ ./ SE/ 的/ 规范/ 定义/ 如下/ [/ 11/ ]/ ./ 定义/ 1/ ./ 满足/ 均衡/ ./ 给定/ 博弈/ 的/ 满足/ 式/ 表述/ G/ ^/ CF/ =/ (/ 足/ / i/ ∈/ / 足/ 均衡/ ./ 因/ 本文/ 假定/ 对/ 任意/ i/ ∈/ / 犪/ max/ / (/ S1/ ,/ S2/ ,/ …/ ,/ SN/ )/ 是/ G/ ^/ 个/ 用户/ 付出/ 最高/ 的/ 评分/ 成本/ ci/ (/ Si/ )/ ,/ 而/ 事实上/ 由于/ i/ ,/ 用户/ 可能/ 并不需要/ 用/ 如此/ 高/ 的/ 成本/ 去/ 换/ Γ/ i/ </ Γ/ max/ 取/ 满意/ 的/ 推荐/ 结果/ ./ 本文/ 主要/ 考虑/ 如何/ 找到/ 符合/ 如下/ 两个/ 条件/ 的/ 满足/ 均衡/ 犪/ +/ =/ (/ a/ +/ (/ 1/ )/ / i/ ∈/ / (/ 2/ )/ 至少/ 存在/ 一个/ 用户/ 不/ 需要/ 对/ 其/ 体验/ 过/ 的/ 所有/ 项目/ 都/ 给予/ 评分/ ,/ 即/ / i/ ∈/ / 3/ 均衡/ 学习/ 在上文/ 建立/ 的/ 博弈/ 模型/ G/ ^/ 动/ 空间/ 上/ 的/ 概率分布/ π/ i/ 、/ 对/ 推荐/ 质量/ 的/ 预期/ Γ/ i/ 、/ 兴趣/ 向量/ 狆/ i/ 、/ 评分/ 成本/ ci/ 都/ 是/ 其/ 私有/ 信息/ (/ privateinformation/ )/ ,/ 因而/ 直接/ 分析/ 该/ 博弈/ 的/ 均衡/ 是/ 很/ 困难/ 的/ ./ 一种/ 可行/ 的/ 方法/ 是/ 设计/ 某种/ 行为/ 规则/ ,/ 让/ 用户/ 在/ 迭代/ 交互/ 的/ 过程/ 中/ 依据/ 该/ 规则/ 不断/ 调整/ 自己/ 的/ 策略/ ,/ 最终/ 学出/ 一种/ 均衡/ 策略/ ./ 结合/ 推荐/ 服务器/ 的/ 工作/ 原理/ ,/ 本文/ 设计/ 了/ 一种/ 学习/ 算法/ ,/ 该/ 算法/ 允许/ 用户/ 以/ 逐渐/ 增加/ 评分/ 数量/ 的/ 方式/ 寻找/ 均衡/ 策略/ ./ 本节/ 首先/ 介绍/ 算法/ 的/ 基本/ 流程/ ,/ 然后/ 对/ 算法/ 的/ 收敛性/ 进行/ 分析/ ./ 3.1/ 均衡/ 学习/ 算法/ 在/ 推荐/ 系统/ 中/ ,/ 用户/ 与/ 推荐/ 服务器/ 的/ 交互/ 是/ 长期/ 的/ —/ —/ —/ 用户/ 不断/ 向/ 推荐/ 服务器/ 提供/ 评分/ 数据/ ,/ 推荐/ 服务器/ 则/ 不断/ 调整/ 反馈/ 给/ 用户/ 的/ 推荐/ ./ 本文/ 规定/ ,/ 每个/ 用户/ 在/ 与/ 推荐/ 服务器/ 迭代/ 交互/ 的/ 过程/ 中/ 按/ 如下/ 规则/ 行动/ :/ 初始/ 时刻/ (/ n/ =/ 0/ )/ ,/ 用户/ i/ 计算/ 其/ 在/ 行动/ 空间/ 上/ 的/ 概率分布/ π/ i/ (/ 0/ )/ =/ (/ π/ 选择/ 行动/ ai/ (/ 0/ )/ ./ 用户/ 选择/ 行动/ A/ (/ k/ )/ 的/ 概率/ π/ (/ k/ )/ i/ (/ 0/ )/ =/ π/ 其中/ ,/ 参数/ α/ (/ α/ >/ 1/ )/ 表示/ 用户/ 对/ 评分/ 成本/ 的/ 重视/ 程度/ ./ α/ 越小/ ,/ 用户/ 越有/ 可能/ 选择/ 较/ 多/ 的/ 项目/ 进行/ 评分/ ./ 归一化/ 因子/ β/ i/ (/ 0/ )/ 按/ 如下/ 方式/ 计算/ :/ 当/ 所有/ 用户/ 选择/ 完/ 初始/ 行动/ 后/ ,/ 推荐/ 服务器/ 根据/ 得到/ 的/ 评分/ 矩阵/ 犚/ (/ 0/ )/ 计算/ 推荐/ ,/ 然后/ 将/ 狉/ ^/ i/ (/ 0/ )/ 返回/ 给/ 用户/ i/ ./ 在此之后/ ,/ 用户/ 以/ 迭代/ 的/ 方式/ 与/ 推荐/ 服务器/ 交互/ ./ 在/ 第/ n/ 次/ 迭代/ 开始/ 时/ (/ n/ =/ 1/ ,/ 2/ ,/ …/ )/ ,/ 用户/ i/ 首先/ 判断/ 目前/ 的/ 推荐/ 狉/ ^/ i/ (/ n/ -/ 1/ )/ 是否/ 已达/ 预期/ ./ 定义/ 指示/ 变量/ vi/ (/ n/ -/ 1/ )/ 如下/ :/ vi/ (/ n/ -/ 1/ )/ =/ 用户/ 根据/ vi/ (/ n/ -/ 1/ )/ 的/ 取值/ 更新/ 概率分布/ π/ i/ (/ n/ )/ =/ (/ π/ 需要/ 特别/ 指出/ 的/ 是/ ,/ 推荐/ 服务器/ 会/ 利用/ 用户/ 以往/ 提/ (/ 1/ )/ i/ (/ n/ )/ ,/ π/ Page5/ 供/ 的/ 全部/ 评分/ 来/ 计算/ 推荐/ ,/ 所以/ 从/ 推荐/ 结果/ 来看/ ,/ 用户/ 在/ 第/ n/ 次/ 迭代/ 中/ 选择/ 若干个/ 新/ 项目/ (/ 记为/ 珟/ Snewi/ (/ n/ )/ )/ 进行/ 评分/ ,/ 其/ 效果/ 等价/ 于/ 用户/ 在/ 初始/ 时刻/ 对/ 集合/ 珟/ Snew/ 评分/ ./ 因此/ ,/ 本文/ 将/ ai/ (/ n/ )/ 定义/ 为/ 从/ 初始/ 时刻/ 至/ 第/ n/ 次/ 迭代/ 结束/ 时/ ,/ 用户/ i/ 已经/ 评/ 过分/ 的/ 所有/ 项目/ 的/ 集合/ ./ 易知/ ai/ (/ n/ )/ / ai/ (/ n/ -/ 1/ )/ ./ i/ (/ n/ )/ ∪/ …/ ∪/ 珟/ Snew/ 如果/ 当前/ 的/ 推荐/ 质量/ 未达/ 预期/ ,/ 即/ vi/ (/ n/ -/ 1/ )/ =/ 0/ ,/ 用户/ i/ 可能/ 将/ 这/ 一/ 结果/ 归结为/ 以下/ 两种/ 原因/ 之一/ :/ 自己/ 提供/ 的/ 评分/ 太/ 少/ ;/ 自己/ 已/ 提供/ 足够/ 多/ 的/ 评分/ ,/ 但/ 其他/ 用户/ 提供/ 的/ 评分/ 太少/ ./ 若/ 是/ 前者/ ,/ 用户/ 会/ 对/ 更/ 多/ 的/ 项目/ 进行/ 评分/ ;/ 若/ 是/ 后者/ ,/ 用户/ 倾向/ 于/ 不/ 选择/ 新/ 的/ 项目/ 评分/ ./ 行动/ 选择/ 概率/ π/ 按/ 如下/ 方式/ 计算/ :/ (/ k/ )/ i/ (/ n/ )/ =/ π/ σ/ i/ (/ n/ -/ 1/ )/ ,/ 烄/ β/ i/ (/ n/ )/ // α/ ci/ 烅/ 0/ ,/ 烆/ 其中/ ,/ σ/ i/ (/ n/ -/ 1/ )/ 表示/ 用户/ i/ 当前/ 的/ 评分/ 完整/ 度/ :/ σ/ i/ (/ n/ -/ 1/ )/ 越高/ ,/ 表明/ 用户/ i/ 已/ 提供/ 的/ 评分/ 越/ 多/ ,/ 则/ 用户/ 继续/ 提供/ 评分/ 的/ 概率/ 越低/ ./ 归一化/ 因子/ β/ i/ (/ n/ )/ 定义/ 如下/ :/ π/ π/ 如果/ 当前/ 的/ 推荐/ 质量/ 已达/ 预期/ ,/ 即/ vi/ (/ n/ -/ 1/ )/ =/ 1/ ,/ 则/ 用户/ 很/ 有/ 可能/ 不再/ 提供/ 更/ 多/ 的/ 评分/ ./ 此时/ ,/ 概率/ (/ k/ )/ i/ (/ n/ )/ 按/ 如下/ 方式/ 确定/ :/ (/ k/ )/ i/ (/ n/ )/ =/ μ/ ,/ 烄/ β/ i/ (/ n/ )/ // α/ ci/ 烅/ 0/ ,/ 烆/ 其中/ ,/ 参数/ μ/ 表示/ 用户/ 维持/ 原有/ 行动/ 的/ 概率/ ,/ 通常/ 有/ 0.5/ </ μ/ / 1/ ./ 归一化/ 因子/ β/ i/ (/ n/ )/ 定义/ 为/ 当/ 所有/ 用户/ 选择/ 了/ 行动/ 后/ ,/ 推荐/ 服务器/ 根据/ 得到/ 的/ 评分/ 矩阵/ 犚/ (/ n/ )/ 计算/ 推荐/ ,/ 然后/ 将/ 狉/ ^/ i/ (/ n/ )/ 返回/ 给/ 用户/ i/ ,/ 之后/ 进入/ 下/ 一轮/ 迭代/ ./ 若/ 经过/ ns/ (/ ns/ >/ 0/ )/ 次/ 迭代/ 之后/ ,/ 所有/ 用户/ 都/ 得到/ 了/ 满意/ 的/ 推荐/ ,/ 则/ 迭代/ 终止/ ,/ 称/ 学习/ 过程/ 收敛/ 于/ 满足/ 均衡/ 犪/ +/ =/ (/ a1/ (/ ns/ )/ ,/ a2/ (/ ns/ )/ ,/ …/ ,/ aN/ (/ ns/ )/ )/ ./ 我们/ 将/ 上述/ 行为/ 规则/ 归纳/ 为/ 算法/ 1/ ./ 算法/ 1/ ./ 满足/ 均衡/ 学习/ 算法/ ./ 1/ ./ n/ =/ 0/ :/ / k/ ∈/ {/ 1/ ,/ 2/ ,/ …/ ,/ K/ }/ ,/ π/ 其中/ ,/ β/ i/ (/ 0/ )/ =/ 2/ ./ 选择/ ai/ (/ 0/ )/ ~/ π/ i/ (/ 0/ )/ ;/ 3/ ./ FORALLn/ >/ 0DO/ :/ 4/ ./ 更新/ π/ i/ (/ n/ )/ :/ / k/ ∈/ {/ 1/ ,/ 2/ ,/ …/ ,/ K/ }/ ,/ (/ k/ )/ i/ (/ n/ )/ =/ π/ 其中/ ,/ γ/ i/ (/ n/ )/ =/ 5/ ./ 选择/ ai/ (/ n/ )/ ~/ π/ i/ (/ n/ )/ 6/ ./ ENDFOR3/ ./ 2/ 收敛性/ 分析/ 为了/ 分析/ 上文/ 所/ 提/ 均衡/ 学习/ 算法/ 的/ 收敛性/ ,/ 本/ 小节/ 首先/ 给出/ “/ 用户/ 状态/ ”/ 的/ 定义/ ,/ 然后/ 推导/ 算法/ 的/ 收敛/ 条件/ ./ 3.2/ ./ 1/ 用户/ 状态/ 如图/ 1/ 所示/ ,/ 给定/ 用户/ 预期/ Γ/ i/ ,/ 等式/ h/ (/ σ/ i/ ,/ σ/ -/ i/ ;/ 狆/ i/ )/ =/ Γ/ i/ 定义/ 了/ σ/ i/ ~/ σ/ -/ i/ 平面/ 上/ 的/ 一条/ 曲线/ 段/ ./ 由/ 假设/ 1/ 可知/ ,/ 只有/ 当/ σ/ i/ 和/ σ/ -/ i/ 分别/ 高于/ 某个/ 阈值/ 时/ ,/ 用户/ i/ 才/ 有/ 可能/ 得到/ 满足/ ./ 阈值/ σ/ i/ ,/ min/ 和/ σ/ -/ i/ ,/ min/ 分别/ 由/ h/ (/ σ/ i/ ,/ min/ ,/ 1/ ;/ 狆/ i/ )/ =/ Γ/ i/ 和/ h/ (/ 1/ ,/ σ/ -/ i/ ,/ min/ ;/ 狆/ i/ )/ =/ Γ/ i/ 确定/ ./ σ/ i/ ,/ min/ 和/ σ/ -/ i/ ,/ min/ 图/ 1/ 用户/ 状态/ 示意图/ (/ 网格/ 区域/ :/ “/ 满足/ ”/ ;/ 斜线/ 区域/ :/ Page6/ 分别/ 代表/ 了/ 对/ 用户/ i/ 自身/ 提供/ 评分/ 数量/ 和/ 对/ 其他/ 用户/ 提供/ 评分/ 数量/ 的/ 最低/ 要求/ ./ 在/ 学习/ 过程/ 中/ ,/ 每个/ 用户/ 的/ 评分/ 完整/ 度/ 随着/ 迭代/ 次数/ 的/ 增加/ 而/ 增加/ ,/ 即/ 有/ σ/ i/ (/ n/ )/ / σ/ i/ (/ n/ -/ 1/ )/ ./ 根据/ 式/ (/ 12/ )/ ,/ 定义/ σ/ -/ i/ (/ n/ -/ 1/ )/ 如下/ :/ σ/ -/ i/ (/ n/ -/ 1/ )/ =/ 易知/ σ/ -/ i/ (/ n/ )/ / σ/ -/ i/ (/ n/ -/ 1/ )/ ./ 假定/ 存在/ 某个/ n0/ (/ n0/ / 1/ )/ 使得/ σ/ i/ (/ n0/ )/ / σ/ i/ ,/ min/ 对/ 所有/ i/ 均/ 成立/ ,/ 则/ 从/ 第/ n0/ +/ 1/ 次/ 迭代/ 开始/ ,/ 每个/ 用户/ i/ 处于/ 以下/ 3/ 种/ 状态/ 之一/ :/ (/ 1/ )/ 满足/ (/ Satisfied/ )/ ./ 用户/ i/ 已/ 得到/ 满意/ 的/ 推荐/ ,/ 即/ h/ (/ σ/ i/ (/ n/ -/ 1/ )/ ,/ σ/ -/ i/ (/ n/ -/ 1/ )/ ;/ 狆/ i/ )/ / Γ/ i/ ./ 用户/ 进入/ 满足/ 状态/ 后/ 会/ 一直/ 保持/ 该/ 状态/ ,/ 因为/ 随着/ 迭代/ 次数/ 的/ 增加/ ,/ σ/ i/ 和/ σ/ -/ i/ 会/ 增加/ 或/ 保持/ 不变/ ,/ 且/ 由/ 假设/ 1/ 可知/ h/ (/ σ/ i/ ,/ σ/ -/ i/ ;/ 狆/ i/ )/ 亦/ 增加/ 或/ 保持/ 不变/ ./ (/ 2/ )/ 接近/ 满足/ (/ Proximitytosatisfied/ )/ ./ 用户/ i/ 尚未/ 得到/ 满意/ 的/ 推荐/ ,/ 即/ h/ (/ σ/ i/ (/ n/ -/ 1/ )/ ,/ σ/ -/ i/ (/ n/ -/ 1/ )/ ;/ 狆/ i/ )/ </ Γ/ i/ ,/ 且/ 用户/ i/ 尚未/ 对/ Si/ 中/ 的/ 所有/ 项目/ 都/ 进行/ 评分/ ,/ 即/ σ/ i/ (/ n/ -/ 1/ )/ </ 1/ ,/ 但/ 此时/ 其他/ 用户/ 提供/ 的/ 评分/ 数量/ 已/ 达到/ 用户/ i/ 的/ 最低/ 要求/ ,/ 即/ σ/ -/ i/ (/ n/ -/ 1/ )/ / σ/ -/ i/ ,/ min/ ./ 在/ 这种/ 情况/ 下/ ,/ 即使/ 其他/ 用户/ 不再/ 提供/ 更/ 多/ 的/ 评分/ ,/ 用户/ i/ 也/ 可以/ 通过/ 增加/ 自己/ 的/ σ/ i/ 进入/ 满足/ 状态/ ./ (/ 3/ )/ 远离/ 满足/ (/ Farfromsatisfied/ )/ ./ 用户/ i/ 尚未/ 得到/ 满意/ 的/ 推荐/ ,/ 并且/ 其他/ 用户/ 提供/ 的/ 评分/ 数量/ 未/ 达到/ 用户/ i/ 的/ 最低/ 要求/ ,/ 即/ σ/ -/ i/ (/ n/ -/ 1/ )/ </ σ/ -/ i/ ,/ min/ ./ 在/ 这种/ 情况/ 下/ ,/ 如果/ 其他/ 用户/ 在/ 后续/ 的/ 迭代/ 过程/ 中/ 能/ 提供/ 足够/ 多/ 的/ 评分/ ,/ 那么/ 用户/ i/ 可/ 进入/ 接近/ 满足/ 状态/ ,/ 否则/ 用户/ 将/ 永远/ 停留/ 在/ 远离/ 满足/ 状态/ ./ 分别/ 用/ ZS/ ,/ ZP/ ,/ ZF/ 表示/ 上述/ 3/ 种/ 状态/ ./ 用户/ i/ 在/ 第/ n/ (/ n/ / n0/ )/ 次/ 迭代/ 开始/ 时/ 的/ 状态/ 记为/ zi/ (/ n/ )/ ./ 3.2/ ./ 2/ 收敛/ 条件/ 在/ 第/ n/ (/ n/ / n0/ )/ 次/ 迭代/ 开始/ 时/ ,/ 可/ 根据/ 用户/ 状态/ 将/ 全体/ 用户/ 分为/ 两组/ :/ 已/ 满足/ 的/ 用户/ ,/ zi/ (/ n/ )/ =/ ZS/ }/ ,/ 未/ 满足/ 的/ 用户/ / zi/ (/ n/ )/ =/ ZP/ ∨/ zi/ (/ n/ )/ =/ ZF/ }/ ./ 随着/ 迭代/ 次数/ 的/ 增加/ ,/ 未/ 满足/ 的/ 用户数/ 逐渐/ 下降/ ./ 若/ 算法/ 在/ 第/ nS/ 次/ 迭代/ 开始/ 时/ 达到/ 满足/ 均衡/ ,/ 则/ 有/ |/ / US/ (/ nS/ )/ |/ =/ 0/ ./ 从/ 上/ 文对/ 用户/ 状态/ 的/ 定义/ 可知/ ,/ 为了/ 判断/ 能否/ 达到/ 所有/ 用户/ 都/ 处于/ 满足/ 状态/ 的/ 均衡/ ,/ 关键在于/ 分析/ 清楚/ 用户/ 评分/ 完整/ 度/ 在/ 迭代/ 过程/ 中/ 的/ 变化/ ./ 由/ 算法/ 1/ 可知/ ,/ 评分/ 完整/ 度/ 在/ 一次/ 迭代/ 中/ 的/ 增量/ Δ/ σ/ i/ (/ n/ )/ / σ/ i/ (/ n/ )/ -/ σ/ i/ (/ n/ -/ 1/ )/ 是/ 随机/ 的/ ,/ 因此/ 很难/ 定量分析/ 用户/ 状态/ 的/ 变化/ ./ 此处/ 对/ 算法/ 1/ 进行/ 简化/ ,/ 然后/ 推导/ 简化/ 后/ 算法/ 的/ 收敛/ 条件/ ./ (/ 1/ )/ 简化/ 的/ 均衡/ 学习/ 算法/ 初始/ 时刻/ ,/ 每个/ 用户/ i/ 从/ 集合/ Si/ 中/ 随机/ 选择/ 一个/ 项目/ 进行/ 评分/ ./ 因而/ 对/ 任一/ i/ ∈/ / 1/ ./ 在/ 第/ n/ 次/ 迭代/ 开始/ 时/ (/ n/ >/ 0/ )/ ,/ 若/ h/ (/ σ/ i/ (/ n/ -/ 1/ )/ ,/ |/ Si/ |/ σ/ -/ i/ (/ n/ -/ 1/ )/ ;/ 狆/ i/ )/ / Γ/ i/ ,/ 则/ 用户/ i/ 维持/ 原有/ 行动/ ,/ 即/ ai/ (/ n/ )/ =/ ai/ (/ n/ -/ 1/ )/ ;/ 若/ h/ (/ σ/ i/ (/ n/ -/ 1/ )/ ,/ σ/ -/ i/ (/ n/ -/ 1/ )/ ;/ 狆/ i/ )/ </ Γ/ i/ ,/ 则/ 用户/ i/ 从/ 集合/ Si/ \/ ai/ (/ n/ -/ 1/ )/ 中/ 随机/ 选择/ 一个/ 项目/ 进行/ 评分/ ./ 由/ 上述/ 行为/ 规则/ 可知/ :/ 若/ i/ ∈/ / S/ (/ n/ )/ ,/ 则/ 有/ Δ/ σ/ i/ (/ n/ )/ =/ 0/ ;/ 若/ i/ ∈/ / US/ (/ n/ )/ ,/ 则/ 有/ Δ/ σ/ i/ (/ n/ )/ =/ (/ 2/ )/ 两类/ 用户/ 为了/ 推导/ 均衡/ 学习/ 算法/ 收敛/ 条件/ 的/ 解析/ 表达式/ ,/ 除了/ 对/ 算法/ 本身/ 进行/ 简化/ ,/ 本文/ 亦/ 做出/ 如下/ 假设/ ./ 假设/ 2/ ./ 全体/ 用户/ 可/ 分为/ 两组/ ,/ 分别/ 记为/ 和/ / B/ ./ ①/ / i/ ∈/ / A/ ,/ Γ/ i/ =/ η/ A/ Γ/ max/ ②/ / i/ ∈/ / B/ ,/ Γ/ i/ =/ η/ B/ Γ/ max/ ③/ / i/ ∈/ / 1/ / M0/ </ |/ S/ |/ ;/ ④/ / i/ ∈/ / 等式/ :/ 其中/ ,/ 1M0/ 根据上述/ 假设/ ,/ σ/ i/ ,/ min/ 和/ σ/ -/ i/ ,/ min/ 可/ 按/ 如下/ 方式/ 计算/ :/ 若/ 0/ </ η/ i/ / σ/ -/ i/ ,/ min/ =/ 则/ σ/ i/ ,/ min/ =/ σ/ -/ i/ ,/ min/ =/ 2/ η/ Page7/ (/ 3/ )/ 评分/ 完整/ 度/ 的/ 变化/ 基于/ 上述/ 简化/ 和/ 假设/ ,/ 我们/ 可/ 定量分析/ 算法/ 的/ 收敛/ 条件/ ./ 考虑/ 着/ 迭代/ 的/ 进行/ ,/ 用户/ 在/ 正方形/ 区域/ [/ 0/ ,/ 1/ ]/ 2/ 中/ 从/ 左下角/ 向/ 右上方/ 移动/ ./ 由/ 简化/ 的/ 均衡/ 学习/ 算法/ 可知/ ,/ 第/ 1/ 次/ 迭代/ 开始/ 时/ ,/ 用户/ i/ 位于/ 点/ 1/ 入/ 满足/ 状态/ 之前/ ,/ 如下/ 两个/ 等式/ 对/ 所有/ 用户/ 均/ 成立/ :/ Δ/ σ/ -/ i/ (/ n/ )/ =/ σ/ -/ i/ (/ n/ )/ -/ σ/ -/ i/ (/ n/ -/ 1/ )/ =/ / A/ 中/ 的/ 用户/ 对/ 推荐/ 结果/ 有/ 相对/ 较/ 低/ 的/ 预期/ ,/ 因此/ 这些/ 用户/ 先于/ 某个/ nA/ ∈/ {/ 1/ ,/ 2/ ,/ …/ ,/ M0/ -/ 1/ }/ 使得/ / US/ (/ nA/ )/ =/ / B/ ./ 在/ 第/ nA/ 次/ 迭代/ 开始/ 前/ ,/ 用户/ 始终/ 沿着/ 直线/ σ/ -/ i/ =/ σ/ i/ 移动/ ./ 第/ nA/ 次/ 迭代/ 开始/ 时/ ,/ 用户/ i/ 位于/ 点/ nA/ 若/ zi/ (/ nA/ )/ =/ ZP/ ,/ 则/ 用户/ i/ 可/ 在/ 若干次/ 迭代/ 后/ 进入/ ZS/ 状态/ ./ 下面/ 重点/ 分析/ 另/ 一种/ 情况/ ,/ 即/ zi/ (/ nA/ )/ =/ ZF/ ./ 在/ 第/ nA/ 次/ 迭代/ 中/ ,/ 进行/ 评分/ ,/ 而/ 在/ σ/ i/ ~/ σ/ -/ i/ 平面/ 上/ 沿着/ 斜率/ 为/ kB/ (/ kB/ </ 1/ )/ 的/ 直线/ 移动/ :/ kB/ =/ 之后/ ,/ 用户/ i/ 继续/ 沿着/ 该/ 方向/ 移动/ ,/ 直到/ 下述/ 两种/ 情况/ 之一/ 发生/ :/ 用户/ i/ 进入/ 满足/ 状态/ ;/ 用户/ i/ 尚未/ 满足/ 但/ 已/ 对/ 集合/ Si/ 中/ 的/ 所有/ 项目/ 进行/ 了/ 评分/ ,/ 即/ σ/ i/ =/ 1/ ./ 当/ 第/ 2/ 种/ 情况/ 出现/ 时/ ,/ 用户/ i/ 将/ 永远/ 不能/ 满足/ ./ 其/ 原因/ 是期/ ,/ 这/ 意味着/ 此时/ 除/ 用户/ i/ 外/ 提供/ 了/ 完整/ 的/ 评分/ ,/ σ/ -/ i/ 无法/ 继续/ 增加/ ./ 如图/ 2/ 所示/ ,/ 当/ kB/ 小于/ 某个/ 阈值/ kmin/ 时/ ,/ 上述/ 第/ 2/ 种/ 情况/ 就/ 会/ 出现/ ./ kmin/ 定义/ 如下/ :/ kmin/ =/ 将式/ (/ 20/ )/ 和/ (/ 21/ )/ 代入/ kB/ </ kmin/ 可得/ σ/ -/ i/ ,/ min/ >/ 式/ (/ 22/ )/ 的/ 右半/ 部分/ 实际上/ 就是/ 如下/ 情况/ 对应/ 的/ σ/ -/ i/ :/ / B/ 中/ 的/ 所有/ 用户/ 都/ 提供/ 了/ 完整/ 的/ 评分/ ,/ 而/ 用户/ 只/ 提供/ 了/ 能/ 使/ 自己/ 得到/ 满足/ 的/ 必要/ 数量/ 的/ 评分/ ./ (/ 4/ )/ 用户/ 预期/ 与/ 收敛性/ 式/ (/ 22/ )/ 说明/ 如果/ 一个/ 用户/ 对/ 推荐/ 结果/ 有/ 很/ 高/ 的/ 预期/ ,/ 即/ 需要/ 其他/ 用户/ 付出/ 较/ 高/ 的/ 评分/ 成本/ ,/ 那么/ 该/ 用户/ 就/ 可能/ 无法/ 得到/ 满意/ 的/ 推荐/ ./ 由/ 假设/ 2/ 可知/ ,/ 当/ 用户/ 的/ 预期/ 不/ 同时/ ,/ σ/ -/ i/ ,/ min/ 的/ 定义/ 方式/ 不同/ :/ ①/ 如果/ η/ A/ </ η/ B/ / σ/ -/ i/ ,/ min/ =/ 因/ M0/ / 1/ 且/ |/ / B/ |/ / 1/ ,/ 上述/ 不等式/ 不/ 成立/ ./ 也就是说/ ,/ 当/ η/ A/ </ η/ B/ / 法/ 必然/ 收敛/ ./ ②/ 如果/ 1/ σ/ -/ i/ ,/ min/ =/ 2/ η/ zi/ (/ nA/ )/ =/ ZS/ 可知/ 式/ (/ 24/ )/ 成立/ ,/ 则/ 算法/ 不能/ 收敛/ :/ 基于/ 上述/ 讨论/ ,/ 本文/ 给出/ 如下/ 定理/ ./ 定理/ 1/ ./ 在/ 假设/ 2/ 成立/ 的/ 前提/ 下/ ,/ 若/ 下述/ 两个/ 条件/ 之一/ 成立/ ,/ 则/ 简化/ 后/ 的/ 均衡/ 学习/ 算法/ 可以/ 收敛/ Page8/ 到/ 博弈/ G/ ^/ ①/ η/ A/ </ η/ B/ / ②/ (/ 5/ )/ 收敛/ 阈值/ 为了/ 更好/ 的/ 理解/ 用户/ 预期/ 对/ 算法/ 收敛性/ 的/ 影响/ ,/ 本文/ 给出/ 如下/ 分析/ :/ ①/ 给定/ ρ/ N/ / / B/ 式/ 成立/ ,/ 则/ 简化/ 后/ 的/ 均衡/ 学习/ 算法/ 无法/ 收敛/ :/ η/ B/ >/ 用/ θ/ B/ 表示/ 上述/ 不等式/ 的/ 右半/ 部分/ ./ 如图/ 4/ (/ a/ )/ 所示/ ,/ 在/ 给定/ ρ/ N/ 的/ 增长/ 速率/ 相对/ 较/ 低/ ./ 这/ 表明/ 当/ 用户/ 对/ 推荐/ 结果/ 的/ 整体/ 预期/ 变高时/ (/ η/ A/ 差异/ 不/ 显著/ ,/ 仍会/ 有/ 部分/ 用户/ 无法/ 得到/ 满足/ ./ 从图/ 4/ (/ a/ )/ 还/ 可以/ 看出/ ,/ 给定/ η/ A/ 这/ 意味着/ 当有/ 越来越/ 多/ 的/ 用户/ 对/ 推荐/ 结果/ 抱/ 有/ 较/ 高图/ 4/ 均衡/ 学习/ 算法/ 的/ 收敛/ 条件/ (/ (/ a/ )/ 给定/ ρ/ N/ 和/ η/ A/ ,/ 若/ η/ B/ >/ θ/ B/ ,/ 则/ 简化/ 后/ 的/ 学习/ 算法/ 无法/ 收敛/ ./ 绘制/ 曲线/ 时/ ,/ 设/ 0.3/ / η/ A/ / 0.9/ ,/ N/ =/ 10000/ ;/ (/ b/ )/ 给定/ ρ/ η/ ,/ 若/ η/ B/ >/ η/ B/ ,/ min/ 且/ ρ/ N/ </ θ/ N/ ,/ 则/ 简化/ 后/ 的/ 学习/ 算法/ 无法/ 收敛/ ./ 绘制/ 曲线/ 时/ ,/ 设/ 14/ 仿真/ 分析/ 为了/ 检验所/ 提/ 均衡/ 学习/ 算法/ 的/ 可行性/ ,/ 本文/ 用/ 真实/ 的/ 评分/ 数据/ 进行/ 了/ 一系列/ 仿真/ 实验/ ./ 本节/ 首先/ 对/ 实验/ 所用/ 的/ 数据/ 集/ 以及/ 仿真/ 参数/ 的/ 设置/ 进行/ 说明/ ,/ 然后/ 对/ 不同/ 参数设置/ 下/ 的/ 仿真/ 结果/ 进行/ 分析/ ,/ 最后/ 对/ 3.2/ 节/ 给出/ 的/ 算法/ 收敛/ 条件/ 进行/ 验证/ ./ 预期/ 时/ ,/ 用户/ 可/ 期望/ 得到/ 更/ 高质量/ 的/ 推荐/ ./ ②/ 给定/ ρ/ η/ / 且/ 下述/ 不等式/ 成立/ ,/ 则/ 简化/ 的/ 均衡/ 学习/ 算法/ 无法/ 收敛/ :/ 用/ θ/ N/ 表示/ 上述/ 不等式/ 的/ 右半/ 部分/ ./ 该/ 不等式/ 暗含/ 着/ θ/ N/ >/ 0/ 这一/ 条件/ ./ 由/ θ/ N/ >/ 0/ 可得/ η/ B/ >/ -/ 2N/ ρ/ η/ +/ ,/ θ/ N/ 随着/ η/ B/ 用/ η/ B/ ,/ min/ 定/ ρ/ η/ 户/ 的/ 预期/ 变得/ 更高时/ ,/ 只有/ 当所/ 增加/ 时/ ,/ 才/ 有/ 可能/ 实现/ 满足/ 均衡/ ./ 另外/ ,/ 给定/ η/ B/ θ/ N/ 随着/ ρ/ η/ 上/ 的/ 差异/ 变大时/ ,/ 可以/ 有/ 更/ 多/ 的/ 用户/ 预期/ 得到/ 高质量/ 的/ 推荐/ ./ 上述/ 结论/ 与/ 从/ 图/ 4/ (/ a/ )/ 中/ 得到/ 的/ 结论/ 是/ 一致/ 的/ ./ 4.1/ 数据/ 集/ 和/ 参数设置/ 本文/ 分别/ 用/ 来自/ Jester/ 数据/ 集/ [/ 15/ ]/ 和/ MovieLens/ 数据/ 集/ ①/ 的/ 评分/ 数据/ 进行/ 了/ 仿真/ ./ 4.1/ ./ 1Jester/ 本文/ 使用/ 的/ Jester/ 数据/ 集/ 包含/ 了/ 24983/ 个/ 用户/ ①/ http/ :/ // // files/ ./ grouplens/ ./ org/ // datasets/ // movielens/ // ml/ -/ 1m/ ./ zipPage9/ 对/ 100/ 个/ 笑话/ 的/ 评分/ ./ 用户/ 对/ 笑话/ 的/ 评分/ 在/ 区间/ [/ -/ 10/ ,/ 10/ ]/ 上/ 取值/ ./ 若/ 用户/ 未/ 给出/ 某个/ 笑话/ 的/ 评分/ ,/ 则/ 用/ “/ 99/ ”/ 表示/ 这一/ 未知/ 的/ 评分/ ./ 在/ 所有/ 用户/ 中/ ,/ 有/ 7200/ 个/ 用户/ 对/ 全部/ 的/ 100/ 个/ 笑话/ 都/ 给出/ 了/ 评分/ ./ 考虑/ 到/ 在/ 均衡/ 学习/ 过程/ 中需/ 根据/ 用户/ 对/ 所有/ 项目/ 的/ “/ 潜在/ ”/ 评分/ 来/ 评估/ 推荐/ 质量/ (/ 参见/ 式/ (/ 3/ )/ )/ ,/ 本文/ 仅/ 选择/ 这/ 7200/ 个/ 用户/ 的/ 评分/ 作为/ 实验/ 用/ 数据/ ./ 我们/ 将/ 原始/ 评分/ 数据/ 转换/ 为/ 评分/ 矩阵/ 犚/ =/ [/ rij/ ]/ 7200/ ×/ 100/ ,/ 并/ 将/ rij/ 调整/ 至/ 区间/ [/ 10.0/ ,/ 30.0/ ]/ (/ 以/ rij/ =/ 0/ 表示/ 未知/ 的/ 评分/ )/ ./ 均衡/ 学习/ 算法/ 涉及/ 的/ 参数/ 按/ 如下/ 方式/ 设置/ :/ 矩阵/ 犚/ 的/ 每/ 一行/ 被/ 视为/ 对应/ 用户/ 的/ 兴趣/ 向量/ 狆/ i/ ;/ 假定/ 每个/ 用户/ 体验/ 过/ 的/ 项目/ 总数/ 均/ 为/ 70/ ,/ 即/ |/ Si/ |/ =/ 70/ ./ 为/ 确定/ 每个/ 用户/ 的/ Si/ ,/ 从/ 矩阵/ 犚/ 的/ 每/ 一行/ 中/ 随机/ 选择/ 30/ 个/ 元素/ 置/ 为/ 零/ ./ 处理/ 后/ 的/ 评分/ 矩阵/ 记为/ 犚/ ′/ ;/ 按式/ (/ 2/ )/ 预测/ 未知/ 评分/ ,/ 设/ |/ Neighbour/ (/ i/ )/ |/ =/ 按式/ (/ 3/ )/ 计算/ 推荐/ 质量/ gi/ (/ 狉/ ^/ i/ )/ ;/ 以/ 评分/ 的/ 数量/ 作为/ 评分/ 成本/ ,/ 即/ ci/ (/ ai/ )/ =/ |/ ai/ |/ ;/ 由式/ (/ 11/ )/ 和/ (/ 14/ )/ 可知/ ,/ 算法/ 的/ 收敛/ 速度/ 与/ 参数/ α/ 的/ 取值/ 密切相关/ ./ 考虑/ 到/ 函数/ f/ (/ x/ )/ =/ 1/ // α/ x/ 在/ 区间/ [/ 0/ ,/ 70/ ]/ 上/ 的/ 变化趋势/ ,/ 仿真/ 时设/ α/ =/ 1.2/ ;/ 分别/ 设/ μ/ =/ 0.9/ 和/ μ/ =/ 1/ 以/ 模拟/ 如下/ 两种/ 情况/ :/ 已/ 满足/ 的/ 用户/ 继续/ 提供/ 评分/ ,/ 已/ 满足/ 的/ 用户/ 不再/ 提供/ 评分/ ./ 为/ 设置/ Γ/ i/ ,/ 首先/ 利用/ 犚/ 和/ 犚/ ′/ 计算/ 每个/ 用户/ 所/ 能/ 获得/ 的/ 最优/ 推荐/ 质量/ Γ/ max/ (/ 0/ </ η/ i/ </ 1/ )/ ./ η/ i4/ ./ 1.2/ MovieLens/ 本文/ 采用/ 的/ MovieLens/ 数据/ 集/ 包含/ 了/ 6040/ 个/ 用户/ 对/ 3900/ 部/ 电影/ 的/ 评分/ ./ 与/ 上/ 文对/ Jester/ 数据/ 集/ 的/ 处理/ 类似/ ,/ 此处/ 亦/ 假定/ 每个/ 用户/ 体验/ 过/ 的/ 项目/ 的/ 总数/ 为/ 70/ ,/ 因而/ 我们/ 滤除/ 那些/ 评分/ 数量/ 低于/ 70/ 的/ 用户/ ,/ 并/ 滤除/ 那些/ 未/ 获得/ 评分/ 的/ 项目/ ,/ 最终/ 保留/ 3631/ 个/ 用户/ 对/ 3675/ 个/ 项目/ 的/ 评分/ ,/ 对应/ 的/ 评分/ 矩阵/ 为/ 犚/ =/ [/ rij/ ]/ 3631/ ×/ 3675/ ,/ 其中/ rij/ ∈/ {/ 0/ ,/ 1/ ,/ …/ ,/ 5/ }/ ,/ rij/ =/ 0/ 表示/ 评分/ 未知/ ./ 与/ Jester/ 数据/ 不同/ ,/ 这一/ 评分/ 矩阵/ 是/ 非常/ 稀疏/ 的/ ,/ 矩阵/ 中非/ 零/ 元素/ 的/ 比例/ 仅为/ 6.78/ %/ ./ 均衡/ 学习/ 算法/ 的/ 参数/ 按/ 如下/ 方式/ 设置/ :/ 矩阵/ 犚/ 的/ 每/ 一行/ 被/ 视为/ 对应/ 用户/ 的/ 兴趣/ 向量/ 狆/ i/ ;/ 从/ 犚/ 的/ 每/ 一行/ 中/ 随机/ 选择/ 70/ 个/ 非/ 零/ 元素/ 构成/ 对应/ 用户/ 的/ 完整/ 评分/ 集合/ ,/ 即/ 用户/ 只能/ 对/ 这/ 70/ 个/ 元素/ 对应/ 的/ 项目/ 进行/ 评分/ ;/ 按式/ (/ 2/ )/ 预测/ 未知/ 评分/ ,/ 设/ |/ Neighbour/ (/ i/ )/ |/ =/ 20/ ;/ 按式/ (/ 3/ )/ 计算/ 推荐/ 质量/ ;/ 参数/ α/ ,/ μ/ 和/ Γ/ i/ 的/ 设置/ 方法/ 与/ 前文/ 所述/ 相同/ ./ 4.2/ 均衡/ 学习/ 仿真/ 结果/ 为/ 检验/ 算法/ 1/ 的/ 可行性/ ,/ 本文/ 测试/ 了/ 多组/ {/ η/ i/ 给定/ 评分/ 矩阵/ 犚/ 和/ 参数/ μ/ ,/ 分别/ 在/ 如下/ 4/ 种/ 设置/ 下/ 运行/ 学习/ 算法/ :/ (/ 1/ )/ / i/ ∈/ / (/ 2/ )/ / i/ ∈/ / (/ 3/ )/ 随机/ 选择/ 1/ %/ 的/ 用户/ ,/ 令其/ 对应/ 的/ η/ i/ =/ 0.85/ ,/ 其余/ 用户/ 的/ η/ i/ =/ 0.5/ ;/ (/ 4/ )/ 随机/ 选择/ 20/ %/ 的/ 用户/ ,/ 令其/ 对应/ 的/ η/ i/ =/ 0.85/ ,/ 其余/ 用户/ 的/ η/ i/ =/ 0.5/ ;/ 为/ 降低/ 随机性/ 的/ 影响/ ,/ 给定/ 一组/ 参数/ ,/ 重复/ 运行/ 算法/ 5/ 次/ ./ 在/ 每/ 一次/ 运行/ 中/ ,/ 当/ 所有/ 用户/ 都/ 已/ 满足/ 或/ 迭代/ 次数/ 达到/ 10000/ 次时/ ,/ 算法/ 终止/ ./ 每次/ 运行/ 后/ ,/ 记录/ 算法/ 终止/ 时/ 的/ 迭代/ 次数/ nstop/ 、/ 已/ 满足/ 的/ 用户数/ NS/ 和/ 用户/ 评分/ 完整/ 度/ 的/ 平均值/ σ/ -/ |/ Si/ |/ ./ 表/ 1/ 和表/ 2/ 分别/ 给出/ 了/ Jester/ 数据/ 集/ 和/ Movie/ -/ Lens/ 数据/ 集上/ 的/ 仿真/ 结果/ ./ 可以/ 看出/ :/ 当/ 所有/ 用户/ 对/ 推荐/ 质量/ 有着/ 相似/ 的/ 预期/ 时/ ,/ 即使/ 预期/ 很/ 高/ (/ η/ i/ =/ 0.85/ )/ 并且/ 用户/ 在/ 满足/ 之后/ 不再/ 参与/ 评分/ ,/ 满足/ 均衡/ 也/ 是/ 可以/ 实现/ 的/ ./ 给定/ μ/ 的/ 取值/ ,/ 随着/ 用户/ 预期/ 的/ 升高/ ,/ 算法/ 的/ 收敛/ 时间/ 变/ 长/ ,/ σ/ -/ 的/ 项目/ 进行/ 评分/ 才能/ 得到/ 满意/ 的/ 推荐/ ./ 给定/ 一组/ {/ η/ i/ }/ Ni/ =/ 1/ ,/ 比较/ 不同/ μ/ 对应/ 的/ 仿真/ 结果/ 可以/ 看到/ ,/ 相比/ 于/ μ/ =/ 0.9/ 的/ 情况/ ,/ 当/ μ/ =/ 1/ ,/ 算法/ 的/ 收敛/ 时间/ 变/ 长/ ,/ 但/ 用户/ 评分/ 完整/ 度/ 下降/ ./ 收敛/ 时间/ 变长/ 的/ 原因/ 在于/ 当/ μ/ =/ 0.9/ 时/ ,/ 已/ 满足/ 的/ 用户/ 会/ 继续/ 为/ 推荐/ 质量/ 的/ 提升/ 做出/ 贡献/ ,/ 因此/ 其他/ 那些/ 间/ 内/ 得到/ 满意/ 的/ 推荐/ ./ 而/ 当/ μ/ =/ 1/ 时/ ,/ 未/ 满足/ 的/ 用户/ 只能/ 依靠/ 他们/ 自己/ 提升/ 推荐/ 质量/ ,/ 因此/ 需要/ 更/ 多/ 的/ 时间/ 才能/ 达到/ 满足/ 均衡/ ./ 从/ 评分/ 完整/ 度/ 来看/ ,/ μ/ =/ 0.9/ 意味着/ 在/ 达到/ 满足/ 均衡/ 时/ ,/ 用户/ 提供/ 的/ 评分/ 数量/ 可能/ 远高于/ 与其/ 预期/ 相对/ 应/ 的/ 必要/ 的/ 评分/ 数量/ ./ 而/ 当/ μ/ =/ 1/ 时/ ,/ 用户/ 倾向/ 于/ 只/ 提供/ 能/ 满足/ 其/ 个人/ 预期/ 的/ 最少/ 的/ 评分/ ,/ 因此/ 当/ 算法/ 收敛/ 时/ ,/ 用户/ 评分/ 的/ 完整/ 度/ 较/ 低/ ./ 当/ 大多数/ 用户/ 对/ 推荐/ 结果/ 有着/ 适中/ 的/ 预期/ (/ η/ i/ =/ 0.5/ )/ 而/ 小/ 部分/ 用户/ 的/ 预期/ 很/ 高时/ (/ η/ i/ =/ 0.85/ )/ ,/ 若/ μ/ =/ 0.9/ ,/ 满足/ 均衡/ 仍/ 是/ 可以/ 实现/ 的/ ,/ 只不过/ 此时/ 算法/ 的/ 收敛/ 速度/ 要/ 慢于/ 所有/ 用户/ 预期/ 都/ 不/ 高/ 的/ 情况/ ,/ 并且/ 在/ 达到/ 均衡/ 时/ 用户/ 评分/ 的/ 完整/ 度/ 接近/ 于/ 所有/ 用户/ 都/ 持有/ 高/ 预期/ 的/ 情况/ ./ 这一/ 结果表明/ ,/ 为了/ 满足/ 少部分/ 用户/ 的/ 需求/ ,/ 那些/ 预期/ 不高/ 的/ 用户/ 需要/ 在/ 得到/ 满意/ 的/ 推荐/ 之后/ 继续/ 提供/ 很多/ 评分/ ./ 当/ μ/ =/ 1/ 时/ ,/ 满足/ 的/ 用户/ 不再/ 提供/ 评分/ ./ 因而/ 当/ 大多数/ 用户/ 满足/ 之后/ ,/ 余/ Page10/ 表/ 1Jester/ 数据/ 集上/ 的/ 均衡/ 学习/ 仿真/ 结果/ 表/ 2MovieLens/ 数据/ 集上/ 的/ 均衡/ 学习/ 仿真/ 结果/ η/ i/ =/ 0.5/ η/ i/ =/ 0.851/ %/ :/ η/ i/ =/ 0.85/ ,/ 99/ %/ :/ η/ i/ =/ 0.520/ %/ :/ η/ i/ =/ 0.85/ ,/ 80/ %/ :/ η/ i/ =/ 0.5/ η/ i/ =/ 0.5/ η/ i/ =/ 0.851/ %/ :/ η/ i/ =/ 0.85/ ,/ 99/ %/ :/ η/ i/ =/ 0.520/ %/ :/ η/ i/ =/ 0.85/ ,/ 80/ %/ :/ η/ i/ =/ 0.5/ 下/ 那些/ 未/ 满足/ 的/ 用户/ 很难/ 再/ 使/ 推荐/ 质量/ 有/ 显著/ 提升/ ,/ 算法/ 无法/ 在/ 10000/ 次/ 迭代/ 内/ 收敛/ ./ 观察/ σ/ -/ 可以/ 看出/ ,/ 大多数/ 用户/ 只是/ 提供/ 了/ 能够/ 满足/ 其中/ 等/ 预期/ 的/ 评分/ ,/ 但/ 这/ 一/ 数量/ 的/ 评分/ 不足以/ 产生/ 很/ 高质量/ 的/ 推荐/ ./ 为/ 更/ 清楚/ 地/ 观察/ 用户/ 预期/ 对/ 均衡/ 学习/ 结果/ 的/ 影响/ ,/ 我们/ 将/ 不同/ 参数设置/ 对应/ 的/ |/ / S/ (/ n/ )/ |/ 变化/ 曲线/ 绘制/ 于图/ 5/ 中/ (/ 以/ Jester/ 数据/ 集为例/ )/ ./ 可以/ 看到/ ,/ 在/ 第/ 4/ 种/ 设置/ 下/ (/ 图/ 5/ 中/ 圆圈/ 标记/ 的/ 曲线/ )/ ,/ 经过/ 约/ 15/ 次/ 迭代/ ,/ 80/ %/ 的/ 用户/ 就/ 已/ 满足/ ,/ 这/ 与/ 第/ 1/ 种/ 设置/ 的/ 仿图/ 5/ 均衡/ 学习/ 过程/ 中/ “/ 满足/ ”/ 用户/ 的/ 数量/ 的/ 变化/ 52672000.543120372000/ ./ 91440372000.88190372000/ ./ 91057336310.92156036310/ ./ 99115736310.96467836310/ ./ 993/ 真/ 结果/ (/ 图/ 5/ 中用/ 圆点/ 标记/ 的/ 曲线/ )/ 类似/ ./ 而/ 在此之后/ ,/ |/ / S/ (/ n/ )/ |/ 的/ 增长/ 明显/ 变缓/ ,/ 经过/ 很/ 多次/ 迭代/ 才/ 达到/ 满足/ 均衡/ ./ 由/ 上述/ 仿真/ 结果/ 可以/ 得到/ 关于/ 协同/ 过滤/ 系统/ 中/ 满足/ 均衡/ 的/ 直观/ 理解/ :/ 若/ 所有/ 用户/ 对/ 推荐/ 质量/ 的/ 预期/ 都/ 不/ 高/ ,/ 则/ 低成本/ 的/ 满足/ 均衡/ 是/ 可以/ 实现/ 的/ ,/ 即/ 每个/ 用户/ 只/ 需/ 提供/ 较少/ 的/ 评分/ 就/ 能/ 得到/ 满意/ 的/ 推荐/ ./ 稍后/ 将/ 通过/ 另一组/ 仿真/ 对/ 3.2/ 节/ 提出/ 的/ 收敛/ 条件/ 进行/ 验证/ ./ 4.3/ 奖励/ 机制/ 从/ 上/ 一/ 小节/ 给出/ 的/ 仿真/ 结果/ 可以/ 看出/ ,/ 当/ 不同/ 用户/ 对/ 推荐/ 质量/ 有着/ 相似/ 的/ 预期/ 时/ ,/ 仅靠/ 推荐/ 质量/ 这一/ 内部/ 激励/ 就/ 可以/ 让/ 用户/ 自发/ 地/ 提供/ 足够/ 多/ 的/ 评分/ 数据/ 以/ 使/ 推荐/ 服务器/ 产生/ 符合/ 所有/ 用户/ 预期/ 的/ 推荐/ ./ 而/ 当/ 不同/ 用户/ 对/ 推荐/ 质量/ 的/ 预期/ 相差/ 较大/ 时且/ 用户/ 在/ 满足/ 之后/ 不再/ 对/ 更/ 多/ 的/ 项目/ 评分/ 时/ ,/ 均衡/ 学习/ 算法/ 无法/ 收敛/ 到/ 满足/ 均衡/ ,/ 这/ 意味着/ 此时/ 推荐/ 服务器/ 有/ 必要/ 提供/ 一些/ 外部/ 激励/ 以/ 促使/ 用户/ 参与/ 评分/ ./ 假定/ 当/ 一个/ 用户/ 选择/ 行动/ A/ (/ k/ )/ 的/ 成本/ 为/ ci/ (/ A/ (/ k/ )/ )/ / |/ A/ (/ k/ )/ 奖励/ 为/ b/ (/ A/ (/ k/ )/ )/ / κ/ |/ A/ (/ k/ )/ 个/ 项目/ 评分/ 后/ 所能/ 获得/ 的/ 奖励/ (/ 0/ </ κ/ </ 1/ )/ ./ 由/ 算法/ 1/ 可知/ ,/ 用户/ 在/ 选择/ 行动/ 时/ 倾向/ 于/ 选择/ 低成本/ 的/ 行/ Page11/ 动/ ./ 推荐/ 服务器/ 向/ 用户/ 给予/ 奖励/ ,/ 这/ 相当于/ 用户/ 的/ 评分/ 成本/ 由/ 原来/ 的/ ci/ (/ A/ (/ k/ )/ )/ 降低/ 为/ ci/ (/ A/ (/ k/ )/ )/ -/ b/ (/ A/ (/ k/ )/ )/ ,/ 因而/ 用户/ 选择/ 较/ 多/ 项目/ 进行/ 评分/ 的/ 概率/ 变高/ ./ 另一方面/ ,/ 当/ 用户/ 的/ 预期/ 得到/ 满足/ 之后/ ,/ 虽然/ 推荐/ 质量/ 这一/ 内在激励/ 失效/ ,/ 但/ 如果/ 推荐/ 服务器/ 提供/ 奖励/ ,/ 用户/ 仍/ 有/ 动机/ 对/ 更/ 多/ 的/ 项目/ 评分/ ./ 本文/ 规定/ ,/ 在/ 均衡/ 学习/ 过程/ 中/ ,/ 若/ 存在/ 评分/ 奖励/ ,/ 已/ 满足/ 的/ 用户/ 维持/ 原有/ 行动/ 的/ 概率/ 为/ μ/ / 1/ -/ κ/ ./ 由/ 0/ </ κ/ </ 1/ 可知/ ,/ 此时/ μ/ </ 1/ ,/ 因而/ 均衡/ 学习/ 算法/ 总能/ 收敛/ 到/ 满足/ 均衡/ ./ 本文/ 在/ Jester/ 数据/ 集上/ 对/ 带有/ 评分/ 奖励/ 的/ 均衡/ 表/ 3/ 不同/ 奖励/ 下/ 的/ 均衡/ 学习/ 仿真/ 结果/ (/ Jester/ 数据/ 集/ )/ κ/ =/ 0.01/ κ/ =/ 0.1/ κ/ =/ 0.5/ κ/ =/ 0.9/ 上述/ 关于/ 奖励/ 机制/ 的/ 简单/ 讨论/ 表明/ ,/ 推荐/ 服务器/ 可/ 通过/ 向/ 用户/ 提供/ 外部/ 奖励/ 的/ 方式/ 促使/ 用户/ 间/ 的/ 博弈/ 尽快/ 达到/ 满足/ 均衡/ ./ 推荐/ 服务器/ 可/ 通过观察/ 用户/ 以往/ 的/ 评分/ 行为/ 大致/ 估计/ 用户/ 的/ 评分/ 成本/ 、/ 用户/ 对/ 推荐/ 质量/ 的/ 预期/ 等/ ,/ 进而/ 设计/ 合适/ 的/ 奖励/ 规则/ ./ 具体/ 的/ 设计/ 方法/ 有待/ 进一步/ 研究/ ./ 4.4/ 推荐/ 质量/ 和/ 评分/ 完整/ 度/ 之间/ 的/ 关系/ 第/ 3.2/ 节/ 给出/ 的/ 关于/ 均衡/ 学习/ 算法/ 收敛性/ 的/ 理论/ 分析/ 是/ 基于/ 若干/ 假设/ 的/ ./ 在/ 检验/ 收敛/ 条件/ 之前/ ,/ 此处/ 先/ 利用/ Jester/ 数据/ 集对/ 假设/ 1/ 和/ 假设/ 2/ 的/ 合理性/ 进行/ 说明/ ./ 实验/ 方法/ 如下/ :/ 利用/ 评分/ 矩阵/ 犚/ ′/ 为/ 每个/ 用户/ i/ 构造/ 一组/ 矩阵/ {/ 犚/ i/ ,/ k/ }/ ,/ 其中/ 每个/ 矩阵/ 犚/ i/ ,/ k/ 对应/ 于/ 一组/ 特定/ 的/ σ/ i/ 和/ σ/ -/ i/ ./ σ/ i/ 在/ 集合/ 1/ 取值/ ,/ σ/ -/ i/ 在/ 集合/ 1/ 给定/ σ/ i/ =/ 机/ 选择/ 5/ 个/ 非/ 零/ 元素/ ,/ 然后/ 将/ 这些/ 元素/ 置/ 为/ 零/ ,/ 接着/ 再/ 从/ 犚/ ′/ 的/ 其他/ 行中/ 随机/ 选择/ 90/ %/ 的/ 非/ 零/ 元素/ ,/ 并/ 将/ 这些/ 元素/ 置/ 为/ 零/ ./ 将/ 协同/ 过滤/ 算法/ 应用/ 每个/ 犚/ i/ ,/ k/ ,/ 并/ 根据/ 用户/ i/ 的/ 兴趣/ 向量/ 对/ 推荐/ 结果/ 进行/ 评估/ ,/ 得到/ 与/ 每组/ (/ σ/ i/ ,/ σ/ -/ i/ )/ 对应/ 的/ h/ (/ σ/ i/ ,/ σ/ -/ i/ ;/ 狆/ i/ )/ ./ 根据/ 每个/ 用户/ 学习/ 算法/ 进行/ 了/ 仿真/ ./ 在/ 仿真/ 中/ ,/ 分别/ 设/ κ/ =/ 0.01/ ,/ 0.1/ ,/ 0.5/ ,/ 0.9/ ,/ 其他/ 参数/ 的/ 设计/ 与/ 上/ 一/ 小节/ 相同/ ./ 仿真/ 结果/ 如表/ 3/ 所示/ ./ 对比/ 表/ 1/ 和表/ 3/ 可以/ 看出/ ,/ 当/ 推荐/ 服务器/ 实施/ 评分/ 奖励/ 时/ ,/ 均衡/ 学习/ 算法/ 的/ 收敛/ 速度/ 明显/ 加快/ ./ 奖励/ 越高/ ,/ 算法/ 的/ 收敛/ 速度/ 越快/ ./ 例如/ ,/ 当仅/ 有/ 1/ %/ 的/ 用户/ 对/ 推荐/ 质量/ 的/ 预期/ 很/ 高时/ ,/ 由表/ 1/ 可知/ ,/ 若/ 没有/ 评分/ 奖励/ 且/ μ/ =/ 0.9/ ,/ 学习/ 算法/ 要/ 经过/ 至少/ 400/ 次/ 迭代/ 才能/ 达到/ 均衡/ ;/ 而/ 当/ 推荐/ 服务器/ 提供/ 评分/ 奖励/ 且/ κ/ =/ 0.1/ 时/ ,/ 此时/ 亦/ 有/ μ/ =/ 0.9/ ,/ 但/ 学习/ 算法/ 只/ 需/ 经过/ 约/ 200/ 次/ 迭代/ 便/ 可/ 达到/ 均衡/ ./ 510272000.93820272000/ ./ 8991872000.904372000/ ./ 925/ 的/ 实验/ 数据/ {/ (/ σ/ i/ ,/ σ/ -/ i/ ,/ h/ (/ σ/ i/ ,/ σ/ -/ i/ ;/ 狆/ i/ )/ )/ }/ ,/ 可以/ 绘制/ 出/ 与/ 该/ 用户/ 对应/ 的/ h/ (/ σ/ i/ ,/ σ/ -/ i/ ;/ 狆/ i/ )/ 曲面/ 图/ ./ 图/ 6/ (/ a/ )/ 给出/ 了/ 某/ 一/ 用户/ 对应/ 的/ 曲面/ 图/ ,/ 其他/ 用户/ 的/ 曲面/ 图/ 与/ 之/ 类似/ ./ 从图/ 6/ (/ a/ )/ 可以/ 看到/ ,/ 推荐/ 质量/ 的确/ 会/ 随/ σ/ i/ 和/ σ/ -/ iPage12/ 的/ 增加/ 而/ 提升/ ,/ 这/ 表明/ 假设/ 1/ 是/ 合理/ 的/ ./ 另外/ ,/ 从图/ 6/ (/ b/ )/ 所示/ 的/ 等高线图/ 可以/ 看出/ ,/ 给定/ h/ (/ σ/ i/ ,/ σ/ -/ i/ ;/ 狆/ i/ )/ 的/ 取值/ ,/ σ/ i/ 和/ σ/ -/ i/ 之间/ 的/ 关系/ 可/ 大致/ 用/ 二次曲线/ 描述/ ,/ 这/ 表明/ 假设/ 2/ 也/ 是/ 合理/ 的/ ./ 4.5/ 收敛/ 条件/ 测试/ ,/ 先/ 按式/ (/ 26/ )/ 计算/ θ/ B/ ,/ 然后/ 将/ η/ B/ 本文/ 利用/ Jester/ 数据/ 集对/ 3.2/ 节/ 给出/ 的/ 均衡/ 学习/ 算法/ 收敛/ 条件/ 进行/ 了/ 验证/ ./ 按照/ 假设/ 2/ 的/ 描述/ ,/ 全部/ 7200/ 个/ 用户/ 被/ 随机/ 分为/ NA/ 和/ NB/ 两组/ ,/ η/ A0/ ./ 5/ ,/ ρ/ N/ ρ/ N/ θ/ B/ -/ 0.05/ ,/ η/ B/ =/ θ/ B/ 和/ η/ B/ =/ θ/ B/ +/ 0.05/ ./ 为/ 证明/ 在/ 所有/ 用户/ 都/ 有/ 高/ 预期/ 的/ 情况/ 下/ 算法/ 是/ 可以/ 收敛/ 的/ ,/ 对/ η/ A/ =/ η/ B/ =/ θ/ B/ +/ 0.05/ 的/ 情况/ 也/ 进行/ 了/ 仿真/ ./ 给定/ 一组/ (/ ρ/ N/ )/ ,/ 将/ 简化/ 的/ 均衡/ 学习/ 算法/ 运行/ 10/ 次/ ./ 在/ 每/ 一/ η/ A/ 次/ 运行/ 中/ ,/ 当/ 出现/ 如下/ 两种/ 情况/ 之一/ 时/ 算法/ 终止/ :/ 所/ ,/ η/ B/ 有/ 用户/ 均/ 已/ 满足/ ,/ 未/ 满足/ 的/ 用户/ 已经/ 对/ 其/ 体验/ 过/ 的/ 表/ 4/ 收敛性/ 测试/ 结果/ ρ/ N0/ ./ 010.50/ ./ 7920.10/ ./ 236870706568707065687070675/ 结论/ 在/ 协同/ 过滤/ 推荐/ 系统/ 中/ ,/ 用户/ 的/ 评分/ 行为/ 是/ 相互影响/ 的/ ,/ 各/ 用户/ 以/ 推荐/ 服务器/ 为/ 媒介/ 进行/ 着/ 某种/ 复杂/ 的/ 交互/ ./ 本文/ 将/ 用户/ 间/ 的/ 这种/ 交互/ 建模/ 为/ 一种/ 不/ 完全/ 信息/ 博弈/ ,/ 定义/ 了/ 该/ 博弈/ 对应/ 的/ 满足/ 均衡/ ,/ 并/ 提出/ 了/ 一种/ 均衡/ 学习/ 算法/ ./ 所提/ 算法/ 的/ 基本/ 思想/ 是/ :/ 用户/ 在/ 与/ 推荐/ 服务器/ 迭代/ 交互/ 的/ 过程/ 中/ ,/ 根据/ 当前/ 所有/ 项目/ 进行/ 了/ 评分/ ./ 算法/ 终止/ 时/ 的/ 迭代/ 次数/ 记为/ nstop/ ,/ 满足/ 的/ 用户数/ 记为/ NS/ ./ 从表/ 4/ 所示/ 的/ 仿真/ 结果/ 可以/ 看到/ ,/ 给定/ η/ A/ ,/ 当/ η/ B/ =/ θ/ B/ -/ 0.05/ 时/ ,/ 满足/ 均衡/ 总是/ 可以/ 实现/ 的/ ;/ ρ/ N/ 当/ η/ B/ =/ θ/ B/ 时/ ,/ θ/ B/ +/ 0.05/ 时/ ,/ 结果/ 与/ 3.2/ 节/ 的/ 理论/ 分析/ 结果/ 有/ 些许/ 出入/ ,/ 因为/ 由式/ (/ 26/ )/ 可知/ ,/ 当/ η/ B/ / θ/ B/ 时/ 满足/ 均衡/ 应是/ 可以/ 实现/ 的/ ./ 造成/ 理论/ 分析/ 与/ 仿真/ 结果/ 不/ 一致/ 的/ 原因/ 可能/ 是/ 用户/ 评分/ 完整/ 度/ 与/ 推荐/ 质量/ 之间/ 的/ 关系/ 并/ 不/ 严格/ 符合/ 假设/ 2/ 所/ 定义/ 的/ 二次曲线/ 关系/ ./ 从/ 与/ η/ A/ =/ η/ B/ =/ θ/ B/ +/ 0.05/ 对应/ 的/ 结果/ 可/ 看出/ ,/ 即使/ 所有/ 用户/ 都/ 有/ 较/ 高/ 的/ 预期/ ,/ 用户/ 不/ 提供/ 完整/ 的/ 评分/ 也/ 能/ 实现/ 满足/ 均衡/ (/ nstop/ </ |/ Si/ |/ )/ ./ 这一/ 结果/ 进一步/ 表明/ ,/ 在/ 一个/ 协同/ 过滤/ 系统/ 中/ ,/ 用户/ 能否/ 通过/ 自发/ 的/ 评分/ 实现/ 满足/ 均衡/ ,/ 取决于/ 不同/ 用户/ 对/ 推荐/ 质量/ 是否/ 有着/ 相似/ 的/ 预期/ ./ 4677070656870706669707066/ 所得/ 推荐/ 是否/ 达到/ 预期/ 来/ 决定/ 是否/ 提供/ 更/ 多/ 的/ 评分/ ./ 在/ 适当/ 的/ 简化/ 假设/ 下/ ,/ 本文/ 分析/ 了/ 算法/ 的/ 收敛/ 条件/ ./ 在/ 真实/ 数据/ 上/ 的/ 仿真/ 结果表明/ ,/ 当/ 所有/ 用户/ 对/ 推荐/ 质量/ 有着/ 相似/ 的/ 预期/ 时/ ,/ 通过/ 用户/ 自发/ 的/ 评分/ 行为/ 确实/ 是/ 可以/ 实现/ 满足/ 均衡/ 的/ ./ 用户/ 的/ 积极参与/ 对/ 协同/ 过滤/ 系统/ 的/ 良好/ 运作/ 至关重要/ ./ 本文/ 将/ 推荐/ 质量/ 视为/ 促使/ 用户/ 参与/ 评分/ 的/ 一种/ 内部/ 激励/ ,/ 但/ 给出/ 的/ 博弈/ 分析/ 也/ 可为/ 外部/ 激励机制/ 的/ 设计/ 提供/ 一些/ 启发/ ./ 在/ 接下来/ 的/ 工作/ 中/ ,/ 我们/ Page13/ 将/ 研究/ 如何/ 结合/ 内部/ 激励/ 和/ 外部/ 激励/ 来/ 引导/ 用户/ 在/ 协作/ 式/ 系统/ 中/ 的/ 行为/ ./ 

