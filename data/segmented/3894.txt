Page1/ 云/ 数据/ 管理系统/ 能耗/ 基准/ 测试/ 与/ 分析/ 宋杰/ 1/ )/ 李/ 甜甜/ 1/ )/ 朱志良/ 1/ )/ 鲍玉斌/ 2/ )/ 于戈/ 2/ )/ 1/ )/ (/ 东北大学/ 软件/ 学院/ 沈阳/ 110819/ )/ 2/ )/ (/ 东北大学/ 信息科学/ 与/ 工程学院/ 沈阳/ 110819/ )/ 摘要/ 云/ 数据/ 管理系统/ 是/ 一种/ 新兴/ 的/ 数据/ 管理系统/ ./ 为了/ 研究/ 云/ 数据/ 管理系统/ 的/ 能耗/ 优化/ ,/ 实现/ “/ 绿色/ 计算/ ”/ ,/ 首先/ 要/ 定义/ 能耗/ 的/ 度量/ 模型/ 和/ 基准/ 测试方法/ ,/ 分析/ 系统/ 的/ 能耗/ 特点/ ./ 目前/ 云/ 数据/ 管理系统/ 的/ 基准/ 测试/ 主要/ 集中/ 在/ 性能/ 方面/ ,/ 对/ 能耗/ 方面/ 的/ 评估/ 和/ 优化/ 工作/ 很少/ ;/ 对/ 测量/ 仪器/ 、/ 测试/ 手段/ 、/ 测试用例/ 以及/ 能耗/ 基本规律/ 的/ 研究/ 存在/ 空白/ ./ 文中/ 提出/ 了/ 一种/ 能耗/ 的/ 度量/ 模型/ 和/ 数学/ 表达/ ;/ 定义/ 了/ 一组/ 数据/ 装载/ 、/ 查询/ 和/ 分析/ 用例/ 来/ 测试/ 云/ 数据/ 管理系统/ 的/ 能耗/ ;/ 设计/ 了/ 系统/ 能耗/ 的/ 测量方法/ ;/ 分析/ 了/ 若干/ 云/ 数据/ 管理系统/ 在/ 执行/ 数据/ 装载/ 、/ 读取/ 、/ 查询/ 、/ 聚集/ 和/ 连接/ 等/ 操作/ 时/ 的/ 能耗/ 特征/ ,/ 提出/ 了/ 通过/ 降低/ “/ 等待/ 能耗/ ”/ 而/ 进行/ 云/ 数据/ 管理系统/ 的/ 能耗/ 优化/ ./ 大量/ 实验/ 数据/ 证明/ ,/ 尽管/ 云/ 计算/ 被/ 认为/ 是/ 一种/ 绿色/ 计算/ ,/ 但/ 文中/ 测试/ 的/ 云/ 数据/ 管理系统/ 在/ 能耗/ 方面/ 差异/ 较大/ ,/ 需要/ 对/ 部分/ 系统/ 进行/ 进一步/ 的/ 优化/ ./ 关键词/ 云/ 数据/ 管理系统/ ;/ 能耗/ ;/ 基准/ 测试/ ;/ MapReduce/ ;/ 大/ 数据/ ;/ 云/ 计算/ ;/ 绿色/ 计算/ 1/ 引言/ 云/ 数据库/ (/ CloudbasedDatabase/ )/ [/ 1/ ]/ 是/ 一种/ 基于/ 云/ 技术/ 的/ 数据库系统/ ,/ 是/ 云/ 计算/ 中/ 极具/ 潜力/ 的/ 数据/ 存储/ 和/ 管理模式/ ./ 而云/ 数据/ 管理系统/ (/ CloudDataManagementSystem/ )/ 通过/ 营造/ “/ 面向/ 服务/ 的/ 数据管理/ 和/ 提供/ (/ ServiceOrientedDataManagingandProvisioning/ )/ ”/ 环境/ ,/ 蕴含着/ 前所未有/ 的/ 数据/ 交付/ 能力/ ./ 云/ 数据/ 管理系统/ 成本/ 低廉/ ,/ 有着/ 良好/ 的/ 并行性/ 和/ 伸缩性/ ,/ 支持/ 海量/ 数据管理/ 和/ 大规模/ 集群/ 系统/ ,/ 支持/ 服务化/ 的/ 访问/ 和/ 按/ 需/ 使用/ 、/ 按/ 需/ 付费/ [/ 2/ ]/ ./ 云/ 数据/ 管理系统/ 是/ 一种/ 新兴/ 的/ 数据/ 管理系统/ ./ 目前/ 已经/ 存在/ 很多/ 云/ 数据/ 管理系统/ ,/ 其中/ 开源/ 系统/ 包括/ HBase/ 、/ Cassandra/ 、/ Hive/ 、/ MongoDB/ 、/ HadoopDB/ 、/ GridSQL/ 、/ Hypertable/ 、/ CouchDB/ 、/ Dynomite/ 、/ Voldemort/ 、/ Elastras/ 等/ ,/ 商业系统/ 则/ 有/ 亚马逊/ 的/ SimpleDB/ 、/ 微软/ 的/ AzureSQL/ 、/ 谷歌/ 的/ BigTable/ 、/ 雅虎/ 的/ PNUTS/ 和/ EMC/ 的/ GreenPlum/ ./ 上述/ 云/ 数据/ 管理系统/ 实现/ 技术/ 各不相同/ ,/ 但/ 较/ 传统/ 数据/ 管理系统/ 最/ 显著/ 的/ 优点/ 是/ 节省/ 开销/ ./ HBase/ 宣传/ 自身/ 适用/ 于/ 那些/ “/ 一年/ 在/ Oracle/ 数据库/ 上/ 的/ 花费/ 超过/ 一个/ 小/ 国家/ 国民/ 生产总值/ 的/ 企业/ ”/ ①/ ./ 云/ 数据/ 管理系统/ 确实/ 能/ 在/ 保证/ 服务质量/ 的/ 同时/ 有效/ 地/ 降低成本/ ./ 从/ 数据服务/ 角度/ ,/ 它/ 提供/ “/ 数据/ 即/ 服务/ ”/ (/ DataasaService/ )/ 的/ 使用/ 模型/ 和/ “/ 按/ 需/ 使用/ ,/ 按/ 需/ 付费/ ”/ 的/ 付费/ 方式/ ,/ 用户/ 不必/ 购买/ 昂贵/ 的/ 软硬件/ 资源/ ;/ 从/ 数据管理/ 角度/ ,/ 虚拟化/ 技术/ 和/ 分布式/ 的/ 数据管理/ 方式/ ,/ 有利于/ 合理/ 分布/ 数据/ 和/ 调度/ 任务/ ,/ 使/ 负载/ 更为/ 集中/ (/ WorkloadConcentration/ )/ ,/ 减少/ 空闲/ 的/ IT/ 资源/ ./ 因此/ ,/ 可以/ 认为/ 云/ 数据/ 管理系统/ 是/ 一种/ 追求/ 节能/ ,/ 更为/ “/ 绿色/ ”/ 的/ 数据管理/ 模式/ ./ 尽管/ 云/ 数据管理/ 被/ 认为/ 是/ 一种/ 绿色/ 的/ 数据管理/ 模式/ ,/ 但/ 其/ 本身/ 并/ 没有/ 提供/ 成熟/ 的/ 解决方案/ 来/ 评价/ 和/ 降低/ 能耗/ ,/ 仍/ 需要/ 能耗/ 优化/ 方法/ 来/ 切实/ 地/ 实现/ “/ 绿色/ 计算/ ”/ ./ 研究/ 云/ 数据/ 管理系统/ 的/ 能耗/ 优化/ ,/ 首先/ 是/ 要/ 定义/ 能耗/ 的/ 度量/ 模型/ 和/ 基准/ 测试方法/ ,/ 分析/ 各种/ 云/ 数据/ 管理系统/ 的/ 能耗/ 特点/ ./ 就/ 我们/ 所知/ ,/ 目前/ 云/ 数据/ 管理系统/ 的/ 基准/ 测试/ 主要/ 集中/ 在/ 性能/ 方面/ ,/ 对/ 能耗/ 方面/ 的/ 评估/ 和/ 优化/ 工作/ 很少/ ;/ 对/ 测量/ 仪器/ 、/ 测试/ 手段/ 、/ 测试用例/ 以及/ 能耗/ 基本规律/ 的/ 研究/ 存在/ 空白/ ./ 为/ 更好/ 地/ 降低/ 能耗/ ,/ 本文/ 将/ 从/ 以下/ 几个/ 方面/ 开展/ 研究/ :/ 首先/ ,/ 本文/ 设计/ 了/ 一组/ 用于/ 测试/ 云/ 数据/ 管理系统/ 能耗/ 的/ 数据/ 操作/ 和/ 分析/ 用例/ ,/ 并/ 定义/ 了/ 能耗/ 度量/ 模型/ ./ 本文/ 未/ 采用/ 经典/ 的/ 数据库/ 测试用例/ TPC/ -/ H/ ②/ ,/ 这是/ 由于/ 云/ 数据/ 管理系统/ 的/ 特殊性/ 和/ 不/ 成熟性/ ,/ TPC/ -/ H/ 的/ 诸多/ 查询/ 过于/ 复杂/ 而/ 难以/ 支持/ ./ 本文/ 沿用/ 文献/ [/ 3/ ]/ 中/ 的/ 测试用例/ ,/ 该/ 用例/ 被/ 广泛/ 地/ 运用/ 在/ 基于/ MapReduce/ 的/ 数据分析/ 的/ 基准/ 测试/ 上/ [/ 4/ -/ 5/ ]/ ,/ 本/ 研究/ 在/ 云/ 数据/ 管理系统/ 上/ 实现/ 该/ 用例/ 并/ 略加/ 改进/ ./ 其次/ ,/ 本文/ 提出/ 了/ 一种/ 降低/ 能耗/ 的/ 新思路/ ./ 传统/ 的/ 集群/ 系统/ 能耗/ 分析方法/ 主要/ 评估/ “/ 空闲/ 能耗/ ”/ ,/ 即/ 关闭/ 空闲/ 的/ 计算机/ 以/ 减少/ 空闲/ 能耗/ ,/ “/ 空闲/ 能耗/ ”/ 越/ 多/ ,/ 整体/ 能耗/ 提升/ 空间/ 越大/ ./ 而/ 本文/ 针对/ 云/ 数据/ 管理系统/ 的/ 特点/ ,/ 提出/ 减少/ “/ 等待/ 能耗/ ”/ 以/ 节能/ ./ 节点/ 因/ 等待/ 作业/ 调度/ 、/ I/ // O/ 操作/ 或/ 其它/ 节点/ 的/ 运算/ 结果/ 而/ 处于/ 暂时/ 的/ “/ 空闲/ ”/ ,/ 等待时间/ 中/ 产生/ 的/ 能耗/ 为/ “/ 等待/ 能耗/ ”/ ./ 与/ “/ 空闲/ 能耗/ ”/ 不同/ ,/ 某/ 节点/ 产生/ “/ 等待/ 能耗/ ”/ 时/ ,/ 该/ 节点/ 任务/ 尚未/ 执行/ 结束/ ,/ 无法/ 通过/ 关闭/ 该/ 节点/ 来/ 节能/ ,/ 但/ 我们/ 可以/ 通过/ 减少/ 节点/ 等待/ 而/ 降低/ “/ 等待/ 能耗/ ”/ ./ “/ 等待/ 能耗/ ”/ 越/ 少/ ,/ 系统/ 能耗/ 越/ 优化/ ./ 最后/ ,/ 本文/ 对/ 主流/ 的/ 云/ 数据/ 管理系统/ HBase/ 、/ Cassandra/ 、/ Hive/ 、/ MongoDB/ 、/ HadoopDB/ 和/ GridSQL/ 在/ 不同/ 运行/ 条件/ 下/ 执行/ 数据/ 装载/ 、/ 读取/ 、/ 查询/ 、/ 聚集/ 和/ 连接/ 等/ 操作/ 时/ 的/ 基准/ 能耗/ 进行/ 测量/ [/ 6/ ]/ ,/ 总结/ 其/ 能耗/ 规律/ ./ 上述/ 数据/ 管理系统/ 的/ 实现/ 方式/ 各不相同/ ,/ 但/ 都/ 广泛/ 的/ 运用/ 在/ 云/ 计算/ 的/ 各个/ 应用/ 中/ ./ 实验/ 证明/ ,/ 尽管/ 云/ 计算/ 被/ 认为/ 是/ 一种/ 绿色/ 计算/ ,/ 但/ 上述/ 云/ 数据/ 管理系统/ 在/ 能耗/ 方面/ 差异/ 很大/ ,/ 其/ 能耗/ 有/ 很大/ 的/ 优化/ 空间/ ./ 对云/ 数据/ 管理系统/ 进行/ 能耗/ 基准/ 测试/ 有着/ 重要/ 的/ 意义/ :/ (/ 1/ )/ 分析/ 各种/ 云/ 数据/ 管理系统/ 的/ 能耗/ 特征/ ,/ 总结/ 影响/ 能耗/ 的/ 因素/ ,/ 提出/ 能耗/ 优化/ 的/ 基本/ 方法/ ;/ (/ 2/ )/ 通过/ 测试/ 和/ 比较/ 各云/ 数据/ 管理系统/ 之间/ 的/ 能耗/ ,/ 可/ 指导/ 系统/ 选择/ 和/ 低能耗/ 系统/ 的/ 设计/ 和/ 研发/ ;/ (/ 3/ )/ 通过/ 总结/ 云/ 数据/ 管理系统/ 能耗/ 规律/ ,/ 可/ 优化/ 基于/ 该/ 系统/ 的/ 低能耗/ 应用软件/ ;/ (/ 4/ )/ 使/ 能耗/ 作为/ 一种/ 服务质量/ 要素/ ,/ 云/ 数据/ 服务提供商/ 可以/ 准确/ 计算/ 能源/ 成本/ ,/ 服务/ 使用者/ 也/ 准确/ 地/ 按/ 需/ 使用/ ,/ 按/ 需/ 付费/ ./ 本文/ 第/ 2/ 节/ 简要/ 介绍/ 研究/ 背景/ ,/ 包括/ 云/ 数据/ 管理系统/ 和/ MapReduce/ ;/ 第/ 3/ 节/ 介绍/ 基准/ 测试/ 方面/ 的/ 相关/ 工作/ ;/ 第/ 4/ 节/ 定义/ 本文/ 提出/ 的/ 基准/ 测试用例/ 和/ 能耗/ 度量/ 度/ 模型/ ;/ 第/ 5/ 节/ 定义/ 了/ 本文/ 提出/ 的/ “/ 等待/ 能耗/ ”/ ,/ 介绍/ 了/ 能耗/ 的/ 分析方法/ 、/ 测量方法/ 和/ 测试环境/ ;/ 第/ 6/ 节/ 分析/ 和/ 比较/ 了/ HBase/ 、/ Cassandra/ 、/ Hive/ 、/ MongoDB/ 、/ HadoopDB/ 和/ GridSQL/ 在/ 相同/ 测试用例/ 下/ 的/ 能耗/ ,/ 提出/ 能耗/ 优化/ 方法/ ;/ 最后/ ,/ 在/ 第/ 7/ 节对/ 本文/ 的/ 研究/ 进行/ 总结/ 并/ 提出/ 进一步/ 的/ 工作/ ./ ①/ ②/ Page32/ 研究/ 背景/ 目前/ 学术界/ 尚/ 没有/ 给出/ 云/ 数据库/ 的/ 确切/ 的/ 定义/ ./ 通常/ 认为/ 云/ 数据库/ 是/ 基于/ CAP/ 理论/ [/ 7/ -/ 8/ ]/ 、/ BASE/ 理论/ [/ 9/ ]/ 和/ 数据库/ Sharding/ ①/ ②/ 技术/ 而/ 发展/ 起来/ 的/ 新型/ 数据库/ ,/ 其/ 与/ 现有/ 的/ 关系/ 型/ 数据库/ 存在/ 较大/ 差别/ ,/ 大部分/ 云/ 数据库/ 采用/ 非/ 关系/ 型/ 的/ 、/ 更为/ 松散/ 的/ 数据模型/ [/ 10/ ]/ ./ 云/ 数据库/ 的/ 出现/ 是/ 为了/ 解决/ 关系数据库/ 伸缩性/ 差/ 、/ 读写/ 慢/ 、/ 成本/ 高/ 、/ 支撑/ 容量/ 有限/ 等/ 问题/ ;/ 其/ 设计/ 针对/ “/ 快速/ 响应/ 请求/ ”/ 、/ “/ 支撑/ 海量/ 的/ 数据/ 和/ 流量/ ”/ 、/ “/ 大规模/ 集群/ ”/ 、/ “/ 低/ 运营/ 成本/ ”/ 等/ 需求/ [/ 11/ ]/ ./ 大部分/ 云/ 数据/ 管理系统/ (/ 如/ HBase/ 、/ Hive/ 和/ Cassandra/ )/ 采用/ 键值/ (/ Key/ -/ Value/ )/ 数据模型/ 存储/ 和/ 管理/ 数据/ ,/ 也/ 有/ 的/ 沿用/ 传统/ 的/ 关系/ 模型/ (/ 如/ Hadoop/ -/ DB/ 和/ GridSQL/ )/ ./ 键值/ 数据模型/ 是/ 一种/ 稀疏/ 的/ 、/ 分布/ 的/ 、/ 持久/ 化/ 的/ 、/ 多维/ 有序/ 映射/ 表/ (/ Map/ )/ [/ 12/ ]/ ,/ 其/ 特点/ 是/ 简单/ 而/ 灵活/ ./ 还有/ 一些/ 云/ 数据/ 管理系统/ 基于/ 文档/ 数据模型/ (/ JSON/ 或/ XML/ 文档/ 等/ )/ ,/ 如/ MongoDB/ 、/ BaseX/ 等/ ./ 云/ 数据/ 管理系统/ 大多/ 通过/ Shared/ -/ Nothing/ 体系结构/ 和/ MapReduce/ 编程/ 模型/ 实现/ 大规模/ 的/ 分布式计算/ ./ MapReduce/ [/ 13/ ]/ 是/ 用于/ 可/ 扩展/ 并行/ 数据处理/ 的/ 编程/ 模型/ ,/ 该/ 模型/ 有/ 多种/ 实现/ ,/ 如谷歌/ MapRe/ -/ duce/ 、/ ApacheHadoop/ ③/ 、/ Map/ -/ Reduce/ -/ Merge/ [/ 14/ ]/ 、/ 多核/ 和/ 多处理器/ 系统/ 的/ MapReduce/ [/ 15/ ]/ 等/ ./ MapReduce/ 被/ 广泛/ 地/ 运用/ 于/ 海量/ 数据/ 的/ 分析/ 和/ 处理/ 中/ ./ 一个/ MapReduce/ 程序/ 仅/ 包含/ 两个/ 函数/ ,/ 即/ Map/ 和/ Reduce/ ,/ 它们/ 定义/ 了/ 用户/ 处理/ “/ 键值/ 对/ 数据/ ”/ 的/ 逻辑/ ./ 程序/ 的/ 输入/ 数据/ 集/ 位于/ 分布式文件系统/ 或/ 数据库系统/ 中/ ,/ 采用/ “/ 迁移/ 运算/ 而/ 非/ 迁移/ 数据/ ”/ 的/ 方式/ ,/ MapReduce/ 程序/ 被/ 下载/ 到/ 每个/ 数据/ 节点/ 并/ 执行/ ,/ 输出/ 结果/ 仍/ 保存/ 在/ 分布式文件系统/ 或/ 数据库系统/ 中/ ./ Map/ 和/ Reduce/ 函数/ 的/ 输入/ 和/ 输出/ 都/ 是/ 用户/ 定义/ 格式/ 的/ 键值/ 对/ 形式/ ,/ 其中/ Map/ 函数/ 输入/ 〈/ KeyInM/ ,/ ValueIn/ 函数/ 的/ 输入/ 为/ 〈/ KeyInR/ 〉/ ,/ 其中/ KeyOutValueOutR/ 是/ 保存/ ValueOutValueInMapReduce/ 将/ 作业/ (/ Job/ )/ 分解/ 为/ 任务/ (/ Task/ )/ 并/ 在/ 每个/ 节点/ 上/ 并行/ 地/ 执行/ ./ MapReduce/ 程序/ 首先/ 将/ 输入/ 数据/ 分割/ 成/ M/ 份/ (/ 通常/ M/ 大于/ 节点/ 个数/ )/ ,/ 作为/ M/ 个/ Map/ 实例/ 的/ 输入/ ./ Map/ 函数/ 从/ 一份/ 输入/ 中/ 读取/ 每/ 一条/ 记录/ ,/ 对/ 其/ 进行/ 必要/ 的/ 过滤/ 和/ 转换/ ,/ 然后/ 输出/ 〈/ KeyOut/ 果/ 被/ Hash/ 函数/ 按/ KeyOut/ 每个/ 组/ 被/ 写入/ 到/ 处理/ 节点/ 的/ 本地/ 磁盘/ 中/ ./ 所有/ Map/ 函数/ 终止/ 时/ ,/ M/ 个/ Map/ 实例/ 把/ M/ 份/ 输入/ 文件/ 映射/ 成/ M/ ×/ R/ 个/ 中间/ 文件/ ./ 由于/ 所有/ Map/ 函数/ 的/ 分割/ 函数/ 都/ 一样/ ,/ 因此/ 相同/ 键/ Hash/ 值/ (/ 设为/ j/ )/ 的/ Map/ 函数/ 输出/ 结果/ 被/ 存在/ 文件/ Fij/ (/ 1/ / i/ / M/ ,/ 1/ / j/ / R/ )/ 中/ ./ MapReduce/ 的/ 第二阶段/ 执行/ R/ 个/ Reduce/ 实例/ ,/ R/ 通常/ 是/ 节点/ 的/ 数量/ ./ 每个/ Reduce/ 实例/ Rj/ 输入/ 文件/ 为/ Fij/ (/ 1/ / i/ / M/ )/ ./ 这些/ 文件/ 从/ 各个/ 节点/ 通过/ 网络/ 传输/ 汇聚/ 到/ 执行/ 节点/ ./ Reduce/ 函数/ 输入/ Fij/ 的/ 键/ M/ 和/ 对应/ 的/ 一组/ ValueOutKeyOut/ 数据库系统/ 输出/ 若干/ 〈/ KeyOut/ 录/ ./ 所有/ 的/ 从/ Map/ 阶段/ 产生/ 的/ 具有/ 相同/ Hash/ 值/ 的/ M/ ,/ ValueOut/ 〈/ KeyOut/ 例/ 处理/ ./ 输入/ 数据/ 集以/ 集合/ 的/ 形式/ 存在/ 于/ 分布式/ 文件/ 或/ 数据库系统/ 中/ 的/ 一个/ 或/ 多个/ 分区/ 中/ ./ MapReduce/ 的/ 调度/ 器/ 决定/ 执行/ 多少/ 个/ Map/ 实例/ ,/ 以及/ 如何/ 把/ 它们/ 分配/ 给/ 可用/ 的/ 节点/ ;/ 同样/ ,/ 调度/ 器/ 还/ 必须/ 决定/ 运行/ Reduce/ 实例/ 的/ 数量/ 和/ 执行/ 位置/ (/ 一些/ MapReduce/ 实现/ ,/ 如/ Hadoop/ 早期/ 版本/ 让/ 用户/ 指定/ Map/ 和/ Reduce/ 的/ 数目/ )/ ./ MapReduce/ 中央/ 控制器/ 协调/ 每个/ 节点/ 上/ 的/ 运算/ ,/ 一旦/ 最终/ 结果/ 以新/ 数据/ 的/ 形式/ 写入/ 分布式/ 文件/ 或/ 数据库系统/ 中/ ,/ MapRedcue/ 程序执行/ 完毕/ ./ 本文/ 将/ 具体/ 考察/ 五种/ 云/ 数据/ 管理系统/ 在/ 能耗/ 上/ 的/ 差异/ ./ 其中/ ,/ HBase/ 是/ 一个/ 分布式/ 的/ 、/ 面向/ 列/ (/ Column/ -/ Oriented/ )/ 的/ 开源/ 数据库/ ,/ 构建/ 于/ Hadoop/ 分布式文件系统/ (/ HDFS/ )/ 之上/ ,/ 支持/ MapReduce/ 编程/ 模型/ ,/ 用于/ 管理/ 需/ 随机/ 访问/ 、/ 实时/ 读写/ 的/ 海量/ 数据/ ;/ Hive/ 基于/ 分布式文件系统/ ,/ 而/ HadoopDB/ 基于/ 关系/ 型/ 数据库/ PostgreSQL/ ;/ 两者/ 都/ 支持/ SQL/ 和/ MapReduce/ 编程/ 模型/ ,/ 使/ SQL/ 按/ MapReduce/ 的/ 方式/ 执行/ ,/ 兼顾/ 效率/ 和/ 水平/ 伸缩性/ (/ Scaleout/ )/ ;/ Cassandra/ 是/ 一个/ 环状/ 的/ 、/ 面向/ 列/ 的/ 云/ 数据/ 管理系统/ ,/ 基于/ P2P/ 技术/ 的/ 去/ 中心化/ (/ Decentralized/ )/ 存储/ 实现/ 高/ 并行性/ 和/ 伸缩性/ ;/ MongoDB/ 是/ 一个/ 分布式/ 的/ 文档/ 数据库/ ,/ 通过/ 索引/ 机制/ 支持/ 高效/ 的/ 查询/ ;/ GridSQL/ 是/ 一种/ 并行/ 化/ 关系/ 型/ 数据/ 管理系统/ ,/ 是/ EnterpriseDB/ 的/ 开源/ 实现/ ,/ 基于/ 开源/ 数据库/ Post/ -/ greSQL/ ,/ 采取/ 非/ 共享/ (/ SharedNothing/ )/ 的/ 体系结构/ 并行/ 地/ 处理/ 数据/ 请求/ ,/ 是/ 高性能/ 并行/ 数据库/ 产品/ ./ ①/ ②/ ③/ Page43/ 相关/ 工作/ 基准/ 测试/ 是/ 指/ “/ 运行/ 一系列/ 程序/ 或/ 操作/ 以/ 获得/ 待测/ 对象/ 性能/ ”/ 的/ 过程/ ./ 应用程序/ 的/ 基准/ 测试/ 可/ 分为/ 三类/ [/ 16/ ]/ :/ 基于/ 向量/ 的/ (/ Vector/ -/ based/ )/ 、/ 基于/ 路径/ 的/ (/ Trace/ -/ based/ )/ 和/ 混合/ 的/ (/ Hybrid/ )/ 方法/ ./ 基于/ 向量/ 的/ 方法/ 把/ 系统/ 的/ 性能/ 抽象/ 为/ 多个/ 向量/ ,/ 对/ 每个/ 向量/ 进行/ 基准/ 测试/ ,/ 最后/ 得出/ 整体/ 系统/ 性能/ ,/ 如/ 文献/ [/ 16/ ]/ 中/ 提及/ 的/ 使用/ 基于/ 向量/ 的/ 基准/ 测试方法/ 测试/ Java/ 虚拟机/ 性能/ ;/ 基于/ 路径/ 的/ 基准/ 测试方法/ 则/ 着重/ 考虑/ 测试用例/ 而/ 非待/ 测系统/ ,/ 首先/ 定义/ 可/ 用于/ 进行/ 基准/ 测试/ 的/ 测试用例/ (/ 路径/ )/ ,/ 然后/ 定义/ 影响/ 用例/ 负载/ 的/ 因素/ ,/ 如/ 数据量/ 、/ 并发/ 量/ 等/ ,/ 以及/ 生成/ 不同/ 负载/ 的/ 用例/ 的/ 方法/ ./ TPC/ 测试/ 就是/ 基于/ 路径/ 的/ 基准/ 测试/ ./ 混合/ 方法/ 则/ 结合/ 了/ 两种/ 方法/ ./ 本/ 研究/ 提出/ 的/ 云/ 数据/ 管理系统/ 能耗/ 基准/ 测试/ 是/ 基于/ 路径/ 的/ 基准/ 测试/ ./ 到/ 目前为止/ ,/ 我们/ 没有/ 发现/ 对云/ 数据库/ 或/ 其它/ 数据库/ 的/ 能耗/ 基准/ 测试/ 的/ 报道/ ,/ 大部分/ 基准/ 测试/ 针对/ 的/ 是/ 数据库/ 性能/ ./ 其中/ TPC/ -/ H/ [/ 3/ ]/ 是/ 一个/ 决策支持系统/ 的/ 基准/ 测试/ 集/ ,/ 它/ 由/ 行业/ 相关/ 的/ 数据/ 组成/ ,/ 包括/ 8/ 张表/ 和/ 22/ 个/ 面向/ 业务/ 的/ 查询/ ./ TPC/ -/ H/ 的/ 测试/ 结果/ 还/ 可以/ 通过/ QphH/ @/ Size/ 和/ Price/ // QphH/ 来/ 衡量/ ,/ 许多/ 数据库/ 集群/ 都/ 使用/ 两者/ 比较/ 性能/ [/ 3/ ]/ ./ TPC/ -/ H/ 使用/ 了/ 海量/ 的/ 数据/ 集/ 和/ 极其/ 复杂/ 的/ 查询/ ,/ 这些/ 查询/ 对于/ 云/ 数据/ 管理系统/ 来说/ 过于/ 复杂/ ,/ 难以实现/ (/ 如/ 目前/ HBase/ 和/ Cassandra/ 尚/ 不/ 提供/ 显式/ 的/ 表/ 连接/ 和/ 分组/ 聚集/ 操作/ )/ ./ 有/ 一些/ 基准/ 测试/ 则/ 是/ 定义/ 在/ 其它/ 基准/ 测试/ 基础/ 上/ ./ 如/ STMBench7/ [/ 17/ ]/ 是/ 一个/ 用来/ 评估/ 软件/ 事务性/ 内存/ (/ SoftwareTransactionalMemory/ ,/ STM/ )/ 的/ 基准/ ,/ STMBench7/ 是/ 以/ 面向对象/ 数据库系统/ 的/ 基准/ 测试/ —/ —/ —/ OO7/ 基准/ 测试/ [/ 18/ ]/ 为/ 基础/ ./ 由于/ 云/ 数据/ 管理系统/ 的/ 特殊性/ ,/ 我们/ 无法/ 参照/ 现有/ 关系数据库/ 的/ 基准/ 测试/ 集/ ,/ 但/ 我们/ 参考/ 了/ 部分/ 现有/ 的/ MapReduce/ 基准/ 测试/ 研究/ [/ 3/ -/ 5/ ]/ ./ 目前/ 存在/ 一些/ 针对/ 云/ 数据/ 管理系统/ 性能/ 的/ 基准/ 测试/ ./ 文献/ [/ 12/ ]/ 评估/ 了/ 不/ 支持/ 结构化/ 查询语言/ 的/ 数据/ 管理系统/ 的/ 性能/ ,/ 如谷歌/ BigTable/ ./ 雅虎公司/ 研发/ 的/ YCSB/ (/ YahooCloudServingBenchmark/ )/ [/ 19/ ]/ 框架/ 提供/ 一组/ 由/ 数据/ 插入/ 、/ 读取/ 、/ 更新/ 和/ 扫描/ 等/ 云/ 数据管理/ 服务/ 而/ 组合/ 的/ 测试用例/ ,/ 可以/ 用来/ 衡量/ 云/ 数据/ 管理系统/ 的/ 性能/ ,/ YCSB/ 使用/ 的/ 测试用例/ 仅/ 涉及/ 最/ 基本/ 的/ 数据/ 操作/ ,/ 对于/ 数据管理/ 操作/ 来说/ 过于/ 简单/ ./ 文献/ [/ 3/ ]/ 在/ HadoopHDFS/ 上/ 基于/ MapReduce/ 实现/ 了/ 传统/ 数据库/ 的/ 增加/ 、/ 查询/ 、/ 聚合/ 和/ 连接/ 操作/ ,/ 并且/ 比较/ 了/ 这些/ 操作/ 的/ Hadoop/ 实现/ 和/ 并行/ 数据库/ 实现/ 的/ 性能/ ./ 此外/ ,/ 云/ 数据/ 管理系统/ 多/ 基于/ MapReduce/ 机制/ 并行/ 地/ 执行/ 数据/ 查询/ 作业/ ,/ 虽然/ MapReduce/ 编程/ 模型/ 已/ 得到/ 广泛/ 运用/ ,/ 对/ MapReduce/ 的/ 性能/ 衡量/ 和/ 基准/ 测试/ 工作/ 仍/ 较为/ 有限/ ./ 除/ 前文/ 提到/ 的/ 云/ 数据/ 管理系统/ 基准/ 测试/ 中/ 直接/ 或/ 间接/ 地/ 测试/ 了/ MapReduce/ 性能/ 以外/ ,/ 一些/ 研究/ 设计/ 了/ 如/ 计数/ 、/ 排序/ 、/ Grep/ 、/ 矩阵/ 乘法/ 和/ 聚合/ 算法/ 等/ 面向/ CPU/ 密集型/ 、/ I/ // O/ 密集型/ 、/ (/ Map/ 阶段/ )/ CPU/ 和/ (/ Reduce/ 阶段/ )/ I/ // O/ 密集型/ 、/ 交互/ 型/ 的/ MapReduce/ 用例/ [/ 13/ ,/ 15/ ,/ 20/ ]/ ./ 本/ 研究/ 更多地/ 延续/ 了/ 文献/ [/ 3/ ]/ 的/ 设计/ ,/ 但/ 本文/ 提出/ 的/ 基准/ 测试/ 与/ 上述/ 工作/ 有/ 明显/ 差别/ ./ 首先/ ,/ 我们/ 提出/ 的/ 是/ 能耗/ 基准/ 测试/ 而/ 非/ 性能/ 基准/ 测试/ ;/ 其次/ ,/ 我们/ 并非/ 针对/ MapReduce/ 模型/ 或/ 文件系统/ ,/ 而是/ 数据/ 管理系统/ ;/ 同时/ 为/ 更好/ 地/ 衡量/ 能耗/ ,/ 我们/ 对/ 部分/ 测试用例/ 进行/ 了/ 扩展/ ,/ 对/ 用例/ 的/ 云/ 数据/ 管理系统/ 实现/ 进行/ 了/ 优化/ ./ 本文/ 的/ 目标/ 不是/ 研究/ 能耗/ 的/ 优化/ 方法/ ,/ 而是/ 研究/ 能耗/ 的/ 基准/ 测试方法/ ,/ 但/ 也/ 为/ 能耗/ 优化/ 方法/ 提供/ 参考/ ./ 文献/ [/ 21/ ]/ 从/ 网络/ 全局/ 角度/ 研究/ 网络/ 的/ 能耗/ 模型/ 和/ 算法/ ,/ 对/ 不同/ 的/ 数据/ 传递/ 模式/ 的/ 能耗/ 进行/ 建模/ 并/ 研究/ 其/ 能耗/ 优化/ 方法/ ./ 文献/ [/ 22/ ]/ 从/ 能耗/ 测量/ 、/ 能耗/ 建模/ 、/ 能耗/ 管理/ 实现/ 机制/ 、/ 能耗/ 管理/ 优化/ 算法/ 四个/ 方面/ 对/ 虚拟化/ 云/ 计算/ 平台/ 能耗/ 管理/ 的/ 最新/ 研究成果/ 进行/ 了/ 介绍/ ,/ 从中/ 可以/ 看出/ 很多/ 现有/ 研究/ 都/ 是从/ “/ 负载/ 集中/ ”/ 和/ “/ 开关/ 算法/ ”/ 的/ 角度/ 来/ 考虑/ 能耗/ 优化/ [/ 23/ -/ 24/ ]/ ,/ 因此/ 多/ 关注/ “/ 空闲/ 能耗/ ”/ ,/ 让/ 空闲/ 的/ 节点/ 休眠/ [/ 25/ ]/ 以/ 节省/ 能耗/ ./ 而/ 本文/ 在/ 实验/ 数据/ 的/ 基础/ 上/ ,/ 分析/ 了/ 云/ 数据/ 管理系统/ 的/ 能耗/ 规律/ ,/ 并/ 提出/ 通过/ “/ 等待/ 能耗/ ”/ 来/ 评价/ 系统/ 的/ 能耗/ 特性/ ,/ 指出/ 即使/ 节点/ 没有/ 完全/ 空闲/ ,/ 也/ 会/ 因/ 等待/ 调度/ 、/ I/ // O/ 操作/ 等/ 其它/ 资源/ 而/ 产生/ 等待/ 能耗/ ,/ 降低/ 整体/ 能效/ ./ “/ 等待/ 能耗/ ”/ 过多/ 是/ 造成/ 部分/ 云/ 数据/ 管理系统/ 能耗/ 高/ 的/ 原因/ ./ 从/ 研究/ 对象/ 角度/ 部分/ 现有/ 研究/ 分析/ 了/ MapReduce/ 执行/ 环境/ 的/ 各种/ 静态/ 参数/ 对/ 其/ 能耗/ 的/ 影响/ [/ 26/ -/ 28/ ]/ ,/ 但/ 没有/ 针对/ 数据库/ 操作/ ,/ 如/ 查询/ 、/ 聚合/ 和/ 连接/ 运算/ 加以分析/ ./ 就/ 云/ 数据/ 管理系统/ 而言/ ,/ 尚未/ 见/ 其/ 软件/ 层面/ 能耗/ 分析/ 和/ 比较/ 之类/ 的/ 研究/ 报道/ ./ 4/ 能耗/ 基准/ 测试/ 本节/ 将/ 介绍/ 基准/ 测试用例/ 的/ 数据模型/ 、/ 用例/ 的/ 定义/ 和/ 基于/ MapReduce/ 的/ 实现/ 算法/ ./ 4.1/ 数据模型/ 本文/ 设计/ 的/ 基准/ 测试用例/ 基于/ 三个/ 简单/ 的/ 表/ 结构/ ./ Grep/ 查询/ 作业/ 来自/ 文献/ [/ 13/ ]/ ,/ 是/ MapReduce/ 程序/ 中/ 最具/ 代表性/ 的/ 应用/ 例子/ ./ 在/ Grep/ 作业/ 中/ ,/ 系统/ Page5/ 必须/ 从/ 一个/ 数据/ 集合/ 中/ 扫描/ 出/ 一个/ 包含/ 三个/ 字母/ 的/ 样式/ ,/ 数据/ 集合/ 由/ 大量/ 长度/ 为/ 100/ 字节/ 的/ 记录/ 组成/ ./ 每条/ 记录/ 中前/ 10/ 个/ 字节/ 是/ 一个/ 唯一/ 的/ 标识/ ,/ 后面/ 90/ 字节/ 是/ 随机/ 的/ 值/ ./ 样式/ 匹配/ 查找/ 在/ 后/ 90/ 字节/ 中/ 进行/ ,/ 平均/ 每/ 10000/ 条/ 记录/ 中/ 才能/ 查找/ 到/ 一次/ ./ 因此/ 本文/ 设计/ 了/ Grep/ 表来/ 支持/ 这种/ 查询/ ,/ 表/ 结构/ 如图/ 1/ 所示/ ./ 本文/ 还/ 设计/ 了/ 另外/ 两个/ 数据库/ 表/ 结构/ (/ 图/ 1/ )/ ,/ 用来/ 保存/ 一个/ Web/ 网站/ 的/ 日志/ 信息/ ,/ 其中/ UserVisits/ 表/ 的/ destURL/ 字段/ 参照/ 了/ Rankings/ 表/ 的/ pageURL/ 字段/ ,/ 由于/ 大多数/ 云/ 数据库/ 不/ 支持/ 外键/ 连接/ ,/ 因此/ 我们/ 没有/ 建立/ 表/ Rankings/ 和/ UserVisits/ 之间/ 的/ 连接/ ./ 我们/ 采用/ 随机/ 算法/ 生成/ 数据/ (/ 服从/ 平均/ 分布/ )/ ,/ 表/ 1/ 列出/ 了/ 上述/ 数据库/ 表中/ 每个/ 字段/ 的/ 数据/ 生成/ 规则/ ./ 其中/ Grep/ 和/ UserVisits/ 的/ 数据量/ (/ 条/ )/ 为/ 1/ 亿/ (/ 约/ 30GB/ )/ ;/ Rankings/ 表/ 的/ 数据量/ 是/ UserVisits/ 表/ 的/ 十分之一/ ;/ 数据/ 复制/ 因子/ 为/ 3/ ./ 表字/ 段/ 生成/ 规则/ (/ 随机数/ 服从/ 平均/ 分布/ )/ GrepfieldRankingsUserVisits4/ ./ 2/ 测试用例/ 本/ 小节/ 描述/ 基准/ 测试用例/ ,/ 基准/ 测试用例/ 包括/ 数据/ 的/ 装载/ 、/ 查询/ 和/ 分析/ 用例/ ,/ 每个/ 用例/ 都/ 有/ 两个/ 参数/ 来/ 限定/ 其/ 执行/ 环境/ 和/ 负载/ ,/ 即/ 节点/ 数量/ 和/ 数据量/ ./ 本节/ 还/ 介绍/ 用例/ 在/ 云/ 数据/ 管理系统/ 上/ 的/ 实现/ 方法/ ./ 部分/ 基于/ Key/ -/ Value/ 数据模型/ 的/ 云/ 数据/ 管理系统/ 尚/ 不/ 支持/ 标准/ SQL/ ,/ 也/ 不/ 显式/ 地/ 支持/ 分组/ 、/ 聚集/ 和表/ 连接/ 等/ 较为/ 复杂/ 的/ 关系/ 操作/ ,/ 因此/ 需要/ 通过/ 编程/ 实现/ ./ 本节/ 将/ 着重/ 描述/ 测试用例/ 的/ HBase/ 实现/ 方式/ 以及/ MapReduce/ 算法/ ,/ 其它/ 非/ 关系数据/ 管理系统/ 与/ 此/ 类似/ ./ 4.2/ ./ 1/ 数据/ 装载/ (/ Loading/ )/ 数据/ 装载/ 测试用例/ 定义/ 为/ “/ 以/ 不同/ 线程/ 数目/ 并行/ 地/ 对/ Grep/ 、/ Rankings/ 和/ UserVisits/ 表/ 加载/ 数据/ ”/ ./ HBase/ 装载/ 数据/ 的/ 方式/ 有/ 3/ 种/ :/ (/ 1/ )/ 直接/ 使用/ HBase/ 提供/ 的/ API/ 逐条/ 数据/ 插入/ ;/ (/ 2/ )/ 利用/ MapReduce/ 程序/ ,/ 从/ 现有/ 分布式文件系统/ (/ 如/ HDFS/ )/ 中/ 的/ 文件/ 导入/ 至/ HBase/ ;/ (/ 3/ )/ 把/ 数据/ 写入/ HFile/ 格式文件/ ,/ 然后/ 由/ HBase/ 自动/ 从/ HFile/ 中/ 加载/ 数据/ ./ 本用例/ 由于/ 测试数据/ 随机/ 生成/ ,/ 因此/ 无法/ 利用/ MapReduce/ 从/ 文件系统/ 中/ 加载/ 数据/ ./ 文献/ [/ 3/ ]/ 在/ 研究/ HDFS/ 数据/ 加载/ 时/ 采用/ 先生/ 成/ 数据文件/ 然后/ 加载/ 至/ HDFS/ 的/ 方式/ ,/ 文献/ [/ 3/ ]/ 的/ 性能/ 数据/ 假设/ 数据文件/ 已经/ 存在/ ,/ 忽略/ 了/ 生成/ 文件/ 的/ 时间/ ./ 本/ 研究/ 中/ ,/ 我们/ 实现/ 了/ 三种/ 数据/ 加载/ 方式/ ,/ 发现/ 第一种/ 效率/ 最高/ ,/ 因为/ 后/ 两种/ 方法/ 都/ 需要/ 生成/ 临时/ 的/ 数据文件/ ,/ I/ // O/ 读写/ 将/ 是/ 一个/ 很大/ 的/ 开销/ ,/ 因此/ 我们/ 采用/ 直接/ 装载/ 的/ 方式/ ./ 4.2/ ./ 2Grep/ 运算/ (/ Grep/ )/ 模糊/ 查询/ 用例/ 执行/ 如下/ SQL/ 描述/ 的/ 功能/ :/ SELECT/ / FROMGrepWHEREfieldLIKE/ ‘/ %/ XYZ/ %/ ’/ 为/ 满足/ 上述/ 查询/ 的/ 命中率/ 为/ 万分之一/ ,/ 我们/ 定义/ Grep/ 表/ 的/ field/ 字段/ 每条/ 记录/ 的/ 每个/ 字符/ 都/ 从/ 95/ 个/ 不同/ 的/ 字符/ (/ ASCII/ 码/ 32/ ~/ 126/ ,/ 键盘/ 字符/ )/ 中/ 随机/ 选择/ ,/ field/ 字/ 段长度/ 为/ 90/ 字符/ ,/ 因此/ 每条/ 记录/ 包含/ 88/ 个/ 长度/ 为/ 3/ 的/ 子串/ ./ 长度/ 为/ 3/ 的/ 字符串/ 是/ “/ XYZ/ ”/ (/ “/ XYZ/ ”/ 可以/ 用/ 任意/ 三个/ 键盘/ 字符/ 替换/ )/ 的/ 概率/ 为/ 953/ 分/ 之一/ ,/ 因此/ 上述/ 查询/ 的/ 命中率/ 约/ 为/ 953/ // 88/ 分/ 之一/ ,/ 基本/ 满足要求/ ./ 此外/ ,/ 基准/ 测试/ 的/ 查询/ 条件/ 需要/ 动态变化/ ,/ 每个/ 请求/ 的/ 查询/ 条件/ 三个/ 字符/ 也/ 是从/ ASCII/ 码/ 32/ ~/ 126/ 中/ 随机/ 选取/ 的/ ,/ 避免/ 了/ 多次/ 测试/ 或/ 并发/ 请求/ 环境/ 下/ 同一/ 查询/ 反复/ 执行/ ,/ 且/ 保证/ 了/ 查询/ 命中率/ 基本一致/ ./ HBase/ 能够/ 很/ 好/ 地/ 完成/ 基于/ MapReduce/ 的/ 查/ Page6/ 询/ 功能/ ./ 每个/ 节点/ 的/ Map/ 实例/ 遍历/ 数据/ 集/ ,/ 对/ 每条/ 数据/ 进行/ 字符串/ 匹配/ 检测/ ,/ 输出/ 满足条件/ 的/ 结果/ ./ 由于/ 不/ 存在/ 进一步/ 运算/ ,/ 因此/ 不/ 需要/ Reduce/ 函数/ ,/ Map/ 方法/ 的/ 输出/ 即/ 为/ 程序/ 最终/ 的/ 输出/ ./ 4.2/ ./ 3/ 查询/ 运算/ (/ Selection/ )/ 范围/ 查询/ 用例/ 执行/ 如下/ SQL/ 描述/ 的/ 功能/ :/ SELECTpageURLFROMRankingsWHEREpageRank/ >/ 10/ 范围/ 查询/ 是/ 一个/ 轻量级/ 的/ 过滤器/ ,/ 它/ 从/ Rankings/ 表中/ 找出/ pageRank/ 高于/ 用户/ 定义/ 的/ 阈值/ 的/ pageURL/ ./ 测试用例/ 同样/ 使用/ 动态/ 的/ 查询/ 条件/ ,/ 将/ 其设/ 为/ 9.5/ ~/ 10.5/ 之间/ 的/ 一个/ 随机/ 小数/ ,/ 由于/ pageRank/ 是/ 一个/ 0/ ~/ 100/ 之间/ 的/ 一个/ 随机数/ ,/ 因此/ 查询/ 命中率/ 近似/ 为/ 90/ %/ ./ 4.2/ ./ 4/ 聚合/ 运算/ (/ Aggregation/ )/ 聚合/ 运算/ 用例/ 执行/ 如下/ SQL/ 描述/ 的/ 功能/ :/ SELECTsourceIP/ ,/ SUM/ (/ adRevenue/ )/ FROMUserVisitsGROUPBYsourceIP/ ;/ 聚合/ 运算/ 从/ UserVisits/ 表中/ 计算/ 出/ 每个/ sourceIP/ 所/ 产生/ 的/ adRevenue/ (/ 广告/ 收入/ )/ 之/ 和/ ./ 设计/ 该/ 测试用例/ 是/ 为了/ 度量/ 针对/ 单个/ 数据表/ 的/ 并行/ 分析/ 能耗/ ./ 由于/ 有/ 65536/ 种/ sourceIP/ (/ 参见/ 表/ 1/ )/ ,/ 查询/ 返回/ 65536/ 条/ 分组/ 记录/ ./ HBase/ 提供/ 的/ API/ 不能/ 自动/ 地/ 支持/ 分组/ 和/ 求和/ 操作/ ./ 我们/ 通过/ 一个/ MapReduce/ 程序/ 完成/ 该/ 用例/ ./ Map/ 方法/ 输入/ 每条/ UserVisits/ 记录/ ,/ 输出/ 以/ sourceIP/ 为键/ ,/ adRevenue/ 为值/ 的/ 中间/ 结果/ ./ Reduce/ 方法/ 将/ 每个/ sourceIP/ 关联/ 的/ adRevenue/ 累/ 加在一起/ ,/ 并/ 输出/ 以/ sourceIP/ 为键/ ,/ sumAdRevenue/ 为值/ 的/ 结果/ ./ 算法/ 1/ ./ 聚合/ 运算/ 测试用例/ 的/ MapReduce/ 实现/ ./ 1/ ./ Map/ (/ KeyIn2/ ./ Write/ (/ row/ ./ sourceIPasKeyOut3/ ./ }/ 4/ ./ Reduce/ (/ KeyIn5/ ./ Doublesum/ =/ 0.0/ ;/ 6/ ./ FOREACHadRevenueINadRevenueArray/ ;/ 7/ ./ sum/ +/ =/ adRevenue/ ;/ 8/ ./ ENDFOR9/ ./ Write/ (/ ipasKeyOut10/ ./ }/ 4.2/ ./ 5/ 连接/ 运算/ (/ Join/ )/ 连接/ 运算/ 用例/ 是/ 数据库/ 常用/ 操作/ 的/ 集合/ ,/ 包括/ 一次/ 范围/ 查询/ 、/ 一次/ 表/ 连接/ 和/ 一次/ 分组/ 聚集/ ./ 测试用例/ 执行/ 如下/ SQL/ 描述/ 的/ 功能/ :/ SELECTINTOResultssourceIP/ ,/ AVG/ (/ pageRank/ )/ asvgPageRank/ ,/ SUM/ (/ adRevenue/ )/ astotalRevenueFROMRankingsASR/ ,/ UserVisitsASUVWHERER/ ./ pageURL/ =/ UV/ ./ destURLANDUV/ ./ visitDateBETWEENDate/ (/ ‘/ 2005/ -/ 01/ -/ 01/ ’/ )/ ANDDate/ (/ ‘/ 2006/ -/ 01/ -/ 01/ ’/ )/ GROUPBYUV/ ./ sourceIP/ ;/ Join/ 作业/ 包含/ 了/ 四个/ 子/ 任务/ ,/ 在/ 两个/ 数据/ 集上/ 执行/ 复杂/ 的/ 计算/ ./ 第一个/ 子/ 任务/ 在/ UserVisits/ 表中/ 查找/ 特定/ 时间/ 范围/ 内/ 的/ sourceIP/ ./ 我们/ 在/ 实验/ 中/ 使用/ 了/ 随机/ 的/ 一年/ 间隔/ 作为/ 查询/ 范围/ ,/ 在/ UserVisits/ 表中/ visitDate/ 字段/ 为/ 2000/ 年/ ~/ 2010/ 年间/ 的/ 随机/ 时间/ ,/ 因此/ ,/ 选中/ 的/ 数据/ 占/ UserVisits/ 总/ 数据量/ 的/ 10/ %/ ;/ 第二个/ 子/ 任务/ 连接/ UserVisits/ 和/ Rankings/ 表/ ;/ 第三个/ 子/ 任务/ 按照/ sourceIP/ 对/ Rankings/ 表/ 的/ pageRank/ 分组/ 求/ 平均/ ,/ 对/ UserVisits/ 的/ adRevenue/ 字段/ 分组/ 求和/ ;/ 最后/ 一个/ 子/ 任务/ 把/ 按/ sourceIP/ 的/ 统计/ 结果/ 写入/ Results/ 表/ ./ 上述/ 任务/ 最/ 突出/ 的/ 是/ 表/ 连接/ 操作/ ,/ HBase/ 没有/ 提供/ 连接/ 查询/ 的/ API/ ,/ 也/ 不/ 支持/ 在/ 稀疏/ 表上/ 创建/ 外键/ ,/ 我们/ 将/ 用/ MapReduce/ 实现/ 这一/ 过程/ ./ 使用/ MapReduce/ 完成/ 表/ 连接/ 的/ 方法/ 有/ 很/ 多种/ ,/ 如/ MapSideJoin/ ,/ ReduceSideJoin/ ,/ SemiJoin/ ,/ DistributedHashJoin/ 和/ BloomFilterbasedJoin/ 等/ [/ 29/ -/ 32/ ]/ ./ 其中/ ReduceSideJoin/ 是/ 一种/ 最/ 简单/ 的/ 连接/ 方式/ ,/ Map/ 函数/ 同时/ 读取/ 主表/ 和/ 从表/ ,/ 并/ 为/ 每条/ 数据/ 打/ 一个/ 标签/ (/ Tag/ )/ 以/ 区别/ 该/ 数据/ 的/ 来源/ ;/ Reduce/ 函数/ 获取/ Key/ 相同/ 的/ 来/ 自主/ 表和/ 从表/ 文件/ 的/ 数据/ 并/ 进行/ 连接/ (/ 笛卡尔/ 乘积/ )/ ./ 文献/ [/ 3/ ]/ 在/ HDFS/ 上/ 使用/ 三次/ MapReduce/ 完成/ 该/ 用例/ ,/ 相比而言/ ,/ 我们/ 在/ HBase/ 上/ 优化/ 为/ 两次/ MapReduce/ ./ 这/ 两个/ 阶段/ 分别/ 是/ 连接/ 阶段/ 和/ 聚集/ 阶段/ ,/ 前/ 一/ 阶段/ 的/ Reduce/ 函数/ 输出/ 是/ 后/ 一/ 阶段/ Map/ 函数/ 的/ 输入/ ./ 其中/ 聚集/ 阶段/ 同/ 聚合/ 运算/ 测试用例/ (/ 算法/ 1/ )/ 类似/ ,/ 仅/ 需要/ 考虑/ 区别/ 输入/ 数据/ 哪些/ 是/ pageRank/ 列/ 数据/ ,/ 哪些/ 是/ adRevenue/ 列/ 数据/ ,/ 对/ 不同/ 的/ 列/ 聚集/ 算法/ 不同/ (/ pageRank/ 为求/ 平均数/ ,/ adRevenue/ 为/ 求和/ )/ ,/ 且/ Reduce/ 函数/ 输出/ 到/ Results/ 表中/ ./ 我们/ 着重/ 阐述/ 连接/ 阶段/ 的/ MapReduce/ 操作/ ./ 具体/ 算法/ 如/ 算法/ 2/ 所述/ ./ 算法/ 2/ ./ 连接/ 运算/ 测试用例/ 的/ 连接/ 阶段/ MapReduce/ 实现/ ./ 1/ ./ Map/ (/ KeyIn2/ ./ IF/ (/ rowisRanking/ )/ 3/ ./ Write/ (/ row/ ./ pageURLasKeyOutPage74/ ./ ENDIF5/ ./ IF/ (/ rowisUserVisits/ )/ 6/ ./ IF/ (/ row/ ./ visitDataebetweenbegin/ _/ dateand7/ ./ pair/ =/ newPair/ (/ row/ ./ sourceIP/ ,/ 8/ ./ Write/ (/ row/ ./ destURLasKeyOut9/ ./ ENDIF10/ ./ ENDIF11/ ./ }/ 12/ ./ Reduce/ (/ KeyIn13/ ./ SetipSet/ ;/ 14/ ./ SetpageRankSet/ ;/ 15/ ./ FOREACHelementINarrays16/ ./ IF/ (/ elementisUserVisits/ )/ 17/ ./ ipSet/ ./ add/ (/ element/ ./ sourceIP/ )/ ;/ 18/ ./ Write/ (/ element/ ./ sourceIPasKeyOut19/ ./ ENDIF20/ ./ ELSEIF/ (/ elementisRanking/ )/ 21/ ./ pageRankSet/ ./ add/ (/ elementaspageRank/ )/ 22/ ./ ENDELSE23/ ./ ENDFOR24/ ./ FOREACHipINipSet25/ ./ FOREACHpageRankINpageRankSet26/ ./ Write/ (/ ipasKeyOut27/ ./ ENDFOR28/ ./ ENDFOR29/ ./ }/ 连接/ 阶段/ 的/ Map/ 函数/ 输入/ Rankings/ 表/ 的/ 所有/ 数据/ 和/ UserVisits/ 表/ 的/ 所有/ 数据/ ,/ Map/ 函数/ 对/ 每/ 一条/ 数据/ 进行/ 判断/ ,/ 如果/ 是/ UserVisits/ 记录/ ,/ 则/ 对/ 其/ 按/ visitDate/ 进行/ 过滤/ ;/ 如果/ 在/ 给定日期/ 范围/ 内/ ,/ 则/ 输出/ 以/ destURL/ 为键/ ,/ sourceIP/ 和/ adRevenue/ 组成/ 的/ 二元/ 组为值/ 的/ 中间/ 结果/ ./ 如果/ 是/ Rankings/ 记录/ ,/ 则/ 直接/ 输出/ 以/ pageURL/ 为键/ ,/ 以/ pageRank/ 为值/ 的/ 中间/ 结果/ ./ 连接/ 阶段/ 的/ Reduce/ 函数/ 输入/ 以/ URL/ 为键/ ,/ 由/ (/ sourceIP/ ,/ adRevenue/ )/ 二元/ 组/ 或/ pageRank/ 组成/ 的/ 集合/ 为值/ 的/ 数据/ ./ Reduce/ 函数/ 对/ 其/ 进行/ 约简/ ,/ 输出/ 以/ sourceIP/ 为键/ ,/ adRevenue/ 或/ pageRank/ 为值/ 的/ 结果/ ./ 4.3/ 能耗/ 模型/ 对云/ 数据/ 管理系统/ 的/ 能耗/ 进行/ 基准/ 测试/ 和/ 分析/ 比较/ ,/ 首先/ 要/ 定义/ 能耗/ 的/ 度量/ 模型/ ./ 能耗/ 的/ 度量/ 对象/ 归根结底/ 包括/ 三/ 部分/ :/ 计算机/ 、/ 网络/ 和/ 空调/ 等/ 辅助/ 设备/ ./ 关于/ 网络/ 的/ 能耗/ 模型/ ,/ 已有/ 很多/ 相关/ 研究/ ,/ 如/ 文献/ [/ 33/ ]/ 提出/ 的/ 一种/ 基于/ 随机/ 模型/ 的/ 绿色/ 评价/ 框架/ ./ 本文/ 仅/ 考虑/ 计算机/ 能耗/ ,/ 网络设备/ 和/ 空调/ 等/ 辅助/ 设备/ 的/ 能耗/ 本文/ 暂/ 不/ 涉及/ ./ 基于/ 前/ 文/ 定义/ 的/ 基准/ 测试用例/ ,/ 我们/ 采用/ 基准/ 能耗/ 来/ 衡量/ 云/ 数据/ 管理系统/ 针对/ 上述/ 用例/ 的/ 能耗/ ./ 每次/ 数据库/ 查询/ 作业/ 可以/ 看作/ 一个/ 事务/ (/ Transaction/ )/ ,/ 基准/ 能耗/ 是/ 特定/ 环境/ 下云/ 数据/ 管理系统/ 完成/ 一个/ 事务时/ 的/ 能耗/ ./ 定义/ 1/ ./ 基准/ 能耗/ ./ 基准/ 能耗/ 是/ 云/ 数据/ 管理系统/ 在/ 特定/ 执行/ 环境/ 下/ (/ 如/ 节点/ 数量/ 、/ 数据量/ 、/ 数据/ 复制/ 份数/ 等/ )/ ,/ 平均/ 完成/ 某个/ 基准/ 测试用例/ 消耗/ 的/ 能量/ ./ 云/ 数据/ 管理系统/ 中/ 的/ 每个/ 计算机/ 节点/ 都/ 有/ 额定/ 的/ 功率/ ,/ 但/ 文献/ [/ 34/ ]/ 证实/ 了/ 计算机/ 功率/ 是/ 动态变化/ 的/ ,/ 与/ 计算机/ 的/ 繁忙/ 程度/ 有关/ ,/ 当/ 计算机/ 空闲/ 时/ ,/ 功率较/ 低/ ,/ 反之亦然/ ./ 因此/ 不能/ 简单/ 地/ 使用/ 用例/ 执行/ 时间/ 与/ 额定功率/ 的/ 乘积/ 来/ 计算/ 基准/ 能耗/ ./ 对于/ 云/ 数据/ 管理系统/ ,/ 设/ 存在/ N/ 个/ 节点/ ,/ 每个/ 记为/ ci/ (/ 1/ / i/ / N/ )/ ,/ 对于/ ci/ 节点/ 功率/ 为/ pi/ (/ t/ )/ (/ 单位/ 为/ 瓦特/ )/ ,/ 在/ T/ 时间/ 内/ 完成/ 某/ 同一/ 基准/ 测试用例/ 的/ M/ 次/ 请求/ ,/ 则/ 基准/ 能耗/ 的/ 计算方法/ 为/ 5/ 能耗/ 的/ 分析/ 和/ 测量/ 本/ 节/ 介绍/ 能耗/ 分析方法/ 、/ 测量方法/ 和/ 实验/ 环境/ ./ 5.1/ 能耗/ 分析方法/ 基准/ 能耗/ 能够/ 清晰/ 地/ 度量/ 云/ 数据/ 管理系统/ 的/ 能耗/ ,/ 但/ 粒度/ 过大/ ,/ 过于/ 笼统/ ./ 为/ 确定/ 云/ 数据/ 管理系统/ 的/ 能耗/ 规律/ 和/ 能耗/ 优化/ 的/ 目标/ ,/ 我们/ 对/ 基准/ 能耗/ 做/ 进一步/ 的/ 分析/ ./ 目前/ 能耗/ 优化/ 主要/ 有/ 两种/ 思路/ ,/ 一是/ 开发/ 更加/ 节能/ 的/ 硬件/ 设备/ ,/ 二是/ 关闭/ 空闲/ 设备/ ./ 而/ 本文/ 则/ 结合/ 云/ 数据/ 管理系统/ 的/ 特点/ ,/ 提出/ 能耗/ 优化/ 的/ 第三个/ 思路/ :/ 减少/ “/ 等待/ 能耗/ ”/ ./ 定义/ 2/ ./ 等待/ 能耗/ ./ 在/ 集群/ 环境/ 中/ ,/ 计算机/ 节点/ (/ 或/ 计算机/ 某/ 组件/ )/ 因/ 等待/ 其它/ 资源/ 而/ 处于/ “/ 被动/ 空闲/ ”/ 的/ 时间段/ 内/ 消耗/ 的/ 能量/ 称为/ 等待/ 能耗/ ./ 例如/ ,/ 当/ 集群/ 中/ 某个/ 节点/ 在/ 等/ 其它/ 节点/ 的/ 运算/ 结果/ 时/ ,/ 节点/ 产生/ 等待/ 能耗/ ,/ 该/ 节点/ 并非/ 真正/ 意义/ 上/ 空闲/ ,/ 通常/ 无法/ 关闭/ 该/ 节点/ 以/ 节能/ ,/ 因此/ 我们/ 应该/ 尽量减少/ 节点/ 对/ 资源/ 等/ 的/ 等待/ ,/ 以/ 减少/ 等待/ 能耗/ ./ 同理/ ,/ 对于/ 一个/ 计算机/ 内部/ 的/ 各个/ 组成部分/ ,/ 如/ CPU/ 运算/ 也/ 会/ 因/ 等待/ I/ // O/ 操作/ 而/ 阻塞/ ,/ 产生/ 等待/ 能耗/ ./ 等待/ 能耗/ 的/ 定义/ 和/ 分析/ 符合/ 云/ 数据/ 管理系统/ 的/ Page8/ 特点/ ./ 首先/ ,/ 云/ 数据/ 管理系统/ 是/ 一种/ 分布式/ 的/ 集群/ 系统/ ,/ 作业/ 在/ 各个/ 节点/ 间/ 调度/ 执行/ ,/ 若/ 作业/ 调度/ 策略/ 不合理/ ,/ 会/ 导致/ 部分/ 节点/ 等待/ 作业/ 而/ 空闲/ ;/ 其次/ ,/ MapReduce/ 模型/ 把/ 作业/ 分解成/ 任务/ (/ 如/ Map/ 任务/ 和/ Reduce/ 任务/ )/ 并/ 部署/ 到/ 每个/ 节点/ 并行/ 的/ 执行/ ,/ 由于/ 数据分布/ 特点/ 和/ 任务/ 执行/ 的/ 特点/ ,/ 很/ 可能/ 会因/ 任务/ 结束/ 时间/ 不/ 同步/ 而/ 造成/ 部分/ 节点/ 等待/ (/ 如/ Reduce/ 任务/ 需要/ 等待/ 所有/ Map/ 任务/ 执行/ 结束/ )/ ;/ 最后/ ,/ 云/ 数据/ 管理系统/ 执行/ 的/ 大部分/ 是/ I/ // O/ 密集型/ 运算/ ,/ 对于/ 每个/ 节点/ ,/ CPU/ 的/ 执行/ 速度/ 要/ 远大于/ 本地/ I/ // O/ 和/ 网络/ I/ // O/ 的/ 读写/ 速度/ ,/ CPU/ 易因/ 等待/ I/ // O/ 操作/ 而/ 产生/ 等待/ 能耗/ ./ 计算机/ 的/ 能耗/ 主要/ 包括/ CPU/ 能耗/ 、/ 硬盘/ 能耗/ 、/ 内存/ 能耗/ 、/ 显卡/ 能耗/ 、/ 网卡/ 能耗/ 、/ 主板/ 能耗/ 以及/ 其它/ 附属/ 设备/ 的/ 能耗/ (/ 对于/ 云/ 数据/ 管理系统/ 的/ 每个/ 节点/ ,/ 我们/ 不/ 考虑/ 显示器/ ,/ 键盘/ 鼠标/ 等/ 人机接口/ 设备/ 的/ 能耗/ )/ ,/ 其中/ 以/ CPU/ 能耗/ 为主/ ,/ 占/ 节点/ 能耗/ 的/ 70/ %/ ./ 此外/ ,/ 节点/ 在/ “/ 等待/ 调度/ ”/ 、/ “/ 等待/ 其它/ 节点/ ”/ 或/ “/ 等待/ I/ // O/ 操作/ ”/ 时其/ CPU/ 也/ 处于/ 低/ 使用率/ 的/ 等待/ 状态/ ./ 因此/ ,/ 考虑/ CPU/ 等待/ 能耗/ 等同于/ 考虑/ 节点/ 等待/ 能耗/ ./ 我们/ 定义/ 如表/ 2/ 所述/ 的/ 若干/ 特征/ 属性/ 来/ 观察/ 各/ 系统/ 在/ 各/ 测试用例/ 执行/ 时/ 的/ CPU/ 等待/ 能耗/ 特征/ ./ 其中/ CPU/ 使用率/ 、/ 磁盘/ I/ // O/ 读写/ 速率/ 、/ 内存/ 的/ 使用率/ 以及/ 网络通信/ 量/ 分别/ 代表/ 了/ CPU/ 、/ 硬盘/ 、/ 内存/ 以及/ 网卡/ 繁忙/ 程度/ ./ 特征/ 属性/ 单位/ CbusyDr/ // sByte/ 硬盘/ 每秒/ 读入/ 数据量/ Dw/ // sByte/ 硬盘/ 每秒/ 写入/ 数据量/ MusedNin/ // sByte/ 网卡/ 每秒/ 接收/ 的/ 数据量/ Nout/ // sByte/ 网卡/ 每秒/ 发送/ 的/ 数据量/ 5.2/ 能耗/ 测量方法/ 测量/ E/ (/ T/ )/ 的/ 值/ 的/ 方法/ 有/ 很/ 多种/ ./ 所有/ 计算机/ 都/ 有/ 额定功率/ ,/ 但/ 实际/ 功率/ 却是/ 动态变化/ 的/ ,/ 因此/ 很难/ 找到/ pi/ (/ t/ )/ 的/ 数学/ 表达/ ,/ 并/ 按照/ 式/ (/ 1/ )/ 计算/ 能耗/ E/ (/ T/ )/ ./ 我们/ 使用/ 功率/ 计/ (/ PowerMeter/ )/ 每隔/ Δ/ t/ 时间/ 对/ 节点/ 进行/ 一次/ 实时/ 功率/ 的/ 采集/ ,/ 根据/ 积分/ 的/ 定义/ :/ E/ (/ T/ )/ =/ ∑/ N/ 当/ Δ/ t/ 足够/ 小时/ ,/ 式/ (/ 2/ )/ 的/ 值/ 即/ 为/ E/ (/ T/ )/ 的/ 值/ ./ 但/ 大部分/ 计算机设备/ 或/ 操作系统/ 都/ 未/ 提供/ 实时/ 功率/ 的/ 测量/ 接口/ ,/ 测量/ pi/ (/ tj/ )/ 存在/ 一定/ 难度/ ,/ 需为/ 云内/ 每个/ 计算机/ 节点/ 安装/ 功率/ 计/ ,/ 并且/ 控制/ 他们/ 之间/ 的/ 通信/ (/ 时钟/ 同步/ ,/ 数据/ 汇总/ )/ ./ 我们/ 选用/ 的/ 功率/ 测量/ 设备/ 可以/ 通过/ USB/ 数据线/ 与/ 远程/ 计算机/ 通信/ ,/ 每隔/ 1.5/ s/ ~/ 3s/ (/ 受限于/ 采集/ 设备/ )/ 采集/ 一次/ 实时/ 功率/ ,/ 采集/ 间隔/ 不/ 等/ ./ 此外/ ,/ 除/ 按式/ (/ 2/ )/ 的/ 方式/ 计算/ ,/ 还/ 可以/ 通过/ 电力/ 监测仪/ 直接/ 测量/ T/ 时间/ 经过/ 电缆线/ 的/ 电量/ 即/ 为/ 整个/ 云/ 计算/ 系统/ 的/ 整体/ 能耗/ 值/ ,/ 但/ 此时/ 要/ 考虑/ 仪器/ 的/ 最大/ 功率/ 测量范围/ ./ 主流/ 操作系统/ 都/ 提供/ 了/ 对/ 系统资源/ 进行/ 监控/ 的/ 接口/ ,/ 因此/ 特征/ 属性/ 的/ 测量/ 均/ 不/ 需要/ 特殊/ 的/ 仪器/ ,/ 也/ 不/ 依赖于/ 特定/ 的/ 云/ 数据/ 管理系统/ 种类/ ./ 我们/ 在/ 云/ 系统/ 中/ 每个/ 节点/ 上/ 部署/ 监控/ 代理/ (/ 软件/ )/ ,/ 设置/ 代理/ 在/ T/ 时间/ 内/ 每隔/ Δ/ t/ 时间/ 对/ 节点/ 进行/ 一次/ 特征/ 属性/ 值/ 的/ 采样/ ,/ 云/ 计算/ 中/ 分布式/ 的/ 编程/ 环境/ 可以/ 很/ 容易/ 地/ 同步/ 这些/ 监控/ 代理/ 并/ 汇总/ 数据/ ./ 5.3/ 实验/ 环境/ 我们/ 使用/ 12/ 台/ 高档/ 微机/ 搭建/ 了/ 一个/ 集群/ 环境/ ./ HBase/ 、/ Hive/ 、/ HadoopDB/ 、/ MongoDB/ 和/ GridSQL/ 都/ 为主/ 从/ 结构/ ,/ 其中/ 1/ 台/ 节点/ 作为/ 主/ 节点/ ,/ 11/ 台/ 节点/ 作为/ 从/ 节点/ ./ 而/ Cassandra/ 为/ 环状/ 结构/ ,/ 12/ 台/ 节点均/ 为/ 数据/ 节点/ ./ 实验/ 环境/ 细节/ 如表/ 3/ 所述/ ./ 节点/ 操作系统/ CentOS5/ ./ 6/ ,/ Linux2/ ./ 6.18/ 内核/ 云/ 数据/ 管理系统/ 编程/ 环境/ JavaSE6/ 功率/ 计/ 特征/ 属性/ 频率/ Δ/ t1s/ 采集/ 1/ 次/ 数据/ (/ Δ/ t/ =/ 1s/ )/ 测量/ 单位/ 并发/ 性/ 数据量/ Page96/ 基准/ 能耗/ 比较/ 与/ 分析/ 我们/ 逐一/ 考察/ 了/ HBase/ 、/ Cassandra/ 、/ Hive/ 、/ MongoDB/ 、/ HadoopDB/ 和/ GridSQL/ 的/ 能耗/ 特性/ ,/ 表/ 4/ 对比/ 了/ 上述/ 几个/ 云/ 数据/ 管理系统/ 的/ 特点/ ./ 名称/ 数据/ 存储/ 数据模型/ 编程/ 模型/ 支持/ SQLHBase/ 分布式文件系统/ Key/ -/ ValueMapReduce/ 否/ Cassandra/ 本地/ 文件系统/ Key/ -/ ValueMapReduce/ 否/ Hive/ 分布式文件系统/ 关系/ 型/ MapReduce/ 是/ MongoDB/ 本地/ 文件系统/ Key/ -/ ValueMapReduce/ 否/ HadoopDB/ 关系/ 型/ 数据库/ 关系/ 型/ MapReduce/ 是/ GridSQL/ 关系/ 型/ 数据库/ 关系/ 型/ 并行算法/ 是从/ 实现/ 技术/ 考虑/ ,/ HBase/ 和/ Cassandra/ 较为/ 相似/ ,/ Hive/ 和/ MongoDB/ 较为/ 相似/ ,/ HadoopDB/ 和/ GridSQL/ 较为/ 相似/ ./ 6.1/ 基准/ 能耗/ 比较/ 本/ 节/ 比较/ HBase/ 、/ Cassandra/ 、/ Hive/ 、/ MongoDB/ 、/ HadoopDB/ 和/ GridSQL/ 的/ 基准/ 能耗/ ./ 为了/ 表述/ 简单/ ,/ Loading/ 用例/ 采用/ “/ 20/ 并发/ 线程/ 下/ 对/ Grep/ 表/ 装载/ 100/ 万/ 记录/ ”/ ./ HadoopDB/ 的/ 数据/ 装载/ 较为/ 复杂/ ,/ 需要/ 手工/ 完成/ ,/ 而/ Hive/ 的/ 数据/ 装载/ 实为/ 把/ 数据文件/ 纳入/ 分布式文件系统/ HDFS/ ,/ 因此/ 我们/ 没有/ 比较/ 这/ 两种/ 系统/ 的/ 装载/ 用例/ 能耗/ ./ 每种/ 系统/ 执行/ 不同/ 基准/ 测试用例/ 的/ 能耗/ 和/ 效率/ 对/ 比如/ 图/ 2/ (/ 对数/ 坐标轴/ )/ 所示/ ./ 图/ 2/ 云/ 数据/ 管理系统/ 在/ 不同/ 测试用例/ 下/ 能耗/ 和/ 执行/ 时间/ 的/ 比较/ 图/ 2/ (/ a/ )/ 表明/ HBase/ 和/ Cassandra/ 的/ 装载/ 基准/ 能耗/ 要/ 明显/ 低于/ GridSQL/ ,/ 在/ 并发/ 装载/ 的/ 情况/ 下/ 优势/ 更为/ 明显/ ./ GridSQL/ 是/ 关系/ 型/ 数据库/ ,/ 数据类型/ 丰富/ ,/ 数据/ 约束/ 多/ ,/ 需要/ 对/ 数据/ 进行/ 大量/ 约束/ 检测/ 和/ 格式/ 转换/ ,/ HBase/ 和/ Cassandra/ 则/ 有着/ 简单/ 的/ 表/ 结构/ 和/ 单一/ 的/ 数据格式/ ,/ 对/ 每条/ 数据/ 标记/ 时间/ 戳/ 后/ 写入/ 文件/ 中/ ,/ 后/ 两者/ 装载/ 数据/ 更为/ 简单/ ,/ 需要/ 的/ 运算/ 更/ 少/ ,/ 能耗/ 更/ 低/ ./ 此外/ HBase/ 和/ Cassandra/ 数据/ 存在/ 复制/ 而/ GridSQL/ 没有/ 复制/ 机制/ ,/ 因此/ ,/ HBase/ 和/ Cassandra/ 写入/ 的/ 数据量/ 大于/ GridSQL/ ,/ 前/ 两者/ 实际/ 数据/ 写入/ 速度/ 更快/ ./ MongoDB/ 采用/ 两段式/ 数据/ 装载/ ,/ 第一阶段/ 将/ 数据/ 写入/ 节点/ ,/ 这个/ 阶段/ 的/ 能耗/ 很/ 低且/ 效率/ 很/ 高/ ;/ 第二阶段/ 是/ 节点/ 间点/ 自动/ 分发/ 并/ 平衡/ 数据/ 的/ 过程/ ,/ 这个/ 阶段/ 是/ 在/ 后台/ 运行/ 并/ 持续/ 较长/ 的/ 时间/ ,/ 其/ 开始/ 和/ 结束/ 时间/ 无法/ 测量/ ./ 图/ 2/ 数据/ 为/ 第一阶段/ 的/ 测量/ 值/ ,/ 因此/ 能耗/ 很/ 低/ ,/ 效率/ 很/ 高/ ./ 对于/ Grep/ 和/ Selection/ 两种/ 查询/ 用例/ ,/ GridSQL/ 耗能/ 很/ 低/ ,/ MongoDB/ 、/ HadoopDB/ 和/ Hive/ 次之/ ,/ 而/ HBase/ 和/ Cassandra/ 能耗/ 很/ 高/ ;/ 对于/ Aggregation/ 和/ Join/ 两种/ 分析/ 型/ 用例/ ,/ HBase/ 和/ Cassandra/ 的/ 能耗/ 要/ 远远/ 高于/ Hive/ 、/ HadoopDB/ 和/ GridSQL/ ./ 对于/ Aggregation/ 这种/ 单表/ 分组/ 聚合/ 操作/ ,/ 大部分/ 系统/ 的/ 基准/ 能耗/ 和/ 查询/ 用例/ 的/ 能耗/ 规律/ 一致/ ./ MongoDB/ 的/ Aggregation/ 用例/ 性能/ 要/ 优于/ Join/ 用例/ ,/ 这/ 是因为/ MongoDB/ 的/ Group/ 关键字/ 不/ 支持/ 大/ 数据量/ 运算/ ,/ 因此/ Aggregation/ 通过/ MapRe/ -/ duce/ 实现/ ,/ 其/ “/ 内存/ 映射/ 机制/ ”/ 不能/ 很/ 好/ 的/ 发挥/ ,/ 且/ 需要/ 对/ 全部/ 数据/ 进行/ 读取/ ,/ 磁盘/ I/ // O/ 增加/ ./ 对于/ 多表/ 连接/ 操作/ ,/ 由于/ 各/ 系统/ 的/ 实现/ 方法/ 不同/ ,/ Hadoop/ -/ DB/ 的/ 能耗/ 最低/ ,/ 性能/ 最优/ ./ 综合/ 上述/ 数据/ ,/ 基于/ 关系/ 型/ 数据库/ 和/ MapRedcue/ 并行/ 编程/ 模型/ 的/ HadoopDB/ 在/ 本/ 测试环境/ 中/ 能耗/ 最低/ ./ 我们/ 可以/ 通过/ 云/ 数据/ 管理系统/ 的/ 性能/ 差异/ 来/ 解释/ 上述/ 现象/ ./ 性能/ 直接/ 决定/ 用例/ 的/ 执行/ 时间/ ,/ 一般/ 情况/ ,/ 执行/ 时间/ 越长/ 则/ 能耗/ 自然/ 越高/ ,/ 然而/ 也/ 存在/ 特例/ ,/ 如/ 实验/ 用/ 计算机/ 的/ 有功/ 功率/ 在/ 50W/ ~/ 100W/ 之间/ 浮动/ ,/ 若该/ 计算机/ 以/ 最小/ 功率/ 运行/ 2s/ 和/ 以/ 最大/ 功率/ 运行/ 1s/ ,/ 能耗/ 相同/ ./ 因此/ ,/ 我们/ 不能/ 简单/ 地/ 认为/ 执行/ 时间/ 长/ 的/ 用例/ 能耗/ 一定/ 高/ ./ 但/ 在/ 本例/ 中/ ,/ 执行/ 时间/ 对/ 能耗/ 起到/ 决定/ 因素/ ./ 分析/ 数据/ 可/ 发现/ ,/ 各个系统/ “/ 平均功率/ ”/ (/ 能耗/ 除以/ 执行/ 时间/ )/ 基本一致/ ,/ 说明/ 功率/ 的/ 浮动/ 被/ 平均化/ ./ MapReduce/ 的/ 性能/ 问题/ 已经/ 备受/ 关注/ [/ 3/ -/ 6/ ]/ ,/ 本/ 研究/ 并/ 不/ 进一步/ 分析/ 系统/ 性能/ 因素/ ,/ 而/ 从/ 另外/ 一个/ 角度/ 分析/ 能耗/ 差异/ ./ 以/ HBase/ 和/ HadoopDB/ 为例/ ,/ 在/ 同样/ 的/ 实验/ 环境/ 和/ 执行/ 用例/ 下/ ,/ 经/ 推理/ ,/ 造成/ HBase/ 能耗/ 高于/ HadoopDB/ 的/ 原因/ 有/ 两个/ :/ 一是/ 算法/ 复杂度/ 高/ ,/ 完成/ 相同/ 的/ 用例/ ,/ HBase/ 需要/ 执行/ 更/ 多/ 的/ 运算/ ;/ 其二/ 是/ HBase/ 在/ 运算/ 过程/ 中/ 等待/ 能耗/ Page10/ 多/ ,/ 能源/ 效率/ 不高/ ./ 本/ 研究/ 并非/ 比较/ 各个/ 云/ 数据/ 管理系统/ 的/ 实现/ 算法/ 细节/ ,/ 因此/ 我们/ 将/ 针对/ 第二个/ 原因/ 做/ 进一步/ 分析/ ./ 6.2/ 等待/ 能耗/ 分析/ 本/ 节/ 分析/ 各个/ 云/ 数据/ 管理系统/ 的/ 等待/ 能耗/ (/ 定义/ 2/ )/ ./ 如/ 5.1/ 节/ 所述/ ,/ 我们/ 着重/ 研究/ CPU/ 的/ 等待/ 能耗/ ./ 部署/ 在/ 各个/ 节点/ 上/ 的/ 监控/ 软件/ 每隔/ 一秒钟/ 测量/ Cbusy/ (/ CPU/ 使用率/ )/ 、/ Dr/ // s/ (/ 硬盘/ 读/ 速率/ )/ 、/ Dw/ // s/ (/ 硬盘/ 写/ 速率/ )/ 、/ Nin/ // s/ (/ 网卡/ 读/ 速率/ )/ 、/ Nout/ // s/ (/ 网卡/ 写/ 速率/ )/ 和/ Mused/ (/ 内存/ 使用率/ )/ 一次/ ./ 实验/ 记录/ 了/ 6/ 种/ 系统/ 执行/ 5/ 个/ 基准/ 用例/ 时/ 12/ 个/ 节点/ 的/ 所有/ 特征/ 属性/ 值/ ./ 记录/ 图/ 3CPU/ 使用率/ 、/ 本地/ I/ // O/ 和/ 网络/ I/ // O/ 的/ 比较/ 由图/ 3/ (/ a/ -/ 1/ )/ ,/ (/ b/ -/ 1/ )/ ,/ (/ c/ -/ 1/ )/ ,/ (/ d/ -/ 1/ )/ 可以/ 看出/ ,/ 各种/ 云/ 数据/ 管理系统/ 的/ CPU/ 使用率/ 很/ 低/ ,/ 执行/ 查询/ 类用例/ (/ Grep/ 和/ Selection/ )/ 时/ CPU/ 使用率/ 低于/ 33/ %/ ,/ 执行/ 分析/ 类型/ 用例/ (/ Aggregation/ 和/ Join/ )/ CPU/ 使用率/ 低于/ 51/ %/ 左右/ ./ CPU/ 使用率/ 低/ 但/ 每个/ 节点/ 的/ 运算/ 并/ 没有/ 执行/ 完毕/ ,/ 因此/ CPU/ 并非/ 处于/ 一种/ 空闲/ 繁多/ 无法/ 逐一/ 展示/ ,/ 本节/ 将/ 列举/ Grep/ ,/ Selection/ ,/ Aggregation/ 和/ Join/ 用例/ 的/ 部分/ 统计数据/ 和/ 实时/ 数据/ ./ 首先/ ,/ 所有/ 系统/ 在/ 执行/ 所有/ 用例/ 时/ Mused/ 值均/ 非常/ 高/ (/ 不/ 低于/ 90/ %/ )/ ,/ 说明/ 内存/ 被/ 充分/ 的/ 使用/ ,/ 这/ 也/ 符合/ I/ // O/ 密集型/ 运算/ 的/ 特点/ ,/ 大量/ 内存/ 被/ 用作/ 数据/ 缓存/ 和/ 本地/ (/ 网络/ )/ I/ // O/ 写/ 的/ 缓冲区/ ./ 为/ 便于/ 观察/ 各/ 系统/ 的/ CPU/ 、/ 本地/ I/ // O/ 和/ 网络/ I/ // O/ 特征/ ,/ 我们/ 统计/ 了/ 所有/ 节点/ 在/ 执行/ 每/ 测试用例/ 时/ 的/ CPU/ 使用率/ 、/ 硬盘/ 读写/ 数据/ 总量/ 和/ 网络/ 数据传输/ 总量/ ,/ 并/ 比较/ 它们/ 之间/ 的/ 关系/ ,/ 如图/ 3/ 所示/ ./ 状态/ ,/ 而是/ 一种/ 等待/ 状态/ ,/ 产生/ 等待/ 能耗/ ./ 进一步/ 实验/ 分析/ 知/ ,/ 造成/ CPU/ 等待/ 的/ 原因/ 主要/ 有/ 以下/ 3/ 点/ ./ (/ 1/ )/ CPU/ 等待/ 作业/ 调度/ 本/ 研究/ 没有/ 扩展/ 现有/ 云/ 数据/ 管理系统/ 的/ 作业/ 调度/ 算法/ ,/ 而是/ 通过/ 作业/ 的/ 请求/ 方式/ 来/ 影响/ 调度/ 算法/ ./ 图/ 3/ 是/ 在/ 顺序/ 的/ 作业/ 请求/ 条件/ 下测/ 得/ ,/ 因此/ 各/ 系统/ Page11/ 将/ 采用/ 顺序/ 作业/ 调度/ 方式/ ,/ 节点/ 会因/ 等待/ 作业/ 调度/ 而/ 产生/ 等待/ 能耗/ ./ 若/ 采用/ 并发/ 请求/ 的/ 方式/ ,/ 则/ 各/ 系统/ 的/ 作业/ 调度/ 方式/ 将会/ 对应/ 调整/ ./ 实验/ 证明/ ,/ 采用/ 并发/ 作业/ 请求/ 时/ ,/ 节点/ CPU/ 使用率/ 会/ 随/ 并发/ 程度/ 的/ 增加/ 而/ 逐渐/ 增加/ ,/ 等待/ 能耗/ 减少/ ,/ 并/ 最后/ 达到/ 一个/ 稳定/ 值/ ./ 然而/ ,/ 对于/ 不同/ 的/ 系统/ ,/ “/ 并发/ 请求/ 调度/ ”/ 对/ 降低/ 等待/ 能耗/ 的/ 贡献/ 不同/ ./ HBase/ 、/ Cassandra/ 、/ Hive/ 的/ CPU/ 使用率/ 随/ 并发/ 量/ 变化/ 很小/ ,/ 而/ MongoDB/ 、/ HadoopDB/ 和/ GridSQL/ 则/ 有/ 明显/ 的/ 改观/ ./ 我们/ 测试/ 了/ 各/ 系统/ 执行/ 并发/ Grep/ 作业/ 的/ 性能/ ,/ 以/ HBase/ 和/ GridSQL/ 为例/ ./ 对于/ GridSQL/ ,/ 当/ 并发/ 请求/ 为/ 60/ 时/ ,/ GridSQL/ 的/ CPU/ 使用率/ 升为/ 80/ %/ ;/ 对于/ HBase/ ,/ 并发/ 请求/ 为/ 60/ 时/ ,/ 使用率/ 升为/ 35/ %/ ,/ 使用率/ 依旧/ 很/ 低/ ./ 我们/ 对/ HBase/ 和/ GridSQL/ 的/ 主/ 节点/ (/ 仅/ 一个/ )/ 和/ 从/ 节点/ (/ 任选/ 一个/ )/ 在/ Grep/ 执行/ 时间/ 内/ 的/ CPU/ 使用率/ 进行/ 了/ 直方图/ 统计/ (/ 用/ 0/ ~/ 100/ %/ 的/ 10/ 个/ 等/ 宽/ 区间/ 加以/ 划分/ )/ ,/ 图/ 4/ 中/ y/ 轴为/ 每个/ 区间/ 出现/ 的/ 使用率/ 频数/ 占/ 总/ 采样/ 数/ 的/ 比例/ ./ 图/ 5/ 则/ 给出/ 了/ 实时/ CPU/ 使用率/ 的/ 变化/ 曲线/ ./ 图/ 4/ 并发/ 请求/ 下/ HBase/ 和/ GridSQL/ 的/ 主/ 节点/ 和/ 从/ 图/ 4/ 可/ 直观/ 地/ 看出/ :/ GridSQL/ 和/ HBase/ 的/ 主/ 节点/ CPU/ 使用率/ 不高/ ,/ 而/ GridSQL/ 的/ 从/ 节点/ 大部分/ 时间/ 处于/ CPU/ 高/ 使用率/ 的/ 状态/ ,/ CPU/ 等待/ 明显/ 减少/ ,/ 说明/ 图/ 3/ (/ a/ -/ 1/ )/ 中/ GridSQL/ 的/ CPU/ 使用率/ 低/ 是因为/ “/ 等待/ 作业/ 调度/ ”/ ./ 而/ HBase/ 的/ 从/ 节点/ 恰好相反/ ,/ 尽管/ “/ 并发/ 调度/ ”/ 使/ CPU/ 使用率/ 较图/ 3/ (/ a/ -/ 1/ )/ 有所提高/ ,/ 但/ 依然/ 很/ 低/ ,/ CPU/ 还/ 在/ 等待/ 其它/ 资源/ ,/ 如/ 节点/ 同步/ 、/ 本地/ I/ // O/ 或/ 网络/ I/ // O/ ./ 从图/ 5/ 可/ 更/ 明显/ 地/ 看出/ ,/ GridSQL/ 的/ 从/ 节点/ CPU/ 一直/ 处于/ 高/ 使用率/ 的/ 运算/ 状态/ 直到/ 用例/ 结束/ ,/ 而/ HBase/ 的/ 从/ 节点/ CPU/ 使用率/ 则/ 忽高忽低/ ,/ 处于/ 阶段性/ 的/ “/ 空闲/ ”/ 状态/ ./ 对比/ GridSQL/ 的/ CPU/ 使用率/ 特征/ ,/ HBase/ 的/ 查询/ 和/ 调度/ 等/ 算法/ 存在/ 很大/ 的/ 优化/ 空间/ ./ (/ 2/ )/ CPU/ 等待/ 本地/ I/ // O/ 或/ 网络/ I/ // O/ 操作/ 导致/ HBase/ 和/ Cassandra/ 的/ CPU/ 等待/ 的/ 原因/ 之一/ 是/ “/ 等待/ I/ // O/ 操作/ ”/ ./ 从图/ 3/ 可以/ 看出/ ,/ MongoDB/ 、/ HadoopDB/ 和/ GridSQL/ 执行/ 测试用例/ 时/ 本地/ I/ // O/ 和/ 网络/ I/ // O/ 最少/ ,/ Hive/ 次之/ ,/ 而/ 非/ 关系/ 型/ 的/ HBase/ 和/ Cassandra/ 执行/ 测试用例/ 时/ 本地/ I/ // O/ 和/ 网络/ I/ // O/ 则/ 很/ 高/ ./ 尤其/ 是/ Join/ 用例/ (/ 图/ 3/ (/ d/ -/ 3/ )/ )/ ,/ HadoopDB/ 和/ GridSQL/ 采用/ 了/ 优化/ 的/ 表/ 连接/ 算法/ [/ 35/ -/ 36/ ]/ ,/ 网络/ 数据/ 迁移/ 很小/ ,/ 而/ HBase/ 则/ 有/ 5GB/ 的/ 网络/ 数据/ 读写/ ,/ 因此/ 执行/ 效率/ 较前/ 两者/ 慢/ 了/ 上/ 百倍/ (/ 参见/ 图/ 2/ (/ b/ )/ )/ ./ 图/ 3/ (/ a/ -/ 3/ )/ 中/ Cassandra/ 执行/ Grep/ 用例/ 时/ 网络/ I/ // O/ 稍/ 高/ ,/ 这/ 是因为/ Cassandra/ 是/ 去/ 中心化/ 的/ ,/ 而/ MapRedcue/ 框架/ 需要/ 有/ 一个/ JobTracker/ 和/ 多个/ TaskTracker/ ,/ 因此/ Cassandra/ 中/ 总会/ 有/ 一个/ 节点/ 被选为/ JobTracker/ ,/ 而/ 该/ 节点/ 保存/ 的/ 数据/ 就/ 会/ 传输/ 给/ 其它/ TaskTracker/ 处理/ ./ 从图/ 2/ 可以/ 看出/ MongoDB/ 的/ Aggregation/ 用例/ 能耗/ 高/ ,/ 而图/ 3/ (/ d/ -/ 2/ )/ 可以/ 看出/ 能耗/ 高/ 的/ 原因/ 是/ 本地/ 磁盘/ 访问量/ 大/ ,/ 因为/ 用例/ 通过/ MapReduce/ 实现/ 而/ 非/ 使用/ MongoDB/ 提供/ 的/ 查询语言/ ,/ “/ 内存/ 映射/ 机制/ ”/ 不能/ 很/ 好/ 地/ 发挥/ ,/ 且/ 需要/ 对/ 全部/ 数据/ 进行/ 读取/ ,/ 磁盘/ I/ // O/ 增加/ ./ 这/ 进一步/ 证明/ 了/ 空闲/ 能耗/ 的/ 产生/ 是因为/ “/ CPU/ 等待/ 本地/ I/ // O/ 或/ 网络/ I/ // O/ 操作/ ”/ ./ 一方面/ ,/ CPU/ 处理/ 数据/ 的/ 速度/ 一般/ 大于/ 硬盘/ 或/ 网卡/ 读写/ 数据/ 的/ 速度/ ,/ 因此/ ,/ 在/ 内存/ 受限/ 的/ 环境/ 下/ ,/ 大量/ 的/ 本地/ I/ // O/ 和/ 网络/ I/ // O/ 会/ 使/ CPU/ 因/ 等待/ I/ // O/ 操作/ 而/ 空闲/ ;/ I/ // O/ 越多/ ,/ CPU/ 等待时间/ 越长/ ,/ 等待/ 能耗/ 越大/ ./ 另一方面/ ,/ 系统/ 能耗/ 很大/ 程度/ 上/ 取决于/ 效率/ ,/ I/ // O/ 密集型/ 运算/ 的/ 运算/ 效率/ 主要/ 取决于/ I/ // O/ 效率/ ,/ 在/ 集群/ 环境/ 中/ 网络/ I/ // O/ 明显/ 要/ 慢于/ 本地/ I/ // O/ ./ 因此/ ,/ 无论是/ 基准/ 能耗/ 还是/ CPU/ 等待/ 能耗/ ,/ 都/ 很大/ 程/ Page12/ 度上/ 取决于/ 网络/ 读写/ 的/ 数据量/ 大小/ ./ (/ 3/ )/ CPU/ 等待/ 其他/ 节点/ 任务/ 完毕/ 导致/ HBase/ 和/ Cassandra/ 的/ CPU/ 等待/ 的/ 另/ 一个/ 原因/ 是/ “/ 等待/ 其它/ 节点/ 执行/ 完毕/ ”/ ,/ 因/ 节点/ 间/ 执行/ 任务/ 不/ 同步/ 而/ 导致/ 部分/ 节点/ 等待/ ,/ 产生/ 等待/ 能耗/ ./ 由于/ CPU/ 使用率/ 、/ 本地/ I/ // O/ 和/ 网络/ I/ // O/ 都/ 能够/ 表征/ 节点/ 的/ 繁忙/ 程度/ ,/ 而/ 其中/ 以/ CPU/ 使用率/ 最/ 具有/ 代表性/ ./ 本文/ 仅/ 给出/ CPU/ 使用率/ 实时/ 曲线/ ./ 为/ 简化/ 作图/ ,/ 我们/ 仅/ 选取/ 了/ 主/ 节点/ 和/ 3/ 个/ 从/ 节点/ 的/ CPU/ 使用率/ 实时/ 数据/ ./ 图/ 6/ 是/ 3/ 种云/ 数据/ 管理系统/ 执行/ Grep/ 用例/ 时/ 的/ 4/ 个/ 节点/ 的/ 实时/ CPU/ 使用率/ 曲线/ (/ Selection/ ,/ Aggregation/ 用例/ 与/ 此/ 类似/ )/ ./ 图中/ 可以/ 明显/ 地/ 看出/ ,/ 对于/ HBase/ (/ Cassandra/ 类似/ )/ ,/ 从/ 节点/ 的/ 并行性/ 较差/ ,/ 有些/ 节点/ 较/ 早/ 完成/ 运算/ ,/ 有些/ 则/ 较晚/ ,/ 节点/ 之间/ 相互/ 等待/ ;/ 而/ Hive/ 和/ HadoopDB/ (/ GridSQL/ 类似/ )/ 的/ 各个/ 从/ 节点/ 并行性/ 好/ ,/ CPU/ 使用率/ 同时/ 达到/ 峰值/ 和/ 谷值/ ,/ 节点/ 间/ 无/ 明显/ 相互/ 等待/ 的/ 现象/ ./ 由此可见/ ,/ 提高/ 节点/ 间/ 并行性/ 会/ 减少/ CPU/ 等待/ ,/ 进而/ 降低/ 能耗/ ./ 图/ 6Grep/ 用例/ 下/ HBase/ ,/ Hive/ ,/ HadoopDB/ 的/ 主/ 节点/ 和/ 图/ 7/ 展示/ 了/ Join/ 用例/ 执行/ 时/ HBase/ 、/ Hive/ 和/ GridSQL/ 的/ 主/ 节点/ 和/ 从/ 节点/ 实时/ CPU/ 使用率/ 曲线/ ./ 总体/ 上/ 可以/ 看出/ Hive/ 和/ GridSQL/ 的/ 并行性/ 要/ 优于/ HBase/ ,/ 前/ 两者/ 的/ 从/ 节点/ CPU/ 使用率/ 曲线/ 基本/ 吻合/ ./ Join/ 用例/ 明显/ 地/ 分为/ 两个/ 阶段/ ,/ 即/ “/ 表/ 连接/ ”/ 阶段/ 和/ “/ 分组/ 聚集/ ”/ 阶段/ ./ 对于/ HBase/ ,/ 两个/ 阶段/ 是/ 通过/ 两个/ 独立/ 的/ MapReduce/ 完成/ 的/ ,/ “/ 表/ 连接/ ”/ 阶段/ 采用/ ReduceSideJoin/ ,/ 大量/ 数据/ 在/ 节点/ 间/ 迁移/ ,/ 因此/ CPU/ 使用率/ 不高/ ;/ “/ 分组/ 聚集/ ”/ 的/ Map/ 阶段/ 数据/ 在/ 本地/ 计算/ 完成/ ,/ 而/ Reduce/ 阶段/ 又/ 存在/ 数据/ 的/ 迁移/ ,/ 因此/ 对应/ CPU/ 较/ 繁忙/ 和/ 较/ 空闲/ 两个/ 阶段/ ./ Hive/ 的/ “/ 表/ 连接/ ”/ 阶段/ 采用/ 的/ 是/ LookupJoin/ 的/ 方式/ ,/ 需要/ 反复/ 查找/ “/ 连接/ 键/ ”/ 是否/ 存在/ ,/ 因此/ 节点/ 在/ 执行/ 本地/ 查找/ 时/ 较为/ 繁忙/ ,/ 在/ 等待/ 其它/ 节点/ 查找/ 结果/ 时/ 较为/ 空闲/ ,/ 曲线/ 峰谷/ 交替/ ;/ 而/ “/ 分组/ 聚集/ ”/ 阶段/ 由于/ 对/ “/ sourceIP/ ”/ 进行/ 了/ 分组/ ,/ 数据/ 均/ 为/ 本地/ 读写/ ,/ 网络/ I/ // O/ 少/ ,/ 节点/ CPU/ 工作/ 稳定/ ./ GridSQL/ 的/ “/ 表/ 连接/ ”/ 阶段/ 采用/ DistributedHashJoin/ ,/ 连接/ 开始/ 阶段/ 数据/ 需要/ 重新/ 分区/ 和/ 迁移/ ,/ 网络/ I/ // O/ 密集/ ,/ CPU/ 使用率/ 较/ 小/ ,/ “/ 分组/ 聚集/ ”/ 阶段/ 则/ 主要/ 是/ 在/ 本地/ 操作/ ,/ CPU/ 工作/ 稳定/ ./ 图/ 7Join/ 用例/ 下/ HBase/ ,/ Hive/ ,/ GridSQL/ 的/ 主/ 节点/ 和/ Join/ 用例/ 的/ 能耗/ 或是/ 执行/ 性能/ 主要/ 取决于/ 各/ 系统/ 的/ “/ 表/ 连接/ ”/ 实现/ 算法/ ,/ 很多/ 数据/ 管理系统/ 会/ 根据/ 数据/ 的/ 特征/ 来/ 对/ “/ 表/ 连接/ ”/ 进行/ 优化/ ,/ 本文/ 不过/ 多地/ 讨论/ 这些/ 算法/ ./ 总体/ 上/ ,/ 有效/ 地/ 减少/ 网络/ I/ // O/ 可以/ Page13/ 提高/ Join/ 用例/ 的/ 执行/ 性能/ ,/ 降低/ 能耗/ ./ 6.3/ 优化/ 方法/ 本/ 实验/ 对/ 6/ 种云/ 数据/ 管理系统/ 的/ 能耗/ 和/ 性能/ 做/ 了/ 分析/ 和/ 比较/ ./ 可以/ 看出/ ,/ 以/ HBase/ 为/ 代表/ 的/ 非/ 关系/ 型/ 云/ 数据/ 管理系统/ 的/ 基准/ 能耗/ 要/ 高于/ 以/ Hadoop/ -/ DB/ 为/ 代表/ 的/ 关系/ 型/ 云/ 数据/ 管理系统/ ./ 实验/ 证明/ 六种/ 系统/ 在/ 执行/ 基准/ 测试/ 时/ CPU/ 多为/ 等待/ 状态/ ,/ 造成/ CPU/ 等待/ 的/ 原因/ 有/ 三/ :/ 其一/ 是/ 节点/ 等待/ 作业/ 调度/ ,/ 可以/ 通过/ 增加/ 作业/ 请求/ 的/ 并发/ 性/ 得到/ 改善/ ;/ 其二/ 是/ 节点/ 运算/ 存在/ 瓶颈/ ,/ CPU/ 在/ 等待/ 网络/ I/ // O/ 操作/ ;/ 其三/ 是/ 节点/ 间/ 因/ 任务/ 执行/ 不/ 同步/ 而/ 相互/ 等待/ ./ 因此/ ,/ 从/ 方法/ 层面/ ,/ 我们/ 可/ 从/ 资源分配/ 策略/ 、/ 数据/ 布局/ 策略/ 和/ 作业/ 执行/ 策略/ 3/ 个/ 角度/ 来/ 优化/ 能耗/ :/ (/ 1/ )/ 通过/ 能耗/ 优化/ 的/ 资源分配/ 和/ 任务调度/ 策略/ ,/ 让/ 每个/ 任务/ 占有/ 的/ 各种/ 资源/ 间/ 比例/ 合理/ ,/ 在/ 最大化/ 使用/ 节点/ 资源/ 的/ 同时/ 避免/ 资源/ 间/ 等待/ 而/ 产生/ 的/ 瓶颈/ ;/ (/ 2/ )/ 通过/ 合理/ 的/ 数据/ 布局/ 策略/ ,/ 提高/ 节点/ 间/ 并行性/ ,/ 减少/ 因/ 任务/ 同步/ 而/ 造成/ 的/ 节点/ 等待/ ;/ (/ 3/ )/ 通过/ 对/ 作业/ 执行/ 策略/ I/ // O/ 代价/ 的/ 评估/ ,/ 动态/ 地/ 根据/ 上下文/ 选择/ I/ // O/ 代价/ 最小/ 的/ 执行/ 策略/ ,/ 并/ 优化/ 现有/ 执行/ 策略/ 的/ I/ // O/ 代价/ ./ 同样/ ,/ 也/ 可以/ 采用/ 一些/ 技术手段/ 优化/ 能耗/ ./ 我们/ 以/ HBase/ 为例/ 对/ CPU/ 等待/ 现象/ 的/ 成因/ 做/ 了/ 如下/ 推理/ 并/ 提出/ 优化/ 方法/ :/ (/ 1/ )/ HBase/ 不/ 提供/ 索引/ 机制/ ,/ 数据/ 查询/ 和/ 分析/ 需要/ 扫描/ 整个/ 数据/ 集/ ,/ I/ // O/ 操作/ 多/ ,/ 算法/ 复杂度/ 高/ ,/ 引入/ 索引/ 机制/ 可以/ 减少/ I/ // O/ 操作/ ;/ (/ 2/ )/ 数据/ 节点/ (/ 从/ 节点/ )/ 存在/ I/ // O/ 瓶颈/ ,/ MapReduce/ 编程/ 模型/ 尽管/ 独立/ 于/ 存储系统/ ,/ 但/ 在/ 本/ 实验/ 中是/ 依赖/ HBase/ 实现/ 的/ ,/ 其本质/ 是/ 分布式文件系统/ HDFS/ ,/ 基准/ 用例/ 都/ 需要/ 通过/ HDFS/ 的/ I/ // O/ 接口/ 扫描/ 数据/ ,/ 接口/ 读写/ 速度/ 较慢/ 导致/ CPU/ 等待/ ,/ 可以/ 针对/ 数据/ 访问/ 接口/ 进行/ 性能/ 优化/ ;/ (/ 3/ )/ MapReduce/ 的/ 中间/ 结果/ 以/ 文件/ 方式/ 输出/ ,/ 这/ 又/ 增加/ 了/ 算法/ 的/ I/ // O/ 操作/ ,/ 因此/ 可以/ 优化/ MapReduce/ 任务/ 执行/ 算法/ ,/ 减少/ 中间/ 结果/ 的/ 输出/ ,/ 减少/ I/ // O/ 操作/ ;/ (/ 4/ )/ 使用/ 高速/ 网络/ 或/ 固态/ 硬盘/ 来/ 降低/ I/ // O/ 操作/ 的/ 代价/ ./ 上述/ 优化/ 方法/ 根据/ 本/ 实验/ 分析/ 结果/ 以及/ HBase/ 和/ MapRedcue/ 的/ 特性/ 推理/ 得出/ ,/ 进一步/ 验证/ 这些/ 优化/ 方法/ 是/ 我们/ 下/ 一步/ 的/ 工作/ ./ 7/ 结论/ 和/ 进一步/ 工作/ 本文/ 提出/ 了/ 一种/ 面向/ 云/ 数据/ 管理系统/ 的/ 能耗/ 模型/ 、/ 基准/ 测试用例/ 和/ 测量方法/ ./ 基于/ 此/ ,/ 分析/ 了/ 典型/ 的/ 云/ 数据/ 管理系统/ HBase/ 、/ Cassandra/ 、/ Hive/ 、/ MongoDB/ 、/ HadoopDB/ 和/ GridSQL/ 的/ 能耗/ 和/ 性能/ 特征/ ,/ 针对/ 云/ 数据/ 管理系统/ 的/ 特点/ 提出/ 减少/ 等待/ 能耗/ 以/ 实现/ 节能/ 的/ 方法/ ,/ 并/ 以/ HBase/ 为例/ 指出/ 具体/ 实现/ 办法/ ./ 本文/ 通过/ 实验/ 得出/ 如下/ 结论/ :/ (/ 1/ )/ 尽管/ 都/ 采用/ SharedNothing/ 体系结构/ 和/ MapReduce/ 编程/ 模型/ ,/ 但/ 以/ HBase/ 为例/ 的/ 非/ 关系/ 型/ 云/ 数据/ 管理系统/ 执行/ 数据/ 查询/ 和/ 分析/ 基准/ 测试用例/ 的/ 能耗/ 高于/ 以/ HadoopDB/ 为例/ 的/ 关系/ 型/ 云/ 数据/ 管理系统/ ,/ 但/ 前者/ 执行/ 数据/ 装载/ 基准/ 测试用例/ 的/ 能耗/ 低于/ 后者/ ./ 能耗/ 的/ 高低/ 主要/ 取决于/ 测试用例/ 的/ 执行/ 时间/ ./ (/ 2/ )/ 几种/ 云/ 数据/ 管理系统/ 在/ 执行/ 基准/ 测试用例/ 时/ CPU/ 等待时间/ 较/ 多/ ,/ 产生/ 等待/ 能耗/ ./ (/ 3/ )/ 产生/ 等待/ 能耗/ 的/ 原因/ 是/ 作业/ 调度/ 不合理/ ,/ 节点/ 在/ 等待/ 作业/ 调度/ ;/ CPU/ 等待/ 本地/ I/ // O/ 和/ 网络/ I/ // O/ 操作/ ;/ 以及/ 节点/ 之间/ 任务/ 执行/ 不/ 同步/ 而/ 相互/ 等待/ ./ (/ 4/ )/ 从/ 方法/ 层面/ ,/ 可/ 通过/ 合理/ 的/ 资源分配/ 和/ 任务调度/ 策略/ 来/ 提高/ 资源/ 使用率/ ;/ 或/ 合理/ 的/ 数据/ 布局/ 策略/ 提高/ 节点/ 间/ 并行性/ ;/ 或/ 作业/ 执行/ 策略/ I/ // O/ 代价/ 的/ 评估/ 和/ 动态/ 的/ 选择/ 等/ 方法/ 优化/ 能耗/ ./ (/ 5/ )/ 从/ 技术/ 层面/ ,/ 可以/ 通过/ 增加/ 索引/ 机制/ 、/ 优化/ I/ // O/ 性能/ 、/ 优化/ 数据/ 存储/ 格式/ 、/ 选择/ 高效/ 存储/ 来/ 保存/ 中间/ 结果/ 、/ 优化/ 网络/ 传输/ 、/ 优化/ 节点/ 间/ 的/ 调度/ 算法/ 等/ 技术/ 来/ 减少/ CPU/ 等待/ ,/ 降低/ 能耗/ ./ 本文/ 定义/ 的/ 能耗/ 模型/ 、/ 基准/ 测试用例/ 、/ 测试方法/ 和/ 得出/ 的/ 相关/ 结论/ 都/ 有助于/ 测试/ 和/ 比较/ 各云/ 数据/ 管理系统/ 的/ 能耗/ ,/ 研究/ 能耗/ 优化/ 的/ 基本/ 方法/ ,/ 指导/ 云/ 数据/ 管理系统/ 选择/ 和/ 低能耗/ 云/ 数据/ 管理系统/ 的/ 设计/ 和/ 研发/ ./ 本/ 研究/ 还/ 处于/ 初步阶段/ ,/ 我们/ 将/ 依据/ 本文/ 给出/ 的/ 优化/ 思路/ ,/ 进一步/ 研究/ 云/ 数据/ 管理系统/ 的/ 能耗/ 优化/ 方法/ ./ 同时/ ,/ 可以/ 建立/ 基于/ QoS/ 的/ 能耗/ 模型/ ,/ 能耗/ 作为/ 一种/ 服务质量/ 要素/ ,/ 可以/ 使云/ 数据/ 服务提供商/ 准确/ 地/ 计算/ 能源/ 成本/ ,/ 服务/ 使用者/ 也/ 准确/ 地/ 按/ 需/ 使用/ ,/ 按/ 需/ 付费/ ./ 

