Page1/ 神经网络/ 七十年/ :/ 回顾/ 与/ 展望/ 1/ )/ (/ 西安电子科技大学/ 智能/ 感知/ 与/ 图像/ 理解/ 教育部/ 重点/ 实验室/ 、/ 智能/ 感知/ 与/ 计算/ 国际/ 联合/ 研究/ 中心/ 、/ 智能/ 感知/ 与/ 计算/ 国际/ 合作/ 联合/ 实验室/ 西安/ 710071/ )/ 2/ )/ (/ 西安电子科技大学/ 计算机/ 学院/ 西安/ 710071/ )/ 摘要/ 作为/ 联接/ 主义/ 智能/ 实现/ 的/ 典范/ ,/ 神经网络/ 采用/ 广泛/ 互联/ 的/ 结构/ 与/ 有效/ 的/ 学习/ 机制/ 来/ 模拟/ 人脑/ 信息处理/ 的/ 过程/ ,/ 是/ 人工智能/ 发展/ 中/ 的/ 重要/ 方法/ ,/ 也/ 是/ 当前/ 类脑/ 智能/ 研究/ 中/ 的/ 有效/ 工具/ ./ 在/ 七十年/ 的/ 发展/ 历程/ 中/ ,/ 神经网络/ 曾/ 历经/ 质疑/ 、/ 批判/ 与/ 冷落/ ,/ 同时/ 也/ 几度/ 繁荣/ 并/ 取得/ 了/ 许多/ 瞩目/ 的/ 成就/ ./ 从/ 20/ 世纪/ 40/ 年代/ 的/ M/ -/ P/ 神经元/ 和/ Hebb/ 学习/ 规则/ ,/ 到/ 50/ 年代/ 的/ Hodykin/ -/ Huxley/ 方程/ 、/ 感知器/ 模型/ 与/ 自/ 适应/ 滤波器/ ,/ 再/ 到/ 60/ 年代/ 的/ 自/ 组织/ 映射/ 网络/ 、/ 神经/ 认知/ 机/ 、/ 自/ 适应/ 共振/ 网络/ ,/ 许多/ 神经计算/ 模型/ 都/ 发展/ 成为/ 信号处理/ 、/ 计算机/ 视觉/ 、/ 自然语言/ 处理/ 与/ 优化/ 计算/ 等/ 领域/ 的/ 经典/ 方法/ ,/ 为/ 该/ 领域/ 带来/ 了/ 里程碑式/ 的/ 影响/ ./ 目前/ ,/ 模拟/ 人脑/ 复杂/ 的/ 层次化/ 认知/ 特点/ 的/ 深度/ 学习/ 已经/ 成为/ 类脑/ 智能/ 中/ 的/ 一个/ 重要/ 研究/ 方向/ ./ 通过/ 增加/ 网络层/ 数所/ 构造/ 的/ “/ 深层/ 神经网络/ ”/ 使/ 机器/ 能够/ 获得/ “/ 抽象概念/ ”/ 能力/ ,/ 在/ 诸多/ 领域/ 都/ 取得/ 了/ 巨大/ 的/ 成功/ ,/ 又/ 掀起/ 了/ 神经网络/ 研究/ 的/ 一个/ 新高潮/ ./ 文中/ 回顾/ 了/ 神经网络/ 的/ 发展/ 历程/ ,/ 综述/ 了/ 其/ 当前/ 研究进展/ 以及/ 存在/ 的/ 问题/ ,/ 展望/ 了/ 未来/ 神经网络/ 的/ 发展/ 方向/ ./ 关键词/ 类脑/ 智能/ ;/ 神经网络/ ;/ 深度/ 学习/ ;/ 大/ 数据/ ;/ 并行计算/ ;/ 机器/ 学习/ 1/ 引言/ 实现/ 人工智能/ 是/ 人类/ 长期以来/ 一直/ 追求/ 的/ 梦想/ ./ 虽然/ 计算机技术/ 在/ 过去/ 几十年/ 里/ 取得/ 了/ 长足/ 的/ 发展/ ,/ 但是/ 实现/ 真正/ 意义/ 上/ 的/ 机器/ 智能/ 至今/ 仍然/ 困难重重/ ./ 伴随/ 着/ 神经/ 解剖学/ 的/ 发展/ ,/ 观测/ 大脑/ 微观/ 结构/ 的/ 技术手段/ 日益/ 丰富/ ,/ 人类/ 对/ 大脑/ 组织/ 的/ 形态/ 、/ 结构/ 与/ 活动/ 的/ 认识/ 越来越/ 深入/ ,/ 人脑/ 信息处理/ 的/ 奥秘/ 也/ 正在/ 被/ 逐步/ 揭示/ ./ 如何/ 借助/ 神经科学/ 、/ 脑科学/ 与/ 认知科学/ 的/ 研究成果/ ,/ 研究/ 大脑/ 信息/ 表征/ 、/ 转换/ 机理/ 和/ 学习/ 规则/ ,/ 建立/ 模拟/ 大脑/ 信息处理/ 过程/ 的/ 智能/ 计算/ 模型/ ,/ 最终/ 使/ 机器/ 掌握/ 人类/ 的/ 认知/ 规律/ ,/ 是/ “/ 类脑/ 智能/ ”/ 的/ 研究/ 目标/ ./ 近年来/ ,/ 类脑/ 智能/ 已/ 成为/ 世界/ 各国/ 研究/ 和/ 角逐/ 的/ 热点/ ./ 继/ 美国/ 及/ 欧盟/ 各国/ 之后/ ,/ 我国/ 经过/ 两三年/ 筹备/ 的/ “/ 中国/ 脑科学/ 计划/ ”/ 在/ 2015/ 年/ 浮出/ 水面/ ,/ 科技部/ 正在/ 规划/ “/ 脑科学/ 与/ 类脑/ 研究/ ”/ 的/ 重大/ 专项/ ,/ 北京大学/ 、/ 清华大学/ 、/ 复旦大学/ 等/ 高校/ 和/ 中国科学院/ 等/ 研究/ 机构/ 也/ 发力/ 推动/ 神经/ 与/ 类脑/ 计算/ 的/ 相关/ 研究/ ,/ 大规模/ “/ 类脑/ 智能/ ”/ 的/ 研究/ 正/ 蓄势待发/ ./ 类脑/ 智能/ 是/ 涉及/ 计算/ 科学/ 、/ 认知科学/ 、/ 神经科学/ 与/ 脑科学/ 的/ 交叉/ 前沿/ 方向/ ./ 类脑/ 智能/ 的/ 实现/ 离不开/ 大脑/ 神经系统/ 的/ 研究/ ./ 众所周知/ ,/ 人脑/ 是/ 由/ 几十/ 多亿/ 个/ 高度/ 互联/ 的/ 神经元/ 组成/ 的/ 复杂/ 生物/ 网络/ ,/ 也/ 是/ 人类/ 分析/ 、/ 联想/ 、/ 记忆/ 和/ 逻辑推理/ 等/ 能力/ 的/ 来源/ ./ 神经元/ 之间/ 通过/ 突触/ 连接/ 以/ 相互/ 传递信息/ ,/ 连接/ 的/ 方式/ 和/ 强度/ 随着/ 学习/ 发生/ 改变/ ,/ 从而/ 将/ 学习/ 到/ 的/ 知识/ 进行/ 存储/ ./ 模拟/ 人脑/ 中/ 信息/ 存储/ 和/ 处理/ 的/ 基本/ 单元/ -/ 神经元/ 而/ 组成/ 的/ 人工神经网络/ 模型/ 具有/ 自/ 学习/ 与/ 自/ 组织/ 等/ 智能/ 行为/ ,/ 能够/ 使/ 机器/ 具有/ 一定/ 程度/ 上/ 的/ 智能/ 水平/ ./ 神经网络/ 的/ 计算/ 结构/ 和/ 学习/ 规则/ 遵照/ 生物/ 神经网络/ 设计/ ,/ 在/ 数字/ 计算机/ 中/ ,/ 神经细胞/ 接收/ 周围/ 细胞/ 的/ 刺激/ 并/ 产生/ 相应/ 输出/ 信号/ 的/ 过程/ 可以/ 用/ “/ 线性/ 加权/ 和/ ”/ 及/ “/ 函数/ 映射/ ”/ 的/ 方式/ 来/ 模拟/ ,/ 而/ 网络结构/ 和/ 权值/ 调整/ 的/ 过程/ 用/ 优化/ 学习/ 算法/ 实现/ ./ 按照/ 该/ 方式/ 建立/ 的/ 这种/ 仿生/ 智能/ 计算/ 模型/ 虽然/ 不能/ 和/ 生物/ 神经网络/ 完全/ 等价/ 和/ 媲美/ ,/ 但/ 已经/ 在/ 某些/ 方面/ 取得/ 了/ 优越/ 的/ 性能/ ./ 从/ 20/ 世纪/ 40/ 年代/ 的/ M/ -/ P/ 神经元/ 和/ Hebb/ 学习/ 规则/ ,/ 到/ 50/ 年代/ 的/ Hodykin/ -/ Huxley/ 方程/ 、/ 感知器/ 模型/ 与/ 自/ 适应/ 滤波器/ ,/ 再/ 到/ 60/ 年代/ 的/ 自/ 组织/ 映射/ 网络/ 、/ 神经/ 认知/ 机/ 、/ 自/ 适应/ 共振/ 网络/ ,/ 许多/ 神经计算/ 模型/ 都/ 发展/ 成为/ 信号处理/ 、/ 计算机/ 视觉/ 、/ 自然语言/ 处理/ 与/ 优化/ 计算/ 等/ 领域/ 的/ 经典/ 方法/ ,/ 为/ 该/ 领域/ 带来/ 了/ 里程碑式/ 的/ 影响/ ./ 目前/ 神经网络/ 已经/ 发展/ 了/ 上/ 百种/ 模型/ ,/ 在/ 诸如/ 手写体/ 识别/ [/ 1/ -/ 2/ ]/ 、/ 图像/ 标注/ [/ 3/ ]/ 、/ 语义/ 理解/ [/ 4/ -/ 6/ ]/ 和/ 语音/ 识别/ [/ 7/ -/ 9/ ]/ 等/ 技术/ 领域/ 取得/ 了/ 非常/ 成功/ 的/ 应用/ ./ 从/ 数据/ 容量/ 和/ 处理速度/ 来看/ ,/ 目前/ 大多数/ 神经网络/ 是/ 生物/ 网络/ 的/ 简化/ 形式/ ,/ 在/ 应对/ 海量/ 数据/ 和/ 处理/ 复杂/ 任务/ 时/ 显得/ 力不从心/ ./ 例如/ ,/ 人脑/ 被/ 证明/ 可以/ 在/ 没有/ 导师/ 监督/ 的/ 情况/ 下/ 主动/ 地/ 完成/ 学习/ 任务/ ,/ 仅/ 凭借/ 传统/ 的/ 浅层/ 神经网络/ 是/ 无法/ 实现/ 这/ 一点/ 的/ ./ 最近/ 发展/ 起来/ 的/ 深层/ 神经网络/ 就是/ 一/ 种类/ 脑/ 智能/ 软件系统/ ,/ 它/ 使得/ 人工智能/ 的/ 研究/ 进入/ 了/ 一个/ 新/ 阶段/ ./ 深层/ 神经网络/ 通过/ 增加/ 网络/ 的/ 层数/ 来/ 模拟/ 人脑/ 复杂/ 的/ 层次化/ 认知/ 规律/ ,/ 以/ 使/ 机器/ 获得/ “/ 抽象概念/ ”/ 的/ 能力/ ,/ 在/ 无/ 监督/ 特征/ 学习/ 方面/ 具有/ 更强/ 的/ 能力/ ./ 然而/ ,/ 受到/ 计算/ 平台/ 和/ 学习/ 算法/ 的/ 限制/ ,/ 对/ 深层/ 神经网络/ 的/ 研究/ 曾一度/ 消弭/ ./ 2006/ 年/ ,/ Hinton/ 在/ 《/ 科学/ 》/ 上/ 提出/ 了/ 一种/ 面向/ 复杂/ 通用/ 学习/ 任务/ 的/ 深层/ 神经网络/ ,/ 指出/ 具有/ 大量/ 隐层/ 的/ 网络/ 具有/ 优异/ 的/ 特征/ 学习/ 能力/ ,/ 而/ 网络/ 的/ 训练/ 可以/ 采用/ “/ 逐层/ 初始化/ ”/ 与/ “/ 反向/ 微调/ ”/ 技术/ 解决/ ./ 人类/ 借助/ 神经网络/ 找到/ 了/ 处理/ “/ 抽象概念/ ”/ 的/ 方法/ ,/ 神经网络/ 的/ 研究/ 又/ 进入/ 了/ 一个/ 崭新/ 的/ 时代/ [/ 10/ -/ 12/ ]/ ,/ 深度/ 学习/ 的/ 概念/ 开始/ 被/ 提出/ ./ 深度/ 学习/ 兴起/ 的/ 背景/ 是/ 计算能力/ 的/ 提高/ 与/ 大/ 数据/ 时代/ 的/ 来临/ ,/ 其/ 核心理念/ 是/ 通过/ 增加/ 网络/ 的/ 层数/ 来/ 让/ 机器/ 自动/ 地/ 从/ 数据/ 中/ 进行/ 学习/ ./ 深层/ 神经网络/ 能够/ 获得/ 巨大成功/ 与其/ 对应/ 在/ 训练/ 算法/ 上/ 所/ 取得/ 的/ 突破性/ 进展/ 是/ 密不可分/ 的/ ./ 传统/ 的/ 反向/ 传播/ 算法/ (/ BackPropagation/ )/ 随着/ 传递/ 层数/ 的/ 增加/ ,/ 残差/ 会越/ Page3/ 来/ 越/ 小/ ,/ 出现/ 所谓/ 的/ “/ 梯度/ 扩散/ ”/ (/ GradientDiffusion/ )/ 现象/ ,/ 故而/ 不/ 适于/ 深层/ 网络/ 的/ 训练/ ./ 深度/ 学习/ 模型/ 中/ 的/ 受限/ 玻尔兹曼/ 机/ (/ RestrictedBoltzmannMachines/ )/ 和/ 自/ 编码器/ (/ Auto/ -/ Encoder/ )/ 采用/ 了/ “/ 自下而上/ 的/ 无/ 监督/ 学习/ ”/ 和/ “/ 自顶向下/ 的/ 监督/ 学习/ ”/ 策略/ 来/ 实现/ 对/ 网络/ 的/ “/ 预/ 训练/ ”/ 和/ “/ 微调/ ”/ ,/ 可/ 使/ 学习/ 算法/ 收敛/ 到/ 较为理想/ 的/ 解上/ ,/ 而/ 当前/ 使用/ 更为/ 广泛/ 的/ 卷积/ 神经网络/ (/ ConvolutionalNeuralNetworks/ )/ 则/ 采用/ 局部/ 感受/ 野/ 、/ 权值/ 共享/ 和/ 时空/ 亚/ 采样/ 的/ 思想/ ,/ 显著/ 地/ 减少/ 了/ 网络/ 中/ 自由/ 参数/ 的/ 个数/ ,/ 并且/ 使得/ 采用/ 反向/ 传播/ 来/ 进行/ 网络/ 的/ 并行/ 学习/ 成为/ 可能/ ./ 除了/ 以上/ 优势/ 外/ ,/ 深度/ 学习/ 最具/ 吸引力/ 的/ 地方/ 还/ 在于/ 能/ 凭借/ 无/ 标签/ 的/ 数据/ 来/ 进行/ 学习/ ,/ 而/ 不/ 需要/ 依赖于/ 监督/ 信息/ 的/ 支撑/ [/ 13/ ]/ ./ 现实/ 世界/ 的/ 很多/ 问题/ 中/ ,/ 对/ 数据/ 的/ 标记/ 通常/ 是/ 耗时/ 耗力/ 甚至/ 是/ 不/ 可行/ 的/ ,/ 无/ 监督/ 学习/ 可以/ 自动/ 抽取/ 出/ 抽象/ 的/ 高层/ 属性/ 和/ 特征/ ,/ 是/ 解决/ 样本/ 标记/ 难/ 问题/ 的/ 一个/ 重大突破/ ./ 深度/ 学习/ 的/ 成功/ 引起/ 了/ 包括/ 产业界/ 和/ 学术界/ 在内/ 的/ 诸多/ 人士/ 的/ 关注/ ,/ 其/ 影响力/ 甚至/ 上升/ 到/ 了/ 国家/ 战略/ 层面/ ./ 2012/ 年/ 6/ 月/ ,/ 《/ 纽约时报/ 》/ 披露/ 了/ GoogleBrain/ 项目/ ,/ 该/ 项目/ 拟/ 计划/ 在/ 包含/ 16000/ 个/ 中央/ 处理单元/ 的/ 分布式/ 并行计算/ 平台/ 上/ 构建/ 一种/ 被/ 称之为/ “/ 深度/ 神经网络/ ”/ 的/ 类脑/ 学习/ 模型/ ,/ 其/ 主要/ 负责人/ 为/ 机器/ 学习/ 界/ 的/ 泰斗/ 、/ 来自/ 斯坦福大学/ 的/ Ng/ 教授/ 和/ Google/ 软件架构/ 天才/ 、/ 大型/ 并发/ 编程/ 框架/ MapReduce/ 的/ 作者/ JeffDean/ ;/ 2012/ 年/ 10/ 月/ ,/ 在/ 天津/ 举行/ 的/ “/ 21/ 世纪/ 的/ 计算/ 大会/ ”/ 上/ ,/ 微软/ 首席/ 研究/ 官/ RickRashid/ 展示/ 了/ 一套/ 全自动/ 同声/ 传译/ 系统/ ,/ 演讲者/ 的/ 英文/ 能够/ 被/ 实时/ 、/ 流畅地/ 转换成/ 与/ 之/ 对应/ 的/ 、/ 音色/ 相近/ 的/ 中文/ ,/ 其/ 背后/ 的/ 关键技术/ 深度/ 神经网络/ 也/ 逐渐/ 被/ 人们/ 所知/ ./ 2013/ 年/ 1/ 月/ ,/ 作为/ 百度/ 公司/ 创始人/ 兼/ CEO/ 的/ 李彦宏/ 在/ 其/ 年/ 会上/ 宣布/ 了/ 成立/ 百度/ 研究院/ 的/ 计划/ ,/ 并且/ 强调/ 首当其冲/ 的/ 就是/ 组建/ “/ 深度/ 学习/ 研究所/ ”/ ./ 在/ 2015/ 年/ 3/ 月/ 9/ 日/ 的/ 两会/ 期间/ ,/ 李彦宏/ 又/ 提议/ 设立/ “/ 中国/ 大脑/ ”/ 计划/ 的/ 提案/ ,/ 与/ 2013/ 年/ 1/ 月/ 和/ 2013/ 年/ 4/ 月/ 的/ “/ 欧盟/ 大脑/ 计划/ ”/ 和/ “/ 美国/ 大脑/ 计划/ ”/ 相/ 呼应/ ./ 2015/ 年/ 3/ 月/ ,/ 阿里巴巴公司/ 的/ 创始人/ 马云/ 通过/ 支付宝/ 的/ “/ 刷/ 脸/ 支付/ ”/ 功能/ ,/ 在/ 德国/ 举行/ 的/ IT/ 博览会/ 上/ 成功/ 购得/ 了/ 一款/ 汉诺威/ 纪念邮票/ ./ 这一/ 人脸识别/ 技术/ 在/ 商业/ 领域/ 的/ 应用/ 雏形/ 所/ 采用/ 的/ 是/ 基于/ 神经网络/ 的/ 技术/ ,/ 其/ 网络/ 训练/ 所/ 使用/ 的/ 正是/ “/ 深度/ 学习/ 算法/ ”/ ./ 在/ 学术界/ ,/ 以/ Hinton/ 、/ LeCun/ 、/ Bengio/ 和/ Ng/ 等/ 为/ 代表/ 的/ 神经网络/ 大师/ 们/ 不断/ 将/ 深度/ 学习/ 的/ 研究/ 推向/ 新/ 的/ 高峰/ ,/ 对/ 包括/ 计算机/ 视觉/ 、/ 自然语言/ 处理/ 和/ 机器/ 学习/ 在内/ 的/ 诸多/ 领域/ 带来/ 了/ 深远/ 的/ 影响/ [/ 14/ ]/ ./ 自/ 2006/ 年/ 深度/ 学习/ 出现/ 以来/ ,/ 关于/ 深度/ 学习/ 理论/ 和/ 应用/ 方面/ 的/ 研究/ 文献/ 在/ 国际/ 知名/ 期刊/ 和/ 会议/ 上/ 不断涌现/ ,/ 如/ 《/ 自然/ 》/ 、/ 《/ 科学/ 》/ 、/ PAMI/ 、/ NIPS/ 、/ CVPR/ 、/ ICML/ 等/ ./ 同时/ ,/ 由/ Bengio/ 等/ 人/ 编写/ 的/ 第一本/ 关于/ 深度/ 学习/ 的/ 专著/ “/ DeepLearning/ ”/ 也/ 即将/ 由/ MIT/ 出版社/ 出版/ ./ 包括/ 斯坦福大学/ 、/ 卡内基/ 梅隆/ 大学/ 、/ 纽约大学/ 、/ 多伦多/ 大学/ 等/ 在内/ 的/ 机构/ 都/ 提供/ 了/ 深度/ 学习/ 的/ 公开/ 课程/ ,/ 并/ 公开/ 了/ 实验/ 数据/ 和/ 源代码/ ,/ 为/ 深度/ 学习/ 的/ 进一步/ 发展/ 做出/ 了/ 贡献/ ./ 在/ 国内/ ,/ 深度/ 学习/ 也/ 受到/ 了/ 学术界/ 的/ 广泛/ 关注/ ,/ 但/ 目前/ 主要/ 是/ 以/ 深度/ 学习/ 的/ 应用/ 研究/ 为主/ ,/ 在/ 理论/ 方面/ 的/ 工作/ 相对/ 较少/ ./ 以/ 北京大学/ 、/ 浙江大学/ 、/ 上海交通大学/ 、/ 哈尔滨工业大学/ 和/ 西安电子科技大学/ 等/ 为/ 代表/ 的/ 研究/ 人员/ 将/ 深度/ 学习/ 算法/ 应用/ 到/ 遥感/ 图像/ 分类/ [/ 15/ ]/ 、/ 多媒体/ 检索/ [/ 16/ ]/ 、/ 交通流/ 预测/ [/ 17/ ]/ 和/ 盲/ 图像/ 质量/ 评价/ [/ 18/ ]/ 等/ 领域/ ,/ 取得/ 了/ 较/ 传统/ 方法/ 更优/ 的/ 效果/ ./ 本文/ 将/ 以/ 神经网络/ 的/ 理论/ 和/ 应用/ 为主/ 线/ ,/ 回顾/ 神经网络/ 在/ 过去/ 七十多年/ 的/ 发展/ 历程/ 及/ 主要/ 成就/ ,/ 重点/ 对/ 新近/ 发展/ 起来/ 的/ 深度/ 学习/ 进行/ 阐述/ 和/ 讨论/ ,/ 并/ 对/ 未来/ 的/ 研究/ 方向/ 做出/ 展望/ ./ 2/ 神经网络/ 发展/ 回顾/ 自从/ 西班牙/ 解剖学家/ Cajal/ 于/ 19/ 世纪末/ 创立/ 了/ 神经元/ 学说/ 以来/ ,/ 关于/ 神经元/ 的/ 生物学/ 特征/ 和/ 相关/ 的/ 电学/ 性质/ 在/ 之后/ 被/ 相继/ 发现/ ./ 1943/ 年/ ,/ 神经元/ 的/ M/ -/ P/ 模型/ (/ 如图/ 1/ 所示/ )/ 在/ 论文/ 《/ 神经/ 活动/ 中/ 所/ 蕴含/ 思想/ 的/ 逻辑/ 活动/ 》/ 中/ 被/ 首次/ 提出/ [/ 19/ ]/ ,/ 创建/ 该/ 模型/ 的/ 是/ 来自/ 美国/ 的/ 心理学家/ McCulloch/ 以及/ 另/ 一位/ 数学家/ Pitts/ ./ 图中/ ,/ xi/ (/ i/ =/ 1/ ,/ 2/ ,/ …/ ,/ n/ )/ 表示/ 来自/ 与/ 当前/ 神经元/ 相连/ 的/ 其他/ 神经元/ 传递/ 的/ 输入/ 信号/ ,/ wij/ 代表/ 从/ 神经元/ j/ 到/ 神经元/ i/ 的/ 连接/ 强度/ 或/ 权值/ ,/ θ/ i/ 为/ 神经元/ 的/ 激活/ 阈值/ 或/ 偏置/ ,/ f/ 称作/ 激活/ 函数/ 或/ 转移/ 函数/ ./ 神经元/ Page4/ 的/ 输出/ yi/ 可以/ 表示/ 为/ 如下/ 形式/ :/ 该/ 模型/ 从/ 逻辑/ 功能/ 器件/ 的/ 角度/ 来/ 描述/ 神经元/ ,/ 为/ 神经网络/ 的/ 理论/ 研究/ 开辟/ 了/ 道路/ ./ M/ -/ P/ 模型/ 是/ 对/ 生物/ 神经元/ 信息处理/ 模式/ 的/ 数学/ 简化/ ,/ 后续/ 的/ 神经网络/ 研究/ 工作/ 都/ 是/ 以/ 它/ 为/ 基础/ 的/ ./ 1949/ 年/ ,/ 在/ 《/ 行为/ 的/ 组织/ 》/ 一书中/ 心理学家/ Hebb/ 对/ 神经元/ 之间/ 连接/ 强度/ 的/ 变化/ 规则/ 进行/ 了/ 分析/ ,/ 并/ 基于/ 此/ 提出/ 了/ 著名/ 的/ Hebb/ 学习/ 规则/ [/ 20/ ]/ ./ 受/ 启发/ 于巴浦/ 洛夫/ 的/ 条件反射/ 实验/ ,/ Hebb/ 认为/ 如果/ 两个/ 神经元/ 在/ 同一/ 时刻/ 被/ 激发/ ,/ 则/ 它们/ 之间/ 的/ 联系/ 应该/ 被/ 强化/ ,/ 基于/ 此/ 所/ 定义/ 的/ Hebb/ 学习/ 规则/ 如下/ 所示/ :/ 其中/ ,/ wij/ (/ t/ +/ 1/ )/ 和/ wij/ (/ t/ )/ 分别/ 表示/ 在/ t/ +/ 1/ 和/ t/ 时刻/ 时/ ,/ 神经元/ j/ 到/ 神经元/ i/ 之间/ 的/ 连接/ 强度/ ,/ 而/ yi/ 和/ yj/ 则/ 为/ 神经元/ i/ 和/ j/ 的/ 输出/ ./ Hebb/ 规则/ 隶属于/ 无/ 监督/ 学习/ 算法/ 的/ 范畴/ ,/ 其/ 主要/ 思想/ 是/ 根据/ 两个/ 神经元/ 的/ 激发/ 状态/ 来/ 调整/ 其/ 连接/ 关系/ ,/ 以此/ 实现/ 对/ 简单/ 神经/ 活动/ 的/ 模拟/ ./ 继/ Hebb/ 学习/ 规则/ 之后/ ,/ 神经元/ 的/ 有/ 监督/ Delta/ 学习/ 规则/ 被/ 提出/ ,/ 用以/ 解决/ 在/ 输入输出/ 已知/ 的/ 情况/ 下/ 神经元/ 权值/ 的/ 学习/ 问题/ ./ 该/ 算法/ 通过/ 对/ 连接/ 权值/ 进行/ 不断/ 调整/ 以/ 使/ 神经元/ 的/ 实际/ 输出/ 和/ 期望/ 输出/ 到达/ 一致/ ,/ 其/ 学习/ 修正/ 公式/ 如下/ [/ 21/ ]/ :/ 其中/ ,/ α/ 为/ 算法/ 的/ 学习/ 速率/ ,/ di/ 和/ yi/ 为/ 神经元/ i/ 的/ 期望/ 输出/ 和/ 实际/ 输出/ ,/ xj/ (/ t/ )/ 表示/ 神经元/ j/ 在/ t/ 时刻/ 的/ 状态/ (/ 激活/ 或/ 抑制/ )/ ./ 从/ 直观/ 上/ 来说/ ,/ 当/ 神经元/ i/ 的/ 实际/ 输出/ 比/ 期望/ 输出/ 大/ ,/ 则/ 减小/ 与/ 已/ 激活/ 神经元/ 的/ 连接/ 权重/ ,/ 同时/ 增加/ 与/ 已/ 抑制/ 神经元/ 的/ 连接/ 权重/ ;/ 当/ 神经元/ i/ 的/ 实际/ 输出/ 比/ 期望/ 输出/ 小/ ,/ 则/ 增加/ 与/ 已/ 激活/ 神经元/ 的/ 连接/ 权重/ ,/ 同时/ 减小/ 与/ 已/ 抑制/ 神经元/ 的/ 连接/ 权重/ ./ 通过/ 这样/ 的/ 调节/ 过程/ ,/ 神经元/ 会/ 将/ 输入/ 和/ 输出/ 之间/ 的/ 正确/ 映射/ 关系/ 存储/ 在/ 权值/ 中/ ,/ 从而/ 具备/ 了/ 对/ 数据/ 的/ 表示/ 能力/ ./ Hebb/ 学习/ 规则/ 和/ Delta/ 学习/ 规则/ 都/ 是/ 针对/ 单个/ 神经元/ 而/ 提出/ 的/ ,/ 在/ 神经元/ 组成/ 的/ 网络/ 中/ 参数/ 的/ 学习/ 规则/ 将会/ 在/ 后续/ 述及/ ./ 以上/ 先驱者/ 所/ 做/ 的/ 研究/ 工作/ 为/ 后来/ 神经计算/ 的/ 出现/ 铺平/ 了/ 道路/ ,/ 激发/ 了/ 许多/ 学者/ 对/ 这/ 一/ 领域/ 的/ 继续/ 探索/ 和/ 研究/ ./ 1958/ 年/ ,/ Rosenblatt/ 等/ 人/ 成功/ 研制/ 出/ 了/ 代号/ 为/ MarkI/ 的/ 感知机/ (/ Perceptron/ )/ ,/ 这是/ 历史/ 上/ 首个/ 将/ 神经网络/ 的/ 学习/ 功能/ 用于/ 模式识别/ 的/ 装置/ ,/ 标志/ 着/ 神经/ 网路/ 进入/ 了/ 新/ 的/ 发展/ 阶段/ [/ 22/ ]/ ./ 感知机/ 是/ 二/ 分类/ 的/ 线性/ 判别/ 模型/ ,/ 旨在/ 通过/ 最小化/ 误/ 分类/ 损失/ 函数/ 来/ 优化/ 分类/ 超平面/ ,/ 从而/ 对/ 新/ 的/ 实例/ 实现/ 准确/ 预测/ ./ 假设/ 输入/ 特征向量/ 空间/ 为/ x/ ∈/ / y/ =/ {/ -/ 1/ ,/ +/ 1/ }/ ,/ 则/ 感知机/ 模型/ 如下/ :/ 其中/ ,/ w/ 和/ b/ 为/ 神经元/ 的/ 权值/ 向量/ 和/ 偏置/ ;/ w/ ·/ x/ 表示/ w/ 和/ x/ 的/ 内积/ ;/ sign/ 为/ 符号/ 函数/ :/ 感知机/ 的/ 假设/ 空间/ 是/ 定义/ 在/ 特征/ 空间/ 中/ 的/ 所有/ 线性/ 分类器/ ,/ 所得/ 的/ 超平面/ 把/ 特征/ 空间/ 划分/ 为/ 两/ 部分/ ,/ 位于/ 两侧/ 的/ 点/ 分别/ 为/ 正负/ 两类/ ./ 感知机/ 参数/ 的/ 学习/ 是/ 基于/ 经验/ 损失/ 函数/ 最小化/ 的/ ,/ 旨在/ 最小化/ 误/ 分类/ 点到/ 决策/ 平面/ 的/ 距离/ ./ 给定/ 一组/ 数据/ 集/ T/ =/ {/ (/ x1/ ,/ y1/ )/ ,/ (/ x2/ ,/ y2/ )/ ,/ …/ ,/ (/ xn/ ,/ yn/ )/ }/ ,/ 假设/ 超平面/ S/ 下误/ 分类/ 点/ 的/ 集合/ 为/ M/ ,/ 则/ 感知机/ 学习/ 的/ 损失/ 函数/ 定义/ 为/ 感知机/ 学习/ 算法/ 通过/ 最小化/ 经验/ 风险/ 来/ 优化/ 参数/ w/ 和/ b/ :/ 优化/ 过程/ 采用/ 随机/ 梯度/ 下降/ 法/ ,/ 每次/ 随机/ 选取/ 一个/ 误/ 分类/ 点/ 使/ 其/ 梯度/ 下降/ ./ 首先/ 分别/ 求/ 出/ 损失/ 函数/ 对/ w/ 和/ b/ 偏/ 导数/ :/ 然后/ ,/ 随机/ 选取/ 一个/ 误/ 分类/ 点/ (/ xi/ ,/ yi/ )/ 对/ w/ 和/ b/ 进行/ 更新/ 其中/ ,/ 0/ </ η/ / 1/ 是/ 学习/ 步长/ ./ 以上/ 为/ 感知机/ 学习/ 的/ 原始/ 形式/ ,/ 与/ 之/ 相对/ 应/ 的/ 另/ 一种/ 结构/ 是/ 感知机/ 学习/ 的/ 对偶/ 形式/ ./ 其/ 基本/ 思想/ 是/ 将/ w/ 和/ b/ 表示/ 为/ 所有/ 实例/ 点/ 的/ 线性组合/ 形式/ ,/ 通过/ 求解/ 系数/ 来/ 得到/ w/ 和/ b/ ./ 不失/ 一般性/ ,/ 首先/ 将/ w/ 和/ b/ 的/ 初始值/ 设为/ 0/ ,/ 对于/ 误/ 分类/ 点/ 按照/ 式/ (/ 10/ )/ 和/ (/ 11/ )/ 的/ 规则/ 来/ 对/ w/ 和/ b/ 的/ 值/ 进行/ 更新/ ./ 假设/ 总共/ 进行/ 了/ n/ 次/ 更新/ ,/ 则/ 最终/ 学习/ 到/ 的/ w/ 和/ b/ 可/ 表示/ 为/ Page5/ 其中/ ,/ ai/ =/ ni/ η/ (/ ni/ 为/ 第/ i/ 次时/ 的/ 累积/ 更新/ 次数/ )/ ./ 继/ 感知机/ 之后/ ,/ 许多/ 新/ 的/ 学习型/ 神经网络/ 模型/ 被/ 提出/ ,/ 其中/ 包括/ Widrow/ 等/ 人/ 设计/ 的/ 自/ 适应/ 线性元件/ Adaline/ [/ 23/ ]/ 和/ 由/ Steinbuch/ 等/ 人/ 设计/ 的/ 被/ 称为/ 学习/ 矩阵/ 的/ 二进制/ 联想/ 网络/ 及其/ 硬件/ 实现/ [/ 24/ ]/ ./ 随着/ 对/ 感知机/ 研究/ 的/ 逐渐/ 深入/ ,/ 1969/ 年/ Minsky/ 和/ Papert/ 从/ 数学/ 的/ 角度/ 证明/ 了/ 单层/ 神经网络/ 具有/ 有限/ 的/ 功能/ ,/ 甚至/ 在/ 面对/ 简单/ 的/ “/ 异或/ ”/ 逻辑/ 问题/ 时/ 也/ 显得/ 无能为力/ [/ 25/ ]/ ./ 同时/ ,/ 他们/ 发现/ 许多/ 复杂/ 的/ 函数/ 关系/ 是/ 无法/ 通过/ 对/ 单层/ 网络/ 训练/ 得到/ 的/ ,/ 至于/ 多层/ 网络/ 是否/ 可行/ 还/ 值得/ 怀疑/ ./ 他们/ 所著/ 的/ 《/ 感知机/ 》/ 一书/ 出版/ 后/ 给/ 当时/ 神经网络/ 感知机/ 方向/ 的/ 研究/ 泼/ 了/ 一盆/ 冷水/ ,/ 美国/ 和/ 前/ 苏联/ 在/ 此后/ 很长/ 一段时间/ 内/ 也/ 未/ 资助/ 过/ 神经网络/ 方面/ 的/ 研究/ 工作/ ./ 虽然/ 感知机/ 具备/ 了/ 基本/ 的/ 神经计算/ 单元/ 和/ 网络结构/ ,/ 也/ 拥有/ 一套/ 有效/ 的/ 参数/ 学习/ 算法/ ,/ 但是/ 其/ 特定/ 的/ 结构/ 使得/ 其/ 在/ 很多/ 问题/ 上/ 都/ 不能/ 奏效/ ./ 此后/ 很长/ 一段时间/ 内/ 神经网络/ 的/ 研究/ 处在/ 低迷/ 期/ ,/ 直到/ 1982/ 年/ 美国加州理工学院/ 的/ Hopfield/ 提出/ 了/ 连续/ 和/ 离散/ 的/ Hopfield/ 神经网络/ 模型/ ,/ 并/ 采用/ 全/ 互联/ 型/ 神经网络/ 尝试/ 对非/ 多项式/ 复杂度/ 的/ 旅行/ 商/ 问题/ (/ TravellingSalesmanProblem/ ,/ TSP/ )/ 进行/ 了/ 求解/ ,/ 促进/ 神经网络/ 的/ 研究/ 再次/ 进入/ 了/ 蓬勃发展/ 的/ 时期/ [/ 26/ ]/ ./ Hopfield/ 强调/ 工程/ 实践/ 的/ 重要性/ ,/ 他/ 利用/ 电阻/ 、/ 电容/ 和/ 运算/ 放大器/ 等/ 元件/ 组成/ 的/ 模拟/ 电路/ 实现/ 了/ 对/ 网络/ 神经元/ 的/ 描述/ ,/ 把/ 最优化/ 问题/ 的/ 目标/ 函数/ 转换成/ Hopfield/ 神经网络/ 的/ 能量/ 函数/ ,/ 通过/ 网络/ 能量/ 函数/ 最小化/ 来/ 寻找/ 对应/ 问题/ 的/ 最优/ 解/ ./ Hopfield/ 网络/ 是/ 一种/ 循环/ 神经网络/ ,/ 从/ 输出/ 到/ 输入/ 有/ 反馈/ 连接/ ,/ 典型/ 的/ Hopfield/ 神经网络/ 模型/ 如图/ 2/ 所示/ ./ 图/ 2/ 中/ ,/ 每组/ 运算/ 放大器/ 及其/ 相关/ 的/ 电阻/ 、/ 电容/ 组成/ 的/ 网络/ 代表/ 一个/ 神经元/ ./ 每个/ 神经元/ 有/ 两组/ 输入/ ,/ 一组/ 是/ 恒定/ 的/ 外部/ 电流/ ,/ 另一组/ 是/ 来自/ 其他/ 运算/ 放大器/ 输出/ 的/ 正向/ 或/ 反向/ 的/ 反馈/ 连接/ ./ 假设/ 第/ i/ 个/ 神经元/ 的/ 内部/ 膜电位/ 为/ Ui/ (/ i/ =/ 1/ ,/ 2/ ,/ …/ ,/ n/ )/ ,/ 细胞膜/ 的/ 输入/ 电容/ 和/ 传递/ 电阻/ 分别/ 为/ Ci/ 和/ Ri/ ,/ 神经元/ 的/ 输出/ 电位/ 为/ Vi/ ,/ 外部/ 输入/ 电流/ 为/ Ii/ ,/ 并用/ 电阻/ Rij/ (/ i/ ,/ j/ =/ 1/ ,/ 2/ ,/ …/ ,/ n/ )/ 来/ 模拟/ 第/ i/ 个/ 和/ 第/ j/ 个/ 神经元/ 之间/ 的/ 突触/ 特性/ ./ 由/ 基尔霍夫/ 电流/ 定律/ (/ Kirchhoff/ ’/ sCurrentLaw/ ,/ KCL/ )/ 可知/ ,/ 放大器/ 输入/ 节点/ 处/ 的/ 流入/ 电流/ 和/ 流出/ 电流/ 保持平衡/ ,/ 亦/ 即/ 有/ 下式/ 成立/ :/ nVj/ (/ t/ )/ ∑/ j/ =/ 1Rij/ 同时/ ,/ 每/ 一个/ 运算/ 放大器/ 模拟/ 了/ 神经元/ 输入/ 和/ 输出/ 之间/ 的/ 非线性/ 特性/ ,/ 即/ 有/ 其中/ ,/ fi/ 代表/ 了/ 第/ i/ 个/ 神经元/ 的/ 传递函数/ ,/ 并/ 定义/ ij/ (/ i/ ,/ j/ =/ 1/ ,/ 2/ ,/ …/ ,/ n/ )/ 为/ 网络/ 的/ 权/ 系数/ 矩阵/ ./ 为/ W/ =/ R/ -/ 1/ 证明/ 连续型/ 网络/ 的/ 稳定性/ ,/ Hopfield/ 定义/ 了/ 如下/ 的/ 能量/ 函数/ :/ E/ (/ t/ )/ =/ -/ 其中/ ,/ f/ -/ 1/ 为/ 神经元/ 传递函数/ 的/ 反函数/ ./ 经过/ 推导/ 后/ 得出/ 以下/ 两点/ 结论/ :/ 一是/ 对于/ 具有/ 单调/ 递增/ 传递函数/ 且/ 对称/ 权/ 系数/ 矩阵/ 的/ 网络/ 来说/ ,/ 其/ 能量/ 会/ 随着/ 时间/ 的/ 变化/ 而/ 趋于稳定/ ;/ 二是/ 当且/ 仅/ 当/ 网络/ 中/ 所有/ 神经元/ 的/ 输出/ 不再/ 随/ 时间/ 变化/ 时/ ,/ 则/ 可以/ 认为/ 网络/ 的/ 能量/ 保持/ 不变/ ./ 在/ 将/ 网络/ 用于/ 求解/ 诸如/ 旅行/ 商/ 的/ 组合/ 优化/ 问题/ 时/ ,/ Hopfield/ 将/ 优化/ 的/ 目标/ 函数/ 转化/ 为/ 网络/ 的/ 能量/ 函数/ ,/ 对应/ 地/ 将/ 待/ 求解/ 问题/ 的/ 变量/ 用/ 网络/ 中/ 神经元/ 的/ 状态/ 来/ 表示/ ./ 由/ 这样/ 的/ 表示/ 方式/ 可知/ 当/ 网络/ 的/ 能量/ 衰减/ 到/ 稳定/ 值时/ ,/ 问题/ 的/ 最优/ 解/ 也/ 随之/ 求出/ ./ Hopfield/ 网络/ 一个/ 重要/ 的/ 特点/ 是/ 它/ 可以/ 实现/ 联想/ 记忆/ 功能/ ,/ 亦/ 即/ 作为/ 联想/ 存储器/ ./ 当/ 网络/ 的/ 权/ 系数/ 通过/ 学习/ 训练/ 确定/ 之后/ ,/ 即便/ 输入/ 不/ 完整/ 或者/ 部分/ 不/ 正确/ 的/ 数据/ ,/ 网络/ 仍旧/ 可以/ 通过/ 联想/ 记忆/ 来/ 给出/ 完整/ 的/ 数据/ 输出/ 结果/ ./ Hopfield/ 提出/ 该/ 模型/ 后/ ,/ 许多/ 人/ 试图/ 对/ 其/ 进行/ 进一步/ 的/ 扩展/ ,/ 以/ 希望/ 能够/ 设计/ 出/ 更/ 接近/ 人脑/ 功能/ 特性/ 的/ 网络/ 模型/ ./ 1983/ 年/ ,/ “/ 隐/ 单元/ ”/ 的/ 概念/ 首次/ 被/ Sejnowski/ 和/ Hinton/ 提出/ ,/ 并且/ 他们/ 基于/ 此/ 设计/ 出/ 了/ 波尔兹曼/ 机/ (/ BoltzmannMachine/ ,/ BM/ )/ ,/ 其/ 结构/ 如图/ 3/ 所示/ [/ 27/ -/ 28/ ]/ ./ 波尔兹曼/ 机是/ 一种/ 由/ 随机/ 神经元/ 全/ 连接/ 组成/ 的/ 反馈/ 神经网络/ ,/ 其/ 包含/ 一个/ 可见/ 层/ 和/ 一个/ 隐层/ ./ 网络/ 中神/ Page6/ 经元/ 的/ 输出/ 只有/ 两种/ 状态/ (/ 未激活/ 和/ 激活/ ,/ 用/ 二进制/ 0/ 和/ 1/ 表示/ )/ ,/ 其/ 取值/ 根据/ 概率/ 统计/ 规则/ 决定/ ./ 波尔兹曼/ 机/ 具有/ 较强/ 的/ 无/ 监督/ 学习/ 能力/ ,/ 可以/ 从/ 数据/ 中/ 学习/ 到/ 复杂/ 的/ 知识/ 规则/ ,/ 然而/ 也/ 存在/ 着/ 训练/ 和/ 学习/ 时间/ 过长/ 的/ 问题/ ./ 此外/ ,/ 不仅/ 难以/ 准确/ 计算/ BM/ 所/ 表示/ 的/ 分布/ ,/ 得到/ 服从/ BM/ 所/ 表示/ 分布/ 的/ 随机样本/ 也/ 很/ 困难/ ./ 基于/ 以上/ 原因/ ,/ 对/ 波尔兹曼/ 机/ 进行/ 了/ 改进/ ,/ 引入/ 了/ 限制/ 波尔兹曼/ 机/ (/ RestrictedBoltzmannMachine/ ,/ RBM/ )/ [/ 29/ ]/ ./ 相比/ 于/ 波尔兹曼/ 机/ ,/ RBM/ 的/ 网络结构/ 中层/ 内/ 神经元/ 之间/ 没有/ 连接/ ,/ 尽管/ RBM/ 所/ 表示/ 的/ 分布/ 仍然/ 无法/ 有效/ 计算/ ,/ 但/ 可以/ 通过/ Gibbs/ 采样/ 得到/ 服从/ RBM/ 所/ 表示/ 分布/ 的/ 随机样本/ ./ Hinton/ 于/ 2002/ 年/ 提出/ 了/ 一个/ RBM/ 学习/ 的/ 快速/ 算法/ (/ 对比/ 散度/ )/ ,/ 只要/ 隐层/ 单元/ 的/ 数目/ 足够/ 多时/ ,/ RBM/ 就/ 能/ 拟合/ 任意/ 离散/ 分布/ [/ 30/ ]/ ./ RBM/ 已/ 被/ 用于/ 解决/ 不同/ 的/ 机器/ 学习/ 问题/ ,/ 比如/ 分类/ 、/ 回归/ 、/ 降维/ 、/ 高维/ 时间/ 序列/ 建模/ 、/ 语音/ 图像/ 特征提取/ 和/ 协同/ 过滤/ 等/ 方面/ [/ 31/ -/ 33/ ]/ ./ 同时/ ,/ 作为/ 目前/ 深度/ 学习/ 主要/ 框架/ 之一/ 的/ 深度/ 信念/ 网/ 也/ 是/ 以/ RBM/ 为/ 基本/ 组成/ 单元/ 的/ ./ 这一/ 阶段/ 的/ 神经网络/ 已经/ 从/ 起初/ 的/ 单层/ 结构/ 扩展/ 到/ 了/ 双层/ ,/ 隐含/ 层/ 的/ 出现/ 使得/ 网络/ 具有/ 更强/ 的/ 数据表示/ 能力/ ./ 虽然/ 层数/ 的/ 增加/ 可以/ 为/ 网络/ 提供/ 更大/ 的/ 灵活性/ ,/ 但是/ 参数/ 的/ 训练/ 算法/ 一直/ 是/ 制约/ 多层/ 神经网络/ 发展/ 的/ 一个/ 重要/ 瓶颈/ ./ 直到/ 1974/ 年/ ,/ Werbos/ 在/ 他/ 的/ 博士论文/ 里/ 提出/ 了/ 用于/ 神经网络/ 学习/ 的/ BP/ (/ BackPropagation/ )/ 算法/ ,/ 才/ 为/ 多层/ 神经网络/ 的/ 学习/ 训练/ 与/ 实现/ 提供/ 了/ 一种/ 切实可行/ 的/ 解决/ 途径/ ,/ 同时/ 在/ 1986/ 年/ 由/ Rumelhart/ 和/ McCelland/ 为首/ 的/ 科学家/ 小组/ 对/ 多层/ 网络/ 的/ 误差/ 反向/ 传播/ 算法/ 进行/ 了/ 详尽/ 的/ 分析/ ,/ 进一步/ 推动/ 了/ BP/ 算法/ 的/ 发展/ [/ 34/ -/ 37/ ]/ ./ BP/ 网络/ 的/ 拓扑/ 结构/ 包括/ 输入/ 层/ 、/ 隐层/ 和/ 输出/ 层/ ,/ 它/ 能够/ 在/ 事先/ 不/ 知道/ 输入输出/ 具体/ 数学/ 表达式/ 的/ 情况/ 下/ ,/ 通过/ 学习/ 来/ 存储/ 这种/ 复杂/ 的/ 映射/ 关系/ ./ 其/ 网络/ 中/ 参数/ 的/ 学习/ 通常/ 采用/ 反向/ 传播/ 的/ 策略/ ,/ 借助/ 最速/ 梯度/ 信息/ 来/ 寻找/ 使/ 网络/ 误差/ 最小化/ 的/ 参数/ 组合/ ./ 常见/ 的/ 3/ 层/ BP/ 网络/ 模型/ 如图/ 4/ 所示/ ./ 其中/ ,/ 各/ 节点/ 的/ 传递函数/ f/ 必须/ 满足/ 处处/ 可导/ 的/ 条件/ ,/ 最/ 常用/ 的/ 为/ Sigmoid/ 函数/ ,/ 第/ i/ 个/ 神经元/ 的/ 净/ 输入/ 为/ neti/ ,/ 输出/ 为/ Oi/ ./ 如果/ 网络/ 输出/ 层/ 第/ k/ 个/ 神经元/ 的/ 期望/ 输出/ 为/ y/ / 由于/ BP/ 算法/ 按照/ 误差/ 函数/ E/ 的/ 负/ 梯度/ 修改权/ 值/ ,/ 故权值/ 的/ 更新/ 公式/ 可/ 表示/ 为/ 其中/ ,/ t/ 表示/ 迭代/ 次数/ ,/ gt/ =/ 层/ 神经元/ 权值/ 的/ 更新/ 公式/ 为/ wt/ +/ 1kj/ =/ wt/ 其中/ ,/ δ/ k/ 称作/ 输出/ 层/ 第/ k/ 个/ 神经元/ 的/ 学习/ 误差/ ./ 对/ 隐含/ 层/ 神经元/ 权值/ 的/ 更新/ 公式/ 为/ wt/ +/ 1ji/ =/ wt/ =/ wt/ =/ wt/ =/ wt/ =/ wt/ 其中/ ,/ δ/ j/ 称作/ 隐含/ 层/ 第/ j/ 个/ 神经元/ 的/ 学习/ 误差/ ./ BP/ 的/ 误差/ 反向/ 传播/ 思想/ 可以/ 概括/ 为/ :/ 利用/ 输出/ 层/ 的/ 误差/ 来/ 估计/ 出其/ 直接/ 前导/ 层/ 的/ 误差/ ,/ 再/ 借助于/ 这个/ 新/ 的/ 误差/ 来/ 计算/ 更前/ 一层/ 的/ 误差/ ,/ 按照/ 这样/ 的/ 方式/ 逐层/ 反/ 传下去/ 便/ 可以/ 得到/ 所有/ 各层/ 的/ 误差/ 估计/ ./ BP/ 算法/ 的/ 提出/ 在/ 一定/ 程度/ 上/ 解决/ 了/ 多层/ 网络/ 参数/ 训练/ Page7/ 难/ 的/ 问题/ ,/ 但是/ 其/ 自身/ 也/ 存在/ 如下/ 一些/ 问题/ ./ 首先/ ,/ 误差/ 在/ 反向/ 传播/ 过程/ 中/ 会/ 逐渐/ 衰减/ ,/ 经过/ 多层/ 的/ 传递/ 后/ 将/ 会/ 变得/ 很小/ ,/ 这/ 使得/ BP/ 在/ 深层/ 网络/ 中/ 并/ 不/ 可行/ ./ 其次/ ,/ BP/ 采用/ 最速/ 梯度/ 下降/ 的/ 优化/ 思想/ ,/ 而/ 实际/ 问题/ 的/ 误差/ 函数/ 通常/ 不是/ 凸/ 的/ ,/ 存在/ 众多/ 局部/ 极小值/ 点/ ,/ 算法/ 很难/ 得到/ 最优/ 解/ ./ 再次/ ,/ 由于/ 训练/ 过程/ 中/ 依靠/ 于/ 导数/ 信息/ 来/ 进行/ 权值/ 的/ 调整/ ,/ 当权/ 值/ 调节/ 过大/ 时会/ 使/ 大部分/ 神经元/ 的/ 加权/ 和/ 过/ 大/ ,/ 致使/ 传递函数/ 工作/ 于/ S/ 型函数/ 的/ 饱和/ 区/ ,/ 所以/ 权值/ 的/ 调整/ 会/ 出现/ 停顿/ 的/ 情况/ ./ 最后/ ,/ 对于/ 一些/ 复杂/ 网络/ 的/ 优化/ 问题/ ,/ BP/ 算法/ 受到/ 学习/ 速率/ 的/ 限制/ 需要/ 花费/ 几个/ 小时/ ,/ 甚至/ 更长/ 的/ 时间/ 来/ 完成/ 训练任务/ ./ 此后/ 于/ 1989/ 年/ ,/ Cybenko/ 、/ Funahashi/ 、/ Hornik/ 等/ 人/ 相继/ 对/ BP/ 神经网络/ 的/ 非线性/ 函数/ 逼近/ 性能/ 进行/ 了/ 分析/ ,/ 并/ 证明/ 了/ 对于/ 具有/ 单隐层/ 、/ 传递函数/ 为/ sigmoid/ 的/ 连续型/ 前馈/ 神经网络/ 可以/ 以/ 任意/ 精度/ 逼近/ 任何/ 复杂/ 的/ 连续/ 映射/ [/ 38/ -/ 40/ ]/ ./ 根据/ 研究/ 结果显示/ ,/ 只要/ 隐层/ 神经元/ 的/ 个数/ 足够/ 多/ ,/ BP/ 神经网络/ 就/ 能够/ 保证/ 对/ 复杂/ 连续/ 映射/ 关系/ 的/ 刻画/ 能力/ ,/ 具有/ 重要/ 的/ 理论/ 和/ 现实/ 指导意义/ ./ 继/ BP/ 之后/ ,/ 为/ 模拟/ 生物/ 神经元/ 的/ 局部/ 响应/ 特性/ ,/ Broomhead/ 和/ Lowe/ 于/ 1988/ 年/ 将/ 径向/ 基/ 函数/ 引入/ 到/ 了/ 神经网络/ 的/ 设计/ 中/ ,/ 形成/ 了/ 径向/ 基/ 神经网络/ RBF/ [/ 41/ ]/ ./ 后来/ ,/ Jackson/ 和/ Park/ 分别/ 于/ 1989/ 年/ 和/ 1991/ 年/ 对/ RBF/ 在/ 非线性/ 连续函数/ 上/ 的/ 一致/ 逼近/ 性能/ 进行/ 了/ 论证/ [/ 42/ -/ 43/ ]/ ./ RBF/ 神经网络/ 是/ 一种/ 3/ 层/ 的/ 前/ 向/ 网络/ ,/ 其/ 基本/ 工作/ 原理/ 是/ :/ 利用/ RBF/ 构成/ 的/ 隐藏/ 层/ 空间/ 对/ 低维/ 的/ 输入/ 矢量/ 进行/ 投影/ ,/ 将/ 数据/ 变换/ 到/ 高/ 维空间/ 中/ 去/ ,/ 以/ 使/ 原来/ 线性/ 不可/ 分/ 的/ 问题/ 能够/ 变得/ 线性/ 可分/ ./ 图/ 5/ 为/ 径向/ 基/ 神经网络/ 的/ 基本/ 结构/ 示意图/ ./ 由于/ 输入/ 层/ 在/ RBF/ 网络/ 中/ 仅仅/ 起到/ 信号/ 的/ 传输/ 作用/ ,/ 故而/ 输入/ 层/ 和/ 隐含/ 层/ 之间/ 的/ 连接/ 权值/ 都/ 为/ 1/ ,/ 隐含/ 层/ 实现/ 对/ 输入/ 特征/ 的/ 非线性/ 投影/ ,/ 而/ 输出/ 层则/ 负责/ 最后/ 的/ 线性/ 加权/ 求和/ ./ RBF/ 网络/ 中待/ 学习/ 优化/ 的/ 参数/ 包括/ :/ 基/ 函数/ 的/ 中心/ 和/ 方差/ 以及/ 隐含/ 层到/ 输出/ 层/ 的/ 连接/ 权值/ ./ 输出/ 层/ 负责/ 通过/ 线性/ 优化/ 策略/ 来/ 实现/ 对/ 权值/ 的/ 优化/ ,/ 学习/ 速度/ 通常/ 较/ 快/ ;/ 而/ 隐含/ 层则/ 需要/ 采用/ 非线性/ 优化/ 的/ 方法/ 对/ 激活/ 函数/ 的/ 参数/ 调整/ ,/ 故而/ 其/ 学习/ 速度/ 较慢/ ./ RBF/ 网络/ 的/ 参数/ 学习/ 方法/ 按照/ 径向/ 基/ 函数/ 中心/ 的/ 选取/ 有/ 不同/ 的/ 类型/ ,/ 主要/ 包括/ 自/ 组织/ 选取/ 法/ 、/ 随机/ 中心/ 法/ 、/ 有/ 监督/ 中心/ 法/ 和/ 正交/ 最小/ 二/ 乘法/ 等/ ./ 以自/ 组织法/ 为例/ ,/ 其/ 学习/ 主要/ 包括/ 两个/ 阶段/ ,/ 第一阶段/ 为/ 无/ 监督/ 和/ 自/ 组织/ 学习/ 阶段/ ,/ 用以/ 确定/ 隐含/ 层基/ 函数/ 的/ 中心/ 及/ 方差/ ;/ 第二阶段/ 是/ 有/ 监督/ 学习/ 过程/ ,/ 可/ 实现/ 隐含/ 层到/ 输出/ 层/ 之间/ 的/ 连接/ 权值/ 的/ 求解/ ./ RBF/ 网络/ 有/ 很快/ 的/ 学习/ 收敛/ 速度/ ,/ 一个/ 很/ 重要/ 的/ 原因/ 在于/ 其/ 属于/ 局部/ 逼近/ 网络/ ,/ 不/ 需要/ 学习/ 隐含/ 层/ 的/ 权值/ ,/ 避免/ 了/ 误差/ 在/ 网络/ 中/ 耗时/ 的/ 逐层/ 传递/ 过程/ ./ RBF/ 网络/ 也/ 是/ 神经网络/ 真正/ 走向/ 实用化/ 的/ 一个/ 重要/ 标志/ ,/ 其/ 已/ 被/ 成功/ 应用/ 于/ 非线性/ 函数/ 逼近/ 、/ 模式/ 分类/ 、/ 控制系统/ 建模/ 、/ 时变/ 数据分析/ 和/ 故障/ 分析/ 诊断/ 等/ 工程/ 领域/ [/ 43/ -/ 45/ ]/ ./ 应当/ 指出/ 的/ 是/ 蔡少棠/ 等/ 人/ 提出/ 了/ 细胞/ 神经网络/ (/ CellularNeuralNetworks/ )/ [/ 46/ -/ 47/ ]/ ,/ Zhang/ 等/ 人/ 提出/ 了/ 小波/ 神经网络/ [/ 48/ ]/ ,/ Jiao/ 等/ 人/ 提出/ 了/ 多小波/ 神经网络/ [/ 49/ ]/ ,/ Yang/ 等/ 人/ 提出/ 了/ 脊波/ 神经网络/ [/ 50/ ]/ ,/ 这些/ 模型/ 在/ 非/ 平稳/ 、/ 非线性/ 、/ 非/ 高斯/ 信号/ 与/ 图像处理/ 中/ 表现/ 出/ 良好/ 的/ 应用/ 潜力/ 和/ 价值/ ./ 此后/ ,/ 神经网络/ 与/ 机器/ 学习/ 和/ 模式识别/ 的/ 融合/ 呈现出/ 前所未有/ 的/ 局面/ ,/ SVM/ 、/ PCA/ 、/ ICA/ 、/ LDA/ 等/ 模型/ 得到/ 广泛/ 关注/ 和/ 研究/ ,/ 表现/ 出/ 良好/ 的/ 性能/ ,/ 有力/ 促进/ 了/ 这/ 一/ 领域/ 的/ 进展/ ./ 其中/ ,/ 薄列峰/ 等/ 人/ 提出/ 的/ 大规模/ SVM/ [/ 51/ -/ 53/ ]/ 是/ 这方面/ 的/ 典型/ 代表/ ./ 进入/ 21/ 世纪/ 以来/ ,/ 国内外/ 在/ 神经网络/ 的/ 理论/ 和/ 应用/ 研究/ 上/ 也/ 取得/ 了/ 若干/ 突破性/ 成果/ ./ 特别/ 应当/ 指出/ 的/ 是/ ,/ 香港中文大学/ 的/ Xu/ 提出/ 了/ Bayes/ 学习机/ 和/ Y/ -/ Y/ 机/ ,/ 并/ 证明/ 了/ EM/ 算法/ 的/ 收敛性/ [/ 54/ -/ 55/ ]/ ,/ 清华大学/ Zhang/ 等/ 人/ 提出/ 了/ PLN/ 神经网络/ 模型/ [/ 56/ ]/ ,/ 中国科学院/ 半导体/ 研究所/ 王守觉/ 等/ 人/ 对/ 神经网络/ 的/ 硬件/ 实现/ 及其/ 在/ 模式识别/ 领域/ 的/ 应用/ 进行/ 了/ 广泛/ 而/ 深入/ 的/ 研究/ [/ 57/ -/ 58/ ]/ 等/ ./ 复旦大学/ 陈天平/ 教授/ 、/ 西安交通大学/ 徐宗本/ 教授/ 在/ 多层/ 和/ 径向/ 基/ 神经网络/ 的/ 逼近/ 性能/ 以及/ Cohen/ —/ Grossberg/ 和/ 具有/ 时延/ 的/ Hopfield/ 神经网络/ 的/ 稳定性/ 方面/ 开展/ 了/ 相关/ 研究/ ,/ 并/ 得出/ 了/ 一些/ 具有/ 指导意义/ 的/ 结论/ [/ 59/ -/ 62/ ]/ ./ 在/ 生物/ 神经网络/ 模型/ 与/ 机理/ 方面/ ,/ 张香桐/ 、/ 郭爱克/ 、/ 汪云九/ 、/ 陈琳/ 、/ 汪德亮/ 、/ 刘德荣/ 等/ 人/ 做/ 了/ 先驱/ 性/ 的/ 工作/ ,/ 赢得/ 了/ 国际/ 同行/ 的/ 赞誉/ ./ 伯明翰/ 大学/ 的/ 姚新/ 将/ 进化/ 计算/ 的/ 搜索/ 机制/ 引/ Page8/ 入到/ 人工神经网络/ 中/ ,/ 提出/ 了/ 进化/ 人工神经网络/ 的/ 概念/ ,/ 并且/ 对/ 进化/ 神经网络/ 进行/ 集成/ 以/ 提高/ 网络/ 性能/ [/ 63/ -/ 64/ ]/ ./ 萨里/ 大学/ 的/ 金耀初/ 利用/ 多/ 目标/ 遗传算法/ 进行/ 神经网络/ 的/ 正则/ 化/ 和/ 集成/ ,/ 并且/ 将/ 网络/ 用于/ 复杂/ 系统/ 的/ 建模/ 和/ 控制/ 当中/ [/ 65/ -/ 66/ ]/ ./ 中国/ 科学技术/ 大学/ 陈国良/ 院士/ 提出/ 了/ 主从/ 通用/ 神经网络/ 模型/ ,/ 并且/ 开发/ 出/ 了/ 通用/ 并行/ 神经网络/ 模拟系统/ ,/ 为/ 神经网络/ 提供/ 了/ 高级/ 描述语言/ 以及/ 编辑/ 和/ 可/ 执行/ 环境/ [/ 67/ -/ 68/ ]/ ./ 上海交通大学/ 赵同/ 和/ 戚/ 飞虎/ 等/ 人/ 提出/ 了/ 基于/ 遗传算法/ 的/ 协同/ 神经网络/ 中/ 参数/ 的/ 优化/ 方法/ [/ 69/ -/ 70/ ]/ ./ 清华大学/ 吴佑寿/ 等/ 人/ ,/ 中国科学院自动化研究所/ 戴汝为/ 、/ 刘迎建/ 等/ 人/ 在/ 汉字/ 图像识别/ 上/ 取得/ 了/ 较/ Hopfield/ 网络/ 更优/ 的/ 性能/ ./ 华中科技大学/ 廖晓昕/ 和/ 四川大学/ 章毅/ 在/ 神经网络/ 的/ 稳定性/ 和/ 收敛性/ 方面/ 进行/ 了/ 深入研究/ ./ 东南大学/ 曹/ 进德/ 等/ 人/ 对/ 具有/ 时延/ 的/ CNN/ 网络/ 、/ 回归/ 神经网络/ 、/ Cohen/ —/ Grossberg/ 网络/ 和/ 联想/ 记忆/ 网络/ 等/ 的/ 稳定性/ 和/ 周期性/ 进行/ 了/ 深入/ 的/ 研究/ [/ 71/ -/ 74/ ]/ ./ 南京大学/ 周志华/ 等/ 人于/ 2001/ 年/ 提出/ 了/ 用/ 遗传算法/ 来/ 进行/ 多个/ 神经网络/ 的/ 选择性/ 集成/ 的/ 模型/ GASEN/ [/ 75/ -/ 76/ ]/ ,/ 并/ 证明/ 集成/ 部分/ 网络/ 比/ 使用/ 单个/ 网络/ 或者/ 集成/ 所有/ 网络/ 有/ 更/ 强/ 的/ 泛化/ 能力/ ;/ 于/ 2003/ 年/ 分别/ 提出/ 了/ 用于/ 解释/ 集成/ 神经网络/ 功能/ 的/ 方法/ REFNE/ [/ 77/ ]/ ,/ 此/ 方法/ 可以/ 提取/ 出/ 具有/ 高/ 保真度/ 或强/ 泛化/ 能力/ 的/ 规则/ 来/ 提高/ 集成/ 网络/ 的/ 可/ 理解/ 性/ ,/ 以及/ 可/ 用于/ 医疗/ 诊断/ 的/ 模型/ C4/ ./ 5Rule/ -/ PANE/ [/ 78/ ]/ ,/ 此/ 模型/ 结合/ 了/ 集成/ 神经网络/ 的/ 强/ 泛化/ 能力/ 和/ C4/ ./ 5/ 规则/ 推理/ 的/ 高度/ 可/ 理解/ 性/ ;/ 于/ 2004/ 年/ 提出/ 了/ 一种/ 全新/ 的/ 决策树/ 算法/ NeC4/ ./ 5/ [/ 79/ ]/ ,/ 在/ UCIMachineLearningRepository/ 上/ 取得/ 了/ 较/ 传统/ C4/ ./ 5/ 方法/ 更优/ 的/ 分类/ 性能/ ;/ 同时/ 于/ 2006/ 年/ 提出/ 了/ 通过/ 训练/ 代价/ 敏感/ 的/ 神经网络/ 来/ 解决/ 样本/ 不/ 平衡/ 问题/ 的/ 新/ 方法/ [/ 80/ ]/ ./ 西南/ 大学/ 廖晓峰/ 等/ 人/ 在/ 带/ 时滞/ 神经网络/ 的/ 鲁棒性/ 和/ 稳定性/ 方面/ 做出/ 了/ 突出贡献/ ,/ 研究成果/ 在/ 模式识别/ 和/ 自动控制/ 领域/ 得到/ 了/ 广泛应用/ [/ 81/ -/ 83/ ]/ ./ 南京航空航天大学/ 陈松灿/ PARNEC/ 团队/ 相继/ 提出/ 了/ ICBP/ (/ ImprovedCircularBackPropa/ -/ gation/ )/ 、/ DLS/ (/ DiscountedLeastSquares/ )/ -/ ICBP/ 、/ ChainedDLS/ -/ ICBP/ 和/ Plane/ -/ Gaussian/ 等/ 神经网络/ 模型/ ,/ 用以/ 提升/ 神经网络/ 的/ 泛化/ 和/ 适应能力/ ,/ 并/ 更/ 好地解决/ 局部/ 极小值/ 问题/ [/ 84/ -/ 87/ ]/ ./ 香港中文大学/ 王军/ 对/ 递归/ 神经网络/ 及其/ 在/ 线性规划/ 、/ 最短/ 路径/ 寻优/ 、/ 降/ 秩/ 矩阵/ 伪逆/ 求解/ 等/ 问题/ 的/ 应用/ 上/ 进行/ 了/ 深入/ 的/ 研究/ ,/ 推动/ 了/ 神经网络/ 在/ 工程/ 领域/ 的/ 应用/ [/ 88/ -/ 90/ ]/ ./ 西安交通大学/ 郑/ 南宁/ 使用/ 确定性/ 退火/ 方法/ 训练/ 径向/ 基/ 神经网络/ ,/ 取得/ 了/ 较/ 传统/ BP/ 算法/ 更好/ 的/ 学习/ 精度/ 和/ 泛化/ 能力/ ,/ 同时/ 降低/ 了/ 学习/ 所/ 需/ 的/ 时间/ [/ 91/ ]/ ./ 此外/ ,/ 国内外/ 一些/ 学者/ 和/ 专家/ 也/ 出版/ 了/ 关于/ 神经网络/ 方面/ 的/ 系统/ 论著/ ./ 在/ 国内/ ,/ 主要/ 包括/ 西安电子科技大学/ 焦/ 李成/ 所/ 编著/ 的/ 《/ 神经/ 网络系统/ 理论/ 》/ [/ 92/ ]/ 、/ 《/ 神经网络/ 的/ 应用/ 与/ 实现/ 》/ [/ 93/ ]/ 和/ 《/ 神经网络/ 计算/ 》/ [/ 94/ ]/ 、/ 北京邮电大学/ 钟义信/ 等/ 人/ 合编/ 的/ 《/ 智能/ 理论/ 与/ 技术/ —/ —/ —/ 人工智能/ 与/ 神经网络/ 》/ [/ 95/ ]/ 、/ 四川大学/ 章毅/ 编写/ 的/ 《/ ConvergenceAnalysisofRecurrentNeuralNetworks/ 》/ [/ 96/ ]/ 、/ 中国科学院/ 计算所/ 史忠植/ 编写/ 的/ 《/ 神经网络/ 》/ [/ 97/ ]/ 、/ 西南/ 交通/ 大学/ 靳蕃/ 等/ 人/ 编著/ 的/ 《/ 神经网络/ 与/ 神经/ 计算机/ 》/ [/ 98/ ]/ 、/ 南京大学/ 周志华/ 编著/ 的/ 《/ 神经网络/ 及其/ 应用/ 》/ [/ 99/ ]/ 、/ 复旦大学/ 张立明/ 编著/ 的/ 《/ 人工神经网络/ 的/ 模型/ 及其/ 应用/ 》/ [/ 100/ ]/ 、/ 中国科学院自动化研究所/ 黄秉宪/ 编著/ 的/ 《/ 脑/ 的/ 高级/ 功能/ 与/ 神经网络/ 》/ [/ 101/ ]/ 、/ 北京工商大学/ 韩力/ 群/ 编著/ 的/ 《/ 人工神经网络/ 教程/ 》/ [/ 102/ ]/ 和/ 《/ 人工神经网络/ 理论/ 、/ 设计/ 及/ 应用/ 》/ [/ 103/ ]/ 、/ 清华大学/ 袁曾/ 任/ 编写/ 的/ 《/ 人工/ 神经元网络/ 及其/ 应用/ 》/ [/ 104/ ]/ 、/ 北京理工大学/ 陈/ 祥光/ 和/ 裴/ 旭东/ 编著/ 的/ 《/ 人工神经网络/ 技术/ 及/ 应用/ 》/ [/ 105/ ]/ 、/ 北京/ 交通/ 大学/ 罗/ 四维/ 编著/ 的/ 《/ 人工神经网络/ 建造/ 》/ [/ 106/ ]/ 、/ 浙江大学/ 杨建刚/ 编著/ 的/ 《/ 人工神经网络/ 实用教程/ 》/ [/ 107/ ]/ 、/ 合肥工业大学/ 高隽/ 编著/ 的/ 《/ 人工神经网络/ 原理/ 及/ 仿真/ 实例/ 》/ [/ 108/ ]/ 、/ 上海/ 海事/ 大学/ 朱大奇/ 和/ 史慧/ 编著/ 的/ 《/ 人工神经网络/ 原理/ 及/ 应用/ 》/ [/ 109/ ]/ ./ 在/ 国外/ ,/ 主要/ 包括/ 美国/ 俄克拉何马/ 州立大学/ 的/ Hagan/ 等/ 人/ 编写/ 的/ 《/ NeuralNetworkDesign/ 》/ [/ 110/ ]/ 、/ 加拿大/ 麦克马斯特/ 大学/ Haykin/ 编写/ 的/ 《/ NeuralNetworksandLearningMachines/ 》/ [/ 111/ ]/ 和/ 《/ NeuralNetworks/ :/ AComprehensiveFoundation/ 》/ [/ 112/ ]/ 、/ 美国/ 路易斯维尔/ 大学/ 的/ Zurada/ 编写/ 的/ 《/ IntroductiontoArtificialNeuralSystems/ 》/ [/ 113/ ]/ 、/ 美国/ 卡内基/ 梅隆/ 大学/ 的/ Mitchell/ 编写/ 的/ 《/ MachineLearning/ 》/ [/ 114/ ]/ 、/ 美国/ 休斯顿/ 大学/ 的/ Freeman/ 和/ Skapura/ 合著/ 的/ 《/ NeuralNetworks/ :/ Algorithms/ ,/ Applications/ ,/ andProgrammingTechniques/ 》/ [/ 115/ ]/ 、/ 美国/ 乔治亚/ 南方/ 大学/ 的/ Fausett/ 编写/ 的/ 《/ FundamentalsofNeuralNetworks/ :/ Architectures/ ,/ AlgorithmsandApplications/ 》/ [/ 116/ ]/ 、/ 澳大利亚/ 莫/ 纳什/ 大学/ 的/ Veelenturf/ 编写/ 的/ 《/ AnalysisandApplicationsofArtificialNeuralNetworks/ 》/ [/ 117/ ]/ 、/ 荷兰/ 阿姆斯特丹/ 大学/ 的/ Krose/ 等/ 人/ 编写/ 的/ 《/ AnIntroductiontoNeuralNetworks/ 》/ [/ 118/ ]/ 、/ 英国/ 佩斯利/ 大学/ 的/ Fyfe/ 编写/ 的/ 《/ ArtificialNeuralNetworks/ 》/ [/ 119/ ]/ 、/ 美国麻省理工学院/ 的/ Kasabov/ 和/ Arbib/ 分别/ 编写/ 的/ 《/ FoundationsofNeuralNetworks/ ,/ FuzzySystems/ ,/ Page9andKnowledgeEngineering/ 》/ [/ 120/ ]/ 和/ 《/ TheHandbookofBrainTheoryandNeuralNetworks/ 》/ [/ 121/ ]/ 、/ 加拿大/ 萨斯喀彻温/ 大学/ 的/ Gupta/ 等/ 人/ 编著/ 的/ 《/ StaticandDynamicNeuralNetworks/ :/ FromFundamentalstoAdvancedTheory/ 》/ [/ 122/ ]/ 、/ 由/ Taylor/ 编辑出版/ 的/ 《/ MethodsandProceduresfortheVerificationandValidationofArtificialNeuralNetworks/ 》/ [/ 123/ ]/ 、/ 西班牙/ 拉科/ 鲁尼亚/ 大学/ 的/ Rabual/ 和/ Dorado/ 合著/ 的/ 《/ ArtificialNeuralNetworksinReal/ -/ lifeApplications/ 》/ [/ 124/ ]/ 、/ 俄罗斯/ 莫斯科/ 物理/ 技术/ 学院/ 的/ Galushkin/ 编写/ 的/ 《/ NeuralNetworksTheory/ 》/ [/ 125/ ]/ ./ 以上/ 著作/ 从/ 神经网络/ 的/ 基本原理/ 、/ 网络/ 设计/ 优化/ 以及/ 网络/ 的/ 应用/ 等/ 角度/ 对/ 神经网络/ 做/ 了/ 系统/ 的/ 梳理/ 和/ 阐释/ ,/ 是/ 学习/ 和/ 研究/ 神经网络/ 的/ 重要/ 参考/ 书籍/ ./ 同时/ ,/ 神经网络/ 也/ 为/ 一些/ 学科/ 的/ 发展/ 奠定/ 了/ 坚实/ 的/ 基础/ ,/ 形成/ 了/ 新/ 的/ 理论体系/ 和/ 方法论/ ./ 其中/ ,/ 西安电子科技大学/ 焦/ 李成/ 等/ 人/ 编著/ 的/ 《/ 自/ 适应/ 多/ 尺度/ 网络理论/ 与/ 应用/ 》/ [/ 126/ ]/ 和/ 《/ 智能/ 目标/ 识别/ 与/ 分类/ 》/ [/ 127/ ]/ 、/ 清华大学/ 杨行峻/ 编著/ 的/ 《/ 人工神经网络/ 与/ 盲/ 信号处理/ 》/ [/ 128/ ]/ 、/ 清华大学/ 阎/ 平凡/ 和/ 张长/ 水/ 编著/ 的/ 《/ 人工神经网络/ 与/ 模拟/ 进化/ 计算/ 》/ [/ 129/ ]/ 、/ 中国/ 科学技术/ 大学/ 丛爽/ 编著/ 的/ 《/ 神经网络/ 、/ 模糊控制/ 及其/ 在/ 运动/ 控制/ 中/ 的/ 应用/ 》/ [/ 130/ ]/ 、/ 东北大学/ 虞和济/ 编著/ 的/ 《/ 基于/ 神经网络/ 的/ 智能/ 诊断/ 》/ [/ 131/ ]/ 、/ 哈尔滨工业大学/ 权太范/ 编著/ 的/ 《/ 信息/ 融合/ 神经网络/ —/ —/ —/ 模糊推理/ 理论/ 与/ 应用/ 》/ [/ 132/ ]/ 、/ 华中科技大学/ 廖晓昕/ 编著/ 的/ 《/ StabilityofDynamicalSystems/ 》/ [/ 133/ ]/ 和/ 《/ 动力系统/ 的/ 稳定性/ 理论/ 和/ 应用/ 》/ [/ 134/ ]/ 、/ 中国科学院/ 大学/ 的/ 刘德荣/ 等/ 人/ 编著/ 的/ 《/ QualitativeAnalysisandSynthesisofRecurrentNeuralNet/ -/ works/ 》/ [/ 135/ ]/ 、/ 国防科学技术大学/ 的/ 胡/ 德文/ 等/ 人/ 编著/ 的/ 《/ 神经网络/ 自/ 适应控制/ 》/ [/ 136/ ]/ 、/ 中国科学院/ 自动化/ 所/ 戴汝为/ 院士/ 编著/ 的/ 《/ 人工智能/ 》/ [/ 137/ ]/ 、/ 清华大学/ 罗发龙/ 和/ 李衍达/ 院士/ 合著/ 的/ 《/ 神经网络/ 信号处理/ 》/ [/ 138/ ]/ 、/ 清华大学/ 阎/ 平凡/ 等/ 人/ 编著/ 的/ 《/ 神经网络/ 与/ 模糊控制/ 》/ [/ 139/ ]/ 和/ 《/ 人工神经网络/ 与/ 模拟/ 进化/ 计算/ 》/ [/ 140/ ]/ 、/ 东北大学/ 张化光/ 编著/ 的/ 《/ 递归/ 时滞/ 神经网络/ 的/ 综合/ 分析/ 与/ 动态/ 特性/ 研究/ 》/ [/ 141/ ]/ 、/ 中国科学院/ 合肥/ 智能/ 所/ 黄德双/ 编著/ 的/ 《/ 神经网络/ 模式识别/ 系统/ 理论/ 》/ [/ 142/ ]/ 是/ 这方面/ 的/ 典型/ 代表/ ./ 3/ 深度/ 学习/ 研究进展/ 神经网络/ 曾/ 是/ 机器/ 学习/ 领域/ 中/ 一个/ 特别/ 火/ 的/ 研究/ 方向/ ,/ 但/ 由于/ 其/ 容易/ 过/ 拟合/ 且/ 参数/ 训练/ 速度慢/ ,/ 后来/ 又/ 慢慢/ 淡出/ 了/ 人们/ 的/ 视线/ ./ 传统/ 的/ 人工神经网络/ 相比/ 生物/ 神经网络/ 是/ 一个/ 浅层/ 的/ 结构/ ,/ 这/ 也/ 是/ 为什么/ 人工神经网络/ 不能/ 像/ 人脑/ 一样/ 智能/ 的/ 原因/ 之一/ ./ 随着/ 计算机/ 处理速度/ 和/ 存储/ 能力/ 的/ 提高/ ,/ 深层/ 神经网络/ 的/ 设计/ 和/ 实现/ 已/ 逐渐/ 成为/ 可能/ ./ 2006/ 年/ ,/ 一篇/ 题为/ 《/ ReducingtheDimensionalityofDatawithNeuralNetworks/ 》/ 的/ 文章/ 在/ 《/ 科学/ 》/ 杂志/ 上/ 发表/ [/ 12/ ]/ ,/ 掀起/ 了/ 深度/ 学习/ 在/ 学术界/ 和/ 工业界/ 的/ 研究/ 热潮/ ,/ 其/ 作者/ 是/ 来自/ 加拿大多伦多大学/ 的/ 教授/ Hinton/ 和/ 他/ 的/ 学生/ Salakhutdinov/ ./ 他们/ 在/ 文中/ 独辟蹊径/ 阐述/ 了/ 两个/ 重要/ 观点/ :/ 一是/ 多/ 隐层/ 的/ 神经网络/ 可以/ 学习/ 到/ 能/ 刻画/ 数据/ 本质属性/ 的/ 特征/ ,/ 对/ 数据/ 可视化/ 和/ 分类/ 等/ 任务/ 有/ 很大/ 帮助/ ;/ 二是/ 可以/ 借助于/ 无/ 监督/ 的/ “/ 逐层/ 初始化/ ”/ 策略/ 来/ 有效/ 克服/ 深层/ 神经网络/ 在/ 训练/ 上/ 存在/ 的/ 难度/ ./ 正如/ 之前/ 提到/ 的/ ,/ 神经网络/ 的/ 训练/ 算法/ 一直/ 是/ 制约/ 其/ 发展/ 的/ 一个/ 瓶颈/ ,/ 网络层/ 数/ 的/ 增加/ 对/ 参数/ 学习/ 算法/ 提出/ 了/ 更/ 严峻/ 的/ 挑战/ ./ 传统/ 的/ BP/ 算法/ 实际上/ 对于/ 仅/ 含/ 几层/ 的/ 网络/ 训练/ 效果/ 就/ 已经/ 很/ 不/ 理想/ ,/ 更/ 不/ 可能/ 完成/ 对/ 深层/ 网络/ 的/ 学习/ 任务/ ./ 基于/ 此/ ,/ Hinton/ 等/ 人/ 提出/ 了/ 基于/ “/ 逐层/ 预/ 训练/ ”/ 和/ “/ 精调/ ”/ 的/ 两/ 阶段/ 策略/ ,/ 解决/ 了/ 深度/ 学习/ 中/ 网络/ 参数/ 训练/ 的/ 难题/ [/ 143/ -/ 145/ ]/ ./ 继/ Hinton/ 之后/ ,/ 纽约大学/ 的/ LeCun/ 、/ 蒙特利尔/ 大学/ 的/ Bengio/ 和/ 斯坦福大学/ 的/ Ng/ 等/ 人/ 分别/ 在/ 深度/ 学习/ 领域/ 展开/ 了/ 研究/ ,/ 并/ 提出/ 了/ 自/ 编码器/ [/ 146/ -/ 150/ ]/ 、/ 深度/ 置信/ 网/ [/ 151/ -/ 155/ ]/ 、/ 卷积/ 神经网络/ 等/ 深度/ 模型/ [/ 156/ -/ 160/ ]/ ,/ 在/ 多个/ 领域/ 得到/ 了/ 应用/ ./ 2015/ 年/ CVPR/ 收录/ 的/ 论文/ 中/ 与/ 深度/ 学习/ 有关/ 的/ 就/ 有/ 近/ 百篇/ ,/ 应用/ 遍及/ 计算机/ 视觉/ 的/ 各个/ 方向/ ./ 以下/ 将/ 对/ 深度/ 学习/ 在/ 过去/ 十年/ 内/ 的/ 发展/ 进行/ 一定/ 的/ 梳理/ ,/ 并/ 对/ 一些/ 典型/ 的/ 深度/ 学习/ 模型/ 进行/ 回顾/ 和/ 分析/ ./ 自/ 编码器/ (/ Autoencoder/ )/ 是/ 一种/ 无/ 监督/ 的/ 特征/ 学习/ 网络/ ,/ 它/ 利用/ 反向/ 传播/ 算法/ ,/ 让/ 目标/ 输出/ 值/ 等于/ 输入/ 值/ ,/ 其/ 结构/ 如图/ 6/ 所示/ ./ 对于/ 一个/ 输入/ x/ ∈/ / 首先/ 将/ 其/ 通过/ 一个/ 特征/ 映射/ 得到/ 对应/ 的/ 隐藏/ 层/ 表示/ h/ ∈/ / m/ ,/ 隐藏/ 层/ 表示/ 接着/ 被/ 投影/ 到/ 输出/ 层/ y/ ∈/ / Page10/ 且/ 希望/ 输出/ 与/ 原始/ 输入/ 尽可能/ 相等/ ./ 自/ 编码器/ 试图/ 学习/ 一个/ 恒等/ 函数/ ,/ 当/ 隐藏/ 层/ 的/ 数目/ 小于/ 输入/ 层/ 的/ 数目/ 时/ 可以/ 实现/ 对/ 信号/ 的/ 压缩/ 表示/ ,/ 获得/ 对/ 输入/ 数据/ 有/ 意义/ 的/ 特征/ 表示/ ./ 通常/ 隐层/ 权值/ 矩阵/ 和/ 输出/ 层权值/ 矩阵/ 互为/ 转置/ ,/ 这样/ 大大减少/ 了/ 网络/ 的/ 参数/ 个数/ ./ 当/ 输入/ 数据/ 中/ 包含/ 噪声/ 时/ ,/ 自/ 编码器/ 的/ 性能/ 将会/ 受到/ 影响/ ./ 为了/ 使自/ 编码器/ 更加/ 鲁棒/ ,/ 2008/ 年/ Vincent/ 及/ Bengio/ 等/ 人/ 提出/ 了/ 去/ 噪自/ 编码器/ (/ DenoisingAutoencoder/ )/ 的/ 概念/ ,/ 在/ 输入/ 数据/ 进行/ 映射/ 之前/ 先对/ 其/ 添加/ 随机噪声/ ,/ 然后/ 将/ 加噪后/ 的/ 数据/ 进行/ 编码/ 和/ 解码/ 操作/ ,/ 并/ 希望/ 解码/ 出来/ 的/ 输出/ 信号/ 能够/ 逼近/ 原来/ 的/ 干净/ 输入/ 信号/ [/ 161/ ]/ ./ 去/ 噪自/ 编码器/ 的/ 原理/ 如图/ 7/ 所示/ ./ 图/ 7/ 中/ ,/ x/ 是/ 原始/ 信号/ ,/ x/ / 是/ 加噪后/ 的/ 信号/ ,/ h/ 是/ 编码/ 后/ 的/ 信号/ ,/ y/ 是/ 解码/ 后/ 的/ 信号/ ,/ d/ (/ x/ ,/ y/ )/ 是/ 原始/ 信号/ 和/ 解码/ 后/ 信号/ 的/ 差异/ ,/ 通常/ 希望/ 其/ 越小越/ 好/ ./ 通过/ 在/ 原始/ 信号/ 中/ 加入/ 一定量/ 的/ 随机噪声/ 来/ 模拟/ 真实/ 数据/ 中/ 存在/ 的/ 干扰/ ,/ 可以/ 更加/ 鲁棒/ 地/ 从/ 数据/ 中/ 学习/ 到/ 有/ 意义/ 的/ 特征/ ./ 如果/ 将/ 稀疏/ 性/ 引入/ 到/ 自/ 编码器/ 中/ 还/ 可以/ 得到/ 另/ 一种/ 被/ 称为/ 稀疏/ 自/ 编码器/ (/ SparseAutoencoder/ )/ 的/ 网络/ ,/ 这种/ 网络/ 限制/ 每次/ 获得/ 的/ 编码/ 尽量/ 稀疏/ ,/ 从而/ 来/ 模拟/ 人脑/ 中/ 神经元/ 刺激/ 和/ 抑制/ 的/ 规律/ ./ 稀疏/ 自/ 编码器/ 的/ 优化/ 模型/ 如下/ :/ min/ 其中/ ,/ h/ =/ WTx/ 为/ 编码/ 后/ 的/ 信号/ ,/ 通过/ 式/ (/ 21/ )/ 中/ 的/ 第二项/ 可以/ 约束/ 编码/ 信号/ 足够/ 稀疏/ ,/ 来/ 获得/ 对/ 原始/ 信号/ 更加/ 紧凑/ 简洁/ 的/ 表示/ ./ 同时/ ,/ 将/ 若干个/ 自/ 编码器/ 堆叠/ 在/ 一起/ 可以/ 形成/ 栈式/ 自/ 编码器/ ,/ 这种/ 深层/ 网络/ 能/ 学习/ 到/ 输入/ 信号/ 的/ 层次化/ 表示/ ,/ 更/ 有利于/ 提取/ 数据/ 中/ 所/ 蕴含/ 的/ 抽象/ 特征/ [/ 162/ ]/ ./ 一个/ 简单/ 的/ 栈式/ 自/ 编码器/ 的/ 结构/ 如图/ 8/ 所示/ ./ 首先/ ,/ 将/ 原始数据/ x/ 输入/ 到/ 栈式/ 自/ 编码器/ 中/ ,/ 通过/ 第一层/ 的/ 编码/ 得到/ 原始数据/ 的/ 一阶/ 特征/ 表示/ h1/ ,/ 然后/ 将/ 此/ 一阶/ 特征/ 作为/ 下/ 一个/ 自/ 编码器/ 的/ 输入/ ,/ 对/ 其/ 进行/ 进一步/ 的/ 编码/ 得到/ 二阶/ 特征/ h2/ ,/ 如此/ 重复/ 进行/ 直到/ 编码/ 完毕/ ./ 编码/ 后/ 的/ 各阶/ 特征/ 便/ 构成/ 了/ 对/ 原始数据/ 的/ 层次化/ 描述/ ,/ 可以/ 用于/ 后续/ 的/ 分类/ 和/ 识别/ 任务/ 中/ ./ 在/ 训练/ 阶段/ ,/ 首先/ 从/ 第一层/ 开始/ ,/ 按照/ 单个/ 自/ 编码器/ 的/ 训练/ 方式/ 逐层/ 训练/ 网络/ 参数/ ,/ 接着/ 将/ 最后/ 一层/ 的/ 输出/ 和/ 期望/ 输出/ 的/ 误差/ 进行/ 逐层/ 反向/ 传播/ ,/ 微调/ 网络/ 各层/ 的/ 参数/ ./ 深度/ 信念/ 网/ (/ DeepBeliefNetwork/ ,/ DBN/ )/ 是/ 由/ Hinton/ 在/ 2006/ 年/ 提出/ 的/ ,/ 它/ 是/ 一种/ 生成/ 模型/ ,/ 通过/ 训练/ 神经元/ 之间/ 的/ 权重/ ,/ 可以/ 让/ 整个/ 神经网络/ 按照/ 最大/ 概率/ 来/ 生成/ 训练/ 数据/ ,/ 其/ 结构/ 如图/ 9/ 所示/ [/ 10/ ]/ ./ DBN/ 是/ 由/ 多层/ RBM/ 堆叠/ 而成/ 的/ ,/ 神经元/ 可以/ 分为/ 显性/ 神经元/ 和/ 隐性/ 神经元/ ,/ 显性/ 神经元/ 用于/ 接受/ 输入/ ,/ 隐性/ 神经元/ 用以/ 提取/ 特征/ ,/ 最/ 顶上/ 的/ 两层/ 连接/ 是/ 无/ 向/ 的/ 用以/ 组成/ 联想/ 记忆/ 单元/ ./ DBN/ 的/ 参数/ 学习/ 过程/ 分为/ 无/ 监督/ 贪婪/ 逐层/ 训练/ 和/ ContrastiveWake/ -/ Sleep/ 调优/ 两个/ 阶段/ ./ 在/ 第一阶段/ ,/ 首先/ 充分/ 训练/ 第一个/ RBM/ ,/ 接着/ 固定/ 第一个/ RBM/ 的/ 权重/ 和/ 偏置/ ,/ 将/ 其/ 隐层/ 神经元/ 的/ 状态/ 作为/ 第二个/ RBM/ 的/ 输入/ 向量/ ,/ 再/ 对/ 第二个/ RBM/ 进行/ 训练/ ,/ 如此/ 依次/ 进行/ ./ 如果/ 顶层/ 的/ RBM/ 除了/ 有/ 显性/ 神经元/ 还/ 包括/ 代表/ 分类/ 标签/ 的/ 神经元/ ,/ 则/ 需要/ 将/ 其/ 一起/ 进行/ 训练/ ./ 第二个/ 阶段/ 是/ 参数/ 的/ 调优/ 过程/ ,/ 分为/ Wake/ (/ 认知/ 过程/ )/ 和/ Sleep/ (/ 生成/ 过程/ )/ 两个/ 子/ 阶段/ ./ 在/ Wake/ 阶段/ ,/ 外界/ 的/ 输入/ 特征/ 在/ 向上/ 权重/ 的/ 作用/ 下/ 产生/ 出/ 各层/ 的/ 输出/ ,/ 再/ 基于/ 梯度/ 下降/ 法来/ 修改/ 各层/ 的/ 下行/ 权重/ ;/ 在/ Sleep/ 阶段/ 通过/ 顶层/ 表示/ 和/ 向下/ 权重/ 生成/ 底层/ 的/ 状态/ ,/ 同时/ 修改/ 向上/ 的/ 权重/ ./ DBN/ 在/ 特征提取/ 过程/ 中/ ,/ 先/ 将/ 输入/ 信号/ 拉/ 成/ 向量/ 再/ 投入/ 到/ 网络/ 中/ ,/ 忽略/ 了/ 数据/ 中/ 存在/ 的/ 空间结构/ ,/ 在/ 处理/ 图像/ 和/ 视频/ 这样/ 的/ 二维/ 或/ 高维/ 信号/ 时会/ 存在/ 一定/ 问题/ ./ 同时/ ,/ 当/ 输入/ 信号/ 的/ 维数/ 过/ 高且/ 网络/ 的/ 深度/ 过/ Page11/ 深时/ ,/ 网络/ 中/ 需要/ 训练/ 的/ 参数/ 会/ 很多/ ,/ 这/ 给/ 存储/ 和/ 计算/ 提出/ 了/ 很/ 高/ 的/ 要求/ ./ 目前/ ,/ 更为/ 常用/ 的/ 一种/ 深层/ 网络/ (/ 也/ 是/ 首个/ 被/ 真正/ 成功/ 训练/ 的/ 深层/ 网络/ )/ 是/ 卷积/ 神经网络/ (/ ConvolutionalNeuralNetworks/ ,/ CNNs/ )/ ,/ 它/ 在/ 处理/ 手写体/ 识别/ 等/ 图像处理/ 任务/ 中/ 表现/ 出/ 了/ 优越/ 的/ 性能/ ./ 卷积/ 神经网络/ 采用/ 了/ 局部/ 感受/ 野/ 、/ 权值/ 共享/ 以及/ 时间/ 或/ 空间/ 亚/ 采样/ 的/ 结构/ 思想/ ,/ 使得/ 网络/ 中/ 自由/ 参数/ 的/ 个数/ 大大减少/ ,/ 降低/ 了/ 网络/ 参数/ 选择/ 的/ 复杂度/ ,/ 其/ 在/ 识别/ 具有/ 移位/ 、/ 伸缩/ 或/ 扭曲/ 不变/ 的/ 二维/ 模式/ 时有/ 很/ 好/ 的/ 性能/ ,/ 最为/ 典型/ 的/ 例子/ 便是/ 用于/ 美国/ 大多数/ 银行/ 支票/ 上/ 手写/ 数字/ 识别/ 的/ LeNet/ -/ 5/ [/ 163/ -/ 164/ ]/ ./ 卷积/ 神经网络/ 的/ 基本原理/ 如图/ 10/ 所示/ ./ CNN/ 是/ 一种/ 多层/ 网络结构/ ,/ 每/ 一层/ 是/ 由/ 包含/ 多个/ 独立/ 神经元/ 的/ 若干/ 二维/ 平面/ 组成/ 的/ ,/ 这些/ 二维/ 平面/ 通常/ 被称作/ 卷积/ 核/ ./ 以图/ 10/ 为例/ ,/ 输入/ 图像/ 首先/ 和/ 3/ 个/ 可/ 调节/ 的/ 卷积/ 核/ 进行/ 卷积/ 运算/ ,/ 在/ C1/ 层/ 得到/ 对应/ 的/ 3/ 个/ 特征/ 映射/ 图/ ,/ 然后/ 对/ 每/ 一幅/ 特征/ 映射/ 图/ 进行/ 池化/ 后/ 经过/ Sigmoid/ 函数/ 产生/ S2/ 层/ 的/ 3/ 幅/ 特征/ 映射/ 图/ ./ 这些/ 特征/ 映射/ 图/ 继续/ 通过/ 滤波/ 操作/ 后/ 得到/ C3/ 层/ ,/ 再/ 按照/ 和/ S2/ 层/ 一样/ 的/ 处理/ 方法/ 得到/ S4/ 的/ 结果/ ./ 在/ 最后/ 一层/ 将/ 所有/ 像素/ 光栅/ 化拉成/ 一个/ 列/ ,/ 并/ 输入/ 到/ 传统/ 的/ 神经网络/ 之中/ 得到/ 最终/ 的/ 输出/ 结果/ ./ CNN/ 一个/ 很/ 重要/ 的/ 特点/ 就是/ 通过/ 局部/ 感受/ 野/ 、/ 权值/ 共享/ 以及/ 时间/ 或/ 空间/ 亚/ 采样/ 3/ 种/ 思想/ 减少/ 了/ 网络/ 中/ 自由/ 参数/ 的/ 个数/ ,/ 获得/ 了/ 某种程度/ 的/ 位移/ 、/ 尺度/ 、/ 形变/ 不变性/ ./ CNN/ 的/ 训练/ 算法/ 上/ 仍然/ 采用/ 传统/ 的/ 误差/ 反向/ 传播/ 思想/ ,/ 通常/ 具有/ 较/ 快/ 的/ 收敛/ 速度/ ./ 2014/ 年/ ,/ Mnih/ 等/ 人/ 针对/ 图像/ 分类/ 任务/ 提出/ 了/ 基于/ 视觉/ 注意/ 的/ 递归/ 神经网络/ ,/ 进一步/ 降低/ 了/ 卷积/ 神经网络/ 在/ 处理/ 图像/ 时/ 的/ 计算/ 开销/ [/ 165/ ]/ ./ 通过/ 视觉/ 注意/ 机制/ 有/ 选择地/ 从/ 图像/ 中/ 提取/ 有利于/ 分类/ 的/ 特征/ ,/ 可以/ 更/ 准确/ 地/ 对待/ 识别/ 物体/ 进行/ 描述/ ,/ 同时/ 避免/ 了/ 冗余/ 的/ 全局/ 处理/ ./ 深度/ 学习/ 思想/ 的/ 提出/ 是/ 对/ 传统/ 特征选择/ 与/ 提取/ 框架/ 的/ 突破/ ,/ 正对/ 包括/ 计算机/ 视觉/ 、/ 自然语言/ 处理/ (/ NaturalLanguageProcessing/ ,/ NLP/ )/ 、/ 生物医学/ 分析/ 、/ 遥感/ 影像/ 解译/ 在内/ 的/ 诸多/ 领域/ 产生/ 越来越/ 重要/ 的/ 影响/ ./ Krizhevsky/ 等/ 人/ 训练/ 了/ 一个/ 包含/ 65/ 万个/ 神经元/ 的/ 深层/ 卷积/ 网络/ 来/ 对/ 包括/ 1000/ 类/ 的/ 120/ 万幅/ 图像/ 进行/ 分类/ ,/ 并/ 取得/ 了/ 比/ 经典/ 方法/ 更优/ 的/ 分类/ 性能/ [/ 166/ -/ 167/ ]/ ./ Farabet/ 等/ 人/ 提出/ 了/ 多/ 尺度/ 卷积/ 神经网络/ 来/ 捕捉/ 区域/ 的/ 纹理/ 、/ 形状/ 和/ 文本/ 特征/ 用以/ 场景/ 标注/ ,/ 避免/ 了/ 手工/ 特征/ 的/ 设计/ 和/ 组合/ 所/ 存在/ 的/ 问题/ [/ 3/ ]/ ./ Ciresan/ 等/ 人/ 提出/ 基于/ GPU/ 并行/ 加速/ 的/ 卷积/ 神经网络/ ,/ 用以/ 自动/ 学习/ 交通/ 标识符/ 的/ 特征/ ,/ 在/ 德国/ 交通/ 标识符/ 数据库/ 上/ 取得/ 了/ 比/ 人工/ 判读/ 更/ 高/ 的/ 识别率/ [/ 168/ ]/ ./ Tang/ 等/ 人/ 提出/ 了/ 鲁棒/ 波尔兹曼/ 机/ (/ RobustBoltzmannMachine/ ,/ RoBM/ )/ ,/ 使得/ 波尔兹曼/ 机对/ 干扰/ 更加/ 鲁棒/ ,/ 在/ 人脸/ 数据/ 集上/ 的/ 去/ 噪/ 和/ 修补/ 任务/ 中/ 取得/ 了/ 更好/ 的/ 效果/ [/ 33/ ]/ ./ Mohamed/ 等/ 人/ 提出/ 了/ 用/ 深度/ 信念/ 网来/ 进行/ 语音/ 建模/ ,/ 克服/ 了/ 传统/ 的/ 隐/ 马尔可夫/ 模型/ 在/ 建模/ 时所/ 施加/ 的/ 条件/ 独立/ 假设/ ,/ 能/ 提取/ 出/ 深层/ 的/ 表示/ 特征/ 用以/ 识别/ 任务/ [/ 7/ ]/ ./ Socher/ 等/ 人/ 提出/ 了/ 基于/ 递归/ 自/ 编码器/ (/ RecursiveAutoencoders/ ,/ RAEs/ )/ 的/ 自然语言/ 释义/ 检测/ (/ ParaphraseDetection/ )/ ,/ 借助于/ 无/ 监督/ 的/ RAEs/ 来/ 学习/ 句子/ 的/ 特征向量/ ,/ 并/ 将/ 学习/ 到/ 的/ 特征/ 用以/ 度量/ 两个/ 句子/ 中词/ 和/ 短语/ 的/ 相似性/ ,/ 在/ MSRP/ 释义/ 语料库/ 上/ 取得/ 了/ 较/ 经典/ 方法/ 更优/ 的/ 性能/ [/ 148/ ]/ ./ Glorot/ 等/ 人/ 基于/ 堆栈/ 去/ 噪自/ 编码器/ 从/ 评论/ 数据/ 中/ 无/ 监督/ 地/ 学习/ 特征/ 用于/ 情感/ 分类/ ,/ 在/ 亚马逊/ 等/ 工业/ 数据/ 集上/ 取得/ 了/ 较/ 经典/ 方法/ 更优/ 的/ 分类/ 精度/ [/ 169/ ]/ ./ Zhao/ 等/ 人/ 将/ 2013/ 年/ ImageNet/ 竞赛/ 上/ 获胜/ 的/ 卷积/ 网络/ 用以/ 显著/ 检测/ 建模/ ,/ 通过/ 全局/ 和/ 局部/ 文本/ 的/ 联合/ 学习/ 在/ 多个/ 测试数据/ 集上/ 取得/ 了/ 较/ 传统/ 方法/ 更优/ 的/ 检测/ 性能/ [/ 6/ ]/ ./ Williamson/ 等/ 人/ 使用/ 深度/ 神经网络/ 估计/ 语音/ 的/ 非负/ 激活/ 矩阵/ ,/ 用以/ 从/ 噪声/ 中/ 提取/ 干净/ 语音/ 信息/ ,/ 并/ 获得/ 了/ 比/ Masking/ 和/ NMF/ 方法/ 更好/ 的/ 提取/ 效果/ [/ 170/ ]/ ./ 徐宗本/ 等/ 人/ 提出/ 用/ 卷积/ 神经网络/ 来/ 预测/ 图像/ 块/ 运动/ 模糊/ 的/ 概率分布/ ,/ 在/ 单幅/ 图像/ 的/ 非/ 均匀/ 运动/ 去模糊/ 问题/ 上/ 取得/ 了/ 较为理想/ 的/ 结果/ [/ 171/ ]/ ./ Ouyang/ 等/ 人/ 将/ 行人/ 检测/ 问题/ 中/ 的/ 特征提取/ 、/ 变形/ 和/ 遮挡/ 处理/ 以及/ 分类/ 4/ 个/ 模块/ 统一/ 于/ 深度/ 学习/ 框架/ 之下/ ,/ 通过/ 各/ 部分/ 之间/ 的/ 协同/ 来/ 达到/ 整体/ 性能/ 的/ 提升/ ,/ 并/ 在/ 最大/ 的/ 行人/ 检测/ 数据库/ Caltech/ 上/ ,/ 以/ 9/ %/ 的/ 优势/ 超越/ 之前/ 最好/ 的/ 方法/ [/ 172/ ]/ ./ 类似/ 地/ ,/ 在/ 姿态/ 估计/ 问题/ 上/ 也/ 将/ 视觉表象/ 得分/ 、/ 表象/ 混合/ 类型/ 和/ 变形/ 这/ 3/ 类/ 信息/ 结合/ 起来/ ,/ 统一/ 于/ 多源/ 深度/ 模型/ 之中/ ,/ 在/ 3/ 组/ 基准/ 数据/ 集上/ 较/ 现有/ 方法/ 性能/ 提高/ 了/ 8.6/ %/ [/ 173/ ]/ ./ Wang/ 等/ 人/ 通过/ 离线/ 的/ 方式/ 从/ 自然/ 图像/ 中/ 训练/ 了/ 用于/ 描述/ 待/ 跟踪/ 物体/ 特征/ 的/ 堆栈/ 去/ 噪自/ Page12/ 编码器/ ,/ 在/ 复杂/ 场景/ 中/ 可以/ 提取/ 出/ 更加/ 通用/ 的/ 特征/ 用于/ 分类/ ,/ 并/ 在/ 一些/ 具有/ 挑战性/ 的/ 视频/ 序列/ 上/ 获得/ 了/ 比/ 经典/ 方法/ 更/ 准确/ 的/ 跟踪/ 精度/ 和/ 更/ 低/ 的/ 时间/ 开销/ [/ 174/ ]/ ./ Sun/ 等/ 人/ 将/ 卷积/ 神经网络/ 和/ 受限/ 玻尔兹曼/ 机/ 结合/ 起来/ ,/ 组成/ 了/ 混合/ 的/ 深度/ 神经网络/ 用于/ 人脸/ 验证/ ,/ 在/ LFW/ 数据/ 集上/ 获得/ 了/ 更优/ 的/ 验证/ 性能/ [/ 175/ ]/ ./ Wan/ 等/ 人/ 提出/ 用/ 深度/ 学习/ 模型/ 来/ 尝试/ 解决/ “/ 语义/ 鸿沟/ ”/ 问题/ ,/ 并/ 在/ 基于/ 内容/ 的/ 图像/ 检索/ 问题/ 上/ 验证/ 了/ 所/ 提/ 思路/ 的/ 有效性/ [/ 176/ ]/ ./ Dong/ 等/ 人/ 提出/ 利用/ 卷积/ 神经网络/ 来/ 直接/ 学习/ 从/ 低/ 分辨/ 到/ 高/ 分辨/ 图像/ 的/ 映射/ 关系/ ,/ 并且/ 将/ 传统/ 基于/ 稀疏/ 表示/ 的/ 方法/ 统一/ 于/ 此/ 框架/ 之下/ ,/ 通过/ 联合/ 优化/ 的/ 方式/ 得到/ 了/ 更好/ 的/ 超/ 分辨/ 重建/ 效果/ [/ 177/ ]/ ./ 在/ 通用/ 结构/ 的/ 设计/ 上/ ,/ Jia/ 等/ 人/ 开发/ 了/ 深度/ 卷积/ 网络/ 模型/ Caffe/ ,/ 可/ 用于/ 大规模/ 工业/ 应用领域/ ,/ 并且/ 已/ 被/ 用作/ 多个/ 问题/ 的/ 求解/ 方案/ [/ 178/ ]/ ./ Eitel/ 等/ 人/ 提出/ 了/ 基于/ 双层/ CNN/ 的/ RGB/ -/ D/ 物体/ 识别/ 网络结构/ ,/ 并/ 在/ 含有/ 噪声/ 的/ RGB/ -/ D/ 物体/ 数据库/ 上/ 取得/ 了/ 最优/ 的/ 识别/ 结果/ [/ 179/ ]/ ./ Palangi/ 等/ 人/ 将/ 深度/ 学习/ 的/ 概念/ 和/ 序列/ 建模/ 的/ 方法/ 结合/ 起来/ ,/ 用于/ 提升/ 多/ 观测/ 向量/ 下/ 的/ 压缩/ 感知/ 问题/ 的/ 求解/ 性能/ [/ 180/ ]/ ./ 为/ 避免/ 无/ 参考/ 图像/ 质量/ 评价/ 过程/ 中/ 繁琐/ 的/ 手工/ 特征/ 设计/ ,/ Kang/ 等/ 人/ 提出/ 将/ 特征/ 学习/ 和/ 回归/ 过程/ 统一/ 到/ CNN/ 的/ 优化/ 框架/ 下/ ,/ 并/ 在/ LIVE/ 数据/ 集上/ 取得/ 了/ 最优性/ 能/ ,/ 且/ 该/ 方法/ 可/ 用于/ 图像/ 局部/ 质量/ 的/ 评估/ [/ 181/ ]/ ./ Chen/ 等/ 人/ 提出/ 通过/ 学习/ 的/ 方式/ 来/ 让/ CNN/ 选择/ 最优/ 的/ 局部/ 接收/ 场/ ,/ 并/ 用于/ 汉字手写/ 体/ 识别/ ,/ 极大/ 地/ 提升/ 了/ 传统/ CNN/ 的/ 性能/ [/ 182/ ]/ ./ Maturana/ 等/ 人/ 将/ 体积/ 占用/ 图/ 和/ 3DCNN/ 进行/ 耦合/ ,/ 设计/ 出/ 了/ 可/ 用于/ 检测/ 植被/ 覆盖/ 地区/ 潜在/ 的/ 被/ 遮挡/ 障碍物/ 的/ 系统/ ,/ 并/ 将/ 其/ 应用/ 到/ 激光雷达/ 点云下/ 的/ 自主/ 飞行器/ 安全/ 降落/ 区域/ 的/ 检测/ 中/ ,/ 取得/ 了/ 较为理想/ 的/ 效果/ [/ 183/ ]/ ./ Tomczak/ 将/ 分类/ 受限/ 玻尔兹曼/ 机/ (/ ClassificationRestrictedBoltzmannMachine/ )/ 作为/ 独立/ 的/ 非线性/ 分类器/ 用于/ 5/ 类/ 不同/ 的/ 医学/ 问题/ 领域/ ,/ 并/ 通过/ 在/ 模型/ 中/ 添加/ 正则/ 项来/ 获得/ 稀疏/ 解/ [/ 184/ ]/ ./ Zhang/ 等/ 人/ 提出/ 了/ Coarse/ -/ to/ -/ fine/ 的/ 自/ 编码/ 网络/ CFAN/ 用于/ 人脸/ 对准/ ,/ 首先/ 用/ 第一组/ 堆栈/ 自/ 编码/ 网络/ (/ StackedAuto/ -/ encoderNetworks/ ,/ SANs/ )/ 来/ 快速/ 预测/ 脸部/ 的/ 特征/ 点/ ,/ 之后/ 用/ 第二组/ 堆栈/ 自/ 编码/ 网络/ 来/ 对/ 其/ 修正/ ,/ 在/ 3/ 组/ 数据/ 集上/ CFAN/ 都/ 取得/ 了/ 实时/ 且/ 最优/ 的/ 性能/ [/ 185/ ]/ ./ Mi/ 等/ 人/ 将/ 堆栈/ 自/ 编码器/ 用于/ 垃圾邮件/ 检测/ 任务/ ,/ 取得/ 了/ 较/ 朴素/ 贝叶斯/ 、/ 支撑/ 向量/ 机/ 、/ 决策树/ 、/ 集成/ 、/ 随机/ 森林/ 和/ 传统/ 神经网络/ 更优/ 的/ 性能/ [/ 186/ ]/ ./ 在/ 遥感/ 领域/ ,/ Yue/ 等/ 人/ 将/ 卷积/ 神经网络/ 用于/ 高/ 光谱/ 图像/ 分类/ [/ 187/ ]/ 、/ Zhou/ 等/ 人/ 将/ 自/ 编码器/ 用于/ 高/ 分辨/ 遥感/ 影像/ 的/ 检索/ 任务/ [/ 188/ ]/ ,/ 并/ 获得/ 了/ 较为/ 满意/ 的/ 分类/ 和/ 检索/ 结果/ ./ 目前/ ,/ 深度/ 网络/ 的/ 自动/ 特征提取/ 能力/ 正/ 受到/ 自然/ 、/ 生物医学/ 和/ 遥感/ 等/ 多个/ 领域/ 的/ 广泛/ 关注/ ,/ 并且/ 基于/ 深度/ 网络/ 的/ 方法/ 在/ 多个/ 任务/ 上/ 都/ 显示/ 出/ 了/ 优越/ 的/ 性能/ ,/ 在/ 未来/ 将会/ 有/ 更加/ 广阔/ 的/ 应用/ 前景/ [/ 189/ -/ 190/ ]/ ./ 近年来/ 深度/ 学习/ 的/ 研究/ 方兴未艾/ ,/ 这方面/ 的/ 书籍/ 也/ 不断涌现/ ./ 其中/ 具有/ 代表性/ 的/ 著作/ 有/ 加拿大/ 蒙特利尔/ 大学/ 的/ Bengio/ 编写/ 的/ 《/ LearningDeepArchitecturesforAI/ 》/ [/ 11/ ]/ 、/ 美国伊利诺伊大学/ 的/ Ohlsson/ 编著/ 的/ 《/ DeepLearning/ :/ HowtheMindOverridesExperience/ 》/ [/ 191/ ]/ 、/ 美国麻省理工学院/ 的/ Buduma/ 编著/ 的/ 《/ FundamentalsofDeepLearning/ 》/ [/ 192/ ]/ 、/ 微软公司/ 的/ Deng/ 和/ Yu/ 合著/ 的/ 《/ DeepLearning/ :/ MethodsandApplications/ 》/ [/ 193/ ]/ 和/ 《/ AutomaticSpeechRecognition/ :/ ADeepLearningApproach/ 》/ [/ 194/ ]/ 、/ 美国伊利诺伊大学/ 的/ Nath/ 和/ Levinson/ 合著/ 的/ 《/ AutonomousRoboticsandDeepLearning/ 》/ [/ 195/ ]/ 等/ ./ 4/ 总结/ 和/ 展望/ 作为/ 联接/ 主义/ 智能/ 实现/ 的/ 典范/ ,/ 人工神经网络/ 采用/ 广泛/ 互联/ 的/ 结构/ 与/ 有效/ 的/ 学习/ 机制/ 来/ 模拟/ 人脑/ 智能/ 信息处理/ 的/ 过程/ ,/ 是/ 人工智能/ 发展/ 历程/ 中/ 的/ 重要/ 方法/ ,/ 也/ 是/ 类脑/ 智能/ 研究/ 中/ 的/ 有效/ 工具/ ./ 在/ 神经网络/ 七十年/ 的/ 发展/ 历程/ 中/ ,/ 曾经/ 几遭/ 冷遇/ 又/ 几度/ 繁荣/ ./ 本文/ 回顾/ 了/ 神经网络/ 在/ 过去/ 七十年/ 的/ 发展/ 历程/ ,/ 介绍/ 了/ 神经网络/ 在/ 各个/ 发展/ 阶段/ 所/ 取得/ 的/ 成果/ 和/ 面临/ 的/ 挑战/ ./ 未来/ 基于/ 神经网络/ 的/ 类脑/ 智能/ 的/ 研究/ 还有/ 许多/ 亟待解决/ 的/ 问题/ 与/ 挑战/ :/ (/ 1/ )/ 认知/ 神经网络/ 尽管/ 深度/ 神经网络/ 在/ 语音/ 识别/ 和/ 图像/ // 视频/ 识别/ 等/ 任务/ 中/ 显示/ 出/ 很大/ 的/ 优势/ ,/ 现有/ 的/ 人工神经网络/ 结构/ 还/ 远远/ 不及/ 生物/ 神经网络/ 结构复杂/ ,/ 仍然/ 是/ 对/ 生物/ 神经系统/ 信息处理/ 的/ 初级/ 模拟/ ,/ 这是/ 制约/ 神经网络/ 智能化/ 水平/ 的/ 一个/ 重要/ 瓶颈/ ./ 目前/ 深层/ 神经网络/ 仅能/ 完成/ 一些/ 简单/ 的/ 语音/ 与/ 视觉/ 理解/ 任务/ ,/ 在/ 理论/ 上/ 还/ 存在/ 很多/ 局限/ ,/ 训练/ 网络/ 的/ 学习/ 算法/ 目前/ 也/ 十分/ 有限/ ./ 另一方面/ ,/ 神经/ 认知/ 计算/ 科学/ 对/ 视觉/ 注意力/ 、/ 推理/ 、/ 抉择/ 、/ 学习/ 等/ 认知/ 功能/ 的/ 研究/ 方兴未艾/ ./ 如何/ 从/ 脑科学/ 和/ 神经/ 认知科学/ 寻找/ 借鉴/ ,/ 从/ 理论/ 上/ 发展/ 出/ 功能/ 更加/ 强大/ 的/ 类脑/ 计算/ 模型/ 如/ 认知/ 神经网络/ ,/ 来/ 解决/ 人工智能/ 面临/ 的/ 局限/ ,/ 将/ 有/ 可望/ 实现/ 更/ 高层次/ Page13/ 的/ 类脑/ 智能/ ./ (/ 2/ )/ 主动/ 神经网络/ 生物/ 个体/ 在/ 与/ 环境/ 接触/ 过程/ 中/ ,/ 智能/ 水平/ 会/ 得到/ 提高/ ./ 人脑/ 可以/ 在/ 没有/ 监督/ 信息/ 时/ 主动/ 地/ 从/ 周围环境/ 中/ 学习/ ,/ 实现/ 对/ 客观/ 世界/ 中/ 物体/ 的/ 区分/ ./ 因此/ ,/ 如果/ 要/ 实现/ 更加/ 高级/ 的/ 智能/ 行为/ ,/ 现有/ 神经网络/ 的/ 发展/ 需要/ 突破/ 利用/ 神经元/ 与/ 网络结构/ 的/ 结构/ 模拟/ 思路/ ,/ 从/ 结构/ 模拟/ 向/ 功能/ 模拟/ 乃至/ 行为/ 模拟/ 转换/ ,/ 借鉴/ 人/ 与/ 环境/ 之间/ 的/ 交互/ 过程/ ,/ 主动/ 且/ 自动/ 地/ 完成/ 增强/ 学习/ ,/ 以/ 摆脱/ 对/ 监督/ 信息/ 的/ 依赖/ ,/ 在/ 更/ 严苛/ 的/ 环境/ 下/ 完成/ 学习/ 任务/ ,/ 这/ 也/ 是/ 实现/ 高级/ 类脑/ 智能/ 的/ 可能/ 途径/ ./ (/ 3/ )/ 感知/ -/ 理解/ -/ 决策/ 神经网络/ 类脑/ 智能/ 行为/ 可以/ 大概/ 归结为/ “/ 感知/ ”/ 、/ “/ 理解/ ”/ 与/ “/ 决策/ ”/ 这/ 3/ 个/ 方面/ ./ 目前/ 的/ 神经网络/ 模型/ 的/ 功能/ 大都/ 局限/ 在/ 对/ 数据/ 的/ 理解/ 层面/ ,/ 而/ 事实上/ 一个/ 高级/ 的/ 智能/ 机器/ 应该/ 具有/ 环境/ 感知/ 与/ 推理/ 决策/ 的/ 功能/ ./ 如何/ 发展/ 具有/ 环境/ 感知/ 、/ 数据/ 理解/ 以及/ 推理/ 决策/ 能力/ 的/ 网络/ 模型/ ,/ 也/ 是/ 实现/ 高级/ 类脑/ 智能/ 的/ 必然/ 要求/ ./ (/ 4/ )/ 复杂/ 神经网络/ 实现/ 机器/ 计算能力/ 的/ 提高/ 曾经/ 将/ 神经网络/ 重新/ 拉/ 回/ 大众/ 关注/ 的/ 视野/ ./ 对于/ 许多/ 互联网/ 公司/ 来说/ ,/ 如何/ 实现/ 对/ 海量/ 大/ 数据/ 的/ 快速/ 高效/ 训练/ 是/ 深层/ 神经网络/ 走向/ 实用化/ 的/ 重要/ 标志/ ./ 现有/ 的/ Hadoop/ 平台/ 不/ 适合/ 迭代/ 运算/ ,/ SGD/ 又/ 不能/ 依/ 并行/ 方式/ 工作/ ,/ 而/ GPU/ 在/ 训练/ DNN/ 时/ 仍然/ 显得/ 比较/ 吃力/ ./ 同时/ ,/ 平台/ 的/ 能耗/ 问题/ 也/ 成为/ 制约/ 其/ 进一步/ 发展/ 的/ 主要/ 因素/ ./ 为/ 迎接/ 未来/ 深度/ 学习/ 在/ 产业化/ 过程/ 中/ 的/ 挑战/ ,/ 高性能/ 并行/ 加速/ 计算/ 平台/ 的/ 开发/ 成为/ 当务之急/ ./ 另一方面/ ,/ 生物/ 神经元/ 之间/ 的/ 连接/ 带有/ 随机/ 和/ 动态性/ ,/ 而/ 不是/ 如/ 人工神经网络/ 那样/ 确定/ 和/ 一成不变/ ,/ 如何/ 用/ 计算机硬件/ 或者/ 算法/ 来/ 模拟/ 这一/ 过程/ 虽/ 极具/ 挑战/ 但/ 意义/ 重大/ ./ (/ 5/ )/ 深度/ 神经网络/ 深层/ 神经网络/ 一个/ 最/ 主要/ 的/ 特点/ 在于/ 其/ 具有/ 大量/ 可调/ 的/ 自由/ 参数/ ,/ 这/ 使得/ 其/ 构建/ 起/ 的/ 模型/ 具有/ 较/ 高/ 的/ 灵活性/ ./ 但/ 另一方面/ 却/ 缺乏/ 有力/ 的/ 理论指导/ 和/ 支撑/ ,/ 大多数/ 情况/ 下/ 仍/ 过分/ 依赖于/ 经验/ ,/ 带有/ 一定/ 程度/ 的/ 随机性/ ./ 如此/ 复杂/ 的/ 模型/ 很/ 容易/ 在/ 特定/ 数据/ 集上/ 得到/ 近乎/ 理想/ 的/ 拟合/ 效果/ ,/ 然而/ 在/ 推广/ 泛化/ 性能/ 上/ 却/ 往往/ 很难/ 得到/ 保障/ ./ 为/ 防止/ 过/ 拟合/ 带来/ 的/ 问题/ ,/ 今后/ 应当/ 在/ 数据/ 的/ 规模/ 、/ 网络/ 的/ 结构/ 以及/ 模型/ 的/ 正则/ 化等/ 方面/ 开展/ 工作/ ,/ 使得/ 深度/ 神经网络/ 更好/ 地/ 发挥/ 其/ 功能/ ./ (/ 6/ )/ 大/ 数据/ 深度/ 学习/ 深度/ 学习/ 的/ 兴起/ 很大/ 程度/ 上/ 归功于/ 海量/ 可用/ 的/ 数据/ ./ 当前/ ,/ 实验/ 神经科学/ 与/ 各个/ 工程/ 应用领域/ 给/ 我们/ 带来/ 了/ 呈/ 指数/ 增长/ 的/ 海量/ 复杂/ 数据/ ,/ 通过/ 各种/ 不同/ 的/ 形态/ 被/ 呈现/ 出来/ (/ 如/ 文本/ 、/ 图像/ 、/ 音频/ 、/ 视频/ 、/ 基因/ 数据/ 、/ 复杂/ 网络/ 等/ )/ ,/ 且/ 具有/ 不同/ 的/ 分布/ ,/ 使得/ 神经网络/ 所/ 面临/ 的/ 数据/ 特性/ 发生/ 了/ 本质/ 变化/ ./ 这/ 给/ 统计/ 学习/ 意义/ 下/ 的/ 神经网络/ 模型/ 的/ 结构设计/ 、/ 参数/ 选取/ 、/ 训练/ 算法/ ,/ 以及/ 时效性/ 等/ 方面/ 都/ 提出/ 了/ 新/ 的/ 挑战/ ./ 因此/ ,/ 如何/ 针对/ 大/ 数据/ 设计/ 有效/ 的/ 深度/ 神经网络/ 模型/ 与/ 学习/ 理论/ ,/ 从/ 指数/ 增长/ 的/ 数据/ 中/ 获得/ 指数/ 增长/ 的/ 知识/ ,/ 是/ 深度/ 学习/ 深化/ 研究/ 中/ 必须/ 面临/ 的/ 挑战/ ./ 

