Page1/ 基于/ 隐式/ 用户/ 反馈/ 数据流/ 的/ 实时/ 个性化/ 推荐/ 1/ )/ (/ 中山大学/ 信息科学/ 与/ 技术/ 学院/ 广州/ 510006/ )/ 2/ )/ (/ 广东/ 东软/ 学院/ 国际合作部/ 广东/ 佛山/ 528225/ )/ 摘要/ 大多数/ 的/ 传统/ 推荐/ 系统/ 是/ 基于/ 用户/ 评分/ 构建/ ,/ 并/ 采用/ 离线/ 批量/ 的/ 训练/ 模式/ ./ 该文/ 研究/ 以下/ 两个/ 问题/ :/ (/ 1/ )/ 基于/ 隐式/ 用户/ 反馈/ 构建/ 推荐/ 系统/ ./ 与/ 显式/ 评分/ 相比/ ,/ 隐式/ 反馈/ 存在/ 范围/ 更广且/ 更/ 易于/ 收集/ ;/ (/ 2/ )/ 基于/ 反馈/ 数据流/ 进行/ 实时/ 推荐/ ,/ 以此/ 来/ 保障/ 更强/ 的/ 推荐/ 时效性/ ./ 为了/ 克服/ 由/ 隐式/ 反馈/ 本质特征/ 导致/ 的/ 不/ 平衡/ 类标/ 问题/ ,/ 直接/ 对/ 可/ 观察/ 的/ 用户/ 选择/ 行为/ 进行/ 概率/ 建模/ ,/ 在/ 训练/ 时/ 无需/ 引入/ 负/ 样本/ ./ 为了/ 提高/ 训练/ 效率/ 并/ 及时/ 抓住/ 用户/ 兴趣/ 的/ 变化/ ,/ 该文/ 提出/ 的/ 在线/ 学习/ 算法/ 在/ 强化/ 学习/ 用户/ 新/ 倾向/ 的/ 同时/ 弱化/ 了/ 学习/ 用户/ 惯常/ 行为/ 与/ 噪声/ ,/ 通过/ 比较/ 反馈/ 发生/ 概率/ 与/ 用户/ 置信度/ 来/ 为/ 每/ 一个/ 反馈/ 动态/ 调节/ 学习/ 步长/ ./ 最后/ ,/ 该文/ 设计/ 了/ 在线/ 评价/ 机制/ ,/ 并/ 在/ 两个/ 真实/ 数据/ 集上/ 进行/ 了/ 丰富/ 的/ 实验/ ./ 实验/ 结果/ 验证/ 了/ 所/ 提/ 方法/ 的/ 有效性/ ,/ 并/ 展示/ 了/ 其/ 在/ 推荐/ 精度/ 、/ 推荐/ 多样性/ 、/ 可/ 解释性/ 、/ 训练/ 效率/ 、/ 健壮性/ 以及/ 冷启动/ 适应能力/ 等/ 多个/ 方面/ 的/ 优势/ ./ 关键词/ 隐式/ 反馈/ ;/ 在线/ 学习/ ;/ 推荐/ 系统/ ;/ 大/ 数据/ 1/ 引言/ 推荐/ 系统/ 已经/ 成为/ 解决/ 信息/ 过载/ 问题/ 的/ 重要/ 工具/ ./ 它/ 通过/ 对/ 用户/ 兴趣/ 倾向/ 进行/ 建模/ ,/ 向/ 用户/ 主动/ 地/ 推送/ 其/ 感兴趣/ 的/ 信息/ ,/ 进而/ 提供/ 个性化/ 服务/ ./ 近年来/ ,/ 推荐/ 系统/ 的/ 研究/ 已经/ 取得/ 了/ 一系列/ 进展/ [/ 1/ ]/ ./ 大/ 数据/ 时代/ 对/ 推荐/ 算法/ 与/ 推荐/ 模型/ 提出/ 了/ 新/ 的/ 需求/ 与/ 挑战/ ./ 1.1/ 大规模/ 隐式/ 反馈/ 数据/ 利用/ 大多数/ 传统/ 的/ 推荐/ 系统/ [/ 2/ -/ 3/ ]/ 是/ 基于/ 用户/ 评分/ 来/ 构建/ 的/ ./ 用户/ 评分/ 虽然/ 能/ 显式/ 地/ 反应/ 出/ 用户/ 的/ 喜恶/ 倾向/ ,/ 但/ 并/ 不/ 总是/ 可得/ 的/ ./ 与/ 之/ 相对/ 的/ ,/ 隐式/ 用户/ 反馈/ (/ ImplicitUserFeedback/ )/ 普遍存在/ 于/ 互联网/ 生活/ 中/ ./ 用户/ 在/ 观看/ 电影/ 、/ 收听/ 音乐/ 或/ 浏览/ 朋友圈/ 时/ 的/ 多种形式/ 的/ 选择/ 行为/ 都/ 可以/ 看成/ 是/ 隐式/ 反馈/ ./ 由于/ 它/ 是/ 用户/ 在/ 使用/ 系统/ 服务/ 的/ 过程/ 中/ 不/ 自觉/ 留下/ 的/ ,/ 所以/ 它/ 的/ 收集/ 成本/ 很/ 低且/ 不/ 影响/ 用户/ 体验/ ./ 也/ 正/ 因为/ 隐式/ 反馈/ 具有/ 范围广/ 且/ 易于/ 收集/ 的/ 优良/ 特性/ ,/ 其/ 数据/ 规模/ 往往/ 较大/ ./ 这种/ 反馈/ 虽然/ 能/ 隐含地/ 表达/ 出/ 用户/ 正/ 的/ 倾向/ (/ 喜欢/ )/ ,/ 但/ 不能/ 表达/ 用户/ 负/ 的/ 倾向/ (/ 不/ 喜欢/ )/ ./ 这种/ 只有/ 正反馈/ 而/ 没有/ 负反馈/ 的/ 设置/ 成为/ 了/ 传统/ 推荐/ 模型/ 的/ 巨大/ 障碍/ [/ 4/ ]/ ./ 因此/ ,/ 基于/ 隐式/ 反馈/ 构建/ 推荐/ 系统/ 的/ 研究/ 具有/ 理论意义/ 和/ 极强/ 的/ 现实/ 应用/ 价值/ [/ 5/ ]/ ./ 1.2/ 在线/ 学习/ 与/ 实时/ 推荐/ 推荐/ 的/ 时效性/ 会/ 极大/ 地/ 影响/ 推荐/ 质量/ 与/ 用户/ 满意度/ [/ 6/ ]/ ./ 通过/ 整合/ 时间/ 信息/ ,/ 动态/ 推荐/ 模型/ [/ 7/ -/ 8/ ]/ 能够/ 提高/ 推荐/ 准确率/ ./ 但是/ ,/ 这些/ 模型/ 大多/ 采用/ 离线/ 批量/ 的/ 训练/ 模式/ ,/ 因此/ 存在/ 以下/ 局限性/ ./ (/ 1/ )/ 训练/ 效率/ 低下/ 与/ 高/ 时效性/ 需求/ 的/ 冲突/ 批量/ 训练/ 是/ 指/ 通过/ 收集/ 一批/ 训练样本/ 进行/ 一次性/ 学习/ 得出/ 模型/ 参数/ 的/ 训练/ 方式/ ./ 为了/ 保证/ 训练/ 出/ 的/ 模型/ 的/ 可用性/ ,/ 我们/ 需要/ 隔/ 一段时间/ 后/ 再/ 重新/ 训练/ ./ 这种/ 方式/ 的/ 训练/ 效率/ 较/ 低/ ./ 特别/ 是/ 当/ 数据/ 规模/ 较大/ 时/ ,/ 一方面/ ,/ 训练/ 时间/ 不/ 可能/ 被/ 无限/ 压缩/ ;/ 另一方面/ ,/ 频繁/ 的/ 重新/ 训练/ 将/ 导致/ 计算成本/ 过高/ ./ 因此/ ,/ 离线/ 批量/ 的/ 训练/ 模式/ 难以/ 满足/ 高/ 时效性/ 的/ 需求/ ./ (/ 2/ )/ 信息/ 利用/ 不/ 及时/ 进而/ 影响/ 推荐/ 质量/ 离线/ 训练/ 也/ 就/ 意味着/ 无法/ 及时/ 利用/ 最近/ 的/ 用户/ 反馈/ 信息/ ,/ 因为/ 在/ 训练/ 时/ 来不及/ 收集/ 这些/ 信息/ ./ 只有/ 等到/ 下/ 一次/ 重新/ 训练/ 时/ ,/ 这些/ 信息/ 才能/ 发挥作用/ ,/ 但/ 有/ 可能/ 到/ 那时/ ,/ 这些/ 信息/ 已经/ “/ 过时/ ”/ 了/ ./ 然而/ ,/ 最近/ 的/ 用户/ 反馈/ 极具/ 价值/ ,/ 因为/ 它们/ 反映/ 了/ 用户/ 的/ 最新/ 兴趣/ 或/ 当前/ 需求/ ./ 这种/ 信息/ 损失/ 势必/ 影响/ 推荐/ 质量/ ./ 此外/ ,/ 离线/ 训练/ 还是/ 导致/ 冷启动/ 问题/ 的/ 根本原因/ 之一/ ./ 之所以/ 难以/ 对/ “/ 新/ 用户/ ”/ 进行/ 推荐/ 是因为/ 在/ 训练/ 过程/ 中/ 推荐/ 系统/ 没有/ 学习/ 过该/ 用户/ 的/ 反馈/ ./ 而/ 事实上/ ,/ 该/ 新/ 用户/ 很/ 可能/ 在/ 最近/ 的/ 几分钟/ 内/ 已经/ 提供/ 了/ 反馈/ 信息/ ,/ 只是/ 离线/ 系统/ 还/ 未/ 来得及/ 加以/ 利用/ ./ 在线/ 学习/ 与/ 实时/ 推荐/ 的/ 框架/ 可以/ 克服/ 以上/ 两点/ 局限/ ./ 如图/ 1/ 所示/ ,/ 用户/ 反馈/ 数据/ 以/ 数据流/ 的/ 形式/ 持续/ 进入/ 系统/ ,/ 对于/ 每/ 一个/ 用户/ 反馈/ ,/ 推荐/ 系统/ 都/ 能够/ 及时/ 接收/ 并/ 在线/ 更新/ 模型/ 参数/ ,/ 提供/ 实时/ 的/ 推荐/ 服务/ ./ 该/ 框架/ 避免/ 了/ 重新/ 训练/ ,/ 因而/ 显著/ 地/ 提高/ 了/ 训练/ 效率/ ,/ 从根本上/ 解决/ 了/ 训练/ 效率/ 低下/ 与/ 高/ 时效性/ 需求/ 间/ 的/ 矛盾/ [/ 9/ ]/ ./ 该/ 框架/ 能够/ 在/ 最/ 短时间/ 内/ 捕获/ 用户/ 兴趣/ 的/ 新/ 变化/ ,/ 同时/ 压缩/ 了/ 冷启动/ 空间/ ,/ 因此/ 能/ 提高/ 推荐/ 质量/ ./ 但是/ ,/ 在线/ 流式/ 计算/ 系统对/ 噪声/ 较为/ 敏感/ ,/ 因为/ 采用/ 流式/ 的/ 数据处理/ 方式/ 则/ 难以/ 进行/ 数据/ 清洗/ 和/ 噪声/ 过滤/ ,/ 与此同时/ ,/ 在线/ 环境/ 更/ 容易/ 受到/ 恶意/ 攻击/ (/ 比如/ 由/ 爬虫/ 发起/ 的/ 大量/ 随机/ 访问/ )/ ./ 这/ 就/ 对/ 在线/ 推荐/ 模型/ 的/ 健壮性/ ,/ 特别/ 是/ 抗/ 攻击能力/ 提出/ 了/ 更/ 高/ 的/ 要求/ [/ 10/ ]/ ./ 在/ 此/ 背景/ 下/ ,/ 本文/ 研究/ 了/ 如何/ 基于/ 隐式/ 用户/ 反馈/ 数据流/ 来/ 构建/ 在线/ 的/ 推荐/ 模型/ ,/ 提供/ 实时/ 的/ 个性化/ 推荐/ 服务/ ./ 本文/ 工作/ 致力于/ 提高/ 推荐/ 系统/ 在/ 适用范围/ 、/ 训练/ 效率/ 以及/ 推荐/ 质量/ 等/ 多方面/ 的/ 能力/ ,/ 主要/ 贡献/ 包括/ :/ (/ 1/ )/ 提出/ 了/ 隐式/ 反馈/ 推荐/ 模型/ ,/ 通过/ 在/ 概率/ 生成/ 模型/ 的/ 基本/ 框架/ 下/ 最大化/ 可/ 观察/ 用户/ 的/ 选择/ 行为/ 发生/ 的/ 概率/ ,/ 将/ 推荐/ 问题/ 转化/ 为/ 优化/ 问题/ ,/ 进而/ 解决/ 了/ 隐式/ 反馈/ 数据/ 导致/ 的/ 不/ 平衡/ 类标/ 问题/ ;/ (/ 2/ )/ 通过/ 对/ 用户/ 敏感/ 程度/ 以及/ 从众/ 程度/ 进行/ 个性化/ 设置/ ,/ 给出/ 了/ 整合/ 隐式/ 用户/ 反馈/ 、/ 产品/ 特征/ 、/ 流行/ 度/ 等/ 多种/ 异构/ 信息/ 的/ 应用/ 范例/ ;/ (/ 3/ )/ 采用/ 在线/ 学习/ 方式/ ,/ 提出/ 了/ 以/ 较/ 低/ 的/ 训练/ 开销/ 处理/ 大规模/ 流式/ 反馈/ 数据/ 的/ 在线/ 推荐/ 模型/ ./ 该/ 模型/ 采用/ 动态/ 学习/ 步长/ ,/ 在/ 及时/ 抓住/ 用户/ 及/ 环境/ 动/ Page3/ 态/ 变化/ 的/ 同时/ ,/ 降低/ 了/ 噪声/ 的/ 影响/ ;/ (/ 4/ )/ 设计/ 了/ 在线/ 评价/ 机制/ 并/ 在/ 两个/ 真实/ 数据/ 集上/ 进行/ 了/ 丰富/ 的/ 实验/ ,/ 验证/ 了/ 本文/ 提出/ 的/ 模型/ 不仅/ 在/ 推荐/ 质量/ 与/ 训练/ 效率/ 上/ 存在/ 明显/ 优势/ ,/ 更/ 在/ 推荐/ 结果/ 多样性/ 、/ 可/ 解释性/ 、/ 健壮性/ 以及/ 冷启动/ 适应能力/ 等/ 方面/ 具备/ 竞争力/ ./ 2/ 相关/ 工作/ 2.1/ 基于/ 隐式/ 反馈/ 的/ 推荐/ 传统/ 推荐/ 模型/ 难以/ 直接/ 应用/ 于/ 隐式/ 反馈/ 推荐/ 场景/ ,/ 这/ 是因为/ 隐式/ 反馈/ 数据/ 本身/ 只/ 包含/ 用户/ 正/ 的/ 倾向/ 而/ 缺乏/ 用户/ 负/ 的/ 倾向/ ./ Pan/ 等/ 人/ [/ 4/ ]/ 最早/ 将/ 这/ 一/ 问题/ 定义/ 为/ OneClassCollaborativeFiltering/ ./ 更/ 一般/ 地/ ,/ 可/ 将/ 其/ 概括/ 为/ 不/ 平衡/ 类标/ 问题/ (/ UnbalancedClassProblem/ )/ [/ 1/ ]/ ,/ 其/ 核心/ 在于/ ,/ 数据/ 集中/ 正负/ 样本/ 极度/ 不/ 平衡/ 导致/ 难以/ 训练/ ./ 目前/ ,/ 应/ 对此/ 问题/ 的/ 总体/ 思路/ 是/ 引入/ 负/ 样本/ ./ 总结/ 起来/ 有/ 3/ 种/ 方式/ :/ (/ 1/ )/ 基于/ 规则/ 指定/ 负/ 样本/ ./ 在/ 某些/ 特殊/ 的/ 场景/ 中/ ,/ 可以/ 大/ 概率/ 地/ 认为/ 某些/ 未知/ 标签/ 的/ 样本/ 是/ 负/ 的/ ./ 比如/ 认为/ 夹/ 在/ 两条/ 被/ 转发/ 的/ 微博/ 之间/ 的/ 其他/ 未/ 被/ 转发/ 的/ 微博为/ 负/ 的/ ,/ 因为/ 很/ 可能/ 用户/ 浏览/ 了/ 它们/ 但/ 并/ 不/ 喜欢/ [/ 11/ ]/ ./ 由于/ 依赖于/ 领域/ 知识/ ,/ 该/ 方法/ 的/ 推广性/ 不/ 强/ ;/ (/ 2/ )/ 从/ 未知/ 标签/ 样本/ 中/ 随机抽样/ 作为/ 负/ 样本/ ./ 这种/ 方法/ 一般/ 假设/ 未知/ 标签/ 样本/ 中/ 大多数/ 为/ 负/ 样本/ ,/ 所以/ 随机抽样/ 得到/ 的/ 样本/ 很/ 可能/ 是/ 负/ 的/ [/ 4/ ,/ 9/ ,/ 12/ ]/ ;/ (/ 3/ )/ 将/ 未知/ 标签/ 样本/ 都/ 作为/ 负/ 样本/ ,/ 但/ 设置/ 较/ 小/ 的/ 权重/ ./ 权重/ 反映/ 了/ 这些/ 样本/ 是/ 负/ 样本/ 的/ 置信/ 程度/ [/ 4/ ,/ 13/ ]/ ./ 其/ 矛盾/ 点/ 在于/ :/ 为了/ 训练/ 不得不/ 引入/ 负/ 样本/ ,/ 又/ 无法/ 保证/ 引入/ 的/ 负/ 样本/ 是/ “/ 真负/ ”/ ./ 本质/ 上/ 这些/ 方法/ 都/ 试图/ 去/ 平衡/ 这一/ 矛盾/ ./ 此外/ ,/ 引入/ 负/ 样本/ 会/ 增加/ 训练/ 负担/ ,/ 数据/ 规模/ 较大/ 时/ 将/ 影响/ 训练/ 效率/ ./ 本文/ 所/ 提出/ 的/ 模型/ 则/ 采用/ 概率/ 生成/ 模型/ 的/ 技术/ 思路/ ,/ 通过/ 最大化/ 已/ 观察/ 到/ 的/ 用户/ 反馈/ 发生/ 的/ 概率/ 来/ 直接/ 对/ 用户/ 选择/ 倾向/ 进行/ 建模/ ,/ 无需/ 负/ 样本/ 即可/ 进行/ 训练/ ,/ 因此/ 天然/ 地/ 适用/ 于/ 隐式/ 反馈/ 推荐/ 场景/ ./ 2.2/ 动态/ 推荐/ 与/ 实时/ 推荐/ 近年来/ ,/ 有着/ 完备/ 的/ 概率/ 解释/ ,/ 较/ 高/ 预测/ 准确率/ 和/ 良好/ 可扩展性/ 的/ 概率/ 矩阵/ 分解/ (/ ProbabilisticMatrixFactorization/ )/ [/ 2/ ]/ 已经/ 逐渐/ 取代/ 传统/ 的/ 基于/ 邻居/ 的/ 协同/ 过滤/ 算法/ [/ 3/ ]/ 并/ 成为/ 主流/ 模型/ ./ 在/ 此基础/ 上/ ,/ 通过/ 整合/ 时间/ 信息/ ,/ timeSVD/ ++/ [/ 7/ ]/ 与/ DMF/ [/ 8/ ]/ 等/ 动态/ 模型/ 进一步/ 地/ 提高/ 了/ 预测/ 准确率/ ./ 但是/ 这些/ 动态/ 模型/ 依然/ 遵循/ 离线/ 批量/ 训练/ 模式/ ./ 为了/ 降低/ 离线/ 训练/ 方式/ 高昂/ 的/ 重/ 计算/ 开销/ ,/ Abernethy/ 等/ 人/ [/ 14/ ]/ 提出/ 了/ 在/ 传统/ 矩阵/ 分解/ 模型/ 上/ 使用/ 在线/ 梯度/ 下降/ 算法/ (/ OnlineGradientDescent/ ,/ OGD/ )/ 来/ 实时/ 地/ 更新/ 模型/ 参数/ ./ Zhao/ 等/ 人/ [/ 15/ ]/ 从/ 交互/ 的/ 角度/ 出发/ 来/ 设计/ 推荐/ 策略/ ,/ 用/ 在线/ 学习/ 的/ 方法/ 使/ 推荐/ 系统/ 能够/ 在/ 尽可能/ 短/ 的/ 交互/ 过程/ 中/ 掌握/ 用户/ 兴趣/ ./ 这些/ 实时/ 推荐/ 模型/ 都/ 是/ 基于/ 用户/ 评分/ 的/ ./ Diaz/ -/ Aviles/ 等/ 人/ [/ 9/ ]/ 在/ 基于/ 隐式/ 反馈/ 数据流/ 构建/ 实时/ 推荐/ 模型/ 的/ 方向/ 上/ 进行/ 了/ 积极/ 的/ 探索/ ,/ 提出/ 的/ 模型/ RMFX/ 采用/ 抽样/ 负/ 样本/ 的/ 策略/ ,/ 对/ 用户/ 转发/ 微博/ 的/ 行为/ 进行/ 排序/ 学习/ ./ 这些/ 模型/ 都/ 采用/ 固定/ 学习/ 步长/ 的/ 方式/ 来/ 进行/ 在线/ 学习/ ,/ 将/ 所有/ 用户/ 反馈/ (/ 包括/ 噪声/ )/ 不/ 加/ 区别/ 地/ 对待/ ,/ 因此/ 限制/ 了/ 学习效果/ ./ 而/ 本文/ 提出/ 的/ 在线/ 学习/ 方法/ 根据/ 用户/ 反馈/ 的/ 价值/ 自动/ 调节/ 学习/ 步长/ ,/ 能/ 有效/ 地/ 提高/ 学习效果/ ./ 3/ 模型/ 本文/ 要/ 研究/ 的/ 问题/ 是/ 如何/ 基于/ 隐式/ 用户/ 反馈/ 数据流/ 构建/ 在线/ 推荐/ 模型/ ,/ 该/ 模型/ 能够/ 利用/ 用户/ 反馈/ 从而/ 在线/ 更新/ 模型/ 参数/ 并/ 产生/ 实时/ 推荐/ 结果/ ./ 本节/ 首先/ 介绍/ 了/ 基于/ 隐式/ 反馈/ 的/ 统一/ 推荐/ 模型/ IFRM/ ./ 然后/ 给出/ 了/ 通用/ 应用/ 范例/ 展示/ 如何/ 灵活/ 地/ 整合/ 用户/ 反馈/ 、/ 产品描述/ 以及/ 流行/ 度/ 等/ 多种/ 异构/ 信息/ ./ 最后/ 介绍/ 了/ IFRM/ 的/ 在线/ 学习/ 算法/ ./ 3.1/ 隐式/ 反馈/ 推荐/ 模型/ (/ ImplicitFeedbackRecommendationModel/ ,/ IFRM/ )/ 隐式/ 反馈/ 数据/ 的/ 特点/ 是/ 只有/ 正/ 样本/ 而/ 缺乏/ 负/ 样本/ ,/ 为了/ 避免/ 在/ 训练/ 时/ 需要/ 引入/ 负/ 样本/ ,/ 我们/ 采用/ 概率/ 生成/ 模型/ 直接/ 对/ 用户/ 选择/ 行为/ 进行/ 建模/ ./ 其/ 核心/ 点/ 在于/ 对/ 观察/ 到/ 的/ 用户/ 选择/ 行为/ 发生/ 的/ 原因/ 进行/ 合理/ 假设/ ./ 我们/ 假设/ 用户/ 选择/ 行为/ 受/ 用户/ 对/ 产品/ 的/ “/ 选择/ 倾向/ 度/ ”/ 决定/ ,/ 而/ 这种/ 倾向性/ 程度/ 是/ 相对而言/ 的/ ./ 比如/ ,/ 用户/ 之所以/ 会/ 选择/ 某/ 产品/ 是因为/ 用户/ 对/ 该/ 产品/ 具有/ 比/ 一般/ 产品/ 更/ 高/ 的/ 选择/ 倾向/ ./ 基于/ 此/ ,/ 首先/ 给出/ 如下/ 形式化/ 的/ 定义/ ./ 定义/ 1/ ./ 用户/ i/ 选择/ 产品/ j/ 发生/ 的/ 概率/ Prij/ 由/ Δ/ ij/ 决定/ ./ Δ/ ij/ 描述/ 了/ 用户/ i/ 对/ 产品/ j/ 的/ 相对/ 选择/ 倾向/ 程度/ ./ Δ/ ij/ 与/ 选择/ 倾向/ 度/ Aij/ 以及/ 平均/ 选择/ 倾向/ 度/ Ai/ 相关/ :/ Page4/ 其中/ ,/ M/ 是/ 总/ 产品/ 数/ ,/ φ/ (/ x/ )/ =/ 其/ 作用/ 是/ 将/ Δ/ ij/ 归一到/ (/ 0/ ,/ 1/ )/ 区间/ ./ 值得注意/ 的/ 是/ ,/ 这里/ 也/ 可/ 采用/ 标准/ sigmoid/ 函数/ φ/ (/ x/ )/ =/ ex/ // 1/ +/ ex/ ,/ 但/ 本文/ 采用/ 的/ 形式/ 利于/ 在线/ 学习/ (/ 详见/ 3.3/ 节/ )/ ,/ 也/ 能/ 增强/ 推荐/ 结果/ 的/ 可/ 解释性/ (/ 详见/ 用户/ 置信度/ θ/ 的/ 计算/ 及/ 相关/ 实验/ )/ ./ 可以/ 将/ Aij/ 看成/ 是/ 与/ 用户/ i/ 和/ 产品/ j/ 相关/ 的/ 函数/ ,/ 其/ 形式/ 可/ 根据/ 具体/ 应用/ 场景/ 中/ 可/ 利用/ 信息/ 的/ 不同/ 而/ 灵活/ 设计/ (/ 见/ 3.2/ 节/ )/ ,/ 在/ 此/ 暂/ 不/ 详细/ 展开讨论/ ./ 将/ 观察/ 到/ 的/ 用户/ 选择/ 行为/ 的/ 集合/ 记为/ O/ =/ {/ 〈/ i/ ,/ j/ 〉/ |/ 用户/ i/ 选择/ 了/ 产品/ j/ }/ ,/ 假设/ 选择/ 行为/ 是/ 相互/ 独立/ 的/ ,/ 则/ 似然/ 概率/ 为/ P/ (/ O/ Θ/ )/ =/ ∏/ 〈/ i/ ,/ j/ 〉/ ∈/ O/ 其中/ ,/ Θ/ 泛指/ 模型/ 参数/ ,/ 与/ Aij/ 的/ 具体/ 设计/ 相关/ ./ 应用/ 贝叶斯/ 公式/ 并/ 假设/ Θ/ 服从/ 均值/ 为/ 0/ ,/ 方差/ 为/ σ/ 2/ 的/ 高斯/ 先验/ 分布/ ,/ 可得/ 后验/ 概率/ :/ P/ (/ Θ/ O/ )/ ∝/ P/ (/ O/ Θ/ )/ P/ (/ Θ/ )/ =/ ∏/ 〈/ i/ ,/ j/ 〉/ ∈/ O/ 训练/ 的/ 目标/ 即/ 为/ 最大化/ 后验/ 概率/ ,/ 对式/ (/ 4/ )/ 取/ 对数/ 并/ 取反/ 后/ ,/ 得到/ 如下/ 等价/ 的/ 优化/ 目标/ :/ argmin/ Θ/ L/ ·/ 其中/ ·/ 2/ 控制参数/ 复杂度/ ./ 观察/ 优化/ 目标/ 可知/ IFRM/ 具有/ 如下/ 3/ 点/ 优势/ :/ (/ 1/ )/ 仅/ 依赖于/ 用户/ 选择/ 行为/ 集/ O/ ,/ 训练/ 时/ 无需/ 负/ 样本/ ,/ 因此/ 天然/ 适用/ 于/ 隐式/ 反馈/ 推荐/ 场景/ ;/ (/ 2/ )/ 提供/ 了/ 概率/ 框架/ 但/ 未/ 具体/ 定义/ 模型/ 参数/ Θ/ ,/ 因此/ 可/ 推广性/ 强/ ,/ 3.2/ 节/ 给出/ 了/ 推广应用/ 范例/ ;/ (/ 3/ )/ 目标/ 为/ 连加/ 形式/ ,/ 易于/ 在线/ 学习/ ,/ 详见/ 3.3/ 节/ ./ 转化/ 为/ 最优化/ 问题/ 后/ ,/ 可以/ 使用/ 随机/ 梯度/ 下降/ 法/ 训练/ 得到/ 模型/ 参数/ Θ/ ./ 具体来说/ ,/ 首先/ 随机/ 初始化/ 全部/ 的/ 模型/ 参数/ ,/ 重复/ 地/ 从/ 训练/ 集/ O/ 中/ 抽取/ 一个/ 样本/ ,/ 沿着/ 导数/ 负/ 方向/ 更新/ 相关/ 参数/ ,/ 直至/ 算法/ 收敛/ 或/ 达到/ 迭代/ 次数/ 上限/ ./ 在/ 推荐/ 时/ ,/ 由/ Θ/ 可以/ 计算/ 出/ 相应/ 的/ 选择/ 倾向/ 度/ Aij/ ,/ 向/ 用户/ i/ 推荐/ 具有/ 较/ 高/ Aij/ 的/ 产品/ ./ 3.2/ IFRM/ 应用/ 范例/ 本/ 小节/ 将/ 介绍/ 一种/ 可/ 适用/ 于/ 大多数/ 隐式/ 反馈/ 推荐/ 场景/ 的/ IFRM/ 应用/ 范例/ ,/ 展示/ 如何/ 通过/ 合理/ 设计/ 选择/ 倾向/ 度/ Aij/ 来/ 整合/ 多种/ 异构/ 信息/ ./ 首先/ ,/ 用户/ 的/ 选择/ 倾向/ 受/ 喜爱/ 程度/ P/ 影响/ ,/ 受/ 潜在/ 要素/ 模型/ [/ 2/ ]/ 的/ 启发/ ,/ 可/ 在/ K/ 维/ 潜在/ 特征/ 空间/ 对/ 用户/ 和/ 产品/ 进行/ 重新/ 表达/ ,/ 令/ Ui/ =/ (/ Ui1/ ,/ Ui2/ ,/ …/ ,/ UiK/ )/ ,/ Vj/ =/ (/ Vj1/ ,/ Vj2/ ,/ …/ ,/ VjK/ )/ ./ 假设/ 用户/ i/ 对/ 产品/ j/ 的/ 喜爱/ 程度/ Pij/ 由/ 潜在/ 特征/ 共同/ 决定/ :/ Pij/ =/ Ui/ ·/ Vj/ =/ KUikVjk/ ./ ∑/ k/ =/ 1/ 其次/ ,/ 用户/ 选择/ 行为/ 与/ 上下文/ 条件/ I/ 相关/ [/ 11/ ]/ ./ 比如/ ,/ 用户/ 喜欢/ 某/ 演员/ ,/ 因此/ 倾向/ 于/ 观看/ 由该/ 演员/ 出演/ 的/ 电影/ ./ 将/ 产品/ 的/ 描述性/ 特征/ 抽取/ 出来/ ,/ 可以/ 得到/ 产品/ 的/ 特征向量/ 犉/ j/ =/ (/ Fj1/ ,/ Fj2/ ,/ …/ ,/ FiQ/ )/ ./ 举例来说/ ,/ 某/ 动作/ 科幻电影/ 的/ 特征向量/ 在/ “/ 类别/ =/ 动作/ ”/ 以及/ “/ 类别/ =/ 科幻/ ”/ 这/ 两个/ 特征/ 分量/ 上/ 的/ 值/ 为/ 1/ ,/ 在/ “/ 类别/ =/ 喜剧/ ”/ 等/ 其他/ 关于/ 类别/ 的/ 特征/ 分量/ 上/ 的/ 值/ 为/ 0/ ./ 为了/ 平衡/ 各/ 特征/ 的/ 影响/ ,/ 建议/ 对/ 特征值/ 进行/ 归一化/ 处理/ ./ 由于/ 不同/ 的/ 用户/ 对于/ 不同/ 的/ 上下文/ 特征/ 的/ 敏感/ 程度/ 是/ 不同/ 的/ ,/ 本文/ 引入/ 个性化/ 的/ 敏感/ 程度/ 概念/ ,/ 定义/ 用户/ i/ 的/ 敏感/ 程度/ 向量/ 为/ 犛/ i/ =/ (/ Si1/ ,/ Si2/ ,/ …/ ,/ SiQ/ )/ ./ 用户/ 敏感度/ 与/ 产品/ 特征/ 共同/ 起/ 作用/ :/ Iij/ =/ Si/ ·/ Fj/ =/ QSiqFjq/ ./ ∑/ q/ =/ 1/ 最后/ ,/ 社会/ 环境/ E/ 也/ 会/ 影响/ 用户/ 选择/ 行为/ [/ 11/ ]/ ./ 比如/ ,/ 用户/ 倾向/ 于/ 观看/ 近期/ 热播/ 的/ 电影/ 或/ 收听/ 时/ 下/ 流行/ 的/ 歌曲/ ./ 利用/ 社交/ 信息/ 可以/ 对/ 社会/ 环境/ 对/ 用户/ 产生/ 的/ 影响/ 进行/ 量化/ 估计/ ,/ 但/ 许多/ 应用/ 场景/ 中/ 没有/ 社交/ 环境/ ./ 因此/ 本文/ 采用/ 时/ 下/ 流行/ 度/ (/ TemporalPopularity/ ,/ TP/ )/ 对/ 社交/ 影响/ 进行/ 简单/ 的/ 近似/ ./ 受/ 用户数量/ 规模/ 的/ 限制/ ,/ 这个/ 估计/ 是/ 有/ 偏/ 的/ ,/ 但/ 实验/ 表明/ 是/ 有效/ 的/ ./ 记/ TPj/ 为/ 产品/ j/ 的/ 时/ 下/ 流行/ 度/ ,/ 其值/ 为/ 最近/ 时间/ 窗中/ 被/ 选择/ 的/ 次数/ ./ 社会/ 环境/ 对于/ 较为/ 从众/ 的/ 用户/ 影响/ 较大/ ,/ 而/ 对于/ 不/ 从众/ 的/ 用户/ 影响/ 较/ 小/ ./ 为此/ ,/ 本文/ 引入/ 了/ 个性化/ 的/ 用户/ 从众/ 度/ ci/ ,/ 并令/ Eij/ =/ ci/ ·/ TPj/ ./ 线性/ 模型/ 具有/ 泛化/ 能力/ 强且/ 易于/ 求解/ 的/ 优点/ ,/ 本文/ 采用/ 线性/ 模型/ 来/ 整合/ P/ 、/ I/ 以及/ E/ 这/ 3/ 个/ 影响/ 因素/ :/ 在/ 式/ (/ 6/ )/ 中/ ,/ U/ ,/ V/ ,/ S/ ,/ c/ 是/ 需要/ 估计/ 的/ 模型/ 参数/ ;/ φ/ 1/ 为/ 权重/ ,/ 用来/ 权衡/ 各/ 影响/ 因素/ 发挥/ 的/ 作用/ ,/ 可/ φ/ 2/ 根据/ 领域/ 先验/ 知识/ 进行/ 设置/ ,/ 本文/ 实验/ 在/ 两个/ 数据/ ,/ φ/ 3/ 集上/ 对/ 参数/ 调节/ 产生/ 的/ 影响/ 进行/ 了/ 细致/ 的/ 分析/ 和/ 讨论/ ./ 在/ 实际/ 应用/ 中/ ,/ 也/ 可以/ 增加/ 一个/ burn/ -/ in/ 离线/ 训/ Page5/ 练/ 过程/ ,/ 使用/ EM/ 算法/ 自动/ 找到/ 适用/ 于/ 具体/ 应用/ 场景/ 的/ 较优/ 取值/ ,/ 避免/ 手动/ 调节/ ./ 虽然/ 该/ 应用/ 范例/ 具有/ 通用性/ ,/ 但/ 仍/ 建议/ 读者/ 根据/ 应用/ 场景/ 的/ 特性/ 进行/ 更/ 合理/ 的/ 假设/ ./ 此外/ ,/ 建议/ 各/ 要素/ 初始化/ 值/ 的/ 规模/ 应/ 相当/ ,/ 利于/ 模型/ 自行/ 调优/ ./ 将式/ (/ 6/ )/ 代入/ IFRM/ 框架/ ,/ 得到/ 如下/ 模型/ :/ argminU/ ,/ V/ ,/ S/ ,/ cL/ ·/ 其中/ Aij/ 由式/ (/ 6/ )/ 定义/ ,/ λ/ 1/ ,/ λ/ 2/ 是/ 正则/ 化/ 系数/ ,/ 使用/ 随机/ 梯度/ 下降/ 法在/ 导数/ 负/ 方向/ 上/ 分别/ 对模型/ 参数/ U/ ,/ V/ ,/ S/ ,/ c/ 进行/ 迭代/ 更新/ ,/ 即可/ 求得/ 局部/ 最优/ 解/ ./ 3.3/ 在线/ 隐式/ 反馈/ 推荐/ 模型/ (/ onlineImplicitFeed/ -/ backRecommendationModel/ ,/ oIFRM/ )/ IFRM/ 仍/ 采用/ 离线/ 批量/ 训练/ 模式/ ,/ 这种/ 训练/ 模式/ 在/ 训练/ 效率/ 与/ 推荐/ 质量/ 上/ 存在/ 局限性/ ,/ 本/ 小节/ 将/ 介绍/ 如何/ 将/ IFRM/ 扩展/ 为/ 在线/ 模型/ ./ 由式/ (/ 5/ )/ 和/ 式/ (/ 7/ )/ 可以/ 看出/ ,/ IFRM/ 的/ 优化/ 目标/ 由/ 损失/ 项/ 与/ 正则/ 项/ 构成/ ,/ 其中/ 损失/ 项以/ 连加/ 的/ 形式/ 整合/ 了/ 所有/ 用户/ 反馈/ ,/ 这为/ 实现/ 在线/ 学习/ 提供/ 了/ 便利/ ./ 基本/ 方案/ 是/ 采用/ 在线/ 梯度/ 下降/ 法来/ 对/ 每/ 一个/ 用户/ 反馈/ 在线/ 地/ 更新/ 模型/ 参数/ [/ 14/ ]/ ./ 使用/ 这种/ 学习/ 方式/ ,/ 用户/ 反馈/ 逐一/ 流入/ 推荐/ 系统/ 并/ 仅/ 被/ 学习/ 一次/ ./ 由于/ 模型/ 不能/ 进行/ 重新学习/ ,/ 把握/ 好/ 这/ 唯一/ 的/ 学习/ 机会/ 就/ 显得/ 特别/ 重要/ ./ 因此/ ,/ 如何/ 确定/ 学习/ 步长/ 成为/ 了/ 在线/ 更新/ 的/ 关键所在/ ./ 过大/ 的/ 学习/ 步长/ 意味着/ 较强/ 的/ 训练/ 程度/ ,/ 存在/ 过/ 训练/ (/ OverTraining/ )/ 的/ 风险/ ;/ 而/ 过/ 小/ 的/ 学习/ 步长/ 意味着/ 较/ 低/ 的/ 训练/ 程度/ ,/ 可能/ 导致/ 欠/ 训练/ (/ UnderTraining/ )/ ./ 已有/ 的/ 方法/ [/ 9/ ,/ 14/ ]/ 依赖/ 人工/ 设置/ 统一/ 的/ 学习/ 率/ 和/ 迭代/ 次数/ ./ 这样/ 做/ 有/ 3/ 点缺陷/ :/ (/ 1/ )/ 实际/ 使用/ 过程/ 中/ 需要/ 进行/ 大量/ 的/ 调参/ 实验/ ;/ (/ 2/ )/ 不同/ 的/ 用户/ 反馈/ 对系统/ 的/ 价值/ 是/ 不同/ 的/ ,/ 因此/ 全局/ 统一/ 的/ 学习/ 步长/ 势必会/ 影响/ 学习效果/ ;/ (/ 3/ )/ 对/ 所有/ 用户/ 反馈/ (/ 包括/ 噪声/ )/ 一视同仁/ ,/ 将/ 大大降低/ 模型/ 的/ 鲁棒性/ ,/ 而/ 抗噪/ 和/ 抗/ 攻击能力/ 对于/ 在线/ 系统/ 来说/ 是/ 极其重要/ 的/ ./ 为了/ 克服/ 这些/ 缺陷/ ,/ 我们/ 应该/ 根据/ 每/ 一个/ 反馈/ 对系统/ 的/ 价值/ 来/ 动态/ 调节/ 学习/ 步长/ ./ 从/ 价值/ 角度/ 可以/ 将/ 用户/ 反馈/ 归为/ 3/ 类/ :/ (/ 1/ )/ 反映/ 用户/ 新/ 兴趣/ 的/ 反馈/ ./ 这类/ 反馈/ 对系统/ 而言/ 具有/ 很/ 高/ 的/ 价值/ ,/ 因此/ 需要/ 通过/ 加强/ 学习/ 来/ 迅速/ 抓住/ 用户/ 兴趣/ 的/ 变化/ ;/ (/ 2/ )/ 符合/ 用户/ 习惯/ 行为/ 的/ 反馈/ ./ 因为/ 推荐/ 系统/ 已经/ 较/ 好/ 地/ 掌握/ 了/ 此类/ 的/ 用户/ 行为/ ,/ 所以/ 这/ 类/ 反馈/ 对系统/ 的/ 价值/ 不高/ ,/ 应该/ 弱化/ 学习/ 以/ 提高/ 学习/ 效率/ (/ 减少/ 训练/ 迭代/ 次数/ )/ ;/ (/ 3/ )/ 噪声/ ./ 这类/ 反馈/ 对系统/ 的/ 影响/ 是/ 负面/ 的/ ,/ 所以/ 应该/ 忽略/ 以/ 提高/ 模型/ 的/ 鲁棒性/ ./ 于是/ ,/ 问题/ 的/ 关键在于/ 如何/ 判别/ 出/ 不同/ 的/ 反馈/ 类型/ ./ 反馈/ 发生/ 的/ 概率/ 与/ 用户/ 置信度/ 这/ 两个/ 概念/ 与/ 反馈/ 类型/ 息息相关/ ./ 能够/ 反映/ 用户/ 新/ 兴趣/ 的/ 反馈/ 具有/ 较/ 小/ 的/ 发生/ 概率/ (/ 用户/ 不同/ 以往/ 的/ 行为/ )/ 以及/ 较大/ 的/ 置信度/ (/ 一直/ 以来/ 用户/ 的/ 此类/ 行为/ 都/ 较为/ 可靠/ )/ ,/ 在/ 训练/ 时应/ 增加/ 迭代/ 次数/ 以/ 达到/ 强化/ 学习/ 的/ 目的/ ./ 符合/ 用户/ 惯常/ 行为/ 的/ 反馈/ 具有/ 较大/ 的/ 发生/ 概率/ ,/ 噪声/ 特别/ 是/ 由/ 恶意/ 用户/ 发起/ 的/ 攻击/ 则/ 具有/ 较/ 小/ 的/ 置信度/ ./ 对于/ 这/ 两类/ 反馈/ 在/ 训练/ 时应/ 减少/ 迭代/ 次数/ 以/ 达到/ 弱化/ 学习/ 的/ 目的/ ./ 综上/ 分析/ ,/ 可以/ 通过/ 比较/ 反馈/ 发生/ 的/ 概率/ 与/ 用户/ 置信度/ 来/ 自动控制/ 每/ 一个/ 反馈/ 的/ 学习/ 步长/ ./ 这是/ oIFRM/ 在线/ 学习/ 算法/ 的/ 核心/ 所在/ (/ 算法/ 1/ 第/ 4/ 行/ )/ ./ 下面/ 介绍/ 反馈/ 发生/ 的/ 概率/ 与/ 用户/ 置信度/ 的/ 具体/ 定义/ ./ 一方面/ ,/ IFRM/ 具备/ 完备/ 的/ 概率/ 解释/ (/ 见/ 3.1/ 节/ )/ ,/ 可以/ 将/ 用户/ 选择/ 概率/ Prij/ 作为/ 反馈/ 发生/ 的/ 概率/ ./ 使用/ φ/ (/ x/ )/ =/ 下/ 特性/ :/ 当/ Prij/ ≈/ 0.5/ 时/ ,/ Aij/ ≈/ Ai/ ,/ 用户/ i/ 对/ 产品/ j/ 与/ 一般/ 产品/ 相比/ 没有/ 明显/ 区别/ ,/ 此时/ 推荐/ 系统/ 最/ 不/ 确定/ 用户/ 是否/ 会/ 选择/ 该/ 产品/ ;/ 当/ Prij/ 接近/ 1/ 时/ ,/ 推荐/ 系统/ 认为/ 用户/ 很/ 可能/ 选择/ 该/ 产品/ ,/ 反馈/ 行为/ 发生/ 的/ 可能性/ 很/ 高/ ;/ 当/ Prij/ 接近/ 0/ 时/ ,/ 推荐/ 系统/ 认为/ 用户/ 很/ 可能/ 不会/ 选择/ 该/ 产品/ ,/ 反馈/ 发生/ 的/ 概率/ 较/ 低/ ./ 综上所述/ ,/ Prij/ 符合/ 反馈/ 发生/ 概率/ 的/ 直观/ 解释/ ./ 另一方面/ ,/ 引入/ 用户/ 置信度/ 变量/ θ/ 来/ 衡量/ 用户/ 行为/ 的/ 可靠/ 程度/ ./ 如果/ 用户/ i/ 的/ 大部分/ 行为/ 符合/ 推荐/ 系统/ 预期/ ,/ 则/ 系统/ 认为/ 用户/ i/ 的/ 反馈/ 可靠/ 度/ 较/ 高/ ,/ 此时/ 对/ 用户/ i/ 的/ 反馈/ 较为/ 重视/ ;/ 反之/ ,/ 如果/ 用户/ i/ 存在/ 较/ 多/ 出乎/ 系统/ 预料/ 的/ 反馈/ ,/ 则/ 用户/ i/ 的/ 置信度/ 不高/ ,/ 其/ 反馈/ 对系统/ 的/ 影响/ 也/ 较/ 小/ ,/ 以此/ 屏蔽/ 由/ 恶意/ 用户/ 发起/ 的/ 攻击/ ,/ 进而/ 提高/ 系统/ 的/ 抗/ 攻击能力/ ./ 算法/ 1/ 展示/ 了/ oIFRM/ 在线/ 学习/ 算法/ ,/ 使用/ 3.2/ 节/ 给出/ 的/ 通用/ 应用/ 范例/ (/ 式/ (/ 7/ )/ )/ 构造/ 模型/ ./ 对模型/ 参数/ 进行/ 随机/ 初始化/ 并/ 归一化/ 后/ ,/ 顺序/ 地/ 从/ 隐式/ 反馈/ 数据流/ 中/ 读入/ 一个/ 用户/ 反馈/ 〈/ i/ ,/ j/ 〉/ ,/ 首先/ 根据/ Prij/ 来/ 调节/ θ/ i/ ./ 新/ 的/ θ/ i/ 为/ 所有/ Pri/ / 的/ 平均值/ ./ 然后/ 更新/ 相关/ 参数/ ,/ 通过/ 比较/ Prij/ 和/ θ/ i/ 来/ 控制/ 学习/ 步长/ ./ Page6/ 算法/ 1/ ./ oIFRM/ 在线/ 学习/ 算法/ ./ 输入/ :/ 隐式/ 反馈/ 数据流/ FS/ =/ {/ …/ ,/ 〈/ Useri/ ,/ Itemj/ 〉/ ,/ …/ }/ 实时/ 输出/ :/ 模型/ 参数/ U/ ,/ V/ ,/ S/ ,/ c1/ ./ 初始化/ :/ 随机/ 构造/ U/ ,/ V/ ,/ S/ 并/ 归一化/ 2/ ./ 从/ FS/ 中/ 顺序/ 读取/ 一个/ 反馈/ 〈/ i/ ,/ j/ 〉/ :/ 3/ ./ 根据/ 〈/ i/ ,/ j/ 〉/ 发生/ 的/ 估计/ 概率/ Prij/ 调节/ θ/ i4/ ./ DOWHILEPrij/ </ θ/ i/ :/ 5.6/ ./ 7.8/ ./ 9/ ./ ENDWHILE10/ ./ 直到/ FS/ 为空/ 4/ 数据/ 集/ 与/ 源码/ 本文/ 使用/ 两个/ 数据/ 集/ 进行/ 实验/ ./ 其中/ Last/ ./ fm/ 是/ 国外/ 知名/ 音乐网站/ ,/ 豆瓣/ 是/ 国内/ 知名/ 影评/ 网站/ ./ 收听/ 音乐/ 与/ 观看/ 电影/ 是/ 较/ 典型/ 的/ 两类/ 隐式/ 用户/ 反馈/ ,/ 特别/ 适合/ 用来/ 研究/ 隐式/ 反馈/ 推荐/ 系统/ ./ 从/ Last/ ./ fm/ 官方/ 提供/ 的/ API/ (/ http/ :/ // // www/ ./ last/ ./ fm/ // api/ )/ 收集/ 了/ 132/ 个/ 用户/ 的/ 收听/ 记录/ (/ 〈/ 用户/ ,/ 歌手/ ,/ 歌曲/ ,/ 时间/ 戳/ 〉/ 的/ 四元组/ )/ 时间跨度/ 由/ 2005/ 年/ 2/ 月/ 14/ 日到/ 2005/ 年/ 7/ 月/ 31/ 日/ ./ 本/ 实验/ 将/ 歌曲/ 看作/ 产品/ (/ Item/ )/ ,/ 将/ 歌手/ 信息/ 作为/ 描述/ 歌曲/ 的/ 特征/ (/ Feature/ )/ ./ 将/ 所有/ 收集/ 到/ 的/ 记录/ 按/ 时间/ 排序/ 得到/ 隐式/ 反馈/ 数据流/ ./ 从/ 豆瓣/ 网/ 抓取/ 了/ 6250/ 个/ 用户/ 从/ 2005/ 年/ 5/ 月/ 3/ 日到/ 2006/ 年/ 12/ 月/ 31/ 日/ 的/ 所有/ 观影/ 记录/ ,/ 并/ 抓取/ 了/ 相关/ 电影/ 的/ 介绍/ 页面/ ,/ 从中/ 提取/ 出/ 14496/ 个/ 特征/ ,/ 包括/ 主演/ (/ 14233/ 个/ )/ ,/ 类别/ (/ 37/ 种/ )/ ,/ 语言/ (/ 130/ 种/ )/ ,/ 发行/ 地区/ (/ 96/ 个/ )/ ./ 详细/ 统计/ 信息/ 如表/ 1/ 所示/ ./ 数据/ 集/ LastFM/ 豆瓣/ 本文/ 提供/ 了/ 数据/ 集/ 以及/ 源码/ (/ 下载/ 地址/ 见/ 论文/ 首页/ )/ ,/ 已经/ 对/ 数据/ 集/ 属性/ 部分/ 进行/ 了/ 归一化/ 处理/ ,/ 并用/ Python/ 语言/ 实现/ 了/ 算法/ 1/ ,/ 并/ 采用/ 在线/ 评价/ 的/ 方式/ 进行/ 测试/ (/ 详见/ 5.1/ 节/ )/ ./ 代码/ 的/ 详细/ 使用/ 说明/ 请/ 参考/ 下载/ 包中/ 的/ readme/ ./ txt/ ./ 5/ 实验/ 本节/ 将/ 详细/ 报告/ 实验/ 方法/ 和/ 相关/ 结论/ ,/ 从/ 多个/ 角度/ 展示/ oIFRM/ 的/ 表现/ ./ 5.1/ 实验/ 设置/ 传统/ 的/ 实验设计/ 大多/ 采用/ 离线/ 方式/ ,/ 先/ 将/ 数据/ 集切/ 分为/ 两/ 部分/ ,/ 使用/ 一部分/ 训练/ 模型/ ,/ 用/ 另/ 一部分/ 进行/ 测试/ ./ 这种/ 离线/ 测试/ 的/ 方式/ 并/ 不能/ 很/ 好/ 地/ 评价/ 模型/ 的/ 线/ 上/ 表现/ ./ 工业界/ 常用/ A/ // B/ 测试/ 来/ 比较/ 不同/ 算法/ ,/ 但/ 其/ 代价/ 较大/ ./ 本文/ 通过/ 模拟/ 真实/ 在线/ 环境/ ,/ 从而/ 设计/ 了/ 一种/ 可以/ 离线/ 比较/ 不同/ 推荐/ 模型/ 线上/ 表现/ 的/ 实验/ 框架/ ,/ 如图/ 2/ 所示/ ./ 为了/ 克服/ 隐式/ 反馈/ 的/ 数据/ 缺乏/ 负/ 样本/ 因而/ 难以/ 评价/ 的/ 困难/ ,/ 我们/ 对于/ 每个/ 反馈/ ,/ 随机/ 从/ 全体/ 产品/ 集合/ 中/ 抽样/ 1000/ 个/ 产品/ ,/ 与/ 该/ 用户/ 真实/ 选择/ 的/ 产品/ 一起/ 组成/ 长度/ 为/ 1001/ 的/ 候选/ 列表/ ,/ 该/ 方法/ 已/ 被/ 广泛/ 地/ 用于/ 评价/ 隐式/ 反馈/ 推荐/ 模型/ [/ 9/ ,/ 16/ ]/ ./ 本/ 实验/ 的/ 任务/ 是/ 重排/ 候选/ 列表/ 得到/ TopN/ 推荐/ 列表/ ,/ 并/ 尽可能/ 的/ 令/ 用户/ 真实/ 选择/ 的/ 产品/ 排/ 在/ 前面/ ./ 该/ 任务/ 与/ 图/ 1/ 描述/ 的/ 在线/ 学习/ 与/ 实时/ 推荐/ 框架/ 是/ 一致/ 的/ ./ 采用/ 推荐/ 系统/ 中/ 常用/ 的/ 3/ 个/ 指标/ 来/ 进行/ 评价/ ,/ 包括/ 反映/ 召回/ 率/ 的/ Rank/ 值/ ,/ 反映/ 准确率/ 的/ TopN/ 命中率/ (/ HitRatio/ )/ 以及/ 反映/ 推荐/ 多样性/ 的/ 覆盖率/ (/ Corverage/ )/ ./ Rank/ 值为/ 用户/ 真实/ 选择/ 产品/ j/ 在/ 推荐/ 列表/ 中/ 的/ 百分比/ 位置/ ./ 如果/ j/ 排在/ 推荐/ 列表/ 首位/ ,/ 则/ Rank/ =/ 0/ ;/ 如果/ 排/ 在/ 末位/ ,/ 则/ Rank/ =/ 100/ %/ ./ 命中率/ 指/ 系统/ 每次/ 推荐/ 给/ 用户/ N/ 个/ 产品/ ,/ 导致用户/ 选择/ 的/ 推荐/ 占/ 总/ 推荐/ 次数/ 的/ 比例/ ./ 覆盖率/ 指/ 推荐/ 过/ 的/ 产品/ 占/ 全部/ 产品/ 的/ 比例/ ./ 为了/ 使/ 评价/ 结果/ 更/ 具有/ 统计/ 意义/ ,/ 在/ 该/ 实验/ 中/ ,/ 平均/ 每/ 5000/ 次连续/ 的/ 推荐/ 得到/ 1/ 次/ 评价/ (/ 对应/ 图/ 3/ 中/ 的/ 1/ 个点/ )/ ./ Page7/ 图/ 3Top30/ 实时/ 推荐/ 效果/ 动态/ 比较/ 5.2/ 对比/ 算法/ 根据/ 隐式/ 反馈/ 推荐/ 的/ 特点/ 以及/ 实时/ 推荐/ 的/ 要求/ ,/ 选择/ 以下/ 6/ 种/ 算法/ 进行/ 对比/ :/ (/ 1/ )/ iMF/ (/ implicitMatrixFactorization/ )/ [/ 13/ ]/ 将/ 已/ 观察/ 到/ 的/ 用户/ 反馈/ 置/ 为/ 1/ ,/ 其他/ 置/ 为/ 0/ ,/ 并/ 设置/ 相对/ 小/ 的/ 置信度/ 权重/ ,/ 将/ 问题/ 转化/ 为/ 0/ -/ 1/ 矩阵/ 分解/ ./ (/ 2/ )/ BPR/ (/ BayesianPersonalizedRanking/ )/ [/ 12/ ]/ 采用/ 随机抽样/ 抽取/ 负/ 样本/ 的/ 方式/ 将/ 问题/ 转化/ 为/ pairwise/ 排序/ 学习/ ./ (/ 3/ )/ bIFRM/ (/ batchIFRM/ )/ 使用/ 随机/ 梯度/ 下降/ 算法/ 采用/ 离线/ 批量/ 的/ 训练/ 方式/ 求解/ IFRM/ 模型/ 参数/ ./ 在/ 实验/ 中/ ,/ 每/ 收集/ 到/ 5/ 万个/ 用户/ 反馈/ 后/ 将/ 新/ 收集/ 到/ 的/ 反馈/ 加入/ 原/ 训练/ 集后/ 重新/ 训练/ ./ (/ 4/ )/ TP/ (/ TemporalPopularity/ )/ 是/ IFRM/ 的/ 一个/ 组成部分/ ./ 通过/ 统计/ 滑动/ 窗口/ 中/ 产品/ 被/ 选择/ 的/ 次数/ 来/ 向/ 用户/ 推荐/ 时下/ 最/ 流行/ 的/ 产品/ ./ 由于/ 音乐/ 及/ 电影/ 的/ 推荐/ 时效性/ 要求/ 都/ 较/ 强/ ,/ TP/ 在/ 本/ 实验/ 中/ 具有/ 竞争力/ ./ Page8/ (/ 5/ )/ RMFX/ (/ StreamRankingMatrixFactoriza/ -/ tion/ )/ [/ 9/ ]/ 是/ 较/ 新/ 的/ 在线/ 隐式/ 反馈/ 推荐/ 模型/ ,/ 与/ oIFRM/ 不同/ ,/ RMFX/ 需要/ 抽取/ 负/ 样本/ 并/ 使用/ 固定/ 的/ 学习/ 步长/ ./ (/ 6/ )/ DMF/ (/ DynamicMatrixFactorization/ )/ [/ 8/ ]/ 在/ 非负/ 矩阵/ 分解/ 的/ 基础/ 上/ 结合/ 线性/ 动态/ 系统/ 来/ 对/ 用户/ 选择/ 行为/ 进行/ 动态/ 建模/ ./ 虽然/ 整合/ 了/ 时间/ 信息/ ,/ 但/ DMF/ 仍/ 采用/ 离线/ 批量/ 的/ 训练/ 模式/ ./ 它/ 是/ 冷启动/ 相关/ 实验/ 中/ 的/ 主要/ 对比/ 基线/ ./ 5.3/ 参数/ 调节/ 首先/ ,/ 通过/ 多次/ 实验/ ,/ 调节/ 对比/ 算法/ 的/ 参数/ 使/ 其/ 达到/ 效果/ 最优/ ./ 根据/ 文献/ [/ 8/ -/ 9/ ]/ ,/ 我们/ 经验/ 地/ 设置/ 潜在/ 特征/ 维数/ 为/ 50/ ,/ 迭代/ 次数/ 为/ 30/ ,/ 学习/ 率为/ 10/ -/ 4/ ./ 表/ 2/ 列出/ 了/ oIFRM/ 可以/ 调节/ 的/ 参数/ ,/ 并/ 给出/ 了/ 可/ 直接/ 适用/ 于/ 多种/ 应用/ 场景/ (/ 包括/ 2/ 个/ 数据/ 集/ 以及/ 多种/ 实验/ 设置/ )/ 的/ 默认值/ ./ 5.4/ 节中/ 报告/ 的/ 全部/ 实验/ 结果/ 都/ 直接/ 使用/ 了/ 该组/ 默认/ 参数/ ./ 由/ 多次/ 实验/ 可以/ 看出/ ,/ oIFRM/ 的/ 参数/ 容易/ 理解/ 和/ 设置/ ,/ 具有/ 较强/ 的/ 适用性/ 表/ 3/ 调节/ oIFRM/ 应用/ 范例/ 3/ 方面/ 因素/ 的/ 权重/ 对/ 效果/ 提升/ 的/ 影响/ (/ “/ S/ ”/ 表示/ 显著/ 水平/ 犘/ 值/ 小于/ 0.001/ )/ 数据/ 集/ φ/ 1/ φ/ 2/ φ/ 3LASTFM/ 豆瓣/ 5.4/ 实验/ 结果/ 与/ 分析/ 实验/ 1/ ./ 推荐/ 质量/ 与/ 可/ 解释性/ ./ 表/ 4/ 列出/ 了/ 6/ 种/ 算法/ 的/ 整体/ 推荐/ 效果/ ,/ 分析/ 得到/ 如下/ 结论/ :/ (/ 1/ )/ 在线/ 模型/ 比/ 离线/ 模型/ 在/ 推荐/ 质量/ 上/ 更具/ 优势/ ,/ 这/ 是因为/ 在线/ 学习/ 能/ 及时/ 利用/ 反馈/ 信息/ ,/ 而/ 离线/ 训练/ 在/ 推荐/ 时效性/ 上/ 存在/ 局限性/ ;/ (/ 2/ )/ 本文/ 提出/ 的/ 模型/ IFRM/ 在/ 两种/ 训练/ 模式/ 下/ 与/ 对比/ 算法/ 相比/ 都/ 取得/ 了/ 更好/ 的/ 推荐/ 效果/ ;/ (/ 3/ )/ 设置/ N/ =/ 5/ ,/ 20/ ,/ 30/ ,/ 各/ 模型/ 在/ 不同/ 的/ TopN/ 推荐/ 中/ 趋势/ 一致/ ,/ 因此/ 后面/ 实验/ 中/ 只/ 报告/ N/ =/ 30/ 时/ 的/ 结果/ ./ 图/ 3/ 以/ 动态/ 的/ 方式/ 展示/ 了/ 各/ 模型/ 的/ 线/ 上/ 表现/ ,/ 可/ 得/ 以下/ 观察/ 结果/ :/ (/ 1/ )/ 在线/ 学习/ 的/ 优势/ 在/ LastFM/ 数据/ 集上/ 更为/ 和/ 推广/ 能力/ ./ 参数/ DW/ λ/ 1/ λ/ 2/ α/ θ/ ,/ φ/ 33.2/ 节/ 给出/ 的/ IFRM/ 应用/ 范例/ 引入/ 了/ 参数/ φ/ 1/ 来/ 权衡/ 各/ 因素/ 的/ 影响/ ./ 我们/ 使用/ 网格/ 搜索/ 策略/ φ/ 2/ 来/ 对/ 其/ 进行/ 调节/ ,/ 实验/ 结果/ 如表/ 3/ 所示/ ./ 为了/ 得到/ 统计/ 意义/ 上/ 稳定/ 的/ 实验/ 结果/ ,/ 我们/ 只/ 记录/ 了/ 模型/ 稳定/ 后/ (/ 最后/ 10/ 万次/ 反馈/ )/ 的/ 评价/ 结果/ ,/ 将/ 实验/ 重复/ 了/ 10/ 次/ 并/ 进行/ t/ 检验/ ./ 观察/ 发现/ ,/ 喜爱/ 程度/ 、/ 上下文/ 条件/ 与/ 社会/ 环境/ 都/ 会/ 影响/ 用户/ 选择/ ./ 后续/ 实验/ 分别/ 在/ 两个/ 数据/ 集上/ 选取/ 了/ 能够/ 取得/ 最好/ 效果/ 的/ 设置/ ./ Top30/ 推荐/ 命中率/ 平均/ 较/ TP/ 提升/ // %/ t/ 检验/ P/ 值/ 15.315/ ./ 453.84/ ./ 80.110/ ./ 44.453/ ./ 80.00/ ./ 3/ -/ 9.4/ -/ 0.10/ ./ 1/ -/ 0.30/ ./ 10.3/ 明显/ ./ 因为/ 电影/ 更新/ 速度/ 较慢/ 且/ 用户/ 兴趣/ 较为/ 稳定/ ,/ 而/ 用户/ 对/ 音乐/ 的/ 兴趣/ 更/ 易/ 发生变化/ ,/ 此时/ ,/ 在线/ 学习/ 能/ 更/ 及时/ 地/ 捕获/ 这种/ 变化/ ;/ (/ 2/ )/ 对比/ 3/ 种/ 在线/ 模型/ ,/ oIFRM/ 除了/ 在/ 豆瓣/ 数据/ 集上/ 命中率/ 与/ TP/ 较为/ 接近/ 外/ ,/ 整体/ 上/ 表现/ 出/ 了/ 明显/ 的/ 优越性/ ./ 这/ 是因为/ 豆瓣/ 本身/ 就/ 会/ 在/ 首页/ 向/ 用户/ 推荐/ 近期/ 上映/ 的/ 电影/ ,/ 所以/ TP/ 策略/ 已经/ 可以/ 达到/ 较/ 高/ 的/ 推荐/ 命中率/ ./ 但是/ ,/ TP/ 策略/ 不利于/ 发现/ 用户/ 个性化/ 的/ 需求/ ,/ 所以/ 在/ 推荐/ 召回/ 率/ 以及/ 多样性/ 等/ 指标/ 上/ 明显/ 不及/ oIFRM/ ./ RMFX/ 同样/ 倾向/ 于/ 推荐/ 热门/ 产品/ ,/ 这/ 是因为/ 受/ 优化/ 目标/ 驱使/ ,/ 更新/ 算法/ 不断/ 加大/ 热门/ 产品/ 的/ 潜在/ 特征/ ,/ 使/ 热门/ 产品/ 更/ 容易/ 被/ 推荐/ ;/ Page9/ 表/ 4/ 实时/ Top/ 犖/ 推荐/ 整体/ 效果/ 比较稳定/ 后/ 推荐/ 命中率/ Top20/ 数据/ 集/ LASTFM/ 豆瓣/ (/ 3/ )/ 对比/ 3/ 种/ 离线/ 模型/ ,/ bIFRM/ 能/ 产生/ 更/ 精准/ 的/ 推荐/ ./ 值得注意/ 的/ 是/ ,/ 测试/ 时/ 随机/ 采样/ 作为/ 负/ 样本/ 与/ BPR/ 的/ 采样/ 过程/ 看似/ 一致/ ,/ 但/ 实际上/ ,/ BPR/ 每次/ 只/ 采样/ 1/ 个/ 负/ 样本/ ,/ 与/ 测试/ 场景/ 中/ 采样/ 大量/ (/ 1000/ 个/ )/ 负/ 样本/ 是/ 有/ 本质区别/ 的/ ./ 相反/ ,/ IFRM/ 则/ 使用/ “/ 平均/ 选择/ 倾向/ 度/ ”/ 来/ 近似/ 大量/ 负/ 样本/ 的/ 统计/ 均值/ ,/ 因此/ 更加/ 契合/ 测试/ 场景/ ,/ 并/ 取得/ 了/ 更好/ 的/ 推荐/ 质量/ ,/ 同时/ 这种/ 采样/ 大量/ 负/ 样本/ 的/ 设置/ 也/ 更/ 贴近/ 真实/ 应用/ 场景/ ;/ (/ 4/ )/ 通过/ 整合/ 多种/ 异构/ 信息/ ,/ IFRM/ 的/ 推荐/ 结果/ 更加/ 多样化/ ./ 由于/ 初期/ 缺乏/ 足够/ 的/ 反馈/ 来/ 建模/ 用户/ 敏感度/ 和/ 从众/ 度/ ,/ oIFRM/ 倾向/ 于/ 推荐/ 时/ 下/ 流行/ 的/ 产品/ (/ TP/ 起/ 主导作用/ )/ ,/ 随着/ 学习/ 到/ 的/ 用户/ 反馈/ 逐渐/ 增多/ ,/ oIFRM/ 的/ 推荐/ 结果/ 也/ 越来越/ 个性化/ ./ 跟踪/ 用户/ 从众/ 度/ c/ ,/ 记录/ 其/ 统计/ 值如图/ 4/ 所示/ ./ oIFRM/ 在/ 可/ 解释性/ 方面/ 具有/ 如下/ 优势/ :/ (/ 1/ )/ 对/ 应用环境/ 的/ 理解/ ./ 平均/ 而言/ ,/ Last/ ./ fm/ 用户/ 比/ 豆瓣/ 用户/ 更/ 不/ 从众/ ./ 在/ LastFM/ 数据/ 上/ ,/ 用户/ 平均/ 从众/ 度/ 持续/ 下降/ 到/ 0.7/ 以下/ ,/ 而/ 在/ 豆瓣/ 数据/ 上/ ,/ 该值/ 稳定/ 在/ 1.0/ 左右/ ./ Last/ ./ fm/ 用户/ 从众/ 程度/ 的/ 差异性/ 较大/ ,/ 部分/ 用户/ 有/ 自己/ 较为/ 独特/ 的/ 品味/ ;/ 而/ 豆瓣/ 用户/ 的/ 从众/ 度/ 分布图/ 4/ 个性化/ 的/ 用户/ 从众/ 程度/ 的/ 动态/ 统计/ 则/ 比较/ 集中/ ,/ 其中/ 大多数/ 倾向/ 于/ 观看/ 时/ 下/ 流行/ 的/ 电影/ (/ 均值/ 线/ 与/ 最大值/ 线/ 非常/ 接近/ )/ ,/ 因为/ 观看/ 电影/ 需要/ 花费/ 更/ 多/ 的/ 时间/ 成本/ 和/ 经济/ 成本/ ;/ (/ 2/ )/ 对/ 用户/ 的/ 理解/ ./ 通过/ 持续/ 学习/ ,/ 从众/ 度/ 最大值/ 与/ 最小值/ 之间/ 的/ 差距/ 逐渐/ 变/ 大/ ,/ 从众/ 度/ 的/ 个性化/ 程度/ 也/ 越来越/ 高/ ,/ oIFRM/ 对/ 用户/ 的/ 理解/ 程度/ 也/ 越来越/ 深/ ./ 在/ 对/ 用户/ 进行/ 推荐/ 时/ ,/ 可以/ 依据/ 其/ 从众/ 度/ 选择/ 不同/ 的/ 推荐/ 策略/ ,/ 比如/ 向/ 比较/ 从众/ 的/ 用户/ 推荐/ 热门/ 产品/ ,/ 并/ 强调/ 产品/ 的/ 流行/ 度/ ,/ 向/ 较/ 个性/ 的/ 用户/ 推荐/ 小/ 众/ 产品/ ,/ 并/ 强调/ 产品/ 特色/ ./ 增强/ 推荐/ 结果/ 的/ 可/ 解释性/ 将/ 有助于/ 提高/ 推荐/ 接受/ 率/ 和/ 用户/ 满意度/ ./ 该/ 实验/ 验证/ 了/ 个性化/ 用户/ 从众/ 程度/ 是/ 有效/ 的/ ./ 实验/ 2/ ./ 训练/ 效率/ 与/ 吞吐量/ ./ 实验/ 2/ 通过/ 比较/ oIFRM/ 与/ 其他/ 在线/ 推荐/ 模型/ 的/ 训练/ 效率/ 来/ 验证/ oIFRM/ 处理/ 大规模/ 数据流/ 的/ 能力/ ,/ 并/ 进一步/ 给出/ 系统/ 吞吐量/ 的/ 估计/ ./ 由于/ 离线/ 模型/ 需要/ 重/ 训练/ ,/ 其/ 时间/ 开销/ 与/ 在线/ 模型/ 不/ 具有/ 可比性/ ,/ 因此/ 本文/ 仅/ 选用/ 在线/ 模型/ TP/ 和/ RMFX/ 作为/ 对比/ 算法/ ./ 实验/ 部署/ 在/ 配备/ 酷睿/ i5/ 双核/ 处理器/ 2GB/ 内存/ 的/ PC/ 上/ ,/ 所有/ 算法/ 使用/ Python/ 语言/ 实现/ ./ 需要/ Page10/ 说明/ 的/ 是/ ,/ 用/ C语言/ 实现/ 并/ 部署/ 在/ 高性能/ 服务器/ 上能/ 进一步提高/ 训练/ 效率/ ,/ 因此/ 这里/ 的/ 报告/ 结果/ 仅/ 是/ 保守/ 估计/ ./ 在/ 实际/ 应用/ 中/ ,/ 还/ 可以/ 使用/ 抽样/ 或/ 并行计算/ 技术/ [/ 17/ ]/ 来/ 进一步/ 减少/ 训练/ 的/ 时间/ 开销/ ,/ 提高/ 系统/ 的/ 吞吐量/ ./ 在/ 一般/ 情况/ 下/ ,/ 潜在/ 特征/ 维数/ D/ 对/ 训练/ 时间/ 的/ 影响/ 较大/ ,/ 因此/ 本文/ 考察/ D/ =/ 5/ ,/ 50/ ,/ 200/ 时/ 各/ 模型/ 进行/ 一次/ 更新/ 的/ 平均/ 时间/ 开销/ ./ 为了/ 使/ 结果/ 更/ 可靠/ ,/ 我们/ 将/ 实验/ 重复/ 10/ 次/ ,/ 统计/ 结果/ 在/ 表/ 5/ 中/ 列出/ ./ TP/ 的/ 优势/ 非常明显/ ,/ 其/ 每次/ 更新/ 的/ 时间/ 复杂度/ 仅为/ O/ (/ 1/ )/ ./ 与/ RMFX/ 相比/ ,/ oIFRM/ 将/ 时间/ 开销/ 缩减/ 了/ 约/ 50/ %/ ,/ 这/ 是因为/ RMFX/ 对/ 所有/ 反馈/ 采用/ 固定/ 的/ 迭代/ 次数/ ,/ 而/ oIFRM/ 区别对待/ 每/ 一个/ 反馈/ ,/ 使/ 学习/ 更加/ 有效/ (/ 有些/ 价值/ 较/ 低/ 的/ 反馈/ 被/ 忽略/ )/ ./ 值得注意/ 的/ 是/ ,/ 潜在/ 特征/ 维数/ D/ 对/ oIFRM/ 的/ 学习/ 效率/ 影响/ 不/ 大/ ,/ 甚至/ 设置/ 更大/ 的/ D/ 时/ ,/ 训练/ 时间/ 不增/ 反降/ ./ 经过/ 调试/ 模型/ 我们/ 发现/ ,/ 较大/ 的/ D/ 导致/ 个性化/ 程度较高/ ,/ 进而/ 导致用户/ 置信度/ θ/ 持续/ 变小/ ,/ 此时/ 迭代/ 次数/ 也/ 相应/ 变小/ ./ 表/ 5/ 一次/ 更新/ 的/ 平均/ 时间/ 开销/ (/ 单位/ :/ ms/ )/ 模型/ // 数据/ 集/ D/ =/ 5D/ =/ 50D/ =/ 200/ 对于/ 在线/ 系统/ 来说/ ,/ 吞吐量/ 是/ 一个/ 重要/ 指标/ ./ 经/ 估算/ ,/ oIFRM/ 的/ 数据/ 吞吐能力/ 为/ 429/ 个/ 用户/ 反馈/ 每秒/ ,/ 已经/ 能够/ 满足/ 一般/ 的/ 应用/ 需求/ ./ 实验/ 3/ ./ 系统/ 健壮性/ ./ 由于/ 难以/ 对/ 数据流/ 进行/ 在线/ 清洗/ ,/ 因此/ 对于/ 在线/ 系统/ 来说/ ,/ 健壮性/ 显得/ 尤其/ 重要/ ./ 当/ 面临/ 恶意/ 攻击/ 时/ ,/ 在线/ 推荐/ 系统/ 的/ 推荐/ 质量/ 将/ 受到/ 影响/ ,/ 更/ 甚者/ 可能/ 导致系统/ 崩溃/ ./ 为此/ ,/ 实验/ 3/ 通过/ 模拟/ 恶意/ 攻击/ 情景/ 来/ 评估/ 各/ 模型/ 的/ 抗/ 攻击能力/ ./ 对于/ 隐式/ 反馈/ 推荐/ 系统/ 来说/ ,/ 爬虫/ 的/ 随机/ 访问/ (/ 点击/ )/ 是/ 一种/ 常见/ 的/ 攻击/ 来源/ ./ 我们/ 随机/ 选择/ 了/ 两个/ 用户/ 作为/ 攻击者/ ,/ 分别/ 在/ 两个/ 时间/ 点/ 各/ 发起/ 1/ 万次/ 随机/ 访问/ ./ 观察/ 图/ 5/ 可以/ 发现/ ,/ RMFX/ 在/ 第/ 8/ 万/ 和/ 第/ 16/ 万次/ 反馈/ 后/ 推荐/ 命中率/ 明显/ 下滑/ ,/ 恰好/ 与/ 预先/ 设置/ 的/ 两个/ 攻击/ 时间/ 点/ 吻合/ ./ 而/ oIFRM/ 几乎/ 没有/ 受到/ 任何/ 影响/ ./ 通过/ 监测/ 用户/ 置信度/ θ/ 的/ 变化/ 我们/ 发现/ ,/ 与/ 正常/ 情况/ 相比/ (/ 图/ 6/ 中/ 实线/ )/ ,/ 发动/ 攻击/ 之后/ (/ 图/ 6/ 中/ 虚线/ )/ ,/ 这/ 两个/ 用户/ 的/ 置信度/ 有/ 明显/ 下降/ ./ 这/ 意味着/ 系统/ 已经/ 判别/ 出/ 用户/ 行为/ 的/ 异常/ 并/ 屏蔽/ 了/ 该/ 用户/ 的/ 反馈/ (/ 攻击/ )/ ,/ 进而/ 保证/ 了/ 系统/ 的/ 推荐/ 质量/ ./ 可见/ ,/ 引入/ 用户/ 置信度/ 有助于/ 提高/ 模型/ 的/ 鲁棒性/ ./ 在/ 实际/ 应用/ 中/ ,/ 可以/ 通过/ 监测/ θ/ 来/ 识别/ 异常/ 用户/ ./ 此外/ ,/ 由于/ θ/ 表示/ 系统对/ 用户/ 进行/ 推荐/ 时/ 的/ 信心/ ,/ 可以/ 使用/ 其他/ 模型/ 对/ θ/ 值较/ 低/ 的/ 用户/ 进行/ 推荐/ ./ 实验/ 4/ ./ 热启动/ 性能/ 提升/ 以及/ 冷启动/ 适应能力/ ./ 由于/ 离线/ 批量/ 训练/ 是/ 导致/ 冷启动/ 问题/ 的/ 根源/ ,/ 因此/ 设计/ 实验/ 4/ ,/ 其/ 目的/ 在于/ 验证/ 在线/ 学习/ 是否/ 能/ 有效/ 地/ 提高/ 冷启动/ 的/ 适应能力/ ./ 对比/ 算法/ 选用/ 较/ 新/ 的/ 动态/ 模型/ DMF/ ./ 该/ 模型/ 同样/ 整合/ 了/ 时间/ 信息/ ,/ 但/ 采用/ 离线/ 训练/ 模式/ ./ 为了/ 定义/ 冷启动/ 用户/ 和/ 产品/ ,/ 遵循/ 传统/ 的/ 测试方法/ ,/ 根据/ 时间/ 将/ 前/ 30/ 万/ 反馈/ 数据/ 作为/ 训练/ 集/ ,/ 余下/ 的/ 作为/ 测试/ 集/ ./ 统计/ 信息/ 如表/ 6/ 所示/ ./ 设/ User/ 为/ 训练/ 集/ 的/ 用户/ 集合/ ,/ Item/ 为/ 训练/ 集/ 的/ 产品/ 集合/ ./ 对/ 测试/ 样本/ 进行/ 筛选/ 构造/ 出/ 以下/ 4/ 种/ 场景/ :/ (/ 1/ )/ 热启动/ 场景/ ,/ 要求/ 用户/ 和/ 产品/ 都/ 在/ 训练/ 集中/ 出现/ 过/ ,/ TEST1/ =/ {/ 〈/ i/ ,/ j/ 〉/ |/ i/ ∈/ UserANDj/ ∈/ Item/ }/ ;/ (/ 2/ )/ 冷启动/ 用户/ 场景/ ,/ 要求/ 用户/ 未/ 在/ 训练/ 集中/ 出现/ 过/ ,/ TEST2/ =/ {/ 〈/ i/ ,/ j/ 〉/ |/ i/ / User/ }/ ;/ (/ 3/ )/ 冷启动/ 产品/ 场景/ ,/ 要求/ 产品/ 未/ 在/ 训练/ 集中/ 出现/ 过/ ,/ TEST3/ =/ Page11/ {/ 〈/ i/ ,/ j/ 〉/ |/ j/ / Item/ }/ ;/ (/ 4/ )/ 冷启动/ 场景/ ,/ 要求/ 用户/ 或/ 产品/ 其中/ 任意/ 一方/ 是/ 新/ 的/ ,/ TEST4/ =/ {/ 〈/ i/ ,/ j/ 〉/ |/ i/ / User/ 训练/ 集/ 用户数/ 训练/ 集/ 产品/ 数/ 测试/ 集/ 用户数/ 测试/ 集/ 产品/ 数/ 冷启动/ 用户数/ 冷启动/ 用户/ 比例/ // %/ 冷启动/ 产品/ 数/ 冷启动/ 产品/ 比例/ // %/ 表/ 6/ 数据/ 划分/ 统计/ 信息/ LastFM/ 豆瓣/ 数据/ 集/ LastFM/ 豆瓣/ 表/ 7/ 在/ 各/ 场景/ 下/ oIFRM/ 对/ 推荐/ 效果/ 的/ 改进/ AverageRankDMF/ 在/ 训练/ 集上/ 进行/ 离线/ 训练/ ,/ 然后/ 分别/ 用/ 4/ 个/ 测试/ 集来/ 进行/ 测试/ ./ oIFRM/ 则/ 采用/ 在线/ 学习/ 与/ 在线/ 评价/ 结合/ 的/ 方式/ (/ 图/ 2/ )/ ./ 评价/ 指标/ 采用/ 侧重/ 召回/ 率/ 的/ AverageRank/ (/ AR/ )/ 以及/ 侧重/ 准确率/ 的/ AverageHitRatio/ (/ AHR/ )/ ./ 实验/ 结果/ 由表/ 7/ 给出/ ,/ 分析/ 可/ 得/ 如下/ 结论/ :/ (/ 1/ )/ oIFRM/ 在/ 4/ 种/ 场景/ 下/ 都/ 能/ 取得/ 更好/ 的/ 推荐/ 精度/ ./ 这/ 是因为/ oIFRM/ 能/ 及时/ 利用/ 最新/ 的/ 用户/ 反馈/ 信息/ ;/ (/ 2/ )/ oIFRM/ 的/ 冷启动/ 适应能力/ 更强/ ,/ 因为/ 在线/ 学习/ 可以/ 极大/ 地/ 压缩/ 冷启动/ 范围/ ,/ 有些/ 用户/ 或/ 产品/ 在/ DMF/ 看来/ 是/ 全新/ ,/ 但/ 在/ oIFRM/ 看来/ 并/ 不是/ ;/ (/ 3/ )/ oIFRM/ 在/ 冷启动/ 产品/ 场景/ 下/ 优势/ 尤其/ 明显/ ./ 在/ LastFM/ 数据/ 上/ AHR/ 由/ 0/ 提升/ 至/ 0.1192/ ;/ 在/ 豆瓣/ 数据/ 上/ AHR/ 大幅/ 提升/ 485/ %/ ;/ (/ 4/ )/ oIFRM/ 在/ 冷启动/ 用户/ 场景/ 下/ 优势/ 较/ 不/ 明显/ ./ 这/ 是因为/ 新/ 用户/ 反馈/ 的/ 累积/ 过程/ 较为/ 缓慢/ ,/ 限制/ 了/ 在线/ 模型/ 的/ 学习/ ;/ (/ 5/ )/ oIFRM/ 在/ LastFM/ 数据/ 上/ 比/ 在/ 豆瓣/ 数据/ 上/ 效果/ 提升/ 更为/ 显著/ ,/ 这/ 是因为/ LastFM/ 用户/ 反馈/ 频率/ 较/ 高/ ,/ 每位/ 用户/ 的/ 平均/ 反馈/ 数高达/ 465.83/ ./ 可见/ ,/ 在/ 频繁/ 交互/ 的/ 应用/ 场景/ 中/ ,/ oIFRM/ 的/ 优势/ 更/ 容易/ 体现/ ,/ 而/ 在/ 低频/ 交互/ 的/ 应用/ 场景/ 中/ ,/ oIFRM/ 应对/ 冷启动/ 问题/ 有/ 微弱/ 优势/ ./ 实验/ 5/ ./ 动态/ 调整/ 学习/ 步长/ 的/ 有效性/ ./ 根据/ 反馈/ 的/ 价值/ ,/ 动态/ 自动/ 地/ 调节/ 学习/ 步长/ 是/ oIFRM/ 的/ 主要/ 创新/ 点/ ./ 设计/ 实验/ 5/ 的/ 目的/ 在于/ 验证/ 动态/ 学习/ 步长/ 能/ 有效/ 地/ 提高/ 学习效果/ ./ 实验/ 在/ LastFM/ 数据/ 集上/ 进行/ ,/ 基于/ 一样/ 的/ oIFRM/ 模型/ (/ 包括/ 所有/ 参数设置/ 也/ 完全一致/ )/ ,/ 比较/ 采用/ 动态/ 学习/ 步长/ 的/ 更新/ 方式/ 和/ 采用/ 固定/ 学习/ 步长/ 的/ 更新/ 方式/ 的/ 推荐/ 质量/ ./ 实验/ 结果/ 如图/ 7/ 所示/ ,/ 其中/ ,/ IterX/ 表示/ 对/ 每个/ 反/ ORj/ / Item/ }/ ./ 这/ 4/ 个/ 测试/ 集中/ 的/ 样本/ 占/ 全部/ 测试/ 样本/ 的/ 比例/ 在/ 表/ 7/ 中/ “/ 数据/ 比例/ ”/ 一项/ 列出/ ./ 11464/ (/ AR/ )/ 0.14550/ ./ 39730.43150/ ./ 41000.12770/ ./ 09450.57470/ ./ 1590/ 图/ 7/ 动态/ 学习/ 步长/ 与/ 固定/ 学习/ 步长/ 效果/ 对比/ Page12/ 馈/ 都/ 固定/ 迭代/ 学习/ X/ 次/ ./ 观察/ 实验/ 结果/ 可知/ ,/ 使用/ 动态/ 的/ 学习/ 步长/ 能够/ 显著/ 地/ 提高/ 召回/ 率/ 、/ 准确率/ 以及/ 多样性/ ./ 在/ 采用/ 固定/ 学习/ 步长/ 的/ 对比/ 算法/ 中/ ,/ Iter30/ 效果/ 最差/ ,/ 过大/ 的/ 学习/ 步长/ 导致/ 了/ 过/ 训练/ ./ Iter5/ ,/ Iter10/ 以及/ Iter20/ 的/ 学习效果/ 差别/ 不/ 明显/ ,/ 都/ 略/ 好/ 于/ 对比/ 基线/ TP/ ./ 这/ 说明/ 如果/ 采用/ 固定/ 学习/ 步长/ 的/ 在线/ 更新/ 方式/ ,/ 将/ 迭代/ 次数/ 设置/ 为/ 10/ 左右/ 能/ 取得/ 较优/ 的/ 推荐/ 精度/ ./ 但/ 事实上/ ,/ 无论如何/ 设置/ 这一/ 参数/ ,/ 学习效果/ 总/ 存在/ 瓶颈/ ./ 这/ 是因为/ 固定/ 的/ 学习/ 步长/ 无法/ 满足/ 所有/ 反馈/ 的/ 学习/ 要求/ ./ 动态/ 的/ 学习/ 步长/ 帮助/ oIFRM/ 突破/ 了/ 这/ 一/ 瓶颈/ ,/ 在/ 各/ 评价/ 指标/ 上/ 都/ 取得/ 了/ 更/ 明显/ 的/ 改进/ ./ 与此同时/ ,/ 我们/ 记录/ 了/ 动态/ 学习/ 步长/ 的/ oIFRM/ 对/ 每个/ 反馈/ 的/ 训练/ 迭代/ 次数/ ,/ 计算/ 得出/ 均值/ 为/ 17.38/ ,/ 标准差/ 为/ 6.61/ ./ 这/ 说明/ 本文/ 提出/ 的/ 在线/ 学习/ 算法/ 是/ 有效/ 的/ ,/ 通过/ 差异化/ 训练/ 能够/ 提高/ 在线/ 模型/ 的/ 训练/ 效果/ ./ 6/ 总结/ 本文/ 研究/ 了/ 基于/ 隐式/ 用户/ 反馈/ 数据流/ 的/ 在线/ 推荐/ 模型/ ./ 首先/ ,/ 本文/ 提出/ 了/ 基本/ 的/ 隐式/ 反馈/ 推荐/ 框架/ IFRM/ ,/ 通过/ 最大化/ 用户/ 选择/ 行为/ 的/ 发生/ 概率/ 将/ 推荐/ 问题/ 概率/ 建模/ 为/ 优化/ 问题/ ./ 其次/ ,/ 本文/ 在/ IFRM/ 框架/ 下/ 给出/ 了/ 通用/ 的/ 应用/ 范例/ ,/ 展示/ 了/ 如何/ 灵活/ 地/ 整合/ 包括/ 用户/ 反馈/ 、/ 产品/ 特征/ 以及/ 流行/ 度/ 在内/ 的/ 多种/ 信息源/ ./ 再次/ ,/ 本文/ 进一步/ 地/ 提出/ 了/ 在线/ 的/ 隐式/ 反馈/ 推荐/ 模型/ oIFRM/ ,/ 采用/ 动态/ 的/ 学习/ 步长/ 在/ 强化/ 学习/ 用户/ 兴趣/ 变化/ 的/ 同时/ 降低/ 了/ 噪声/ 的/ 影响/ ./ 最后/ ,/ 本文/ 设计/ 了/ 在线/ 评价/ 机制/ 并/ 进行/ 了/ 丰富/ 实验/ ,/ 从/ 推荐/ 召回/ 率/ 、/ 准确率/ 、/ 多样性/ 、/ 可/ 解释性/ 、/ 吞吐量/ 、/ 健壮性/ 以及/ 冷启动/ 适应能力/ 等/ 多方面/ 验证/ 了/ 所/ 提供/ 模型/ 的/ 优越性/ ./ 未来/ ,/ 我们/ 将/ 尝试/ 对/ 社会/ 影响力/ 进行/ 更/ 细粒度/ 地/ 建模/ ,/ 并/ 将/ oIFRM/ 应用/ 于/ 社交/ 网络/ 推荐/ 场景/ ./ 在/ 实际/ 应用/ 中/ ,/ 产品/ 的/ 展现/ 位置/ 很/ 可能/ 会/ 影响/ 用户/ 的/ 选择/ 行为/ ,/ 因此/ 可以/ 结合/ 考虑/ 产品/ 展现/ 位置/ 并/ 通过/ 对/ 展现/ 概率/ 进行/ 估计/ 来/ 改善/ 推荐/ 质量/ ./ 致谢/ 本文/ 部分/ 工作/ 是/ 在/ 作者/ 访问/ 中国人民大学/ 的/ 萨师煊/ 大/ 数据/ 研究/ 中心/ 时/ 完成/ 的/ ./ 该/ 中心/ 获/ 国家/ 高等学校/ 学科/ 创新/ 引智/ 计划/ (/ 111/ 计划/ )/ 等/ 资助/ ./ 在/ 此/ ,/ 我们/ 向/ 对/ 本文/ 工作/ 给予/ 支持/ 和/ 建议/ 的/ 同行/ 表示感谢/ ./ 特别感谢/ 评审/ 老师/ 提出/ 的/ 宝贵意见/ !/ 

