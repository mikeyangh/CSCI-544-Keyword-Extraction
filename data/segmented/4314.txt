Page1/ 面向/ GPU/ 异构/ 并行/ 系统/ 的/ 多任务/ 流/ 编程/ 模型/ 董小社/ 1/ )/ 刘超/ 1/ )/ 王/ 恩东/ 2/ )/ 刘/ 袁/ 1/ )/ 张兴军/ 1/ )/ 1/ )/ (/ 西安交通大学/ 计算机科学/ 与/ 技术/ 系/ 西安/ 710049/ )/ 2/ )/ (/ 高效能/ 服务器/ 和/ 存储技术/ 国家/ 重点/ 实验室/ 济南/ 250013/ )/ 摘要/ 传统/ 并行/ 编程/ 模型/ 和/ 框架/ 不能/ 有效/ 利用/ 和/ 发挥/ GPU/ 异构/ 并行/ 系统/ 特点/ ,/ 应用/ 开发/ 难度/ 大/ ,/ 性能/ 优化/ 困难/ ,/ 文中/ 采用/ 混合/ 编程/ 模型/ 思想/ ,/ 建立/ 了/ 一种/ 以/ 协处理器/ 为/ 中心/ 的/ GPU/ 计算/ 核心/ 与/ CPU/ 控制/ 相/ 融合/ 的/ 多任务/ 流/ 编程/ 模型/ ./ 模型/ 将/ 并行任务/ 与/ CUDA/ 流/ 相结合/ ,/ 利用/ 系统/ 硬件/ 并行性/ 特点/ 实现/ 程序/ 任务/ 级/ 和/ 数据/ 级/ 并行/ ;/ 采用/ 任务/ 间/ 消息/ 通信/ 和/ 任务/ 内/ 数据共享/ 通信/ 方式/ ,/ 既保证/ 对/ 传统/ 并行/ 应用/ 的/ 继承/ 又/ 降低/ 了/ 不同/ 存储空间/ 给/ 应用/ 开发/ 带来/ 的/ 复杂性/ 和/ 难度/ ./ 基于/ 该/ 编程/ 模型/ 实现/ 了/ 一个/ 运行/ 时/ 支持系统/ 原型/ ,/ 测试/ 结果表明/ 可/ 保证/ 高效/ 的/ 数据通信/ ,/ 且/ 能/ 充分利用/ 系统/ 计算能力/ ,/ 提高/ 了/ 应用/ 程序运行/ 效率/ ./ 关键词/ GPU/ ;/ 异构/ 并行/ ;/ 编程/ 模型/ 1/ 引言/ 近年来/ ,/ GPU/ 凭借/ 其/ 强大/ 的/ 浮点/ 计算能力/ 在/ 高性能/ 计算/ 领域/ 得到/ 了/ 广泛应用/ ,/ 其/ 并行/ 数据处理/ 能力/ 已/ 远远/ 超过/ 传统/ CPU/ 的/ 计算能力/ ,/ 以/ “/ 通用/ 主/ 处理器/ +/ 加速/ 协处理器/ ”/ 为/ 特征/ 的/ 新型/ 异构/ 并行/ 结构/ 也/ 成为/ 高性能/ 计算机系统/ 发展/ 的/ 一个/ 新/ 方向/ ./ 2010/ 年/ 11/ 月/ 世界/ 超级计算机/ Top500/ 中/ 排名/ 第一/ 的/ “/ 天河/ 一号/ (/ TH/ -/ 1A/ )/ ”/ 超级计算机/ 便/ 采用/ 了/ 这种/ 异构/ 结构/ [/ 1/ ]/ ./ GPU/ 众核/ 处理器/ 为/ 高性能/ 计算机系统/ 的/ 发展/ 带来/ 了/ 契机/ ,/ 但/ 同时/ 在/ 软件/ 及/ 应用/ 程序开发/ 上/ 也/ 面临/ 着/ 更/ 多/ 的/ 挑战/ 和/ 困难/ [/ 2/ ]/ ./ 传统/ 并行/ 编程/ 模型/ 主要/ 基于/ 同构/ CPU/ 构成/ 的/ 并行/ 系统/ ,/ 不能/ 有效/ 利用/ 和/ 发挥/ 异构/ 并行/ 系统/ 中/ GPU/ 的/ 加速/ 计算能力/ ./ 由于/ GPU/ 结构/ 的/ 特殊性/ ,/ 使用/ GPU/ 运行/ 的/ 程序/ 需要/ 按照/ 专用/ 的/ 编程/ 模型/ 或/ 框架/ 进行/ 开发/ ,/ 如/ CUDA/ 和/ OpenCL/ 等/ ,/ 并且/ GPU/ 显存/ 与/ 系统/ 主存/ 间/ 显式/ 的/ 数据/ 交互/ 特点/ 要求/ 应用程序/ 开发者/ 对/ 数据/ 进行/ 合理/ 的/ 划分/ 和/ 管理/ ,/ 增加/ 了/ 新/ 系统/ 下/ 应用/ 开发/ 和/ 移植/ 的/ 难度/ ./ 为了/ 在/ 有效/ 利用/ GPU/ 异构/ 并行/ 系统/ 计算/ 性能/ 优势/ 的/ 同时/ ,/ 提高/ 应用/ 开发/ 移植/ 效率/ ,/ 本文/ 建立/ 了/ 一种/ 面向/ GPU/ 异构/ 并行/ 系统/ 的/ 多任务/ 流/ 编程/ 模型/ ./ 该/ 模型/ 采用/ 混合/ 编程/ 模型/ 思想/ ,/ 将/ 传统/ 的/ 消息传递/ 模型/ 与/ GPU/ 专用/ 编程/ 模型/ 相结合/ ,/ 通过/ GPU/ 对/ 应用/ 计算/ 核心/ 进行/ 加速/ ,/ 减少/ 程序运行/ 时间/ ./ 同时/ ,/ 提供/ 单个/ 任务/ 流内/ CPU/ -/ GPU/ 数据共享/ 机制/ ,/ 隐藏/ 数据传输/ ,/ 降低/ 应用/ 开发/ 难度/ ./ 本文/ 第/ 2/ 节/ 介绍/ 相关/ 领域/ 的/ 研究/ 工作/ ;/ 第/ 3/ 节/ 介绍/ 编程/ 模型/ 总体设计/ 结构/ ;/ 第/ 4/ 节/ 介绍/ 编程/ 模型/ 的/ 运行/ 时/ 支持系统/ ;/ 第/ 5/ 节对/ 所/ 实现/ 的/ 原型/ 系统/ 进行/ 测试/ 评价/ ;/ 最后/ 在/ 第/ 6/ 节/ 进行/ 总结/ ./ 2/ 相关/ 工作/ 随着/ GPU/ 在/ 通用/ 计算/ 领域/ 的/ 不断/ 发展/ ,/ 许多/ 研究/ 组织/ 和/ 个人/ 针对/ GPU/ 异构/ 并行/ 系统/ 的/ 编程/ 开发/ 与/ 性能/ 优化/ 进行/ 了/ 深入/ 的/ 研究/ ./ Leung/ 等/ 人/ [/ 3/ ]/ 通过/ 修改/ R/ -/ Stream/ 编译器/ ,/ 将/ 高级/ 语言/ 程序/ 自动/ 转化成/ CUDA/ 程序/ ./ 多伦多/ 大学/ 的/ 研究/ 人员/ 提出/ 了/ 一种/ 以/ 制导/ 语句/ 为/ 基础/ 的/ 编程语言/ hiCUDA/ [/ 4/ ]/ ,/ 通过/ 在/ 串行/ 程序/ 中/ 加入/ 制导/ 语句/ 形成/ hiCUDA/ 源程序/ ,/ 然后/ 通过/ 源到/ 源/ 编译器/ 将/ 源程序/ 转为/ CUDA/ 程序/ ./ Chen/ 等/ 人/ [/ 5/ ]/ 通过/ 对/ UP/ C语言/ 进行/ 扩展/ ,/ 将/ 并行执行/ 的/ 循环体/ 使用/ GPU/ 加速/ 执行/ ./ Harvey/ 等/ 人/ [/ 6/ ]/ 设计/ 开发/ 的/ Swan/ 和/ Martinez/ 等/ 人/ [/ 7/ ]/ 实现/ 的/ CU2CL/ 则/ 将/ CUDA/ 程序转换/ 为/ OpenCL/ 程序/ ,/ 从而/ 支持/ 不同/ 架构/ 的/ GPU/ 以及/ 多核/ CPU/ 或/ DSP/ 等/ 多种/ 加速/ 处理单元/ ./ 类似/ 的/ 还有/ OpenMPC/ [/ 8/ ]/ 、/ OpenMPtoGPGPU/ [/ 9/ ]/ 等/ ,/ 这些/ 研究/ 工作/ 都/ 通过/ 建立/ 新/ 的/ 编程语言/ 或是/ 扩展/ 已有/ 的/ 编程语言/ ,/ 使用/ 源到/ 源/ 编译/ 技术/ 对/ 源程序/ 进行/ 转换/ ,/ 自动/ 生成/ 可/ 运行/ 于/ GPU/ 的/ 程序/ ./ 采用/ 这种/ 方法/ ,/ 开发者/ 不必/ 掌握/ GPU/ 专用/ 编程语言/ ,/ 降低/ 了/ 程序开发/ 难度/ ,/ 但/ 自动/ 转换/ 方式/ 的/ 有效性/ 难以/ 保证/ ,/ 而且/ 通常/ 还/ 需要/ 使用者/ 对/ GPU/ 结构/ 有/ 一定/ 了解/ ,/ 从而/ 进行/ 制导/ 语句/ 的/ 标注/ ./ 对于/ 一些/ 复杂/ 应用/ ,/ 转换/ 所/ 生成/ 程序/ 的/ 性能/ 也/ 不是/ 很/ 高/ ./ 另一方面/ 研究/ 采用/ 混合/ 编程/ 模型/ 的/ 思想/ ,/ 将/ 已有/ 的/ 通用/ 编程/ 模型/ 与/ GPU/ 专用/ 编程/ 模型/ 相结合/ ,/ 进行/ 程序开发/ ./ Wang/ 等/ 人/ [/ 10/ ]/ 介绍/ 了/ UPC/ // CUDA/ 混合/ 编程/ 模型/ ,/ 并/ 对/ FT/ 和/ MG/ 算法/ 进行/ 了/ 实现/ 和/ 优化/ ./ FLAT/ [/ 11/ ]/ 编程/ 框架/ 将/ MPI/ 通信/ 操作/ 写/ 在/ GPU/ 运行/ 的/ kernel/ 程序/ 中/ ,/ 通过/ 运行/ 时/ 系统/ 将/ kernel/ 中/ 的/ 通信/ 操作/ 转化/ 为/ CPU/ 上/ 执行/ 的/ 操作/ 完成/ 消息/ 通信/ ./ Barak/ 等/ 人/ [/ 12/ ]/ 开发/ 的/ MGP/ (/ ManyGPUsPackage/ )/ 通过/ 在/ 上层/ 扩展/ OpenMP/ ,/ 在/ 下层/ 建立/ 支持/ OpenCL/ 的/ 运行/ 时/ 系统/ 实现/ 多/ CPU/ // GPU/ 环境/ 下/ 的/ 应用/ 程序开发/ ./ HybridOpenCL/ [/ 13/ ]/ 建立/ 了/ 一个/ 与/ MPI/ // OpenCL/ 类似/ 的/ 程序开发/ 和/ 运行/ 系统/ ,/ 方便/ OpenCL/ 应用程序/ 向/ 集群/ 环境/ 移植/ ./ cudaMPI/ [/ 14/ ]/ 将/ MPI/ 通信接口/ 与/ CUDA/ 数据传输/ 进行/ 封装/ ,/ 形成/ 统一/ 的/ 用户/ 编程/ 接口/ ,/ 方便/ 用户/ 进行/ 应用/ 开发/ ./ MVAPICH2/ -/ GPU/ [/ 15/ ]/ 将/ 涉及/ 显存/ 的/ 数据传输/ 集成/ 到/ MPI/ 发送/ 接收/ 操作/ 中/ ,/ 在/ 程序运行/ 时/ 自动检测/ 消息/ 数据/ 的/ 位置/ 并/ 进行/ 操作/ ,/ 简化/ 了/ 用户/ 编程/ ,/ 但/ 其/ 实现/ 依赖于/ CUDA/ 提供/ 的/ UVA/ (/ UnifiedVirtualAddress/ )/ 支持/ ,/ 且/ 运行/ 时/ 对/ 地址/ 指针/ 的/ 自动检测/ 过程/ 会/ 给/ 消息/ 传输/ 操作/ 带来/ 较/ 多/ 的/ 额外/ 开销/ ./ MPI/ -/ ACC/ [/ 16/ ]/ 则/ 通过/ 增加/ 新/ 的/ MPI/ _/ Data/ _/ Attributes/ 属性/ 类型/ 来/ 指示/ 消息/ 数据/ 的/ 位置/ ,/ 避免/ 了/ 自动检测/ 带来/ 的/ 开销/ ,/ 在/ 数据传输/ 中/ 使用/ 多/ 数据/ 块/ 和/ 流水线/ 传输/ 等/ 优化/ 手段/ 降低/ 延迟/ ,/ 并且/ 支持/ CUDA/ 和/ OpenCL/ 等/ 不同/ GPU/ 编程/ 模型/ ./ 但/ 这些/ 混合/ 编程/ 模型/ 中/ GPU/ 访问/ 的/ 数据/ 仍然/ 需要/ 用户/ 通过/ 显式/ 数据/ 拷贝/ 操作/ 来/ 完成/ ,/ 增加/ 了/ 开发/ 负担/ ,/ 且/ 容易/ 造成/ 冗余/ 的/ 数据/ 拷贝/ ./ MPI/ // CUDA/ 混合/ 编程/ 模型/ 也/ 是/ 目前/ GPU/ 异构/ 并行/ 系统/ 下/ 使用/ 最为/ 广泛/ 的/ 一种/ 编程/ 方法/ ./ 传统/ 的/ MPI/ // CUDA/ 混合/ 编程/ 模型/ 还/ 存在/ 一些/ 不足/ :/ (/ 1/ )/ 没有/ 考虑/ GPU/ 结构/ 的/ 发展/ 对/ 任务/ 并行性/ 的/ 支持/ ,/ 如/ NVIDIAFermi/ 架构/ GPU/ ./ 传统/ 的/ 混合/ Page3/ 编程/ 模型/ 中/ 多个/ 任务/ 占用/ 一个/ GPU/ 时/ 各自/ 拥有/ 独立/ 的/ GPU/ 上下文/ ,/ 抢占/ 使用/ GPU/ ,/ 切换/ 开销/ 大且/ 无法/ 利用/ 单个/ GPU/ 并行执行/ 计算/ 核心/ ;/ (/ 2/ )/ 数据通信/ 没有/ 考虑/ GPU/ 显存/ ,/ 涉及/ 显存/ 的/ 数据通信/ 需要/ 用户/ 显式/ 完成/ ,/ 增加/ 程序开发/ 难度/ 且/ 存在/ 冗余/ 的/ 数据传输/ 过程/ ./ 基于/ 以上/ 分析/ ,/ 本文/ 建立/ 的/ 多任务/ 流/ 编程/ 模型/ 采用/ 消息传递/ 与/ CUDA/ 相结合/ 的/ 混合/ 编程/ 方式/ ,/ 将/ 并行任务/ 以/ 线程/ 方式/ 实现/ 并/ 与/ GPU/ 运行/ 的/ CUDA/ 流/ 相结合/ 形成/ 任务/ 流/ ,/ 建立/ 单一/ GPU/ 上下文/ 降低/ 开销/ ,/ 并且/ 可以/ 利用/ Fermi/ 架构/ GPU/ 并发/ 执行/ kernel/ 的/ 特性/ ①/ ./ 同时/ 结合/ 显存/ 实现/ 任务/ 流间/ 消息传递/ 通信/ 和/ 任务/ 流内/ 数据共享/ 通信/ ,/ 提高/ 通信/ 效率/ ,/ 隐藏/ 主存/ // 显存/ 间/ 显式/ 传输/ ,/ 降低/ 编程/ 开发/ 难度/ ./ 3/ 编程/ 模型/ 总体设计/ 3.1/ 多任务/ 流/ 模型/ 结构/ GPU/ 异构/ 并行/ 系统结构/ 具有/ 以下/ 特点/ :/ (/ 1/ )/ 硬件/ 的/ 多级/ 并行性/ ./ 不同/ 处理器/ 之间/ 的/ 并行/ 以及/ 单个/ 处理器/ 内部/ 的/ 多个/ 处理单元/ 的/ 并行/ ;/ (/ 2/ )/ 共享/ 式/ 存储/ 与/ 分布式/ 存储/ 相结合/ ./ 每个/ GPU/ 拥有/ 可/ 直接/ 访问/ 的/ 独立/ 显存/ ,/ 同时/ GPU/ 与/ CPU/ 共享/ 系统/ 主存/ ;/ (/ 3/ )/ GPU/ 提供/ 主要/ 计算能力/ ./ 系统/ 通过/ GPU/ 加速/ 计算/ ,/ 降低/ 程序运行/ 时间/ ./ 针对/ GPU/ 异构/ 并行/ 系统结构/ 特点/ ,/ 本文/ 在/ 逻辑/ 抽象/ 基础/ 上以/ 协处理器/ 为/ 中心/ ,/ 采用/ 混合/ 编程/ 模型/ 思想/ ,/ 建立/ GPU/ 计算/ 核心/ 与/ CPU/ 控制/ 相/ 融合/ 的/ 多任务/ 流/ 编程/ 模型/ ,/ 该/ 模型/ 总体/ 结构/ 如图/ 1/ 所示/ ./ 由于/ GPU/ 没有/ 自主/ 控制能力/ ,/ 基于/ GPU/ 系统/ 的/ 程序/ 由/ CPU/ 运行/ 的/ 逻辑/ 控制/ 部分/ 和/ GPU/ 运行/ 的/ 计算/ 核心/ 部分/ 共同/ 构成/ ./ 因此/ ,/ 本文/ 将/ CPU/ 和/ GPU/ 协作/ 完成/ 且/ 按照/ 程序/ 书写/ 顺序/ 串行/ 执行/ 的/ 一段/ 程序/ 指令集/ 称为/ 一个/ 任务/ 流/ (/ Task/ -/ stream/ )/ ./ 在/ 多任务/ 流/ 模型/ 中/ ,/ 并行/ 应用程序/ 由/ 多个/ 任务/ 流/ 组成/ ,/ 每个/ 任务/ 流/ 包括/ CPU/ 运行/ 的/ 主控/ 程序/ 和/ GPU/ 运行/ 的/ 计算/ kernel/ ./ 主控/ 程序/ 按照/ 传统/ 的/ 消息传递/ 编程/ 模型/ 进行/ 开发/ ,/ 实现/ 处理器/ 间/ 的/ 任务/ 级/ 并行/ ,/ 计算/ kernel/ 按照/ CUDA/ 编程/ 模型/ 进行/ 开发/ ,/ 实现/ GPU/ 内/ 众多/ 处理单元/ 并行计算/ 的/ 数据/ 级/ 并行/ ./ 整个/ 应用程序/ 表现/ 出/ 任务/ 级/ 与/ 数据/ 级/ 两层/ 并行/ 特征/ ,/ 充分利用/ 和/ 发挥/ 硬件/ 多级/ 并行性/ 的/ 优势/ ./ 每个/ 任务/ 流控制/ 使用/ 的/ GPU/ 采用/ 逻辑/ GPU/ 的/ 概念/ ,/ 多个/ 逻辑/ GPU/ 在/ 物理/ 上/ 可以/ 是/ 同一个/ ,/ 如图/ 1/ 中/ Task/ -/ stream0/ 、/ Task/ -/ stream1/ 和/ Task/ -/ stream2/ 都/ 使用/ GPU0/ ./ 3.2/ 数据通信/ 方式/ 多任务/ 流/ 模型/ 中/ 数据通信/ 包括/ 两/ 方面/ :/ 并行任务/ 间通信/ 和/ 单个/ 任务/ 内/ 主控/ 程序/ 与/ 计算/ kernel/ 间通信/ ./ 由于/ GPU/ 异构/ 并行/ 系统/ 具有/ 分布式/ 存储/ 与/ 共享/ 存储/ 相结合/ 的/ 特点/ ,/ 因此/ 数据通信/ 采用/ 消息传递/ 和/ 数据共享/ 相结合/ 的/ 通信/ 方式/ ,/ 如图/ 2/ 所示/ ./ 并行任务/ 间/ 使用/ 消息传递/ 通信/ 方式/ ,/ 并且/ 与/ 传统/ 的/ 消息/ 通信接口/ MPI/ 保持一致/ ,/ 方便/ 用户/ 掌握/ 和/ 使用/ ,/ 也/ 保证/ 了/ 所/ 开发/ 应用/ 具有/ 良好/ 的/ 可扩展性/ 以及/ 对/ 已有/ 并行/ 应用/ 的/ 继承/ ./ 单个/ 任务/ 流内/ 提供/ 数据共享/ 的/ 通信/ 机制/ ,/ 使得/ 主控/ 程序/ 与/ 计算/ kernel/ 可以/ 通过/ 对/ 共享/ 数据/ 的/ 访问/ 隐藏/ 主存/ // 显存/ 间/ 显式/ 的/ 数据传输/ ,/ 降低/ 不同/ 存储/ 给/ 编程/ 开发/ 带来/ 的/ 复杂性/ ,/ 减轻/ 应用/ 开发/ 负担/ ./ 4/ 编程/ 模型/ 运行/ 时/ 系统/ 4.1/ 运行/ 时/ 系统软件/ 结构/ 本文/ 基于/ NVIDIAGPU/ 平台/ 设计/ 实现/ 了/ 多/ ①/ FermiArchitectureWhitepaper/ ./ http/ :/ // // www/ ./ nvidia/ ./ com/ // Page4/ 任务/ 流/ 编程/ 模型/ 运行/ 时/ 支持系统/ GMMP/ (/ GPUMemorycombinedMessagePassing/ )/ ./ 该/ 系统/ 将/ MPI/ 作为/ 下层/ 多/ 节点/ 并行/ 和/ 通信/ 环境/ ,/ 节点/ 内/ 并行/ 通过/ Pthread/ 以/ 线程/ 方式/ 实现/ ,/ 再/ 结合/ GPU/ 运行/ 所/ 需/ 的/ CUDA/ 软件/ 环境/ ,/ 其/ 软件结构/ 如图/ 3/ 所示/ ./ GMMP/ 系统/ 主要/ 包括/ 任务/ 流/ 管理/ 和/ 任务/ 流通/ 应用程序/ 通过/ GMMP/ 在/ 每个/ 计算/ 节点/ 上/ 创建/ 一个/ MPI/ 进程/ —/ —/ —/ mpi/ _/ process/ ,/ 在/ 每个/ MPI/ 进程/ 中/ 再/ 创建/ 若干/ 线程/ —/ —/ —/ Pthread/ ./ 线程/ 执行/ 任务/ 流/ 的/ 主控/ 程序/ ,/ 控制/ 使用/ 一个/ GPU/ ,/ 并/ 与/ 特定/ 的/ CUDA/ 流/ 关联/ ,/ 任务/ 流/ 的/ 计算/ kernel/ 将/ 属于/ 所/ 关联/ 的/ CUDA/ 流/ ./ 一个/ 计算/ 节点/ 上/ 的/ 所有/ 任务/ 流/ 属于/ 同一个/ MPI/ 进程/ ,/ 称为/ 任务/ 流集/ —/ —/ —/ task/ -/ stream/ -/ set/ ./ 因此/ ,/ GMMP/ 中/ Pthread/ 与/ Task/ -/ stream/ 对应/ ,/ mpi/ _/ process/ 与/ task/ -/ stream/ -/ set/ 对应/ ./ 信/ ,/ 任务/ 流通/ 信/ 又/ 包括/ 消息/ 通信/ 和/ 数据共享/ ./ 4.2/ 并行任务/ 流/ 管理/ 并行任务/ 管理/ 最/ 基本/ 的/ 工作/ 包括/ 3/ 个/ 方面/ :/ 并行任务/ 的/ 创建/ 、/ 结束/ 和/ 同步/ ./ GMMP/ 多任务/ 流/ 采用/ 了/ 进程/ 、/ 线程/ 、/ 流/ 相结合/ 的/ 方式/ ,/ 因此/ 并行任务/ 管理/ 也/ 呈现出/ mpi/ _/ process/ 、/ Pthread/ 及/ cuda/ _/ stream/ 相结合/ 的/ 多层/ 特点/ ./ GMMP/ 并行任务/ 流/ 管理/ 总体/ 实现/ 方式/ 如图/ 4/ 所示/ ,/ 通过/ MPI/ 、/ Pthread/ 及/ CUDA/ 等/ 下层/ 软件/ 环境/ 提供/ 的/ 接口/ 分别/ 完成/ 进程/ 、/ 线程/ 以及/ 流/ 的/ 创建/ 、/ 结束/ 和/ 同步操作/ ./ 多线程/ 同步/ 没有/ 直接/ 的/ 同步/ 调用/ ,/ 本文/ 通过/ 线程/ 条件/ 变量/ 进行/ 实现/ ./ 线程/ 与/ CUDA/ 流/ 的/ 关联/ 则/ 是/ 通过/ 线程/ 私有/ 变量/ 来/ 完成/ ./ GMMP/ 中/ 每个/ 任务/ 流/ 拥有/ 一个/ ID/ ,/ 称为/ gmmp/ _/ rank/ ,/ gmmp/ _/ rank/ 由/ 任务/ 流/ 所属/ MPI/ 进程/ 的/ mpi/ _/ rank/ 和/ 该/ 任务/ 流在/ 进程/ 中/ 的/ 序号/ local/ _/ thread/ _/ rank/ 决定/ ./ 假设/ 一个/ MPI/ 进程/ 中有/ N/ 个/ 线程/ ,/ 即/ N/ 个/ 任务/ 流/ ,/ 则/ gmmp/ _/ rank/ =/ mpi/ _/ rank/ ×/ N/ +/ local/ _/ thread/ _/ rank/ ./ 4.3/ 消息/ 通信/ GMMP/ 消息/ 通信/ 包括/ 节点/ 间/ (/ inter/ -/ node/ )/ 通信/ 和/ 节点/ 内/ (/ intra/ -/ node/ )/ 通信/ ./ 节点/ 间通信/ 发生/ 在/ 不同/ 的/ MPI/ 进程/ 间/ ,/ 借助/ 下层/ MPI/ 消息/ 通信/ 操作/ 完成/ ./ 节点/ 内/ 消息/ 通信/ ,/ 本文/ 通过/ 在/ 系统/ 主存/ 中/ 建立/ 消息/ 缓冲区/ 完成/ ./ GMMP/ 消息/ 通信/ 实现/ 如图/ 5/ 所示/ ./ MPI/ 环境/ 提供/ 了/ 统一/ 、/ 高效/ 的/ 消息传递/ 接口/ ,/ 节点/ 间通信/ 使用/ MPI/ 作为/ 下层/ 通信/ 环境/ ,/ 不但/ 降低/ 了/ GMMP/ 实现/ 的/ 难度/ 且/ 能/ 有效/ 利用/ 软件资源/ 保证/ 通信/ 的/ 高效/ 完成/ ./ 节点/ 内/ 通信/ ,/ 发送/ 方/ 和/ 接收/ 方/ 通过/ 共享/ 的/ 消息/ 缓冲/ 来/ 完成/ 消息传递/ 操作/ ,/ 避免/ 了/ 数据/ 在/ 不同/ 缓冲/ 间/ 的/ 冗余/ 传输/ ,/ 降低/ 了/ 通信/ 延迟/ ;/ GMMP/ 使用/ 页/ 锁定/ 内存/ (/ pinned/ -/ memory/ )/ 作为/ 消息/ 缓冲区/ ,/ 进一步/ 降低/ 主存/ // 显存/ 间/ 的/ 通信/ 延迟/ ./ 随着/ GPU/ 软硬件/ 技术/ 的/ 发展/ ,/ 系统/ 可/ 针对/ 多个/ GPU/ 提供/ P2P/ 式/ 传输/ ,/ 即/ 不同/ 的/ GPU/ 间/ 通过/ 系/ Page5/ 统/ 总线/ 或是/ 互连/ 网络/ 直接/ 进行/ 数据传输/ ,/ 如图/ 4/ 中/ 虚线/ 箭头/ 所示/ ,/ 不再/ 通过/ 主存/ 间接/ 完成/ ,/ GPU/ 间/ 的/ 通信/ 速度/ 将/ 获得/ 更大/ 提高/ ./ 4.4/ 数据共享/ 根据/ CPU/ 和/ GPU/ 分别/ 只能/ 直接/ 访问/ 主存/ 和/ 显存/ 的/ 特点/ ,/ 本文/ 采用/ 主存/ -/ 显存/ 存储/ 映射/ 方法/ 实现/ 任务/ 流内/ CPU/ 和/ GPU/ 对/ 数据/ 的/ 共享/ 访问/ ,/ 如图/ 6/ 所示/ ./ 共享/ 空间/ 由/ 主存/ // 显存/ 间/ 相互/ 映射/ 的/ 一对/ 空间/ 构成/ ,/ 系统/ 维持/ 映射/ 空间数据/ 的/ 一致/ ,/ 保证/ CPU/ 与/ GPU/ 分别/ 访问/ 相应/ 的/ 数据/ 时/ 程序/ 行为/ 的/ 正确性/ ./ GMMP/ 针对/ 共享/ 空间/ 建立/ 了/ 映射/ 表/ ,/ 记录/ 映射/ 空间/ 的/ 实际/ 地址/ 及/ 状态/ 信息/ ./ 共享/ 空间数据/ 的/ 一致性/ 维护/ 是/ GMMP/ 要/ 解决/ 的/ 一个/ 关键问题/ ,/ 在/ 主存/ // 显存/ 映射/ 方式/ 中/ ,/ 共享/ 数据/ 存在/ 如下/ 几种/ 状态/ :/ (/ 1/ )/ IN/ _/ HOST/ // // 最新/ 数据/ 在/ 主存/ 空间/ 中/ (/ 2/ )/ IN/ _/ DEVICE/ // // 最新/ 数据/ 在/ 显存/ 空间/ 中/ (/ 3/ )/ IN/ _/ ALL/ // // 主存/ 、/ 显存/ 中/ 数据/ 一致/ 本文/ 建立/ 了/ 3/ 种/ 映射/ 空间/ 的/ 数据/ 更新/ 策略/ ./ 4.4/ ./ 1/ 简单/ 一致性/ 更新/ 基于/ 程序/ 中/ 计算/ kernel/ 的/ 执行/ ,/ 应用程序/ 在/ 执行/ 计算/ kernel/ 前/ 进行/ p/ _/ host/ →/ p/ _/ device/ 的/ 数据/ 更新/ ,/ 计算/ kernel/ 完成/ 后/ 进行/ p/ _/ device/ →/ p/ _/ host/ 的/ 数据/ 更新/ ,/ 从而/ 保证/ CPU/ 或/ GPU/ 在/ 访问共享/ 空间/ 时/ 访问/ 到/ 最新/ 数据/ ,/ 其/ 状态/ 转换/ 如图/ 7/ 所示/ ./ 这种/ 更新/ 策略/ 操作/ 简单/ ,/ 不/ 需要/ 额外/ 信息/ ,/ 但/ 由于/ 无论/ CPU/ 或/ GPU/ 是否/ 对/ 共享/ 数据/ 进行/ 了/ 修改/ 都/ 要/ 进行/ 数据/ 更新/ ,/ 造成/ 重复/ 的/ 更新/ 操作/ ,/ 效率/ 不高/ ./ 因此/ ,/ 按照/ 计算/ kernel/ 对/ 共享/ 空间/ 读写/ 属性/ ,/ 对/ 简单/ 一致性/ 更新/ 策略/ 进行/ 改进/ :/ (/ 1/ )/ kernel/ 执行/ 前/ 只/ 对/ 读取/ 的/ 共享/ 空间/ 更新/ ;/ (/ 2/ )/ kernel/ 结束/ 后/ 只/ 对/ 写/ 过/ 的/ 共享/ 空间/ 更新/ ./ 这样/ 可以/ 减少/ 数据/ 重复/ 更新/ 的/ 次数/ ,/ 降低/ 更新/ 开销/ ./ 实现/ 时/ 用户/ 可/ 使用/ 制导/ 语句/ 标明/ 计算/ kernel/ 对/ 共享/ 数据/ 的/ 读写/ 属性/ 或/ 通过/ 编译/ 自动识别/ 读写/ 属性/ ./ 4.4/ ./ 2/ 读写/ 一致性/ 更新/ 基于/ 程序/ 对/ 共享/ 空间/ 的/ 读写访问/ 属性/ ,/ 按照/ 不同/ 的/ 读写操作/ 进行/ 状态/ 转换/ 和/ 数据/ 更新/ ./ 应用程序/ 对/ 共享/ 空间/ 的/ 访问/ 包括/ CPU/ 读/ // 写及/ GPU/ 读/ // 写/ ,/ 4/ 种/ 访问/ 操作/ 下/ 的/ 状态/ 转换/ 如图/ 8/ 所示/ ./ 读写/ 一致性/ 更新/ 中/ ,/ 共享/ 空间/ 存在/ 3/ 种/ 状态/ ./ 当/ 访问/ 操作/ 不/ 改变/ 共享/ 空间/ 的/ 状态/ 时/ ,/ 不/ 引发/ 数据/ 更新/ 操作/ ;/ 写/ 操作/ 可能/ 改变/ 状态/ ,/ 但/ 不/ 引发/ 数据/ 更新/ ;/ 读/ 操作/ 在/ 改变/ 状态/ 时/ 引发/ 数据/ 更新/ ./ 这种/ 读/ 访问/ 更新/ 数据/ 的/ 方式/ 保证/ 程序/ 访问/ 最近/ 最新/ 的/ 数据/ ./ 该/ 策略/ 区分/ 程序/ 对/ 共享/ 空间/ 的/ 读写操作/ ,/ 避免/ 了/ 重复/ 的/ 数据/ 更新/ ,/ 但/ 实现/ 复杂/ ,/ 需要/ 提供/ 较/ 多/ 的/ 读写访问/ 信息/ 辅助/ 完成/ ./ 4.4/ ./ 3/ 异步/ 一致性/ 更新/ CUDA/ 环境/ 提供/ 了/ 主存/ -/ 显存/ 间/ 的/ 异步/ 传输/ 功能/ ,/ 采用/ 异步/ 传输/ 可以/ 重叠/ 数据通信/ 和/ 其它/ 程序/ 操作/ ,/ 隐藏/ 主存/ -/ 显存/ 间/ 的/ 传输/ 延迟/ ./ 异步/ 更新/ 策略/ 在/ 读写/ 一致性/ 更新/ 策略/ 的/ 基础/ 上/ 引入/ 异步/ 传输/ 特性/ ,/ 如图/ 9/ 所示/ ./ Page6/ 与/ 读写/ 一致性/ 更新/ 策略/ 中读/ 操作/ 才/ 引发/ 数据/ 更新/ 不同/ ,/ 此时/ 写/ 操作/ 引发/ 异步/ 数据/ 更新/ ,/ 而/ 相应/ 的/ 读/ 操作/ 则/ 检查和/ 等待/ 异步/ 传输/ 完成/ ./ 两个/ 操作/ 之间/ 程序/ 可以/ 执行/ 其他/ 指令/ ,/ 隐藏/ 数据通信/ 延时/ ./ 5/ 验证/ 与/ 测试/ 5.1/ 实验/ 环境/ 测试环境/ 包括/ 5/ 个/ 计算/ 节点/ ,/ 通过/ Infiniband/ 连接/ ,/ 每个/ 节点/ 有/ 2/ 个/ 4/ 核/ IntelXeon5660CPU/ 、/ 4GB/ 内存/ 和/ 2/ 个/ NVIDIATeslaC2050GPU/ ./ 操作系统/ 为/ RedHatLinux5/ 企业/ 版/ ,/ 安装/ CUDAToolkit/ 、/ CUDASDK4/ ./ 0/ 和/ openMPI1/ ./ 3.3/ ./ 5.2/ 通信/ 测试/ 使用/ ping/ -/ pong/ 测试程序/ 对/ 节点/ 内/ 和/ 节点/ 间/ 点到点/ 通信/ 延时/ 进行/ 测试/ ,/ 并/ 与/ 使用/ MPI/ // CUDA/ 混合/ 编程/ 模型/ 方式/ 的/ 测试/ 结果/ 进行/ 比较/ ./ 图/ 10/ (/ a/ )/ 和/ (/ b/ )/ 分别/ 显示/ 了/ 小/ 消息/ 和/ 大/ 消息/ 的/ 节点/ 内/ 通信/ 延时/ ,/ pinned/ 表示/ 在/ GMMP/ 中/ 使用/ 页/ 锁定/ 内存/ 作为/ 消息/ 缓冲/ ./ GMMP/ 小/ 消息/ 通信/ 延时/ 要/ 稍/ 大于/ MPI/ // CUDA/ 方式/ ,/ 这/ 主要/ 是/ 消息/ 缓冲/ 的/ 加锁/ // 解锁/ 操作/ 带来/ 的/ 开销/ ./ 随着/ 通信/ 数据量/ 的/ 增大/ ,/ GMMP/ 减少/ 了/ 数据传输/ 过程/ ,/ 额外/ 开销/ 影响/ 减小/ ,/ 延迟/ 好于/ MPI/ // CUDA/ 方式/ ./ 图/ 11/ 是/ 节点/ 间通信/ 的/ 延迟/ ,/ 由于/ GMMP/ 节点/ 间通信/ 借助/ 下层/ 的/ MPI/ 接口/ 实现/ ,/ 通信/ 过程/ 与/ MPI/ // CUDA/ 方式/ 相同/ ,/ 两者/ 通信/ 延迟/ 也/ 基本相同/ ./ 图/ 12/ 是/ 5/ 个/ 节点/ ,/ 10/ 个/ GPU/ 的/ 集合/ 通信/ 操作/ -/ 广播/ 通信/ 延时/ ,/ GMMP/ 广播/ 延时/ 与/ MPI/ // CUDA/ 基本相同/ ,/ 保持/ 了/ 较/ 高/ 的/ 通信/ 性能/ ./ 5.3/ 多任务/ 流/ 应用/ 测试/ 5.3/ ./ 1/ 单/ GPU/ 多任务/ 测试/ Fermi/ 架构/ GPU/ 支持/ 相同/ GPU/ 上下文/ 的/ 多个/ 计算/ kernel/ 并发/ 执行/ ,/ 这里/ 参照/ 和/ 修改/ CUDASDK/ 中/ concurrent/ _/ kernel/ 测试程序/ ,/ 分别/ 在/ 前代/ GPU/ 产品/ TeslaC1060/ 和/ Fermi/ 架构/ GPU/ 产品/ TeslaC2050/ 上/ 运行/ ,/ 多个/ 并行任务/ 占用/ 一个/ GPU/ ,/ 运行/ 时间/ 如图/ 13/ 所示/ ./ C1060GPU/ 不/ 支持/ 并行/ kernel/ 执行/ ,/ 随着/ 任务/ 数/ 增加/ ,/ 多个/ 任务/ 的/ 计算/ kernel/ 在/ 一个/ GPU/ 上/ 串行/ 执行/ ,/ 运行/ 总/ 时间/ 也/ 逐渐/ 增多/ ./ 在/ C2050/ 上/ MPI/ // CUDA/ 是/ 多/ GPU/ 上下文/ ,/ 多个/ kernel/ 仍然/ 串行/ 执行/ ,/ 而/ GMMP/ 中/ 不同/ 任务/ 流/ 的/ kernel/ 属于/ 同一个/ GPU/ 上下文/ 的/ 不同/ CUDA/ 流/ ,/ 可以/ 并行执行/ ,/ 因此/ 相比/ MPI/ // CUDA/ 减少/ 了/ 运行/ 时间/ ./ Page7/ 图/ 13/ 单/ GPU/ 多任务/ concurrent/ _/ kernel/ 程序运行/ 时间/ 5.3/ ./ 2/ 更新/ 策略/ 测试/ 使用/ 分块/ 矩阵/ 相乘/ 程序/ (/ BlockMatrixMultiply/ ,/ BMM/ )/ 对/ 不同/ 一致性/ 更新/ 策略/ 进行/ 测试/ ,/ 矩阵/ 规模/ 为/ 8192/ ×/ 8192/ 单精度/ 浮点数/ ,/ 运行/ 于/ 多个/ 节点/ ,/ 每个/ 任务/ 单独/ 使用/ 一个/ GPU/ ,/ 程序运行/ 时间/ 如图/ 14/ 所示/ ./ 简单/ 一致性/ 更新/ 存在/ 较/ 多/ 的/ 重复/ 数据/ 更新过程/ ,/ 程序运行/ 效率/ 最差/ ;/ 改进/ 后/ 的/ 简单/ 一致性/ 更新/ 减少/ 了/ 重复/ 更新/ 次数/ ,/ 运行/ 时间/ 有/ 了/ 较大/ 降低/ ./ 读写/ 一致性/ 更新/ 避免/ 了/ 重复/ 更新/ ,/ 程序/ 性能/ 有/ 了/ 进一步提高/ ,/ 与/ MPI/ // CUDA/ 方式/ 基本相同/ ;/ 异步/ 一致性/ 更新/ 通过/ 异步/ 传输/ 隐藏/ 部分/ 通信/ 延时/ ,/ 性能/ 较/ 读写/ 一致性/ 更新/ 有/ 改善/ ./ 5.3/ ./ 3N/ -/ body/ 应用/ 测试/ N/ -/ body/ 问题/ 描述/ 了/ 具有/ 相互/ 引力/ 作用/ 的/ N/ 个/ 粒子/ 运行/ 轨迹/ 的/ 计算/ 过程/ ,/ 广泛应用/ 于/ 物理学/ 、/ 静电学/ 和/ 旋涡/ 流体/ 动力学/ 等/ 领域/ 中/ ./ N/ -/ body/ 问题/ 最/ 直接/ 的/ 算法/ 是/ PP/ (/ particle/ -/ particle/ )/ 算法/ ./ 本文/ 采用/ PP/ 算法/ ,/ 并且/ 使用/ 读写/ 一致性/ 更新/ 方式/ 对/ N/ =/ 65536/ ,/ 迭代/ 500/ 步/ 的/ N/ -/ body/ 问题/ 进行/ 测试/ ,/ 运行/ 时间/ 如表/ 1/ 所示/ ./ 并行任务/ 数表/ 1/ 中/ ,/ mpi/ // cuda/ 和/ gmmp/ 均/ 使用/ GPU/ 加速/ 计算/ ,/ 最/ 多/ 创建/ 10/ 个/ 并行任务/ 分别/ 占用/ 5/ 个/ 节点/ 的/ 10/ 个/ GPU/ ./ 随着/ 任务/ 数/ 增多/ ,/ 程序运行/ 时间/ 下降/ ,/ 但/ 并行/ 加速/ 比/ 减少/ ,/ 这/ 主要/ 是/ 由于/ 每个/ 任务/ 负责/ 的/ 计算/ 量/ 已经/ 很小/ ,/ GPU/ 控制/ 与/ 通信/ 等/ 开销/ 的/ 相对/ 比例/ 增加/ ./ 因此/ 在/ 使用/ GPU/ 异构/ 并行/ 系统/ 运行/ 程序/ 时/ ,/ 并行任务/ 的/ 数量/ 要/ 使得/ 每个/ GPU/ 分配/ 有/ 足够/ 的/ 计算/ 量/ ,/ 从而/ 保证/ 对/ GPU/ 计算资源/ 的/ 充分利用/ ./ 我们/ 也/ 对/ 只/ 使用/ CPU/ 运行/ 的/ mpi/ 程序/ 进行/ 了/ 测试/ ,/ 使用/ 5/ 个/ 节点/ 40/ 个/ CPU/ 核/ 运行/ 40/ 个/ 并行任务/ 的/ 运行/ 时间/ 为/ 3112.17/ s/ ./ gmmp/ 使用/ 10/ 个/ GPU/ 的/ 运行/ 时间/ 相比/ 于/ MPI/ 使用/ 40/ 个/ CPU/ 核/ 加速/ 比/ 提升/ 达到/ 近/ 70/ 倍/ ./ 6/ 结论/ 本文/ 针对/ GPU/ 异构/ 并行/ 系统/ ,/ 建立/ 了/ 一种/ 多任务/ 流/ 编程/ 模型/ ,/ 设计/ 实现/ 了/ 编程/ 模型/ 运行/ 时/ 支持系统/ 原型/ GMMP/ ./ 该/ 模型/ 采用/ 消息传递/ +/ CUDA/ 的/ 混合/ 编程/ 模型/ 思想/ ,/ 利用/ 多级/ 硬件/ 并行性/ 特点/ 实现/ 应用程序/ 的/ 任务/ 级/ 和/ 数据/ 级/ 并行/ ./ 模型/ 采用/ 任务/ 间/ 消息传递/ 与/ 任务/ 内/ 数据共享/ 的/ 通信/ 方式/ ,/ 既保证/ 了/ 并行/ 应用/ 的/ 高效/ 实现/ ,/ 方便/ 应用/ 开发/ 移植/ ,/ 又/ 降低/ 了/ 程序开发/ 负担/ ./ 对/ 运行/ 时/ 支持系统/ GMMP/ 的/ 测试表明/ ,/ GMMP/ 保持/ 了/ 较/ 好/ 的/ 通信/ 性能/ ,/ 可以/ 更好/ 利用/ GPU/ 新/ 特性/ ,/ 充分/ 使用/ GPU/ 提供/ 的/ 计算能力/ Page8/ 对/ 程序/ 计算/ 核心/ 进行/ 加速/ ,/ 提高/ 应用/ 整体/ 运行/ 效率/ 和/ 性能/ ./ 

