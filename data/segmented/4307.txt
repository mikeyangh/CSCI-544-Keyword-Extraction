Page1/ 分布式/ key/ -/ value/ 系统/ 错误/ 污染/ 检测/ 郭晓林/ 1/ )/ 舒/ 继武/ 2/ )/ 易/ 乐天/ 1/ )/ 1/ )/ (/ 清华大学/ 计算机科学/ 与/ 技术/ 系/ 北京/ 100084/ )/ 2/ )/ (/ 清华大学/ 信息科学/ 与/ 技术/ 国家/ 实验室/ 北京/ 100084/ )/ 摘要/ 随着/ key/ -/ value/ 存储系统/ 的/ 广泛/ 使用/ ,/ 越来越/ 多/ 的/ 研究/ 开始/ 关注/ 分布式系统/ 中/ 的/ 可信/ 问题/ ,/ 其中/ 一个/ 重要/ 的/ 问题/ 是/ ,/ 如何/ 在/ 系统/ 被/ 入侵/ 或者/ 管理员/ 配置/ 错误/ 并/ 运行/ 一段时间/ 后/ ,/ 检测/ 出受/ 污染/ 的/ 数据/ ,/ 从而/ 可以/ 在/ 恢复/ 错误/ 数据/ 的/ 同时/ 保留/ 系统/ 的/ 合法/ 更新/ ./ 文中/ 提出/ 了/ 一种/ 基于/ key/ -/ value/ 存储系统/ 的/ 错误/ 污染/ 检测/ 方法/ ,/ 该/ 方法/ 允许/ 在/ 客户端/ 不可/ 信/ 的/ 前提/ 下/ ,/ 检测/ 客户端/ 之间/ 的/ 污染/ 扩散/ ./ 文中/ 设计/ 了/ 一种/ 基于/ 各/ 服务器/ 逻辑/ 时钟/ 的/ 向量/ 时钟/ ,/ 该/ 时钟/ 以/ 用户/ 关联/ 操作/ 为/ 更新/ 规则/ ,/ 记录/ 了/ 跨/ 服务器/ 的/ 读写/ 请求/ 逻辑关系/ ,/ 用以/ 进行/ 错误/ 污染/ 跟踪/ ;/ 同时/ 为了/ 减少/ 大规模/ 系统/ 中/ 污染/ 检测/ 的/ 开销/ ,/ 基于/ 该/ 向量/ 时钟/ ,/ 文中/ 进一步/ 提出/ 了/ 一种/ 在/ 分布式系统/ 中/ 由/ 操作/ 序列/ 构成/ 有/ 向/ 无/ 环图/ 的/ 污染/ 分析方法/ ./ 基于/ 广泛/ 使用/ 的/ Voldmortkey/ -/ value/ 系统/ ,/ 文中/ 实现/ 了/ 一个/ 错误/ 污染/ 检测/ 系统/ ,/ TrackerStore/ ./ 在/ 集群/ 测试环境/ 下/ ,/ 文中/ 对/ 引入/ 新/ 的/ 检测/ 系统/ 后/ 产生/ 的/ 额外/ 延迟/ 开销/ 进行/ 了/ 测试/ ./ 关键词/ 污染/ 检测/ ;/ 分布式/ key/ -/ value/ 系统/ ;/ 向量/ 时钟/ 1/ 引言/ 互联网/ 新一轮/ 的/ 爆炸性/ 增长/ ,/ 给/ 数据/ 存储/ 带来/ 了/ 巨大/ 的/ 挑战/ ./ 据/ Facebook/ 统计/ ,/ 目前/ 其/ 服务器/ 共/ 存储/ 了/ 超过/ 2600/ 亿张/ 图片/ [/ 1/ ]/ ,/ 并且/ 图片/ 的/ 数量/ 正在/ 以/ 每周/ 新增/ 10/ 亿张/ (/ 约合/ 60TB/ )/ 的/ 速度/ 在/ 增长/ ./ Twitter/ 则/ 在/ 2010/ 年/ 8/ 月份/ 便/ 已经/ 有/ 超过/ 2000/ 亿/ Page2/ 条/ 用户/ 记录/ ./ 数据/ 增长/ 的/ 速度/ 是/ 如此/ 之快/ ,/ 以至于/ 传统/ 的/ 存储/ 方法/ 已经/ 无法/ 满足/ 当前/ 需求/ 对于/ 存储/ 节点/ 可扩展性/ 的/ 要求/ ./ 为了/ 获得/ 更/ 高/ 的/ 可扩展性/ ,/ 许多/ 大型/ 的/ 互联网/ 服务提供商/ 使用/ NoSQL/ 数据库/ 来/ 作为/ 后/ 端/ 数据/ 存储/ 与/ 查询/ 系统/ ./ NoSQL/ 系统/ 通常/ 不/ 支持/ 传统/ RDBMS/ (/ RelationalDataBaseManagementSystem/ )/ 的/ 连接/ 操作/ ,/ 提供/ 弱/ 一致性/ (/ 如/ 最终/ 一致性/ )/ ,/ 并/ 采用/ key/ -/ value/ 存储/ 的/ 形式/ ,/ 如/ 基于/ 内存/ 的/ 分布式/ key/ -/ value/ 缓存/ (/ 如/ Memcached/ 、/ Velocity/ )/ ,/ 提供/ 最终/ 一致性/ 的/ 分布式/ key/ -/ value/ 存储系统/ (/ 如/ Amazon/ 的/ Dynamo/ [/ 2/ ]/ 、/ Cassandra/ [/ 3/ ]/ 、/ Voldemort/ ①/ )/ ./ 由于/ 这/ 类/ 系统/ 中/ 所/ 存储/ 的/ 用户/ 数据/ 尤为重要/ ,/ 提高/ 数据/ 的/ 安全性/ 成为/ 亟待/ 处理/ 的/ 问题/ ./ 目前/ 这/ 类/ 系统/ 的/ 数据/ 安全性/ 研究/ 主要/ 包括/ 两个/ 方面/ :/ (/ 1/ )/ 错误/ 防范/ (/ proactive/ )/ ,/ 这些/ 研究/ 假定/ 在/ 某些/ 节点/ 不可/ 信/ 的/ 情况/ 下/ ,/ 用户/ 如何/ 防范/ 这些/ 节点/ 混淆/ 或者/ 窜改/ 存储/ 内容/ [/ 4/ -/ 6/ ]/ ;/ (/ 2/ )/ 错误/ 恢复/ (/ reactive/ )/ ,/ 另/ 一些/ 关于/ 安全性/ 的/ 研究/ 则/ 关注/ 在/ 系统/ 被/ 非法/ 访问者/ 入侵/ 后/ ,/ 如何/ 发现/ 由于/ 入侵者/ 的/ 窜改/ 而/ 导致/ 的/ 内容/ 污染/ 的/ 扩散/ ,/ 从而/ 将/ 系统/ 恢复/ 至/ 正确/ 状态/ [/ 7/ ]/ ./ 大量/ 的/ 研究/ 表明/ [/ 7/ -/ 9/ ]/ ,/ 入侵/ 和/ 管理员/ 配置/ 错误/ 在/ 实际/ 部署/ 中/ 很/ 难/ 避免/ ,/ 因此/ 本文/ 的/ 研究/ 主要/ 针对/ 于/ key/ -/ value/ 存储系统/ 的/ 错误/ 检测/ 恢复/ 技术/ ./ 在/ key/ -/ value/ 系统/ 出现/ 错误/ 之后/ ,/ 传统/ 的/ 恢复/ 方法/ 是/ 将/ 整个/ 系统/ 回溯到/ 一个/ 先前/ 的/ 时间/ 点/ ,/ 这种/ 方法/ 潜在/ 的/ 缺点/ 是/ ,/ 从/ 当前/ 时间/ 点到/ 恢复/ 时间/ 点/ 之间/ 的/ 数据/ 将/ 全部/ 丢失/ ./ 为了/ 尽可能/ 地/ 保留/ 不/ 受/ 污染/ 扩散/ 影响/ 的/ 内容/ 更新/ ,/ 减少/ 数据/ 的/ 丢失/ 量/ ,/ 本文/ 提出/ 一种/ 基于/ 分布式/ key/ -/ value/ 系统/ 的/ 错误/ 污染/ 检测/ 方法/ ./ 该/ 方法/ 通过/ 为/ 用户/ 操作/ 记录/ 关联/ 对应/ 的/ 向量/ 时钟/ ,/ 来/ 跟踪/ 跨/ 服务器/ 的/ 读写/ 请求/ 之间/ 的/ 逻辑关系/ ./ 由于/ 数据/ 访问/ 模式/ 往往/ 具有/ 局部性/ 的/ 特征/ ,/ 使用/ 该/ 污染/ 检测/ 方法/ ,/ 能够/ 高效/ 地/ 检测/ 错误/ 污染/ 和/ 扩散/ ,/ 避免/ 全/ 系统/ 的/ 整体/ 恢复/ ./ 本文/ 实现/ 了/ 一个/ 错误/ 污染/ 检测/ 系统/ TrackerStore/ ,/ 首次/ 在/ 分布式/ 存储系统/ 中/ 引入/ 污染/ 检测/ 机制/ ./ 在/ 实验室/ 的/ 集群/ 测试环境/ 下/ ,/ 引入/ 新/ 的/ 检测/ 系统/ 在/ 读写/ 比例/ 为/ 955/ 的/ 负载/ 下会/ 带来/ 26/ %/ 左右/ 的/ 额外/ 延迟/ 开销/ ,/ 而/ 国际/ 上/ 最新/ 的/ 单机/ 污染/ 检测/ 恢复/ 系统/ 在/ 引入/ 检测/ 机制/ 后/ 会/ 产生/ 24/ %/ ~/ 27/ %/ 的/ 性能/ 开销/ [/ 9/ ]/ ./ 2/ 背景/ 及/ 相关/ 工作/ 2.1/ 分布式/ key/ -/ value/ 系统/ 分布式/ key/ -/ value/ 系统/ 已经/ 被/ 广泛应用/ 到/ 了/ 大规模/ 工业/ 系统/ 中/ ,/ Amazon/ 的/ Dynamo/ [/ 2/ ]/ ,/ 开源/ 的/ 分布式/ key/ -/ value/ 系统/ ApacheCassandra/ [/ 3/ ]/ 和/ Volde/ -/ mort/ 也/ 已经/ 被/ 运用/ 在/ 实际/ 产品/ 的/ 运营/ 中/ ./ 这些/ 系统/ 的/ 共同/ 特点/ 是/ 无/ 中心/ 节点/ ,/ 使用/ DHT/ (/ DistributedHashTable/ ,/ 分布式/ 哈希/ 表/ )/ 技术/ 进行/ key/ 划分/ ,/ 并且/ 提供/ 最终/ 一致性/ (/ EventuallyConsistent/ [/ 10/ ]/ )/ ./ 与/ 传统/ 的/ 存储系统/ 相比/ ,/ 分布式/ key/ -/ value/ 系统/ 具有/ 较为/ 良好/ 的/ 可扩展性/ ,/ 并且/ 通过/ 提供/ 弱/ 一致性/ ,/ 达到/ 较/ 高/ 的/ 可用性/ ./ 同时/ ,/ 这些/ 系统/ 提供/ 了/ 较/ 高/ 的/ 容错/ 能力/ ,/ 以/ 适应/ 在/ 大规模/ 集群/ 系统/ 中/ 频繁/ 出现/ 的/ 节点/ 失效/ 的/ 问题/ ./ 由于/ 越来越/ 多/ 的/ 产品/ 系统/ 使用/ 分布式/ key/ -/ value/ 存储/ 作为/ 后/ 端的/ 数据/ 存储/ 引擎/ ,/ 本文/ 的/ 工作/ 基于/ 这类/ key/ -/ value/ 存储系统/ ,/ 提供/ 高效/ 的/ 错误/ 污染/ 检测/ 和/ 恢复能力/ ./ 2.2/ 分布式/ 存储系统/ 中/ 的/ 可靠/ 可信/ 问题/ 数据/ 的/ 安全/ 可靠性/ 一直/ 是/ 人们/ 对云/ 存储/ 的/ 最大/ 担忧/ 之一/ ./ 许多/ 研究/ 均/ 围绕/ 着/ 这/ 一方面/ 展开/ :/ Depot/ [/ 11/ ]/ 假定/ 在/ 一个/ 分布式/ 存储系统/ 中/ ,/ 客户端/ 和/ 服务器/ 均/ 可能/ 有/ 恶意/ 行为/ ,/ 两者/ 均/ 可以/ 混淆/ 数据/ 更新/ 的/ 顺序/ ./ 因此/ Depot/ 的/ 目的/ 在于/ 保证/ 所有/ 正确/ 节点/ 看到/ 的/ 数据/ 更新/ 顺序/ 一致/ ./ SPORC/ [/ 4/ ]/ 假设/ 在/ 云/ 存储系统/ 中/ 服务器/ 不可/ 信/ ,/ 因此/ 在/ 服务器/ 上/ 保存/ 加密/ 的/ 数据/ ,/ 且/ 每个/ 客户端/ 在/ 本地/ 保存/ 数据/ 拷贝/ ,/ 服务器/ 仅/ 负责/ 确定/ 操作/ 的/ 顺序/ 及/ 提交/ ./ 客户端/ 可以/ 通过/ 哈希/ 链/ 等/ 技术/ 检测/ 服务器/ 是否/ 会/ 混淆/ 更新/ 提交/ 顺序/ ,/ 从而/ 达到/ fork/ / 一致性/ ./ 在/ SPROC/ 之前/ ,/ BFT2F/ [/ 5/ ]/ 同样/ 实现/ 了/ fork/ / 一致性/ ,/ 与其/ 相比/ ,/ SPORC/ 还/ 加入/ 了/ OT/ (/ OperationalTransformation/ ,/ 操作/ 转换/ )/ 技术/ ,/ 使/ 其/ 除了/ 可以/ 检测/ 出/ 用户/ 更新/ 之间/ 的/ 冲突/ 及/ 服务器/ 混淆/ 版本/ 顺序/ 的/ 行为/ ,/ 还/ 能/ 将/ 数据/ 自动/ 恢复/ 到/ 一致/ 的/ 状态/ ./ 2.3/ 污染/ 检测/ 及/ 入侵/ 恢复/ 在/ 系统/ 被/ 入侵/ ,/ 文件系统/ 遭到/ 污染/ 后/ ,/ 管理员/ 往往/ 需要/ 将/ 系统/ 恢复/ 到/ 一个/ 合法/ 的/ 状态/ ./ 使用/ 文件系统/ 快照/ 是/ 系统/ 恢复/ 的/ 一个/ 常用/ 手段/ ,/ 但/ 这个/ 方法/ 会/ 使/ 发生/ 在/ 快照/ 之后/ 入侵/ 之前/ 的/ 合法/ 操作/ 被/ 消除/ ,/ 同时/ 其他/ 未/ 被/ 入侵/ 影响/ 的/ 合法/ 操作/ 也/ 会/ 被/ 消除/ ./ 为了/ 解决/ 这一/ 问题/ ,/ Taser/ [/ 6/ ]/ 入侵/ 恢复/ 系统/ 记录/ 了/ 所有/ 的/ 系统/ 调用/ 以及/ 数据/ 操作/ ,/ 管理员/ 在/ 发生/ 数据/ 污染/ 后/ 可以/ 对/ 日志/ 进行/ 因果/ 关联/ 的/ 分析/ ,/ 选择性/ 地/ 恢复/ 因/ 入侵/ 而/ 被/ 污染/ 的/ 数据文件/ 以及/ 受/ 影响/ 的/ 其他/ 文件/ ./ Taser/ 会/ 将/ 文件/ 恢复/ 到/ 最后/ 一个/ 合法/ 操作/ 之后/ 的/ 状态/ ,/ 这样/ 就/ 尽可能/ 多地/ 保留/ 了/ 合法/ 的/ 操作/ ./ Retro/ [/ 7/ ]/ 的/ 目标/ 与/ Taser/ 相同/ ,/ 但/ 其/ 通过/ 操作/ 历史/ 图/ 以及/ 预测/ ,/ ①/ Voldemort/ ./ http/ :/ // // project/ -/ voldemort/ ./ comPage3/ 重新/ 执行/ 等/ 技术/ ,/ 消除/ 了/ Taser/ 中/ 存在/ 的/ 大量/ 假/ 阳性/ (/ 正确/ 数据/ 被/ 错判/ 为/ 污染/ 数据/ )/ 及假/ 阴性/ (/ 污染/ 数据/ 被/ 检测/ 系统/ 漏过/ )/ ./ WARP/ [/ 8/ ]/ 将/ Retro/ 的/ 功能/ 拓展/ 到/ 了/ Web/ 应用/ 上/ ,/ 将/ Retro/ 中/ 基于/ 文件/ 和/ 进程/ 级别/ 的/ 关系/ 检测/ 进一步/ 细化/ 至/ 单条/ SQL/ 查询/ 的/ 粒度/ ,/ 因此/ 可以/ 进行/ 单/ 数据/ 行/ 级别/ 的/ 回滚/ ./ 同时/ 管理员/ 无需/ 找出/ 每/ 一次/ 入侵/ 的/ 根源/ ,/ 只/ 需要/ 安装/ 补丁/ 修复/ 入侵/ 所/ 利用/ 的/ 系统漏洞/ ,/ 或者/ 纠正错误/ 的/ 系统配置/ ,/ 系统/ 就/ 可以/ 重新/ 执行/ 合法/ 操作/ ,/ 消除/ 入侵/ 产生/ 的/ 影响/ ./ WARP/ 还/ 通过/ 记录/ 用户/ 浏览器/ 中/ 的/ DOM/ (/ DocumentObjectModel/ )/ 操作/ ,/ 来/ 重新/ 播放/ 用户/ 浏览器/ DOM/ 级别/ 的/ 行为/ ./ 这样/ 可以/ 在/ 不/ 需要/ 任何/ 用户/ 输入/ 的/ 前提/ 下/ 保留/ 用户/ 之前/ 的/ 合法/ 操作/ ./ 这些/ 系统/ 研究/ 的/ 数据/ 污染/ 关系/ 均/ 是/ 在/ 本地/ 文件系统/ 中/ ,/ 本文/ 则/ 是/ 在/ 分布式/ 的/ 存储系统/ 中/ 研究/ 污染/ 的/ 检测/ 问题/ ./ 传统/ 系统/ 中/ 经常/ 会/ 出现/ 多人/ 以及/ 多个/ 组织/ 协作/ 编辑/ 同一/ 文件/ 的/ 情形/ ,/ 由此/ 也/ 引发/ 了/ 追踪/ 数据/ 来源/ 的/ 需求/ [/ 9/ ]/ ./ 即/ 找出/ 数据/ 创建/ 以及/ 修改/ 的/ 一系列/ 操作/ 链条/ ./ PASS/ 系统/ [/ 12/ ]/ 实现/ 了/ 这/ 一/ 功能/ ,/ 通过/ PASS/ 系统/ ,/ 用户/ 可以/ 由/ 数据/ 来源/ 信息/ 查找/ 出/ 数据/ 错误/ 的/ 产生/ 位置/ ./ 这/ 对于/ 发现/ 系统/ 入侵/ 时/ 点/ ,/ 界定/ 错误/ 责任/ 来源/ 都/ 有/ 很大/ 的/ 帮助/ ./ 另一方面/ ,/ 版本/ 文件系统/ 提供/ 了/ 将/ 文件/ 从/ 可能/ 的/ 数据/ 损坏/ 中/ 恢复/ 的/ 能力/ ,/ 但/ 如果/ 由/ 人工/ 来/ 判定/ 恢复/ 到/ 何种/ 版本/ 需要/ 大量/ 的/ 时间/ 精力/ ,/ 因此/ 基于/ PASS/ 系统/ 和/ 版本/ 文件系统/ ,/ 有/ 研究者/ 实现/ 了/ 基于/ 因果关系/ 的/ 版本/ 记录/ [/ 13/ ]/ ./ 其/ 目的/ 与/ Taser/ 相似/ ,/ 希望/ 通过/ 分析/ 进程/ 对于/ 文件/ 的/ 读写/ 关系/ ,/ 确定/ 因/ 数据/ 污染/ 而/ 受/ 影响/ 的/ 文件/ ./ 因为/ 使用/ 了/ 版本/ 文件系统/ ,/ 系统/ 不/ 需要/ 像/ Taser/ 一样/ 重新/ 执行/ 被/ 污染/ 文件/ 上/ 的/ 合法/ 操作/ ,/ 也/ 不/ 需要/ 额外/ 记录/ 操作/ 对于/ 数据/ 内容/ 的/ 改动/ ,/ 节省/ 了/ 时间/ 和/ 空间/ ./ 基于/ 云/ 存储/ 的/ 数据/ 起源/ 系统/ [/ 14/ ]/ 研究/ 了/ 如何/ 将/ 云/ 存储系统/ (/ 如/ Amazon/ 的/ S3/ )/ 作为/ PASS/ 系统/ 的/ 存储/ 后/ 端的/ 问题/ ./ 这一/ 研究/ 的/ 主要/ 挑战/ 来自/ 于/ 分布式/ 存储系统/ 的/ 最终/ 一致性/ 特征/ ,/ 这一/ 特征/ 使得/ 起源/ (/ Provenance/ )/ 信息/ 和/ 数据文件/ 会/ 出现/ 不/ 一致/ 的/ 情况/ ,/ 因此/ 该/ 研究/ 的/ 重点/ 是/ 如何/ 扩展/ 原有/ 的/ 起源/ 信息/ 协议/ ,/ 但/ 并/ 没有/ 就/ 如何/ 进行/ 污染/ 检测/ 进行/ 深入探讨/ ./ 本文/ 研究/ 在/ 分布式/ key/ -/ value/ 存储系统/ 中/ 如何/ 进行/ 错误/ 污染/ 的/ 检测/ ./ 讨论/ 了/ 如何/ 应用/ 改进/ 的/ 向量/ 时钟/ ,/ 来/ 分析/ 存储/ 记录/ 间/ 的/ 因果/ 联系/ ./ 与/ Taser/ 、/ Retro/ 、/ WARP/ 只是/ 检测/ 单机/ 系统/ 的/ 错误/ 相比/ ,/ 本文/ 将/ 错误/ 污染/ 的/ 检测/ 扩展/ 到/ 了/ 分布式系统/ 中/ ,/ 着力/ 解决/ 跨/ 服务器/ 间/ 的/ 错误/ 污染/ 传播/ ./ 与/ 追踪/ 数据/ 来源/ 的/ 系统/ 相比/ ,/ 本文/ 的/ 研究/ 假定/ 用户/ 不/ 可靠/ ,/ 因此/ 不/ 依赖/ 客户端/ 记录/ 来源/ 信息/ ,/ 而是/ 通过/ 服务器/ 上/ 的/ 用户/ 操作/ 记录/ ,/ 来/ 判断/ 用户/ 操作/ 之间/ 的/ 逻辑联系/ ./ 3/ 原理/ 概述/ 3.1/ 威胁/ 模型/ 与/ 假设/ 一般/ 对/ 安全性/ 的/ 研究/ 首先/ 需要/ 提出/ 威胁/ 模型/ 以及/ 相关/ 的/ 假设/ ./ 本文/ 假定/ 服务器/ 是/ 可信/ 的/ ,/ 即/ 服务器/ 不会/ 在/ 未经/ 用户/ 许可/ 的/ 前提/ 下/ 对/ 用户/ 存储/ 内容/ 进行/ 非法/ 窜改/ ,/ 也/ 不会/ 使用/ 过时/ 的/ 数据/ 来/ 混淆/ 用户/ 的/ 内容/ ,/ 同时/ 不会/ 窃取/ 用户/ 内容/ ./ 本文/ 做出/ 这样/ 的/ 假设/ 的/ 理由/ 在于/ ,/ 基于/ 现有/ 研究/ ,/ 用户/ 可以/ 通过/ 签名/ 和/ 数据/ 版本/ 来/ 防范/ 服务器/ 的/ 窜改/ 与/ 混淆/ ,/ 同时/ 通过/ 对/ 内容/ 加密/ 来/ 保证/ 内容/ 的/ 私密性/ ,/ 因此/ 服务器/ 不可/ 信/ 的/ 威胁/ 可以/ 被/ 降低/ 到/ 一个/ 可以/ 容忍/ 的/ 程度/ ./ 本文/ 还/ 假设/ ,/ 服务器/ 是/ 可靠/ 的/ ,/ 即/ 服务器/ 不会/ 因/ 断电/ ,/ 软硬件/ 错误/ 等/ 故障/ 而/ 产生/ 数据/ 丢失/ ./ 在/ 这方面/ ,/ 对于/ 副本/ 、/ 备份/ 、/ 恢复/ 的/ 研究/ 已经/ 十分/ 充分/ ,/ 所以/ 本文/ 在/ 威胁/ 模型/ 中将/ 这个/ 因素/ 排除/ 在外/ ./ 在/ 本文/ 的/ 研究/ 中/ ,/ 威胁/ 来自/ 于/ 不可/ 信/ 的/ 用户/ ,/ 包括/ 授权/ 的/ 用户/ 由于/ 被/ 入侵/ (/ 如/ 账号/ 被/ 窃取/ )/ 转变/ 为/ 不可/ 信/ 用户/ 并且/ 对/ 服务器/ 数据/ 进行/ 窜改/ ;/ 或者/ 管理员/ 由于/ 软件/ 配置/ 错误/ ,/ 对/ 服务器端/ 数据/ 造成/ 污染/ ./ 用户/ 由于/ 被/ 入侵/ 或者/ 管理员/ 配置/ 错误/ 导致/ 的/ 对/ 服务器/ 上/ 存储/ 的/ 数据/ 的/ 污染/ 可能/ 不能/ 被/ 及时发现/ ,/ 因此/ 合法/ 的/ 用户/ 会/ 读取/ 这些/ 不/ 正确/ 的/ 数据/ ,/ 从而/ 造成/ 错误/ 污染/ 的/ 扩散/ ./ 本文/ 假设/ ,/ 在/ 因/ 用户/ 被/ 入侵/ 或者/ 配置/ 错误/ 而/ 产生/ 数据/ 错误/ 后/ 的/ 某个/ 时/ 点/ ,/ 入侵/ 或者/ 配置/ 错误/ 的/ 行为/ 可以/ 被/ 系统管理员/ 发现/ ,/ 并且/ 可以/ 推断出/ 一个/ 离/ 现在/ 最近/ 的/ 正确/ 时点/ ./ 从/ 该/ 特定/ 时/ 点/ 之后/ ,/ 对应/ 的/ 用户/ 被/ 认为/ 是/ 不可/ 信/ 的/ ,/ 因此/ 其/ 所有/ 数据/ 修改/ 行为/ 均/ 被/ 认定/ 为/ 不可/ 信/ 的/ ,/ 即/ 其/ 写入/ 的/ 数据/ 均/ 为/ 错误/ 的/ ./ 在/ 错误/ 数据/ 得到/ 纠正/ 之前/ ,/ 其他/ 可信/ 用户/ 如果/ 在/ 另/ 一时/ 点/ 读取/ 了/ 该/ 错误/ 数据/ ,/ 则/ 认为/ 可信/ 用户/ 在/ 该/ 时点/ 之后/ 也/ 将/ 变得/ 不可/ 信/ ./ 系统/ 的/ 目标/ 是/ 检测/ 出该/ 类型/ 所有/ 错误/ ,/ 并/ 最终/ 达到/ 恢复/ 正确/ 数据/ 的/ 目的/ ./ 以图/ 1/ 为例/ 可以/ 说明/ 以上/ 所/ 描述/ 的/ 威胁/ 模型/ ./ 如图/ ,/ 系统/ 中/ 存在/ 3/ 个/ 服务器/ 以及/ 3/ 个/ 用户/ ,/ 假设/ 已知/ 用户/ A/ 在/ 向/ 服务器/ 1/ 写入/ Foo1/ (/ Set/ 操作/ )/ 时/ 已经/ 不可/ 信/ ,/ 用户/ B/ 在/ 用户/ A/ 写入/ 后/ ,/ 从/ 服务器/ 1/ 读取/ Foo1/ (/ Get/ 操作/ )/ ,/ 之后/ 向/ 服务器/ 2/ 写入/ Foo2/ (/ Set/ 操作/ )/ ./ 由于/ 用户/ B/ 读取/ 了/ 不可/ 信/ 用户/ 写入/ 的/ 值/ Page4Foo1/ ,/ 其/ 读取/ Foo1/ 之后/ 的/ 所有/ 写入/ 都/ 应该/ 视为/ 不可/ 信/ 的/ 行为/ ./ 因此/ Foo2/ 是/ 不可/ 信/ 的/ 写入/ 值/ ,/ 这/ 也/ 导致用户/ C/ 读取/ Foo2/ 后/ 变得/ 不可/ 信/ ,/ 进而/ 使得/ 其向/ 服务器/ 3/ 写入/ 的/ 值/ Foo3/ 也/ 被/ 标记/ 为/ 不可/ 信/ ./ 3.2/ 污染/ 检测/ 目标/ 及/ 原理/ 本文/ 研究/ 的/ 目标/ 是/ ,/ 在/ 确定/ 污染/ 起点/ 之后/ ,/ 检测/ 出/ 所有/ 不可/ 信/ 的/ 写/ 操作/ ,/ 包括/ 由/ 直接/ 污染源/ 造成/ 的/ 不可/ 信写/ 操作/ 和/ 由于/ 读取/ 不可/ 信/ 的/ 写入/ 值/ 产生/ 的/ 错误/ 污染/ 扩散/ ./ 污染/ 扩散/ 的/ 定义/ 如下/ :/ (/ 1/ )/ 可以/ 确定/ 一个/ 时刻/ T0/ ,/ 从/ 该/ 时刻/ 起/ ,/ 某个/ 用户/ 写入/ 的/ 任意/ 值均/ 不可/ 信/ ./ (/ 2/ )/ 假设/ 从/ T1/ (/ T1/ >/ T0/ )/ 时刻/ 起/ 用户/ A/ 不可/ 信/ ,/ 则/ 该/ 时刻/ 后/ 其/ 向/ 服务器/ 写入/ 的/ 〈/ key/ ,/ value/ 〉/ 对/ 均/ 被/ 视为/ 错误/ 值/ ./ 若/ T2/ 时刻/ (/ T1/ </ T2/ )/ 用户/ B/ 从/ 同一/ 服务器/ 读取/ key/ 对应/ 的/ 值/ ,/ 则/ T2/ 时刻/ 后/ B/ 向/ 任意/ 服务器/ 写入/ 的/ 值/ 均/ 不可/ 信/ ,/ 也/ 即/ B/ 被/ 污染/ ./ (/ 3/ )/ 上述/ 时刻/ 均/ 为/ 全局/ 同一时间/ ./ 检测/ 系统/ 应该/ 最大/ 程度/ 找出/ 上述/ 污染/ ,/ 允许/ 存在/ 少量/ 的/ 假/ 阳性/ (/ falsepositive/ )/ ,/ 即/ 系统/ 将/ 某/ 一/ 用户/ 误认为/ 被/ 污染/ ./ 但/ 不应/ 产生/ 真/ 阴性/ (/ truenegative/ )/ ,/ 即不应/ 漏过/ 根据/ 定义/ 可以/ 判定/ 的/ 污染/ 扩散/ ./ 根据/ 以上/ 定义/ ,/ 要/ 在/ 读写/ 日志/ 中/ 检测/ 出/ 错误/ 污染/ ,/ 需要/ 依赖于/ 全局/ 时钟/ ./ 但/ 实际上/ ,/ 在/ 分布式系统/ 中/ 实现/ 全局/ 时钟/ 较为/ 复杂/ ,/ 而且/ 会/ 给/ 系统/ 强加/ 不必要/ 的/ 开销/ ./ 目前/ 大部分/ key/ -/ value/ 分布式系统/ 均/ 不/ 依赖于/ 全局/ 时钟/ ,/ 因此/ 检测/ 系统/ 使用/ 了/ 向量/ 时钟/ 来/ 处理/ 这一/ 问题/ ./ 由于/ 在/ 威胁/ 模型/ 中/ ,/ 系统/ 假定/ 了/ 用户/ 是/ 不可/ 信/ 的/ ,/ 因此/ 这里/ 只/ 使用/ 每个/ 服务器/ 的/ 逻辑/ 时钟/ 来/ 构成/ 向量/ 时钟/ ./ 服务器/ 的/ 逻辑/ 时钟/ 定义/ 为/ :/ (/ 1/ )/ Get/ 操作/ 不/ 改变/ 服务器/ 逻辑/ 时钟/ 的/ 数值/ ./ (/ 2/ )/ 服务器/ 每/ 完成/ 用户/ 的/ 一个/ Set/ 操作/ 请求/ ,/ 本地/ 逻辑/ 时钟/ 的/ 值加/ 1/ ./ 所有/ 服务器/ 的/ 逻辑/ 时钟/ 构成/ 了/ 系统/ 的/ 向量/ 时钟/ ./ 使用/ 系统/ 向量/ 时钟/ ,/ 就/ 可以/ 判定/ 一个/ 用户/ 从/ 一个/ 服务器/ 读取/ 内容/ 的/ Get/ 操作/ 和/ 其/ 向/ 另/ 一个/ 服务器/ 写入/ 内容/ 的/ Put/ 操作/ 之间/ 的/ 逻辑关系/ ./ 3.3/ 向量/ 时钟/ 更新/ 规则/ 每个/ 服务器/ 均/ 保存/ 着/ 一个/ 系统/ 向量/ 时钟/ Veci/ ./ 其中/ ,/ Veci/ (/ k/ )/ 即/ 为/ 服务器/ i/ 上/ 保存/ 的/ 服务器/ k/ 的/ 逻辑/ 时钟/ 值/ ./ Veci/ (/ i/ )/ 即/ 为/ 服务器/ i/ 自己/ 的/ 本地/ 逻辑/ 时钟/ 值/ ./ 服务器/ 在/ 特定/ 事件/ 发生/ 时会/ 与/ 其他/ 服务器/ 同步/ 逻辑/ 时钟/ ./ 根据/ 3.2/ 节中/ 定义/ 可知/ ,/ 逻辑/ 时钟/ 只有/ 在/ Set/ 操作/ 发生/ 时才/ 会/ 产生/ 变化/ ,/ 因此/ 一种/ 策略/ 是/ 在/ 每次/ Set/ 操作/ 发生/ 后/ ,/ 服务器/ 立刻/ 与/ 其他/ 服务器/ 同步/ ,/ 更新/ 自己/ 保存/ 的/ 其他/ 服务器/ 的/ 逻辑/ 时钟/ ,/ 同时/ 将/ 自己/ 的/ 逻辑/ 时钟/ 更新/ 到/ 其他/ 服务器/ 上/ ./ 这种/ 策略/ 可以/ 最大/ 程度/ 确保/ 每个/ 服务器/ 的/ 向量/ 时钟/ 与/ 其他/ 服务器/ 同步/ ,/ 但/ 也/ 带来/ 较为/ 严重/ 的/ 性能/ 开销/ ./ 在/ 这里/ 每/ 一个/ Set/ 操作/ 都/ 会/ 导致/ 一个/ 系统/ 广播/ 的/ 出现/ ,/ 这/ 就/ 加重/ 了/ 服务器/ 与/ 网络/ 的/ 负载/ ./ 考虑/ 一种/ 新/ 的/ 策略/ ,/ 由于/ Set/ 操作/ 写入/ 的/ 值/ 在/ 没有/ 被/ Get/ 操作/ 读取/ 之前/ 并/ 不会/ 对系统/ 造成/ 污染/ ,/ 因此/ 可以/ 考虑/ 将/ Set/ 操作/ 产生/ 的/ 向量/ 时钟/ 同步/ 推后/ 至/ 相应/ 的/ 值/ 被/ Get/ 操作/ 读取/ 之时/ ./ 这一/ 改进/ 可以/ 降低/ 向量/ 时钟/ 同步/ 的/ 频率/ ,/ 而且/ 可以/ 保证/ 错误/ 污染/ 关系/ 仍/ 可/ 检出/ ./ 但/ 由于/ Set/ 操作/ 对应/ 的/ 向量/ 时钟/ 被/ 推迟/ 了/ ,/ 发生/ 在/ Set/ 操作/ 之后/ 的/ Get/ 操作/ 可能/ 会/ 被/ 标记/ 上/ 一个/ 早于/ Set/ 操作/ 的/ 向量/ 时钟/ ./ 假如/ Get/ 操作/ 是/ 被/ 污染/ 的/ ,/ 那么/ Set/ 操作/ 也/ 会/ 被/ 误判/ 为/ 被/ 污染/ ,/ 也/ 即/ 系统/ 出现/ 假/ 阳性/ 的/ 概率/ 上升/ ./ 综合/ 起来/ ,/ 服务器/ 的/ 本地/ 逻辑/ 时钟/ 和/ 其/ 保存/ 的/ 其他/ 服务器/ 逻辑/ 时钟/ 的/ 更新/ 规则/ 如下/ :/ 分析/ 操作/ 之间/ 的/ 联系/ ,/ 可以/ 从/ 最/ 基本/ 的/ 情况/ 出/ (/ 1/ )/ 服务器/ i/ 接收/ 到/ 一个/ Set/ 操作/ ,/ 本地/ 逻辑/ 时钟/ 自增/ 一个/ 单位/ ,/ 本地/ 记录/ 的/ 其他/ 服务器/ 逻辑/ 时钟/ 不变/ ./ (/ 2/ )/ 服务器/ i/ 接收/ 到/ 一个/ Get/ 操作/ ,/ 若/ 读取/ 了/ 之前/ Set/ 的/ 值/ ,/ 则/ 进行/ 一次/ 与/ 其他/ 服务器/ 的/ 时钟/ 同步操作/ ./ 向量/ 时钟/ 的/ 意义/ 在于/ 作为/ 每/ 一个/ 发生/ 的/ Get/ 与/ Set/ 操作/ 的/ 时间/ 标记/ ./ 对于/ 服务器/ i/ 上/ 的/ Set/ 操作/ ,/ 最后/ 记录/ 的/ 向量/ 时钟/ 的/ 值/ 按/ 如下/ 方法/ 确认/ :/ (/ 1/ )/ 向量/ 时钟/ 的/ 时钟/ 分量/ i/ 为/ Set/ 操作/ 发生/ 时/ (/ 2/ )/ 向量/ 时钟/ 的/ 其他/ 时钟/ 分量/ 为/ 发生/ Get/ 读取/ 事件/ 后/ 进行/ 时钟/ 同步/ 得到/ 的/ 值/ ./ 对于/ 服务器/ i/ 上/ 的/ Get/ 操作/ ,/ 以/ 其/ 发生/ 时/ 服务器/ i/ 上/ 保存/ 的/ 各/ 服务器/ 逻辑/ 时钟/ 值/ (/ 包括/ 服务器/ i/ 自己/ 的/ 逻辑/ 时钟/ )/ 构成/ 该/ 操作/ 的/ 向量/ 时钟/ 值/ ./ 若/ 发生/ 了/ 前述/ 的/ 与/ 其他/ 服务器/ 同步/ 的/ 操作/ ,/ 则/ 各/ 服务器/ 的/ 逻辑/ 时钟/ 使用/ 的/ 是/ 同步/ 之后/ 的/ 值/ ./ 3.4/ 不同/ 类型/ 操作/ 之间/ 的/ 因果/ 关联/ 服务器/ i/ 本地/ 逻辑/ 时钟/ 自增后/ 的/ 数值/ ./ Page5/ 发/ ./ 对于/ 任意/ 两个/ 操作/ 来说/ ,/ 可能/ 出现/ 的/ 组合/ 有/ Get/ -/ Get/ 、/ Set/ -/ Set/ 、/ Get/ -/ Set/ 、/ Set/ -/ Get4/ 种/ ./ 第/ 1/ 种/ 对/ 数据/ 无/ 影响/ ,/ 第/ 2/ 种/ 前后/ 的/ 两个/ 操作/ 之间/ 不会/ 出现/ 污染/ 扩散/ ./ 会/ 产生/ 污染/ 扩散/ 的/ 两类/ 操作/ 是/ Get/ -/ Set/ 和/ Set/ -/ Get/ 组合/ ,/ 下面/ 将/ 分别/ 详细/ 介绍/ ./ 对于/ Get/ -/ Set/ 组合/ 来说/ ,/ 只有/ 两个/ 操作/ 同时/ 属于/ 一个/ 用户/ 才/ 会/ 产生/ 污染/ 扩散/ 的/ 联系/ ./ 也就是说/ ,/ 如果/ 通过/ 记录下来/ 的/ 向量/ 时钟/ 可以/ 判定/ 用户/ 进行/ 的/ 某个/ Get/ 操作/ 在/ 其/ 进行/ 的/ 另/ 一个/ Set/ 操作/ 之前/ (/ 无论是/ 在/ 同一/ 服务器/ 之上/ 还是/ 在/ 不同/ 服务器/ 上/ )/ ,/ 且/ Get/ 操作/ 已经/ 被/ 判定/ 为/ 不可/ 信/ 操作/ ,/ 则/ 根据/ 本文/ 的/ 威胁/ 模型/ ,/ Set/ 操作/ 也/ 会/ 被/ 标记/ 为/ 不可/ 信/ ,/ 也/ 即/ 其/ 写入/ 的/ 为/ 错误/ 值/ ./ 对于/ Set/ -/ Get/ 组合/ 来说/ ,/ 两个/ 操作/ 必须/ 针是/ 对/ 同一/ 服务器/ 上/ 同一/ key/ 才/ 会/ 产生/ 污染/ 扩散/ 的/ 联系/ ./ 也/ 即/ ,/ 当/ 一个/ 已经/ 被/ 污染/ 的/ 用户/ 通过/ Set/ 操作/ 写入/ 某个/ 〈/ key/ ,/ value/ 〉/ 对时/ ,/ 后续/ 通过/ Get/ 操作/ 读取/ 该/ key/ 的/ 另/ 一/ 用户/ 会/ 被/ 污染/ ./ 由于/ 是/ 在/ 同一/ 服务器/ 上/ ,/ 对/ 同一/ key/ 的/ 访问/ 会/ 由/ 读写/ 锁/ 保护/ ,/ 因此/ Set/ 和/ Get/ 操作/ 之间/ 的/ 精确/ 顺序/ 很/ 容易/ 确定/ ./ 一个/ 不可/ 信/ 的/ Set/ 操作/ 将/ 导致/ 其后/ 读取/ Set/ 操作/ 写入/ 值/ 的/ 所有/ Get/ 操作/ 均/ 被/ 污染/ ./ 从/ 上面/ 分析/ 看/ ,/ 向量/ 时钟/ 最/ 重要/ 的/ 作用/ 是/ 分析/ 同一/ 用户/ 在/ 两台/ 服务器/ (/ 相同/ 服务器/ 是/ 一个/ 特例/ ,/ 可以/ 使用/ 同样/ 的/ 时钟/ 向量/ 判定/ 方法/ )/ 的/ Get/ 操作/ 和/ Set/ 操作/ 之间/ 的/ 逻辑关系/ ./ 本文/ 的/ 目标/ 是/ 保证/ 所有/ 逻辑/ 上/ 在/ Get/ 之后/ 的/ Set/ 操作/ 都/ 能/ 用/ 向量/ 时钟/ 的/ 方法/ 检测/ 出来/ ./ 假设/ 有/ 服务器/ i/ 与/ j/ ,/ 用户/ 在/ 服务器/ i/ 进行/ 了/ Get/ 操作/ ,/ 并且/ 在/ Get/ 操作/ 完成/ 之后/ 才/ 在/ 服务器/ j/ 进行/ 了/ Set/ 操作/ ./ 比较/ 服务器/ i/ 的/ Get/ 操作/ 与/ 服务器/ j/ 的/ Set/ 操作/ 分别/ 对应/ 的/ 向量/ 时钟/ ,/ 根据/ 3.3/ 节中/ 的/ 定义/ ,/ 服务器/ j/ 的/ Set/ 操作/ 所/ 进行/ 的/ 时钟/ 向量/ 同步/ 在/ 服务器/ i/ 的/ Get/ 操作/ 之后/ ,/ 而/ 逻辑/ 时钟/ 又/ 是/ 单调/ 递增/ 的/ ,/ 因此/ 对/ j/ 以外/ 的/ 分量/ ,/ 可以/ 确定/ Veci/ (/ k/ )/ / Vecj/ (/ k/ )/ ,/ k/ ≠/ j/ ,/ 而/ Veci/ (/ j/ )/ </ Vecj/ (/ j/ )/ ./ 这里/ 的/ 严格/ 小于/ 关系/ 是/ 可以/ 证明/ 的/ :/ 服务器/ j/ 收到/ Set/ 操作/ 后会/ 自增/ 其/ 本地/ 逻辑/ 时钟/ ,/ Set/ 操作/ 所/ 对应/ 的/ 向量/ 时钟/ 的/ 逻辑/ 时钟/ 分量/ j/ 使用/ 的/ 是/ 自/ 增后/ 的/ 值/ ,/ 而/ Get/ 操作/ 使用/ 的/ 必定/ 是/ 自/ 增前/ 的/ 值/ ,/ 因此/ 严格/ 小于/ 的/ 关系/ 成立/ ./ 对于/ 每/ 一个/ 操作/ 组合/ ,/ 系统/ 都/ 将/ 依照/ 上面/ 的/ 规则/ 判断/ 因果/ 联系/ ,/ 最后/ 根据/ 污染源/ 头/ 的/ 位置/ ,/ 就/ 可以/ 判断/ 出/ 系统/ 中/ 哪些/ 内容/ 已经/ 不可/ 信/ ,/ 哪些/ 虽然/ 是/ 在/ 出现/ 污染/ 后/ 才/ 写入/ 的/ 数据/ 但/ 依然/ 可信/ ./ 3.5/ 因果关系/ 有向图/ 构造/ 要/ 分析/ 所有/ 操作/ 之间/ 的/ 因果关系/ ,/ 达到/ 错误/ 污染/ 检测/ 的/ 目标/ ,/ 就/ 需要/ 以/ 3.4/ 节中/ 的/ 判断/ 方法/ ,/ 由点/ 及面/ ,/ 从/ 操作/ 序列/ 中/ 构造/ 出/ 所有/ 操作/ 之间/ 的/ 联系/ 图/ ./ 由于/ 时序/ 上/ 的/ 单调/ 性/ ,/ 最后/ 构造/ 出来/ 的/ 图/ 必定/ 是/ 一个/ 有/ 向/ 无/ 环图/ ./ 这样/ ,/ 从/ 操作/ 序列/ 中/ 检测/ 错误/ 污染/ 的/ 问题/ ,/ 也/ 自然/ 转化/ 为/ 有/ 向/ 无/ 环图/ 的/ 遍历/ 问题/ ./ 为了/ 构造/ 有/ 向/ 无/ 环图/ ,/ 每/ 一个/ 操作/ 都/ 是/ 图/ 中/ 的/ 一个/ 节点/ ,/ 因此/ 操作/ 序列/ 最后/ 会/ 转化/ 为/ Get/ 和/ Set/ 两类/ 节点/ ./ 对于/ Get/ 节点/ ,/ 其/ 与/ 那些/ 同属/ 一个/ 用户/ 且/ 逻辑/ 上/ 发生/ 在/ Get/ 节点/ 之后/ 的/ Set/ 节点/ 之间/ 存在/ 一条/ 从/ Get/ 节点/ 到/ Set/ 节点/ 的/ 有/ 向/ 边/ ./ 对于/ Set/ 节点/ ,/ 其/ 与/ 前述/ 的/ 访问/ 同一/ key/ 且/ 发生/ 在/ 同一/ 服务器/ 之上/ 的/ Get/ 节点/ 之间/ 存在/ 一条/ 从/ Set/ 节点/ 到/ Get/ 节点/ 的/ 有/ 向/ 边/ ./ 通过/ 这样/ 的/ 转换/ 操作/ ,/ 操作/ 序列/ 转化/ 为/ 一个/ 有向图/ ./ 在/ 有向图/ 中/ ,/ 进行/ 污染/ 检测/ 等价/ 于/ 从/ 某/ 节点/ 开始/ 进行/ 有向图/ 的/ 广度/ 优先/ 搜索/ ./ 遍历/ 的/ 起点/ 是/ 污染源/ ,/ 遍历/ 中/ 访问/ 到/ 的/ 每个/ 节点/ 都/ 是/ 不可/ 信/ 的/ 节点/ ./ 这样/ 就/ 完成/ 了/ 污染/ 检测/ 算法/ 的/ 设计/ ./ 4/ 系统/ 实现/ 本/ 节/ 主要/ 描述/ 我们/ 如何/ 实现/ 错误/ 污染/ 检测/ 方法/ ./ 错误/ 污染/ 恢复/ 方法/ 的/ 原型/ 实现/ ,/ TrackerStore/ ,/ 是/ 基于/ 被/ 广泛/ 使用/ 的/ Voldmort/ 系统/ ./ 与/ 其他/ key/ -/ value/ 存储系统/ 类似/ ,/ Voldmort/ 系统/ 主要/ 向/ 用户/ 提供/ get/ 和/ put/ 接口/ ./ 由于/ TrackerStore/ 的/ 实现/ 仅/ 与/ Voldmort/ 提供/ 的/ 用户/ 接口/ 相关/ ,/ 在/ 此/ 我们/ 不再/ 敷述/ Voldmort/ 的/ 具体/ 实现/ ./ 4.1/ 系统/ 组成/ 如图/ 2/ 所示/ ,/ 组成/ 系统/ 的/ 部分/ 包括/ ,/ 基本/ 的/ key/ -/ value/ 存储模块/ ,/ 在/ 客户端/ 和/ 服务器端/ 中间/ 新添/ 的/ 追/ Page6/ 踪/ 模块/ TrackerStore/ 、/ 缓存/ 模块/ 以及/ 日志/ 模块/ ./ 版本/ 监控/ 模块/ 维护/ 本地/ 的/ 逻辑/ 时钟/ 并且/ 在/ 必要/ 的/ 时刻/ 与/ 其他/ 服务器/ 的/ 版本/ 监控/ 模块/ 进行/ 通讯/ ,/ 进行/ 逻辑/ 时钟/ 同步/ ./ 上述/ 模块/ 与/ 日志/ 系统/ 共同/ 组成/ 系统/ 的/ 监控/ 子系统/ ./ 为了/ 降低/ 本地/ IO/ 负载/ ,/ 日志/ 系统/ 可以/ 设置/ 在/ 独立/ 的/ 服务器/ 上/ ./ 系统/ 另/ 一个/ 重要/ 的/ 组成部分/ 则/ 是/ 监控/ 日志/ 分析/ 模块/ ,/ 如图/ 3/ 所示/ ./ 每个/ 服务器/ 均/ 有/ 自己/ 的/ 监控/ 日志/ 记录/ ,/ 在/ 分析/ 系统/ 的/ 错误/ 污染/ 时/ ,/ 需要/ 将/ 所有/ 日志/ 整合/ ,/ 构造/ 出/ 相应/ 的/ 关系/ 图/ 并/ 进行/ 分析/ ./ 尽管/ 可以/ 直接/ 将/ 所有/ 日志/ 同步/ 到/ 一台/ 服务器/ 上/ 并/ 将/ 日志/ 进行/ 合并/ ,/ 排序/ ,/ 但是/ 这样/ 会/ 给/ 网络/ 带来/ 较/ 高/ 的/ 瞬时/ 负载/ ./ 同时/ ,/ 计算/ 负载/ 也/ 将/ 集中/ 在/ 某/ 一台/ 服务器/ 之上/ ./ 通过/ 仔细检查/ 错误/ 污染/ 的/ 分析/ 流程/ ,/ 可以/ 发现/ ,/ 错误/ 污染/ 的/ 检测/ 算法/ 具有/ 分布式计算/ 的/ 潜力/ ./ 运用/ 类似/ 于/ MapReduce/ 在/ 页面/ 排序/ 算法/ 上/ 的/ 思想/ ,/ 本文/ 将/ 分析/ 模块/ 分解/ 为主/ 节点/ 与/ 分析/ 节点/ ./ 错误/ 污染/ 的/ 分析/ 由主/ 节点/ 发起/ ,/ 每个/ 服务器/ 均/ 有/ 分析/ 节点/ 负责/ 归约/ 本地/ 的/ 监控/ 日志/ 记录/ ,/ 并/ 将/ 结果/ 返回/ 给/ 主/ 节点/ ,/ 主/ 节点/ 合并/ 所有/ 分析/ 节点/ 归约/ 的/ 结果/ ,/ 根据/ 需要/ 向/ 各/ 服务器/ 的/ 分析/ 节点/ 分配/ 新/ 的/ 分析/ 任务/ ./ 4.2/ 向量/ 时钟/ 延后/ 更新/ 技术/ 追踪/ 记录/ 模块/ 由/ TrackerStore/ ,/ 日志/ 系统/ 和/ 服务器/ 版本/ 监控/ 模块/ 组成/ ./ TrackerStore/ 会/ 把/ 接收/ 到/ 的/ 所有/ 请求/ 都/ 提交/ 给/ 真正/ 的/ KeyValueStore/ ,/ 并/ 将/ 相应/ 的/ 信息/ 传递/ 给/ 日志/ 系统/ ./ TrackerStore/ 中/ 最/ 重要/ 的/ 技术/ 是/ 根据/ 3.3/ 节所/ 阐述/ 的/ 原理/ 实现/ 的/ 向量/ 时钟/ 延后/ 更新/ 技术/ ./ 每个/ 被/ TrackerStore/ 截获/ 的/ Set/ 操作/ 并/ 不会/ 立刻/ 传递/ 到/ 日志/ 系统/ 进行/ 记录/ ./ 系统/ 会/ 将/ Set/ 操作/ 的/ key/ 值/ 和/ 对应/ 的/ 本地/ 逻辑/ 时钟/ 先/ 存放/ 在/ 缓存/ 中/ ./ 当/ TrackerStore/ 截获/ Get/ 操作/ 时/ ,/ 同样/ 先/ 检查/ 本地/ 缓存/ 中/ 是否/ 存放/ 了/ Get/ 操作/ 要/ 获取/ 的/ key/ 值/ ,/ 若无则/ 直接/ 从/ 服务器/ 版本/ 监控/ 模块/ 获取/ 一个/ 当前/ 服务器/ 保存/ 的/ 向量/ 时钟/ ,/ 作为/ Get/ 操作/ 的/ 时钟/ 并/ 传递/ 给/ 日志/ 模块/ 进行/ 记录/ ./ 否则/ 说明/ 本次/ Get/ 操作/ 要/ 获取/ 的/ key/ 对应/ 的/ 值/ 是/ 之前/ 未/ 被/ 记录/ 的/ Set/ 操作/ 所/ 设置/ 的/ ./ 因此/ 需要/ 首先/ 处理/ 缓存/ 中/ 所有/ 未/ 被/ 记录/ 的/ Set/ 操作/ ,/ 再/ 记录/ 本次/ 的/ Get/ 操作/ ./ 在/ 处理/ 缓存/ 中/ 的/ Set/ 操作/ 之前/ ,/ 版本/ 监控/ 系统/ 会/ 进行/ 一次/ 时钟/ 同步/ ,/ 其/ 将/ 自身/ 最新/ 的/ 本地/ 逻辑/ 时钟/ 发送给/ 其他/ 服务器/ 的/ 版本/ 监控/ 系统/ ,/ 同时/ 获取/ 其他/ 服务器/ 最新/ 的/ 逻辑/ 时钟/ 值/ ./ 同步/ 结束/ 后/ ,/ 发起/ 同步/ 的/ 服务器/ i/ 得到/ 当时/ 最新/ 的/ 时钟/ 向量/ ,/ 而/ 其他/ 服务器/ 则/ 更新/ 了/ 其/ 本地/ 时钟/ 向量/ 中/ 服务器/ i/ 对应/ 的/ 分量/ 的/ 值/ ./ 根据/ 更新/ 后/ 向量/ 时钟/ ,/ Get/ 与/ Set/ 操作/ 按/ 3.3/ 节中/ 所述/ 确定/ 各自/ 对应/ 的/ 向量/ 时钟/ 并/ 传递/ 至/ 日志/ 模块/ 进行/ 记录/ ,/ 记录/ 完毕/ 后/ 清空/ 缓存/ ./ TrackerStore/ ,/ 本地/ 缓存/ 和/ 版本/ 监控/ 模块/ 一起/ ,/ 实现/ 了/ 延后/ 更新/ 时钟/ 向量/ 的/ 策略/ ./ 但/ 事实上/ ,/ 如前所述/ ,/ 延后/ 更新/ 策略/ 并非/ 没有/ 缺陷/ ./ 时钟/ 向量/ 更新/ 的/ 周期/ 越长/ ,/ 出现/ 假/ 阳性/ 的/ 概率/ 就/ 会/ 越/ 大/ ./ 可以/ 通过/ 缓存/ 定时/ 清洗/ 进一步/ 改进/ 延后/ 更新/ 技术/ ./ 也/ 即/ ,/ 定期/ 进行/ 服务器/ 向量/ 时钟/ 的/ 同步/ 并且/ 处理/ 缓存/ 中/ 所有/ 未/ 被/ 记录/ 的/ Set/ 操作/ ./ 系统/ 频繁/ 进行/ 时钟/ 同步/ 导致/ 的/ 负载/ 上升/ 与/ 降低/ 假/ 阳性/ 概率/ 之间/ 的/ 取舍/ ,/ 可以/ 通过/ 设置/ 缓存/ 清洗/ 的/ 时间/ 周期/ 来/ 达到/ ./ 缓存/ 清洗/ 周期/ 越短/ ,/ 则/ 负载/ 越/ 接近/ 于/ 实时/ 的/ 时钟/ 同步/ ,/ 周期/ 越长/ ,/ 则/ 越/ 接近/ 于/ 完全/ 的/ 延后/ 更新/ 时钟/ 向量/ 策略/ ./ 在/ 本文/ 进行/ 的/ 实验/ 中/ ,/ 周期/ 取/ 1s/ ./ 4.3/ 操作/ 关系/ 有向图/ 的/ 分布式/ 构造/ 技术/ 基于/ TrackerStore/ 提供/ 的/ 日志/ 记录/ ,/ 分析/ 模块/ 可以/ 按/ 3.5/ 节中/ 所述/ 方法/ 构造/ 一个/ 操作/ 关系/ 图/ ,/ 并/ 进行/ 污染/ 检测/ ./ 但/ 在/ 实际/ 中/ ,/ 每台/ 服务器/ 均/ 有/ 自己/ 的/ TrackerStore/ ,/ 因此/ 会/ 产生/ 独立/ 的/ 日志/ 文件/ ,/ 要/ 分析/ 这些/ 文件/ 最/ 直接/ 的/ 方法/ 是/ 指定/ 一个/ 服务器/ ,/ 将/ 所有/ 日志/ 均/ 传输/ 至该/ 服务器/ ,/ 由该/ 服务器进行/ 一次/ 日志/ 的/ 合并/ 操作/ ,/ 之后/ 再/ 构建/ 操作/ 关系/ 图/ ./ 这种/ 方法/ 简单易行/ 但/ 效率/ 低下/ ,/ 不仅/ 传输/ 日志/ 会/ 造成/ 较大/ 的/ 网络/ 负载/ ,/ 通过/ 单一/ 节点/ 来/ 进行/ 计算/ 也/ 无法/ 充分利用/ 分布式系统/ 的/ 资源/ ./ 因此/ 这里/ 应该/ 使用/ 各/ 服务器/ 自行/ 构造/ 关系/ 图/ 并/ 计算/ 出受/ 污染/ 的/ 内容/ ,/ 然后/ 再/ 将/ 结果/ 与/ 其他/ 服务器/ 交换/ 并/ 进行/ 迭代/ 运算/ ./ 然而/ 如果/ 服务/ Page7/ 器/ 之间/ 交换/ 的/ 是/ 完整/ 的/ 关系/ 图/ ,/ 同样/ 会/ 产生/ 较大/ 的/ 网络/ 负载/ ,/ 所以/ 在/ 具体/ 实现/ 中/ 本文/ 对/ 算法/ 进行/ 了/ 改进/ :/ 首先/ 将/ 关系/ 图中/ 的/ 节点/ 分为/ 操作/ 节点/ 及/ 用户/ 节点/ 两类/ ./ 对于/ 每/ 一个/ Get/ 操作/ 或/ Set/ 操作/ ,/ 系统/ 均会/ 构造/ 一个/ 操作/ 节点/ ,/ 同时/ 对于/ 在/ 日志/ 中/ 出现/ 的/ 每/ 一个/ 用户/ ,/ 同样/ 构造/ 一个/ 用户/ 节点/ ./ 本文/ 假设/ 用户/ 一旦/ 读取/ 了/ 一个/ 被/ 污染/ 的/ 值/ ,/ 其/ 之后/ 的/ 所有/ 操作/ 也/ 将/ 会/ 被/ 视为/ 不可/ 信/ ,/ 因此/ 对于/ 用户/ 节点/ 来说/ ,/ 需要/ 保存/ 的/ 是/ 其/ 最早/ 被/ 污染/ 的/ 时刻/ (/ 被/ 入侵/ 或者/ 因/ 读取/ 被/ 污染/ 的/ 值/ 而/ 受到/ 影响/ )/ ./ 这样/ 各/ 服务器/ 本地/ 的/ 关系/ 图/ 就/ 可以/ 构造/ 出来/ :/ 对于/ 同一/ 服务器/ 上/ 操作/ 同一个/ key/ 的/ Set/ 和/ Get/ 操作/ ,/ 若/ Set/ 操作/ 的/ 逻辑/ 时钟/ 小于/ Get/ 操作/ 的/ 逻辑/ 时钟/ ,/ 则/ 增加/ 一条/ 从/ Set/ 操作/ 节点/ 到/ Get/ 操作/ 节点/ 的/ 有/ 向/ 边/ ,/ 表示/ Set/ 操作/ 节点/ 会/ 影响/ Get/ 操作/ 节点/ ./ 对于/ 同一/ 用户/ 发出/ 的/ Get/ 和/ Set/ 操作/ ,/ 前面/ 章节/ 提出/ 需要/ 根据/ 向量/ 时钟/ 判定/ 先后顺序/ ./ 但是/ 这/ 两个/ 操作/ 节点/ 可能/ 位于/ 不同/ 的/ 服务器/ ,/ 所以/ 为了/ 能/ 在/ 不/ 同步/ 日志/ 内容/ 的/ 前提/ 下/ 构造/ 本地/ 关系/ 图/ ,/ 需要/ 以/ 用户/ 节点/ 作为/ 同一/ 用户/ 发出/ 的/ Get/ 和/ Set/ 操作/ 的/ 中介/ ./ 对于/ 每/ 一个/ Get/ 操作/ 节点/ ,/ 均/ 增加/ 一条/ 从/ Get/ 操作/ 节点/ 到/ 对应/ 用户/ 节点/ 的/ 有/ 向/ 边/ ,/ 对于/ 每个/ Set/ 节点/ ,/ 则/ 增加/ 一条/ 从/ 用户/ 节点/ 到/ Set/ 节点/ 的/ 边/ ,/ 由此/ 本地/ 关系/ 图/ 构造/ 完成/ ./ 在/ 进行/ 错误/ 污染/ 分析/ 时/ ,/ 首先/ 要/ 确定/ 用户/ 变得/ 不可/ 信/ 的/ 时点/ ,/ 这部分/ 数据/ 由/ 管理员/ 在/ 主/ 节点/ 输入/ ,/ 并/ 由主/ 节点/ 为/ 每/ 一个/ 分析/ 节点/ 指定/ ./ 分析/ 节点/ 进行/ 本地/ 的/ 错误/ 污染/ 检测/ :/ 首先/ 设置/ 不可/ 信/ 用户/ 节点/ 最早/ 被/ 污染/ 的/ 时刻/ ,/ 其次/ 分析/ 节点/ 会/ 遍历/ 用户/ 节点/ 的/ 所有/ 出边/ ,/ 找出/ 在/ 污染/ 时刻/ 后/ 用户/ 产生/ 的/ Set/ 操作/ 节点/ 并/ 将/ 其/ 标记/ ,/ 之后/ 从/ Set/ 操作/ 节点/ 的/ 出边/ 遍历/ 所有/ 受/ Set/ 操作/ 影响/ 的/ Get/ 操作/ 节点/ 并/ 做/ 标记/ ./ 对于/ 每/ 一个/ 标记/ 为/ “/ 被/ 污染/ ”/ 的/ Get/ 操作/ 节点/ ,/ 更新/ 其/ 指向/ 的/ 用户/ 节点/ 的/ 向量/ 时钟/ ,/ 使得/ 用户/ 节点/ 向量/ 时钟/ 的/ 每/ 一/ 分量/ 均/ 小于/ 或/ 等于/ Get/ 操作/ 节点/ 的/ 向量/ 时钟/ 的/ 对应/ 分量/ ./ 若/ 用户/ 节点/ 向量/ 时钟/ 被/ 更新/ ,/ 则/ 其/ 需要/ 找出/ 新/ 的/ 受/ 影响/ 的/ Set/ 操作/ 节点/ 并/ 重复/ 前述/ 过程/ ./ 每/ 一个/ 分析/ 节点/ 完成/ 图/ 的/ 遍历/ 后/ ,/ 也/ 就/ 完成/ 了/ 一次/ 本地/ 错误/ 污染/ 的/ 检测/ ,/ 产生/ 了/ 所有/ 用户/ 节点/ 最早/ 被/ 污染/ 的/ 向量/ 时钟/ ./ 所有/ 的/ 分析/ 节点均/ 会/ 将/ 自己/ 分析/ 得出/ 的/ 用户/ 节点/ 数据/ 传回/ 主/ 节点/ ,/ 由/ 其/ 合并/ 不同/ 分析/ 节点/ 的/ 用户/ 节点/ 并/ 重新/ 更新/ 分析/ 节点/ 的/ 用户/ 节点/ 信息/ ./ 分析/ 节点/ 得到/ 更新/ 的/ 数据/ 后/ 再/ 一次/ 进行/ 错误/ 污染/ 检测/ ./ 经过/ 数次/ 迭代/ ,/ 当/ 所有/ 分析/ 节点/ 都/ 不再/ 更新/ 用户/ 节点/ 数据/ 时/ ,/ 整个/ 分析/ 过程/ 完成/ ,/ 分析/ 节点/ 向主/ 节点/ 输出/ 被/ 污染/ 的/ key/ 值/ ./ 5/ 性能/ 评测/ 为/ key/ -/ value/ 存储系统/ 增加/ 污染/ 检测/ 组件/ 会/ 给/ 整个/ 系统/ 带来/ 额外/ 的/ 开销/ ./ 因此/ 本文/ 通过/ 测量/ 系统/ 添加/ 污染/ 检测/ 组件/ 前后/ 的/ 请求/ 延迟/ 来/ 衡量/ 污染/ 检测/ 给/ 系统/ 整体/ 带来/ 的/ 开销/ ./ 实验/ 环境/ 中/ ,/ 本文/ 使用/ 了/ 5/ 台/ 配置/ AMDOpteron4/ 核/ 处理器/ ,/ 16GB/ 内存/ 的/ 服务器/ 来/ 部署/ 分布式/ key/ -/ value/ 存储系统/ ,/ 每台/ 机器/ 均/ 使用/ 本地/ 的/ SATA/ 磁盘/ ./ 同时/ 使用/ 10/ 个/ 客户端/ 连接/ 到/ 服务器/ ./ 节点/ 间/ 通过/ GB/ 交换机/ 互联/ ./ 我们/ 首先/ 使用/ Voldmort/ 标准/ 的/ Benchmark/ 进行/ 测试/ ,/ 该/ Benchmark/ 工具/ 是/ Linkedin/ 公司/ 根据/ 其/ key/ -/ value/ 负载/ 需求/ 开发/ 的/ ./ 在/ 工作/ 负载/ 上/ ,/ Bench/ -/ mark/ 预先/ 装载/ 5000/ 组/ key/ -/ value/ 对/ ,/ 并/ 使用/ 不同/ 大小/ 的/ 值/ 进行/ 测试/ ,/ 包括/ 1KB/ 、/ 10KB/ 、/ 128KB/ ./ 客户端/ 向/ 服务器/ 发出/ 一系列/ Get/ 和/ Set/ 请求/ ,/ 每个/ 客户端/ 读写/ 的/ key/ 都/ 是从/ key/ 空间/ 中/ 随机/ 均匀/ 选取/ ./ 为了/ 模拟/ 不同/ 的/ key/ -/ value/ 工作/ 负载/ ,/ 本文/ 在/ 测试/ 中/ 使用/ 不同/ 的/ 读写/ 比例/ ./ 比如/ 在读/ 延迟/ 测试/ 中/ ,/ 100/ %/ 读/ 和/ 80/ %/ 读/ 20/ %/ 写/ 的/ 负载/ 用来/ 模拟/ 典型/ 的/ 互联网/ 应用/ 负载/ ,/ 50/ %/ 读/ 50/ %/ 写/ 是/ 用来/ 模拟/ 如/ 删/ 冗/ ,/ 会话/ 连接/ 等/ 工作/ 负载/ ./ 在/ 延迟/ 测试/ 中/ ,/ 客户端/ 会/ 向/ 服务器/ 发出/ 100000/ 个/ 请求/ ,/ 本文/ 分别/ 测量/ 读写操作/ 的/ 平均/ 延迟/ ./ 在/ 平均/ 读/ 延迟/ 图/ 4/ 中/ ,/ 基准/ 100/ %/ 和/ Tracker100/ %/ 分别/ 表示/ 在/ 100/ %/ 读/ 的/ 负载/ 中/ ,/ 未/ 添加/ 检测/ 模块/ 的/ Voldemort/ 系统/ 以及/ 添加/ 了/ 检测/ 模块/ 的/ TrackerStore/ 的/ 性能/ 情况/ ./ 其他/ 以此类推/ ./ 而/ 在/ 平均/ 写/ 延迟/ 的/ 图/ 5/ 中/ ,/ 基准/ 100/ %/ 和/ Tracker100/ %/ 代表/ 的/ 是/ 100/ %/ 写/ 操作/ 的/ 负载/ ./ Page8/ 从/ 实验/ 的/ 结果/ 看/ ,/ 引入/ 污染/ 检测/ 后/ 系统/ 读写/ 延迟/ 都/ 产生/ 了/ 一定/ 程度/ 的/ 上升/ ./ 同时/ ,/ 在/ 本文/ 的/ 系统/ 的/ 设计/ 中/ ,/ 当/ 用户/ 的/ Get/ 操作/ 读取/ 了/ 一个/ 最近/ Set/ 操作/ 写入/ 的/ 值时/ ,/ 需要/ 进行/ 一次/ 版本/ 同步/ 以及/ 记录/ Set/ 操作/ 的/ 信息/ ,/ 因此/ 当/ Set/ 操作/ 的/ 比例/ 上升时/ ,/ Get/ 操作/ 需要/ 进行/ 这种/ 额外/ 操作/ 的/ 概率/ 也/ 会/ 相应/ 上升/ ./ 可以/ 预期/ ,/ 新/ 检测/ 组件/ 给/ Get/ 操作/ 的/ 延迟时间/ 带来/ 的/ 额外/ 开销/ 将/ 随着/ Set/ 操作/ 比例/ 的/ 上升/ 而/ 增加/ ./ 而/ 在/ 原来/ 的/ 系统/ 中/ ,/ Get/ 操作/ 的/ 延迟/ 随着/ Set/ 操作/ 比例/ 的/ 上升/ 也/ 会/ 有/ 一定/ 提高/ (/ 主要/ 是/ 读写/ 锁/ 的/ 争用/ )/ ,/ 但/ 幅度/ 相对/ 新/ 系统/ 较/ 小/ ./ 从/ 具体/ 的/ 数值/ 比较/ 上/ ,/ 在/ 1KB/ 、/ 10KB/ 、/ 128KB/ ,/ 读/ 比例/ 100/ %/ 的/ 实验/ 中/ ,/ 引入/ 污染/ 检测/ 后/ ,/ 系统/ 的/ 读/ 延迟/ 分别/ 上升/ 了/ 0.02/ ms/ 、/ 0.2/ ms/ 、/ 0.8/ ms/ ,/ 系统/ 额外/ 开销/ 较/ 小/ ,/ 而/ 在读/ 比例/ 下降/ 到/ 80/ %/ 的/ 实验/ 中/ ,/ 系统/ 读/ 延迟/ 则/ 分别/ 上升/ 了/ 0.09/ ms/ 、/ 0.3/ ms/ 、/ 1.5/ ms/ ./ 因此/ 当写/ 操作/ 在/ 系统/ 负载/ 中/ 比例/ 较/ 高时/ ,/ 污染/ 检测/ 会/ 给/ 系统/ 带来/ 一定/ 的/ 额外/ 开销/ ./ 同时/ ,/ 随着/ 请求/ 操作/ 的/ 大小/ 的/ 增加/ ,/ 额外/ 开销/ 在/ 总/ 延迟/ 中/ 占/ 比/ 逐渐/ 减少/ ./ 为了/ 估计/ 系统/ 在/ 真实/ 负载/ 下/ 的/ 性能/ 开销/ ,/ 本文/ 另外/ 使用/ Yahoo/ !/ 的/ 云/ 存储/ 测试/ 框架/ Yahoo/ !/ CloudServingBenchmark/ [/ 15/ ]/ (/ YCSB/ )/ 中/ 的/ 不同/ 模拟/ 负载/ 进行/ 了/ 评测/ ./ 该/ Benchmark/ 是/ Yahoo/ !/ 根据/ 其/ 生产/ 环境/ 中/ 的/ 工作/ 负载/ 进行/ 的/ 抽象/ ,/ 其/ 典型/ 负载/ 包括/ A/ -/ 密集/ 更新/ 负载/ (/ 读/ 与/ 更新/ 操作/ 的/ 比例/ 为/ 50/ // 50/ ,/ 典型/ 应用/ 为/ 用户/ 会话/ 存储/ )/ ,/ B/ -/ 密集/ 读/ 负载/ (/ 读/ 与/ 更新/ 操作/ 的/ 比例/ 为/ 95/ // 5/ ,/ 如/ 读取/ 图片/ 标签/ )/ ,/ C/ -/ 只读/ 负载/ (/ 如/ 用户/ 信息/ 缓存/ )/ ,/ D/ -/ 读取/ 最新/ 值/ 负载/ (/ 读/ 与/ 插入/ 操作/ 的/ 比例/ 为/ 95/ // 5/ ,/ 如/ 读取/ 用户/ 最新/ 状态/ )/ ,/ E/ -/ 读取/ -/ 修改/ 负载/ (/ 读/ 以及/ 读/ -/ 修改/ 的/ 操作/ 比例/ 为/ 50/ // 50/ )/ ./ 所有/ 负载/ 的/ 记录/ 大小/ 都/ 为/ 1KB/ ,/ 每/ 一种/ 负载/ 都/ 包含/ 50/ 万条/ 测试/ 记录/ ,/ 结果/ 如图/ 6/ 、/ 图/ 7/ 所示/ ./ 从/ 测试/ 结果/ 看/ ,/ 当写/ 比例/ 上升时/ (/ 对于/ YCSB/ 的/ 5/ 类/ 负载/ 来说/ ,/ 写/ 比例/ 从/ 高到/ 地/ 依次/ 为/ A/ 、/ E/ 、/ B/ 、/ D/ 、/ C/ ./ 其中/ C/ 负载/ 是/ 只读/ 负载/ ,/ 因此/ 无写/ 延迟/ 数据/ )/ 系统/ 的/ 读/ 延迟/ 开销/ 有/ 明显/ 上升/ ./ 在/ 较为/ 常见/ 的/ 以读/ 操作/ 为主/ 的/ 负载/ B/ 、/ C/ 、/ D/ 中/ ,/ 引入/ 污染/ 检测/ 会/ 使/ 读/ 操作/ 增加/ 26/ %/ 左右/ 的/ 延迟/ 开销/ ./ 国外/ 类似/ 的/ 系统/ WARP/ 在/ 单机/ 环境/ 下/ 引入/ 污染/ 检测/ 会/ 增加/ 24/ %/ ~/ 27/ %/ 的/ 额外/ 性能/ 开销/ ,/ 因此/ 本文/ 提出/ 的/ 检测/ 系统/ 性能/ 开销/ 与/ 国外/ 类似/ 工作/ 相同/ ,/ 但/ 将/ 检测/ 能力/ 从/ 单机/ 扩展/ 到/ 了/ 分布式/ 网络/ 中/ ./ 另外/ ,/ 从/ 测试/ 结果/ 中/ 可以/ 看出/ ,/ A/ 类及/ E/ 类/ 负载/ 的/ 额外/ 开销/ 明显/ 高于/ B/ 、/ C/ 、/ D/ 类/ ,/ 其中/ A/ 类/ 又/ 高于/ E/ 类/ ./ 这里/ 的/ 主要/ 原因/ 与/ 标准/ Benchmark/ 里/ 出现/ 的/ 现象/ 相同/ ,/ 即/ 写/ 操作/ (/ Set/ 操作/ )/ 比例/ 上升/ 后/ ,/ 读/ 操作/ (/ Get/ 操作/ )/ 需要/ 进行/ 版本/ 同步/ 的/ 概率/ 也/ 随之/ 提高/ ,/ 因此/ 读/ 操作/ 的/ 额外/ 开销/ 也/ 会/ 增加/ ./ 对于/ A/ 类/ 负载/ 来说/ ,/ 读/ 操作/ 与/ 更新/ 操作/ 的/ 比例/ 为/ 5050/ ,/ 而/ 每个/ 更新/ 操作/ 中/ 需要/ 进行/ 一次/ 读/ 和/ 一次/ 写/ ,/ 因此/ 整体/ 读写/ 比例/ 为/ 7525/ ;/ 对于/ E/ 类/ 负载/ 来说/ ,/ 读/ 与/ “/ 读/ -/ 修改/ ”/ 操作/ 的/ 比例/ 同样/ 是/ 5050/ ,/ 但/ 一次/ “/ 读/ -/ 修改/ ”/ 实质/ 由/ 一个/ 读/ 和/ 一个/ 更新/ 操作/ 组成/ ,/ 因此/ E/ 类/ 负载/ 整体/ 读写/ 比例/ 为/ 8317/ ,/ 新/ 系统/ 在/ E/ 类/ 负载/ 下/ 产生/ 的/ 额外/ 开销/ 应/ 低于/ A/ 类/ 负载/ ./ 5/ 类/ 负载/ 所/ 体现/ 出/ 的/ 性能/ 变化趋势/ 与/ 理论/ 估计/ 情况/ 一致/ ./ Page96/ 小结/ 目前/ NoSQL/ 类/ 数据系统/ 尤其/ 是/ key/ -/ value/ 系统/ 日益/ 流行/ ,/ 由此/ 带来/ 一系列/ 对于/ 数据安全/ 可靠性/ 的/ 隐忧/ ./ 本文/ 通过/ 将/ 传统/ 系统/ 中/ 污染/ 检测/ 引入/ 分布式/ key/ -/ value/ 系统/ 中/ ,/ 为/ 这类/ 系统/ 提供/ 了/ 一种/ 新/ 的/ 数据/ 可信/ 验证/ 工具/ ./ 同时/ ,/ 通过/ 合理安排/ 对系统/ 的/ 追踪/ 记录/ 行为/ ,/ 只/ 需要/ 付出/ 较/ 低/ 的/ 性能/ 代价/ ,/ 就/ 可以/ 为/ 分布式/ key/ -/ value/ 系统/ 引入/ 污染/ 检测/ 的/ 能力/ ,/ 具有/ 较/ 高/ 的/ 效率/ ./ 

