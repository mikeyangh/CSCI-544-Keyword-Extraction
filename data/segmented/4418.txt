Page1/ 基于/ 在线/ 消息传递/ 的/ 主题/ 追踪/ 方法/ 龚声蓉/ 叶芸/ 刘/ 纯平/ 季怡/ (/ 苏州大学/ 计算机科学/ 与/ 技术/ 学院/ 江苏/ 苏州/ 215006/ )/ 摘要/ 主题/ 追踪/ 因/ 可以/ 有效/ 地/ 汇集/ 和/ 组织/ 分散/ 在/ 不同/ 时间/ 、/ 地点/ 的/ 信息/ ,/ 并/ 从/ 主题/ 层次/ 的/ 角度/ 对/ 某个/ 主题/ 相关/ 事件/ 的/ 时效性/ 、/ 动态/ 演化/ 关系/ 等/ 得到/ 比较/ 全面/ 的/ 把握/ ,/ 成为/ 当前/ 数据挖掘/ 领域/ 的/ 重要/ 研究/ 方向/ ./ 现有/ 基于/ 概率/ 主题/ 模型/ 的/ 主题/ 追踪/ 方法/ 主要/ 以/ 潜在/ 狄利克/ 雷/ 分布/ (/ LatentDirichletallocation/ ,/ LDA/ )/ 模型/ 为/ 基础/ ,/ 采用/ 在线/ 吉布斯/ 采样/ (/ OnlineGibbsSampling/ ,/ OGS/ )/ 和/ 在线/ 变分/ 贝叶斯/ (/ OnlineVariationalBayesian/ ,/ OVB/ )/ 算法/ 进行/ 参数估计/ ./ OGS/ 和/ OVB/ 算法/ 尽管/ 解决/ 了/ LDA/ 模型/ 中/ 使用/ 传统/ 离线/ 近似/ 推理方法/ 所/ 需/ 内存空间/ 的/ 大小/ 随/ 数据/ 集/ 的/ 增长/ 而/ 不断/ 增加/ ,/ 无法/ 训练/ 海量/ 数据/ 集/ 以及/ 数据流/ 数据/ 的/ 问题/ ,/ 但/ 训练/ 的/ 精度/ 和/ 速度/ 均/ 有待/ 提高/ ./ 该文/ 基于/ LDA/ 模型/ 的/ 改进/ 因子/ 图/ 提出/ 了/ 一种/ 在线/ 消息传递/ (/ OnlineBeliefPropagation/ ,/ OBP/ )/ 的/ 主题/ 追踪/ 算法/ ./ 该/ 算法/ 借助/ 因子/ 图中/ 消息传递/ (/ BeliefPropagation/ ,/ BP/ )/ 算法/ 的/ 推理/ ,/ 通过/ 切分/ 海量/ 数据/ 集为/ 段/ ,/ 并用/ 前/ 一段/ 数据/ 集/ 训练/ 后/ 的/ 参数/ 计算/ 当前/ 段/ 的/ 梯度/ 下降/ ,/ 使得/ 主题/ 追踪/ 更加/ 快速/ 和/ 准确/ ./ 四组/ 大规模/ 文本/ 数据/ 集/ 的/ 实验/ 对比/ 表明/ ,/ LDA/ 模型/ 中/ OBP/ 算法/ 在/ 速度/ 和/ 精度/ 上均/ 优越/ 于/ OGS/ 和/ OVB/ 算法/ ,/ 文中/ 也/ 从/ 理论/ 上/ 进一步/ 验证/ 了/ OBP/ 算法/ 的/ 收敛性/ ,/ 并/ 给出/ 了/ 主题/ 追踪/ 的/ 具体/ 应用/ ./ 关键词/ 潜在/ 狄利克/ 雷/ 分布/ ;/ 吉布斯/ 采样/ ;/ 变分/ 贝叶斯/ ;/ 消息传递/ 算法/ ;/ 主题/ 追踪/ ;/ 社交/ 网络/ ;/ 社会/ 计算/ 1/ 引言/ 随着/ 信息技术/ 的/ 高速/ 发展/ ,/ 来源于/ 门户网站/ 、/ 电子商务/ 网站/ 、/ 社交/ 网站/ 、/ 论坛/ 、/ 博客/ 和/ 微博/ 等/ 信息/ 正以/ 指数/ 级/ 的/ 方式/ 增长/ ./ 搜索引擎/ 虽然/ 可以/ 方便/ 的/ 提供/ 很多/ 信息/ 资源/ ,/ 但/ 却/ 不能/ 有效/ 地/ 发现/ 和/ 管理/ 与/ 某/ 一/ 主题/ 相关/ 的/ 信息/ ./ 如何/ 从/ 上述/ 海量/ 的/ 文本/ 集中/ 寻找/ 热点话题/ 成为/ 当前/ 信息检索/ (/ InformationRetrieval/ ,/ IR/ )/ 领域/ 的/ 研究/ 关键/ ,/ 而/ 基于/ 概率/ 主题/ 模型/ 的/ 主题/ 追踪/ 方法/ 则/ 可以/ 有效/ 地/ 发现/ 与/ 该/ 主题/ 相关/ 的/ 信息/ [/ 1/ ]/ ./ 现有/ 基于/ 概率/ 主题/ 模型/ 的/ 主题/ 追踪/ 方法/ ,/ 利用/ 快速/ 的/ 学习/ 算法/ 从/ 高维/ 稀疏/ 的/ 单词/ 数据/ 中/ 提取/ 低维/ 的/ 主题/ 表示/ ,/ 从而/ 追踪/ 主题/ 的/ 不断/ 变化趋势/ ./ 常见/ 的/ 主题/ 模型/ 有/ 空间/ 向量/ 模型/ [/ 2/ ]/ 、/ 潜在/ 语义/ 分析/ (/ LatentSemanticAnalysis/ ,/ LSA/ )/ [/ 3/ ]/ 模型/ 、/ 概率/ 潜在/ 语义/ 分析/ (/ ProbabilisticLatentSemanticAnalysis/ ,/ PLSA/ )/ [/ 4/ ]/ 模型/ 和/ 潜在/ 狄利克/ 雷/ 分布/ (/ LDA/ )/ 模型/ [/ 5/ ]/ 等/ ./ PLSA/ 模型/ 和/ LDA/ 模型/ 通过/ 联合/ 概率/ 描述/ 文本/ 单词/ 和/ 主题/ 之间/ 的/ 关系/ ,/ 并且/ 每个/ 概率/ 均/ 有/ 合理/ 的/ 物理/ 解释/ ,/ 从而/ 能够/ 很/ 好地解决/ 文档/ 聚类/ 的/ 问题/ ,/ 是/ 目前/ 最/ 常用/ 的/ 主题/ 模型/ ./ 应用/ 这/ 两种/ 模型/ 的/ 核心/ 在于/ 学习/ 模型/ 中/ 的/ 参数/ ,/ 而/ 参数/ 的/ 个数/ 对模型/ 的/ 复杂程度/ 有/ 很大/ 的/ 影响/ ./ PLSA/ 模型/ 中/ 的/ 参数/ 个数/ 会/ 随/ 训练/ 文本/ 的/ 不断/ 增加/ 而/ 增加/ ,/ LDA/ 模型/ 对外/ 始终/ 只有/ 两个/ 超参/ ,/ 因此/ LDA/ 模型/ 更/ 有利于/ 训练/ 海量/ 数据/ 集/ ./ LDA/ 模型/ 学习/ 的/ 关键在于/ 从/ 主题/ 和/ 单词/ 的/ 联合/ 概率/ 中/ 推断出/ 在/ 可/ 观测/ 变量/ 下/ 主题/ 的/ 后验/ 概率分布/ ,/ 但是/ 无法/ 直接/ 通过/ 后验/ 概率分布/ 求解/ 模型/ 中/ 的/ 参数/ ,/ 一般/ 需要/ 采用/ 近似/ 后验/ 推理方法/ ./ 目前/ 广泛/ 采用/ 的/ 近似/ 推理方法/ 有/ 吉布斯/ 采样/ (/ GibbsSampling/ ,/ GS/ )/ [/ 6/ ]/ 、/ 变分/ 贝叶斯/ (/ VariationalBayesian/ ,/ VB/ )/ [/ 5/ ]/ 和/ 离线/ 消息传递/ (/ BeliefPropagation/ ,/ BP/ )/ 算法/ [/ 7/ ]/ ./ 离线/ GS/ 、/ VB/ 和/ BP/ 近似/ 推理/ 算法/ 已经/ 在/ 小规模/ 数据/ 集上/ 取得/ 了/ 应用/ ./ 文献/ [/ 5/ ]/ 提出/ 了/ VB/ 近似/ 推理/ 算法/ 对/ 文本/ 数据/ 分类/ ,/ 在/ 平均/ 包含/ 16000/ 篇/ 文本/ 的/ 文本/ 集上/ 的/ 实验/ 表明/ ,/ LDA/ 模型/ 比/ PLSA/ 模型/ 在/ 分类/ 速度/ 和/ 精度/ 方面/ 有/ 实质性/ 的/ 提高/ ./ 文献/ [/ 6/ ]/ 首次/ 提出/ 了/ GS/ 近似/ 推理/ 算法/ ,/ 并/ 对/ 图像/ 进行/ 分类/ ./ 在/ 2000/ 幅/ ,/ 每幅/ 大小/ 为/ 5/ ×/ 5/ 像素/ 的/ 图像/ 数据/ 集上/ 的/ 实验/ 验证/ 了/ GS/ 比/ VB/ 收敛/ 的/ 速度/ 快且/ 精度高/ ./ 文献/ [/ 7/ ]/ 基于/ LDA/ 模型/ ,/ 在/ 四组/ 文本/ 数据/ 集上/ 对比/ 分析/ 了/ BP/ 、/ GS/ 和/ VB/ 三种/ 近似/ 推理/ 算法/ ./ 实验/ 表明/ BP/ 近似/ 推理/ 算法/ 的/ 精度/ 和/ 训练/ 速度/ 均/ 优于/ GS/ 和/ VB/ 算法/ ./ 这些/ 离线/ 算法/ 尽管/ 简单/ 稳定/ ,/ 但/ 收敛/ 速度/ 通常/ 很/ 慢/ ,/ 且/ 需要/ 将/ 整个/ 训练/ 集/ 加载/ 到/ 内存/ ./ 由于/ 实际/ 中/ 往往/ 处理/ 的/ 是/ 大规模/ 的/ 实时/ 流/ 数据/ ,/ 如/ 博客/ 等/ 流型/ 数据/ ,/ 离线/ 算法/ 因/ 数据/ 集/ 本身/ 不/ 完全/ 以及/ 内存不足/ 而/ 无法/ 处理/ ./ 为了/ 克服/ 离线/ 算法/ 在/ 处理/ 这类/ 流/ 数据/ 时/ 的/ 缺陷/ ,/ 将/ 海量/ 数据/ 集切/ 分成/ 若干/ 小/ 段/ ,/ 然后/ 顺序/ 处理/ 每/ 一段/ 数据/ 的/ 在线/ 学习/ 算法/ 成为/ 一个/ 首选/ ./ 对/ 每个/ 时间段/ ,/ 在线/ 算法/ 只/ 加载/ 一小/ 段/ 数据/ 到/ 内存/ ,/ 并/ 对/ 当前/ 段用/ 梯度/ 下降/ 法/ [/ 8/ ]/ 估计/ 模型/ 的/ 参数/ ,/ 在/ 当前/ 段/ 训练/ 结束/ 后/ ,/ 将/ 该段/ 数据/ 集/ 移出/ 内存/ ,/ 再/ 加载/ 下/ 一段/ 数据/ 集/ 进行/ 学习/ ./ 目前/ 在线/ 学习/ 算法/ 已经/ 在/ 主题/ 模型/ 的/ 近似/ 推理/ [/ 9/ -/ 14/ ]/ 、/ 目标/ 检测/ [/ 15/ -/ 16/ ]/ 、/ 大规模/ 矩阵/ 分解/ [/ 17/ -/ 18/ ]/ 、/ 高维/ 数据分析/ [/ 19/ ]/ 和/ SVM/ 中/ 的/ 核/ 函数/ 在线/ 学习/ [/ 20/ ]/ 等/ 众多/ 方面/ 取得/ 了/ 广泛/ 的/ 应用/ ./ Canini/ 等/ 人/ [/ 21/ ]/ 提出/ 了/ 基于/ LDA/ 模型/ 的/ 增长/ 式/ 吉布斯/ 采样/ (/ IncrementalGibbsSampling/ ,/ IGS/ )/ 算法/ ,/ 即/ 在线/ 吉布斯/ 采样/ (/ OGS/ )/ 算法/ ;/ Hoffman/ 等/ 人/ [/ 12/ ]/ 提出/ 了/ 在线/ 变分/ 贝叶斯/ (/ OVB/ )/ 算法/ ./ 这/ 两类/ 在线/ 近似/ 推理方法/ 在/ 分类/ 时比/ 离线/ 算法/ 需要/ 较少/ 的/ 迭代/ 次数/ 就/ 能/ 达到/ 收敛/ ,/ 此外/ 需要/ 的/ 内存空间/ 是/ 固定/ 的/ ,/ 仅/ 与/ 每/ 一小/ 段/ 数据/ 集/ 的/ 大小/ 成/ 比例/ ./ 但/ OGS/ 和/ OVB/ 方法/ 以/ 离线/ GS/ 和/ VB/ 算法/ 为/ 基础/ ,/ 而/ GS/ 算法/ 在/ 近似/ 推理/ 时/ 需要/ 对/ 所有/ 文本/ 中/ 的/ 每个/ 单词/ 训练/ ,/ VB/ 算法/ 中/ 引入/ 了/ 时间/ 复杂度/ 较/ 高/ 的/ digamma/ 函数/ ,/ 这/ 就/ 导致/ OGS/ 和/ OVB/ 算法/ 的/ 精度/ 和/ 速度/ 都/ 有待/ 提高/ ./ 在/ OGS/ 在线/ 学习/ 算法/ 的/ 基础/ 上/ ,/ 文献/ [/ 22/ ]/ 和/ 文献/ [/ 10/ ]/ 提出/ 了/ 基于/ OGS/ 的/ 在线/ 主题/ 追踪/ 算法/ ./ 为了/ 提高/ 主题/ 追踪/ 的/ 精度/ 和/ 速度/ ,/ 本文/ 提出/ 基于/ LDA/ 模型/ 的/ 在线/ 消息传递/ (/ OBP/ )/ 算法/ ,/ 该/ 算法/ 可以/ 将/ 隐藏/ 变量/ 的/ 联合/ 概率分布/ 分解成/ 因子/ 间/ 的/ 乘积/ ,/ 计算/ 其后/ 验/ 边界/ 概率/ 而/ 非/ 后验/ 联合/ 概率/ ,/ 即/ 变量/ 与/ 因子/ Page3/ 之间/ 传递/ 的/ 消息/ ,/ 而/ 消息/ 可以/ 通过/ 本地/ 计算/ 并/ 归一化/ 得到/ ./ 文献/ [/ 23/ ]/ 已/ 提出/ 了/ 基于/ PLSA/ 模型/ 的/ OBP/ 算法/ ,/ 由于/ PLSA/ 模型/ 中/ 包含/ 的/ 参数/ 会/ 随/ 训练/ 文本/ 的/ 不断/ 增加/ 而/ 增加/ ,/ 使得/ 模型/ 的/ 复杂度/ 也/ 不断/ 增加/ ./ 不同于/ 文献/ [/ 23/ ]/ ,/ LDA/ 模型/ 是/ 将/ 参数/ 看作/ 变量/ ,/ 引入/ 了/ 两个/ 语料库/ 级/ 超级/ 参数/ ,/ 使得/ 模型/ 对外/ 始终/ 只有/ 两个/ 参数/ ./ 实验/ 表明/ LDA/ 模型/ 下/ 的/ OBP/ 算法/ 比/ PLSA/ 模型/ 下/ 的/ OBP/ 算法/ 更优越/ ,/ 且/ LDA/ 模型/ 下/ 的/ OBP/ 算法/ 比/ OGS/ 和/ OVB/ 算法/ 更/ 准确/ 和/ 快速/ ./ 更/ 具体/ 的/ 说/ ,/ OBP/ 算法/ 是/ 把/ 海量/ 数据/ 集切/ 分成/ 若干/ 独立/ 小段/ ./ 训练/ 时先/ 随机/ 初始化/ 第/ 1/ 段/ 数据/ 集/ 的/ 参数/ ,/ 训练/ 结束/ 后/ 保存/ 训练/ 结果/ ,/ 而/ 从/ 第/ 2/ 段/ 到/ 最后/ 一段/ ,/ OBP/ 算法/ 将/ 前/ 一段/ 数据/ 集/ 训练/ 的/ 结果/ 作为/ 当前/ 段/ 参数/ 的/ 初始化/ ,/ 然后/ 依次/ 训练/ 每/ 一段/ 数据/ 集/ ./ 对于/ 每/ 段/ 数据/ 集/ ,/ OBP/ 算法/ 使用/ 随机/ 优化/ 方法/ 稳定/ 学习/ 后/ 的/ 参数/ ,/ 确保/ OBP/ 算法/ 收敛/ 到/ 目标/ 函数/ 的/ 局部/ 最优/ ,/ 最后/ 本文/ 从/ 理论/ 方面/ 进一步/ 验证/ 了/ OBP/ 算法/ 的/ 有效性/ ./ 本文/ 第/ 2/ 节/ 简单/ 介绍/ LDA/ 模型/ 及/ 现有/ 在线/ 学习/ 算法/ 分析/ ;/ 第/ 3/ 节/ 介绍/ LDA/ 模型/ 的/ 因子/ 图/ 表示/ ,/ 给出/ 应用/ 于/ 大规模/ 数据/ 集/ 的/ LDA/ 模型/ 的/ OBP/ 算法/ ,/ 并/ 从/ 理论/ 上/ 证明/ OBP/ 算法/ 的/ 收敛性/ ;/ 第/ 4/ 节/ 给出/ OBP/ 算法/ 在/ 4/ 个/ 大规模/ 数据/ 集下/ 实验/ 对比/ 及/ 主题/ 追踪/ 的/ 具体/ 应用/ ;/ 最后/ 一节/ 为/ 总结/ ./ 2LDA/ 及其/ 在线/ 学习/ 算法/ 分析/ 2.1/ LDA/ 模型/ 概述/ LDA/ 模型/ 是/ 将/ 实际/ 可/ 观测/ 的/ “/ 文档/ -/ 词/ ”/ 的/ 高维/ 稀疏/ 空间/ ,/ 通过/ 快速/ 的/ 学习/ 算法/ 降低/ 到/ 低/ 维空间/ ,/ 图/ 1/ 给出/ 了/ LDA/ 模型/ 的/ 3/ 层/ 概率/ 图/ 表示/ ./ LDA/ 模型/ 由/ 单词/ 、/ 文本/ 和/ 语料库/ 3/ 层/ 构成/ ./ 其中/ 单词/ 层/ 包括/ 可/ 观测/ 的/ 单词/ wn/ (/ 1/ / wn/ / W/ )/ 和/ 隐藏/ 主题/ zn/ (/ zn/ =/ k/ ,/ 1/ / k/ / K/ )/ ;/ 文本/ 层/ 包括/ 指定/ 文本/ 所/ 对应/ 的/ 主题/ 分布/ θ/ d/ 和/ 指定/ 主题/ 对应/ 单词表/ 的/ 概率分布/ / k/ 和/ ;/ 语料/ 层/ 包括/ 控制/ 文本/ 层/ θ/ d/ 和/ / k/ 变量/ 的/ α/ 和/ β/ 超参/ ,/ D/ 为/ 语料库/ 中/ 总/ 文档/ 数/ ,/ N/ 为/ 平均/ 每篇/ 文本/ 的/ 单词/ 数/ ,/ W/ 为/ 单词表/ 大小/ ,/ K/ 为/ 总/ 主题/ 数/ ./ 模型/ 中/ 用到/ 的/ 符号/ 标记/ 说明/ 如表/ 1/ 所示/ ./ 在/ 每篇/ 文档/ 只有/ 单一/ 主题/ 的/ 假设/ 前提/ 下/ ,/ 展开/ 下面/ 的/ 分析/ 和/ 讨论/ ./ 1/ / d/ / D1/ / w/ / W1/ / k/ / Kw/ =/ {/ w/ ,/ d/ }/ z/ =/ {/ zw/ ,/ d/ }/ LDA/ 是/ 一个/ 生成/ 模型/ ,/ 即/ 文本/ 可以/ 由/ 多个/ 隐藏/ 主题/ 混合/ 而/ 构成/ ./ 基于/ LDA/ 模型/ ,/ 文本/ 生成/ 的/ 过程/ 如下/ :/ (/ 1/ )/ 根据/ 先验/ 分布/ θ/ d/ ~/ Dirichlet/ (/ α/ )/ ,/ 随机/ 选择/ 一个/ 多项式/ 分布/ θ/ d/ ,/ 其中/ 1/ / d/ / D/ ,/ 确定/ 文本/ 主题/ 分布/ ;/ (/ 2/ )/ 根据/ 先验/ 分布/ / k/ ~/ Dirichlet/ (/ β/ )/ ,/ 随机/ 选择/ 一个/ 多项式/ 分布/ / k/ ,/ 其中/ 1/ / k/ / K/ ,/ 确定/ 该/ 主题/ 下/ 词表/ 中/ 的/ 单词/ 分布/ ;/ (/ 3/ )/ 对/ 文本/ d/ 中/ 的/ 每个/ 单词/ w/ ,/ 1/ / w/ / W/ :/ 首先/ 根据/ zj/ ~/ Discrete/ (/ θ/ d/ )/ 选择/ 一个/ 主题/ zj/ ,/ 然后/ 根据/ wi/ ~/ Discrete/ (/ zj/ )/ 从/ 被/ 选中/ 主题/ 所/ 对应/ 的/ 单词/ 分布/ 中/ 选择/ 一个/ 单词/ wi/ ,/ 其中/ 1/ / i/ / N/ ./ 2.2/ OGS/ 和/ OVB/ 算法/ 分析/ LDA/ 模型/ 的/ 目标/ 是/ 在/ 给定/ 文本/ 数据/ 集/ w/ =/ (/ w1/ ,/ …/ ,/ wN/ )/ 的/ 条件/ 下/ ,/ 推断出/ 文本/ 对应/ 的/ 主题/ 分布/ θ/ ,/ 主题/ 对应/ 单词表/ 的/ 概率分布/ / 和/ 单词/ 所属/ 隐藏/ 主题/ 变量/ z/ =/ (/ z1/ ,/ …/ ,/ zN/ )/ 分布/ ./ 但/ 后验/ 概率分布/ p/ (/ θ/ ,/ / ,/ z/ |/ w/ )/ 的/ 复杂性/ 使得/ 我们/ 不能/ 直接/ 求解/ ,/ 而是/ 通常/ 采用/ 离线/ GS/ 、/ VB/ 、/ BP/ 和/ 在线/ OGS/ 、/ OVB/ 近似/ 推理/ 算法/ ./ 在线/ 算法/ OGS/ 和/ OVB/ 分别/ 以/ 离线/ GS/ 和/ VB/ 算法/ 为/ 基础/ ,/ 而/ GS/ 算法/ 从/ 后验/ 边际/ 概率/ p/ (/ z/ )/ 中/ ,/ 对/ 每个/ 单词/ w/ 采样/ 一个/ 主题/ 标签/ z/ ./ 理论/ 上/ 而言/ ,/ 多次/ 扫描/ 迭代/ 后/ p/ (/ z/ )/ 会/ 收敛/ 到/ 真实/ 后验/ 概率分布/ ./ 由于/ GS/ 算法/ 需对/ 每个/ 单词/ 扫描/ ,/ 当/ 每篇/ 文本/ 中/ 单词/ 数量/ 较大/ ,/ 扫描时间/ 必然/ 增加/ ./ 此外/ ,/ GS/ 算法/ 收敛/ 速度/ 很/ 慢/ ,/ 实际/ 中/ 需要/ 对/ 文档/ -/ 词汇/ 矩阵/ 扫描/ 500/ ~/ 1000/ 次/ 才/ 会/ 收敛/ ./ 因此/ ,/ GS/ 算法/ 无法/ 满足/ 对/ 海量/ 数据/ 的/ 处理/ ./ VB/ 算法/ 利用/ 一个/ 可以/ 分解/ 且/ 方便/ 优化/ 的/ 近似/ 下界/ 函数/ 逼近/ 后验/ 概率函数/ ./ 由于/ VB/ 算法/ 中/ 下界/ 函数/ 与/ 真实/ 目标/ 函数/ 间/ 存在/ 误差/ ,/ 收敛/ 时/ 精度/ 不如/ GS/ 算法/ ./ 因此/ 为了/ 克服/ 这一/ 不足/ ,/ 引入/ 了/ 较/ 复杂/ 的/ digamma/ 函数/ ,/ 但/ 这/ 大大降低/ 了/ VB/ 算法/ 的/ 计算/ 效率/ ,/ 甚至/ 使/ 其/ 收敛/ 速度/ 低于/ GS/ 算法/ ./ 鉴于/ Page4/ 此/ ,/ 文献/ [/ 6/ ]/ 提出/ 了/ 离线/ BP/ 算法/ ,/ 并/ 验证/ 了/ BP/ 算法/ 优于/ 离线/ GS/ 和/ VB/ 算法/ ./ 针对/ 海量/ 数据/ 训练/ ,/ 直接/ 利用/ 在线/ OGS/ 和/ OVB/ 算法/ ,/ 其/ 精度/ 和/ 速度/ 都/ 有待/ 改善/ ./ 本文/ 借助/ 基于/ 离线/ GS/ 和/ VB/ 算法/ 的/ 在线/ OGS/ 和/ OVB/ 算法/ 构建/ 思想/ ,/ 提出/ 了/ 基于/ 离线/ BP/ 的/ 在线/ OBP/ 算法/ ./ 3LDA/ 模型/ 在线/ 消息传递/ 算法/ 3.1/ LDA/ 模型/ 的/ 因子/ 图/ 表示/ 离线/ 消息传递/ BP/ 算法/ [/ 7/ ]/ 是/ 一种/ 从/ 马尔可夫/ 框架/ 推导/ 出/ 的/ 新颖/ 近似/ 推理方法/ ,/ 为了/ 推理/ 消息传递/ ,/ Zeng/ 等/ 人/ [/ 7/ ]/ 将/ 传统/ LDA/ 模型/ 的/ 概率/ 图/ 表示/ (/ 图/ 1/ )/ 转变/ 为/ 因子/ 图/ 表示/ (/ 图/ 2/ )/ ./ 离线/ BP/ 算法/ 将/ 主题/ 模型/ 视为/ 贴标签/ 问题/ ,/ 即/ 为/ 单词表/ 中/ 所有/ 单词/ 索引/ w/ =/ {/ w/ ,/ d/ }/ 赋予/ 语义/ 标签/ z/ =/ {/ zw/ ,/ d/ }/ ./ 在/ 无/ 向/ 概率/ 图/ 模型/ 中/ ,/ 马尔可夫/ 模型/ 可以/ 借助/ 邻居/ 系统/ 和/ 团/ 势函数/ ,/ 基于/ 最大化/ 后验/ 估计/ 获得/ 的/ 最大化/ 后验/ 概率/ 指派/ 最佳/ 主题/ 标签/ ./ 因此/ 因子/ 图/ 表示/ 的/ LDA/ 模型/ 的/ 离线/ BP/ 算法/ ,/ 首先/ 定义/ 主题/ 标签/ zw/ ,/ d/ 的/ 邻居/ 系统/ z/ -/ w/ ,/ d/ 和/ zw/ ,/ -/ d/ ,/ 其中/ z/ -/ w/ ,/ d/ 表示/ 除/ w/ 外/ ,/ 文本/ d/ 中/ 所有/ 单词/ 的/ 主题/ 标签/ ,/ zw/ ,/ -/ d/ 表示/ 除/ 文本/ d/ 外/ ,/ 单词/ w/ 在/ 所有/ 文本/ 中/ 的/ 主题/ 标签/ ;/ 其次/ 设置/ 合适/ 的/ 团/ 势函数/ ,/ 以/ 惩罚/ 或/ 奖励/ 邻居/ 系统/ 中/ 不同/ 的/ 局部/ 标签/ ,/ 从而/ 实现/ 主题/ 模型/ 的/ 3/ 个/ 本质/ 假设/ :/ 共现/ 、/ 平滑/ 和/ 聚集/ [/ 7/ ]/ ./ 图/ 2/ 中/ 方框/ 表示/ 因子/ θ/ d/ 和/ / w/ ,/ 圆圈/ 表示/ 的/ zw/ ,/ d/ 是/ 两个/ 因子/ 间/ 的/ 连接/ 变量/ ./ 由于/ 图/ 1/ 和/ 图/ 2/ 具有/ 相同/ 的/ 邻居/ 系统/ 、/ 相同/ 的/ 连接/ 隐藏/ 变量/ 以及/ 团/ 势函数/ ,/ 因此/ 从/ 主题/ 模型/ 角度/ 而言/ ,/ 图/ 2/ 与/ 图/ 1/ 等价/ ./ 3.2/ LDA/ 模型/ 的/ BP/ 算法/ 在/ LDA/ 模型/ 中/ ,/ BP/ 算法/ 不/ 直接/ 求后验/ 分布/ p/ (/ z/ |/ w/ )/ ,/ 而是/ 求/ 边缘/ 概率/ p/ (/ zw/ ,/ d/ )/ ,/ 即/ 消息/ μ/ (/ zw/ ,/ d/ )/ ./ 消息/ μ/ (/ zw/ ,/ d/ )/ 等于/ 邻居/ 系统/ 的/ 消息/ ,/ 即/ μ/ (/ zw/ ,/ d/ )/ ∝/ μ/ θ/ d/ →/ zw/ ,/ d/ (/ zw/ ,/ d/ )/ ×/ μ/ / w/ →/ zw/ ,/ d/ (/ zw/ ,/ d/ )/ (/ 1/ )/ 其中/ 箭头/ 方向/ 为/ 消息传递/ 方向/ ./ 为了/ 描述/ 简单/ ,/ 下面/ 均/ 用/ μ/ (/ zw/ -/ ,/ d/ )/ =/ ∑/ v/ ≠/ w/ μ/ (/ zv/ ,/ d/ )/ ,/ μ/ (/ z/ ·/ ,/ d/ )/ =/ ∑/ w/ μ/ (/ zw/ ,/ d/ )/ ,/ μ/ (/ zw/ ,/ 珔/ d/ )/ =/ ∑/ s/ ≠/ d/ μ/ (/ zw/ ,/ s/ )/ ,/ μ/ (/ zw/ ,/ ·/ )/ =/ ∑/ d/ μ/ (/ zw/ ,/ d/ )/ 来/ 代替/ ./ 因子/ 传递/ 给/ 变量/ 的/ 消息/ 是/ 所有/ 邻居/ 变量/ 传入/ 消息/ 的/ 叠加/ ,/ 即/ 基于/ 马尔可夫/ 主题/ 平滑/ 先验/ ,/ 本文/ 设/ 因子/ 函数/ 为/ 为了/ 便于/ 文本/ 间/ 的/ 可比性/ ,/ 等式/ (/ 4/ )/ 用/ 文本/ d/ 所有/ 主题/ 的/ 消息/ 归一化/ 了/ 传入/ 消息/ ./ 同理/ ,/ 为了/ 单词/ 间/ 的/ 可比性/ ,/ 等式/ (/ 5/ )/ 用/ 单词表/ 中/ 所有/ 单词/ 归一化/ 了/ 传入/ 消息/ 上/ ./ 因此/ 消息/ 更新/ 等式/ 可写/ 为/ μ/ (/ zw/ ,/ d/ =/ k/ )/ ∝/ μ/ (/ zw/ -/ ,/ d/ =/ k/ )/ +/ α/ 其中/ ∑/ k/ μ/ (/ zw/ ,/ d/ =/ k/ )/ 实际/ 为/ ∑/ zw/ ,/ d/ =/ k/ μ/ (/ zw/ ,/ d/ =/ k/ )/ ,/ 为了/ 简洁性/ ,/ 用/ ∑/ k/ μ/ (/ zw/ ,/ d/ =/ k/ )/ 来/ 表示/ ./ 对/ 更新/ 的/ 消息/ 归一化/ ,/ 即/ ∑/ k/ μ/ (/ zw/ ,/ d/ =/ k/ )/ =/ 1/ ./ 然后/ 更新/ 参数/ θ/ d/ 和/ / w/ ,/ 直到/ 最大/ 循环/ 次数/ :/ 3.3/ LDA/ 模型/ 在线/ 消息传递/ 算法/ 鉴于/ LDA/ 模型/ 中/ 离线/ 算法/ 内存空间/ 随/ 数据/ 集/ 大小/ 的/ 增长/ 而/ 不断/ 增加/ ,/ 不能/ 用于/ 处理/ 海量/ 数据/ 集/ ./ 仿照/ OGS/ 和/ OVB/ 在线/ 算法/ 构建/ 思路/ ,/ 我们/ 提出/ 了/ 在线/ OBP/ 算法/ 来/ 估计/ LDA/ 模型/ 中/ 的/ 参数/ ./ 图/ 3/ 给出/ 了/ 在线/ 学习/ 的/ 主要/ 思想/ ./ OBP/ 算法/ 将/ 整个/ 数据/ 集切/ 分成/ 一系列/ 小/ 段/ ,/ 对于/ 第/ 1/ 段/ 数据/ 集/ ,/ OBP/ 算法/ 和/ 离线/ BP/ 算法/ 相同/ ,/ 从/ 第/ 2/ 段/ 到/ 最后/ 一段/ ,/ OBP/ 算法/ 先/ 固定/ 前/ 一段/ 的/ 参数/ / w/ (/ k/ )/ ,/ 然后/ 计算/ 当前/ 段/ 的/ 消息/ ./ 当/ OBP/ 算法/ 收敛/ 或/ 达到/ 最大/ 迭代/ 次数/ 时/ 更新/ 参数/ / w/ (/ k/ )/ ./ 根据/ 在线/ 随机/ 优化/ 理论/ ,/ 文中/ 权重/ 函数/ 选用/ 指数函数/ 形式/ :/ 并且/ 当前/ 段/ 及/ 已/ 训练/ 段/ 结果/ 分别/ 设置/ 权重/ 为/ ρ/ t/ 和/ Page51/ -/ ρ/ t/ ,/ 其中/ 参数/ κ/ ∈/ (/ 0.5/ ,/ 1/ ]/ 控制/ 已经/ 处理/ 过/ 的/ 数据/ 集/ ,/ 参数/ τ/ 0/ / 0/ 用于/ 减小/ 每/ 段/ 开始/ 迭代/ 时/ 的/ 影响/ ./ 图/ 3/ 中当/ S/ =/ D/ 且/ κ/ =/ 0/ 时/ ,/ 在线/ 消息传递/ 算法/ 即/ 转化/ 为/ 离线/ 消息传递/ 算法/ ./ 在/ 训练/ 中/ ,/ 首先/ 在/ 0/ 与/ 1/ 之间/ 随机/ 初始化/ 第/ 1/ 段/ 参数/ / w/ (/ k/ )/ 、/ μ/ w/ ,/ d/ (/ k/ )/ 和/ θ/ d/ (/ k/ )/ ,/ 为了/ 简洁/ ,/ 记/ μ/ (/ zw/ ,/ d/ =/ k/ )/ =/ μ/ w/ ,/ d/ (/ k/ )/ 训练/ 结束/ 后/ 保存/ 参数/ / w/ (/ k/ )/ ./ 从/ 第/ 2/ 段/ 到/ 最后/ 一段/ ,/ 只/ 需/ 随机/ 初始化/ θ/ d/ (/ k/ )/ 参数/ ,/ 固定/ 参数/ / w/ (/ k/ )/ ,/ 更新/ 消息/ 直到/ 收敛/ ./ 在/ 该/ 算法/ 中/ ,/ 根据/ θ/ d/ (/ k/ )/ 的/ 差值/ 来/ 判断/ 当前/ 段/ 是否/ 收敛/ ,/ 也/ 可以/ 采用/ 等价/ 的/ μ/ w/ ,/ d/ 的/ 差值/ 来/ 判断/ ,/ 因为/ 在/ 更新/ 参数/ μ/ w/ ,/ d/ 时/ 是/ 固定/ 参数/ / w/ (/ k/ )/ ./ 基于/ 收敛/ 后/ 的/ 消息/ ,/ 估计/ 参数/ / w/ (/ k/ )/ new/ ∝/ (/ μ/ w/ ,/ ·/ (/ k/ )/ +/ β/ )/ ∑/ w/ 数/ / w/ (/ k/ )/ ,/ 取/ 当前/ 段/ 和/ 已/ 训练/ 段/ 的/ 权重/ 和/ :/ / w/ (/ k/ )/ =/ (/ 1/ -/ ρ/ t/ )/ / w/ (/ k/ )/ +/ ρ/ t/ / w/ (/ k/ )/ new/ (/ 10/ )/ 从式/ (/ 10/ )/ 可以/ 看出/ ,/ 参数/ / w/ (/ k/ )/ 是/ 对/ 当前/ 段/ 及/ 所有/ 已经/ 训练/ 段/ 的/ 结果/ 进行/ 权重/ 加/ 和/ ,/ 因此/ ,/ 距离/ 当前/ 段/ 越/ 远/ 的/ 数据/ 段/ 被/ 乘/ 了/ 多重/ 权重/ 因子/ ,/ 对/ 当前/ 数据/ 段/ 的/ 影响/ 也/ 越/ 小/ ;/ 相反/ 距离/ 当前/ 段/ 越近/ 的/ 数据/ 段/ 对/ 当前/ 段/ 的/ 影响/ 也/ 就/ 越/ 大/ ./ 所以/ 当/ 处理/ 数据/ 内容/ 信息/ 分布/ 不/ 一致/ 的/ 实时/ 数据流/ 时/ ,/ 也/ 能/ 根据/ 相邻/ 数据/ 段/ 的/ 内容/ 比较/ 快速/ 的/ 给出/ 正确/ 的/ 主题/ 信息/ ./ OBP/ 算法/ 的/ 时间/ 和/ 空间/ 复杂性/ 相比/ OGS/ 和/ OVB/ 算法/ 而言/ 都/ 是/ 最小/ 的/ ./ OBP/ 算法/ 在/ 每次/ 迭代/ 过程/ 中/ 只/ 计算/ 各个/ 单词/ 间/ 的/ 消息传递/ ,/ OGS/ 算法/ 却/ 要/ 对/ 所有/ 文本/ 中/ 的/ 每个/ 单词/ 计算/ ,/ 因此/ OBP/ 算法/ 、/ OGS/ 算法/ 和/ OVB/ 算法/ 的/ 时间/ 复杂度/ 分别/ 为/ O/ (/ KDWDT/ )/ 、/ O/ (/ KDNDT/ )/ 和/ O/ (/ KDWDT/ )/ ,/ 其中/ K/ 是/ 主题/ 数/ ,/ D/ 是/ 当前/ 段/ 的/ 文本/ 数/ ,/ WD/ 是/ 单词表/ 大小/ ,/ 而/ ND/ 是/ 每篇/ 文本/ 的/ 单词/ 数/ ,/ T/ 是/ 迭代/ 收敛/ 次数/ ./ 尽管/ OVB/ 算法/ 和/ OBP/ 算法/ 的/ 时间/ 复杂度/ 相同/ ,/ 但/ OVB/ 算法/ 因/ 引入/ 了/ 非常/ 耗时/ 的/ digamma/ 函数/ ,/ 基于/ 文本/ 的/ 稀疏/ 性/ WD/ 通常/ 远/ 小于/ ND/ ,/ 所以/ 每次/ 迭代/ 时间/ ,/ OBP/ 算法/ 少于/ OGS/ 和/ OVB/ 算法/ ./ 此外/ ,/ OGS/ 、/ OVB/ 和/ OBP/ 算法/ 的/ 空间/ 复杂度/ 分别/ 是/ O/ (/ KND/ +/ KS/ )/ 、/ O/ (/ KND/ +/ KS/ )/ 和/ O/ (/ KWD/ +/ KS/ )/ ,/ 其中/ S/ 是/ 每段/ 数据/ 集中/ 的/ 文本/ 数/ ./ 因此/ OBP/ 算法/ 相对/ OGS/ 和/ OVB/ 的/ 空间/ 复杂度/ 也/ 是/ 最小/ 的/ ./ OBP/ 算法/ 见/ 算法/ 1/ ./ 算法/ 1/ ./ 在线/ 消息传递/ 算法/ ./ 输入/ :/ / w/ ,/ θ/ d/ 和/ μ/ w/ ,/ d/ 输出/ :/ / w/ 和/ θ/ d/ 定义/ ρ/ t/ =/ (/ τ/ 0/ +/ t/ )/ -/ κ/ 随机/ 初始化/ 并/ 归一化/ 第/ 1/ 段/ 参数/ / w/ 和/ μ/ w/ ,/ dFort/ =/ 0todo/ 初始化/ 当前/ 段/ θ/ drepeat/ μ/ t/ +/ 1until1/ 计算/ 当前/ 段/ 数据/ 集/ / w/ (/ k/ )/ new/ ∝/ (/ μ/ w/ ,/ ·/ (/ k/ )/ +/ β/ )/ ∑/ wEndfor3/ ./ 4LDA/ 模型/ 的/ 收敛性/ 分析/ 给定/ 文本/ 数据/ 集/ w/ =/ (/ w1/ ,/ …/ ,/ wN/ )/ ,/ LDA/ 模型/ 推断/ 文本/ 对应/ 主题/ 分布/ θ/ 、/ 主题/ 对应/ 单词表/ 概率分布/ / 和/ 单词/ 所属/ 隐藏/ 主题/ 变量/ z/ =/ (/ z1/ ,/ …/ ,/ zN/ )/ 分布/ ./ 算法/ 1/ 则/ 可以/ 收敛/ 到/ 一个/ 稳定/ 值/ ,/ 下面/ 给出/ 证明/ ./ L/ (/ w/ ,/ / ,/ θ/ ,/ μ/ )/ =/ ∏/ dLDA/ 模型/ 的/ 目标/ 函数/ 为/ ∝/ ∑/ d/ =/ ∑/ (/ d/ =/ ∑/ d/ 因此/ ,/ 用/ μ/ (/ nd/ ,/ / )/ 代表/ μ/ d/ 和/ θ/ (/ nd/ ,/ / )/ 代表/ θ/ d/ 计算/ μ/ 和/ Page6/ θ/ ./ 最大化/ L/ (/ n/ ,/ / )/ =/ ∑/ d/ 可以/ 通过/ 估计/ 参数/ / 来/ 实现/ ./ 在线/ OBP/ 算法/ 收敛性/ 可用/ 随机/ 自然/ 梯度/ 下降/ 的/ 方法/ 来/ 分析/ ./ 在/ 随机/ 最优/ 算法/ 中/ 最优化/ 目标/ 函数/ 一般/ 用/ 梯度/ 估计/ 来/ 完成/ ./ 首先/ 定义/ 不断/ 采样/ 文本/ 函数/ s/ (/ n/ )/ =/ 1/ 否则/ I/ (/ n/ =/ nd/ )/ =/ 0/ ./ 因此/ 似然/ 目标/ 函数/ 可/ 重写/ 为/ L/ (/ s/ ,/ / )/ =/ DEs/ [/ / (/ n/ ,/ μ/ (/ n/ ,/ / )/ ,/ θ/ (/ n/ ,/ / )/ ,/ / )/ / ]/ (/ 12/ )/ 其中/ / 定义/ 见/ 等式/ (/ 11/ )/ ./ 给定/ / ,/ 等式/ (/ 11/ )/ 的/ 最大化/ ,/ 可/ 通过/ nt/ ~/ s/ 不断/ 采样/ 观测/ 样本/ ,/ μ/ t/ =/ μ/ (/ nt/ ,/ / )/ ,/ θ/ t/ =/ θ/ (/ nt/ ,/ / )/ 来/ 实现/ ,/ 因此/ 更新/ / 参数/ 为/ 其中/ 权重/ ρ/ t/ =/ (/ τ/ 0/ +/ t/ )/ -/ κ/ ./ 对/ 每篇/ 文本/ nt/ ,/ 固定/ 参数/ / ,/ 将/ μ/ t/ 和/ θ/ t/ 参数均/ 看作/ 随机变量/ ,/ 则/ 有/ Ε/ s/ [/ D/ / / / (/ nt/ ,/ μ/ t/ ,/ θ/ t/ ,/ / )/ |/ / ]/ =/ / / ∑/ d/ ∑/ t/ =/ 0/ ρ/ 2/ θ/ d/ ,/ / )/ 会/ 收敛/ 到/ 0/ [/ 7/ ]/ ,/ 因此/ / 将会/ 收敛/ 到/ 某个/ 稳定/ 值/ ./ 等式/ (/ 13/ )/ 中/ 只用/ 了/ 一/ 阶梯/ 度/ ./ 若/ 对/ 梯度/ 乘以/ 一个/ 合适/ 的/ 正定/ 矩阵/ 犎/ 的/ 逆/ ,/ 可/ 加速/ 随机/ 梯度/ 算法/ ,/ 常用/ 正定/ 矩阵/ 犎/ 是/ 目标/ 函数/ 的/ 哈森/ 矩阵/ [/ 7/ ]/ ./ t/ </ ,/ 参数/ / 收敛/ ,/ 并且/ 梯度/ / / ∑/ d/ / / (/ nt/ ,/ μ/ t/ ,/ θ/ t/ ,/ / )/ / / k/ ,/ w/ 对/ 等式/ (/ 13/ )/ 乘以/ ρ/ tD/ 再/ 加上/ / ,/ 便/ 得到/ 算法/ 1/ 中/ 参数/ / 的/ 更新/ 等式/ ./ 4/ 实验/ 结果/ 与/ 分析/ 实验/ 采用/ 4/ 组/ 海量/ 数据/ 集/ :/ 美国/ 政坛/ 领域/ 关于/ 政治/ 博客/ blog/ [/ 24/ ]/ 、/ 邮件/ enron/ [/ 25/ ]/ 、/ 新闻/ 摘要/ nytimes/ [/ 25/ ]/ 和/ 摘要/ pubmed/ [/ 25/ ]/ ./ 4/ 个/ 数据/ 集/ 大小/ 如表/ 2/ 所示/ ,/ 其中/ D/ 为/ 数据/ 集总/ 文本/ 数/ ,/ W/ 为/ 数据/ 集/ 对应/ 单词表/ 的/ 总/ 单词/ 数/ ./ 在/ 训练/ 前先/ 打乱/ 重排/ 数据/ 集/ ,/ Train/ 为/ 训练/ 文档/ 数/ ,/ Test/ 为/ 测试/ 文档/ 数/ ./ 所有/ 实验/ 迭代/ 次数/ 为/ 500/ ,/ 实际/ 迭代/ 次数/ 以/ 模型/ 收敛/ 为止/ ,/ 主题/ 数均/ 为/ 10/ ~/ 50/ ,/ 步长/ 为/ 10/ ./ 在/ CPU/ 为/ 两个/ 6/ 核/ 、/ 频率/ 为/ 3.46/ GHz/ 和/ 内存/ 128GB/ 的/ SunFireX4270M2/ 服务器/ 下用/ MATLAB/ [/ 26/ ]/ 和/ MEX/ C++/ 获得/ 实验/ ./ D5177398613000008200000W3357428102102660141043Train4500360002500008000000Test677386150000200000/ 为/ 验证/ LDA/ 模型/ 下/ OBP/ 算法/ 的/ 高效性/ 和/ 准确性/ ,/ 在/ 4/ 个/ 数据/ 集上/ 比较/ 了/ OBP/ 和/ OGS/ 及/ OVB/ 算法/ 的/ 混淆/ 度/ [/ 5/ ]/ 和/ 训练/ 消耗/ 时间/ ,/ OBP/ 算法/ 在/ PLSA/ 模型/ 和/ LDA/ 模型/ 上/ 的/ 实验/ 对比/ ,/ 且/ 给出/ 了/ nytimes/ 数据/ 集上/ 主题/ 随/ 训练/ 数据/ 集/ 变化/ 的/ 演变/ 图/ ./ 4.1/ 评估/ 学习/ 参数/ 在线/ 学习/ 算法/ 的/ 权重/ 函数/ [/ 12/ ]/ 中/ 引入/ 了/ 3/ 个/ 学习/ 参数/ 、/ 控制/ 已/ 训练/ 数据/ 段/ 被/ 遗忘/ 的/ 缓慢/ 程度/ κ/ ∈/ (/ 0.5/ ,/ 1/ ]/ 参数/ 、/ 用于/ 降低/ 每/ 段/ 数据/ 集/ 起始/ 迭代/ 结果/ 影响/ 的/ 常数/ τ/ 0/ (/ τ/ 0/ / 0/ )/ ,/ 和/ 限制/ 切分/ 后/ 每段/ 数据/ 集/ 文本/ 数/ 参数/ S/ ./ LDA/ 模型/ 在线/ 算法/ 训练/ 结果/ 与/ 3/ 个/ 学习/ 参数/ 的/ 有效性/ 密切相关/ ,/ 通常/ 对于/ S/ 值/ 的/ 选取/ 是/ 在/ 内存容量/ 范围/ 内越/ 大越/ 好/ ;/ 若/ S/ =/ D/ ,/ 则/ 在线/ 算法/ 等价/ 于/ 传统/ 的/ 离线/ 算法/ ./ 表/ 3/ 给出/ 了/ enron/ 数据/ 集上/ 最佳/ 参数值/ 的/ 选取/ 和/ 测试/ 集/ 预测/ 混淆/ 度值/ ./ 从表/ 3/ 中/ 不同/ 参数值/ 组/ 的/ 实验/ 分析/ 可以/ 看出/ ,/ 当/ κ/ =/ 0.6/ 和/ τ/ 0/ =/ 1/ 时/ ,/ 测试/ 混淆/ 度值/ 最小/ ./ 为了/ 更好/ 的/ 对比/ LDA/ 模型/ 的/ 不同/ 近似/ 推理/ 算法/ ,/ 必须/ 在/ 相同/ 参数/ 条件/ 下/ 进行/ ,/ 为此/ 固定/ κ/ =/ 0.6/ 对/ 不同/ 参数值/ 组/ 实验/ ,/ 对/ 参数/ 的/ 选取/ 进行/ 了/ 对比/ (/ 表/ 4/ )/ ./ 从表/ 4/ 中/ 可以/ 看出/ ,/ 当/ τ/ 0/ 越小/ ,/ S/ 越大/ ,/ 对应/ 的/ 预测/ 混淆/ 度值/ 也/ 越/ 小/ ./ 因此/ ,/ 本文/ 所有/ 实验/ 均/ 选用/ κ/ =/ 0.6/ 和/ τ/ 0/ =/ 1/ ,/ 而/ S/ 值/ 的/ 选取/ 则/ 根据/ 具体/ 数据/ 集/ 的/ 大小/ 确定/ ./ 参数/ κ/ τ/ 0S/ 混淆/ 度组/ 10.9102445554/ ./ 1/ 组/ 20.81024165490/ ./ 6/ 组/ 30.71024643817/ ./ 9/ 组/ 40.62562562936/ ./ 2/ 组/ 50.66410242353/ ./ 6/ 组/ 60.6/ 组/ 70.6/ Page7/ 参数/ κ/ τ/ 0S/ 混淆/ 度组/ 10.610242563045/ ./ 2/ 组/ 20.62562562936/ ./ 2/ 组/ 30.625610242472/ ./ 7/ 组/ 40.66410242353/ ./ 6/ 组/ 50.66440002151/ ./ 0/ 组/ 60.6/ 组/ 70.64/ ./ 2/ 算法/ 自身/ 性能/ 分析/ 在/ 假设/ 海量/ 数据/ 分段/ 后/ ,/ 后/ 段/ 权重/ 训练/ 依赖/ 前段/ 训练/ 结果/ 的/ 前提/ 下/ ,/ 采用/ 提出/ 的/ OBP/ 算法/ 对/ 海量/ 图/ 4/ 海量/ 数据/ 集/ 前段/ 训练/ 结果/ 对后/ 段/ 的/ 影响/ 4.3/ LDA/ 模型/ 不同/ 算法/ 的/ 对比/ 分析/ 下面/ 给出/ LDA/ 模型/ 下/ ,/ OBP/ 与/ OGS/ 及/ OVB/ 算法/ 在/ 4/ 个/ 数据/ 集/ blog/ ,/ enron/ ,/ nytimes/ 和/ pubmed/ 上/ 混淆/ 度/ 和/ 训练/ 耗时/ 的/ 对比/ 分析/ ./ 图/ 5/ 给出/ 了/ OBP/ 、/ OGS/ 及/ OVB/ 算法/ 的/ 混淆/ 度/ 对比/ 分析/ 结果/ ./ 在/ 4/ 个/ 大规模/ 数据/ 集上/ ,/ OBP/ 算法/ 的/ 混淆/ 度均/ 低于/ OGS/ 和/ OVB/ 算法/ ,/ 这/ 说明/ 相对/ 于/ OGS/ 和/ OVB/ 算法/ ,/ 用/ OBP/ 算法/ 训练/ LDA/ 模型/ ,/ 具有/ 更好/ 的/ 预测/ 性能/ ./ 图/ 6/ 也/ 给出/ 了/ 3/ 种/ 在线/ 算法/ 训练/ 时间/ 的/ 对比/ 分析/ ./ 由于/ digamma/ 函数/ 的/ 引入/ ,/ OVB/ 算法/ 训练/ 非常/ 耗时/ ,/ 图/ 6/ 中所/ 给/ OVB/ 算法/ 的/ 时间/ 是/ 其/ 真实/ 时间/ 的/ 0.3/ 倍/ ./ OBP/ 算法/ 相比/ OGS/ 图/ 5/ 主题/ 上/ 混淆/ 度/ 的/ 对比/ 数据/ 进行/ 训练/ ./ 为/ 验证/ 这一/ 假设/ 的/ 准确性/ ,/ 采用/ 混淆/ 度/ 和/ 训练/ 时间/ 两个/ 评价/ 指标/ ,/ 在/ nytimes/ 数据/ 集上/ 分别/ 进行/ 了/ 权重/ 依赖/ rely/ 和/ 完全/ 独立/ independent/ 的/ 实验/ ,/ 实验/ 结果/ 如图/ 4/ 所示/ ./ 其中/ 混淆/ 度/ 是/ 评价/ 用/ 训练/ 数据/ 集/ 训练/ 所/ 得到/ 的/ 结果/ 来/ 预测/ 测试/ 集/ 的/ 一个/ 客观/ 指标/ ,/ 值越/ 小/ 表明/ 对/ 未知/ 测试/ 集/ 的/ 预测/ 能力/ 越好/ ./ 从图/ 4/ 中/ 可以/ 看出/ ,/ 完全/ 独立/ 实验/ 不仅/ 模型/ 预测/ 集/ 的/ 混淆/ 度值/ 大/ ,/ 而且/ 训练/ 也/ 更加/ 耗时/ ,/ 若后/ 段/ 依赖/ 前段/ 训练/ 结果/ ,/ 模型/ 收敛/ 所/ 需/ 迭代/ 次数/ 减少/ ,/ 相对/ 耗时/ 少/ ./ 实验/ 表明/ ,/ OBP/ 算法/ 对/ 海量/ 数据/ 集/ 的/ 分段/ 假设/ 在/ 训练/ LDA/ 模型/ 上/ 是/ 可行/ 的/ ./ 和/ OVB/ 算法/ 都/ 要/ 快速/ ./ 因此/ ,/ 在/ LDA/ 模型/ 中/ ,/ OBP/ 算法/ 相对/ 于/ OGS/ 和/ OVB/ 算法/ 更加/ 的/ 高效/ 和/ 准确/ ./ 4.4/ OBP/ 算法/ 在/ LDA/ 模型/ 与/ PLSA/ 模型/ 上/ 的/ 对比/ 分析/ 文献/ [/ 23/ ]/ 中/ 已经/ 提出/ 了/ 基于/ PLSA/ 模型/ 因子/ 图/ 的/ OBP/ 算法/ ,/ 并/ 给出/ 了/ 算法/ 的/ 具体/ 实现/ 过程/ 以及/ 算法/ 收敛/ 条件/ ./ 从/ 主题/ 模型/ 的/ 角度/ ,/ PLSA/ 模型/ 中/ 参数/ 的/ 个数/ 会/ 随/ 训练/ 文本/ 数/ 的/ 增加/ 而/ 不断/ 增加/ ,/ 从而/ 导致/ 在/ 训练/ 海量/ 数据/ 时/ 模型/ 中/ 参数/ 的/ 个数/ 较为/ 庞大/ ./ LDA/ 模型/ 是/ 在/ PLSA/ 模型/ 的/ 基础/ 上/ 提出/ 来/ 的/ ,/ 是/ 一个/ 层次/ 贝叶斯/ 模型/ ,/ 模型/ 中将/ 参数/ 看作/ 随机变量/ ,/ 并且/ 引入/ 了/ 控制参数/ 的/ 参数/ ,/ 即/ 语料库/ 级/ 超参/ ,/ Page8/ 图/ 6/ 主题/ 上/ 训练/ 耗时/ 的/ 对比/ 因此/ LDA/ 模型/ 对外/ 表现/ 出/ 的/ 参数/ 始终/ 只有/ 两个/ 超参/ ,/ 有效/ 地/ 减少/ 了/ 模型/ 中/ 参数/ 个数/ ./ 图/ 7/ 和/ 图/ 8/ 分别/ 给出/ 了/ enron/ 和/ nytimes/ 数据/ 集在/ PLSA/ 和/ LDA/ 模型/ 上/ 训练/ 混淆/ 度/ 和/ 训练/ 耗时/ 对比/ ./ 从图/ 7/ 和/ 图/ 8/ 可以/ 明显/ 的/ 看出/ ,/ 在/ 相同/ 训练/ 数据/ 集上/ ,/ LDA/ 模型/ 不仅/ 训练/ 的/ 混淆/ 度值/ 低于/ PLSA/ 模型/ ,/ 而且/ 训练/ 所/ 消耗/ 的/ 时间/ 也/ 远/ 少于/ PLSA/ 模型/ ./ 这/ 就/ 从/ 实验/ 上/ 进一步/ 验证/ 了/ LDA/ 模型/ 下/ 的/ OBP/ 算法/ 比/ PLSA/ 模型/ 的/ OBP/ 算法/ 预测/ 的/ 更加/ 精确/ ./ 4.5/ 主题/ 追踪/ 主题/ 追踪/ 的/ 目标/ 是/ 针对/ 不断/ 增长/ 的/ 数据流/ ,/ 追踪/ 某个/ 给定/ 主题/ 随/ 时间/ 的/ 不断/ 变化/ ./ 基本思路/ 是/ ,/ 根据/ 给定/ 的/ 训练/ 文本/ ,/ 采用/ 主题/ 模型/ 的/ 近似/ 推理/ 算法/ 对/ 训练/ 文本/ 进行/ 学习/ ,/ 得到/ 每篇/ 文本/ 属于/ 各个/ 主题/ 的/ 概率/ 以及/ 各个/ 主题/ 对应/ 单词表/ 的/ 概率分布/ ./ 当新/ 的/ 流/ 数据/ 到来/ 时/ ,/ 按照/ 主题/ 模型/ 已经/ 训练/ 的/ 结果/ 对/ 新/ 数据/ 进行/ 预测/ ,/ 一方面/ 预测/ 新/ 数据/ 中/ 每篇/ 文本/ 属于/ 各/ 主题/ 的/ 概率/ 值/ ;/ 另一方面/ 同时/ 更新/ 各/ 主题/ 所/ 对应/ 单词表/ 的/ 概率分布/ ./ 为了/ 更/ 准确/ 给出/ 主题/ 追踪/ 信息/ ,/ 首先/ 验证/ OBP/ 算法/ 在/ LDA/ 模型/ 下/ 预测/ 测试/ 集/ 的/ 准确性/ ./ 图/ 9/ 给出/ 了/ 在/ nytimes/ 数据/ 集上/ ,/ OBP/ 算法/ 和/ OGS/ 及/ OVB/ 算法/ 对/ 测试数据/ 集/ 预测/ 的/ 混淆/ 度/ 对比/ 分析/ 图/ ,/ 实验/ 选取/ nytimes/ 数据/ 集/ 的/ 前/ 25000/ 篇/ 文本/ 作为/ 训练样本/ 集/ ,/ 后/ 50000/ 篇/ 文本/ 作为/ 测试/ 样本/ 集/ ./ 训练/ 前先/ 将/ 训练/ 集切/ 分成/ 10/ 段/ 视为/ 数据流/ ./ 由于/ 目前/ 给定/ 标签/ 的/ 语料库/ 都/ 是/ 小规模/ 的/ 数据/ 集/ ,/ 而且/ 仅/ 有/ 很少/ 的/ 语料库/ 会/ 给定/ 标签/ ,/ 本文/ 采用/ 的/ 4/ 组/ 数据/ 集均/ 未/ 给定/ 标签/ ,/ 主题/ 个数/ 的/ 最佳/ 取值/ 未知/ ,/ 所以/ 文中/ 在/ 进行/ 主题/ 追踪/ 时/ ,/ 主题/ 个数/ 选定/ 为/ K/ =/ 50/ ./ 图/ 9/ 给出/ 了/ nytimes/ 数据/ 集在/ OGS/ ,/ OVB/ 和/ OBP/ 三种/ 数据/ 集上/ 训练/ 数据流/ 数据/ 对应/ 的/ 预测/ 混淆/ 度值/ 对比/ ,/ 其中/ 横坐标/ 表示/ 当前/ 已经/ 训练/ 过/ 的/ 总/ 文本/ 数/ ,/ 纵坐标/ 是/ 当前/ 训练样本/ 对应/ 的/ 测试/ 混淆/ 度/ ./ 从图/ 9/ 中/ 可/ 看出/ ,/ 在/ 整个/ 训练/ 过程/ 中/ ,/ OBP/ 算法/ 的/ 混淆/ 度值/ 均/ 小于/ OGS/ 和/ OVB/ 算法/ ,/ 而且/ 在/ 最终/ 收敛/ 时/ ,/ OBP/ 算法/ 的/ 混淆/ 度值/ 也/ 远/ 小于/ OGS/ 和/ OVB/ 算法/ ,/ 这/ 说明/ 在/ 同一/ 模型/ 和/ 相同/ 训练样本/ 集下/ ,/ OBP/ 算法/ 对/ 未知/ 测试/ 集/ 的/ 预测/ 能力/ 最好/ ./ 此外/ ,/ OBP/ 算法/ 随/ 训练样本/ 数/ 的/ 不断/ 增加/ ,/ 混淆/ 度值/ 降低/ 越/ 迅速/ ,/ 表明/ 收敛/ 速度/ 图/ 9nytimes/ 数据/ 集在/ OBP/ 与/ OGS/ 及/ OVB/ 算法/ 的/ 混淆/ 度/ 对比/ Page9/ 也/ 更/ 快/ ./ 实验/ 表明/ ,/ 基于/ LDA/ 模型/ ,/ OBP/ 算法/ 预测/ 准确性/ 随/ 处理/ 数据流/ 数据/ 的/ 增加/ 而/ 不断/ 上升/ ,/ 即/ 在/ 追踪/ 某个/ 主题/ 时/ ,/ 能/ 更/ 准确/ 的/ 给出/ 与/ 该/ 主题/ 相关/ 的/ 信息/ ./ 由于/ 数据/ 获取/ 的/ 限制/ ,/ 图/ 9/ 给出/ 的/ 实验/ 是/ 将/ 海量/ 数据/ 切分/ 后/ 视为/ 数据流/ 处理/ ,/ 而/ 并/ 不是/ 真的/ 数据流/ ,/ 对/ 同一个/ 数据/ 集/ ,/ 其/ 内部/ 可能/ 是/ 服从/ 一致/ 分布/ ,/ 但/ 对流/ 数据/ ,/ 可能/ 其/ 服从/ 的/ 分布/ 会/ 随/ 时间/ 不断/ 变化/ ./ 本文/ 的/ 在线/ 算法/ 是/ 对/ 当前/ 段/ 和/ 已经/ 得到/ 的/ 结果/ 取/ 权重/ 叠加/ ,/ 而/ 权重/ 均/ 是/ (/ 0/ ,/ 1/ )/ 之间/ 的/ 小数/ ,/ 所以/ 对/ 任意/ 段/ 数据/ ,/ 对/ 其/ 影响/ 最大/ 的/ 是/ 其/ 相邻/ 段/ ,/ 而/ 距/ 当前/ 段/ 很/ 远/ 的/ 数据/ 段/ 由于/ 经过/ 若干段/ 权重/ 的/ 相乘/ ,/ 影响/ 就/ 很小/ ./ 所以/ ,/ 若/ 处理/ 的/ 数据/ 存在/ 不/ 一致/ 分布/ ,/ 则/ 不/ 一致/ 的/ 前/ 几段/ 训练/ 结果/ 可能/ 不/ 准确/ ,/ 但是/ 若干段/ 之后/ 又/ 能/ 准确/ 预测/ ./ 为了/ 验证/ 在/ LDA/ 模型/ 下/ OBP/ 算法/ 能/ 应用/ 于/ 主题/ 追踪/ ,/ 表/ 5/ 给出/ 了/ 模型/ 在/ enron/ 语料库/ 前/ 9/ 个/ 数据/ 段/ 训练/ 后/ 对应/ 主题/ 的/ 变化/ ,/ 由于/ 空间/ 的/ 限制/ ,/ 表/ 5/ 中仅/ 给出/ 了/ 第/ 10/ ,/ 20/ ,/ 30/ 和/ 40/ 个/ 主题/ 在/ 第/ 1/ ,/ 第/ 5/ 和/ 第/ 9/ 段/ 数据/ 集/ 训练/ 之后/ 所/ 包含/ 的/ 单词/ ./ 实验/ 是/ 选取/ enron/ 语料库/ 前/ 36000/ 篇/ 文本/ ,/ 并/ 将/ 其均/ 分成/ 9/ 段/ 数据/ 集视/ 做/ 数据流/ 处理/ ,/ 且/ 模型/ 训练/ 时/ 主题/ 的/ 个数/ 选定/ 为/ 50/ ./ 根据/ 表/ 5/ 中/ 列出/ 的/ 主题/ 在/ 训练/ 若干段/ 数据流/ 之后/ 的/ 不断/ 变化/ ,/ 可以/ 看出/ 每个/ 主题/ 所/ 包含/ 的/ 单词/ 会/ 随/ 时间/ 不断/ 的/ 变化/ ,/ 但/ 始终/ 是/ 围绕/ 当前/ 主题/ 的/ 主旨/ ,/ 如/ 第/ 10/ 个/ 主题/ ,/ 在/ 训练/ 完第/ 1/ 个/ 数据/ 段/ 后/ 包含/ office/ ,/ interview/ 等/ 与/ 工作/ 相关/ 的/ 词/ ,/ 训练/ 第/ 5/ 段/ 和/ 第/ 9/ 段/ 后/ 的/ 主题/ 包含/ 的/ 单词/ 也/ 主要/ 是/ 围绕/ 图/ 10nytimes/ 数据/ 集在/ OBP/ 算法/ 上前/ 25/ 个/ 主题/ 演变/ 过程/ (/ 横坐标/ 为/ 当前/ 训练/ 集所/ 处理/ 的/ 训练/ 段数/ ;/ 纵坐标/ 为/ 各/ 主题/ 在/ 当前/ 数据/ 段/ 所/ 对应/ 的/ 概率/ 值/ ;/ 右侧/ 为/ 当前/ 主题/ 中/ 概率/ 值/ 较大/ 的/ 部分/ 单词/ )/ business/ 浮动/ ./ 因此/ LDA/ 模型/ 下/ 的/ OBP/ 算法/ 可以/ 用于/ 处理/ 数据流/ 数据/ ,/ 并/ 追踪/ 主题/ 的/ 不断/ 变化/ ./ 表/ 5OBP/ 算法/ 在/ enron/ 数据流/ 上/ 的/ 主题/ 变化/ 1/ :/ enaofficegrouplondonprocessinterviewsallyanalystrole5/ :/ teamgroupprocessofficeenasallybusinesssupportforward9/ :/ groupteamofficeprocessbusinesssupportenamanagementbusiness/ _/ unitprogramforward1/ :/ bassericdadrespondweekendlumthinkdprFridaydinner5/ :/ bassemployeesdprrespondericlarryweekendFridaythink9/ :/ respondemployeesFridaybassericdprweekendthinkopendadfloor1/ :/ accountaccesspassworduseronlinepageoasisstatementport/ -/ 5/ :/ pageaccountaccessserviceusercustomeronlinepasswordsite9/ :/ accessaccountpagecustomerserviceuseronlinesitestatementfoliovisitstatementschwab1/ :/ agreementattachedcostsectionplanloanprogramprovidecopynumber5/ :/ agreementplanprovideemployeesattachedissuessectioncostprogramrequired9/ :/ agreementplanprovideissuesproposalcostprogramdirectis/ -/ suesection/ 在/ 处理/ 流/ 数据/ 时/ ,/ LDA/ 模型/ 对/ 每个/ 主题/ 会/ 随流/ 数据/ 训练样本/ 的/ 变化/ 而/ 变化/ ./ 实验/ 中/ 的/ 训练/ 数据/ 是/ 将/ nytimes/ 数据/ 集切/ 分成/ 10/ 段/ 视为/ 数据流/ ,/ 每段/ 25000/ 篇/ 文本/ ./ 图/ 10/ 和/ 图/ 11/ 给出/ 了/ OBP/ 算法/ 的/ LDA/ 模型/ 训练/ 流/ 数据/ 时/ ,/ 50/ 个/ 主题/ 的/ 演变/ 图/ ./ 图/ 10/ 给出/ 了/ 前/ 25/ 个/ 主题/ 演变/ 图/ ,/ 图/ 11/ 给出/ 了/ 后/ 25/ 个/ 主题/ 演变/ 图/ ./ Page10/ 图/ 11nytimes/ 数据/ 集在/ OBP/ 算法/ 上后/ 25/ 个/ 主题/ 的/ 演变/ 过程/ (/ 横坐标/ 为/ 当前/ 训练/ 集所/ 处理/ 的/ 训练/ 段数/ ;/ 纵坐标/ 为/ 各/ 主题/ 在/ 当前/ 数据/ 段/ 所/ 对应/ 的/ 概率/ 值/ ;/ 右侧/ 为/ 当前/ 主题/ 中/ 概率/ 值/ 较大/ 的/ 部分/ 单词/ )/ 从图/ 10/ 和/ 图/ 11/ 可以/ 看出/ ,/ 主题/ 随/ 训练/ 数据/ 集/ 的/ 增加/ 而/ 不断/ 变化/ ,/ 如图/ 10/ 中/ 的/ 第/ 1/ 个/ 主题/ 演变/ 图/ ,/ 其/ 概率/ 值/ 随/ 训练/ 文本/ 的/ 增多/ 而/ 不断/ 变/ 大/ ,/ 表明/ 该/ 主题/ 得到/ 的/ 关注度/ 正在/ 持续上升/ ;/ 图/ 10/ 的/ 第/ 13/ 个/ 主题/ 对应/ 的/ 概率/ 值/ 随/ 训练/ 文本/ 的/ 增加/ 而/ 先/ 变大后/ 变小/ ,/ 表明/ 该/ 主题/ 被/ 研究/ 的/ 热度/ 不断/ 下降/ ;/ 图/ 10/ 的/ 第/ 9/ 个/ 主题/ 随/ 训练/ 文本/ 的/ 增加/ ,/ 概率/ 值/ 变化/ 比较/ 缓慢/ ,/ 表明/ 该/ 主题/ 被/ 关注/ 的/ 程度/ 几乎/ 保持/ 不变/ ./ 每个/ 主题/ 演变/ 图/ 反映/ 了/ 主题/ 随/ 训练样本/ 不断/ 增加/ 的/ 变化/ ,/ 结合/ 所有/ 主题/ 图/ 还/ 可以/ 挖掘出/ 哪些/ 主题/ 是/ 当前/ 的/ 热门话题/ ./ 根据/ 主题/ 演变/ 图/ 能够/ 直接/ 反映/ 出/ 各个/ 主题/ 随/ 时间/ 的/ 变化趋势/ ,/ 追踪/ 与/ 各/ 主题/ 相关/ 的/ 数据/ 信息/ ,/ 所以/ 搜索引擎/ 等/ 可以/ 借助/ 该/ 模型/ 通过/ 对/ 历史数据/ 的/ 不断/ 训练/ ,/ 从而/ 给/ 用户/ 提供/ 更加/ 准确/ 的/ 搜索/ 结果/ ./ 鉴于/ 本文/ 主题/ 模型/ 上/ 的/ 主题/ 定义/ 为/ 单词表/ 上/ 的/ 概率分布/ ,/ 即/ 每个/ 主题/ 是/ 由/ 单词表/ 中/ 所有/ 单词/ 的/ 不同/ 排列/ 构成/ ,/ 而图/ 10/ 和/ 图/ 11/ 的/ 每个/ 主题/ 右侧/ 列出/ 了/ 该/ 主题/ 对应/ 概率/ 值/ 较大/ 的/ 部分/ 单词/ ,/ 所以/ 对/ 每个/ 主题/ 会/ 存在/ 相同/ 的/ 单词/ ./ 5/ 结论/ 本文/ 基于/ LDA/ 模型/ 的/ 因子/ 图/ 提出/ 了/ 在线/ 消息传递/ (/ OBP/ )/ 算法/ ,/ 通过/ 实验/ 验证/ 了/ OBP/ 算法/ 比/ OGS/ 和/ OVB/ 算法/ 有/ 显著/ 提高/ ,/ 且/ LDA/ 模型/ 下/ 的/ OBP/ 算法/ 优越/ 于/ PLSA/ 模型/ 下/ 的/ OBP/ 算法/ ,/ 并/ 将/ 其/ 应用/ 到/ 主题/ 追踪/ 上/ ,/ 获取/ 了/ 更/ 准确/ 的/ 信息/ ./ 

