Page1S/ -/ RAID/ 中/ 基于/ 连续/ 数据/ 特征/ 的/ 写/ 优化/ 策略/ 刘/ 靖宇/ 1/ )/ ,/ 2/ )/ 谭毓安/ 1/ )/ ,/ 3/ )/ 薛静锋/ 1/ )/ 马忠梅/ 1/ )/ 李元章/ 1/ )/ 张/ 全新/ 1/ )/ 1/ )/ (/ 北京理工大学/ 计算机科学/ 与/ 技术/ 学院/ 北京/ 100081/ )/ 2/ )/ (/ 河北/ 工业/ 大学/ 计算机科学/ 与/ 软件/ 学院/ 天津/ 300401/ )/ 3/ )/ (/ 北京理工大学/ 北京市/ 海量/ 语言/ 信息处理/ 与/ 云/ 计算/ 应用/ 工程技术/ 中心/ 北京/ 100081/ )/ 摘要/ 节能型/ 磁盘阵列/ S/ -/ RAID/ 通过/ 对/ 磁盘/ 分组/ ,/ 关闭/ 部分/ 磁盘/ ,/ 降低/ 存储系统/ 部分/ 性能/ 来/ 实现/ 节能/ ./ 为/ 避免/ 启动/ 已/ 关闭/ 磁盘/ 而/ 产生/ 额外/ 能耗/ ,/ S/ -/ RAID/ 中/ 的/ 写/ 操作/ 全部/ 采用/ “/ 读/ -/ 改/ -/ 写/ ”/ 方式/ ,/ 影响/ 了/ S/ -/ RAID/ 的/ 写/ 性能/ ./ 本/ 研究/ 提出/ 一种/ S/ -/ RAID/ 的/ 优化结构/ :/ LS/ -/ RAID/ ,/ 在/ 不/ 提高/ 存储系统/ 能耗/ 的/ 条件/ 下/ ,/ 优化/ S/ -/ RAID/ 的/ 写/ 性能/ ./ LS/ -/ RAID/ 适用/ 于/ 以/ 连续/ 访问/ 为主/ 的/ 应用/ ,/ 通过/ 磁盘分区/ ,/ 分离/ 存储系统/ 中/ 的/ 随机/ 访问/ 和/ 顺序/ 访问/ ,/ 降低/ 了/ 随机/ 访问/ 对/ 存储系统/ 性能/ 的/ 影响/ ;/ 提出/ 一种/ 数据/ 增量/ 校验/ 算法/ ,/ 避免/ 了/ 写/ 过程/ 中/ 对/ 数据/ 盘旧/ 数据/ 的/ 读/ 操作/ ,/ 降低/ 了/ 因/ “/ 读/ -/ 改/ -/ 写/ ”/ 导致/ 的/ 写/ 惩罚/ ./ 实验/ 表明/ ,/ 与/ S/ -/ RAID/ 相比/ ,/ 在/ 不/ 增加/ 系统/ 能耗/ 的/ 情况/ 下/ ,/ LS/ -/ RAID/ 的/ 写/ 速率/ 可以/ 提升/ 至少/ 56/ %/ ./ 在/ 提供/ 相同/ 写/ 性能/ 的/ 条件/ 下/ ,/ LS/ -/ RAID/ 可以/ 关闭/ 更/ 多/ 磁盘/ ,/ 进一步/ 降低/ 了/ 存储系统/ 能耗/ ,/ 提升/ 了/ 节能/ 效果/ ./ 关键词/ 连续/ 数据/ 存储/ ;/ 节能/ ;/ 写/ 优化/ ;/ 磁盘阵列/ ;/ 校验/ ;/ 绿色/ 计算/ 1/ 引言/ 由于/ 信息量/ 爆炸式/ 增长/ ,/ 数据中心/ 的/ 规模/ 越来越/ 大/ ,/ 具有/ PB/ (/ petabyte/ )/ 级/ 存储容量/ 的/ 大规模/ 存储系统/ 已经/ 非常/ 普遍/ ,/ 例如/ Facebook/ 网站/ 的/ 数据中心/ 的/ 规模/ 已达/ 21PB/ ,/ 用于/ 存储/ 每天/ 新/ 产生/ 的/ 约/ 60TB/ ~/ 90TB/ (/ 经压缩/ 后/ 仍/ 达/ 10TB/ ~/ 15TB/ )/ 的/ 数据/ [/ 1/ ]/ ./ 近年来/ ,/ 磁盘/ 价格/ 持续/ 下降/ ,/ 其/ 单位/ 存储/ 价格/ 已/ 接近/ 甚至/ 低于/ 磁带/ 和/ 光盘/ ,/ 并且/ 有着/ 更好/ 的/ 读写/ 性能/ ./ 同时/ ,/ 固态/ 盘因/ 容量/ 和/ 价格/ 的/ 原因/ [/ 2/ -/ 3/ ]/ ,/ 还/ 难以/ 应用/ 到/ 大规模/ 存储系统/ 中/ ./ 因此/ ,/ 硬盘/ 已经/ 成为/ 大规模/ 存储系统/ 的/ 一种/ 主要/ 存储介质/ [/ 4/ ]/ ./ 大规模/ 磁盘/ 存储系统/ 通常/ 采用/ 独立/ 磁盘/ 冗余/ 阵列/ (/ RedundantArraysofIndependentDisks/ ,/ RAID/ [/ 5/ -/ 6/ ]/ )/ ,/ 将/ 大量/ 磁盘/ 组合/ 起来/ 并行/ 工作/ 以/ 提高/ 存储系统/ 的/ 性能/ 、/ 容量/ 和/ 可靠性/ ./ 然而/ ,/ 大量/ 磁盘/ 并行/ 工作/ 也/ 带来/ 一个/ 不容忽视/ 的/ 高能耗/ 问题/ ./ 高能耗/ 不仅/ 使/ 存储系统/ 自身/ 能耗/ 成本/ 的/ 增加/ ,/ 还/ 增加/ 了/ 额外/ 的/ 成本/ 消耗/ ./ 比如/ ,/ 高能耗/ 所/ 产生/ 的/ 高热量/ 不仅/ 增加/ 了/ 冷却系统/ 的/ 能耗/ ,/ 还/ 使/ 存储设备/ 的/ 存放/ 密度/ 无法/ 提高/ ,/ 降低/ 了/ 空间/ 利用率/ ,/ 这些/ 都/ 增加/ 了/ 数据中心/ 的/ 成本/ ./ 因此/ ,/ 近年来/ 对/ 磁盘/ 存储系统/ 的/ 节能/ 研究/ 引起/ 了/ 广泛/ 关注/ ,/ 并/ 取得/ 了/ 一些/ 重要/ 成果/ ,/ 例如/ 基于/ Web/ 应用/ 的/ PARAID/ [/ 7/ ]/ 、/ 基于/ 归档/ 存储/ 的/ Pergamum/ [/ 8/ ]/ 和/ 针对/ 具有/ 连续/ 数据/ 访问/ 特征/ 的/ 应用/ 的/ S/ -/ RAID/ [/ 9/ ]/ ./ 这些/ 节能/ 存储系统/ 通过/ 重构/ 磁盘阵列/ 内/ 的/ 数据/ 布局/ ,/ 将/ 数据/ 访问/ 集中/ 到/ 阵列/ 中/ 的/ 部分/ 磁盘/ 上/ ,/ 关闭/ 其余/ 磁盘/ ,/ 通过/ 减少/ 处于/ 运行/ 状态/ 的/ 磁盘/ 数量/ 达到/ 节能/ 的/ 目的/ ./ 因为/ 一些/ 应用/ 系统/ 虽然/ 对/ 存储系统/ 的/ 容量/ 和/ 可靠性/ 有着/ 很/ 高/ 的/ 要求/ ,/ 但/ 对/ 其/ 性能/ 要求/ 却/ 并/ 不/ 苛刻/ ,/ 大量/ 磁盘/ 并行/ 可以/ 提高/ 存储系统/ 的/ 数据/ 传输率/ ,/ 但是/ ,/ 在/ I/ // O/ 负载/ 较/ 低/ 的/ 情况/ 下/ ,/ 这种/ 高性能/ 并/ 不能/ 被/ 充分利用/ ,/ 因此/ 可以/ 通过/ 减少/ 并行/ 磁盘/ 数量/ ,/ 适当/ 降低/ 存储系统/ 性能/ 来/ 实现/ 节能/ ./ 实验/ 表明/ ,/ 这些/ 存储系统/ 都/ 取得/ 了/ 较/ 好/ 的/ 节能/ 效果/ ./ 然而/ ,/ 以/ 关闭/ 部分/ 磁盘/ 的/ 方式/ 来/ 实现/ 节能/ 目的/ 的/ 存储系统/ 除了/ 因/ 并行/ 磁盘/ 数量/ 减少/ 导致/ 的/ 系统/ 性能/ 下降/ 以外/ ,/ 额外/ 的/ 写/ 惩罚/ 也/ 使得/ 单位/ 磁盘/ 写/ 性能/ 严重/ 下降/ ./ 例如/ :/ Pergamum/ [/ 8/ ]/ 单个/ 节点/ (/ 包括/ 一块/ 磁盘/ 、/ 一个/ NVRAM/ 和/ 一个/ 低/ 性能/ CPU/ )/ 写/ 性能/ 低于/ 10MB/ // s/ ,/ 仅/ 相当于/ 普通/ 磁盘/ 性能/ 的/ 10/ %/ 左右/ ;/ S/ -/ RAID/ [/ 9/ ]/ 中/ 所有/ 写/ 操作/ 都/ 采用/ “/ 读/ -/ 改/ -/ 写/ ”/ (/ read/ -/ modify/ -/ write/ )/ 方式/ ,/ 单位/ 磁盘/ 写/ 性能/ 下降/ 了/ 约/ 50/ %/ ./ 提高/ 单位/ 磁盘/ 性能/ ,/ 可以/ 在/ 处于/ 运行/ 状态/ 的/ 磁盘/ 数目/ 相同/ 的/ 情况/ 下/ ,/ 提高/ 存储系统/ 整体/ 性能/ ;/ 同理/ ,/ 在/ 应用/ 系统对/ 存储系统/ 性能/ 要求/ 相同/ 的/ 情况/ 下/ ,/ 可以/ 关闭/ 更/ 多/ 的/ 磁盘/ ,/ 使/ 整个/ 存储系统/ 的/ 节能/ 效果/ 更加/ 显著/ ./ 在/ 前期/ 研究/ 中/ ,/ 我们/ 提出/ 一种/ 以/ 节能/ 为/ 目的/ 的/ S/ -/ RAID/ 数据/ 布局/ [/ 9/ ]/ ,/ S/ -/ RAID/ 中/ 采用/ RAID4/ // 5/ 冗余/ 策略/ ,/ 校验/ 数据/ 由同/ 条带/ 所有/ 数据/ 块/ 进行/ 异或/ 运算/ 得出/ ./ S/ -/ RAID/ 的/ 写/ 操作/ 通常/ 只/ 在/ 部分/ 磁盘/ 中/ 进行/ ,/ 为/ 避免/ 读取/ 处于/ 关闭/ 状态/ 的/ 磁盘/ 中/ 的/ 数据/ ,/ 写/ 操作/ 全部/ 采用/ “/ 读/ -/ 改/ -/ 写/ ”/ 方式/ ./ 这样/ 虽然/ 节约/ 了/ 能耗/ ,/ 但是/ “/ 读/ -/ 改/ -/ 写/ ”/ 方式/ 需要/ 先/ 读取/ 待/ 写入/ 磁盘/ 的/ 旧/ 数据/ 和/ 旧/ 的/ 校验/ 数据/ ,/ 与/ 新/ 数据/ 进行/ 异或/ 运算/ ,/ 计算/ 出新/ 的/ 校验/ 数据/ ,/ 再/ 写入/ 新/ 数据/ 和/ 新/ 的/ 校验/ 数据/ ,/ 因此/ 写/ 效率/ 非常低/ ,/ 性能/ 损失/ 很大/ ./ S/ -/ RAID/ 的/ 应用环境/ 是/ 以/ 连续/ 数据/ 写为/ 主要/ 特征/ 的/ 应用/ 系统/ (/ 例如/ 归档/ 系统/ 、/ 备份/ 系统/ 、/ VTL/ 系统/ 等/ )/ ,/ 这些/ 应用/ 系统/ 的/ 数据/ 通常/ 为/ 一次性/ 写入/ ,/ 不会/ 被/ 删除/ 或者/ 更改/ ,/ 新/ 数据/ 与/ 旧/ 数据/ 无关/ ,/ 并且/ 读/ 操作/ 非常少/ ./ 本文/ 针对/ S/ -/ RAID/ 数据/ 布局/ 及其/ 应用/ 系统/ 特征/ ,/ 提出/ 一种/ 基于/ 日志/ 结构/ 文件系统/ (/ Log/ -/ StructureFileSystem/ ,/ LFS/ )/ 的/ 改进型/ 数据/ 布局/ :/ LS/ -/ RAID/ (/ LFS/ -/ BasedS/ -/ RAID/ )/ ./ LS/ -/ RAID/ 对/ 阵列/ 中/ 磁盘分区/ ,/ 分离/ 随机/ 访问/ 和/ 顺序/ 访问/ ,/ 并/ 针对/ LS/ -/ RAID/ 结构/ 提出/ 一种/ 新/ 的/ 校验/ 值/ 计算方法/ :/ 数据/ 增量/ 校验/ (/ DataIncrementalParity/ ,/ DIP/ )/ ,/ 优化/ LS/ -/ RAID/ 的/ 写/ 性能/ ,/ 改善/ 节能/ 效果/ ./ 2/ 相关/ 工作/ 2.1/ 国内外/ 研究/ 现状/ 早期/ 对/ 存储系统/ 的/ 节能/ 研究/ 主要/ 根据/ 磁盘/ 的/ 运行/ 原理/ 及其/ 物理/ 特征/ ,/ 采用/ 多/ 转速/ 磁盘/ ,/ 通过/ 降低/ 单个/ 磁盘/ 所/ 消耗/ 的/ 能量/ 来/ 实现/ ,/ 比如/ Carrera/ 等/ 人/ [/ 10/ ]/ Page3/ 提出/ 了/ 一种/ 动态/ 多/ 转速/ 磁盘/ 模型/ 和/ 使用/ 于/ 该/ 模型/ 的/ LD/ (/ LoadDirected/ )/ 算法/ ;/ Gurumurthi/ 等/ 人/ [/ 11/ ]/ 提出/ 一种/ DRPM/ (/ DynamicRPM/ )/ 算法/ ,/ 根据/ 平均/ 响应/ 时间/ 和/ 磁盘/ 请求/ 队列/ 的/ 长度/ 来/ 动态/ 调整/ 磁盘/ 转速/ ;/ Sony/ 公司/ 开发/ 出/ 一种/ 2/ -/ 转速/ 商业/ 磁盘/ [/ 12/ -/ 13/ ]/ ./ 由于/ 未/ 考虑/ 整个/ 存储系统/ 的/ 能耗/ 及/ 数据安全/ ,/ 因此/ 对/ 大规模/ 的/ 存储系统/ 来说/ 节能/ 效果/ 有限/ ./ 近年来/ 对/ 存储系统/ 节能/ 的/ 研究/ 主要/ 通过/ 控制/ 整个/ 系统/ 中/ 处于/ 运行/ 状态/ 的/ 磁盘/ 数量/ 来/ 实现/ 节能/ 目的/ ,/ 比如/ Weddle/ 等/ 人/ [/ 7/ ]/ 提出/ 的/ PARAID/ 模型/ 、/ Zhu/ 等/ 人/ [/ 14/ ]/ 提出/ 的/ Hibernator/ 节能/ 存储/ 模型/ 、/ Li/ 等/ 人/ [/ 15/ ]/ 提出/ 的/ eRAID/ 模型/ 、/ 毛波/ 等/ 人/ [/ 16/ ]/ 提出/ 的/ 绿色/ 磁盘阵列/ (/ GRAID/ )/ ./ 此外/ ,/ Otoo/ 等/ 人/ [/ 17/ ]/ 通过/ 动态/ 块/ 交换/ 算法/ ,/ 将/ “/ 热/ 数据/ ”/ 迁移/ 到/ 始终/ 处于/ 活动状态/ 的/ “/ 热/ 单元/ ”/ 中/ ,/ 延长/ 其余/ 磁盘/ 的/ 空闲/ 时间/ ./ 针对/ 归档/ 存储系统/ ,/ Colarelli/ 等/ 人/ [/ 18/ ]/ 提出/ 了/ MAID/ ,/ Storer/ 等/ 人/ [/ 8/ ]/ 提出/ 了/ Pergamum/ ./ 这些/ 节能/ 方案/ 都/ 是/ 通过/ 重新/ 布局/ 阵列/ 中/ 数据/ ,/ 使/ 大部分/ I/ // O/ 访问/ 集中/ 到/ 少数/ 磁盘/ 上/ ,/ 通过/ 关闭/ 无/ I/ // O/ 访问/ 的/ 磁盘/ 来/ 实现/ 节能/ ./ 但是/ 这些/ 节能/ 方案/ 很少/ 考虑/ 由此/ 带来/ 的/ 额外/ 性能/ 损失/ ,/ 由于/ I/ // O/ 访问/ 集中/ 在/ 少数/ 磁盘/ 上/ ,/ 存储系统/ 内/ 的/ 写/ 操作/ 全部/ 为/ “/ 小写/ ”/ ,/ 严重/ 影响/ 系统/ 性能/ ,/ 为/ 满足/ 应用/ 系统/ 的/ 性能/ 要求/ ,/ 不得不/ 增加/ 处于/ 运行/ 状态/ 的/ 磁盘/ 数量/ ,/ 严重/ 影响/ 了/ 整个/ 系统/ 的/ 节能/ 效果/ ./ 另一方面/ ,/ 对/ 存储系统/ 性能/ 的/ 研究/ 由来已久/ ,/ Stodolsky/ 等/ 人/ [/ 19/ -/ 20/ ]/ 提出/ 了/ 一种/ Parity/ -/ logging/ ,/ 利用/ 日志/ 结构/ 缓冲/ 待/ 更新/ 的/ 校验/ 数据/ ,/ 减少/ 校验/ 数据/ 更新/ 次数/ ./ Chao/ 等/ 人/ [/ 21/ ]/ 提出/ 基于/ RAID6/ 的/ 写/ 优化/ 方法/ :/ RAID6L/ ,/ 在/ 日志/ 结构/ 中/ 缓存/ 待/ 写入/ 数据/ ,/ 在/ 缓冲/ 的/ 数据/ 足够/ 多后/ ,/ 再/ 产生/ 新/ 校验/ 数据/ 并/ 将/ 新/ 数据/ 和/ 新/ 校验/ 数据/ 一次/ 写入/ ./ Menon/ 等/ 人/ [/ 22/ ]/ 提出/ 利用/ 浮动/ 校验/ 数据/ 改善/ 存储系统/ 性能/ ,/ 在/ 每个/ 柱面/ 划分/ 出/ 一定/ 的/ 空间/ 作为/ 自由区/ ,/ 更新/ 校验/ 数据/ 时/ ,/ 新/ 校验/ 数据/ 存入/ 距离/ 旧/ 校验/ 数据/ 最近/ 的/ 自由区/ ,/ 每次/ 更新/ 只/ 需/ 一次/ I/ // O/ 操作/ ./ 以上/ 方法/ 主要/ 针对/ 传统/ RAID/ 结构/ ,/ 难于/ 应用/ 到/ 前面/ 提到/ 的/ 大部分/ 磁盘/ 处于/ 关闭/ 状态/ 的/ 节能型/ 存储/ 结构/ 中/ ./ 2.2/ 前期/ 研究成果/ 一些/ 以/ 连续/ 数据/ 访问/ 为主/ 的/ 应用/ 系统/ (/ 如/ :/ 归档/ 系统/ 、/ 数据备份/ 、/ VTL/ 系统/ 等/ )/ ,/ 对/ 存储系统/ 的/ 性能/ 要求/ 并/ 不/ 苛刻/ ,/ 比如/ 数据/ 传输率/ 仅为/ 120MB/ // s/ 的/ 存储系统/ 就/ 能/ 基本/ 满足/ 归档/ 系统/ 的/ 性能需求/ ./ 针对/ 这/ 类/ 应用/ 的/ 特点/ ,/ 我们/ 在/ 前期/ 研究/ 工作/ 中/ ,/ 提出/ 一种/ S/ -/ RAID/ [/ 9/ ]/ 数据/ 布局/ 结构/ ,/ 取得/ 了/ 较/ 好/ 的/ 节能/ 效果/ ./ S/ -/ RAID/ 采用/ 局部/ 并行/ 策略/ ,/ 对/ 阵列/ 中/ 磁盘/ 分组/ ,/ 组内/ 磁盘/ 并行/ 访问/ ,/ 根据/ 对/ 存储系统/ 性能/ 要求/ 设定/ 磁盘/ 组内/ 的/ 磁盘/ 数量/ ,/ 将/ 数据/ 访问/ 在/ 一定/ 时间段/ 内/ 集中/ 到/ 一组/ 磁盘/ 上/ ,/ 通过/ 磁盘/ 调度/ 算法/ ,/ 将/ 超出/ 特定/ 时间/ 间隔/ 无/ 数据/ 请求/ 的/ 磁盘/ 组/ 关闭/ ./ S/ -/ RAID/ 通过/ 牺牲/ 存储系统/ 部分/ 性能/ ,/ 在/ 满足/ 应用/ 需求/ 的/ 前提/ 下/ ,/ 取得/ 了/ 很/ 好/ 的/ 节能/ 效果/ ./ S/ -/ RAID/ 中写/ 操作/ 采用/ “/ 读/ -/ 改/ -/ 写/ ”/ 方式/ ,/ 性能/ 损失/ 较大/ ./ 本文/ 针对/ S/ -/ RAID/ 数据/ 布局/ ,/ 结合/ S/ -/ RAID/ 内/ 数据/ 特征/ ,/ 提出/ 一种/ S/ -/ RAID/ 优化/ 数据/ 布局/ 结构/ LS/ -/ RAID/ ./ 利用/ 磁盘分区/ 和/ 数据/ 增量/ 校验/ ,/ 减少/ 由/ “/ 读/ -/ 改/ -/ 写/ ”/ 带来/ 的/ 写/ 惩罚/ ,/ 提升/ S/ -/ RAID/ 的/ 写/ 性能/ ./ 3S/ -/ RAID/ 概述/ 及/ 数据/ 特征分析/ RAID/ 通过/ 对/ 阵列/ 中/ 磁盘/ 条带/ 化/ ,/ 将/ 写/ 操作/ 分布/ 在/ 所有/ 磁盘/ 上/ 并行执行/ ,/ 提高/ 存储系统/ 的/ 性能/ ./ RAID/ 中/ ,/ 即使/ 在/ 低/ I/ // O/ 情况/ 下/ ,/ 所有/ 磁盘/ 也/ 始终/ 处于/ 活动状态/ ./ 在/ 一些/ 对/ 存储系统/ 性能/ 要求/ 不高/ 的/ 应用/ 中/ (/ 比如/ 归档/ 系统/ 需要/ 海量/ 存储空间/ 和/ 高可靠性/ ,/ 但/ 对/ 存储/ 带宽/ 的/ 需求/ 通常/ 仅/ 单个/ 硬盘/ 的/ 数据/ 传输速率/ 就/ 可以/ 满足/ )/ ,/ 磁盘/ 并行/ 带来/ 的/ 高性能/ 并/ 不能/ 被/ 有效/ 利用/ ,/ 却/ 承受/ 着/ 由此/ 带来/ 的/ 高能耗/ ./ 针对/ 这/ 类/ 应用/ 系统/ ,/ 我们/ 提出/ 了/ 一种/ S/ -/ RAID/ [/ 9/ ]/ 数据/ 布局/ ./ S/ -/ RAID/ 在/ 满足/ 应用/ 系统对/ 存储系统/ 性能/ 要求/ 的/ 前提/ 下/ ,/ 对/ RAID/ 中/ 磁盘/ 分组/ ,/ 使/ 写/ 操作/ 集中/ 在/ 部分/ 磁盘/ 上/ ,/ 通过/ 关闭/ 处于/ 空闲/ 状态/ 的/ 磁盘/ ,/ 达到/ 节能/ 效果/ ./ 3.1/ S/ -/ RAID/ 数据/ 布局/ S/ -/ RAID/ 中/ 可以/ 采用/ RAID4/ 和/ RAID5/ 两种/ 冗余/ 策略/ ,/ 图/ 1/ 为/ 由/ 5/ 块/ 磁盘/ 组成/ 的/ S/ -/ RAID4/ 的/ 数据/ 布局/ 结构/ ./ Page4/ 图/ 1/ 中/ ,/ D0/ 、/ D1/ 、/ D2/ 和/ D3/ 为/ 数据/ 盘/ ,/ P/ 为/ 校验/ 盘/ ,/ 4/ 块/ 数据/ 盘/ 分为/ 两个/ 组/ ,/ 分别/ 为/ G0/ ,/ G1/ ./ G0/ 组/ 包括/ 磁盘/ D0/ 和/ D1/ ,/ G1/ 组/ 包括/ 磁盘/ D2/ 和/ D3/ ,/ 记/ 作/ ,/ G0/ =/ {/ D0/ ,/ D1/ }/ ,/ G1/ =/ {/ D2/ ,/ D3/ }/ ./ SPs/ 表示/ 第/ s/ 条带/ 的/ 校验/ 数据/ ./ 图/ 1/ 中/ ,/ 数据/ 块/ 以/ Bd/ ,/ s/ 表示/ ,/ d/ 表示/ 数据/ 块/ 所在/ 磁盘/ ,/ s/ 表示/ 数据/ 块/ 所在/ 条带/ ,/ 数据/ 块/ 按组/ 排列/ ,/ 逻辑/ 块/ 地址/ 以/ 箭头/ 所示/ 顺序/ 组内/ 相邻/ ./ 因此/ 在/ 连续/ 数据/ 写/ 操作/ 时/ ,/ 存储系统/ 中/ I/ // O/ 访问/ 在/ 一段时间/ 内/ 集中/ 在/ 某/ 一组/ 磁盘/ 上/ ,/ 这时/ 可/ 关闭/ 其余/ 空闲/ 磁盘/ 组/ ./ 由于/ S/ -/ RAID4/ 中/ 校验/ 数据/ 存储/ 在/ 固定/ 校验/ 盘/ 上/ ,/ 所以/ 同/ RAID4/ 一样/ ,/ 校验/ 盘/ 成为/ 系统/ 性能/ 和/ 可靠性/ 的/ 瓶颈/ ./ 为此/ ,/ 我们/ 引入/ S/ -/ RAID5/ 数据/ 布局/ ,/ 将/ 校验/ 数据/ 平均分配/ 到/ 各/ 数据/ 盘中/ ,/ 解决/ 了/ S/ -/ RAID4/ 中/ 的/ 校验/ 盘/ 瓶颈/ 问题/ ./ 但是/ S/ -/ RAID5/ 中/ ,/ 因为/ 校验/ 数据/ 块/ 均匀分布/ 在/ 所有/ 磁盘/ 上/ ,/ 当/ 一个/ 条带/ 写/ 满后/ 开始/ 写入/ 下/ 一个/ 条带/ 时/ ,/ 要/ 启动/ 新/ 的/ 校验/ 数据/ 块/ 所在/ 磁盘/ ,/ 增加/ 了/ 磁盘/ 启停/ 频次/ ,/ 产生/ 额外/ 能耗/ 并/ 增加/ 了/ 磁盘/ 的/ 故障率/ ./ 在/ 条带/ 划分/ 过程/ 中/ ,/ 条带/ 粒度/ 划分/ 过大/ ,/ 虽然/ 校验/ 盘/ 的/ 转换/ 间隔/ 增大/ 、/ 频率/ 降低/ ,/ 但是/ 组内/ 并行性/ 会/ 变差/ ,/ 影响/ 系统/ 性能/ ;/ 条带/ 粒度/ 划分/ 过/ 小/ ,/ 会因/ 校验/ 盘/ 的/ 频繁/ 转换/ ,/ 导致/ 磁盘/ 启停/ 次数/ 增加/ ,/ 产生/ 额外/ 的/ 能耗/ ,/ 使节/ 能/ 效果/ 下降/ ,/ 也/ 降低/ 了/ 磁盘/ 的/ 可靠性/ ./ 针对/ 这一/ 问题/ ,/ 我们/ 采用/ 了/ 条带/ 分组/ 策略/ ,/ S/ -/ RAID5/ 的/ 数据/ 布局/ 结构/ 如图/ 2/ 所示/ ./ 简化/ 起/ 见/ ,/ 图/ 2/ 中/ S/ -/ RAID5/ 由/ 5/ 块/ 磁盘/ 组成/ ,/ 分为/ Stripe0/ ~/ Stripe9/ 共/ 10/ 个/ 条带/ ,/ 但/ 不失/ 一般性/ ./ 校验/ 数据/ 均匀分布/ 在/ 5/ 个/ 数据/ 盘/ 上/ ,/ 每个/ 条带/ 组/ VGroup/ 包括/ 2/ 个/ 条带/ ,/ 条带/ 组内/ 校验/ 盘/ 固定/ ,/ 相当于/ 一个/ S/ -/ RAID4/ 结构/ ./ 条带/ 分组/ 策略/ 即/ 保证/ 了/ S/ -/ RAID5/ 具有/ 良好/ 的/ 并行性/ ,/ 又/ 降低/ 了/ 磁盘/ 启停/ 频率/ ./ 数据/ 块/ 逻辑/ 地址/ 按图/ 中/ 箭头/ 方向/ 排列/ ./ S/ -/ RAID5/ 中/ 校验/ 数据/ 均匀分布/ ,/ 磁盘/ 启停/ 频率/ 较/ S/ -/ RAID4/ 中略/ 高/ ,/ 但/ 有效/ 避免/ 了/ S/ -/ RAID4/ 中/ 的/ 校验/ 盘/ 瓶颈/ ,/ 因此/ 性能/ 和/ 可靠性/ 要/ 高于/ S/ -/ RAID4/ ./ S/ -/ RAID/ 中/ 数据/ 块/ 的/ 逻辑/ 块/ 地址/ 组内/ 相邻/ ./ 因此/ ,/ 当/ 系统/ I/ // O/ 访问/ 以/ 连续/ 数据/ 写/ 为主/ 时/ ,/ 存储系统/ 的/ I/ // O/ 请求/ 在/ 一定/ 时间段/ 内/ 将/ 集中/ 在/ 同/ 一组/ 磁盘/ 上/ ,/ 其余/ 磁盘/ 无/ I/ // O/ 请求/ 发生/ ,/ 处于/ 空闲/ 状态/ ./ S/ -/ RAID/ 中将/ 这部分/ 无/ I/ // O/ 请求/ 、/ 处于/ 空闲/ 状态/ 的/ 磁盘/ 关闭/ ,/ 节省/ 整个/ 存储系统/ 的/ 能耗/ ./ 实验/ 表明/ [/ 9/ ]/ ,/ 由/ 5/ 块/ 磁盘/ 组成/ 的/ S/ -/ RAID5/ 的/ 能耗/ 约/ 为/ 传统/ RAID/ 的/ 74/ %/ ,/ 由/ 12/ 块/ 磁盘/ 组成/ 的/ S/ -/ RAID5/ 的/ 能耗/ 仅为/ 传统/ RAID/ 的/ 35/ %/ 左右/ ./ 3.2/ S/ -/ RAID/ 中/ 的/ 写/ 操作/ 与/ 数据/ 特征/ 3.2/ ./ 1S/ -/ RAID/ 中/ 的/ 写/ 操作/ 全部/ 为/ “/ 读/ -/ 改/ -/ 写/ ”/ 不同/ 层级/ 的/ RAID/ 通过/ 采用/ 不同/ 的/ 镜像/ 和校验/ 技术/ 来/ 提高/ 存储系统/ 的/ 可靠性/ ,/ 但/ 却/ 因此/ 降低/ 了/ 存储系统/ 性能/ [/ 6/ ,/ 23/ -/ 24/ ]/ ./ RAID/ 中写/ 操作/ 分/ 3/ 种/ 情况/ ./ (/ 1/ )/ 当待/ 写入/ 数据/ 恰好/ 与/ RAID/ 中/ 的/ 整个/ 条带/ 对齐/ 时/ ,/ 采用/ “/ 整条/ 写/ ”/ (/ Full/ -/ StripeWrite/ )/ ;/ (/ 2/ )/ 当待/ 写入/ 的/ 数据/ 不能/ 覆盖/ 整个/ 条带/ ,/ 但/ 不少/ 于/ 整个/ 条带/ 的/ 1/ // 2/ 时/ ,/ 称为/ “/ 大写/ ”/ ,/ 这时/ 采用/ “/ 重构/ 写/ ”/ (/ ReconstructWrite/ )/ ;/ (/ 3/ )/ 当待/ 写入/ 数据/ 不足/ 整个/ 条带/ 1/ // 2/ 时/ ,/ 称为/ “/ 小写/ ”/ ,/ 这时/ 采用/ “/ 读/ -/ 改/ -/ 写/ ”/ ./ 除/ “/ 整条/ 写/ ”/ 外/ ,/ “/ 重构/ 写/ ”/ 和/ “/ 读/ -/ 改/ -/ 写/ ”/ 都/ 增加/ 了/ 存储系统/ 的/ I/ // O/ 负载/ 和/ 延时/ [/ 19/ ]/ ./ 在/ 重构/ 写时/ ,/ I/ // O/ 总/ 次数/ 为/ n/ +/ 1/ 次/ ,/ 其中/ n/ 为/ 阵列/ 中/ 数据/ 盘/ 个数/ ;/ “/ 读/ -/ 改/ -/ 写/ ”/ 时/ ,/ I/ // O/ 总/ 次数/ 为/ 2/ (/ m/ +/ 1/ )/ 次/ ,/ 其中/ m/ 为/ 需要/ 写入/ 的/ 磁盘/ 数目/ ./ S/ -/ RAID/ 采用/ 与/ RAID/ 相同/ 的/ 校验/ 方法/ ,/ 校验/ 值/ 的/ 计算方法/ 也/ 相同/ ,/ 为/ 避免/ 启动/ 处于/ 关闭/ 状态/ 的/ 磁盘/ ,/ S/ -/ RAID/ 中写/ 操作/ 无论是/ “/ 大写/ ”/ 还是/ “/ 小写/ ”/ ,/ 均/ 采用/ “/ 读/ -/ 改/ -/ 写/ ”/ 方式/ ,/ 严重/ 降低/ 了/ 系统/ 性能/ ./ 3.2/ ./ 2/ 基于/ NILFS2/ 文件系统/ 的/ 写/ 操作/ 数据/ 特征/ NILFS2/ [/ 25/ ]/ (/ NewImplementationLog/ -/ Struc/ -/ tureFileSystem/ )/ 文件系统/ 是/ 一种/ 基于/ 日志/ 结构/ 的/ 文件系统/ ,/ 从/ Linuxkernel2/ ./ 6.30/ 开始/ ,/ NILFS2/ 已/ 并入/ Linux/ 内核/ ./ NILFS2/ 采用/ 连续/ 快照/ (/ continu/ -/ oussnapshotting/ )/ 技术/ ,/ 写/ 新/ 数据/ 时/ ,/ 始终/ 顺序/ 写入/ 磁盘/ 头部/ ./ NILFS2/ 文件系统/ 中/ 数据/ 被/ 删除/ 后/ 不/ 立即/ 回收/ 空间/ ,/ 新/ 数据/ 仍然/ 以/ 线性/ 逻辑/ 次序/ 顺序/ 写入/ ,/ 直到/ 逻辑/ 存储空间/ 末端/ ,/ 然后/ 再/ 回收/ 已/ 删除/ 数据/ 空间/ ./ NILFS2/ 文件系统/ 在/ 第/ 1/ 次/ 写入/ 时/ 具有/ 良好/ 的/ 写/ 性能/ ./ S/ -/ RAID/ 的/ 主要/ 应用/ 对象/ 中/ ,/ 如/ 归档/ 系统/ 、/ 备份/ 系统/ ,/ 数据/ 通常/ 不会/ 被/ 删除/ 或/ 修改/ ,/ 并且/ 为/ 一次性/ 写入/ ,/ 非常适合/ 采用/ NILFS2/ 文件系统/ ./ 在/ Linuxkernel2/ ./ 6.35/ ./ 32/ 下/ ,/ 配置/ 由/ 5/ 块/ 磁盘/ Page5/ 构成/ 的/ S/ -/ RAID5/ ./ 磁盘/ 采用/ SeagateST3500630AS/ ,/ 容量/ 500GB/ ,/ 转速/ 7200RPM/ ./ 条带/ 中/ 数据/ 块/ 大小/ 设定/ 为/ 64KB/ ,/ 采用/ NILFS2/ [/ 25/ ]/ 文件系统/ ./ 应用/ 系统/ 、/ NILFS2/ 文件系统/ 和/ S/ -/ RAID/ 的/ 层次/ 关系/ 如图/ 3/ 所示/ ./ 使用/ C语言/ 编写/ 数据/ 模拟/ 发生器/ ,/ 不间断/ 随机/ 产生/ 大小/ 介于/ 4KB/ ~/ 4MB/ 之间/ 的/ 二进制/ 文件/ ,/ 模拟/ 连续/ 数据/ 写入/ S/ -/ RAID5/ ./ 利用/ blktrace/ 工具/ 跟踪/ 记录/ S/ -/ RAID5/ 中/ 数据/ 的/ 写/ 操作/ ./ 图/ 3NILFS2/ 文件系统/ 与/ S/ -/ RAID/ 的/ 层次/ 关系/ blktrace/ 是/ Linux/ 内核/ 中/ 一个/ 针对/ 块/ 设备/ I/ // O/ 层/ 的/ 跟踪/ 工具/ ,/ 通过/ blktrace/ ,/ 可以/ 获取/ I/ // O/ 请求/ 队列/ 的/ 各种/ 详细/ 的/ 情况/ ,/ 包括/ 进行/ 读写/ 的/ 进程/ 名称/ 、/ 进程/ 号/ 、/ 执行/ 时间/ 、/ 读写/ 的/ 物理/ 块/ 号/ 、/ 块/ 大小/ 等等/ ./ 于/ NILFS2/ 文件系统/ 的/ 写/ 操作/ 数据/ 特征/ ./ 图/ 4/ 和/ 图/ 5/ 分别/ 为/ 10min/ 和/ 12hS/ -/ RAID/ 中基图/ 4/ 和/ 图/ 5/ 中/ ,/ S/ -/ RAID/ 中/ 基于/ NILFS2/ 文件系统/ 的/ 写/ 由/ 三/ 部分/ 组成/ ,/ 其中/ 位于/ 存储空间/ 首部/ 和/ 末端/ 的/ 两/ 部分/ 反复/ 写入/ ,/ 这/ 两/ 部分/ 为/ NILFS2/ 文件系统/ 的/ 两个/ 超级/ 块/ ,/ 写/ 操作/ 方式/ 为/ 覆盖/ 写/ ;/ 中间/ 部分/ 为/ 一次性/ 顺序/ 写入/ ,/ 逻辑/ 地址/ 连续/ ./ 4LS/ -/ RAID/ :/ 基于/ NILFS2/ 文件系统/ 的/ S/ -/ RAID/ 性能/ 优化/ S/ -/ RAID/ 主要/ 应用/ 于/ 以/ 顺序/ 写/ 为主/ 的/ 应用/ 系统/ 中/ ,/ 其/ 目标/ 在于/ 节能/ ./ 如前所述/ ,/ “/ 小写/ ”/ 对/ S/ -/ RAID/ 的/ 性能/ 影响/ ,/ 使得/ S/ -/ RAID/ 难以达到/ 更优/ 的/ 节能/ 效果/ ./ 通过/ 3.2/ 节中/ 对/ S/ -/ RAID/ 基于/ NILFS2/ 文件系统/ 的/ 数据/ 特征分析/ ,/ 提出/ 一种/ 基于/ NILFS2/ 文件系统/ 的/ 改进型/ S/ -/ RAID/ :/ LS/ -/ RAID/ ,/ 优化/ 存储系统/ 写/ 性能/ ./ LS/ -/ RAID/ 首先/ 对/ 阵列/ 中/ 磁盘分区/ ,/ 将/ 随机/ 读写/ 数据/ 和/ 顺序/ 读写/ 数据/ 分别/ 存放/ ,/ 利用/ 缓存/ 策略/ ,/ 减少/ 随机/ 读写/ 对/ 存储系统/ 性能/ 的/ 影响/ ,/ 同时/ 采用/ 新/ 的/ 校验/ 值/ 计算方法/ :/ 数据/ 增量/ 校验/ 算法/ ,/ 降低/ 由/ “/ 读/ -/ 改/ -/ 写/ ”/ 带来/ 的/ 写/ 惩罚/ ,/ 优化/ 存储系统/ 写/ 性能/ ./ LS/ -/ RAID/ 的/ 主要/ 特点/ 如下/ ./ (/ 1/ )/ 在/ 阵列/ 中/ 每个/ 磁盘/ 的/ 尾部/ 划出/ 一个/ 保留/ 分区/ ,/ 用于/ 存放/ 超级/ 块/ ,/ 称为/ “/ 超级/ 块/ 分区/ ”/ ./ 将/ 原/ NIL/ -/ FS2/ 文件系统/ 位于/ 存储/ 结构/ 中/ 首端/ 与/ 末端/ 的/ 超级/ 块/ 存放/ 在/ 该/ 保留/ 分区/ 中/ ./ (/ 2/ )/ 改进/ 校验/ 数据/ 计算方法/ ,/ 所有/ 磁盘/ 的/ 数据/ 分区/ 组成/ S/ -/ RAID/ 结构/ ,/ 并/ 采用/ 新/ 的/ 校验/ 值/ 计算方法/ (/ 数据/ 增量/ 校验/ )/ ./ 采用/ 数据/ 增量/ 校验/ 算法/ 的/ 前提条件/ 是/ 数据/ 顺序/ 写入/ ./ (/ 3/ )/ 设置/ 一个/ 写/ 地址/ 指针/ ,/ 记录/ 阵列/ 中/ 最后/ 一个/ 写入/ 的/ 数据/ 块/ 地址/ ,/ 用于/ 数据恢复/ 时/ 确定/ 参与/ 校验/ 的/ 磁盘/ 和/ 需要/ 读取/ 的/ 数据/ 块/ ./ 略/ ,/ 提高/ 写/ 效率/ ./ 4.1/ 磁盘分区/ 由/ 3.2/ ./ 2/ 节/ 可知/ ,/ NILFS2/ 文件系统/ 系统/ 存在/ 两个/ 超级/ 块/ ,/ 分别/ 位于/ 逻辑/ 存储空间/ 的/ 首部/ 和/ 末端/ ./ 超级/ 块/ 的/ 写/ 操作/ 非常少/ ,/ 但是/ 存在/ 于/ 整个/ I/ // O/ 过程/ 中/ ,/ 这/ 主要/ 对/ S/ -/ RAID/ 造成/ 以下/ 影响/ ./ (/ 1/ )/ S/ -/ RAID/ 中/ 的/ 第/ 1/ 组/ 磁盘/ 和/ 最末/ 一组/ 磁盘/ 因为/ 一直/ 存在/ 对/ 超级/ 块/ 的/ 访问/ 而/ 无法/ 关闭/ ,/ 影响/ S/ -/ RAID/ 的/ 节能/ 效果/ ./ (/ 2/ )/ 对/ 超级/ 块/ 的/ 访问/ 为/ 随机/ 访问/ ,/ 增加/ 了/ 磁盘/ 的/ 寻道/ 次数/ ,/ 影响/ S/ -/ RAID/ 的/ 整体/ 性能/ ./ 另外/ ,/ 采用/ 数据/ 增量/ 校验/ 要求/ 数据/ 顺序/ 写/ ,/ 而超/ (/ 4/ )/ 采用/ 磁盘/ 组内/ 写/ 对齐/ 和/ 旧/ 校验/ 数据/ 预读/ 策/ Page6/ 级块/ 的/ 存在/ 使得/ 存储系统/ 总是/ 存在/ 少量/ 随机/ 写/ 操作/ ./ 为/ 克服/ 以上/ 缺点/ ,/ LS/ -/ RAID/ 中/ 对/ 阵列/ 中/ 磁盘分区/ ,/ 如图/ 6/ 所示/ ./ LS/ -/ RAID/ 中/ 每个/ 磁盘/ 分为/ 两/ 部分/ :/ 数据/ 分区/ 和/ 超级/ 块/ 分区/ ./ 所有/ 磁盘/ 数据/ 分区/ 组成/ S/ -/ RAID/ ./ 超级/ 块/ 分区/ 独立/ 于/ S/ -/ RAID/ ,/ 在/ S/ -/ RAID/ 初始化/ 时/ ,/ 不/ 条带/ 化/ ./ 在/ 写/ 数据/ 时/ ,/ 直接/ 将/ NILFS2/ 文件系统/ 的/ 超级/ 块/ 写入/ 超级/ 块/ 分区/ ./ 因为/ NILFS2/ 文件系统/ 中/ 的/ 两个/ 超级/ 块/ 的/ 大小/ 都/ 为/ 4KB/ ,/ 所以/ 将/ 超级/ 块/ 分区/ 设定/ 为/ 8KB/ ,/ 以/ 同时/ 容纳/ 两个/ 超级/ 块/ ./ 4.2/ 超级/ 块/ 分区/ 的/ 写/ 操作/ LS/ -/ RAID/ 中将/ NILFS2/ 文件系统/ 的/ 超级/ 块/ 存放/ 在/ 超级/ 块/ 分区/ ,/ 单独/ 读写/ ./ 下面/ 分别/ 对/ 由/ 5/ 块/ 磁盘/ 组成/ 的/ LS/ -/ RAID4/ 和/ LS/ -/ RAID5/ 中/ 超级/ 块/ 的/ 写/ 操作/ 做/ 详细/ 介绍/ ./ 4.2/ ./ 1LS/ -/ RAID4/ 中/ 超级/ 块/ 的/ 写/ 操作/ 由/ 5/ 块/ 磁盘/ 组成/ 的/ LS/ -/ RAID4/ 中/ 超级/ 块/ 的/ 写/ 操作步骤/ 如下/ ./ (/ 1/ )/ 存储系统/ 开始/ 写入/ 数据/ 时/ ,/ 首先/ 写入/ G0/ 组/ 磁盘/ ,/ 因此/ G0/ 组内/ 磁盘/ D0/ 和/ D1/ 为/ 活动状态/ ,/ 超级/ 块/ 写入/ 到/ D0/ 和/ D1/ 的/ 超级/ 块/ 分区/ 中/ ,/ D0/ 和/ D1/ 中/ 两个/ 超级/ 块/ 分区/ 互为/ 镜像/ ./ (/ 2/ )/ 当/ G0/ 组/ 磁盘/ 即将/ 写/ 满时/ ,/ S/ -/ RAID/ 启动/ G1/ 组/ 磁盘/ ,/ D2/ 和/ D3/ 转为/ 活动状态/ ./ (/ 3/ )/ 当/ G0/ 组/ 磁盘/ 写/ 满后/ ,/ 数据/ 开始/ 写入/ G1/ 组/ 磁盘/ ,/ 将/ D0/ 和/ D1/ 中/ 的/ 超级/ 块/ 分别/ 复制到/ D2/ 和/ D3/ 的/ 超级/ 块/ 分区/ 中/ ,/ 超级/ 块/ 的/ 读写/ 转到/ D2/ 和/ D3/ 的/ 超级/ 块/ 分区/ ./ D2/ 和/ D3/ 中/ 两个/ 超级/ 块/ 分区/ 互为/ 镜像/ ./ (/ 4/ )/ 关闭/ 磁盘/ D0/ 和/ D1/ ./ 4.2/ ./ 2S/ -/ RAID5/ 中/ 超级/ 块/ 的/ 写/ 操作/ 由/ 5/ 块/ 磁盘/ 组成/ 的/ LS/ -/ RAID5/ 中/ 超级/ 块/ 的/ 写/ 操作步骤/ 如下/ ./ (/ 1/ )/ 存储系统/ 开始/ 写入/ 数据/ 时/ ,/ 首先/ 写入/ G0/ 组/ 磁盘/ 的/ 条带/ 组/ VGroup0/ ,/ 此时/ G0/ 组内/ 磁盘/ 包括/ D0/ 和/ D1/ ,/ 超级/ 块/ 写入/ 到/ D0/ 和/ D1/ 的/ 超级/ 块/ 分区/ 中/ ,/ D0/ 和/ D1/ 中/ 两个/ 超级/ 块/ 分区/ 互为/ 镜像/ ./ (/ 2/ )/ 数据/ 写/ 满前/ 3/ 个/ 条带/ 组后/ ,/ 开始/ 写入/ VGroup3/ ,/ 此时/ G0/ 组内/ 磁盘/ 包括/ D0/ 和/ D2/ ,/ 将/ 磁盘/ D1/ 超级/ 块/ 分区/ 中/ 数据/ 复制到/ 磁盘/ D2/ 中/ ,/ 由/ D2/ 中/ 超级/ 块/ 分区/ 代替/ 原/ D1/ 中/ 超级/ 块/ 分区/ ./ (/ 3/ )/ 数据/ 写满/ VGroup3/ 后/ ,/ 开始/ 写入/ VGroup4/ ,/ 此时/ G0/ 组内/ 磁盘/ 包括/ D1/ 和/ D2/ ,/ 将/ 磁盘/ D0/ 超级/ 块/ 分区/ 中/ 数据/ 复制到/ 磁盘/ D1/ 中/ ,/ 由/ D1/ 中/ 超级/ 块/ 分区/ 代替/ 原/ D0/ 中/ 超级/ 块/ 分区/ ./ (/ 4/ )/ 写满/ G0/ 组后/ ,/ 数据/ 按/ G1/ 组内/ 数据/ 块/ 逻辑/ 顺序/ 依次/ 写入/ ,/ 超级/ 块/ 的/ 读写/ 位置/ 按步/ (/ 2/ )/ 和/ (/ 3/ )/ 的/ 方式/ 依次/ 转换/ ./ 每个/ 磁盘/ 的/ 超级/ 块/ 分区/ 中/ 都/ 同时/ 存放/ 原/ NIL/ -/ FS2/ 文件系统/ 首端/ 和/ 末端/ 的/ 两个/ 超级/ 块/ ,/ 两个/ 磁盘/ 的/ 超级/ 块/ 分区/ 互为/ 镜像/ ,/ 提高/ 数据/ 可靠性/ ./ 由图/ 4/ 和/ 图/ 5/ 可以/ 看出/ ,/ 超级/ 块/ 的/ 写/ 为/ 覆盖/ 写/ ,/ 但是/ 写/ 超级/ 块/ 的/ 频率/ 非常低/ ,/ 因为/ 超级/ 块/ 常驻/ 内存/ ,/ 只是/ 定期/ 刷/ 回/ 磁盘/ ./ 另外/ ,/ 超级/ 块/ 分区/ 的/ 大小/ 仅为/ 8KB/ ,/ 所以/ 超级/ 块/ 的/ 读写/ 以及/ 在/ 不同/ 磁盘/ 之间/ 复制/ 几乎/ 不会/ 对/ 存储系统/ 的/ 读写/ 性能/ 产生/ 影响/ ,/ 同样/ 单独/ 划分/ 的/ 超级/ 块/ 分区/ 所/ 造成/ 的/ 空间/ 损耗/ 也/ 几乎/ 可以/ 忽略不计/ ./ 除/ 超级/ 块/ 分区/ 外/ ,/ 所有/ 磁盘/ 的/ 数据/ 分区/ 以图/ 1/ 或图/ 2/ 中/ 的/ 数据/ 布局/ 结构/ 组成/ S/ -/ RAID4/ // 5/ ,/ 用以/ 存储/ 数据/ ./ S/ -/ RAID/ 部分/ 中/ 的/ 写/ 操作/ 将/ 全部/ 为/ 顺序/ 写/ ./ 4.3/ 数据/ 分区/ 的/ 写/ 操作/ 4.3/ ./ 1/ 数据/ 分区/ 的/ 顺序/ 写/ 操作/ 通过/ 4.1/ 节/ 提出/ 的/ 磁盘分区/ ,/ 超级/ 块/ 存放/ 在/ “/ 超级/ 块/ 分区/ ”/ ,/ LS/ -/ RAID/ 数据/ 分区/ (/ 等同于/ S/ -/ RAID/ )/ 中/ 的/ 写/ 操作/ 将/ 全部/ 转换/ 为/ 顺序/ 写/ ./ 根据/ 图/ 4/ 和/ 图/ 5/ 中/ 基于/ NILFS2/ 文件系统/ 的/ 数据/ 访问/ 特征/ 可知/ :/ 当对/ LS/ -/ RAID/ 中/ 某个/ 数据/ 块/ 进行/ 写/ 操作/ 时/ ,/ 对于/ 任意/ 数据/ 块/ ,/ 如果/ 其/ 逻辑/ 块/ 地址/ 小于/ 当前/ 写入/ 数据/ 块/ 的/ 逻辑/ 块/ 地址/ ,/ 表明/ 该/ 数据/ 块/ 已/ 写入/ 数据/ ;/ 同样/ ,/ 对于/ 任意/ 数据/ 块/ ,/ 如果/ 其/ 逻辑/ 块/ 地址/ 大于/ 当前/ 数据/ 块/ 的/ 逻辑/ 块/ 地址/ ,/ 表明/ 该/ 数据/ 块/ 未/ 写入/ 数据/ ,/ 如图/ 7/ 所示/ (/ 图中/ 超级/ 块/ 分区/ 部分/ 未/ 标出/ )/ ./ Page7/ 图/ 7/ 中/ ,/ 数据/ 块/ 的/ 逻辑/ 块/ 地址/ LBA/ 按/ 箭头/ 方向/ 升序/ 排列/ ./ 按/ 箭头/ 指向/ 次序/ ,/ B0/ ,/ 0/ ~/ B3/ ,/ 2/ 已有/ 数据/ 写入/ ,/ B3/ ,/ 2/ 为/ 最后/ 一个/ 写入/ 数据/ 的/ 数据/ 块/ ./ 由/ 数据/ 块/ B2/ ,/ 3/ 起始/ 到/ 阵列/ 逻辑/ 地址/ 的/ 最末端/ ,/ 均/ 为/ 未/ 写入/ 数据/ 的/ 空/ 数据/ 块/ ,/ 写/ 地址/ 指针/ PLBA/ 值为/ 最末/ 写入/ 的/ 数据/ 块/ B3/ ,/ 2/ 的/ 逻辑/ 块/ 地址/ ./ 基于/ 以上/ 结论/ ,/ 所以/ 待/ 写入/ 数据/ 在/ LS/ -/ RAID/ 中均/ 为/ 新增/ 数据/ ,/ 通过/ 改进/ 校验/ 数据/ 的/ 计算方法/ ,/ 采用/ 数据/ 增量/ 校验/ ,/ 优化/ LS/ -/ RAID/ 中/ 的/ 写/ 操作/ 性能/ :/ 只/ 计算/ 同/ 一条/ 带中/ 已/ 写入/ 数据/ 的/ 数据/ 块/ 校验/ 值/ ,/ 当空/ 数据/ 块/ 写入/ 数据/ 时/ ,/ 再/ 被/ 计入/ 校验/ 值/ ./ 同时/ ,/ 设置/ 一个/ 写/ 地址/ 指针/ ,/ 记做/ PLBA/ ,/ 记录/ 已/ 写入/ 数据/ 的/ 最末端/ 逻辑/ 块/ 地址/ ,/ 即/ 最后/ 一个/ 写入/ 数据/ 的/ 数据/ 块/ 逻辑/ 地址/ (/ 如图/ 7/ 中/ ,/ PLBA/ ←/ (/ B3/ ,/ 2/ )/ )/ ./ 在/ LS/ -/ RAID/ 初始化/ 时/ ,/ 做/ 如下/ 设置/ ./ (/ 1/ )/ 对/ 阵列/ 中/ 磁盘/ 进行/ 条带/ 化/ 并/ 进行/ 磁盘/ 分组/ (/ 在/ S/ -/ RAID5/ 中/ ,/ 还要/ 进行/ 条带/ 分组/ )/ ;/ (/ 2/ )/ 地址/ 指针/ PLBA/ 置/ 为/ “/ -/ 1/ ”/ ,/ 表示/ 此时/ 阵列/ 内为/ 空/ ,/ 所有/ 数据/ 块/ 均/ 未/ 写入/ 数据/ ./ 同时/ ,/ LS/ -/ RAID/ 采用/ 磁盘/ 组内/ 写/ 对齐/ 和校验/ 值/ 预读/ 来/ 提高/ 系统/ 写/ 性能/ ./ (/ 1/ )/ 磁盘/ 组内/ 写/ 对齐/ ./ 设置/ 缓冲区/ ,/ 缓存/ 待/ 写入/ 数据/ ,/ 当/ 缓冲区/ 内/ 的/ 数据/ 占/ 满/ 一个/ 磁盘/ 组/ 的/ 整个/ 条带/ 时/ ,/ 再/ 写入/ 阵列/ ,/ 可以/ 实现/ 同/ 一组/ 内/ 磁盘/ 的/ 整条/ 写/ ./ 写/ 对齐/ 可以/ 减少/ 计算/ 量/ ,/ 提高/ 并行性/ ,/ 提升/ 写/ 效率/ ./ (/ 2/ )/ 校验/ 值/ 预读/ ./ 如前所述/ ,/ 通过/ 对/ 磁盘分区/ ,/ LS/ -/ RAID/ 数据/ 分区/ 中写/ 操作/ 全部/ 为/ 顺序/ 写/ ,/ 由此可知/ ,/ 当前/ 写/ 操作/ 的/ 逻辑/ 上/ 的/ 下/ 一个/ 条带/ 为/ 即将/ 写入/ 的/ 条带/ ,/ 可以/ 预读/ 该/ 条带/ 的/ 旧/ 校验/ 值/ 参与/ 下/ 一步/ 新/ 校验/ 值/ 的/ 计算/ ./ 适当/ 设定/ 预读/ 条带/ 数目/ 可以/ 最大化/ 系统/ 的/ 写/ 性能/ ,/ 降低/ 写/ 延迟时间/ ./ “/ 读/ -/ 改/ -/ 写/ ”/ 时/ 需要/ 先/ 读取/ 待/ 写入/ 数据/ 块/ 的/ 旧/ 数据/ 和/ 旧/ 的/ 校验/ 数据/ ,/ 但/ 在/ LS/ -/ RAID/ 中/ ,/ 待/ 写入/ 数据/ 块/ 的/ 旧/ 数据/ 无/ 意义/ ,/ 没有/ 计入/ 校验/ 值/ ,/ 因此/ 无需/ 读取/ 待/ 写入/ 数据/ 块/ 的/ 旧/ 数据/ ./ 另外/ ,/ 在/ 写/ 第/ 1/ 组/ 磁盘/ 时/ ,/ 新/ 数据/ 相异/ 或/ 产生/ 校验/ 数据/ 直接/ 写入/ 检验/ 数据/ 块/ 即可/ ,/ 无需/ 读取/ 旧/ 校验/ 数据/ ./ 因此/ ,/ LS/ -/ RAID/ 的/ 写/ 操作/ 分/ 两种/ 情况/ ./ (/ 1/ )/ 写/ 第/ 1/ 组/ 磁盘/ 时/ ,/ 校验/ 值/ 计算公式/ 为式/ (/ 1/ )/ 中/ ,/ Pnew/ 为/ 新/ 产生/ 校验/ 数据/ ,/ Di/ 为/ 第/ 1/ 组/ 磁盘/ 数据/ ./ 计算/ 出/ 校验/ 值后/ ,/ 将/ 数据/ 写入/ 数据/ 盘/ ,/ 将/ 校验/ 值/ 写入/ 校验/ 盘/ ,/ 设置/ 指针/ PLBA/ 的/ 值/ ,/ 指向/ 写入/ 的/ 最后/ 一个/ 数据/ 块/ ./ 以/ 数据/ 块/ 形式/ 表示/ 式/ (/ 1/ )/ 可/ 改写/ 为式/ (/ 2/ )/ 中/ SP/ 为/ 校验/ 数据/ 块/ ,/ s/ 为/ 条带/ 号/ ,/ B/ 为/ 阵列/ 内/ 数据/ 块/ ,/ NG/ 为/ 磁盘/ 组内/ 磁盘/ 数/ ./ 例如/ :/ 在/ 写/ G0/ 磁盘/ 内/ 数据/ 块/ B0/ ,/ 2/ 和/ B1/ ,/ 2/ (/ 如图/ 1/ 所示/ )/ 时/ ,/ 校验/ 数据/ 块/ SP2/ 的/ 值/ 由/ B0/ ,/ 2/ 和/ B1/ ,/ 2/ 直接/ 相异/ 或/ 得出/ :/ SP2/ =/ B0/ ,/ 2/ / B1/ ,/ 2/ ./ 计算/ 出/ 校验/ 值后/ ,/ 将/ 数据/ 写入/ B0/ ,/ 2/ 和/ B1/ ,/ 2/ ,/ 将/ 校验/ 数据/ 写入/ SP2/ ,/ 设置/ 指针/ PLBA/ 的/ 值/ ,/ 指向/ B1/ ,/ 2/ ,/ PLBA/ ←/ (/ B1/ ,/ 2/ )/ ./ “/ 读/ -/ 改/ -/ 写/ ”/ 时/ ,/ 需要/ 读取/ B0/ ,/ 2/ 和/ B1/ ,/ 2/ 中旧/ 数据/ 和/ SP2/ 中旧/ 校验/ 数据/ ,/ 计算/ 完成/ 后/ 写入/ B0/ ,/ 2/ 和/ B1/ ,/ 2/ 新/ 数据/ 和/ SP2/ 新/ 校验/ 数据/ ,/ 存储系统/ 的/ I/ // O/ 次数/ 为/ 6/ 次/ ,/ 而/ LS/ -/ RAID/ 中/ 只/ 需/ 写入/ B0/ ,/ 2/ 、/ B1/ ,/ 2/ 和/ SP2/ ,/ I/ // O/ 次数/ 减少/ 为/ 3/ 次/ ,/ 仅为/ 优化/ 前/ 的/ 50/ %/ ./ (/ 2/ )/ 写/ 其余/ 组/ 磁盘/ 时/ ,/ 校验/ 值/ 计算公式/ 为式/ (/ 3/ )/ 中/ ,/ Pnew/ 为/ 新/ 产生/ 校验/ 数据/ ,/ Dnew/ 为/ 待/ 写入/ 数据/ ,/ Pold/ 为/ 旧/ 校验/ 数据/ ./ 待/ 写入/ 数据/ 块/ 的/ 旧/ 数据/ 无/ 意义/ 并且/ 没有/ 计入/ 校验/ 值/ ,/ 因此/ 计算/ 新/ 的/ 校验/ 数据/ 时/ ,/ 无需/ 读取/ 待/ 写入/ 数据/ 块/ 的/ 旧/ 数据/ ./ 但/ 需要/ 读取/ 旧/ 的/ 校验/ 数据/ ./ 当/ 写入/ 第/ g/ 组/ 磁盘/ 时/ ,/ 以/ 数据/ 块/ 形式/ 表示/ 式/ (/ 3/ )/ 可/ 改写/ 为/ 例如/ :/ 在/ 写/ G1/ 组/ 磁盘/ 内/ 数据/ 块/ B2/ ,/ 2/ 和/ B3/ ,/ 2/ (/ 如图/ 1/ 所示/ )/ 时/ ,/ 校验/ 数据/ 块/ SP2/ 的/ 值/ 是/ 由/ 同/ 条带/ 内/ B0/ ,/ 2/ 和/ B1/ ,/ 2/ 的/ 值/ 计算/ 得到/ 的/ ,/ 这时/ 需要/ 读出/ SP2/ 内/ 的/ 旧/ 校验/ 数据/ ./ 新/ 的/ 校验/ 数据/ 通过/ B2/ ,/ 2/ 和/ B3/ ,/ 2/ 中待/ 写入/ 数据/ 和/ SP2/ 内/ 的/ 旧/ 校验/ 数据/ 异或/ 运算/ 得到/ 计算/ 出新/ 的/ 校验/ 值后/ ,/ 将/ 数据/ 写入/ B2/ ,/ 2/ 和/ B3/ ,/ 2/ ,/ 将/ 新/ 的/ 校验/ 数据/ 写入/ SP2/ ,/ 设置/ 指针/ PLBA/ 的/ 值/ ,/ 指向/ B3/ ,/ 2/ ,/ PLBA/ ←/ (/ B3/ ,/ 2/ )/ ./ 在/ 此/ 过程/ 中/ ,/ 存储系统/ 需要/ 读取/ 旧/ 校验/ 数据/ ,/ 写/ 新/ 数据/ 和/ 新/ 校验/ 数据/ ,/ I/ // O/ 次数/ 共/ 4/ 次/ ,/ 为/ 优化/ 前/ (/ 6/ 次/ )/ 的/ 67/ %/ ./ LS/ -/ RAID/ 在/ 写/ 非/ 第/ 1/ 组/ 磁盘/ 时/ 采用/ 校验/ 值/ 预读/ 策略/ ,/ 设/ 当前/ 写入/ 数据/ 的/ 条带/ 为/ 第/ s/ 条带/ ,/ 因为/ LS/ -/ RAID/ 中为/ 顺序/ 写/ ,/ 则/ 下/ 一个/ 要/ 写入/ 的/ 条带/ 应为/ 第/ s/ +/ 1/ 条带/ ,/ 预/ 读取/ 第/ s/ +/ 1/ 条带/ 的/ 旧/ 校验/ 数据/ ,/ 可以/ 减少/ 甚至/ 避免/ 因读/ 旧/ 校验/ 数据/ 带来/ 的/ 写/ 延迟/ ./ 实际/ 应用/ 中/ ,/ 通常/ 预读/ 第/ s/ +/ 1/ ~/ s/ +/ n/ 条带/ ,/ n/ 的/ 值/ 可根/ Page8/ 据/ 存储系统/ 不同/ 配置/ 来/ 设定/ ,/ 以/ 实现/ 系统/ 最佳/ 性能/ ./ 此外/ ,/ LS/ -/ RAID/ 初始化/ 时/ ,/ 只/ 需/ 设置/ 条带/ 化/ 和/ 分组/ 信息/ 以及/ PLBA/ 的/ 初始值/ ,/ 不/ 需要/ 对/ 数据/ 盘/ 和校验/ 盘/ 进行/ 初始化/ ,/ 缩短/ 了/ 系统/ 初始化/ 时间/ ./ 4.3/ ./ 2/ 垃圾/ 回收/ NILFS2/ 文件系统/ 通过/ 连续/ 快照/ (/ continuoussnap/ -/ shotting/ )/ 技术/ 能够/ 将/ 随机/ 写/ 转换/ 为/ 顺序/ 写/ ./ 数据/ 修改/ 时/ ,/ 新/ 数据/ 被/ 写/ 到/ 日志/ 的/ 头部/ ,/ 而/ 旧/ 数据/ 仍然/ 保留/ ,/ 数据/ 删除/ 也/ 不/ 立即/ 回收/ 空间/ ,/ 直到/ 需要/ 对/ 旧/ 数据/ 进行/ 垃圾/ 收集/ ./ NILFS2/ 文件系统/ 进行/ 垃圾/ 收集/ 后/ ,/ 回收/ 的/ 存储空间/ 再次/ 写入/ 数据/ 时/ ,/ 不再/ 适用/ 于/ 数据/ 增量/ 校验/ ,/ 只能/ 采用/ “/ 读/ -/ 改/ -/ 写/ ”/ 方式/ 写入/ 数据/ ,/ 这时/ ,/ LS/ -/ RAID/ 的/ 写/ 性能/ 降低/ 到/ S/ -/ RAID/ 的/ 水平/ ./ LS/ -/ RAID/ 的/ 典型/ 应用/ 中/ ,/ 比如/ 归档/ 系统/ 、/ 备份/ 系统/ ,/ 数据/ 通常/ 为/ 一次性/ 写入/ ,/ 很少/ 被/ 修改/ 或/ 删除/ ,/ 放弃/ 垃圾/ 回收/ 实际/ 损失/ 的/ 存储空间/ 非常/ 有限/ ./ 因此/ ,/ 放弃/ 垃圾/ 回收/ ,/ 牺牲/ 部分/ 存储空间/ ,/ 可以/ 以/ 极小/ 的/ 代价/ 避免/ LS/ -/ RAID/ 出现/ 性能/ 瓶颈/ ./ 4.4/ 数据恢复/ 大规模/ 存储系统/ 中/ ,/ 磁盘/ 失效/ 是/ 一个/ 不容忽视/ 的/ 问题/ ./ LS/ -/ RAID4/ // 5/ 采用/ RAID4/ // 5/ 冗余/ 校验/ 技术/ ,/ 可以/ 容忍/ 同一时间/ 内/ 任意/ 一块/ 磁盘/ 的/ 失效/ ./ 当/ 阵列/ 中有/ 磁盘/ 失效/ 后/ ,/ 必须/ 及时/ 更换/ 失效/ 磁盘/ 并/ 进行/ 数据恢复/ ,/ 以/ 避免/ 因/ 第/ 2/ 块/ 磁盘/ 故障/ 导致/ 整个/ 存储系统/ 数据/ 失效/ ./ RAID4/ // 5/ 中/ 校验/ 盘/ 失效/ 可/ 通过/ 式/ (/ 6/ )/ 重新/ 恢复/ ,/ 式/ (/ 6/ )/ 中/ ,/ Diskparity/ 为/ 失效/ 校验/ 盘/ 校验/ 数据/ ,/ Diski/ 为/ 数据/ 盘/ 数据/ ./ RAID4/ // 5/ 中/ 失效/ 数据/ 盘/ 的/ 恢复/ 可以/ 通过/ 校验/ 值/ 计算公式/ 的/ 逆运算/ 获得/ ,/ 式/ (/ 7/ )/ 中/ ,/ Diskfailure/ 为/ 失效/ 数据/ 盘/ 数据/ ,/ Diskremain/ 为/ 未/ 失效/ 数据/ 盘/ 数据/ ,/ Diskparity/ 为/ 校验/ 盘/ 校验/ 数据/ ./ 未/ 采用/ 写/ 优化/ 的/ S/ -/ RAID4/ // 5/ 可/ 使用/ 与/ RAID4/ // 5/ 相同/ 的/ 方法/ 恢复/ 数据/ ./ 但是/ 数据恢复/ 时/ 需要/ 预先/ 启动/ 全部/ 磁盘/ ,/ 使/ 存储系统/ 能耗/ 急剧/ 增大/ ./ LS/ -/ RAID/ 中/ ,/ 由于/ 校验/ 数据/ 不是/ 阵列/ 中/ 所有/ 数据/ 盘/ 的/ 异或/ 校验/ ,/ 因此/ 当有/ 磁盘/ 失效/ 时/ ,/ 不能/ 使用/ 上述/ 方法/ 进行/ 数据恢复/ ./ LS/ -/ RAID/ 中/ 磁盘/ 失效/ 分/ 3/ 类/ :/ 未/ 写入/ 数据/ 的/ 数据/ 盘/ 失效/ 、/ 已/ 写入/ 数据/ 的/ 数据/ 盘/ 失效/ 和校验/ 盘/ 失效/ ./ 下面/ 分别/ 介绍/ 3/ 类/ 磁盘/ 失效/ 的/ 数据恢复/ 方法/ ./ (/ 1/ )/ 未/ 写入/ 数据/ 的/ 数据/ 盘/ 失效/ ./ LS/ -/ RAID/ 中/ 数据/ 按/ 逻辑/ 块/ 地址/ 次序/ 顺序/ 写入/ 数据/ 盘/ ,/ 因此/ 阵列/ 中/ 存在/ 着/ 未/ 被/ 写入/ 有效/ 数据/ 的/ 磁盘/ ./ 因为/ 这部分/ 磁盘/ 未/ 被/ 使用/ ,/ 所以/ 失效/ 概率/ 极低/ ,/ 但/ 并非/ 不/ 存在/ ./ 这类/ 磁盘/ 失效/ 后/ 只要/ 使用/ 新/ 的/ 磁盘/ 替换/ 失效/ 磁盘/ 并/ 进行/ 简单/ 初始化/ (/ 如/ 条带/ 化/ 、/ 条带/ 分组/ 等/ )/ 即可/ ,/ 不/ 需要/ 进行/ 数据恢复/ ./ 需要/ 指出/ 的/ 是/ ,/ 在/ 传统/ RAID/ 中/ ,/ 即使/ 无/ 数据/ 写入/ 的/ 磁盘/ 失效/ ,/ 也/ 需要/ 通过/ 式/ (/ 7/ )/ 进行/ 数据恢复/ ,/ 因为/ 磁盘/ 中/ 的/ 数据/ (/ 即使/ 无/ 意义/ )/ 的/ 改变/ 会/ 使/ 校验/ 值/ 失效/ ./ (/ 2/ )/ 已/ 写入/ 数据/ 的/ 数据/ 盘/ 失效/ ./ 当/ 一个/ 已/ 写入/ 有效/ 数据/ 的/ 数据/ 盘/ 失效/ 后/ ,/ 根据/ 地址/ 指针/ PLBA/ 由式/ (/ 8/ )/ 和/ (/ 9/ )/ 计算/ 当前/ 写入/ 块/ 所在/ 的/ 条带/ 号/ s/ 和/ 磁盘/ 号/ d/ ./ d/ =/ (/ (/ PLBA/ -/ LGg/ )/ modNGg/ )/ +/ g/ ×/ NG/ (/ 9/ )/ 式/ (/ 8/ )/ 、/ (/ 9/ )/ 中/ ,/ NG/ 表示/ 每组/ 磁盘/ 个数/ ,/ g/ 表示/ PLBA/ 指向/ 的/ 数据/ 块/ 所在/ 磁盘/ 组/ g/ =/ PLBAS/ ×/ NG/ 中/ 划分/ 的/ 条带/ 总数/ ,/ LGg/ 是/ 该/ 块/ 所在/ 组/ 起始/ 块/ 地址/ LGg/ =/ g/ ×/ S/ ×/ NG/ ./ (/ 1/ )/ 如果/ 失效/ 磁盘/ 为/ 当前/ 组内/ 磁盘/ ,/ 只/ 需/ 恢复/ 0/ ~/ s/ 条带/ 内/ 的/ 数据/ ,/ 条带/ 号/ 大于/ s/ 的/ 区域/ 还/ 未/ 写入/ 数据/ ,/ 因此/ 不/ 需要/ 进行/ 数据恢复/ ./ 数据恢复/ 时/ 需要/ 读取/ 0/ ~/ g/ 组中/ 0/ ~/ s/ 条带/ 内/ 的/ 数据/ 和校验/ 数据/ ,/ 设/ 失效/ 磁盘/ 编号/ 为/ i/ ,/ 通过/ 式/ (/ 10/ )/ 恢复/ 失效/ 磁盘/ 数据/ 块/ :/ Bi/ ,/ m/ =/ B0/ ,/ m/ / B1/ ,/ m/ / …/ / Bi/ -/ 1/ ,/ m/ / Bi/ +/ 1/ ,/ m/ / …/ / 式/ (/ 10/ )/ 中/ ,/ m/ 为/ 条带/ 号/ ./ 例如/ ,/ 在/ LS/ -/ RAID4/ 中/ ,/ 当前/ 数据/ 写入/ 到/ 第/ 2/ 条带/ 后/ ,/ 即/ 写入/ 数据/ 块/ 为/ B2/ ,/ 2/ 和/ B3/ ,/ 2/ (/ 如图/ 7/ 所示/ )/ ,/ 此时/ 磁盘/ D2/ 失效/ (/ 如图/ 7/ 所示/ )/ ./ 用/ 新/ 数据/ 盘/ 替换/ 掉/ 失效/ 的/ D2/ 后/ ,/ 需要/ 恢复/ B2/ ,/ 0/ 、/ DB2/ ,/ 1/ 和/ B2/ ,/ 2/ 的/ 数据/ ,/ B2/ ,/ m/ =/ B0/ ,/ m/ / B1/ ,/ m/ / B3/ ,/ m/ / SPm/ (/ m/ =/ 0/ ,/ 1/ ,/ 2/ )/ (/ 2/ )/ 如果/ 失效/ 磁盘/ 为/ 非/ 当前/ g/ 组内/ 磁盘/ (/ 第/ 0/ ~/ g/ -/ 1/ 组/ )/ ,/ 设/ 阵列/ 中/ 最大/ 条带/ 号/ 为/ S/ ,/ 失效/ 磁盘/ 数据恢复/ 分为/ 两/ 部分/ :/ 0/ ~/ s/ 条带/ 以及/ (/ s/ +/ 1/ )/ ~/ S/ 条带/ ./ 0/ ~/ s/ 条带/ 内/ 的/ 数据恢复/ 需要/ 读取/ 0/ ~/ g/ 组内/ 的/ 所有/ 磁盘/ 数据/ 以及/ 校验/ 数据/ ,/ 通过/ 式/ (/ 10/ )/ 可/ 恢复/ 数据/ ./ (/ s/ +/ 1/ )/ ~/ S/ 条带/ 内/ 的/ 数据恢复/ 需要/ 读取/ 0/ ~/ (/ g/ -/ 1/ )/ 组内/ 的/ 所有/ 磁盘/ 数据/ 以及/ 校验/ 数据/ ,/ 通过/ 式/ (/ 12/ )/ 可/ 恢复/ 数据/ Page9Bm/ ,/ i/ =/ B0/ ,/ m/ / B1/ ,/ m/ / …/ / Bi/ -/ 1/ ,/ m/ / Bi/ +/ 1/ ,/ m/ / …/ / 假设/ 图/ 7/ 中/ 失效/ 磁盘/ 为/ D0/ ,/ 则/ 磁盘/ 数据恢复/ 分为/ 两/ 部分/ B0/ ,/ m/ =/ B1/ ,/ m/ / B2/ ,/ m/ / B3/ ,/ m/ / SPm/ (/ m/ =/ 0/ ,/ 1/ ,/ 2/ )/ (/ 3/ )/ 校验/ 盘/ 失效/ ./ 校验/ 盘/ 失效/ 时/ ,/ 如果/ PLBA/ 的/ 值/ 为/ “/ -/ 1/ ”/ ,/ 说明/ 阵列/ 中/ 还/ 没有/ 数据/ 写入/ ,/ 等同于/ 第/ 1/ 种/ 情况/ ,/ 只/ 需/ 将/ 其/ 替换/ 即可/ ./ 如果/ PLBA/ 的/ 值/ 非/ “/ -/ 1/ ”/ ,/ 根据/ 地址/ 指针/ PLBA/ 计算/ 出/ 当前/ 写入/ 块/ 所在/ 的/ 组号/ g/ 和/ 条带/ 号/ s/ ,/ 校验/ 盘/ 的/ 数据恢复/ 也/ 分/ 两/ 部分/ :/ 0/ ~/ s/ 条带/ 以及/ (/ s/ +/ 1/ )/ ~/ S/ 条带/ ./ 0/ ~/ s/ 条带/ 部分/ 校验/ 值/ 由/ 0/ ~/ g/ 组同/ 条带/ 同/ 位置/ 数据/ 块/ 计算/ 获得/ ,/ 可/ 根据/ 式/ (/ 14/ )/ 计算/ 恢复/ 数据/ ;/ (/ s/ +/ 1/ )/ ~/ S/ 条带/ 部分/ 校验/ 值/ 由/ 0/ ~/ (/ g/ -/ 1/ )/ 组同/ 条带/ 同/ 位置/ 的/ 数据/ 块/ 计算/ 获得/ ,/ 可/ 根据/ 式/ (/ 15/ )/ 计算/ 恢复/ 数据/ SPm/ =/ B0/ ,/ m/ / B1/ ,/ m/ / …/ / B/ (/ g/ +/ 1/ )/ ×/ NG/ -/ 1/ ,/ m/ (/ 0/ / m/ / s/ )/ SPm/ =/ B0/ ,/ m/ / B1/ ,/ m/ / …/ / Bg/ ×/ NG/ -/ 1/ ,/ m/ (/ s/ </ m/ / S/ )/ (/ 15/ )/ 假设/ 图/ 7/ 中/ 失效/ 磁盘/ 为/ 校验/ 盘/ P/ ,/ 则/ 校验/ 数据恢复/ 分为/ 两/ 部分/ SPm/ =/ B0/ ,/ m/ / B1/ ,/ m/ / B2/ ,/ m/ / B3/ ,/ m/ (/ m/ =/ 0/ ,/ 1/ ,/ 2/ )/ 与/ S/ -/ RAID/ 相比/ ,/ LS/ -/ RAID/ 在/ 磁盘/ 失效/ 后/ 进行/ 数据恢复/ 时/ ,/ 读盘/ 数量/ 、/ 次数/ 以及/ 计算/ 量/ 都/ 有所/ 减少/ ,/ 磁盘/ 恢复/ 时间/ 缩短/ ,/ 提高/ 了/ 存储系统/ 的/ 可靠性/ ./ 另外/ ,/ 在/ 能耗/ 方面/ ,/ 由于/ 不/ 需要/ 启动/ 尚未/ 写入/ 数据/ 的/ 磁盘/ ,/ 因此/ ,/ 可以/ 降低/ 存储系统/ 能耗/ ./ 5/ 性能/ 测试/ 及/ 能耗/ 大规模/ 存储系统/ 通常/ 有/ 数百/ 甚至/ 数千/ 磁盘/ 组成/ ,/ 为了/ 方便管理/ ,/ 通常/ 将/ 整个/ 存储系统/ 划分/ 为/ 若干/ 子/ 存储系统/ ,/ 每个/ 子系统/ 一般/ 有/ 12/ 块/ 或/ 16/ 块/ 磁盘/ 组成/ ,/ 构成/ 一个/ RAID/ 结构/ ,/ 称为/ RAID/ 子系统/ ./ 为/ 测试/ LS/ -/ RAID/ 的/ 性能/ 和/ 节能/ 效果/ ,/ 在/ Linuxkernel2/ ./ 6.35/ ./ 32/ 下/ ,/ 构建/ 了/ 由/ 12/ 块/ 硬盘/ 组成/ 的/ LS/ -/ RAID5/ ,/ 条带/ 内/ 数据/ 块/ 大小/ 为/ 64KB/ ,/ 对比/ 测试/ 相同/ 配置/ 下/ 的/ S/ -/ RAID5/ ,/ 实验/ 结果/ 适用/ 于/ 大规模/ 存储系统/ ./ 服务器/ 参数/ 见表/ 1/ ,/ 磁盘/ 参数/ 见表/ 2/ ./ 接口类型/ 接口类型/ 存储容量/ 活动状态/ 能耗/ 空闲/ 状态/ 能耗/ 关闭/ 状态/ 能耗/ 启动/ 时间/ 5.1/ 性能/ 测试/ LS/ -/ RAID/ 分组/ 规模/ 可以/ 根据/ 应用/ 系统/ 需求/ 调整/ 大小/ ,/ 即/ 根据/ 应用/ 系统对/ 存储/ 性能/ 的/ 要求/ 不同/ ,/ 设定/ 不同/ 的/ 磁盘/ 组/ 规模/ ./ 实验/ 中/ 分别/ 对/ 由/ 12/ 块/ 磁盘/ 组成/ 的/ S/ -/ RAID5/ 和/ LS/ -/ RAID5/ 进行/ 了/ 3/ 种/ 不同/ 分组/ 方案/ 的/ 测试/ ,/ 分别/ 为/ 每组/ 1/ 块/ 磁盘/ 、/ 每组/ 2/ 块/ 磁盘/ 和/ 每组/ 3/ 块/ 磁盘/ ./ 实验/ 中/ ,/ 我们/ 利用/ Iometer/ 磁盘/ 测试工具/ ,/ 在/ 100/ %/ 连续/ 负载/ 下/ ,/ 对/ 不同/ 分组/ 方案/ 的/ S/ -/ RAID5/ 采用/ 写/ 优化/ 策略/ 前后/ 分别/ 进行/ 测试/ ,/ 如图/ 8/ 、/ 图/ 9/ 和/ 图/ 10/ 所示/ ./ 图/ 8/ 所示/ 为/ 对/ 每组/ 只/ 包含/ 1/ 块/ 磁盘/ S/ -/ RAID5/ 和/ LS/ -/ RAID5/ 进行/ 不同/ 写/ 请求/ 大小/ 的/ 连续/ 写/ 测试/ 结果/ ./ 当写/ 请求/ 小于/ 64KB/ 时/ ,/ S/ -/ RAID/ 的/ 写/ 性能/ 非常/ 差/ ,/ 数据/ 传输率/ 低于/ 5MB/ // s/ (/ 如图/ 8/ 所示/ )/ ,/ 此时/ 写/ 优化/ 对/ 存储系统/ 性能/ 影响/ 也/ 最大/ ,/ LS/ -/ RAID/ 较/ S/ -/ RAID/ 的/ 性能/ 提升/ 非常明显/ ,/ 性能/ 提升/ 最大值/ 出现/ 在/ 写/ 请求/ 为/ 16KB/ 时/ ,/ 加速/ 比达/ 586/ %/ ,/ 这/ 主要/ 是因为/ 写/ 对齐/ 减少/ 了/ 校验/ 值/ 的/ 计算/ 次数/ ,/ 提高/ 了/ 写/ 效率/ ,/ 并且/ 预读/ 策略/ 减少/ 了/ 存储系统/ 的/ 写/ 延迟/ ,/ 因此/ 对系统/ 性能/ 影响/ 较大/ ./ 但/ 此时/ 无论是/ S/ -/ RAID/ 还是/ LS/ -/ RAID/ ,/ 写/ 性能/ 绝对值/ 都/ 非常低/ ,/ LS/ -/ RAID/ 的/ 数据/ 传输率/ 最大值/ 仍/ 低于/ 20MB/ // s/ ./ 当写/ 请求/ 增大/ 到/ 128KB/ 时/ ,/ 写/ 性能/ 有/ 了/ 较大/ 的/ 提高/ ,/ 最大值/ 出现/ 在/ 写/ 请求/ 大小/ 为/ 1MB/ 时/ ,/ S/ -/ RAID/ 和/ LS/ -/ RAID/ 的/ 数据/ 传输率/ 分别/ 为/ 52.7/ MB/ // s/ 和/ 82.3/ MB/ // s/ ./ 当写/ 请求/ 较大/ 时/ ,/ 对于/ 顺序/ 写/ 操作/ ,/ 写/ 对齐/ 策略/ 对/ 性能/ 影响/ 不/ 大/ ,/ 此时/ 加速/ 比仅/ 约/ 为/ 156/ %/ ,/ 但/ 数据/ 传输率/ 的/ 绝对值/ 提升/ 了/ 近/ 30MB/ // s/ ./ 当写/ 请求/ 大于/ 1MB/ // s/ 时/ ,/ 系统/ 的/ 性能/ 趋于稳定/ ,/ S/ -/ RAID/ 的/ 数据/ 传输率/ 约/ 为/ 50MB/ // s/ ,/ LS/ -/ RAID/ 的/ 数据/ 传输率/ 为/ 80MB/ // s/ 左右/ ./ Page10/ 图/ 8S/ -/ RAID/ 与/ LS/ -/ RAID/ 性能/ 对比/ 测试/ (/ 1/ 磁盘/ // 组/ )/ 图/ 9S/ -/ RAID/ 与/ LS/ -/ RAID/ 性能/ 对比/ 测试/ (/ 2/ 磁盘/ // 组/ )/ 图/ 10S/ -/ RAID/ 与/ LS/ -/ RAID/ 性能/ 对比/ 测试/ (/ 3/ 磁盘/ // 组/ )/ 图/ 9/ 和/ 图/ 10/ 分别/ 为/ 每个/ 磁盘/ 组/ 包括/ 2/ 块/ 磁盘/ 和/ 3/ 块/ 磁盘/ 的/ S/ -/ RAID/ 和/ LS/ -/ RAID/ 的/ 性能/ 测试/ 结果/ ./ 与/ 图/ 8/ 对比/ 可知/ ,/ 采用/ 不同/ 分组/ 规模/ 大小/ 的/ S/ -/ RAID/ 和/ LS/ -/ RAID/ 性能/ 变化/ 基本一致/ ./ 在/ 写/ 请求/ 大小/ 偏/ 小时/ ,/ LS/ -/ RAID/ 较/ S/ -/ RAID/ 性能/ 提升/ 明显/ ,/ 但是/ 存储系统/ 的/ 整体/ 性能/ 并/ 不/ 理想/ ,/ 数据/ 传输率/ 很/ 低/ ./ 每组/ 2/ 块/ 数据/ 盘/ 的/ LS/ -/ RAID/ 较/ S/ -/ RAID/ 的/ 写/ 性能/ 加速/ 比/ 最大值/ 出现/ 在/ 写/ 请求/ 大小/ 为/ 32KB/ 时/ ,/ 加速/ 比达/ 507/ %/ ;/ 每组/ 3/ 块/ 数据/ 盘/ 的/ S/ -/ RAID/ 和/ LS/ -/ RAID/ 性能/ 加速/ 比/ 最大值/ 出现/ 在/ 写/ 请求/ 大小/ 为/ 16KB/ 时/ ,/ 加速/ 比达/ 395/ %/ ./ 图/ 9/ 中/ 在/ 写/ 请求/ 大于/ 64KB/ 时/ ,/ 图/ 10/ 中/ 在/ 写/ 请求/ 大于/ 128KB/ 时/ ,/ 存储系统/ 性能/ 开始/ 明显/ 提升/ ./ 每组/ 2/ 块/ 数据/ 盘/ 的/ S/ -/ RAID/ 性能/ 最大值/ 出现/ 在/ 写/ 请求/ 大小/ 为/ 1MB/ 时/ ,/ 数据/ 传输率/ 为/ 80.1/ MB/ // s/ ,/ LS/ -/ RAID/ 数据/ 传输率/ 最大值/ 131.6/ MB/ // s/ ,/ 性能/ 提升/ 64/ %/ ./ 每组/ 3/ 块/ 数据/ 盘/ 的/ S/ -/ RAID/ 性能/ 最大值/ 出现/ 在/ 写/ 请求/ 块/ 大小/ 为/ 2MB/ 时/ ,/ 数据/ 传输率/ 为/ 123.2/ MB/ // s/ ,/ 而/ LS/ -/ RAID/ 的/ 最大值/ 出现/ 在/ 写/ 请求/ 大小/ 为/ 4MB/ 时/ ,/ 数据/ 传输率/ 为/ 196.9/ MB/ // s/ ./ 写/ 请求/ 大小/ 为/ 2MB/ 和/ 4MB/ 时/ 的/ 性能/ 提升/ 分别/ 为/ 60/ %/ 和/ 63/ %/ ./ 实验/ 表明/ ,/ LS/ -/ RAID/ 在/ 以/ 连续/ 写为/ 主要/ 特征/ 的/ 应用/ 中/ ,/ 避免/ 了/ 对/ 数据/ 盘旧/ 数据/ 的/ 读/ 操作/ ,/ 降低/ 了/ 因/ “/ 读/ -/ 改/ -/ 写/ ”/ 带来/ 的/ 写/ 惩罚/ ,/ 同时/ ,/ 结合/ 磁盘/ 组内/ 写/ 对齐/ 策略/ 以及/ 对/ 旧/ 校验/ 数据/ 的/ 预读/ ,/ 使/ 存储系统/ 性能/ 得到/ 显著/ 提升/ ./ 5.2/ 能耗/ 磁盘/ 的/ 能耗/ 包括/ 两/ 部分/ :/ 电机/ 能耗/ (/ ±/ 12V/ )/ 和/ 磁盘/ 中/ 电子设备/ 能耗/ (/ ±/ 5V/ )/ ./ 参照/ 文献/ [/ 9/ ]/ 中/ 的/ 实验/ 方案/ ,/ 在/ 磁盘/ 与/ 电源/ 之间/ 接入/ 万用表/ (/ 如图/ 11/ 所示/ )/ ./ S/ -/ RAID/ 和/ LS/ -/ RAID/ 的/ 能耗/ 包括/ 所有/ 处于/ 运行/ 状态/ 的/ 磁盘/ 和/ 处于/ 关闭/ 状态/ 的/ 磁盘/ 能耗/ 之/ 和/ ./ 服务器/ 定期/ 采样/ 万用表/ 的/ 电流值/ ,/ 通过/ 公式/ W/ =/ ∑/ iIi/ ,/ 计算/ S/ -/ RAID/ 和/ LS/ -/ RAID/ 的/ 实时/ 功率/ ./ 图/ 12/ 和/ 图/ 13/ 分别/ 为/ S/ -/ RAID/ 和/ LS/ -/ RAID/ 不同/ 分组/ 方案/ 下/ 1h/ 实时/ 能耗/ ./ Page11/ 为/ 获得/ 稳定/ 能耗/ 值/ ,/ 图/ 12/ 和/ 图/ 13/ 数据/ 均/ 在/ 系统/ 正常/ 运行/ 0.5/ h/ 后/ 开始/ 采样/ ,/ 采样/ 周期/ 为/ 1s/ ./ 图/ 12/ 为/ S/ -/ RAID/ 每组/ 分别/ 包括/ 1/ 块/ 磁盘/ 、/ 2/ 块/ 磁盘/ 和/ 3/ 块/ 磁盘/ 时/ 的/ 实时/ 能耗/ ./ 3/ 种/ 分组/ 方案/ 的/ 平均/ 能耗/ 分别/ 为/ 14.128/ W/ 、/ 18.884/ W/ 和/ 23.393/ W/ ./ 图/ 13/ 为/ 相同/ 情况/ 下/ LS/ -/ RAID/ 的/ 实时/ 能耗/ ,/ 3/ 种/ 分组/ 方案/ 的/ 平均/ 能耗/ 分别/ 为/ 13.934/ W/ 、/ 17.991/ W/ 和/ 22.522/ W/ ./ LS/ -/ RAID/ 数据/ 分/ 区内/ 数据/ 全部/ 为/ 顺序/ 写/ ,/ 同时/ ,/ 由于/ 超级/ 块/ 常驻/ 内存/ ,/ 只是/ 定期/ 写回/ 超级/ 块/ 分区/ ,/ 减少/ 了/ 磁盘/ 的/ 寻道/ 次数/ 因此/ ,/ LS/ -/ RAID/ 的/ 能耗/ 峰值/ 和/ 峰值/ 出现/ 的/ 次数/ 都/ 有所/ 降低/ ,/ 平均/ 能耗/ 也/ 略有/ 减少/ 但/ 并/ 不/ 明显/ ./ 对比/ 写/ 性能/ 测试/ 结果/ 可以/ 看出/ ,/ LS/ -/ RAID/ 较/ S/ -/ RAID/ 的/ 写/ 性能/ 能耗/ 比有/ 较大/ 提升/ ,/ 图/ 14/ 为/ S/ -/ RAID/ 在/ 写/ 请求/ 大小/ 为/ 1MB/ // s/ 时/ 采用/ 优化/ 策略/ 前后/ 的/ 写/ 性能/ 能耗/ 对比/ 图/ ./ 图/ 14S/ -/ RAID/ 与/ LS/ -/ RAID1h/ 写/ 性能/ // 能耗/ 对比/ 图由图/ 14/ 可知/ ,/ 在/ 相同/ 能耗/ 条件/ 下/ ,/ 3/ 种/ 分组/ 方案/ 的/ LS/ -/ RAID/ 较/ S/ -/ RAID/ 写/ 性能/ 能耗/ 对/ 比值/ 分别/ 提升/ 了/ 58.34/ %/ 、/ 72.45/ %/ 和/ 69.42/ %/ ./ 在/ 应用/ 系统对/ 性能需求/ 不变/ 的/ 情况/ 下/ ,/ LS/ -/ RAID/ 需要/ 开启/ 的/ 磁盘/ 数量/ 更/ 少/ ,/ 达到/ 了/ 更佳/ 的/ 节能/ 效果/ ./ 表/ 3/ 列出/ 了/ 不同/ 系统/ 最高/ 带宽/ 需求/ 下/ ,/ S/ -/ RAID/ 和/ LS/ -/ RAID/ 所/ 需要/ 开启/ 的/ 数据/ 盘/ 数目/ 及其/ 节能/ 效果/ ./ 表/ 3/ 性能需求/ 与/ S/ -/ RAID/ 所需/ 开启/ 的/ 磁盘/ 数目/ 系统/ 最高/ 带宽/ 需求/ // (/ MB/ // s/ )/ </ 50/ </ 80/ </ 120/ </ 130/ </ 160/ </ 190/ 表/ 3/ 表明/ ,/ 在/ 对/ 应用/ 系统/ 提供/ 相同/ 写/ 性能/ 时/ ,/ LS/ -/ RAID/ 需要/ 同时/ 处于/ 运行/ 状态/ 的/ 磁盘/ 数目/ 明显/ 减少/ ./ 例如/ 当/ 应用/ 系统/ 的/ 最高/ 带宽/ 需求/ 低于/ 50MB/ // s/ 时/ ,/ 存储系统/ 需要/ 开启/ 1/ 数据/ 盘/ +/ 1/ 校验/ 盘/ 即可/ 满足/ 性能/ 要求/ ./ 当/ 应用/ 系统/ 的/ 最高/ 带宽/ 需要/ 为/ 100MB/ // s/ 时/ ,/ S/ -/ RAID/ 每组/ 磁盘/ 数/ 需要/ 3/ 块/ 才能/ 满足/ 系统/ 性能需求/ ,/ 而/ LS/ -/ RAID/ 每组/ 2/ 块/ 数据/ 盘/ 即可/ ,/ 同时/ 处于/ 运行/ 状态/ 的/ 数据/ 盘/ 数目/ 由/ 原来/ 的/ 3/ 块/ 减少/ 到/ 2/ 块/ (/ 除/ 数据/ 盘/ 之外/ ,/ 还要/ 开启/ 1/ 块/ 校验/ 盘/ )/ ./ 处于/ 运行/ 状态/ 磁盘/ 数目/ 的/ 减少/ ,/ 进一步/ 降低/ 了/ LS/ -/ RAID/ 的/ 能耗/ ./ 因此/ ,/ 与/ S/ -/ RAID/ 相比/ ,/ LS/ -/ RAID/ 可以/ 在/ 开启/ 相同/ 数目/ 磁盘/ 情况/ 下/ ,/ 提供/ 更/ 高/ 的/ 系统/ 性能/ ;/ 同理/ ,/ 在/ 提供/ 相同/ 系统/ 性能/ 时/ ,/ 可以/ 减少/ 处于/ 运行/ 状态/ 的/ 磁盘/ 数目/ ,/ 进一步/ 降低/ 存储系统/ 的/ 能耗/ ./ 5.3/ 数据恢复/ 与/ 可靠性/ S/ -/ RAID4/ // 5/ 采用/ RAID4/ // 5/ 冗余/ 方案/ ,/ 可以/ 同时/ 容忍/ 1/ 个/ 磁盘/ 失效/ ./ 其/ 可靠性/ 可由式/ (/ 17/ )/ 表示/ [/ 5/ ,/ 9/ ]/ ,/ MTTFS/ -/ RAID/ =/ Nactivedisksins/ -/ raid/ ×/ (/ Nactivedisksins/ -/ raid/ +/ 1/ )/ ×/ MTTRdisk/ 式/ (/ 17/ )/ 中/ ,/ MTTFS/ -/ RAID/ 表示/ S/ -/ RAID/ 的/ 平均/ 失效/ 间隔时间/ (/ themeantimetofailure/ )/ ,/ MTTFdisk/ 表示/ 磁盘/ 的/ 平均/ 失效/ 间隔时间/ ,/ MTTRdisk/ 表示/ 磁盘/ 的/ 平均/ 修复/ 时间/ (/ themeantimetorepair/ )/ ,/ Nactivedisksins/ -/ raid/ 表示/ S/ -/ RAID/ 中/ 处于/ 活动状态/ 的/ 磁盘/ 数/ ./ 在/ LS/ -/ RAID/ 中/ ,/ 设/ 磁盘/ 容量/ 表示/ 为/ Cfull/ ,/ 磁盘/ 失效/ 时/ 平均/ 已/ 写入/ 数据量/ 表示/ 为/ Cwrite/ ,/ 由/ 4.4/ 节/ 内容/ 可知/ ,/ 与/ S/ -/ RAID/ 相比/ ,/ LS/ -/ RAID/ 修复/ 磁盘/ 时所/ 读取数据/ 量/ 、/ 计算/ 量/ 和/ 写入/ 数据量/ 都/ 有所/ 减少/ ,/ 磁盘/ 修复/ 时间/ 要/ 小于/ Cwrite/ // Cfull/ ×/ MTTRdisk/ ,/ 则/ LS/ -/ RAID/ 的/ 平均/ 失效/ 间隔时间/ 为/ MTTFLS/ -/ RAID/ ≈/ Cwrite/ ×/ Nactivedisksins/ -/ raid/ ×/ (/ Nactivedisksins/ -/ raid/ +/ 1/ )/ ×/ MTTRdiskPage12/ 假设/ 磁盘/ 失效/ 时/ 平均/ 写入/ 数据/ Cwrite/ =/ 1/ // 2/ ×/ Cfull/ ,/ 则/ MTTFLS/ -/ RAID/ ≈/ 2/ ×/ MTTFS/ -/ RAID/ ,/ LS/ -/ RAID/ 的/ 平均/ 失效/ 间隔时间/ 比/ S/ -/ RAID/ 延长/ 了/ 近/ 一倍/ ./ 实验/ 中/ ,/ 由/ 12/ 块/ 磁盘/ 组成/ 的/ S/ -/ RAID5/ 和/ LS/ -/ RAID5/ ,/ 数据/ 盘为/ D0/ ~/ D10/ ,/ 分成/ 5/ 个/ 磁盘/ 组/ ,/ 校验/ 数据/ 均匀分布/ 在/ 所有/ 数据/ 盘/ 上/ ,/ 磁盘/ D11/ 为/ 热备/ 盘/ ./ 为/ 方便/ 计算/ ,/ 磁盘/ 数据/ 分区/ 大小/ 设定/ 为/ 440GB/ ,/ 分表/ 4/ 数据恢复/ 失效/ 磁盘/ D3D0D1D2D3D4D5D6D7D8D9D1S/ -/ RAID7208906/ —/ 7203506720350572089067208906720890672089067203505720890672089064644.95221099/ ./ 60LS/ -/ RAID7208906/ —/ 494906023276216553606553606553606553603616236553606553603952.2869537/ ./ 15S/ -/ RAID720890672089067203505/ —/ 72089067208906720890672089067203505720890672089064753.15226249/ ./ 90LS/ -/ RAID233308323330832328003/ —/ 00003615406553606553601081.0119374/ ./ 01/ 时/ 对/ 旧/ 数据/ 的/ 读/ 操作/ ,/ 降低/ 了/ “/ 读/ -/ 改/ -/ 写/ ”/ 带来/ 的/ 写表/ 4/ 中/ 数据/ 分别/ 对/ 失效/ 磁盘/ D1/ 和/ D3/ 数据恢复/ 惩罚/ ,/ 提高/ 存储系统/ 的/ 写/ 性能/ ,/ 时/ ,/ S/ -/ RAID/ 和/ LS/ -/ RAID/ 的/ 修复/ 时间/ 、/ 能耗/ 和/ 读操/ 实验/ 表明/ ,/ 与/ S/ -/ RAID/ 相比/ ,/ 在/ 能耗/ 不变/ 的/ 情作/ ,/ 其中/ ,/ 修复/ 时间/ 不/ 包括/ 存储系统/ 检测/ 故障/ 盘/ 时间/ 况下/ ,/ LS/ -/ RAID/ 的/ 写/ 性能/ 有/ 了/ 较大/ 提高/ ,/ 降低/ 了/ 单/ 以及/ 磁盘/ 替换/ 时间/ ./ 在/ 写入/ 1TB/ 数据/ 后/ ,/ 数据/ 盘/ D0/ 位/ 性能/ 下/ 的/ 能耗/ ./ 和/ D1/ 被/ 写/ 满/ ,/ D2/ 和/ D3/ 写入/ 了/ 前/ 3/ 个/ 条带/ 组/ ,/ 最后/ 写入/ 的/ 数据/ 位于/ D2/ 和/ D3/ 的/ VGroup2/ ,/ 校验/ 数据/ 位于/ D8/ (/ VGroup2/ 未写/ 满/ )/ ./ D1/ 磁盘/ 失效/ 后/ ,/ 数据恢复/ 需要/ 读取/ D0/ 、/ D2/ 和/ D3/ 中/ 已/ 写入/ 数据/ 和/ 各/ 盘中/ 校验/ 数据/ ./ D3/ 磁盘/ 失效/ 后/ ,/ 数据恢复/ 需要/ 读取/ D0/ 、/ D1/ 和/ D2/ 中/ 相同/ 位置/ 数据/ 以及/ D8/ 、/ D9/ 和/ D10/ 中/ 的/ 校验/ 数据/ ./ 由表/ 4/ 中/ 可以/ 看出/ ,/ 与/ S/ -/ RAID/ 相比/ ,/ LS/ -/ RAID/ 中/ 数据恢复/ 所/ 需/ 时间/ 、/ 能耗/ 以及/ 对/ 磁盘/ 的/ 读/ 操作/ 都/ 大幅/ 减少/ ./ 失效/ 磁盘/ 已/ 写入/ 数据/ 越/ 少时/ ,/ 节省/ 能耗/ 和/ 修复/ 时间/ 效果/ 越/ 明显/ ./ 表/ 4/ 中/ ,/ D2/ 、/ D3/ 和/ D8/ 中/ 读取/ 的/ 数据/ 块/ 较/ 理论值/ 要/ 小/ ,/ 这/ 是因为/ 最后/ 写入/ 磁盘/ 的/ 部分/ 数据/ 和校验/ 数据/ 仍然/ 驻留/ 内存/ ,/ 这部分/ 数据/ 无需/ 再/ 从/ 磁盘/ 中/ 读出/ ./ 文献/ [/ 9/ ]/ 表明/ S/ -/ RAID/ 较/ RAID/ 有着/ 更/ 高/ 的/ 可靠性/ ,/ 表/ 3/ 和表/ 4/ 表明/ ,/ LS/ -/ RAID/ 可以/ 有效/ 降低/ 存储系统/ 中/ 处于/ 活动状态/ 磁盘/ 数/ Nactivedisksins/ -/ raid/ 和/ 磁盘/ 修复/ 时间/ MTTRdisk/ ,/ 在/ 磁盘/ 参数/ 不变/ 的/ 情况/ 下/ ,/ 增加/ 了/ LS/ -/ RAID/ 的/ 平均/ 失效/ 间隔时间/ MTTFLS/ -/ RAID/ ,/ 提高/ 了/ 存储系统/ 的/ 可靠性/ ./ 6/ 结论/ 与/ 展望/ 本文/ 针对/ S/ -/ RAID/ 在/ 写/ 数据/ 时因/ 写/ 惩罚/ 导致/ 的/ 性能/ 下降/ ,/ 提出/ 一种/ 优化结构/ LS/ -/ RAID/ ./ LS/ -/ RAID/ 通过/ 磁盘分区/ 减少/ 了/ 随机/ 访问/ 对/ 存储系统/ 性能/ 的/ 影响/ ./ 同时/ 数据/ 增量/ 校验/ 算法/ 避免/ 了/ 写入/ 数据/ 为/ 11/ 个/ 条带/ 组/ ,/ 每个/ 条带/ 块/ 大小/ 为/ 64KB/ ./ 在/ 写入/ 1TB/ 数据/ 后/ ,/ 人为/ 设置/ 磁盘/ 失效/ ,/ 利用/ blktrace/ 工具/ 跟踪/ 并/ 记录/ S/ -/ RAID/ 和/ LS/ -/ RAID/ 在/ 数据恢复/ 中/ 的/ I/ // O/ 请求/ ,/ 并/ 通过/ 5.2/ 节中/ 的/ 实验/ 方案/ 测量/ 并/ 计算/ 数据恢复/ 过程/ 中/ S/ -/ RAID/ 的/ 能耗/ ./ 表/ 4/ 为/ 数据恢复/ 时/ 的/ I/ // O/ 请求/ 和/ 能耗/ ./ 采用/ 数据/ 增量/ 校验/ 的/ 前提条件/ 是/ 数据/ 顺序/ 写入/ ,/ 为/ 满足条件/ ,/ 本/ 研究/ 中/ 使用/ 了/ 特定/ 的/ 日志/ 式/ 文件系统/ NILFS2/ ,/ 降低/ 了/ S/ -/ RAID/ 应用/ 的/ 普遍性/ ,/ 未来/ 工作/ 之一/ 是/ 通过/ 区间/ 映射/ 管理/ 和/ 写/ 缓冲/ 策略/ ,/ 使/ 这/ 一/ 优化/ 策略/ 适用/ 于/ 不同/ 文件系统/ ./ 另外/ ,/ 考虑/ 引入/ SSD/ 固态/ 盘/ ,/ 单独/ 存储/ 和/ 管理/ 超级/ 块/ 和/ 元/ 数据/ ,/ 提高/ S/ -/ RAID/ 的/ 读写/ 性能/ ./ 

