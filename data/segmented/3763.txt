Page1/ 一种/ 基于/ 高斯/ 隐/ 变量/ 模型/ 的/ 分类/ 算法/ 王/ 秀美/ 高新/ 波/ 李洁/ (/ 西安电子科技大学/ 电子/ 工程学院/ 西安/ 710071/ )/ 摘要/ 高斯/ 过程/ 隐/ 变量/ 模型/ 是/ 近年来/ 新兴/ 的/ 无/ 监督/ 降维/ 方法/ ,/ 它/ 可以/ 找到/ 高维/ 数据/ 的/ 低/ 维流形/ 结构/ ./ 但是/ 由于/ 高斯/ 过程/ 隐/ 变量/ 模型/ 是/ 无/ 监督/ 的/ 概率/ 降维/ 方法/ ,/ 所以/ 当/ 数据/ 集中/ 的/ 样本/ 有/ 类别/ 标记/ 信息/ 时/ ,/ 高斯/ 过程/ 隐/ 变量/ 模型/ 不能/ 利用/ 这些/ 监督/ 信息/ ,/ 实现/ 分类/ 的/ 任务/ ./ 为了/ 使/ 高斯/ 过程/ 隐/ 变量/ 模型/ 可以/ 处理/ 分类/ 任务/ ,/ 文中/ 提出/ 了/ 一种/ 监督/ 的/ 高斯/ 过程/ 隐/ 变量/ 模型/ 分类/ 模型/ ./ 通过/ 最大化/ 后验/ 似然/ 的/ 方法/ 确定/ 观测/ 数据/ 在/ 隐/ 空间/ 的/ 坐标/ ,/ 同时/ 可以/ 完成/ 分类/ 任务/ ./ 实验/ 结果/ 证明/ 了/ 该/ 模型/ 可以/ 有效/ 地/ 用于/ 分类/ ./ 关键词/ 高斯/ 过程/ ;/ 监督/ 学习/ ;/ 降维/ ;/ 线性/ 判别分析/ 1/ 引言/ “/ 维数/ 灾难/ ”/ 是/ 模式识别/ 和/ 机器/ 学习/ 的/ 常见问题/ ,/ 在/ 人脸识别/ 、/ 动态/ 跟踪/ 、/ 语音/ 识别/ 等/ 实际/ 问题/ 中/ ,/ 高维/ 数据/ 之间/ 往往/ 存在/ 着/ 大量/ 的/ 冗余/ ,/ 如何/ 消除/ 冗余/ ,/ 寻找/ 数据/ 间/ 的/ 内在联系/ ,/ 从而/ 减少/ 数据处理/ 过程/ 中/ 的/ 计算/ 量/ ,/ 是/ 当前/ 降维/ 技术/ 研究/ 的/ 重点/ ./ 经典/ 的/ 降维/ 方法/ 多为/ 线性/ 方法/ ,/ 即/ 寻找/ 高维/ 数据/ 的/ 某个/ 线性/ 子/ 空间/ ,/ 将/ 高维/ 数据/ 投影/ 到/ 这个/ 子/ 空间/ 中/ ,/ 再/ 进行/ 识别/ 或/ 分析/ ,/ 大大降低/ 计算/ 量/ ./ 主/ 成分/ 分析/ (/ PrincipleComponentAnalysis/ ,/ PCA/ )/ [/ 1/ -/ 2/ ]/ 和/ 线/ Page2/ 性/ 判别分析/ (/ LinearDiscriminantAnalysis/ ,/ LDA/ )/ [/ 3/ ]/ 是/ 最具/ 代表性/ 的/ 两种/ 线性/ 子/ 空间/ 分析方法/ ./ PCA/ 是/ 一种/ 无/ 监督/ 的/ 降维/ 方法/ ,/ 它/ 的/ 主要/ 思想/ 是/ 用/ 较/ 少/ 的/ 综合/ 变量/ 来/ 代替/ 原来/ 较/ 多/ 的/ 变量/ ,/ 同时/ 要求/ 这/ 几个/ 综合/ 变量/ 互不/ 相关/ ,/ 并/ 尽可能/ 多地/ 表示/ 原来/ 数据/ 的/ 能量/ ./ 它/ 是/ 将/ 多/ 指标/ 化为/ 少数几个/ 综合/ 指标/ 的/ 一种/ 统计分析/ 方法/ ;/ LDA/ 是/ 目前/ 常用/ 的/ 有/ 监督/ 的/ 线性/ 降维/ 方法/ ,/ 它/ 利用/ 已知/ 的/ 监督/ 信息/ 进行/ 数据/ 降维/ ,/ 降维/ 过程/ 最大化/ 样本/ 类间/ 散度/ ,/ 同时/ 最小化/ 类内/ 散度/ ./ 由/ PCA/ 衍生/ 并/ 推广/ 出/ 的/ 概率/ 主/ 成分/ 分析/ (/ ProbabilisticPCA/ ,/ PPCA/ )/ [/ 4/ ]/ 是/ 一种/ 从/ 高斯/ 隐/ 变量/ 模型/ 扩展/ 出来/ 的/ 概率/ 方法/ ,/ 通过/ 对/ 高维/ 数据/ 的/ 联合/ 似然/ 最大化/ ,/ 求得/ 投影/ 方向/ ./ 由于/ PCA/ 、/ PPCA/ 和/ LDA/ 都/ 是/ 线性/ 降维/ 方法/ ,/ 虽然/ 易于/ 实现/ ,/ 但是/ 对/ 一些/ 结构复杂/ 的/ 数据/ 却/ 无能为力/ ,/ 这/ 是/ 由/ 其/ 本身/ 的/ 线性/ 特性/ 造成/ 的/ ./ 核主/ 成分/ 分析/ (/ KernelPCA/ ,/ KPCA/ )/ [/ 5/ ]/ 和/ 核/ 判别分析/ (/ KernelLDA/ ,/ KDA/ )/ [/ 6/ -/ 7/ ]/ 分别/ 是/ 对/ PCA/ 和/ LDA/ 的/ 非线性/ 推广/ ./ 由于/ 核/ 方法/ 是/ 非线性/ ,/ 多数/ 是/ 不/ 可逆/ 的/ ,/ 即/ 只/ 给出/ 了/ 如何/ 从/ 观测/ 空间/ 到/ 低维隐/ 空间/ 的/ 映射/ ,/ 但/ 对于/ 如何/ 建立/ 从/ 低维到/ 高维/ 数据/ 的/ 映射/ ,/ 没有/ 给出/ 解决/ 方法/ ./ 高斯/ 过程/ 隐/ 变量/ 模型/ (/ GP/ -/ LVM/ )/ 是/ 一种/ 非线性/ 降维/ 技术/ [/ 8/ -/ 9/ ]/ ,/ 是/ PPCA/ 的/ 对偶/ 形式/ ./ GP/ -/ LVM/ 克服/ 了/ 线性/ 降维/ 方法/ 的/ 局限/ ,/ 并且/ 利用/ 高斯/ 过程/ [/ 10/ ]/ 建立/ 了/ 从/ 低维隐/ 空间/ 到/ 高/ 维空间/ 的/ 映射/ 关系/ ./ 通过/ 最大化/ 观测/ 数据/ 的/ 联合/ 密度/ ,/ 优化/ 出/ 高维/ 数据/ 在/ 隐/ 变量/ 空间/ 中/ 的/ 坐标/ 位置/ ./ 然而/ ,/ GP/ -/ LVM/ 所用/ 到/ 的/ 高斯/ 过程/ 映射/ 是/ 在/ 样本/ 的/ 每/ 一维/ 上/ 单独/ 进行/ ,/ 是/ 无/ 监督/ 的/ ,/ 当/ 数据/ 中有/ 监督/ 信息/ ,/ 如/ 类别/ 标记/ 可以/ 利用/ 时/ ,/ GP/ -/ LVM/ 不能/ 有效/ 地/ 利用/ 类标/ ,/ 根据/ 样本/ 的/ 类别/ 结构/ 信息/ ,/ 从而/ 建立/ 有效/ 的/ 分类/ 模型/ ./ 为了/ 利用/ 观测/ 数据/ 的/ 监督/ 信息/ ,/ 文献/ [/ 11/ ]/ 中/ 提出/ 了/ 一种/ 利用/ 类标/ 信息/ 的/ 方法/ ,/ 用/ LDA/ 的/ 能量/ 函数/ 作为/ 隐/ 空间/ 变量/ 的/ 先验/ 知识/ ,/ 最大化/ 隐/ 变量/ 的/ 后验/ 知识/ ,/ 得到/ 降维后/ 的/ 样本/ ./ 此/ 方法/ 虽然/ 可以/ 利用/ 监督/ 信息/ ,/ 使得/ 降维后/ 样本/ 在/ 低/ 维空间/ 保持/ 样本/ 类间/ 散度/ 大/ 、/ 类内/ 散度/ 小/ 的/ 性质/ ,/ 但是/ 它/ 没有/ 从根本上/ 解决/ 如何/ 利用/ 类别/ 信息/ 进行/ 分类/ 的/ 问题/ ./ 本文/ 提出/ 了/ 一种/ 监督/ 的/ GP/ -/ LVM/ (/ SGP/ -/ LVM/ )/ 分类/ 模型/ ,/ 该/ 模型/ 基于/ 隐/ 变量/ 模型/ (/ Latentvariablemodels/ )/ 建立/ ,/ 利用/ 了/ 隐/ 变量/ 空间/ 条件/ 独立/ (/ Conditionalindependent/ )/ 的/ 性质/ [/ 12/ ]/ ./ 与/ 文献/ [/ 11/ ]/ 提出/ 的/ 方法/ 不同/ ,/ SGP/ -/ LVM/ 既/ 考虑/ 观测/ 样本/ 的/ 性质/ ,/ 也/ 考虑/ 已知/ 的/ 类标/ 信息/ ,/ 分别/ 建立/ 了/ 从/ 隐/ 空间/ 到/ 观测/ 样本/ 和/ 类别/ 信息/ 集合/ 的/ 映射/ ,/ 前者/ 达到/ 了/ 降维/ 的/ 目的/ ,/ 后者/ 则/ 实现/ 了/ 分类/ 的/ 任务/ ./ 在/ 本文/ 的/ 后/ 半/ 部分/ ,/ 在/ 各种/ 数据/ 上/ 的/ 实验/ 结果/ 证明/ 了/ 本文/ 方法/ 的/ 有效性/ ./ 本文/ 第/ 2/ 节/ 简单/ 介绍/ PPCA/ 和/ GP/ -/ LVM/ 的/ 相关/ 工作/ ;/ 第/ 3/ 节/ 给出/ 如何/ 利用/ 隐/ 变量/ 过程/ 的/ 条件/ 独立/ 性质/ 建立/ 监督/ 的/ GP/ -/ LVM/ 分类/ 算法/ ,/ 包括/ 如何/ 进行/ 分类/ 和/ 如何/ 确定/ 超/ 参数/ 和/ 降维后/ 隐/ 变量/ 的/ 位置/ ;/ 第/ 4/ 节/ 给出/ 实验/ 结果/ 以/ 证明/ 分类/ 算法/ 的/ 有效性/ ;/ 最后/ 进行/ 总结/ 与/ 讨论/ ./ 2/ 预备/ 知识/ 2.1/ 概率/ 主/ 成分/ 分析/ PPCA/ 是/ PCA/ 方法/ 的/ 概率/ 扩展/ [/ 2/ ]/ ,/ 它/ 建立/ 了/ 一种/ 从/ 低/ 维空间/ (/ 隐/ 变量/ 空间/ )/ 到/ 高/ 维空间/ 的/ 线性/ 映射/ ,/ 若/ 犡/ =/ [/ 狓/ 1/ ,/ 狓/ 2/ ,/ …/ ,/ 狓/ N/ ]/ 表示/ 需要/ 降维/ 的/ 高维/ 数据/ ,/ 犣/ =/ [/ 狕/ 1/ ,/ 狕/ 2/ ,/ …/ ,/ 狕/ N/ ]/ 表示/ 降维后/ 的/ 低维/ 数据/ ,/ 并/ 假设/ 噪声/ η/ n/ ∈/ RD/ 符合/ 独立/ 高斯分布/ ,/ η/ n/ ~/ N/ (/ 0/ ,/ β/ -/ 1/ 犐/ )/ ,/ β/ -/ 1/ 为/ 噪声/ 方差/ ./ 从/ 低/ 维空间/ 到/ 观测/ 空间/ 的/ 映射/ 可以/ 表示/ 为/ 其中/ 线性/ 映射/ 由/ 犠/ ∈/ RD/ ×/ d/ 确定/ ,/ 则/ 观测/ 数据/ 点/ 的/ 似然/ 概率/ 为/ 假设/ 隐/ 变量/ 空间/ 中/ 的/ 点/ 是/ 独立/ 同/ 分布/ 的/ :/ 即/ 通过/ 对隐/ 变量/ 空间/ 中/ 的/ 点/ 积分/ 可以/ 推导/ 出/ 边缘/ 似然/ ,/ P/ (/ 狓/ n/ |/ 犠/ ,/ β/ )/ =/ ∫/ p/ (/ 狓/ n/ |/ 狕/ n/ ,/ 犠/ ,/ β/ -/ 1/ 犐/ )/ p/ (/ 狕/ n/ )/ d/ 狕/ n/ 和/ 观测/ 数据/ 的/ 联合/ 似然/ ,/ 用/ 最大化/ 似然/ 的/ 方法/ 得到/ 投影/ 矩阵/ 犠/ ./ 求/ 似然/ 最大化/ 的/ 方法/ 很多/ ,/ 其中/ EM/ [/ 13/ ]/ 算法/ 是/ 比较/ 常用/ 的/ 一种/ ./ EM/ 算法/ 是/ 求/ 参数/ 极大/ 似然/ 估计/ 的/ 一种/ 简单/ 实用/ 的/ 学习/ 算法/ ,/ 对应/ 于/ 本文/ 中/ 提到/ 的/ PPCA/ ,/ 它/ 包括/ 两个/ 步骤/ :/ E/ -/ step/ ./ 计算/ 数据/ 的/ 对数/ 似然/ 函数/ 期望/ ./ 似然/ 函数/ 为/ Page3/ 若记/ 对数/ 似然/ 函数/ 为/ lnp/ (/ 犡/ ;/ 犠/ )/ ,/ 对数/ 似然/ 函数/ 可以/ 表示/ 为/ 则/ E/ -/ step/ 求/ lnp/ (/ 犡/ ;/ 犠/ )/ 的/ 期望值/ E/ {/ lnp/ (/ 犡/ ;/ 犠/ )/ }/ ./ E/ {/ lnp/ (/ 犡/ ;/ 犠/ )/ }/ =/ {/ 2ln/ β/ +/ 1D/ -/ ∑/ Nn/ =/ 11/ 〈/ 狕/ n/ 〉/ T/ 犠/ T/ (/ 狓/ n/ -/ μ/ )/ +/ 1/ β/ 在/ 上式/ 中/ μ/ 表示/ 观测/ 变量/ 犡/ 的/ 均值/ 并且/ 有/ 其中/ 犕/ =/ 犠/ T/ 犠/ +/ β/ 犐/ ./ M/ -/ step/ ./ 关于/ 投影/ 矩阵/ 犠/ 最大化/ E/ {/ lnp/ (/ 犡/ ;/ 犠/ )/ }/ ,/ 即/ 关于/ E/ {/ lnp/ (/ 犡/ ;/ 犠/ )/ }/ 对/ 犠/ 求导/ ,/ 得到/ 最优/ 的/ 犠/ 值/ ,/ 珦/ 犠/ =/ ∑/ N/ 通过/ 交替/ 使用/ E/ -/ step/ 和/ M/ -/ step/ 直至/ 收敛/ ,/ 收敛性/ 判断/ 可以/ 用/ 相邻/ 两次/ 的/ 迭代/ 得到/ 的/ E/ {/ lnp/ (/ 犡/ ;/ 犠/ )/ }/ 的/ 差值/ 判断/ ./ E/ {/ lnp/ (/ 犡/ ;/ 犠/ )/ }/ t/ +/ 1/ -/ E/ {/ lnp/ (/ 犡/ ;/ 犠/ )/ }/ t/ / ε/ ,/ 则/ E/ {/ lnp/ (/ 犡/ ;/ 犠/ )/ }/ 达到/ 一个/ 极值/ 点/ ,/ 从而/ 得到/ 投影/ 矩阵/ 犠/ ./ 详细/ 步骤/ 请/ 参看/ 文献/ [/ 4/ ]/ ./ 2.2/ 高斯/ 过程/ 隐/ 变量/ 模型/ 条件/ 独立/ 是/ 隐/ 变量/ 模型/ 的/ 重要/ 性质/ ,/ 即/ 在/ 隐/ 变量/ 给定/ 的/ 时候/ ,/ 观测/ 变量/ 的/ 各维/ 之间/ 是/ 独立/ 的/ [/ 12/ ]/ ./ 如图/ 1/ 所示/ ,/ 给出/ 隐/ 变量/ 狕/ 时/ ,/ 观测/ 变量/ 狓/ 各维/ 之间/ 条件/ 独立/ ./ GP/ -/ LVM/ 正是/ 基于/ 这个/ 性质/ 建立/ 了/ 从/ 隐/ 变量/ 到/ 观测/ 变量/ 的/ 映射/ 关系/ ./ GP/ -/ LVM/ 的/ 模型/ 是/ 基于/ 图/ 1/ 建立/ ,/ 从/ 所有/ 的/ 隐/ 变量/ 到/ 观测/ 变量/ 的/ 每/ 一维/ 建立/ 映射/ ,/ 给/ 每个/ 映射/ 一个/ 高斯/ 过程/ 先验/ ,/ 则/ 建立/ 了/ 一个/ 非线性/ 隐/ 变量/ 模型/ ./ 这/ 与/ PPCA/ 不同/ ,/ PPCA/ 是/ 假设/ 各个/ 样本/ 独立/ 同/ 分布/ (/ i/ ./ i/ ./ d/ )/ ,/ 假设/ 降维后/ 的/ 样本/ 服从/ 高斯分布/ 而/ 建立/ 模型/ ./ 假设/ 各/ 维度/ 上/ 的/ 映射/ fd/ 独立/ 同/ 分布/ ,/ 服从/ 高斯/ 过程/ ,/ 则/ 观测/ 数据/ d/ 的/ 似然/ 可/ 表示/ 为/ p/ (/ 狓/ :/ ,/ d/ |/ 犣/ ,/ β/ )/ =/ ∫/ p/ (/ 狓/ :/ ,/ d/ |/ 狕/ n/ ,/ fd/ ,/ β/ -/ 1/ 犐/ )/ p/ (/ fd/ )/ dfd/ 由于/ 各维/ 之间/ 独立/ ,/ 则/ 观测/ 数据/ 的/ 似然/ 可以/ 表示/ 为/ 数据/ 各/ 维度/ 似然/ 的/ 乘积/ P/ (/ 犡/ |/ 犣/ ,/ β/ )/ =/ ∏/ D/ 上式/ 可以/ 表示/ 为/ 矩阵/ 的/ 形式/ :/ P/ (/ 犡/ |/ 犣/ ,/ β/ )/ =/ 1/ 其中/ ,/ 犓/ 是/ 协方差/ 函数/ 矩阵/ ,/ 或者/ 称为/ 核/ 函数/ 矩阵/ ./ 若用/ 线性/ 的/ 核/ 函数/ ,/ 则/ 可以/ 定义/ 为/ 犓/ =/ 犡/ 犡/ T/ +/ β/ -/ 1/ 犐/ ./ 若/ 选用/ 非线性/ 核/ 函数/ 则/ 可以/ 定义/ 为/ k/ (/ 狕/ i/ ,/ 狕/ j/ )/ =/ θ/ rbfexp/ -/ γ/ k/ (/ 狕/ i/ ,/ 狕/ j/ )/ 为/ 矩阵/ 犓/ 的/ 第/ i/ 行第/ j/ 列/ 对应/ 的/ 元素/ ;/ δ/ ij/ 为/ Kroneckerdelta/ 函数/ ./ 这时/ ,/ 从隐/ 空间/ 到/ 高/ 维空间/ 的/ 映射/ 是/ 一个/ 非线性/ 映射/ 的/ 高斯/ 过程/ ,/ 而且/ 对于/ 观测/ 数据/ ,/ 每/ 一维/ 是/ 独立/ 的/ ./ 联合/ 似然式/ (/ 9/ )/ 可/ 进一步/ 简化/ 为/ P/ (/ 犡/ |/ 犣/ ,/ β/ )/ =/ 13/ 监督/ 的/ GP/ -/ LVM/ 分类/ 算法/ GP/ -/ LVM/ 算法/ 可以/ 有效/ 地/ 找到/ 高维/ 数据/ (/ 观测/ 数据/ )/ 在/ 低/ 维空间/ 中上/ 的/ 流形/ ,/ 但是/ 由于/ 它/ 是/ 一种/ 无/ 监督/ 的/ 机器/ 学习/ 方法/ ,/ 所以/ 当/ 我们/ 的/ 观测/ 数据/ 中有/ 类标/ 信息/ 时/ ,/ 它/ 不能/ 利用/ 这些/ 类标/ 信息/ ,/ 实现/ 分类/ 目标/ ./ 针对/ 这个/ 问题/ ,/ 我们/ 将/ 监督/ 信息/ 加入/ GP/ -/ LVM/ 算法/ 中/ ,/ 建立/ 一种/ 有/ 监督/ 的/ GP/ -/ LVM/ 分类/ 模型/ ./ 这种/ 有/ 监督/ 的/ 模型/ 是/ 建立/ 在/ 隐/ 变量/ 模型/ 的/ 条件/ 独立/ 性质/ 之上/ 的/ ,/ 我们/ 参考文献/ [/ 14/ -/ 15/ ]/ 中/ 的/ 思路/ 建立/ 分类/ 模型/ ./ 下面/ ,/ 我们/ 将/ 给出/ 监督/ 的/ GP/ -/ LVM/ 模型/ ,/ 我们/ Page4/ 用/ 犔/ =/ [/ 犾/ 1/ ,/ …/ ,/ 犾/ N/ ]/ T/ 表示/ 输出/ 矩阵/ ,/ 其中/ 犾/ i/ ∈/ RC/ ,/ 或者/ 表示/ 分类/ 中/ 观测/ 数据/ 的/ 类标/ ,/ 例如/ 二值/ 分类/ 时/ ,/ 犾/ i/ ∈/ {/ +/ 1/ ,/ -/ 1/ }/ ./ 我们/ 用/ (/ 犡/ ,/ 犢/ )/ 表示/ 观测/ 数据/ ,/ 则/ 可以/ 建立/ 以下/ 映射/ 关系/ :/ 式/ (/ 18/ )/ 中/ ,/ 函数/ f1/ 表示/ 从隐/ 空间/ (/ 低/ 维空间/ )/ 犣/ 到/ 高/ 维空间/ 犡/ 的/ 映射/ 关系/ ./ 若/ f1/ 为/ 高斯/ 过程/ ,/ 则/ 可以/ 用/ Θ/ 1/ 表示/ 映射/ 的/ 超/ 参数/ ./ 同样/ ,/ 函数/ f2/ 表示/ 从/ 犣/ 到/ 犔/ 的/ 映射/ ,/ 映射/ 的/ 超/ 参数/ 用/ Θ/ 2/ 表示/ ./ 模型/ 中/ 的/ 噪声/ 分别/ 为/ ε/ 狓/ 和/ ε/ 犔/ ,/ 假设/ 为/ 零/ 均值/ 的/ 高斯分布/ ,/ 即/ ε/ 狓/ ~/ N/ (/ 0/ ,/ σ/ 2/ 狓/ 犐/ )/ ,/ ε/ 犔/ ~/ N/ (/ 0/ ,/ σ/ 2/ 上面/ 建立/ 了/ 含/ 噪声/ 的/ 概率模型/ ,/ 接下来/ 推导/ 观测/ 数据/ (/ 犡/ ,/ 犔/ )/ 的/ 似然/ p/ (/ 犡/ ,/ 犔/ |/ 犣/ )/ ./ 由于/ 在/ 隐/ 变量/ 模型/ 中/ ,/ 观测/ 数据/ (/ 犡/ ,/ 犢/ )/ 的/ 各维/ 之间/ 独立/ ,/ 即/ p/ (/ 狓/ ,/ 犾/ |/ 狕/ )/ =/ p/ (/ 狓/ |/ 狕/ )/ p/ (/ 犾/ |/ 狕/ )/ ,/ 所以/ 可以/ 得到/ 观测/ 数据/ 的/ 联合/ 似然/ 为/ 此时/ ,/ 我们/ 利用/ 最大/ 似然/ 原则/ 就/ 可以/ 优化/ 得到/ 隐/ 变量/ 犣/ 的/ 值/ ./ 先对式/ (/ 15/ )/ 求/ 对数/ 可以/ 得到/ 犔/ / lnp/ (/ 犡/ ,/ 犔/ |/ 犣/ )/ =/ lnp/ (/ 犡/ |/ 犣/ )/ +/ lnp/ (/ 犔/ |/ 犣/ )/ (/ 21/ )/ 可以/ 看到/ 等式/ 第/ 1/ 项为/ lnp/ (/ 犡/ |/ 犣/ )/ =/ -/ DN/ 即为/ 模型/ GP/ -/ LVM/ 中/ 的/ 似然/ ,/ 是/ 属于/ 原来/ 无/ 监督/ 的/ 部分/ ./ 式/ (/ 16/ )/ 右边/ 的/ 第/ 2/ 项为/ 监督/ 项/ ,/ 如果/ 我们/ 给/ 映射/ f2/ 高斯/ 过程/ 先验/ ,/ 则/ 当/ 犾/ i/ ∈/ RC/ 时/ ,/ 似然/ 可以/ 表示/ 成/ lnp/ (/ 犔/ |/ 犣/ )/ =/ -/ N/ 因此/ ,/ 用/ 尺度/ 共轭/ 梯度/ 方法/ [/ 16/ ]/ 最大化/ 观测/ 数据/ 的/ 联合/ 似然/ ,/ 可以/ 得到/ 隐/ 变量/ 在/ 低/ 维空间/ 中/ 的/ 位置/ 和/ 映射/ 的/ 参数/ ./ 首先/ 用/ 对数/ 似然/ 对/ 隐/ 变量/ 犣/ 求/ 梯度/ ,/ / 犣/ =/ / lnp/ (/ 犡/ |/ 犣/ )/ / 犔/ 其中/ ,/ 根据/ 选用/ 的/ 核/ 函数/ 形式/ 不同/ ,/ / 犓/ 同/ 的/ 值/ ./ 例如/ 若/ 选用/ 线性/ 的/ 核/ 函数/ ,/ 则/ / 犓/ 其次/ 用/ 对数/ 似然/ 对/ 参数/ θ/ 求/ 梯度/ 对隐/ 变量/ 犣/ 和/ 参数/ θ/ 交替/ 优化/ 直至/ 收敛/ ./ 上面/ 给出/ 的/ 是/ 处理/ 分类/ 的/ 情况/ ,/ 其/ 推导/ 是/ 在/ 给出/ 类/ 标/ 犔/ =/ [/ 犾/ 1/ ,/ …/ ,/ 犾/ N/ ]/ T/ ∈/ RN/ ×/ C/ 的/ 基础/ 上/ 得出/ 的/ ./ 若/ 给出/ 的/ 监督/ 信息/ 是/ 回归/ 的/ 输出/ 值/ ,/ 则/ 可以/ 直接/ 将/ 回归/ 值/ 替代/ 类标/ 用于/ 给出/ 的/ 模型/ ,/ 该/ 模型/ 可以/ 直接/ 用于/ 回归/ ./ 4/ 实验/ 结果/ 与/ 分析/ 本文/ 算法/ 在/ 很多/ 数据/ 上/ 进行/ 了/ 实验/ ,/ 以/ 验证/ 算法/ 的/ 有效性/ ./ 实验/ 分为/ 两/ 部分/ :/ 第/ 1/ 部分/ 验证/ 了/ 本/ 算法/ 作为/ 分类器/ 的/ 有效性/ ,/ 第/ 2/ 部分/ 验证/ 了/ 本/ 算法/ 作为/ 回归/ 算法/ 时/ 的/ 有效性/ ./ 第/ 1/ 部分/ 又/ 分为/ 两组/ :/ 第/ 1/ 组/ 给出/ 了/ 本/ 算法/ 对/ USPS/ 手写/ 数字/ 数据/ 降维/ 的/ 结果/ ,/ 第/ 2/ 组/ 实验/ 给出/ 了/ 本/ 算法/ 在/ 人脸/ 数据/ 上/ 的/ 识别/ 结果/ ./ 4.1/ 数据/ 降到/ 2D/ 空间/ 的/ 结果/ 对比/ 实验/ 选取/ 了/ USPS/ 数据库/ 中/ 的/ 手写/ 数字/ (/ HandwrittenDigits/ )/ [/ 8/ ]/ 识别/ 的/ “/ 3/ ”/ 和/ “/ 5/ ”/ ,/ 选取/ 准则/ 是/ 这/ 两个/ 数字/ 相对/ 其它/ 数字/ 较难/ 识别/ ./ USPS/ 的/ 手写/ 数字/ 数据/ 是/ 由/ 0/ ~/ 9/ 的/ 10/ 个/ 数字/ 组成/ ,/ 即/ 数据/ 共/ 分为/ 10/ 类/ ,/ 其中/ 每/ 一类/ 数据/ 由/ 大概/ 500/ 个/ 样本/ 组成/ ,/ 每个/ 样本/ 的/ 维数/ 是/ 256/ ./ 实验/ 是/ 在/ 数字/ “/ 3/ ”/ 和/ “/ 5/ ”/ 上/ 进行/ ,/ 对/ 每类/ 样本/ 选取/ 400/ 个/ ,/ 共有/ 800/ 个/ 样本/ ,/ 分别/ 用/ GP/ -/ LVM/ 算法/ 和/ 监督/ 的/ GP/ -/ LVM/ 算法/ 降到/ 2/ 维/ ,/ 图/ 2/ 给出/ 两种/ 方法/ 降维/ 的/ 结果/ ,/ 其中/ “/ 3/ ”/ 的/ 样本/ 降维后/ 用/ 十字/ 表示/ ,/ “/ 5/ ”/ 的/ 样本/ 降维后/ 用/ 圆圈/ 表示/ ./ 图/ 2/ 表示/ 我们/ 将/ USPS/ 数据/ 用/ GP/ -/ LVM/ 和/ SGP/ -/ LVM/ 降至/ 2/ 维/ 的/ 结果/ ,/ 来/ 比较/ 证明/ 我们/ 提出/ 算法/ 的/ 有效性/ ./ 从/ 左边/ 的/ 图/ 2/ (/ a/ )/ 看出/ ,/ 图像/ 的/ 上/ 方圆/ 点较/ 集中/ 的/ 地方/ 有/ 很多/ “/ +/ ”/ 也/ 在/ 其中/ ,/ 这/ 表明/ GP/ -/ LVM/ 的/ 结果/ 不能/ 有效/ 地/ 将/ 两类/ 数据/ 分开/ ,/ 两种/ Page5/ 图/ 2/ 将/ USPS/ 数据/ 降为/ 2/ 维空间/ 结果显示/ 类型/ 存在/ 交叠/ ./ 图/ 2/ (/ b/ )/ 表示/ 的/ 是/ SGP/ -/ LVM/ 降维/ 的/ 结果/ ,/ 从图/ 中/ 可以/ 看出/ ,/ 该/ 算法/ 不仅/ 可以/ 将/ 两类/ 数据/ 有效/ 分开/ ,/ 而且/ 类/ 间距/ 较大/ ,/ 各自/ 集中/ 的/ 区域/ 很少/ 有/ 不同/ 类型/ 的/ 点/ 出现/ ,/ 这/ 表示/ 当用/ SGP/ -/ LVM/ 将/ 该类/ 数据/ 降至/ 2/ 维时/ ,/ 可以/ 有效/ 地/ 将/ 两类/ 数据/ 分开/ ./ 4.2/ SGP/ -/ LVM/ 、/ GP/ -/ LVM/ 和/ LDA/ 分类/ 结果/ 比较/ 在/ 本/ 小节/ 中/ 我们/ 对/ SGP/ -/ LVM/ 、/ GP/ -/ LVM/ 和/ LDA/ 的/ 分类/ 效果/ 进行/ 验证/ ,/ 在/ 下面/ 的/ 实验/ 中/ ,/ 我们/ 选用/ YaleB/ (/ TheextendedYaleFaceDatabaseB/ )/ 和/ ORL/ (/ OlivettiResearchLaboratory/ )/ 人脸/ 数据/ [/ 15/ ]/ ./ SGP/ -/ LVM/ 可以/ 对/ 数据/ 直接/ 分类/ ,/ 而/ 对于/ GP/ -/ LVM/ 和/ LDA/ 这/ 两种/ 方法/ ,/ 我们/ 先/ 用/ 它们/ 降维/ ,/ 然后/ 用/ 最/ 紧邻/ (/ 1/ -/ NN/ )/ 进行/ 分类/ ./ 对于/ LDA/ ,/ 我们/ 对/ 数据/ 先用/ PCA/ 处理/ ,/ 再用/ LDA/ +/ KNN/ 分类/ ,/ 这样/ 做/ 可以/ 避免/ LDA/ 的/ 欠/ 采样/ 问题/ ./ YaleB/ 数据/ 包含/ 38/ 个人/ 的/ 2414/ 幅/ 人脸/ 图像/ ,/ 每个/ 人有/ 64/ 幅/ 接近/ 正面/ 的/ 不同/ 光照/ 下/ 的/ 人脸/ 图像/ ./ 每幅/ 图片/ 被/ 裁减/ 成/ 32/ ×/ 32/ 大小/ ,/ 即/ 每个/ 样本/ 有/ 1024/ 维/ 的/ 特征/ 来/ 刻画/ ./ 在/ 实验/ 中/ ,/ 对/ 每个/ 人/ 随机/ 选择/ 30/ 幅/ 作为/ 训练/ 图像/ ,/ 余下/ 的/ 38/ 幅图/ 用作/ 测试/ ./ ORL/ 数据库/ 是/ 由/ 40/ 个人/ 的/ 人脸/ 图像/ 组成/ ,/ 每个/ 人有/ 10/ 幅/ 图像/ ,/ 分别/ 在/ 不同/ 时刻/ 、/ 光照/ 、/ 表情/ (/ 睁眼/ 或者/ 闭眼/ ,/ 微笑/ 或者/ 不笑/ )/ 和/ 不同/ 面部/ 细节/ (/ 戴眼镜/ 或者/ 不/ 戴眼镜/ )/ 等/ 情况/ 下/ 得到/ 的/ ./ 所有/ 图像/ 都/ 是/ 32/ ×/ 32/ 像素/ 大小/ ./ 在/ 分类/ 实验/ 中/ ,/ 我们/ 随机/ 选取/ 每个/ 人/ 的/ 5/ 幅/ 图像/ 用作/ 训练/ ,/ 剩余/ 5/ 幅/ 用作/ 测试/ ./ 基于/ 上面/ 两组/ 数据/ 的/ 实验/ 结果/ 如表/ 1/ 所示/ ,/ 最/ 左边/ 的/ 列中/ 的/ d/ 值/ 表示/ 样本/ 降维后/ 的/ 维数/ ,/ 余下/ 3/ 列/ ,/ 从/ 左/ 至/ 右/ 分别/ 表示/ LDA/ 、/ GP/ -/ LVM/ 和/ SGP/ -/ LVM/ 在/ 不同/ 维数/ 上/ 的/ 分类/ 结果/ ./ 对/ YaleB/ 数据/ ,/ 我们/ 分别/ 降至/ 5/ 、/ 15/ 、/ 30/ 和/ 37/ 维/ 比较/ 分类/ 结果/ ;/ 对/ ORL/ 数据/ ,/ 则/ 降维至/ 5/ 、/ 10/ 、/ 15/ 和/ 30/ 维来/ 比较/ 3/ 个/ 方法/ 的/ 分类/ 结果/ ./ YaleBd/ =/ 50.5205/ +/ 0.00850/ ./ 6813/ +/ 0.01900/ ./ 5373/ +/ 0.0240/ d/ =/ 150.2332/ +/ 0.01140/ ./ 5788/ +/ 0.03260/ ./ 4831/ +/ 0.0113/ d/ =/ 300.1495/ +/ 0.00640/ ./ 1341/ +/ 0.00650/ ./ 1218/ +/ 0.0525/ d/ =/ 370.1272/ +/ 0.00980/ ./ 1221/ +/ 0.00900/ ./ 1165/ +/ 0.0667/ ORLLDA/ +/ KNNGP/ -/ LVM/ +/ KNNSGP/ -/ LVMd/ =/ 50.3500/ +/ 0.01840/ ./ 3540/ +/ 0.04290/ ./ 3440/ +/ 0.0363/ d/ =/ 100.1950/ +/ 0.01370/ ./ 1740/ +/ 0.02770/ ./ 1770/ +/ 0.0332/ d/ =/ 150.1720/ +/ 0.02310/ ./ 1570/ +/ 0.02710/ ./ 1453/ +/ 0.0266/ d/ =/ 300.1250/ +/ 0.02620/ ./ 1240/ +/ 0.03230/ ./ 1174/ +/ 0.0469/ 从/ 以上/ 两组/ 实验/ 结果/ 可以/ 看出/ ,/ SGP/ -/ LVM/ 和/ GP/ -/ LVM/ 能/ 得到/ 比/ LDA/ 较/ 好/ 的/ 分类/ 结果/ ./ 对于/ 第/ 1/ 组/ 数据/ YaleB/ ,/ 虽然/ 在/ 维数/ 降至/ d/ =/ 5/ 和/ d/ =/ 15/ 时/ LDA/ 得到/ 的/ 分类/ 效果/ 比/ SGP/ -/ LVM/ 和/ GP/ -/ LVM/ 都/ 要/ 好/ ,/ 但/ 随着/ 维数/ 的/ 增加/ ,/ SGP/ -/ LVM/ 的/ 分类/ 效果/ 明显/ 得到/ 提高/ ./ 特别/ 是/ 对/ ORL/ 数据/ ,/ 与/ LDA/ 和/ GP/ -/ LVM/ 相/ 比较/ ,/ SGP/ -/ LVM/ 的/ 分类/ 效果/ 更加/ 明显/ ./ 尤其/ 当/ 数据/ 降维/ 的/ 维数/ 高于/ 15/ 以后/ ,/ SGP/ -/ LVM/ 就/ 能/ 得到/ 非常/ 好/ 的/ 分类/ 结果/ ./ 这/ 主要/ 是因为/ 相比/ 于/ LDA/ 和/ GP/ -/ LVM/ ,/ SGP/ -/ LVM/ 非线性/ 和/ 监督/ 信息/ 的/ 利用/ 取得/ 了/ 很/ 好/ 的/ 效果/ ./ GP/ -/ LVM/ 虽然/ 可以/ 对/ 高维/ 数据/ 降维/ 得到/ 低/ 维流形/ 结构/ ,/ 但是/ 这种/ 降/ 维流形/ 不能/ 区别/ 类别/ 信息/ ./ SGP/ -/ LVM/ 正是/ 对/ 其/ 进行/ 了/ 改进/ ,/ 使得/ GP/ -/ LVM/ 这种/ 有效/ 的/ 降维/ 方法/ 可以/ 用于/ 分类/ ,/ 以上/ 的/ 实验/ 也/ 证明/ 了/ 本文/ 方法/ 的/ 有效性/ ./ 4.3/ 回归/ 数据处理/ 结果/ 前/ 两组/ 实验/ 验证/ 了/ 本文/ 提出/ 的/ 算法/ 在/ 处理/ 分类/ 数据/ 时/ 的/ 有效性/ ,/ 下面/ 的/ 实验/ 则/ 说明/ 该/ 算法/ 在/ 处理/ 回归/ 问题/ 时/ 同样/ 可以/ 取得/ 较/ 好/ 的/ 效果/ ,/ 实验/ 数据/ Housing/ 和/ ConcreteCompressiveStrength/ 来自/ UCI/ 数据库/ ./ Housing/ 是/ 由/ 506/ 个/ 样本/ 组成/ ,/ 每个/ 样本/ 有/ 13/ 个/ 特征/ 来/ 描述/ ,/ 即/ 样本/ 的/ 维数/ 为/ 13/ ./ 第/ 2/ 个/ Page6ConcreteCompressiveStrength/ 数据/ 包含/ 1030/ 个/ 样本/ ,/ 特征/ 维数/ 为/ 8/ ./ 这/ 两个/ 数据/ 都/ 要/ 回归/ 到/ 1/ 维/ 输出/ 上/ ,/ 输出/ 数据/ 与/ 输入/ 的/ 高维/ 特征/ 间/ 有/ 很/ 强/ 的/ 非线性/ ./ 利用/ 本文/ 提出/ 的/ 算法/ 可以/ 找到/ 高维/ 特征/ 潜在/ 的/ 流形/ 结构/ ,/ 然后/ 建立/ 低维/ 的/ 流形/ 到/ 输出/ 值/ 之间/ 的/ 关系/ ./ 在/ 实验/ 中/ 我们/ 也/ 比较/ 了/ 经典/ 非线性/ 回归/ 模型/ (/ 高斯/ 过程/ 回归/ ,/ GPR/ )/ [/ 17/ ]/ ./ 由于/ 本文/ 提出/ 的/ 方法/ 是/ 先/ 试图/ 消除/ 次要/ 或者/ 无关/ 因素/ 对/ 数据/ 的/ 干扰/ ,/ 找到/ 数据/ 潜在/ 的/ 流形/ 结构/ ,/ 所以/ 需要/ 先/ 高维/ 数据/ 降维/ ,/ 然后/ 从/ 低/ 维空间/ 再/ 映射/ 到/ 图/ 3/ 本文/ 算法/ 与/ GPR/ 回归/ 性能/ 比较/ 5/ 结论/ 本文/ 提出/ 了/ 一种/ 基于/ 高斯/ 过程/ 隐/ 变量/ 模型/ 的/ 非线性/ 分类/ 方法/ ,/ 该/ 方法/ 利用/ 先验/ 的/ 类别/ 信息/ ,/ 得到/ 隐/ 变量/ 的/ 后验/ 概率/ ./ 然后/ 通过/ 后验/ 概率/ ,/ 得到/ 高维/ 数据/ 点/ 在/ 隐/ 空间/ 的/ 位置/ ,/ 同时/ 优化/ 出/ 模型/ 中/ 映射/ 的/ 超/ 参数/ 的/ 值/ ,/ 建立/ 分类/ 模型/ ./ 同时/ 该/ 方法/ 仍然/ 保持/ 了/ 高斯/ 过程/ 隐/ 变量/ 模型/ 的/ 降维/ 优势/ ,/ 即/ 可以/ 有效/ 地/ 找到/ 高维/ 数据/ 的/ 低/ 维流形/ 结构/ ./ 实验/ 结果/ 证明/ 了/ 本文/ 方法/ 的/ 有效性/ ./ 由于/ 我们/ 利用/ 的/ 类别/ 信息/ 得到/ 隐/ 变量/ 的/ 后验/ ,/ 所以/ 这种/ 方法/ 具有/ 开放性/ ,/ 我们/ 可以/ 利用/ 其它/ 的/ 监督/ 信息/ 替代/ 这里/ 的/ 类别/ 信息/ ,/ 例如/ 回归/ 模型/ 的/ 输出/ 值/ ;/ 而/ 若/ 我们/ 已知/ 一些半/ 监督/ 的/ 信息/ ,/ 那么/ 该/ 方法/ 可以/ 很/ 容易/ 扩展/ 到/ 半/ 监督/ 模型/ ./ 

