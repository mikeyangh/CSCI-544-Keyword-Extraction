Page1/ 一种/ 基于/ 线性/ 函数/ 逼近/ 的/ 离/ 策略/ 犙/ (/ λ/ )/ 算法/ 傅/ 启明/ 1/ )/ 刘全/ 1/ )/ ,/ 2/ )/ 王辉/ 1/ )/ 肖飞/ 1/ )/ 于俊/ 1/ )/ 李娇/ 1/ )/ 1/ )/ (/ 苏州大学/ 计算机科学/ 与/ 技术/ 学院/ 江苏/ 苏州/ 215006/ )/ 2/ )/ (/ 吉林大学/ 符号计算/ 与/ 知识/ 工程/ 教育部/ 重点/ 实验室/ 长春/ 130012/ )/ 摘要/ 将/ 函数/ 逼近/ 用于/ 强化/ 学习/ 是/ 目前/ 机器/ 学习/ 领域/ 的/ 一个/ 新/ 的/ 研究/ 热点/ ./ 针对/ 传统/ 的/ 基于/ 查询/ 表及/ 函数/ 逼近/ 的/ Q/ (/ λ/ )/ 学习/ 算法/ 在/ 大规模/ 状态/ 空间/ 中/ 收敛/ 速度慢/ 或者/ 无法/ 收敛/ 的/ 问题/ ,/ 提出/ 一种/ 基于/ 线性/ 函数/ 逼近/ 的/ 离/ 策略/ Q/ (/ λ/ )/ 算法/ ./ 该/ 算法/ 通过/ 引入/ 重要性/ 关联/ 因子/ ,/ 在/ 迭代/ 次数/ 逐步/ 增长/ 的/ 过程/ 中/ ,/ 使得/ 在/ 策略/ 与/ 离/ 策略/ 相/ 统一/ ,/ 确保/ 算法/ 的/ 收敛性/ ./ 同时/ 在/ 保证/ 在/ 策略/ 与/ 离/ 策略/ 的/ 样本/ 数据/ 一致性/ 的/ 前提/ 下/ ,/ 对/ 算法/ 的/ 收敛性/ 给予/ 理论/ 证明/ ./ 将/ 文中/ 提出/ 的/ 算法/ 用于/ Baird/ 反例/ 、/ Mountain/ -/ Car/ 及/ RandomWalk/ 仿真/ 平台/ ,/ 实验/ 结果表明/ ,/ 该/ 算法/ 与/ 传统/ 的/ 基于/ 函数/ 逼近/ 的/ 离/ 策略/ 算法/ 相比/ ,/ 具有/ 较/ 好/ 的/ 收敛性/ ;/ 与/ 传统/ 的/ 基于/ 查询/ 表/ 的/ 算法/ 相比/ ,/ 具有/ 更快/ 的/ 收敛/ 速度/ ,/ 且/ 对于/ 状态/ 空间/ 的/ 增长/ 具有/ 较强/ 的/ 鲁棒性/ ./ 关键词/ 强化/ 学习/ ;/ 函数/ 逼近/ ;/ 离/ 策略/ ;/ Q/ (/ λ/ )/ 算法/ ;/ 机器/ 学习/ 1/ 引言/ 强化/ 学习/ (/ ReinforcementLearning/ ,/ RL/ )/ 是/ 一种/ 从/ 环境/ 状态/ 到/ 动作/ 映射/ 的/ 学习/ ,/ 并/ 期望/ 动作/ 从/ 环境/ 中/ 获得/ 的/ 累积/ 奖赏/ 最大/ [/ 1/ -/ 2/ ]/ ./ 从/ 20/ 世纪/ 80/ 年代/ 末/ 开始/ ,/ 随着/ 对/ 强化/ 学习/ 的/ 数学/ 基础/ 研究/ 取得/ 突破性/ 进展/ 后/ ,/ 对/ 强化/ 学习/ 的/ 研究/ 和/ 应用/ 日益/ 开展/ 起来/ ,/ 强化/ 学习/ 成为/ 目前/ 机器/ 学习/ 领域/ 的/ 研究/ 热点/ 之一/ ./ 近年来/ 在/ 工程/ 应用/ 、/ 模式识别/ 、/ 图像处理/ 、/ 网络/ 优化/ 等/ 领域/ 都/ 得到/ 广泛应用/ ./ 在/ 强化/ 学习/ 中/ ,/ 常用/ 查询/ 表/ (/ Lookup/ -/ Table/ )/ 来/ 描述/ 状态/ 到/ 动作/ 的/ 映射/ ,/ 它/ 是/ 一个二维/ 的/ 表格/ ,/ 状态/ 和/ 动作/ 是/ 表/ 的/ 两个/ 维度/ ,/ 在/ 学习/ 之后/ ,/ 给定/ 相应/ 状态/ 动作/ 对/ 一定/ 的/ 回报/ 值/ ,/ Agent/ 可以/ 根据/ 对应状态/ 动作/ 对/ 的/ 回报/ 值来/ 选取/ 动作/ ./ 对于/ 低维/ 状态/ 空间/ ,/ 它/ 是/ 一个/ 非常/ 简洁/ 有效/ 的/ 方法/ ./ 然而/ ,/ 对于/ 高维/ 大/ 状态/ 空间/ ,/ 容易/ 出现/ “/ 维数灾/ ”/ 的/ 问题/ ,/ 同时/ 方法/ 本身/ 也/ 存在/ 以下/ 3/ 个/ 问题/ :/ (/ 1/ )/ 存储空间/ ./ Lookup/ -/ Table/ 是/ 以表/ 的/ 形式/ 存储/ 从/ 状态/ 到/ 动作/ 的/ 映射/ ,/ 当/ 状态/ 空间/ 较大/ 时/ ,/ 如何/ 存储/ 这个/ 表是/ 需要/ 解决/ 的/ 问题/ ;/ (/ 2/ )/ 表值/ 的/ 查询/ 和/ 修改/ ./ 当/ 状态/ 空间/ 较大/ 时/ ,/ 如何/ 能够/ 及时/ 准确/ 地/ 查询/ 和/ 修改/ 表值/ 也/ 是/ 需要/ 解决/ 的/ 问题/ ;/ (/ 3/ )/ 在/ 学习/ 之后/ ,/ 对于/ 学习/ 过程/ 中/ 没有/ 学习/ 到/ 的/ 状态/ ,/ Agent/ 无法/ 根据/ 查询/ 表/ 选取/ 最优/ 动作/ ,/ 这一/ 问题/ 在/ 连续/ 状态/ 空间/ 任务/ 中/ 更为/ 明显/ [/ 3/ ]/ ./ 针对/ 以上/ 3/ 个/ 问题/ ,/ 在/ 强化/ 学习/ 中/ 引入/ 函数/ 逼近/ 方法/ ./ 通过/ 函数/ 逼近/ ,/ 使/ 学习/ 的/ 经验/ 信息/ 能够/ 从/ 状态/ 空间/ 子集/ 泛化/ 至/ 整个/ 状态/ 空间/ ,/ 采取/ 带有/ 一组/ 参数/ 的/ 近似/ 函数/ 来/ 描述/ 强化/ 学习/ 中/ 的/ 状态值/ 函数/ (/ 或/ 动作/ 值/ 函数/ )/ ,/ Agent/ 根据/ 近似/ 函数/ 选择/ 最优/ 动作/ [/ 4/ -/ 5/ ]/ ./ 根据/ 目标/ 策略/ 和/ 行为/ 策略/ 是否/ 一致/ ,/ 将/ 强化/ 学习/ 分为/ 在/ 策略/ (/ On/ -/ Policy/ )/ 学习/ 和/ 离/ 策略/ (/ Off/ -/ Policy/ )/ 学习/ ./ 如果/ 在/ 学习/ 过程/ 中/ ,/ 动作/ 选择/ 的/ 行为/ 策略/ 和/ 学习/ 改进/ 的/ 目标/ 策略/ 一致/ ,/ 该/ 方法/ 就/ 被/ 称为/ 在/ 策略/ 学习/ ,/ 如/ Sarsa/ 学习/ ,/ 否则/ 被/ 称为/ 离/ 策略/ 学习/ ,/ 如/ Q/ 学习/ ./ 将/ 函数/ 逼近/ 用于/ 在/ 策略/ 强化/ 学习/ ,/ Tsitsiklis/ 等/ 人/ [/ 5/ ]/ 在/ 20/ 世纪/ 90/ 年代/ 从/ 理论/ 和/ 实验/ 两个/ 方面/ 证明/ 了/ 方法/ 的/ 稳定性/ 和/ 收敛性/ ./ 然而/ 对于/ 离/ 策略/ 强化/ 学习/ 方法/ ,/ 由于/ 目标/ 策略/ 和/ 行为/ 策略/ 的/ 不/ 一致/ ,/ 使得/ 行为/ 策略/ 所/ 生成/ 的/ 样本/ 数据/ 与/ 目标/ 策略/ 所/ 需要/ 的/ 样本/ 数据/ 的/ 分布/ 是/ 不/ 一致/ 的/ ,/ 导致/ 方法/ 无法/ 收敛/ ./ 比如/ ,/ 将/ 基于/ 函数/ 逼近/ 的/ Q/ 学习/ 用于/ 著名/ Baird/ 反例/ ,/ 算法/ 无法/ 收敛/ [/ 6/ ]/ ./ Gordon/ [/ 7/ ]/ 对/ 经典/ Q/ 学习/ 算法/ 做出/ 尝试/ 改进/ ,/ 提出/ 的/ 算法/ 能够/ 稳定/ 收敛/ ,/ 但是/ 性能/ 不够/ 理想/ ./ Tadic/ [/ 8/ ]/ 利用/ 线性/ 函数/ 将/ 函数/ 逼近/ 和/ TD/ 学习/ 相结合/ 用于/ 有限/ 维度/ 状态/ 空间/ 问题/ ,/ 并/ 证明/ 了/ 算法/ 有效性/ ,/ 但/ 试验/ 结果表明/ ,/ 与/ 传统/ 的/ TD/ 学习/ 相比/ ,/ 该/ 算法/ 的/ 执行/ 效率/ 不高/ ./ Lagoudakis/ 等/ 人/ [/ 9/ ]/ 利用/ 最小/ 二/ 乘法/ 求解/ 策略/ 迭代/ 问题/ ,/ 但/ 该/ 算法/ 只能/ 用于/ 环境/ 模型/ 已知/ 的/ 情况/ ./ Precup/ 等/ 人/ [/ 10/ ]/ 提出/ 将/ 函数/ 逼近/ 用于/ 离/ 策略/ 的/ TD/ (/ λ/ )/ 算法/ ,/ 并/ 证明/ 了/ 算法/ 收敛性/ ,/ 但/ 该/ 算法/ 对于/ 初始/ 行为/ 策略/ 有/ 一定/ 的/ 要求/ ,/ 算法/ 在/ 执行/ 过程/ 中/ 可能/ 因为/ 初始/ 行为/ 策略/ 不/ 满足要求/ ,/ 而/ 出现/ 无法/ 收敛/ 的/ 情况/ ./ Geramifard/ 等/ 人/ [/ 11/ ]/ 利用/ 最小/ 二/ 乘法/ 实现/ TD/ 问题/ 增量/ 式/ 求解/ ,/ 该/ 方法/ 主要/ 是/ 对/ 在/ 策略/ TD/ 方法/ 的/ 改进/ ,/ 将/ 离线/ 更新/ 方法/ 改为/ 在线/ 增量/ 更新/ 方法/ ./ Sutton/ 等/ 人/ [/ 12/ -/ 13/ ]/ 利用/ 线性/ 函数/ 将/ TD/ 学习/ 与/ 函数/ 逼近/ 相结合/ ,/ 提出/ GTD/ 、/ GTD2/ 及/ TDC/ 算法/ ,/ 并/ 在/ 实验/ 和/ 理论/ 上均/ 证明/ 这/ 几种/ 算法/ 的/ 收敛性/ ,/ 但/ 与/ 传统/ 的/ TD/ 方法/ 相比/ ,/ 算法/ 在/ 执行/ 效率/ 上/ 没有/ 较大/ 的/ 提高/ ./ Maei/ 等/ 人/ [/ 14/ ]/ 提出/ 一种/ 增量/ 式/ 的/ 离/ 策略/ greedy/ -/ GQ/ 算法/ ,/ 但/ 算法/ 要求/ 用于/ 生成/ 样本/ 数据/ 的/ 行为/ 策略/ 必须/ 是/ 稳定性/ 策略/ ./ 本文/ 针对/ 基于/ 查询/ 表/ 的/ Q/ 学习/ 算法/ 在/ 大/ 状态/ 空间/ 中/ 所/ 存在/ 的/ 3/ 个/ 问题/ ,/ 提出/ 一种/ 基于/ 线性/ 函数/ 逼近/ 的/ 离/ 策略/ Q/ (/ λ/ )/ 算法/ (/ Off/ -/ PolicyQ/ (/ λ/ )/ algorithmbasedonGradient/ -/ Descent/ ,/ GDOP/ -/ Q/ (/ λ/ )/ )/ ./ 针对/ 经典/ 的/ 基于/ 函数/ 逼近/ 的/ Q/ (/ λ/ )/ 算法/ ,/ 由于/ 目标/ 策略/ 需要/ 的/ 样本/ 数据/ 与/ 行为/ 策略/ 生成/ 的/ 样本/ 数据分布/ 不/ 一致/ ,/ 从而/ 导致/ 无法/ 收敛/ 的/ 问题/ ,/ GDOP/ -/ Q/ (/ λ/ )/ 算法/ 引入/ 重要性/ 关联/ 因子/ ./ 在/ 此基础/ 之上/ 定义/ 一种/ 新/ 的/ 离/ 策略/ 下/ 的/ 资格/ 迹/ (/ EligibilityTraces/ )/ ,/ 使得/ 在/ 策略/ 与/ 离/ 策略/ 相/ 统一/ ,/ 并/ 在/ 迭代/ 次数/ 逐步/ 增长/ 的/ 过程/ 中/ ,/ 利用/ 梯度/ 下降/ 方法/ 求解/ 一组/ 最优/ 参数/ ,/ 确保/ 算法/ 的/ 收敛性/ ./ 同时/ 在/ 保证/ 在/ 策略/ 与/ 离/ 策略/ 的/ 样本/ 数据/ 一致性/ 的/ 前提/ 下/ ,/ 对/ 算法/ 的/ 收敛性/ 给出/ 了/ 理论/ 证明/ ./ 将/ GDOP/ -/ Q/ (/ λ/ )/ 算法/ 用于/ Baird/ 反例/ 、/ Mountain/ -/ Car/ 及/ RandomWalk/ 仿真/ 平台/ ,/ 实验/ 结果表明/ ,/ 该/ 算法/ 与/ 传统/ 的/ 基于/ 函数/ 逼近/ 的/ 离/ 策略/ 算法/ 相比/ ,/ 能够/ 较/ 好/ 的/ 收敛/ ;/ 与/ 传统/ 的/ 基于/ 查询/ 表/ 的/ TD/ (/ λ/ )/ 系列/ 算法/ 相比/ ,/ 具有/ 较/ 好/ 、/ 较/ 快/ 的/ 收敛/ 速度/ ,/ 且/ 对于/ 状态/ 空间/ 的/ 增长/ 具有/ 较/ 好/ 的/ 鲁棒性/ ./ 2/ 离/ 策略/ 强化/ 学习/ 原则上/ 任何/ 函数/ 逼近/ 方法/ 都/ 可以/ 用于/ 监督/ 学习/ ,/ 比如/ 人工神经网络/ 、/ 决策树/ 以及/ 各种/ 多元/ 线性/ 回/ Page3/ 归/ 方法/ 等/ ./ 然而/ ,/ 对于/ 强化/ 学习/ 来讲/ ,/ 并/ 不是/ 所有/ 的/ 方法/ 都/ 适用/ ./ 大部分/ 的/ 逼近/ 方法/ ,/ 比如/ 人工神经网络/ 、/ 遗传算法/ 等/ ,/ 都/ 假设/ 有/ 一个/ 稳定/ 静态/ 的/ 训练/ 集/ ,/ 所有/ 的/ 逼近/ 操作/ 都/ 是/ 基于/ 稳定/ 静态/ 的/ 训练/ 集/ ./ 但是/ 在/ 强化/ 学习/ 中/ ,/ 学习/ 的/ 过程/ 是/ 在线/ 的/ 、/ 动态/ 的/ ,/ 逼近/ 方法/ 所/ 需要/ 的/ 训练/ 集是/ Agent/ 与/ 环境/ 交互/ 动态/ 实时/ 得到/ 的/ ,/ 因此/ 就/ 需要/ 逼近/ 方法/ 能够/ 有效/ 地/ 从/ 增量/ 式/ 的/ 训练/ 数据/ 中/ 进行/ 学习/ ./ 在/ 强化/ 学习/ 中/ ,/ 还/ 需要/ 逼近/ 方法/ 能够/ 解决/ 目标/ 函数/ 不/ 确定/ 的/ 问题/ ,/ 例如/ ,/ 在/ GPI/ (/ GeneralPolicyIteration/ )/ 控制/ 问题/ 中/ ,/ 当/ 目标/ 策略/ π/ 改变/ 时/ ,/ 方法/ 要/ 能够/ 逼近/ 新/ 的/ 动作/ 值/ 函数/ Q/ π/ ./ 或者/ 即使/ 目标/ 函数/ 是/ 确定/ 的/ ,/ 如果/ Agent/ 与/ 环境/ 交互/ 得到/ 的/ 样本/ 数据/ 是/ 基于/ 自举式/ (/ Bootstrapping/ )/ 方法/ 得到/ 的/ ,/ 那么/ 样本/ 数据/ 对应/ 的/ 状态/ (/ 动作/ )/ 值/ 也/ 是/ 不/ 稳定/ 的/ ,/ 这/ 就/ 要求/ 逼近/ 方法/ 也/ 要/ 能够/ 对/ 这类/ 问题/ 具有/ 较/ 好/ 的/ 鲁棒性/ ./ 2.1/ 梯度/ 下降/ 法/ 与/ 线性/ 函数/ 逼近/ 梯度/ 下降/ 法是/ 利用/ 负/ 梯度方向/ 来/ 决定/ 每次/ 迭代/ 的/ 搜索/ 方向/ ,/ 使得/ 每次/ 迭代/ 能/ 使/ 待/ 优化/ 的/ 目标/ 函数/ 呈/ 非/ 递增/ 变化/ ,/ 它/ 是/ 2/ 范数/ 下/ 的/ 梯度/ 下降/ 法/ ./ 它/ 的/ 简单/ 形式/ 如式/ (/ 1/ )/ 所示/ ./ 其中/ f/ (/ k/ )/ 是/ 目标/ 函数/ ,/ / kf/ (/ k/ )/ 是/ f/ (/ k/ )/ 的/ 梯度/ ./ 在/ 基于/ 梯度/ 下降/ 的/ 逼近/ 方法/ 中/ ,/ 线性/ 函数/ 逼近/ 是/ 最/ 常用/ 、/ 最/ 重要/ 的/ 逼近/ 形式/ ./ 由于/ 其/ 函数/ 构造/ 形式/ 简单/ ,/ 计算/ 量/ 较/ 小/ ,/ 近年来/ ,/ 在/ 基于/ 函数/ 逼近/ 的/ 强化/ 学习/ 方法/ 中/ 得到/ 广泛/ 的/ 应用/ ./ 本文/ 也/ 采用/ 线性/ 函数/ 来/ 构造/ 动作/ 值/ 函数/ (/ Q/ 值/ 函数/ )/ ,/ 如式/ (/ 2/ )/ 所示/ ./ 考虑/ 在/ 标准/ 的/ 强化/ 学习/ 框架/ 下/ ,/ Agent/ 与/ 环境/ 交互/ 构成/ 一个/ 有限/ 马尔科夫/ 决策/ 过程/ (/ MarkovDecisionProcess/ ,/ MDP/ )/ ./ 在/ 策略/ π/ 下/ ,/ 每个/ 时间/ 步/ t/ ,/ 环境/ 状态/ 为/ st/ (/ st/ ∈/ S/ ,/ S/ 为/ 状态/ 空间/ )/ ,/ Agent/ 选择/ 的/ 动作/ 为/ at/ (/ at/ ∈/ A/ ,/ A/ 为/ 动作/ 选择/ 空间/ )/ ,/ 状态/ 动作/ 对/ (/ st/ ,/ at/ )/ 由/ 特征向量/ / (/ st/ ,/ at/ )/ 表示/ (/ / (/ st/ ,/ at/ )/ =/ {/ / (/ 1/ )/ ,/ / (/ 2/ )/ ,/ …/ ,/ / (/ n/ -/ 1/ )/ ,/ / (/ n/ )/ }/ ,/ / (/ st/ ,/ at/ )/ ∈/ / 写为/ / t/ ;/ 环境/ 给出/ 一个/ 奖赏/ 值/ rt/ (/ rt/ ∈/ / )/ ,/ 动作/ 值/ 函数/ 记为/ Q/ (/ st/ ,/ at/ )/ ,/ 简写/ 为/ Qt/ ;/ Qt/ 是/ 关于/ 参数/ 向量/ θ/ t/ (/ θ/ t/ =/ (/ θ/ t/ (/ 1/ )/ ,/ θ/ t/ (/ 2/ )/ ,/ …/ ,/ θ/ t/ (/ n/ )/ )/ T/ 且/ θ/ t/ ∈/ / 向量/ / t/ 的/ 函数/ ,/ 记为/ Qt/ (/ θ/ t/ )/ ,/ Q/ π/ at/ )/ 的/ 期望值/ ./ 随着/ Agent/ 与/ 环境/ 的/ 不断/ 交互/ ,/ 可以/ 得到/ 一组/ 状态/ 、/ 动作/ 、/ 回报/ 值/ 的/ 序列/ ,/ s1/ ,/ a1/ ,/ r2/ ,/ s2/ ,/ a2/ ,/ r3/ ,/ …/ ,/ 将/ 该/ 序列/ 表示/ 为/ 一组/ 四元组/ (/ s1/ ,/ a1/ ,/ r2/ ,/ s2/ )/ ,/ (/ s2/ ,/ a2/ ,/ r3/ ,/ s3/ )/ ,/ …/ ,/ 由于/ 状态/ 动作/ 对/ (/ st/ ,/ at/ )/ 用/ / t/ 表示/ ,/ 因此/ 将/ 序列/ 简化/ 为/ (/ / 1/ ,/ r2/ ,/ / 2/ )/ ,/ (/ / 2/ ,/ r3/ ,/ / 3/ )/ ,/ …/ ./ Qt/ (/ θ/ t/ )/ =/ θ/ Tt/ / t/ ≈/ V/ π/ t/ =/ E/ ∑/ 其中/ γ/ 是/ 折扣/ 因子/ ./ Agent/ 对于/ 环境/ 未知/ ,/ 仅/ 从/ 观察/ 的/ 一组/ 状态/ 转移/ 序列/ 中/ 估计/ 动作/ 值/ ./ 在/ 策略/ TD/ (/ 0/ )/ 中/ ,/ 一步/ TDError/ 及/ 线性/ TDSolution/ 如式/ (/ 3/ )/ 、/ (/ 4/ )/ 所示/ ./ 定义/ 1/ ./ 在/ 策略/ TD/ (/ λ/ )/ 中/ ,/ λ/ -/ TDError/ 及/ 线性/ λ/ -/ TDSolution/ 如式/ (/ 7/ )/ 、/ (/ 8/ )/ 所示/ ./ t/ (/ θ/ )/ =/ R/ (/ n/ )/ rt/ +/ 1/ +/ γ/ rt/ +/ 2/ +/ γ/ 2rt/ +/ 3/ +/ …/ +/ γ/ n/ -/ 1rt/ +/ n/ +/ γ/ n/ θ/ T/ / t/ +/ n/ (/ 5/ )/ δ/ λ/ =/ R/ λ/ t/ (/ θ/ )/ -/ Qt/ (/ θ/ )/ =/ R/ λ/ t/ (/ θ/ )/ -/ θ/ T/ / t/ =/ -/ θ/ T/ / t/ +/ (/ 1/ -/ λ/ )/ λ/ 0/ [/ rt/ +/ 1/ +/ γ/ θ/ T/ / t/ +/ 1/ ]/ +/ (/ 1/ -/ λ/ )/ λ/ 1/ [/ rt/ +/ 1/ +/ γ/ rt/ +/ 2/ +/ γ/ 2/ θ/ T/ / t/ +/ 2/ ]/ +/ (/ 1/ -/ λ/ )/ λ/ 2/ [/ rt/ +/ 1/ +/ γ/ rt/ +/ 2/ +/ γ/ 2rt/ +/ 3/ +/ γ/ 3/ θ/ T/ / t/ +/ 3/ ]/ / =/ -/ θ/ T/ / t/ +/ (/ γ/ λ/ )/ 0/ [/ rt/ +/ 1/ +/ γ/ θ/ T/ / t/ +/ 1/ -/ γ/ λ/ θ/ T/ / t/ +/ 1/ ]/ +/ (/ γ/ λ/ )/ 1/ [/ rt/ +/ 2/ +/ γ/ θ/ T/ / t/ +/ 2/ -/ γ/ λ/ θ/ T/ / t/ +/ 2/ ]/ +/ (/ γ/ λ/ )/ 2/ [/ rt/ +/ 3/ +/ γ/ θ/ T/ / t/ +/ 3/ -/ γ/ λ/ θ/ T/ / t/ +/ 3/ ]/ / =/ (/ γ/ λ/ )/ 0/ [/ rt/ +/ 1/ +/ γ/ θ/ T/ / t/ +/ 1/ -/ θ/ T/ / t/ ]/ +/ (/ γ/ λ/ )/ 1/ [/ rt/ +/ 2/ +/ γ/ θ/ T/ / t/ +/ 2/ -/ θ/ T/ / t/ +/ 1/ ]/ +/ (/ γ/ λ/ )/ 2/ [/ rt/ +/ 3/ +/ γ/ θ/ T/ / t/ +/ 3/ -/ θ/ T/ / t/ +/ 2/ ]/ / ≈/ ∑/ T/ -/ 1/ =/ ∑/ T/ -/ 1/ =/ ∑/ T/ -/ 1/ 其中/ R/ (/ n/ )/ R/ λ/ t/ (/ θ/ )/ 是/ λ/ -/ 回报/ 值/ ,/ 犃/ =/ 犈/ / t/ ∑/ T/ -/ 1b/ =/ 犈/ / t/ ∑/ T/ -/ 1/ 是/ 当前/ θ/ 的/ 差/ 分值/ ,/ 目标/ 需/ 使得/ 犈/ [/ δ/ λ/ / ]/ 等于/ 0/ ./ 因此/ ,/ 取其/ 2/ 范数/ 描述/ 当前/ θ/ 值/ 与/ 目标/ θ/ 值/ 的/ 距离/ ,/ 即/ 如式/ (/ 9/ )/ 所示/ ./ i/ =/ tt/ (/ θ/ )/ 是/ n/ 步/ 截断/ 回报/ 值/ ,/ 简称/ n/ 步/ 回报/ 值/ ,/ {/ i/ =/ ti/ =/ tPage4/ 根据/ 梯度/ 下降/ 法/ ,/ 在/ 每/ 一次/ 迭代/ 过程/ 中/ ,/ 对/ θ/ 的/ 更新/ 如式/ (/ 10/ )/ 所示/ ./ 其中/ α/ t/ 是/ 步长/ 参数/ ./ 将式/ (/ 9/ )/ 代入/ 式/ (/ 10/ )/ ,/ 在/ 每个/ 时间/ 步/ t/ 对/ θ/ t/ 的/ 更新/ 如式/ (/ 11/ )/ 所示/ ./ θ/ t/ +/ 1/ =/ θ/ t/ +/ α/ t/ / θ/ (/ 犈/ [/ δ/ / ]/ T/ 犈/ [/ δ/ λ/ / ]/ )/ =/ θ/ t/ +/ 2/ α/ t/ / θ/ (/ 犈/ [/ δ/ λ/ / ]/ )/ 犈/ [/ δ/ λ/ / ]/ =/ θ/ t/ +/ 2/ α/ t/ (/ 犈/ [/ / (/ / θ/ δ/ λ/ )/ T/ ]/ T/ )/ 犈/ [/ δ/ λ/ / ]/ =/ θ/ t/ +/ 2/ α/ t/ 犈/ / t/ ∑/ T/ -/ 1/ 考虑/ 式/ (/ 11/ )/ 中/ 存在/ 两个/ 期望/ 的/ 乘积/ ,/ 无法/ 通过/ 迭代/ 的/ 方法/ 逼近/ 最优/ θ/ ,/ 因此/ 考虑/ 采用/ 样本/ 值/ 计算/ 其中/ 一个/ ,/ 迭代/ 计算/ 另/ 一个/ ./ 考虑/ 用/ 样本/ 值/ 计算/ {/ 犈/ / t/ ∑/ T/ -/ 1ut/ 为/ 第/ t/ 个/ 时间/ 步后/ 对/ 犈/ [/ δ/ / ]/ 的/ 估计/ ./ i/ =/ t/ θ/ t/ +/ 1/ =/ θ/ t/ +/ α/ t/ ∑/ T/ -/ 1/ 其中/ u1/ =/ 0/ ,/ β/ t/ >/ 0/ ,/ α/ t/ >/ 0/ ,/ β/ t/ ,/ α/ t/ 是/ 步长/ 参数/ ./ 2.2/ 离/ 策略/ 强化/ 学习/ 算法/ 离/ 策略/ 学习/ 是/ 一种/ 将/ 目标/ 策略/ 与/ 行为/ 策略/ 分离/ 的/ 学习/ 方式/ ./ 在/ 将/ 函数/ 逼近/ 与/ 离/ 策略/ 学习/ 结合/ 时/ ,/ 面临/ 这样/ 一个/ 问题/ —/ —/ —/ 逼近/ 求解/ 的/ 所/ 需要/ 的/ 样本/ 数据/ 概率分布/ 与/ 行为/ 策略/ 生成/ 的/ 样本/ 数据/ 概率分布/ 不/ 一致/ ./ 因此/ ,/ 当/ 求解/ 的/ θ/ 收敛/ 时/ ,/ 目标/ 策略/ 却/ 无法/ 收敛/ ./ 本文/ 主要/ 以/ Q/ (/ λ/ )/ 算法/ 为/ 基础/ ,/ 分析/ 离/ 策略/ 强化/ 学习/ 算法/ 运行/ 流程/ 及/ 发散/ 原因/ ,/ 认为/ 解决/ 这一/ 问题/ 主要/ 有/ 以下/ 两种/ 途径/ :/ (/ 1/ )/ 在/ 学习/ 过程/ 中/ ,/ 利用/ 与/ 目标/ 改进/ 策略/ 接近/ 的/ 行为/ 策略/ 来/ 生成/ 样本/ 数据/ ;/ (/ 2/ )/ 将/ 目标/ 改进/ 策略/ 与/ 行为/ 策略/ 以/ 某种/ 方式/ 进行/ 关联/ ,/ 使得/ 生成/ 的/ 样本/ 数据/ 与/ 改进/ 目标/ 策略/ 逼近/ 所/ 需要/ 的/ 数据/ 在/ 概率分布/ 上/ 保持一致/ ./ 假设/ π/ 是/ 目标/ 改进/ 策略/ ,/ b/ 是/ 行为/ 策略/ ,/ 本文/ 引入/ 重要性/ 关联/ 因子/ ρ/ ./ 定义/ 2/ ./ 假设/ 环境/ 状态/ 为/ st/ ,/ 动作/ 为/ at/ ,/ 在/ 目标/ 策略/ π/ 和/ 行为/ 策略/ b/ 下/ ,/ 在/ 状态/ st/ 下/ 选择/ at/ 的/ 概率/ 分别/ 为/ π/ (/ st/ ,/ at/ )/ 和/ b/ (/ st/ ,/ at/ )/ ,/ 且/ 满足/ π/ (/ st/ ,/ at/ )/ >/ 0/ ,/ b/ (/ st/ ,/ at/ )/ >/ 0/ ,/ 则/ 重要性/ 关联/ 因子/ ρ/ t/ =/ π/ (/ st/ ,/ at/ )/ // b/ (/ st/ ,/ at/ )/ ,/ 简称/ 关联/ 因子/ ρ/ t/ ./ 在/ Q/ (/ λ/ )/ 算法/ 中/ ,/ 利用/ 关联/ 因子/ ,/ 重写/ 式/ (/ 5/ )/ ~/ (/ 8/ )/ 、/ (/ 12/ )/ 、/ (/ 13/ )/ ,/ 如式/ (/ 14/ )/ ~/ (/ 19/ )/ 所示/ ./ 珚/ R/ (/ n/ )/ t/ (/ θ/ )/ =/ rt/ +/ 1/ +/ γ/ rt/ +/ 2/ ρ/ t/ +/ 1/ +/ …/ +/ γ/ n/ -/ 1rt/ +/ n/ ρ/ t/ +/ 1/ ρ/ t/ +/ 2/ …/ 珔/ δ/ λ/ =/ ∑/ T/ -/ 1/ {/ 其中/ 犃/ =/ 犈/ / t/ ∑/ T/ -/ 1b/ =/ 犈/ / t/ ∑/ T/ -/ 1/ θ/ t/ +/ 1/ =/ θ/ t/ +/ α/ t/ ∑/ T/ -/ 1Precup/ 等/ 人/ 在/ 2000/ 年/ 首次/ 提出/ 离/ 策略/ n/ 步/ 回报/ ,/ 并/ 证明/ Eb/ {/ 珚/ R/ λ/ t/ |/ s/ =/ st/ ,/ a/ =/ at/ }/ =/ E/ π/ {/ R/ λ/ t/ |/ s/ =/ st/ ,/ a/ =/ at/ }/ ,/ / st/ ∈/ S/ ,/ at/ ∈/ A/ [/ 15/ ]/ ,/ 本文/ 将/ 这/ 观点/ 延伸/ 至/ 基于/ 线性/ 函数/ 逼近/ 的/ Q/ (/ λ/ )/ 中/ ./ 由式/ (/ 13/ )/ 和/ 式/ (/ 19/ )/ 可知/ Δ/ 珔/ θ/ t/ =/ α/ t/ 珚/ R/ λ/ t/ -/ θ/ T/ / (/ )/ t/ / t/ ρ/ 1/ ρ/ 2/ …/ ρ/ t/ =/ α/ 珚/ R/ λ/ t/ -/ θ/ T/ / (/ )/ t/ / t/ ∏/ t/ 定理/ 1/ ./ 在/ 策略/ 与/ 离/ 策略/ 算法/ 关于/ 样本/ 数据分布/ 的/ 一致性/ 证明/ ./ 令/ Δ/ θ/ t/ 与/ Δ/ 珔/ θ/ t/ 分别/ 是/ 在/ 策略/ TD/ (/ λ/ )/ 与/ 离/ 策略/ Q/ (/ λ/ )/ 算法/ 在/ 一个/ 情节/ 结束/ 之后/ 关于/ θ/ 的/ 增量/ 和/ ,/ 假设/ 两个/ 算法/ 分别/ 从/ (/ s0/ ,/ a0/ )/ 开始/ ,/ 策略/ b/ 和/ π/ 分别/ 是/ Q/ (/ λ/ )/ 算法/ 的/ 行为/ 策略/ 和/ 目标/ 策略/ ,/ ρ/ t/ 是/ 关联/ 因子/ ,/ 则/ Eb/ {/ Δ/ 珔/ θ/ t/ |/ s/ =/ st/ ,/ a/ =/ at/ }/ =/ E/ π/ {/ Δ/ θ/ t/ |/ s/ =/ st/ ,/ a/ =/ at/ }/ ./ 证明/ ./ 根据/ 式/ (/ 20/ )/ 、/ (/ 21/ )/ ,/ 重写/ Δ/ θ/ 与/ Δ/ 珔/ θ/ ,/ 并令/ E/ π/ {/ Δ/ θ/ t/ }/ =/ E/ π/ {/ Δ/ θ/ t/ |/ s/ =/ st/ ,/ a/ =/ at/ }/ ,/ Eb/ {/ Δ/ 珔/ θ/ t/ }/ =/ Eb/ {/ Δ/ 珔/ θ/ ts/ =/ st/ ,/ a/ =/ at/ }/ ,/ 得/ Eb/ {/ Δ/ 珔/ θ/ t/ }/ =/ Eb/ ∑/ T/ 因此/ ,/ 要/ 证明/ Eb/ {/ Δ/ 珔/ θ/ t/ }/ =/ E/ π/ {/ Δ/ θ/ t/ }/ ,/ 即/ 要/ 证明/ {/ Eb/ ∑/ T/ 令/ Ω/ t/ 为/ Agent/ 从/ (/ s0/ ,/ a0/ )/ 开始/ 在/ t/ 时刻/ 所/ 能/ 得到/ 的/ 状态/ 转移/ 序列/ 集合/ ,/ ω/ 是/ 其中/ 的/ 一个/ 样本/ 序列/ ,/ 即/ ω/ ∈/ Ω/ t/ ,/ pb/ (/ ω/ )/ 为/ 在/ 行为/ 策略/ b/ 下/ 得到/ ω/ 序列/ 的/ 概率/ ,/ 则/ t/ =/ 1Page5t/ =/ 1t/ =/ 1t/ =/ 1i/ =/ 1/ {/ Eb/ ∑/ T/ =/ ∑/ Tt/ =/ 1/ ∑/ ω/ ∈/ Ω/ t/ =/ ∑/ T/ / t/ ∑/ ω/ ∈/ Ω/ t/ ∏/ t/ π/ (/ sj/ ,/ aj/ )/ ∏/ tb/ (/ sj/ ,/ aj/ )/ (/ Eb/ {/ 珚/ R/ λ/ t/ |/ s/ =/ st/ ,/ a/ =/ at/ }/ -/ θ/ T/ / t/ )/ =/ ∑/ T/ / t/ ∑/ ω/ ∈/ Ω/ t/ ∏/ t/ (/ Eb/ {/ 珚/ R/ λ/ t/ |/ s/ =/ st/ ,/ a/ =/ at/ }/ -/ θ/ T/ / t/ )/ 又/ 因为/ / st/ ∈/ S/ ,/ at/ ∈/ A/ ,/ Eb/ {/ 珚/ R/ λ/ t/ |/ s/ =/ st/ ,/ a/ =/ at/ }/ =/ E/ π/ {/ 珚/ R/ λ/ t/ |/ s/ =/ st/ ,/ a/ =/ at/ }/ ,/ 可/ 得/ {/ Eb/ ∑/ T/ =/ ∑/ Tt/ =/ 1/ (/ Eb/ {/ R/ λ/ t/ |/ s/ =/ st/ ,/ a/ =/ at/ }/ -/ θ/ T/ / t/ )/ =/ ∑/ T/ =/ E/ π/ ∑/ T/ 由/ 定理/ 1/ 可知/ ,/ 在/ Q/ (/ λ/ )/ 中/ 引入/ 关联/ 因子/ ,/ 在/ 任意/ t/ 时刻/ ,/ 对于/ θ/ 的/ 差分/ 累加/ 和/ 与/ 在/ 策略/ TD/ (/ λ/ )/ 中/ θ/ 的/ 差分/ 累加/ 和/ 是/ 一致/ 的/ ./ 进一步/ 可/ 认为/ ,/ 在/ 引入/ 关联/ 因子/ 的/ Q/ (/ λ/ )/ 算法/ 中/ ,/ 行为/ 策略/ 所/ 生成/ 的/ 样本/ 数据/ 与/ 改进/ 目标/ 策略/ 所/ 需要/ 的/ 样本/ 数据/ 的/ 分布/ 是/ 一致/ 的/ ,/ 确保/ 算法/ 不会/ 因为/ 样本/ 数据/ 的/ 不一致性/ 导致/ 无法/ 收敛/ ./ 3/ 犌犇/ 犗/ 犘/ -/ 犙/ (/ λ/ )/ 算法/ t/ =/ 1/ 第/ 2/ 节中/ 主要/ 从/ 向前/ 的/ 观点/ 证明/ 引入/ 重要性/ 关联/ 因子/ 的/ Q/ (/ λ/ )/ 能够/ 取得/ 与/ 在/ 策略/ TD/ (/ λ/ )/ 算法/ 分布/ 一致/ 的/ 样本/ 数据/ ,/ 确保/ 算法/ 不会/ 因为/ 样本/ 数据/ 的/ 不一致性/ 导致/ 无法/ 收敛/ ./ 本节/ 将/ 利用/ 向/ 后/ 的/ 观点/ 具体/ 阐述/ GDOP/ -/ Q/ (/ λ/ )/ 算法/ 的/ 执行/ 流程/ ,/ 并/ 证明/ 算法/ 的/ 收敛性/ ./ 3.1/ 犌犇/ 犗/ 犘/ -/ 犙/ (/ λ/ )/ 在/ 向前/ 的/ 观点/ 中/ ,/ 主要/ 引入/ n/ 步/ 回报/ 值/ ,/ 即/ 对/ 当前/ 动作/ 值/ 的/ 估计/ 是/ 基于/ 后续/ n/ 个/ 奖赏/ 值/ ,/ 但是/ 在/ 实际/ 执行/ 过程/ 中/ ,/ 无法/ 利用/ 增量/ 的/ 方式/ 计算/ 后续/ n/ 个/ 奖赏/ 值/ 的/ 估计/ ./ Sutton/ 等/ 人/ 在/ 文献/ [/ 16/ ]/ 中/ 提出/ 了/ 在/ 概念/ 和/ 计算/ 上/ 比较/ 方便/ 的/ 向/ 后/ 的/ 观点/ ./ 利用/ 向/ 后/ 的/ 观点/ ,/ 可以/ 轻易/ 实现/ 算法/ 的/ 增量/ 式/ 更新/ ./ 在/ 向/ 后/ 观点/ 中/ ,/ 资格/ 迹是/ 一个/ 新/ 引入/ 的/ 参数/ ,/ 它/ 反映/ 了/ 当前/ 状态/ (/ 或/ 状态/ 动作/ 对/ )/ 之前/ n/ 个/ 状态/ (/ 或/ 状态/ 动作/ 对/ )/ 对/ 当前/ TD/ -/ Error/ 的/ 影响/ 程度/ ./ 定义/ 3/ ./ 假设/ 当前/ 时刻/ t/ ,/ 状态/ 动作/ 对/ (/ st/ ,/ at/ )/ ,/ / (/ st/ ,/ at/ )/ 是/ 关于/ (/ st/ ,/ at/ )/ 的/ 特征向量/ ,/ / (/ st/ ,/ at/ )/ =/ {/ / (/ 1/ )/ ,/ / (/ 2/ )/ ,/ …/ ,/ / (/ n/ -/ 1/ )/ ,/ / (/ n/ )/ }/ ,/ 犲/ t/ 是/ 在/ 策略/ 下/ 的/ 资格/ 迹/ ,/ 犲/ t/ =/ {/ 犲/ t/ [/ 1/ ]/ ,/ 犲/ t/ [/ 2/ ]/ ,/ …/ ,/ 犲/ t/ [/ n/ ]/ }/ ,/ 犲/ t/ 是/ 离/ 策略/ 下/ 的/ 资格/ 迹/ ,/ 犲/ t/ =/ {/ 犲/ t/ [/ 1/ ]/ ,/ 犲/ t/ [/ 2/ ]/ ,/ …/ ,/ 犲/ t/ [/ n/ ]/ }/ ,/ 则/ 犲/ t/ [/ i/ ]/ =/ IsstIaat/ +/ 犲/ t/ [/ i/ ]/ =/ ω/ =/ 其中/ γ/ 是/ 折扣/ 因子/ ,/ λ/ 是/ 衰减/ 因子/ ,/ ω/ 是迹/ 因子/ ,/ ρ/ (/ st/ ,/ at/ )/ 是/ 重要性/ 关联/ 因子/ ,/ Ixy/ 是/ 一致性/ 标识/ 函数/ ,/ 如果/ x/ =/ y/ ,/ Ixy/ =/ 1/ ,/ 否则/ Ixy/ =/ 0/ ./ 利用/ 定义/ 3/ 给出/ 的/ 离/ 策略/ 下/ 的/ 资格/ 迹/ ,/ 下面/ 给出/ 完整/ 的/ GDOP/ -/ Q/ (/ λ/ )/ 算法/ ./ 算法/ 1/ ./ GDOP/ -/ Q/ (/ λ/ )/ 算法/ ./ 1/ ./ 初始化/ ./ 任意/ 值/ 初始化/ 资格/ 迹/ 犲/ 和/ 犲/ ,/ θ/ =/ 0/ ,/ π/ 是/ greedy/ 策略/ ,/ b/ 是/ ε/ -/ greedy/ 策略/ ./ 2/ ./ 在/ 每个/ 时间/ 步/ t/ ,/ 状态/ 动作/ 对/ (/ st/ ,/ at/ )/ ,/ / (/ st/ ,/ at/ )/ 是/ 关于/ (/ st/ ,/ at/ )/ 的/ 特征向量/ ,/ 判断/ 当前/ 动作/ at/ 是否是/ 贪心/ 动作/ ,/ 即/ 判断/ 是否/ Q/ (/ st/ ,/ at/ )/ =/ maxaQ/ (/ st/ ,/ at/ )/ ./ 如果/ 判断/ 成立/ ,/ 则/ ω/ =/ ω/ ρ/ (/ st/ ,/ at/ )/ ,/ 对于/ 任意/ / (/ i/ )/ ∈/ / (/ st/ ,/ at/ )/ ,/ 有/ 否则/ ,/ ω/ =/ 1/ ,/ 且/ 犲/ t/ =/ 0/ ./ 犲/ t/ [/ i/ ]/ =/ γ/ λ/ 犲/ t/ -/ 1/ (/ i/ )/ +/ 1/ ,/ 犲/ t/ [/ i/ ]/ =/ γ/ λ/ 犲/ t/ [/ i/ ]/ ω/ +/ 犲/ t/ [/ i/ ]/ ,/ 3/ ./ 采用/ 动作/ at/ ,/ 得到/ 立即/ 奖赏/ rt/ +/ 1/ ,/ 环境/ 迁移/ 至下/ 一个/ 状态/ st/ ./ 如果/ at/ 是/ 贪心/ 动作/ ,/ 转至步/ 4/ ,/ 否则/ ,/ 转至步/ 2.4/ ./ 计算/ Q/ (/ st/ ,/ at/ )/ =/ θ/ T/ / (/ st/ ,/ at/ )/ ,/ δ/ t/ =/ rt/ +/ 1/ -/ Q/ (/ st/ ,/ at/ )/ ./ 5/ ./ 对于/ 所有/ at/ =/ A/ (/ st/ )/ ,/ A/ (/ st/ )/ 是/ 状态/ st/ 下/ 动作/ 集合/ ,/ 计算/ Q/ (/ st/ ,/ at/ )/ =/ θ/ T/ / (/ st/ ,/ at/ )/ ./ 6/ ./ 取/ st/ 下/ 的/ 贪心/ 动作/ ,/ at/ =/ argmaxaQ/ (/ st/ ,/ at/ )/ ./ 7/ ./ 计算/ δ/ t/ =/ δ/ t/ +/ γ/ Q/ (/ st/ ,/ at/ )/ ,/ θ/ t/ =/ θ/ t/ -/ 1/ +/ α/ t/ δ/ t/ 犲/ t/ ,/ α/ t/ =/ k/ α/ t/ -/ 1/ ,/ (/ k/ ∈/ (/ 0/ ,/ 1/ ]/ )/ ,/ 如果/ θ/ 收敛/ ,/ 算法/ 终止/ ,/ 否则/ 转至步/ 2.3/ ./ 2/ 收敛性/ 分析/ 向前/ 观点/ 主要/ 从/ 理论/ 的/ 角度/ 理解/ 算法/ ,/ 向/ 后/ 的/ 观点/ 主要/ 从/ 概念/ 和/ 计算/ 的/ 角度/ 描述/ 算法/ ./ 算法/ 1/ 给出/ 了/ GDOP/ -/ Q/ (/ λ/ )/ 算法/ 完整/ 的/ 执行/ 流程/ ./ 接下来/ 再/ Page6/ 次/ 利用/ 向前/ 观点/ ,/ 证明/ 算法/ 的/ 收敛/ ,/ 下面/ 给出/ 算法/ 收敛性/ 定理/ ./ 定理/ 2/ ./ GDOP/ -/ Q/ (/ λ/ )/ 算法/ 的/ 收敛性/ ./ 根据/ 向前/ 的/ 观点/ ,/ 利用/ 式/ (/ 18/ )/ 、/ (/ 19/ )/ 计算/ θ/ 值/ ./ 设/ 其中/ β/ t/ =/ η/ α/ t/ ,/ η/ >/ 0/ ,/ β/ t/ ,/ α/ t/ ∈/ (/ 0/ ,/ 1/ ]/ ,/ 且/ ∑/ 进一步/ 假设/ (/ / t/ ,/ rt/ ,/ / t/ )/ 满足/ 独立/ 同/ 分布/ ,/ 令/ 犃/ =/ E/ / t/ ∑/ T/ -/ 1b/ =/ E/ / t/ ∑/ T/ -/ 1/ 且/ 假设/ 犃/ 是/ 满/ 秩/ 矩阵/ ,/ 则/ θ/ 必定/ 收敛/ ,/ 且/ 满足/ 犃/ θ/ -/ b/ =/ 0/ (/ λ/ -/ TDSolution/ )/ ./ 证明/ ./ 首先/ 利用/ 式/ (/ 19/ )/ 、/ (/ 20/ )/ ,/ 将/ μ/ t/ ,/ θ/ t/ 合并/ 写成/ 一个/ 长度/ 为/ 2n/ 的/ 向量/ ,/ ρ/ Tt/ =/ (/ 狏/ Tt/ ,/ θ/ Tt/ )/ ./ 狏/ t/ =/ μ/ t/ // 槡/ η/ ,/ 同时/ 构造/ 一个/ 长度/ 为/ 2n/ 的/ 向量/ ,/ 犵/ T/ 且/ ρ/ t/ +/ 1/ =/ ρ/ t/ +/ α/ t/ 槡/ η/ (/ Gt/ +/ 1/ ρ/ t/ +/ 犵/ t/ +/ 1/ )/ ,/ 其中/ Gt/ +/ 1/ =/ -/ 槡/ η/ I/ ,/ / t/ ∑/ T/ -/ 1/ 烄/ (/ γ/ λ/ )/ i/ -/ t/ ∏/ i/ -/ t/ ∑/ T/ -/ 1/ 烆/ i/ =/ t/ 设/ 犌/ 是/ Gt/ 的/ 期望/ ,/ 犵/ 是/ gt/ 的/ 期望/ ,/ 则/ ./ 则/ 由/ 犌/ ρ/ +/ 犵/ =/ 0/ 可以/ 推导/ 出/ 犃/ θ/ -/ b/ =/ 0/ ,/ 其中/ ρ/ T/ =/ (/ 狏/ T/ ,/ θ/ T/ )/ ./ 令/ ρ/ t/ +/ 1/ =/ ρ/ t/ +/ α/ t/ 槡/ η/ (/ 犌/ ρ/ t/ +/ 犵/ +/ (/ Gt/ +/ 1/ -/ 犌/ )/ ρ/ t/ +/ (/ 犵/ t/ +/ 1/ -/ 犵/ )/ )/ =/ ρ/ t/ +/ α/ t/ (/ h/ (/ ρ/ t/ )/ +/ Mt/ +/ 1/ )/ ,/ 其中/ α/ t/ =/ α/ t/ 槡/ η/ ,/ h/ (/ ρ/ t/ )/ =/ 犌/ ρ/ t/ +/ 犵/ ,/ Mt/ +/ 1/ =/ (/ Gt/ +/ 1/ -/ 犌/ )/ ρ/ t/ +/ (/ 犵/ t/ +/ 1/ -/ 犵/ )/ ./ 令/ Γ/ t/ =/ σ/ (/ ρ/ 1/ ,/ M1/ ,/ ρ/ 2/ ,/ M2/ ,/ …/ ,/ ρ/ t/ ,/ Mt/ )/ ./ 若能/ 证明/ ρ/ t/ +/ 1/ =/ ρ/ t/ +/ α/ t/ (/ h/ (/ ρ/ t/ )/ +/ Mt/ +/ 1/ )/ 收敛/ 于/ 犌/ ρ/ +/ 犵/ =/ 0/ ,/ 则/ 可以/ 推出/ 犃/ θ/ -/ b/ =/ 0/ ,/ 定理/ 得证/ ./ 下面/ 证明/ ρ/ t/ +/ 1/ =/ ρ/ t/ +/ α/ t/ (/ h/ (/ ρ/ t/ )/ +/ Mt/ +/ 1/ )/ 收敛/ ./ 为此/ ,/ 利用/ 文献/ [/ 17/ ]/ 给出/ 的/ 定理/ 2.2/ ./ 根据/ 定理/ 2.2/ ,/ 只/ 需要/ 验证/ 以下/ 4/ 个/ 条件/ :/ (/ 1/ )/ 函数/ h/ (/ ρ/ t/ )/ 满足/ Lipschitz/ 条件/ 且/ h/ (/ ρ/ )/ =/ limr/ →/ h/ (/ r/ ρ/ )/ // r/ 存在/ ./ h/ (/ ρ/ 1/ )/ -/ h/ (/ ρ/ 1/ )/ 2/ =/ g/ +/ 犌/ ρ/ 1/ -/ (/ g/ +/ 犌/ ρ/ 2/ )/ 2/ 验证/ :/ 对于/ / ρ/ 1/ ,/ ρ/ 2/ ,/ 有/ 可见/ 函数/ h/ (/ ρ/ t/ )/ 满足/ Lipschitz/ 条件/ ./ 其次/ ,/ limr/ →/ h/ (/ r/ ρ/ )/ // r/ =/ limr/ →/ 犌/ ρ/ ,/ 由于/ 犵/ 有限/ ,/ 因此/ ,/ limr/ →/ 犵/ // r/ =/ 0/ ,/ 从而/ h/ (/ ρ/ )/ =/ limr/ →/ h/ (/ r/ ρ/ )/ // r/ 存在/ 成立/ ./ (/ 2/ )/ 数列/ (/ Mt/ ,/ Γ/ t/ )/ 是/ 熵/ 差分/ 序列/ ,/ 存在/ τ/ 满足/ 犈/ [/ Mt/ +/ 12/ Γ/ t/ |/ ]/ / τ/ (/ 1/ +/ ρ/ t/ 验证/ :/ Mt/ +/ 12/ =/ (/ Gt/ +/ 1/ -/ 犌/ )/ ρ/ t/ +/ (/ 犵/ t/ +/ 1/ -/ 犵/ )/ 2/ 令/ τ/ =/ max/ (/ Gt/ +/ 1/ -/ 犌/ )/ 2/ ,/ (/ 犵/ t/ +/ 1/ -/ 犵/ )/ Mt/ +/ 12/ / τ/ (/ 1/ +/ ρ/ t/ (/ 3/ )/ 由/ 定理/ 条件/ 可知/ ,/ ∑/ 条件/ 成立/ ./ (/ 4/ )/ 常/ 微分方程/ ρ/ ·/ =/ h/ (/ ρ/ )/ 存在/ 一个/ 全局/ 稳定/ 近似/ 解/ ./ 验证/ :/ 由/ 线性/ 微分方程/ 组/ 理论/ 可知/ ,/ 如果/ 犌/ 的/ 所有/ 特征值/ 的/ 实部/ 均/ 为/ 负数/ ,/ 便/ 可/ 推出/ ρ/ ·/ =/ h/ (/ ρ/ )/ =/ 犵/ +/ 犌/ ρ/ 满足条件/ ./ 问题/ 可/ 转化/ 为/ 验证/ 犌/ 的/ 所有/ 特征值/ 都/ 小于/ 0/ ./ 由/ 犌/ =/ -/ 槡/ η/ I/ -/ 犃/ |/ 犌/ |/ =/ -/ 槡/ η/ I0/ -/ 犃/ T/ (/ -/ 槡/ η/ I/ )/ (/ -/ 犃/ )/ =/ (/ -/ 槡/ η/ )/ n/ (/ -/ 1/ )/ n/ (/ 1/ // 槡/ η/ )/ n/ 犃/ T/ 犃/ =/ 犃/ T/ 犃/ ./ 由于/ 犃/ ≠/ 0/ ,/ 且/ 犃/ T/ =/ 犃/ ,/ 得/ 犌/ =/ 犃/ 2/ ≠/ 0/ ./ 设/ 犌/ 的/ 所有/ 特征值/ 为/ λ/ 1/ ,/ λ/ 2/ ,/ …/ ,/ λ/ 2n/ ,/ 则/ 犌/ =/ ∏/ 2n/ λ/ k/ ≠/ 0/ ,/ 则/ 犌/ 的/ 所有/ 特征值/ 均/ 不/ 为/ 0/ ./ k/ =/ 1/ 设/ λ/ 是/ 犌/ 的/ 一个/ 特征值/ ,/ 狓/ 是/ 标准/ 特征向量/ ,/ 则/ 犌/ 狓/ =/ λ/ 狓/ ,/ 且/ 狓/ T/ 狓/ =/ 1/ ./ 因此/ 狓/ T/ 犌/ 狓/ =/ 狓/ T/ λ/ 狓/ =/ λ/ 狓/ T/ 狓/ ,/ 得/ 狓/ T/ 犌/ 狓/ =/ λ/ ./ 令/ 狓/ =/ x1/ λ/ =/ 狓/ T/ 犌/ 狓/ =/ (/ 狓/ T1/ ,/ 狓/ T2/ )/ -/ 槡/ η/ I/ -/ 犃/ =/ (/ 狓/ T1/ ,/ 狓/ T2/ )/ -/ 槡/ η/ 狓/ 1/ -/ 犃/ 狓/ 2/ =/ -/ 槡/ η/ 狓/ T1/ 狓/ 1/ -/ 狓/ T1/ 犃/ 狓/ 2/ +/ 狓/ T2/ 犃/ T/ 狓/ 1/ 由于/ 狓/ T1/ 犃/ 狓/ 2/ =/ 狓/ T2/ 犃/ T/ 狓/ 1/ ,/ 得/ 因此/ ,/ λ/ 的/ 实部/ 就是/ -/ 槡/ η/ 狓/ 12/ ,/ 又/ 因为/ G/ 的/ 所有/ 特征值/ 均/ 不/ 为/ 0/ ,/ 故/ λ/ ≠/ 0/ ./ 因此/ λ/ 的/ 实部/ Re/ (/ λ/ )/ </ Page70/ ,/ 因此/ 问题/ 得证/ ,/ 满足/ 第/ 4/ 个/ 条件/ ./ 根据/ 文献/ [/ 17/ ]/ 给出/ 的/ 定理/ 2.2/ ,/ 且/ 定理/ 满足/ 以上/ 4/ 个/ 条件/ ,/ 则/ ρ/ t/ +/ 1/ =/ ρ/ t/ +/ α/ t/ (/ h/ (/ ρ/ t/ )/ +/ Mt/ +/ 1/ )/ 收敛/ 于/ 犌/ ρ/ +/ 犵/ =/ 0/ ,/ 因此/ 可/ 推出/ 犃/ θ/ -/ b/ =/ 0/ ./ 证毕/ ./ 4/ 实验/ 结果/ 分析/ 为了/ 验证/ 算法/ 的/ 有效性/ ,/ 本文/ 以/ 强化/ 学习/ 中/ 经典/ 的/ Baird/ 反例/ 、/ MountainCar/ 和/ RandomWalk/ 为例/ ./ 其中/ Baird/ 反例/ 是/ 经典/ 的/ 用于/ 验证/ 基于/ 函数/ 逼近/ 的/ 离/ 策略/ 学习/ 算法/ 无法/ 收敛/ 的/ 实例/ ,/ 如图/ 1/ 所示/ ./ MountainCar/ 是/ 强化/ 学习/ 中/ 另/ 一个/ 经典/ 的/ 连续/ 状态/ 空间/ 、/ 情节/ 式/ 任务/ (/ 情节/ 式/ 任务/ 是/ 指/ 包含/ 终结状态/ 的/ 强化/ 学习/ 任务/ )/ ,/ 其中/ 状态/ 的/ 表示/ 较/ Baird/ 反例/ 和/ RandomWalk/ 更为/ 复杂/ ,/ 状态/ 空间/ 也/ 更/ 大/ ,/ 如图/ 2/ 所示/ ./ RandomWalk/ 是/ 一个/ 典型/ 的/ 无/ 折扣/ 、/ 情节/ 式/ 的/ 强化/ 学习/ 任务/ ,/ 经典/ 的/ RandomWalk/ 是/ 在/ 一条/ 直线/ 上/ 包含/ 5/ 的/ 状态/ ,/ 在/ 直线/ 的/ 两端/ 各有/ 一个/ 终结状态/ ./ 当/ Agent/ 到达/ 最右/ 端的/ 终结状态/ 时/ ,/ 立即/ 奖赏/ 为/ 1/ ,/ 其它/ 情况/ 下/ ,/ 立即/ 奖赏/ 为/ 0/ ,/ 如图/ 3/ 所示/ ./ 为了/ 验证/ 算法/ 对于/ 状态/ 空间/ 增长/ 的/ 鲁棒性/ ,/ 本文/ 将/ 经典/ 的/ RandomWalk/ 扩展/ 为/ 19/ 个/ 状态/ ,/ 并/ 验证/ 算法/ 的/ 有效性/ ./ 实验/ 1/ 是/ 强化/ 学习/ 中/ 经典/ 的/ Baird/ 反例/ ,/ 由/ Baird/ 在/ 1995/ 年/ 提出/ ,/ 用于/ 说明/ 经典/ 的/ 基于/ 函数/ 逼近/ 的/ 离/ 策略/ 强化/ 学习/ 算法/ 无法/ 收敛/ ./ 该/ 实验/ 包含/ 6/ 个/ 状态/ ,/ 状态/ 表示/ 函数/ 及/ 状态/ 之间/ 的/ 转移/ 情况/ 如图/ 1/ 所示/ ,/ 且/ 对于/ 所有/ 状态/ 转移/ 的/ 立即/ 奖赏/ 值/ 都/ 是/ 0/ ./ 因此/ ,/ 该/ 实验/ 中/ 每个/ 状态/ 的/ 真实/ 状态值/ (/ 或者/ 动作/ 值/ )/ 都/ 是/ 0/ ,/ 状态/ 表示/ 函数/ 的/ 参数/ θ/ 值/ 都/ 是/ 0/ ./ 将/ 经典/ 的/ 离/ 策略/ 动态/ 规划/ 算法/ 、/ 基于/ 函数/ 逼近/ 的/ Q/ (/ λ/ )/ 算法/ —/ —/ —/ Watkins/ ’/ sQ/ (/ λ/ )/ 算法/ [/ 18/ ]/ 及/ 本文/ 提出/ 的/ GDOP/ -/ Q/ (/ λ/ )/ 用于/ Baird/ 反例/ ,/ 其中/ γ/ =/ 0.99/ ,/ α/ =/ 0.01/ ,/ λ/ =/ 0.1/ ,/ 实验/ 结果/ 如图/ 4/ ~/ 图/ 6/ 所示/ ./ 图中/ 横坐标/ 表示/ 算法/ 所/ 执行/ 的/ episode/ 数目/ ,/ 纵坐标/ 表示/ θ/ 值/ ,/ 以/ θ/ (/ 1/ )/ 的/ 值/ 为例/ ./ 实验/ 表明/ ,/ 对于/ Baird/ 反例/ ,/ 经典/ 的/ 离/ 策略/ 动态/ 规划/ 算法/ 和/ Watkins/ ’/ sQ/ (/ λ/ )/ 算法/ 无法/ 收敛/ ./ 而/ 本文/ 提出/ 的/ 离/ 策略/ GDOP/ -/ Q/ (/ λ/ )/ 利用/ 关联/ 因子/ ,/ 保证/ 行为/ 策略/ 所/ 生成/ 的/ 实验/ 数据/ 和/ 目标/ Page8/ 策略/ 所/ 需要/ 的/ 数据/ 一致/ ,/ 在/ 算法/ 中/ 通过/ 修正/ 资格/ 迹/ et/ 给出/ 新/ 的/ θ/ 更新/ 策略/ ./ 实验/ 结果表明/ ,/ 对于/ 小/ 状态/ 空间/ 的/ Baird/ 反例/ ,/ 算法/ 大约/ 在/ 20/ 个/ episode/ 之后/ 就/ 呈现/ 收敛/ 状态/ ,/ 且/ θ/ (/ 1/ )/ 的/ 值/ 为/ 0/ ./ 实验/ 2/ 将/ Least/ -/ SquaresSARSA/ (/ λ/ )/ [/ 19/ ]/ 、/ Greedy/ -/ GQ/ [/ 14/ ]/ 、/ 基于/ 查询/ 表/ 的/ SARSA/ (/ λ/ )/ 、/ Watkins/ ’/ sQ/ (/ λ/ )/ 及/ GDOP/ -/ Q/ (/ λ/ )/ 算法/ 用于/ MountainCar/ ,/ 比较/ 算法/ 性能/ ,/ 其中/ γ/ =/ 1.0/ ,/ α/ =/ 0.5/ ,/ λ/ =/ 0.95/ ,/ ε/ =/ 0.01/ ./ MountainCar/ 是/ 强化/ 学习/ 中/ 一个/ 经典/ 连续/ 状态/ 空间/ 案例/ ,/ 如图/ 2/ 所示/ ./ 案例/ 中/ 小车/ 由于/ 动力/ 不足/ ,/ 无法/ 直接/ 通过/ 加速/ 达到/ 右侧/ 坡顶/ ,/ 因此/ 需要/ 借助/ 左侧/ 的/ 小坡/ ,/ 通过/ 不断/ 的/ 前后/ 加速/ 到达/ 坡顶/ ./ 小车/ 的/ 动作/ 分为/ 前后/ 加速/ 和/ 不/ 加速/ 3/ 个/ 动作/ ,/ 状态/ 属于/ 一段/ 连续/ 的/ 状态/ 空间/ ,/ 小车/ 到达/ 右侧/ 坡顶/ 获得/ 值为/ 1/ 的/ 立即/ 奖赏/ ,/ 其它/ 情况/ 立即/ 奖赏/ 为/ 0/ ./ 实验/ 结果/ 如图/ 7/ ~/ 图/ 11/ 所示/ ,/ 图中/ 横坐标/ 表示/ 算法/ 所/ 执行/ 的/ episode/ 数目/ ,/ 纵坐标/ 表示/ 小车/ 爬/ 上/ 坡顶/ 所/ 需要/ 的/ 步数/ ./ 通过/ 比较/ ,/ 不难/ 发现/ ,/ 其中/ Watkins/ ’/ sQ/ (/ λ/ )/ 算法/ 无法/ 收敛/ ,/ 其它/ 算法/ 包括/ 本文/ 提出/ 的/ GDOP/ -/ Q/ (/ λ/ )/ 算法/ 都/ 能/ 收敛/ ./ 比较/ 图/ 10/ 和/ 图/ 11/ ,/ Least/ -/ SquaresSARSA/ (/ λ/ )/ 算法/ 的/ 收敛/ 性能/ 和/ 收敛/ 速度/ 均/ 优于/ 基于/ 查询/ 表/ 的/ SARSA/ (/ λ/ )/ 算法/ ,/ 且/ 基于/ 查询/ 表/ 的/ SARSA/ (/ λ/ )/ 算法/ 前期/ 振幅/ 较大/ ./ 这是/ 由于/ 在/ 大规模/ 状态/ 空间/ 中/ ,/ 利用/ 有限/ 训练/ 集/ 得出/ 最优/ 参数/ 可以/ 泛化/ 至/ 整个/ 状态/ 空间/ ,/ 能够/ 保证/ 算法/ 在/ 有限/ 的/ 训练/ 次数/ 后/ 得到/ 较/ 好/ 的/ 收敛/ 性能/ ./ 比较/ 图/ 9/ 和/ 图/ 11/ ,/ 可以/ 看出/ ,/ 在/ 当前/ 状态/ 空间/ 下/ ,/ Greedy/ -/ GQ/ 的/ 收敛/ 速度/ 又/ 要/ 优于/ Least/ -/ SquaresSARSA/ (/ λ/ )/ 算法/ ,/ 但/ 两者/ 的/ 收敛/ 结果/ 类似/ ./ 比较/ 图/ 8/ 和/ 图/ 9/ ,/ GDOP/ -/ Q/ (/ λ/ )/ 算法/ 略/ 优于/ Greedy/ -/ GQ/ ./ 这/ 是因为/ 利用/ 相关性/ 因子/ ,/ 保证/ 行为/ 策略/ 所/ 生成/ 的/ 实验/ 数据/ 和/ 改进/ 策略/ 所/ 需要/ 的/ 数据/ 一致/ ,/ 进而/ 保证/ 算法/ 收敛/ ./ 综合/ 比较/ 所有/ 算法/ 性能/ 图/ ,/ GDOP/ -/ Q/ (/ λ/ )/ 算法/ 能/ 在/ 保证/ 收敛/ 的/ 情况/ ,/ 具有/ 更好/ 的/ 收敛/ 性能/ ./ Page9/ 实验/ 3/ 主要/ 用于/ 检验/ 算法/ 在/ 状态/ 空间/ 发生变化/ 时/ 收敛/ 性能/ 的/ 鲁棒性/ ./ 将/ 基于/ 查询/ 表/ SARSA/ (/ λ/ )/ 、/ Least/ -/ SquaresSARSA/ (/ λ/ )/ 及/ GDOP/ -/ Q/ (/ λ/ )/ 算法/ 用于/ RandomWalk/ ,/ 其中/ γ/ =/ 1.0/ ,/ α/ =/ 0.5/ ,/ λ/ =/ 0.95/ ,/ ε/ =/ 0.01/ ./ 经典/ 的/ RandomWalk/ 中/ 包含/ 5/ 个/ 状态/ ,/ 两端/ 各有/ 一个/ 终/ 状态/ ,/ 呈/ 线性/ 排列/ ,/ 状态/ 之间/ 的/ 转移/ 情况/ 如图/ 3/ 所示/ ./ 当/ 到达/ 右侧/ 终结状态/ 时/ ,/ 立即/ 奖赏/ 为/ 1/ ,/ 其它/ 情况/ 下/ ,/ 立即/ 奖赏/ 为/ 0/ ./ 为了/ 验证/ 算法/ 对于/ 状态/ 空间/ 发生变化/ 时/ 的/ 鲁棒性/ ,/ 将/ 经典/ 的/ 5/ 个/ 状态/ 的/ RandomWalk/ 扩展/ 为/ 19/ 个/ 状态/ ./ 实验/ 结果/ 如图/ 12/ 所示/ ,/ 图中/ 每个/ 点/ 的/ 数据/ 值/ 是/ 对/ 每个/ 算法/ 重复/ 执行/ 100/ 次/ 之后/ 状态值/ 标准差/ (/ RootMeanSquare/ ,/ RMS/ )/ 的/ 平均值/ ,/ 其中/ 横坐标/ 表示/ 算法/ 执行/ 的/ episode/ 数目/ ,/ 纵坐标/ 是/ 在/ 当前/ episode/ 数下/ 算法/ 执行/ 100/ 次/ 之后/ RMS/ 的/ 平均值/ ./ 对于/ 5/ 个/ 状态/ 的/ RandomWalk/ ,/ 3/ 个/ 算法/ 都/ 取得/ 类似/ 的/ 实验/ 结果/ ./ 但是/ 当/ RandomWalk/ 中/ 包含/ 19/ 个/ 状态/ 时/ ,/ 不难/ 发现/ ,/ 对于/ Least/ -/ SquaresSARSA/ (/ λ/ )/ 和/ GDOP/ -/ Q/ (/ λ/ )/ 算法/ ,/ 在/ 收敛/ 性能/ 上/ 与/ 只有/ 5/ 个/ 状态/ 的/ RandomWalk/ 下/ 的/ 性能/ 基本一致/ ,/ GDOP/ -/ Q/ (/ λ/ )/ 在/ 收敛/ 速度/ 和/ 收敛/ 性能/ 上/ 略/ 优于/ Least/ -/ SquaresSARSA/ (/ λ/ )/ ./ 但是/ 对于/ 基于/ 查询/ 表/ 的/ SARSA/ (/ λ/ )/ 算法/ ,/ 在/ 相同/ 的/ 训练/ 次数/ 下/ ,/ 在/ 收敛/ 速度/ 和/ 收敛/ 性能/ 上/ 都/ 明显/ 下降/ ./ 这是/ 由于/ 基于/ 函数/ 逼近/ 的/ 学习/ 算法/ 所求/ 的/ θ/ 值/ 与/ 具体/ 的/ 状态/ 无关/ ,/ 仅/ 与/ 组成/ 状态/ 的/ 特征/ 相关/ ./ 在/ 相同/ 的/ 训练/ 次数/ 下/ ,/ 基于/ 函数/ 逼近/ 的/ 学习/ 算法/ 可以/ 取得/ 更好/ 的/ 收敛/ 性能/ ./ 因此/ ,/ GDOP/ -/ Q/ (/ λ/ )/ 的/ 收敛/ 性能/ 在/ 状态/ 空间/ 发生变化/ 时/ 具有/ 较强/ 的/ 鲁棒性/ ./ 图/ 12/ 基于/ 查询/ 表/ SARSA/ (/ λ/ )/ 、/ Least/ -/ SquaresSARSA/ (/ λ/ )/ 及/ GDOP/ -/ Q/ (/ λ/ )/ 算法/ 在/ 状态/ 空间/ 发生变化/ 时/ 的/ 收敛/ 图/ 5/ 结束语/ 针对/ 当前/ 强化/ 学习/ 领域/ 中/ 经典/ 的/ 基于/ 查询/ 表/ 的/ 强化/ 学习/ 算法/ 在/ 大规模/ 状态/ 空间/ 中/ 收敛/ 速度慢/ 或/ 无法/ 收敛/ 的/ 问题/ ,/ 本文/ 提出/ 利用/ 函数/ 逼近/ 方法/ 求解/ 强化/ 学习策略/ ./ 并/ 进一步/ 针对/ 将/ 函数/ 逼近/ 用于/ 经典/ 离/ 策略/ 强化/ 学习/ 算法/ —/ —/ —/ Q/ (/ λ/ )/ 算法/ 无法/ 收敛/ 的/ 问题/ ,/ 引入/ 重要性/ 关联/ 因子/ ,/ 并/ 证明/ 利用/ 重要性/ 关联/ 因子/ 可以/ 将/ 在/ 策略/ 与/ 离/ 策略/ 相/ 统一/ ./ 基于/ 以上/ 分析/ ,/ 提出/ 一种/ 基于/ 线性/ 函数/ 逼近/ 的/ 离/ 策略/ Q/ (/ λ/ )/ 算法/ —/ —/ —/ GDOP/ -/ Q/ (/ λ/ )/ ,/ 并/ 证明/ 算法/ 的/ 收敛性/ ./ 本文/ 以/ 经典/ 的/ Baird/ 反例/ 、/ MountainCar/ 以及/ Randomwalk/ 为例/ ,/ 将/ GDOP/ -/ Q/ (/ λ/ )/ 与/ 经典/ 动态/ 规划/ 算法/ 、/ 基于/ 查询/ 表在/ 策略/ SARSA/ (/ λ/ )/ 算法/ 、/ Least/ -/ SquaresSARSA/ (/ λ/ )/ 、/ Greedy/ -/ GQ/ 、/ Watkins/ ’/ sQ/ (/ λ/ )/ 算法/ 相/ 比较/ ./ 实验/ 结果表明/ ,/ 该/ 算法/ 不同于/ 经典/ 的/ Watkins/ ’/ sQ/ (/ λ/ )/ 算法/ ,/ 在/ 3/ 个/ 例子/ 中/ 都/ 能够/ 收敛/ ;/ 同时/ 与/ Least/ -/ SquaresSARSA/ (/ λ/ )/ 算法/ 相比/ ,/ 算法/ 性能/ 基本一致/ ,/ 收敛/ 速度/ 略有/ 提高/ ;/ 与/ 基于/ 查询/ 表/ 的/ SARSA/ (/ λ/ )/ 相比/ ,/ 在/ 状态/ 空间/ 发生变化/ ,/ 算法/ 具有/ 较强/ 的/ 鲁棒性/ ./ 在/ 强化/ 学习/ 领域/ ,/ 如何/ 平衡/ 利用/ (/ Agent/ 采用/ 已知/ 的/ 最优/ 策略/ )/ 和/ 探索/ (/ Agent/ 探索/ 新/ 的/ 策略/ ,/ 该/ 策略/ 可能/ 优于/ 当前/ 的/ 最优/ 策略/ )/ 问题/ 是/ 强化/ 学习/ 领域/ 中/ 的/ 一个/ 难点/ ./ 该/ 问题/ 在/ 本文/ 中/ 并/ 没有/ 给出/ 明确/ 的/ 解决方案/ ,/ 在/ 本文/ 的/ 实际/ 实验/ 中仅/ 是/ 采用/ ε/ -/ greedy/ 方法/ 来/ 平衡/ ./ 然而/ ε/ -/ greedy/ 方法/ 没有/ 针对/ 状态/ 本身/ 的/ 不确定性/ 给出/ 明确/ 的/ 探索/ 或者/ 利用/ 信号/ ,/ 具有/ 一定/ 的/ 随机性/ ,/ 弱化/ 了/ 算法/ 的/ 收敛性/ ./ 因此/ ,/ 下/ 一步/ 的/ 工作/ 将/ 从/ 平衡/ 探索/ 和/ 利用/ 的/ 角度/ ,/ 结合/ 离/ 策略/ 的/ 学习/ 算法/ ,/ 提出/ 改进/ 策略/ ,/ 进一步/ 加快/ 算法/ 的/ 收敛/ 速度/ ./ 

